
<!DOCTYPE html><html lang="en">

<head>
  <meta charset="utf-8">
  <meta name="hexo-theme" content="https://github.com/xaoxuu/hexo-theme-stellar/tree/1.33.1" theme-name="Stellar" theme-version="1.33.1">
  
  
  <meta name="generator" content="Hexo 6.3.0">
  <meta http-equiv='x-dns-prefetch-control' content='on' />
  
  <meta name="renderer" content="webkit">
  <meta name="force-rendering" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=Edge,chrome=1">
  <meta name="HandheldFriendly" content="True" >
  <meta name="mobile-web-app-capable" content="yes">
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="theme-color" media="(prefers-color-scheme: dark)" content="#000">
  <meta name="theme-color" content="#f9fafb">
  <title>BERT å®Œæ•´è§£æï¼šä»è®ºæ–‡åˆ° KV Cache - Caiusçš„æ—¥å¸¸åšå®¢</title>

  
    <meta name="description" content="BERT å®Œæ•´è§£æï¼šä»è®ºæ–‡åˆ° KV Cache å­¦ä¹ ç›®æ ‡ï¼šæ·±åº¦ç†è§£ BERT è®ºæ–‡æ ¸å¿ƒåŸç†ã€Q&#x2F;K&#x2F;V äº¤äº’æœºåˆ¶ã€KV Cache ä¼˜åŒ–æŠ€æœ¯ã€Causal Attentionï¼Œä»¥åŠå®Œæ•´çš„è®­ç»ƒé—­ç¯ã€‚   ğŸ–¼ï¸ æ ¸å¿ƒå¯è§†åŒ–å›¾è§£BERT æ•´ä½“æ¶æ„ä¸æ ¸å¿ƒæ¦‚å¿µ) BERT æ¶æ„è¯¦è§£) Q&#x2F;K&#x2F;V æ•°æ®æµå¯è§†åŒ–) æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£) Causal vs Bidirectional Attention å¯¹æ¯”)">
<meta property="og:type" content="article">
<meta property="og:title" content="BERT å®Œæ•´è§£æï¼šä»è®ºæ–‡åˆ° KV Cache">
<meta property="og:url" content="https://caiusy.github.io/2026/02/03/bert-comprehensive-guide/">
<meta property="og:site_name" content="Caiusçš„æ—¥å¸¸åšå®¢">
<meta property="og:description" content="BERT å®Œæ•´è§£æï¼šä»è®ºæ–‡åˆ° KV Cache å­¦ä¹ ç›®æ ‡ï¼šæ·±åº¦ç†è§£ BERT è®ºæ–‡æ ¸å¿ƒåŸç†ã€Q&#x2F;K&#x2F;V äº¤äº’æœºåˆ¶ã€KV Cache ä¼˜åŒ–æŠ€æœ¯ã€Causal Attentionï¼Œä»¥åŠå®Œæ•´çš„è®­ç»ƒé—­ç¯ã€‚   ğŸ–¼ï¸ æ ¸å¿ƒå¯è§†åŒ–å›¾è§£BERT æ•´ä½“æ¶æ„ä¸æ ¸å¿ƒæ¦‚å¿µ) BERT æ¶æ„è¯¦è§£) Q&#x2F;K&#x2F;V æ•°æ®æµå¯è§†åŒ–) æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£) Causal vs Bidirectional Attention å¯¹æ¯”)">
<meta property="og:locale" content="en_US">
<meta property="og:image" content="https://caiusy.github.io/images/avatar.jpg">
<meta property="article:published_time" content="2026-02-03T07:30:00.000Z">
<meta property="article:modified_time" content="2026-02-20T07:15:58.509Z">
<meta property="article:author" content="Caius Lu">
<meta property="article:tag" content="AI">
<meta name="twitter:card" content="summary_large_image">
<meta name="twitter:image" content="https://caiusy.github.io/images/avatar.jpg">
  
  
  
  <meta name="keywords" content="AI">

  <!-- feed -->
  

  <link rel="stylesheet" href="/css/main.css?v=1.33.1">


  

  

  <script type="application/ld+json">{"@context":"https://schema.org","@type":"BlogPosting","author":{"@type":"Person","name":"Caius Lu","sameAs":[],"image":"https://caiusy.github.io/images/avatar.jpg"},"dateCreated":"2026-02-03T15:30:00+08:00","dateModified":"2026-02-20T15:15:58+08:00","datePublished":"2026-02-03T15:30:00+08:00","description":"","headline":"BERT å®Œæ•´è§£æï¼šä»è®ºæ–‡åˆ° KV Cache","mainEntityOfPage":{"@type":"WebPage","@id":"https://caiusy.github.io/2026/02/03/bert-comprehensive-guide/"},"publisher":{"@type":"Organization","name":"Caius Lu","sameAs":[],"image":"https://caiusy.github.io/images/avatar.jpg","logo":{"@type":"ImageObject","url":"https://caiusy.github.io/images/avatar.jpg"}},"url":"https://caiusy.github.io/2026/02/03/bert-comprehensive-guide/","keywords":"AI","thumbnailUrl":"/images/bert_guide/bert_complete_visualizations.png","image":1}</script>
  <link rel="stylesheet" href="/css/custom.css"><link rel="preconnect" href="https://fonts.googleapis.com"><link rel="preconnect" href="https://fonts.gstatic.com" crossorigin><script async src="//busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script>

<style>.github-emoji { position: relative; display: inline-block; width: 1.2em; min-height: 1.2em; overflow: hidden; vertical-align: top; color: transparent; }  .github-emoji > span { position: relative; z-index: 10; }  .github-emoji img, .github-emoji .fancybox { margin: 0 !important; padding: 0 !important; border: none !important; outline: none !important; text-decoration: none !important; user-select: none !important; cursor: auto !important; }  .github-emoji img { height: 1.2em !important; width: 1.2em !important; position: absolute !important; left: 50% !important; top: 50% !important; transform: translate(-50%, -50%) !important; user-select: none !important; cursor: auto !important; } .github-emoji-fallback { color: inherit; } .github-emoji-fallback img { opacity: 0 !important; }</style>
</head>
<body>

<div class="l_body content" id="start" layout="post" type="tech" ><aside class="l_left"><div class="sidebg"></div><div class="leftbar-container">


<header class="header"><div class="logo-wrap"><a class="avatar" href="/about/"><div class="bg" style="opacity:0;background-image:url(https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/avatar/round/rainbow64@3x.webp);"></div><img no-lazy class="avatar" src="/images/avatar.jpg" onerror="javascript:this.classList.add('error');this.src='https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/image/2659360.svg';"></a><a class="title" href="/"><div class="main">Caiusçš„æ—¥å¸¸åšå®¢</div><div class="sub normal cap">è®¡ç®—æœºè§†è§‰</div><div class="sub hover cap" style="opacity:0"> å¤§æ¨¡å‹åˆå­¦è€…</div></a></div></header>

<div class="nav-area">

<nav class="menu dis-select"><a class="nav-item active" title="åšå®¢" href="/" style="color:#6366F1"><img src="https://api.iconify.design/solar:documents-bold-duotone.svg?color=%236366F1"/></a><a class="nav-item" title="å½’æ¡£" href="/archives/" style="color:#10B981"><img src="https://api.iconify.design/solar:archive-bold-duotone.svg?color=%2310B981"/></a><a class="nav-item" title="åˆ†ç±»" href="/categories/" style="color:#F59E0B"><img src="https://api.iconify.design/solar:folder-with-files-bold-duotone.svg?color=%23F59E0B"/></a><a class="nav-item" title="æ ‡ç­¾" href="/tags/" style="color:#EC4899"><img src="https://api.iconify.design/solar:tag-bold-duotone.svg?color=%23EC4899"/></a></nav>
</div>
<div class="widgets">
<div class="search-wrapper" id="search-wrapper"><form class="search-form"><a class="search-button" onclick="document.getElementById(&quot;search-input&quot;).focus();"><svg t="1705074644177" viewBox="0 0 1025 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="1560" width="200" height="200"><path d="M1008.839137 935.96571L792.364903 719.491476a56.783488 56.783488 0 0 0-80.152866 0 358.53545 358.53545 0 1 1 100.857314-335.166073 362.840335 362.840335 0 0 1-3.689902 170.145468 51.248635 51.248635 0 1 0 99.217358 26.444296 462.057693 462.057693 0 1 0-158.255785 242.303546l185.930047 185.725053a51.248635 51.248635 0 0 0 72.568068 0 51.248635 51.248635 0 0 0 0-72.978056z" p-id="1561"></path><path d="M616.479587 615.969233a50.428657 50.428657 0 0 0-61.498362-5.534852 174.655348 174.655348 0 0 1-177.525271 3.484907 49.403684 49.403684 0 0 0-58.833433 6.76482l-3.074918 2.869923a49.403684 49.403684 0 0 0 8.609771 78.10292 277.767601 277.767601 0 0 0 286.992355-5.739847 49.403684 49.403684 0 0 0 8.404776-76.667958z" p-id="1562"></path></svg></a><input type="text" class="search-input" id="search-input" placeholder="Search"></form><div id="search-result"></div><div class="search-no-result">No Results!</div></div>



<widget class="widget-wrapper slide-up recent post-list"><div class="widget-header dis-select"><span class="name">Recent Update</span></div><div class="widget-body fs14"><a class="item title" href="/2026/02/23/graph-theory-and-search/"><span class="title">å›¾è®ºä¸æœç´¢ï¼šä»é›¶åˆ°ç²¾é€šçš„è´¹æ›¼å¼å®Œå…¨æŒ‡å—</span></a><a class="item title" href="/2026/02/22/linkedlist-pointer-complete-guide/"><span class="title">é“¾è¡¨ä¸æŒ‡é’ˆå®Œå…¨å›¾è§£ï¼šä»å†…å­˜æœ¬è´¨åˆ° LeetCode å®æˆ˜</span></a><a class="item title" href="/2026/02/20/20250220-dynamic-programming-complete-guide/"><span class="title">åŠ¨æ€è§„åˆ’å®Œå…¨æŒ‡å—ï¼šä»å…¥é—¨åˆ°ç²¾é€š</span></a><a class="item title" href="/2019/08/28/20190828-shutil%E6%A8%A1%E5%9D%97/"><span class="title">shutilæ¨¡å—</span></a><a class="item title" href="/2019/09/06/20190906-%E5%8F%98%E6%80%81%E8%B7%B3%E5%8F%B0%E9%98%B6/"><span class="title">å˜æ€è·³å°é˜¶</span></a><a class="item title" href="/2019/10/13/20191013-park%E5%81%9C%E8%BD%A6%E5%9C%BA%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/"><span class="title">parkåœè½¦åœºé¡¹ç›®å®æˆ˜</span></a><a class="item title" href="/2026/02/03/bert-comprehensive-guide/"><span class="title">BERT å®Œæ•´è§£æï¼šä»è®ºæ–‡åˆ° KV Cache</span></a><a class="item title" href="/2019/08/28/20190828-opencv%E6%A8%A1%E5%9D%97/"><span class="title">opencvæ¨¡å—</span></a><a class="item title" href="/2019/09/02/20190902-%E7%94%A8%E4%B8%A4%E4%B8%AA%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/"><span class="title">ç”¨ä¸¤ä¸ªæ ˆå®ç°é˜Ÿåˆ—</span></a><a class="item title" href="/2019/09/12/20190912-%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/"><span class="title">åè½¬é“¾è¡¨</span></a></div></widget>
</div>
<footer class="footer dis-select"><div class="social-wrap"><a class="social" href="https://github.com/caiusy" target="_blank" rel="external nofollow noopener noreferrer"><img src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/08a41b181ce68.svg"/></a></div></footer>
</div></aside><div class="l_main" id="main">





<div class="article banner slide-up top">
  <div class="content">
    <div class="top bread-nav footnote"><div class="left"><div class="flex-row" id="breadcrumb"><a class="cap breadcrumb" href="/">Home</a>
<span class="sep"></span><a class="cap breadcrumb" href="/">Blog</a><span class="sep"></span><a class="cap breadcrumb-link" href="/categories/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">æ·±åº¦å­¦ä¹ </a></div>
<div class="flex-row" id="post-meta"><span class="text created">Posted on: <time datetime="2026-02-03T07:30:00.000Z">2026-02-03</time></span><span class="sep updated"></span><span class="text updated">Updated on: <time datetime="2026-02-20T07:15:58.509Z">2026-02-20</time></span></div></div></div>
    
    <div class="bottom only-title">
      
      <div class="text-area">
        <h1 class="text title"><span>BERT å®Œæ•´è§£æï¼šä»è®ºæ–‡åˆ° KV Cache</span></h1>
        
      </div>
    </div>
    
  </div>
  </div><article class="md-text content slide-up"><h1 id="BERT-å®Œæ•´è§£æï¼šä»è®ºæ–‡åˆ°-KV-Cache"><a href="#BERT-å®Œæ•´è§£æï¼šä»è®ºæ–‡åˆ°-KV-Cache" class="headerlink" title="BERT å®Œæ•´è§£æï¼šä»è®ºæ–‡åˆ° KV Cache"></a>BERT å®Œæ•´è§£æï¼šä»è®ºæ–‡åˆ° KV Cache</h1><blockquote>
<p><strong>å­¦ä¹ ç›®æ ‡</strong>ï¼šæ·±åº¦ç†è§£ BERT è®ºæ–‡æ ¸å¿ƒåŸç†ã€Q/K/V äº¤äº’æœºåˆ¶ã€KV Cache ä¼˜åŒ–æŠ€æœ¯ã€Causal Attentionï¼Œä»¥åŠå®Œæ•´çš„è®­ç»ƒé—­ç¯ã€‚</p>
</blockquote>
<hr>
<h2 id="ğŸ–¼ï¸-æ ¸å¿ƒå¯è§†åŒ–å›¾è§£"><a href="#ğŸ–¼ï¸-æ ¸å¿ƒå¯è§†åŒ–å›¾è§£" class="headerlink" title="ğŸ–¼ï¸ æ ¸å¿ƒå¯è§†åŒ–å›¾è§£"></a>ğŸ–¼ï¸ æ ¸å¿ƒå¯è§†åŒ–å›¾è§£</h2><h3 id="BERT-æ•´ä½“æ¶æ„ä¸æ ¸å¿ƒæ¦‚å¿µ"><a href="#BERT-æ•´ä½“æ¶æ„ä¸æ ¸å¿ƒæ¦‚å¿µ" class="headerlink" title="BERT æ•´ä½“æ¶æ„ä¸æ ¸å¿ƒæ¦‚å¿µ"></a>BERT æ•´ä½“æ¶æ„ä¸æ ¸å¿ƒæ¦‚å¿µ</h3><p><img src="/images/bert_guide/bert_complete_visualizations.png">)</p>
<h3 id="BERT-æ¶æ„è¯¦è§£"><a href="#BERT-æ¶æ„è¯¦è§£" class="headerlink" title="BERT æ¶æ„è¯¦è§£"></a>BERT æ¶æ„è¯¦è§£</h3><p><img src="/images/bert_guide/bert_architecture.png">)</p>
<h3 id="Q-x2F-K-x2F-V-æ•°æ®æµå¯è§†åŒ–"><a href="#Q-x2F-K-x2F-V-æ•°æ®æµå¯è§†åŒ–" class="headerlink" title="Q/K/V æ•°æ®æµå¯è§†åŒ–"></a>Q/K/V æ•°æ®æµå¯è§†åŒ–</h3><p><img src="/images/bert_guide/qkv_dataflow.png">)</p>
<h3 id="æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£"><a href="#æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£" class="headerlink" title="æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£"></a>æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£</h3><p><img src="/images/bert_guide/attention_mechanisms.png">)</p>
<h3 id="Causal-vs-Bidirectional-Attention-å¯¹æ¯”"><a href="#Causal-vs-Bidirectional-Attention-å¯¹æ¯”" class="headerlink" title="Causal vs Bidirectional Attention å¯¹æ¯”"></a>Causal vs Bidirectional Attention å¯¹æ¯”</h3><p><img src="/images/bert_guide/causal_vs_bidirectional.png">)</p>
<hr>
<h2 id="ğŸ†š-BERT-vs-â€œAttention-Is-All-You-Needâ€-å¯¹æ¯”åˆ†æ"><a href="#ğŸ†š-BERT-vs-â€œAttention-Is-All-You-Needâ€-å¯¹æ¯”åˆ†æ" class="headerlink" title="ğŸ†š BERT vs â€œAttention Is All You Needâ€ å¯¹æ¯”åˆ†æ"></a>ğŸ†š BERT vs â€œAttention Is All You Needâ€ å¯¹æ¯”åˆ†æ</h2><h3 id="è®ºæ–‡åŸºæœ¬ä¿¡æ¯å¯¹æ¯”"><a href="#è®ºæ–‡åŸºæœ¬ä¿¡æ¯å¯¹æ¯”" class="headerlink" title="è®ºæ–‡åŸºæœ¬ä¿¡æ¯å¯¹æ¯”"></a>è®ºæ–‡åŸºæœ¬ä¿¡æ¯å¯¹æ¯”</h3><table>
<thead>
<tr>
<th>ç»´åº¦</th>
<th>Attention Is All You Need</th>
<th>BERT</th>
</tr>
</thead>
<tbody><tr>
<td><strong>å‘è¡¨æ—¶é—´</strong></td>
<td>2017å¹´6æœˆ</td>
<td>2018å¹´10æœˆ</td>
</tr>
<tr>
<td><strong>ä½œè€…å›¢é˜Ÿ</strong></td>
<td>Google Brain + Google Research</td>
<td>Google AI Language</td>
</tr>
<tr>
<td><strong>æ ¸å¿ƒè´¡çŒ®</strong></td>
<td>æå‡º Transformer æ¶æ„</td>
<td>æå‡ºé¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼</td>
</tr>
<tr>
<td><strong>å¼•ç”¨é‡</strong></td>
<td>10ä¸‡+</td>
<td>9ä¸‡+</td>
</tr>
<tr>
<td><strong>åœ°ä½</strong></td>
<td>å¥ åŸºä¹‹ä½œ</td>
<td>åº”ç”¨çªç ´</td>
</tr>
</tbody></table>
<h3 id="æ¶æ„å…³ç³»"><a href="#æ¶æ„å…³ç³»" class="headerlink" title="æ¶æ„å…³ç³»"></a>æ¶æ„å…³ç³»</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Attention Is All You Need (2017)</span><br><span class="line">        â”‚</span><br><span class="line">        â–¼ æä¾›äº†æ ¸å¿ƒæ¶æ„</span><br><span class="line">   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">   â”‚     Transformer æ¶æ„           â”‚</span><br><span class="line">   â”‚  â€¢ Multi-Head Attention        â”‚</span><br><span class="line">   â”‚  â€¢ Position Encoding           â”‚</span><br><span class="line">   â”‚  â€¢ Layer Normalization         â”‚</span><br><span class="line">   â”‚  â€¢ Feed-Forward Network        â”‚</span><br><span class="line">   â”‚  â€¢ Encoder + Decoder           â”‚</span><br><span class="line">   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br><span class="line">        â”‚</span><br><span class="line">        â–¼ BERT åªç”¨ Encoder éƒ¨åˆ†</span><br><span class="line">   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">   â”‚          BERT (2018)           â”‚</span><br><span class="line">   â”‚  â€¢ åªç”¨ Transformer Encoder    â”‚</span><br><span class="line">   â”‚  â€¢ åŠ å…¥ MLM é¢„è®­ç»ƒä»»åŠ¡         â”‚</span><br><span class="line">   â”‚  â€¢ æå‡ºé¢„è®­ç»ƒ+å¾®è°ƒèŒƒå¼         â”‚</span><br><span class="line">   â”‚  â€¢ åŒå‘æ³¨æ„åŠ›                  â”‚</span><br><span class="line">   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br></pre></td></tr></tbody></table></figure>

<h3 id="æ ¸å¿ƒåŒºåˆ«"><a href="#æ ¸å¿ƒåŒºåˆ«" class="headerlink" title="æ ¸å¿ƒåŒºåˆ«"></a>æ ¸å¿ƒåŒºåˆ«</h3><table>
<thead>
<tr>
<th>ç»´åº¦</th>
<th>Transformer åŸè®ºæ–‡</th>
<th>BERT</th>
</tr>
</thead>
<tbody><tr>
<td><strong>æ¶æ„</strong></td>
<td>Encoder + Decoder</td>
<td>ä»… Encoder</td>
</tr>
<tr>
<td><strong>ä»»åŠ¡</strong></td>
<td>æœºå™¨ç¿»è¯‘ï¼ˆSeq2Seqï¼‰</td>
<td>è¯­è¨€ç†è§£ï¼ˆåˆ†ç±»ã€NERã€QAï¼‰</td>
</tr>
<tr>
<td><strong>æ³¨æ„åŠ›</strong></td>
<td>EncoderåŒå‘ï¼ŒDecoderå•å‘</td>
<td>å…¨éƒ¨åŒå‘</td>
</tr>
<tr>
<td><strong>é¢„è®­ç»ƒ</strong></td>
<td>æ— ï¼ˆç›‘ç£è®­ç»ƒï¼‰</td>
<td>MLM + NSP</td>
</tr>
<tr>
<td><strong>Position Encoding</strong></td>
<td>å›ºå®šçš„æ­£å¼¦å‡½æ•°</td>
<td>å¯å­¦ä¹ çš„ Embedding</td>
</tr>
<tr>
<td><strong>åº”ç”¨</strong></td>
<td>éœ€è¦æˆå¯¹æ•°æ®ï¼ˆä¸­è‹±ç¿»è¯‘ï¼‰</td>
<td>é€šç”¨ç‰¹å¾æå–å™¨</td>
</tr>
</tbody></table>
<h3 id="BERT-ç»§æ‰¿äº†ä»€ä¹ˆï¼Ÿ"><a href="#BERT-ç»§æ‰¿äº†ä»€ä¹ˆï¼Ÿ" class="headerlink" title="BERT ç»§æ‰¿äº†ä»€ä¹ˆï¼Ÿ"></a>BERT ç»§æ‰¿äº†ä»€ä¹ˆï¼Ÿ</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Multi-Head Self-Attention</span></span><br><span class="line">Q = X @ W_Q</span><br><span class="line">K = X @ W_K</span><br><span class="line">V = X @ W_V</span><br><span class="line">Attention(Q, K, V) = softmax(QK^T / âˆšd_k)V</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Layer Normalization + æ®‹å·®è¿æ¥</span></span><br><span class="line">output = LayerNorm(X + Attention(X))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Feed-Forward Network</span></span><br><span class="line">FFN(x) = <span class="built_in">max</span>(<span class="number">0</span>, xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. æ•´ä½“ç»“æ„</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">12</span>):</span><br><span class="line">    X = LayerNorm(X + MultiHeadAttention(X))</span><br><span class="line">    X = LayerNorm(X + FFN(X))</span><br></pre></td></tr></tbody></table></figure>

<h3 id="BERT-çš„åˆ›æ–°ç‚¹"><a href="#BERT-çš„åˆ›æ–°ç‚¹" class="headerlink" title="BERT çš„åˆ›æ–°ç‚¹"></a>BERT çš„åˆ›æ–°ç‚¹</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">åˆ›æ–° 1: åªç”¨ Encoder</span><br><span class="line">       åŸå› ï¼šNLP å¤§å¤šæ•°ä»»åŠ¡æ˜¯"ç†è§£"ï¼Œä¸æ˜¯"ç”Ÿæˆ"</span><br><span class="line"></span><br><span class="line">åˆ›æ–° 2: MLM é¢„è®­ç»ƒä»»åŠ¡</span><br><span class="line">       è®©æ¨¡å‹å­¦ä¼šåŒå‘ä¸Šä¸‹æ–‡ï¼ˆTransformer åŸè®ºæ–‡æ²¡æœ‰é¢„è®­ç»ƒï¼‰</span><br><span class="line"></span><br><span class="line">åˆ›æ–° 3: é¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼</span><br><span class="line">       ä¸€æ¬¡é¢„è®­ç»ƒ â†’ å¤šä¸ªä»»åŠ¡å¤ç”¨</span><br><span class="line"></span><br><span class="line">åˆ›æ–° 4: å¯å­¦ä¹ çš„ä½ç½®ç¼–ç </span><br><span class="line">       Position Embedding æ˜¯è®­ç»ƒå‡ºæ¥çš„ï¼Œä¸æ˜¯å›ºå®šå…¬å¼</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="ğŸ†š-BERT-vs-Attention-Is-All-You-Needï¼šè®ºæ–‡å¯¹æ¯”"><a href="#ğŸ†š-BERT-vs-Attention-Is-All-You-Needï¼šè®ºæ–‡å¯¹æ¯”" class="headerlink" title="ğŸ†š BERT vs Attention Is All You Needï¼šè®ºæ–‡å¯¹æ¯”"></a>ğŸ†š BERT vs Attention Is All You Needï¼šè®ºæ–‡å¯¹æ¯”</h2><h3 id="è®ºæ–‡åŸºæœ¬ä¿¡æ¯å¯¹æ¯”-1"><a href="#è®ºæ–‡åŸºæœ¬ä¿¡æ¯å¯¹æ¯”-1" class="headerlink" title="è®ºæ–‡åŸºæœ¬ä¿¡æ¯å¯¹æ¯”"></a>è®ºæ–‡åŸºæœ¬ä¿¡æ¯å¯¹æ¯”</h3><table>
<thead>
<tr>
<th>ç»´åº¦</th>
<th>Attention Is All You Need</th>
<th>BERT</th>
</tr>
</thead>
<tbody><tr>
<td><strong>å‘è¡¨æ—¶é—´</strong></td>
<td>2017å¹´6æœˆ</td>
<td>2018å¹´10æœˆ</td>
</tr>
<tr>
<td><strong>ä½œè€…å›¢é˜Ÿ</strong></td>
<td>Google Brain + Google Research</td>
<td>Google AI Language</td>
</tr>
<tr>
<td><strong>æ ¸å¿ƒè´¡çŒ®</strong></td>
<td>æå‡º Transformer æ¶æ„</td>
<td>æå‡ºé¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼</td>
</tr>
<tr>
<td><strong>å¼•ç”¨é‡</strong></td>
<td>100,000+</td>
<td>90,000+</td>
</tr>
<tr>
<td><strong>å†å²åœ°ä½</strong></td>
<td>æ¶æ„å¥ åŸºä¹‹ä½œ</td>
<td>åº”ç”¨çªç ´ä¹‹ä½œ</td>
</tr>
</tbody></table>
<h3 id="æ ¸å¿ƒå…³ç³»ï¼šç»§æ‰¿ä¸åˆ›æ–°"><a href="#æ ¸å¿ƒå…³ç³»ï¼šç»§æ‰¿ä¸åˆ›æ–°" class="headerlink" title="æ ¸å¿ƒå…³ç³»ï¼šç»§æ‰¿ä¸åˆ›æ–°"></a>æ ¸å¿ƒå…³ç³»ï¼šç»§æ‰¿ä¸åˆ›æ–°</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Attention Is All You Need (2017)</span><br><span class="line">        â”‚</span><br><span class="line">        â–¼ æä¾›äº†æ ¸å¿ƒæ¶æ„</span><br><span class="line">   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">   â”‚     Transformer æ¶æ„           â”‚</span><br><span class="line">   â”‚  â€¢ Encoder-Decoder ç»“æ„        â”‚</span><br><span class="line">   â”‚  â€¢ Multi-Head Attention        â”‚</span><br><span class="line">   â”‚  â€¢ Position Encoding           â”‚</span><br><span class="line">   â”‚  â€¢ Layer Normalization         â”‚</span><br><span class="line">   â”‚  â€¢ Feed-Forward Network        â”‚</span><br><span class="line">   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br><span class="line">        â”‚</span><br><span class="line">        â–¼ BERT åªç”¨ Encoder éƒ¨åˆ†</span><br><span class="line">   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">   â”‚          BERT (2018)           â”‚</span><br><span class="line">   â”‚  â€¢ åªç”¨ Transformer Encoder    â”‚</span><br><span class="line">   â”‚  â€¢ æå‡º MLM é¢„è®­ç»ƒä»»åŠ¡         â”‚</span><br><span class="line">   â”‚  â€¢ æå‡ºé¢„è®­ç»ƒ+å¾®è°ƒèŒƒå¼         â”‚</span><br><span class="line">   â”‚  â€¢ åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡              â”‚</span><br><span class="line">   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br></pre></td></tr></tbody></table></figure>

<h3 id="æ¶æ„å¯¹æ¯”"><a href="#æ¶æ„å¯¹æ¯”" class="headerlink" title="æ¶æ„å¯¹æ¯”"></a>æ¶æ„å¯¹æ¯”</h3><table>
<thead>
<tr>
<th>ç»´åº¦</th>
<th>Transformer</th>
<th>BERT</th>
</tr>
</thead>
<tbody><tr>
<td><strong>æ¶æ„</strong></td>
<td>Encoder + Decoder</td>
<td>ä»… Encoder</td>
</tr>
<tr>
<td><strong>é€‚ç”¨ä»»åŠ¡</strong></td>
<td>æœºå™¨ç¿»è¯‘ï¼ˆSeq2Seqï¼‰</td>
<td>åˆ†ç±»ã€NERã€é—®ç­”</td>
</tr>
<tr>
<td><strong>æ³¨æ„åŠ›ç±»å‹</strong></td>
<td>Encoderç”¨åŒå‘ï¼ŒDecoderç”¨å•å‘</td>
<td>å…¨éƒ¨åŒå‘</td>
</tr>
<tr>
<td><strong>é¢„è®­ç»ƒä»»åŠ¡</strong></td>
<td>æ— ï¼ˆéœ€è¦å¹³è¡Œè¯­æ–™ï¼‰</td>
<td>MLM + NSP</td>
</tr>
<tr>
<td><strong>Position Embedding</strong></td>
<td>å›ºå®šçš„æ­£å¼¦å‡½æ•°</td>
<td>å¯å­¦ä¹ çš„å‘é‡</td>
</tr>
</tbody></table>
<h3 id="BERT-ç»§æ‰¿äº†ä»€ä¹ˆï¼Ÿ-1"><a href="#BERT-ç»§æ‰¿äº†ä»€ä¹ˆï¼Ÿ-1" class="headerlink" title="BERT ç»§æ‰¿äº†ä»€ä¹ˆï¼Ÿ"></a>BERT ç»§æ‰¿äº†ä»€ä¹ˆï¼Ÿ</h3><p><strong>âœ… å®Œå…¨ç»§æ‰¿</strong>ï¼š</p>
<ul>
<li>Multi-Head Self-Attention æœºåˆ¶</li>
<li>Feed-Forward Network (FFN)</li>
<li>æ®‹å·®è¿æ¥ + Layer Normalization</li>
<li>Q/K/V è®¡ç®—æ–¹å¼</li>
</ul>
<p><strong>ğŸ”§ æ”¹è¿›éƒ¨åˆ†</strong>ï¼š</p>
<ul>
<li>Position Embeddingï¼šä»å›ºå®šæ”¹ä¸ºå¯å­¦ä¹ </li>
<li>åªç”¨ Encoderï¼Œå»æ‰ Decoder</li>
<li>æ·»åŠ  Segment Embedding</li>
<li>æ·»åŠ ç‰¹æ®Š Tokenï¼š<code>[CLS]</code>ã€<code>[SEP]</code>ã€<code>[MASK]</code></li>
</ul>
<hr>
<h2 id="ğŸ†š-BERT-vs-â€œAttention-Is-All-You-Needâ€-å¯¹æ¯”"><a href="#ğŸ†š-BERT-vs-â€œAttention-Is-All-You-Needâ€-å¯¹æ¯”" class="headerlink" title="ğŸ†š BERT vs â€œAttention Is All You Needâ€ å¯¹æ¯”"></a>ğŸ†š BERT vs â€œAttention Is All You Needâ€ å¯¹æ¯”</h2><h3 id="è®ºæ–‡åŸºæœ¬ä¿¡æ¯"><a href="#è®ºæ–‡åŸºæœ¬ä¿¡æ¯" class="headerlink" title="è®ºæ–‡åŸºæœ¬ä¿¡æ¯"></a>è®ºæ–‡åŸºæœ¬ä¿¡æ¯</h3><table>
<thead>
<tr>
<th>ç»´åº¦</th>
<th>Attention Is All You Need</th>
<th>BERT</th>
</tr>
</thead>
<tbody><tr>
<td><strong>å‘è¡¨æ—¶é—´</strong></td>
<td>2017å¹´6æœˆ</td>
<td>2018å¹´10æœˆ</td>
</tr>
<tr>
<td><strong>ä½œè€…å›¢é˜Ÿ</strong></td>
<td>Google Brain + Google Research</td>
<td>Google AI Language</td>
</tr>
<tr>
<td><strong>æ ¸å¿ƒè´¡çŒ®</strong></td>
<td>æå‡º Transformer æ¶æ„</td>
<td>æå‡ºé¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼</td>
</tr>
<tr>
<td><strong>å¼•ç”¨é‡</strong></td>
<td>10ä¸‡+</td>
<td>9ä¸‡+</td>
</tr>
<tr>
<td><strong>åœ°ä½</strong></td>
<td>å¥ åŸºä¹‹ä½œï¼ˆæ¶æ„åˆ›æ–°ï¼‰</td>
<td>åº”ç”¨çªç ´ï¼ˆèŒƒå¼åˆ›æ–°ï¼‰</td>
</tr>
</tbody></table>
<h3 id="æ¶æ„å¯¹æ¯”-1"><a href="#æ¶æ„å¯¹æ¯”-1" class="headerlink" title="æ¶æ„å¯¹æ¯”"></a>æ¶æ„å¯¹æ¯”</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Attention Is All You Need (2017) - åŸå§‹ Transformer</span><br><span class="line">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â”‚   è¾“å…¥ï¼š"æˆ‘çˆ±ä½ "              è¾“å‡ºï¼š"I love you"             â”‚</span><br><span class="line">â”‚        â†“                            â†‘                       â”‚</span><br><span class="line">â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚</span><br><span class="line">â”‚   â”‚ Encoder â”‚ â”€â”€â”€ä¸Šä¸‹æ–‡â”€â”€â”€â†’   â”‚ Decoder â”‚                  â”‚</span><br><span class="line">â”‚   â”‚(6å±‚)    â”‚                 â”‚(6å±‚)    â”‚                  â”‚</span><br><span class="line">â”‚   â”‚åŒå‘æ³¨æ„åŠ›â”‚                 â”‚å•å‘æ³¨æ„åŠ›â”‚                  â”‚</span><br><span class="line">â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â”‚   ç”¨é€”ï¼šæœºå™¨ç¿»è¯‘ï¼ˆseq2seqï¼‰                                  â”‚</span><br><span class="line">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br><span class="line"></span><br><span class="line">BERT (2018) - åªç”¨ Encoder</span><br><span class="line">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â”‚   è¾“å…¥ï¼š"æˆ‘çˆ±[MASK]å¤©å®‰é—¨"                                   â”‚</span><br><span class="line">â”‚        â†“                                                    â”‚</span><br><span class="line">â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚</span><br><span class="line">â”‚   â”‚ Encoder â”‚ â”€â”€â†’ ç›´æ¥è¾“å‡ºæ¯ä¸ªä½ç½®çš„è¡¨ç¤º                    â”‚</span><br><span class="line">â”‚   â”‚(12å±‚)   â”‚      â†“                                       â”‚</span><br><span class="line">â”‚   â”‚åŒå‘æ³¨æ„åŠ›â”‚      åœ¨ [MASK] ä½ç½®é¢„æµ‹ "åŒ—äº¬"                â”‚</span><br><span class="line">â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                                              â”‚</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â”‚   ç”¨é€”ï¼šç†è§£ä»»åŠ¡ï¼ˆåˆ†ç±»ã€NERã€QAï¼‰                            â”‚</span><br><span class="line">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br></pre></td></tr></tbody></table></figure>

<h3 id="BERT-ç»§æ‰¿äº†-Transformer-çš„ä»€ä¹ˆï¼Ÿ"><a href="#BERT-ç»§æ‰¿äº†-Transformer-çš„ä»€ä¹ˆï¼Ÿ" class="headerlink" title="BERT ç»§æ‰¿äº† Transformer çš„ä»€ä¹ˆï¼Ÿ"></a>BERT ç»§æ‰¿äº† Transformer çš„ä»€ä¹ˆï¼Ÿ</h3><table>
<thead>
<tr>
<th>ç»„ä»¶</th>
<th>Transformer</th>
<th>BERT</th>
<th>è¯´æ˜</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Multi-Head Attention</strong></td>
<td>âœ… åŸåˆ›</td>
<td>âœ… å®Œå…¨ç»§æ‰¿</td>
<td>Q/K/V æœºåˆ¶ä¸€æ¨¡ä¸€æ ·</td>
</tr>
<tr>
<td><strong>Position Encoding</strong></td>
<td>âœ… æ­£å¼¦å‡½æ•°</td>
<td>âš ï¸ æ”¹ä¸ºå¯å­¦ä¹ </td>
<td>BERT ç”¨å¯è®­ç»ƒçš„ä½ç½®åµŒå…¥</td>
</tr>
<tr>
<td><strong>Layer Normalization</strong></td>
<td>âœ… åŸåˆ›</td>
<td>âœ… å®Œå…¨ç»§æ‰¿</td>
<td></td>
</tr>
<tr>
<td><strong>Feed-Forward Network</strong></td>
<td>âœ… åŸåˆ›</td>
<td>âœ… å®Œå…¨ç»§æ‰¿</td>
<td></td>
</tr>
<tr>
<td><strong>Encoder ç»“æ„</strong></td>
<td>âœ… 6å±‚</td>
<td>âœ… 12/24å±‚</td>
<td>BERT åŠ æ·±äº†å±‚æ•°</td>
</tr>
<tr>
<td><strong>Decoder ç»“æ„</strong></td>
<td>âœ… 6å±‚</td>
<td>âŒ åˆ é™¤</td>
<td>BERT ä¸éœ€è¦ Decoder</td>
</tr>
</tbody></table>
<h3 id="BERT-çš„åˆ›æ–°ç‚¹-1"><a href="#BERT-çš„åˆ›æ–°ç‚¹-1" class="headerlink" title="BERT çš„åˆ›æ–°ç‚¹"></a>BERT çš„åˆ›æ–°ç‚¹</h3><table>
<thead>
<tr>
<th>åˆ›æ–°</th>
<th>è¯´æ˜</th>
</tr>
</thead>
<tbody><tr>
<td><strong>MLM é¢„è®­ç»ƒä»»åŠ¡</strong></td>
<td>Transformer æ²¡æœ‰é¢„è®­ç»ƒï¼ŒBERT ç”¨ MLM å­¦ä¹ åŒå‘è¡¨ç¤º</td>
</tr>
<tr>
<td><strong>NSP ä»»åŠ¡</strong></td>
<td>å­¦ä¹ å¥å­é—´å…³ç³»ï¼ˆåç»­è¢«è¯æ˜ç”¨å¤„ä¸å¤§ï¼‰</td>
</tr>
<tr>
<td><strong>é¢„è®­ç»ƒ+å¾®è°ƒèŒƒå¼</strong></td>
<td>Transformer æ˜¯ä»»åŠ¡ç‰¹å®šè®­ç»ƒï¼ŒBERT å¼€åˆ›è¿ç§»å­¦ä¹ </td>
</tr>
<tr>
<td><strong>åªç”¨ Encoder</strong></td>
<td>Transformer æ˜¯å®Œæ•´ Encoder-Decoderï¼ŒBERT ç®€åŒ–æ¶æ„</td>
</tr>
</tbody></table>
<h3 id="è”ç³»ï¼šBERT-ç«™åœ¨-Transformer-è‚©è†€ä¸Š"><a href="#è”ç³»ï¼šBERT-ç«™åœ¨-Transformer-è‚©è†€ä¸Š" class="headerlink" title="è”ç³»ï¼šBERT ç«™åœ¨ Transformer è‚©è†€ä¸Š"></a>è”ç³»ï¼šBERT ç«™åœ¨ Transformer è‚©è†€ä¸Š</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">2017 Transformer æä¾›æ ¸å¿ƒæ¶æ„</span><br><span class="line">        â”‚</span><br><span class="line">        â–¼</span><br><span class="line">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">â”‚  â€¢ Multi-Head Attention       â”‚</span><br><span class="line">â”‚  â€¢ Position Encoding          â”‚</span><br><span class="line">â”‚  â€¢ Feed-Forward Network       â”‚</span><br><span class="line">â”‚  â€¢ Encoder-Decoder æ¶æ„       â”‚</span><br><span class="line">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br><span class="line">        â”‚</span><br><span class="line">        â–¼ BERT é€‰æ‹©æ€§ä½¿ç”¨</span><br><span class="line">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">â”‚  âœ… å¤ç”¨ Encoder éƒ¨åˆ†          â”‚</span><br><span class="line">â”‚  âœ… å¤ç”¨ Attention æœºåˆ¶        â”‚</span><br><span class="line">â”‚  âŒ åˆ é™¤ Decoder              â”‚</span><br><span class="line">â”‚  â• åŠ å…¥ MLM é¢„è®­ç»ƒ            â”‚</span><br><span class="line">â”‚  â• æå‡ºé¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼        â”‚</span><br><span class="line">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br></pre></td></tr></tbody></table></figure>

<p><strong>ç®€å•è®°å¿†</strong>ï¼š</p>
<ul>
<li><strong>Transformer</strong> = æä¾›äº†â€å·¥å…·ç®±â€ï¼ˆæ¶æ„ç»„ä»¶ï¼‰</li>
<li><strong>BERT</strong> = ç”¨å·¥å…·ç®±ä¸­çš„éƒ¨åˆ†å·¥å…·ï¼Œå‘æ˜äº†æ–°çš„ä½¿ç”¨æ–¹æ³•ï¼ˆé¢„è®­ç»ƒèŒƒå¼ï¼‰</li>
</ul>
<hr>
<h2 id="ğŸ†š-BERT-vs-â€œAttention-Is-All-You-Needâ€-å¯¹æ¯”-1"><a href="#ğŸ†š-BERT-vs-â€œAttention-Is-All-You-Needâ€-å¯¹æ¯”-1" class="headerlink" title="ğŸ†š BERT vs â€œAttention Is All You Needâ€ å¯¹æ¯”"></a>ğŸ†š BERT vs â€œAttention Is All You Needâ€ å¯¹æ¯”</h2><h3 id="è®ºæ–‡åŸºæœ¬ä¿¡æ¯å¯¹æ¯”-2"><a href="#è®ºæ–‡åŸºæœ¬ä¿¡æ¯å¯¹æ¯”-2" class="headerlink" title="è®ºæ–‡åŸºæœ¬ä¿¡æ¯å¯¹æ¯”"></a>è®ºæ–‡åŸºæœ¬ä¿¡æ¯å¯¹æ¯”</h3><table>
<thead>
<tr>
<th>ç»´åº¦</th>
<th>Attention Is All You Need</th>
<th>BERT</th>
</tr>
</thead>
<tbody><tr>
<td><strong>å‘è¡¨æ—¶é—´</strong></td>
<td>2017å¹´6æœˆ</td>
<td>2018å¹´10æœˆ</td>
</tr>
<tr>
<td><strong>ä½œè€…å›¢é˜Ÿ</strong></td>
<td>Google Brain + Google Research</td>
<td>Google AI Language</td>
</tr>
<tr>
<td><strong>æ ¸å¿ƒè´¡çŒ®</strong></td>
<td>æå‡º Transformer æ¶æ„</td>
<td>æå‡ºé¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼</td>
</tr>
<tr>
<td><strong>æ¶æ„</strong></td>
<td>Encoder + Decoder</td>
<td>ä»… Encoder</td>
</tr>
<tr>
<td><strong>è®­ç»ƒä»»åŠ¡</strong></td>
<td>æœºå™¨ç¿»è¯‘ï¼ˆæœ‰ç›‘ç£ï¼‰</td>
<td>MLM + NSPï¼ˆè‡ªç›‘ç£ï¼‰</td>
</tr>
<tr>
<td><strong>å¼•ç”¨é‡</strong></td>
<td>10ä¸‡+</td>
<td>9ä¸‡+</td>
</tr>
</tbody></table>
<h3 id="ç»§æ‰¿å…³ç³»"><a href="#ç»§æ‰¿å…³ç³»" class="headerlink" title="ç»§æ‰¿å…³ç³»"></a>ç»§æ‰¿å…³ç³»</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Attention Is All You Need (2017)</span><br><span class="line">        â†“ æä¾›æ ¸å¿ƒæ¶æ„</span><br><span class="line">   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">   â”‚     Transformer æ¶æ„           â”‚</span><br><span class="line">   â”‚  â€¢ Multi-Head Attention        â”‚</span><br><span class="line">   â”‚  â€¢ Position Encoding           â”‚</span><br><span class="line">   â”‚  â€¢ Layer Normalization         â”‚</span><br><span class="line">   â”‚  â€¢ Feed-Forward Network        â”‚</span><br><span class="line">   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br><span class="line">        â†“ BERT åªç”¨ Encoder éƒ¨åˆ†</span><br><span class="line">   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">   â”‚          BERT (2018)           â”‚</span><br><span class="line">   â”‚  â€¢ 12å±‚ Transformer Encoder    â”‚</span><br><span class="line">   â”‚  â€¢ MLM é¢„è®­ç»ƒä»»åŠ¡              â”‚</span><br><span class="line">   â”‚  â€¢ é¢„è®­ç»ƒ+å¾®è°ƒèŒƒå¼             â”‚</span><br><span class="line">   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br></pre></td></tr></tbody></table></figure>

<h3 id="æ¶æ„å¯¹æ¯”-2"><a href="#æ¶æ„å¯¹æ¯”-2" class="headerlink" title="æ¶æ„å¯¹æ¯”"></a>æ¶æ„å¯¹æ¯”</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">åŸå§‹ Transformerï¼ˆç¿»è¯‘ä»»åŠ¡ï¼‰ï¼š</span><br><span class="line">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">â”‚   è¾“å…¥ï¼š"æˆ‘çˆ±ä½ "              è¾“å‡ºï¼š"I love you"             â”‚</span><br><span class="line">â”‚        â†“                            â†‘                       â”‚</span><br><span class="line">â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                  â”‚</span><br><span class="line">â”‚   â”‚ Encoder â”‚ â”€â”€â”€ä¸Šä¸‹æ–‡â”€â”€â”€â†’  â”‚ Decoder â”‚                  â”‚</span><br><span class="line">â”‚   â”‚(ç†è§£è¾“å…¥)â”‚                â”‚(ç”Ÿæˆè¾“å‡º)â”‚                  â”‚</span><br><span class="line">â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                  â”‚</span><br><span class="line">â”‚   åŒå‘æ³¨æ„åŠ›                   å•å‘æ³¨æ„åŠ›(Causal)            â”‚</span><br><span class="line">â”‚   æ— éœ€ KV Cache                éœ€è¦ KV Cache                â”‚</span><br><span class="line">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br><span class="line"></span><br><span class="line">BERTï¼ˆç†è§£ä»»åŠ¡ï¼‰ï¼š</span><br><span class="line">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">â”‚   è¾“å…¥ï¼š"æˆ‘çˆ±[MASK]å¤©å®‰é—¨"                                   â”‚</span><br><span class="line">â”‚        â†“                                                    â”‚</span><br><span class="line">â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                                              â”‚</span><br><span class="line">â”‚   â”‚ Encoder â”‚ â”€â”€â†’ ç›´æ¥è¾“å‡ºæ¯ä¸ªä½ç½®çš„è¡¨ç¤º                    â”‚</span><br><span class="line">â”‚   â”‚(ç†è§£è¾“å…¥)â”‚      â†“                                       â”‚</span><br><span class="line">â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      åœ¨ [MASK] ä½ç½®é¢„æµ‹ "åŒ—äº¬"                â”‚</span><br><span class="line">â”‚   åŒå‘æ³¨æ„åŠ›                                                â”‚</span><br><span class="line">â”‚   æ— éœ€ KV Cache                                             â”‚</span><br><span class="line">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br></pre></td></tr></tbody></table></figure>

<h3 id="æ ¸å¿ƒåŒºåˆ«æ€»ç»“"><a href="#æ ¸å¿ƒåŒºåˆ«æ€»ç»“" class="headerlink" title="æ ¸å¿ƒåŒºåˆ«æ€»ç»“"></a>æ ¸å¿ƒåŒºåˆ«æ€»ç»“</h3><table>
<thead>
<tr>
<th>ç»´åº¦</th>
<th>Transformer (åŸè®ºæ–‡)</th>
<th>BERT</th>
</tr>
</thead>
<tbody><tr>
<td><strong>æ¶æ„é€‰æ‹©</strong></td>
<td>Encoder + Decoder</td>
<td>ä»… Encoder</td>
</tr>
<tr>
<td><strong>æ³¨æ„åŠ›æ¨¡å¼</strong></td>
<td>EncoderåŒå‘ + Decoderå•å‘</td>
<td>å…¨éƒ¨åŒå‘</td>
</tr>
<tr>
<td><strong>é€‚ç”¨ä»»åŠ¡</strong></td>
<td>åºåˆ—åˆ°åºåˆ—ï¼ˆç¿»è¯‘ï¼‰</td>
<td>ç†è§£ç±»ä»»åŠ¡</td>
</tr>
<tr>
<td><strong>è®­ç»ƒæ•°æ®</strong></td>
<td>å¹³è¡Œè¯­æ–™ï¼ˆéœ€æ ‡æ³¨ï¼‰</td>
<td>å¤§è§„æ¨¡æ–‡æœ¬ï¼ˆæ— éœ€æ ‡æ³¨ï¼‰</td>
</tr>
<tr>
<td><strong>KV Cache</strong></td>
<td>Decoderéœ€è¦</td>
<td>ä¸éœ€è¦</td>
</tr>
<tr>
<td><strong>å½±å“</strong></td>
<td>å¥ å®šæ¶æ„åŸºç¡€</td>
<td>å¼€åˆ›é¢„è®­ç»ƒèŒƒå¼</td>
</tr>
</tbody></table>
<hr>
<h2 id="ğŸ“„-Part-1-BERT-è®ºæ–‡æ·±åº¦è§£æ"><a href="#ğŸ“„-Part-1-BERT-è®ºæ–‡æ·±åº¦è§£æ" class="headerlink" title="ğŸ“„ Part 1: BERT è®ºæ–‡æ·±åº¦è§£æ"></a>ğŸ“„ Part 1: BERT è®ºæ–‡æ·±åº¦è§£æ</h2><h3 id="1-1-è®ºæ–‡åŸºæœ¬ä¿¡æ¯"><a href="#1-1-è®ºæ–‡åŸºæœ¬ä¿¡æ¯" class="headerlink" title="1.1 è®ºæ–‡åŸºæœ¬ä¿¡æ¯"></a>1.1 è®ºæ–‡åŸºæœ¬ä¿¡æ¯</h3><p><strong>æ ‡é¢˜</strong>: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<br><strong>ä½œè€…</strong>: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova (Google AI Language)<br><strong>å‘è¡¨</strong>: NAACL 2019<br><strong>è®ºæ–‡é“¾æ¥</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></p>
<hr>
<h3 id="1-2-ç ”ç©¶åŠ¨æœºï¼šä¸ºä»€ä¹ˆéœ€è¦-BERTï¼Ÿ"><a href="#1-2-ç ”ç©¶åŠ¨æœºï¼šä¸ºä»€ä¹ˆéœ€è¦-BERTï¼Ÿ" class="headerlink" title="1.2 ç ”ç©¶åŠ¨æœºï¼šä¸ºä»€ä¹ˆéœ€è¦ BERTï¼Ÿ"></a>1.2 ç ”ç©¶åŠ¨æœºï¼šä¸ºä»€ä¹ˆéœ€è¦ BERTï¼Ÿ</h3><p>åœ¨ BERT ä¹‹å‰ï¼ŒNLP é¢„è®­ç»ƒæ¨¡å‹å­˜åœ¨ä¸¤å¤§å±€é™ï¼š</p>
<h4 id="é—®é¢˜-1ï¼šå•å‘è¯­è¨€æ¨¡å‹çš„å±€é™æ€§"><a href="#é—®é¢˜-1ï¼šå•å‘è¯­è¨€æ¨¡å‹çš„å±€é™æ€§" class="headerlink" title="é—®é¢˜ 1ï¼šå•å‘è¯­è¨€æ¨¡å‹çš„å±€é™æ€§"></a>é—®é¢˜ 1ï¼šå•å‘è¯­è¨€æ¨¡å‹çš„å±€é™æ€§</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">GPT-1 (2018):  åªèƒ½ä»å·¦å¾€å³çœ‹</span><br><span class="line">è¾“å…¥: "æˆ‘ çˆ± åŒ—äº¬ å¤©å®‰é—¨"</span><br><span class="line">     â†“   â†“   â†“    â†“</span><br><span class="line">æ¯ä¸ªè¯åªèƒ½çœ‹åˆ°å·¦è¾¹çš„ä¸Šä¸‹æ–‡</span><br><span class="line"></span><br><span class="line">é—®é¢˜: "é“¶è¡Œ" åœ¨ "æˆ‘å»é“¶è¡Œå­˜é’±" vs "æ²³è¾¹çš„é“¶è¡Œå¾ˆé™¡" ä¸­</span><br><span class="line">     å¦‚æœåªçœ‹å·¦è¾¹ï¼Œæ— æ³•åŒºåˆ†æ˜¯ "é‡‘èæœºæ„" è¿˜æ˜¯ "æ²³å²¸"</span><br></pre></td></tr></tbody></table></figure>

<h4 id="é—®é¢˜-2ï¼šæµ…å±‚åŒå‘çš„å±€é™æ€§"><a href="#é—®é¢˜-2ï¼šæµ…å±‚åŒå‘çš„å±€é™æ€§" class="headerlink" title="é—®é¢˜ 2ï¼šæµ…å±‚åŒå‘çš„å±€é™æ€§"></a>é—®é¢˜ 2ï¼šæµ…å±‚åŒå‘çš„å±€é™æ€§</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ELMo (2018): ä½¿ç”¨ä¸¤ä¸ªç‹¬ç«‹çš„ LSTM</span><br><span class="line">    â†’ LSTM (ä»å·¦å¾€å³)</span><br><span class="line">    â†’ LSTM (ä»å³å¾€å·¦)</span><br><span class="line">    â†’ æœ€åæ‹¼æ¥</span><br><span class="line"></span><br><span class="line">é—®é¢˜: ä¸¤ä¸ªæ–¹å‘çš„ä¿¡æ¯åªåœ¨æœ€é¡¶å±‚èåˆï¼Œä¸­é—´å±‚æ— æ³•æ·±åº¦äº¤äº’</span><br></pre></td></tr></tbody></table></figure>

<p><strong>BERT çš„æ ¸å¿ƒåˆ›æ–°</strong>: é€šè¿‡ Masked Language Model (MLM)ï¼Œåœ¨<strong>æ¯ä¸€å±‚</strong>éƒ½å®ç°çœŸæ­£çš„åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚</p>
<hr>
<h3 id="1-3-æ ¸å¿ƒæ–¹æ³•ï¼šä¸¤ä¸ªé¢„è®­ç»ƒä»»åŠ¡"><a href="#1-3-æ ¸å¿ƒæ–¹æ³•ï¼šä¸¤ä¸ªé¢„è®­ç»ƒä»»åŠ¡" class="headerlink" title="1.3 æ ¸å¿ƒæ–¹æ³•ï¼šä¸¤ä¸ªé¢„è®­ç»ƒä»»åŠ¡"></a>1.3 æ ¸å¿ƒæ–¹æ³•ï¼šä¸¤ä¸ªé¢„è®­ç»ƒä»»åŠ¡</h3><h4 id="ä»»åŠ¡-1-Masked-Language-Model-MLM-æ ¸å¿ƒ"><a href="#ä»»åŠ¡-1-Masked-Language-Model-MLM-æ ¸å¿ƒ" class="headerlink" title="ä»»åŠ¡ 1: Masked Language Model (MLM) - æ ¸å¿ƒ"></a>ä»»åŠ¡ 1: Masked Language Model (MLM) - æ ¸å¿ƒ</h4><p><strong>æ“ä½œæµç¨‹</strong>:</p>
<ol>
<li>éšæœºé€‰æ‹© 15% çš„ token è¿›è¡Œ mask</li>
<li>å…¶ä¸­ï¼š<ul>
<li>80% æ›¿æ¢ä¸º <code>[MASK]</code></li>
<li>10% æ›¿æ¢ä¸ºéšæœºè¯</li>
<li>10% ä¿æŒä¸å˜</li>
</ul>
</li>
</ol>
<p><strong>ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ</strong></p>
<ul>
<li>80% <code>[MASK]</code>: è®©æ¨¡å‹å­¦ä¹ é¢„æµ‹</li>
<li>10% éšæœºè¯: é¿å…æ¨¡å‹åªä¾èµ– <code>[MASK]</code> æ ‡è®°</li>
<li>10% ä¸å˜: è®©æ¨¡å‹å­¦ä¹ çœŸå®åˆ†å¸ƒ</li>
</ul>
<p><strong>ä¾‹å­</strong>:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">åŸå§‹å¥å­: "æˆ‘ çˆ± åŒ—äº¬ å¤©å®‰é—¨"</span><br><span class="line">å¤„ç†å:   "æˆ‘ çˆ± [MASK] å¤©å®‰é—¨"   (80%)</span><br><span class="line">æˆ–:       "æˆ‘ çˆ± ä¸Šæµ· å¤©å®‰é—¨"     (10% éšæœº)</span><br><span class="line">æˆ–:       "æˆ‘ çˆ± åŒ—äº¬ å¤©å®‰é—¨"     (10% ä¸å˜)</span><br><span class="line"></span><br><span class="line">Label: ä½ç½® 3 = "åŒ—äº¬"</span><br><span class="line">Loss = CrossEntropy(model_output[3], ID("åŒ—äº¬"))</span><br></pre></td></tr></tbody></table></figure>

<p><strong>ç»´åº¦åˆ†æ</strong>:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">è¾“å…¥: [batch, seq_len] = [<span class="number">32</span>, <span class="number">128</span>]</span><br><span class="line">     â†“ Embedding</span><br><span class="line">     [<span class="number">32</span>, <span class="number">128</span>, <span class="number">768</span>]</span><br><span class="line">     â†“ <span class="number">12</span> å±‚ Transformer Encoder</span><br><span class="line">     [<span class="number">32</span>, <span class="number">128</span>, <span class="number">768</span>]</span><br><span class="line">     â†“ MLM Head (Linear + Softmax)</span><br><span class="line">     [<span class="number">32</span>, <span class="number">128</span>, <span class="number">21128</span>]  <span class="comment"># 21128 = è¯è¡¨å¤§å°</span></span><br><span class="line">     </span><br><span class="line">åªè®¡ç®—è¢« mask ä½ç½®çš„ Loss</span><br></pre></td></tr></tbody></table></figure>

<h4 id="ä»»åŠ¡-2-Next-Sentence-Prediction-NSP"><a href="#ä»»åŠ¡-2-Next-Sentence-Prediction-NSP" class="headerlink" title="ä»»åŠ¡ 2: Next Sentence Prediction (NSP)"></a>ä»»åŠ¡ 2: Next Sentence Prediction (NSP)</h4><p><strong>ç›®çš„</strong>: å­¦ä¹ å¥å­é—´å…³ç³»</p>
<p><strong>è¾“å…¥æ ¼å¼</strong>:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[CLS] å¥å­A [SEP] å¥å­B [SEP]</span><br><span class="line"></span><br><span class="line">æ­£æ ·æœ¬: B ç¡®å®æ˜¯ A çš„ä¸‹ä¸€å¥ (Label = 1)</span><br><span class="line">è´Ÿæ ·æœ¬: B æ˜¯éšæœºé€‰çš„å¥å­ (Label = 0)</span><br></pre></td></tr></tbody></table></figure>

<p><strong>ä¾‹å­</strong>:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">æ­£æ ·æœ¬:</span><br><span class="line">Input: [CLS] ä»Šå¤©å¤©æ°”å¾ˆå¥½ [SEP] æˆ‘ä»¬å»å…¬å›­å§ [SEP]</span><br><span class="line">Label: IsNext (1)</span><br><span class="line"></span><br><span class="line">è´Ÿæ ·æœ¬:</span><br><span class="line">Input: [CLS] ä»Šå¤©å¤©æ°”å¾ˆå¥½ [SEP] äººå·¥æ™ºèƒ½å¾ˆæœ‰è¶£ [SEP]</span><br><span class="line">Label: NotNext (0)</span><br></pre></td></tr></tbody></table></figure>

<p><strong>Loss è®¡ç®—</strong>:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cls_output = encoder_output[:, <span class="number">0</span>, :]  <span class="comment"># [batch, 768] å– [CLS] ä½ç½®</span></span><br><span class="line">logits = nsp_classifier(cls_output)   <span class="comment"># [batch, 2]</span></span><br><span class="line">loss = BinaryCrossEntropy(logits, labels)</span><br></pre></td></tr></tbody></table></figure>

<p><strong>åç»­ç ”ç©¶å‘ç°</strong>: NSP ä»»åŠ¡æ•ˆæœæœ‰é™ï¼ŒRoBERTa ç­‰åç»­å·¥ä½œç§»é™¤äº†è¿™ä¸ªä»»åŠ¡ã€‚</p>
<hr>
<h3 id="1-3-1-MLM-æ·±åº¦è§£æï¼šå®Œå½¢å¡«ç©ºçš„è‰ºæœ¯"><a href="#1-3-1-MLM-æ·±åº¦è§£æï¼šå®Œå½¢å¡«ç©ºçš„è‰ºæœ¯" class="headerlink" title="1.3.1 MLM æ·±åº¦è§£æï¼šå®Œå½¢å¡«ç©ºçš„è‰ºæœ¯"></a>1.3.1 MLM æ·±åº¦è§£æï¼šå®Œå½¢å¡«ç©ºçš„è‰ºæœ¯</h3><h4 id="é€šä¿—ç†è§£"><a href="#é€šä¿—ç†è§£" class="headerlink" title="é€šä¿—ç†è§£"></a>é€šä¿—ç†è§£</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">å°å­¦è¯­æ–‡é¢˜ï¼š</span><br><span class="line">  "å°æ˜ _____ å­¦æ ¡ä¸Šè¯¾"</span><br><span class="line">  </span><br><span class="line">ç­”æ¡ˆï¼šå»ã€åˆ°ã€åœ¨...</span><br><span class="line"></span><br><span class="line">BERT çš„ MLM å°±æ˜¯è®© AI åšè¿™ç§"å®Œå½¢å¡«ç©º"ï¼</span><br></pre></td></tr></tbody></table></figure>

<h4 id="å…·ä½“æ“ä½œæµç¨‹"><a href="#å…·ä½“æ“ä½œæµç¨‹" class="headerlink" title="å…·ä½“æ“ä½œæµç¨‹"></a>å…·ä½“æ“ä½œæµç¨‹</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># åŸå§‹å¥å­</span></span><br><span class="line">åŸå¥ = <span class="string">"æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: éšæœºé€‰æ‹© 15% çš„è¯è¿›è¡Œå¤„ç†</span></span><br><span class="line">é€‰ä¸­ = <span class="string">"åŒ—äº¬"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: å¯¹é€‰ä¸­çš„è¯è¿›è¡Œä¸‰ç§å¤„ç†ï¼ˆéšæœºé€‰ä¸€ç§ï¼‰</span></span><br><span class="line">å¤„ç†å = <span class="string">"æˆ‘çˆ± [MASK] å¤©å®‰é—¨"</span>   <span class="comment"># 80% æ¦‚ç‡ï¼šæ›¿æ¢ä¸º [MASK]</span></span><br><span class="line">æˆ–è€…   = <span class="string">"æˆ‘çˆ± ä¸Šæµ· å¤©å®‰é—¨"</span>     <span class="comment"># 10% æ¦‚ç‡ï¼šæ›¿æ¢ä¸ºéšæœºè¯</span></span><br><span class="line">æˆ–è€…   = <span class="string">"æˆ‘çˆ± åŒ—äº¬ å¤©å®‰é—¨"</span>     <span class="comment"># 10% æ¦‚ç‡ï¼šä¿æŒä¸å˜</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: è®©æ¨¡å‹é¢„æµ‹è¢«é®ä½çš„è¯æ˜¯ä»€ä¹ˆ</span></span><br><span class="line">æ¨¡å‹è¾“å…¥ = <span class="string">"æˆ‘çˆ± [MASK] å¤©å®‰é—¨"</span></span><br><span class="line">æ¨¡å‹è¾“å‡º = <span class="string">"åŒ—äº¬"</span> âœ…  (å¦‚æœé¢„æµ‹å¯¹äº†ï¼Œloss å¾ˆå°)</span><br><span class="line">         = <span class="string">"ä¸Šæµ·"</span> âŒ  (å¦‚æœé¢„æµ‹é”™äº†ï¼Œloss å¾ˆå¤§)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="ä¸ºä»€ä¹ˆ-MLM-èƒ½è®©-BERT-å­¦ä¼šâ€ç†è§£è¯­è¨€â€ï¼Ÿ"><a href="#ä¸ºä»€ä¹ˆ-MLM-èƒ½è®©-BERT-å­¦ä¼šâ€ç†è§£è¯­è¨€â€ï¼Ÿ" class="headerlink" title="ä¸ºä»€ä¹ˆ MLM èƒ½è®© BERT å­¦ä¼šâ€ç†è§£è¯­è¨€â€ï¼Ÿ"></a>ä¸ºä»€ä¹ˆ MLM èƒ½è®© BERT å­¦ä¼šâ€ç†è§£è¯­è¨€â€ï¼Ÿ</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">åœºæ™¯ 1ï¼š</span><br><span class="line">  è¾“å…¥ï¼š"æˆ‘å» [MASK] å­˜é’±"</span><br><span class="line">  æ¨¡å‹å­¦ä¼šï¼šçœ‹åˆ°"å­˜é’±" â†’ é¢„æµ‹"é“¶è¡Œ"ï¼ˆé‡‘èæœºæ„ï¼‰</span><br><span class="line"></span><br><span class="line">åœºæ™¯ 2ï¼š</span><br><span class="line">  è¾“å…¥ï¼š"æ²³è¾¹çš„ [MASK] å¾ˆé™¡å³­"  </span><br><span class="line">  æ¨¡å‹å­¦ä¼šï¼šçœ‹åˆ°"æ²³è¾¹""é™¡å³­" â†’ é¢„æµ‹"æ²³å²¸/å ¤å"</span><br><span class="line"></span><br><span class="line">é€šè¿‡æ•°åäº¿æ¬¡è¿™æ ·çš„"å®Œå½¢å¡«ç©º"è®­ç»ƒåï¼š</span><br><span class="line">  â†’ æ¨¡å‹å­¦ä¼šäº†è¯è¯­ä¹‹é—´çš„å…³ç³»</span><br><span class="line">  â†’ æ¨¡å‹å­¦ä¼šäº†è¯­æ³•ç»“æ„</span><br><span class="line">  â†’ æ¨¡å‹å­¦ä¼šäº†å¸¸è¯†çŸ¥è¯†</span><br></pre></td></tr></tbody></table></figure>

<h4 id="MLM-è®­ç»ƒä»£ç ç¤ºä¾‹"><a href="#MLM-è®­ç»ƒä»£ç ç¤ºä¾‹" class="headerlink" title="MLM è®­ç»ƒä»£ç ç¤ºä¾‹"></a>MLM è®­ç»ƒä»£ç ç¤ºä¾‹</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForMaskedLM</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># åŠ è½½æ¨¡å‹</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-chinese'</span>)</span><br><span class="line">model = BertForMaskedLM.from_pretrained(<span class="string">'bert-base-chinese'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è¾“å…¥ä¸€ä¸ªå¸¦ [MASK] çš„å¥å­</span></span><br><span class="line">text = <span class="string">"æˆ‘çˆ±[MASK]å¤©å®‰é—¨"</span></span><br><span class="line">inputs = tokenizer(text, return_tensors=<span class="string">'pt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹é¢„æµ‹</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = model(**inputs)</span><br><span class="line">    predictions = outputs.logits</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ‰¾åˆ° [MASK] ä½ç½®çš„é¢„æµ‹ç»“æœ</span></span><br><span class="line">mask_index = (inputs[<span class="string">'input_ids'</span>] == tokenizer.mask_token_id).nonzero()[<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">predicted_token_id = predictions[<span class="number">0</span>, mask_index].argmax(dim=-<span class="number">1</span>)</span><br><span class="line">predicted_token = tokenizer.decode(predicted_token_id)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"åŸå¥: <span class="subst">{text}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"é¢„æµ‹: <span class="subst">{predicted_token}</span>"</span>)  <span class="comment"># è¾“å‡º: åŒ—äº¬</span></span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="1-3-2-é¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼è¯¦è§£"><a href="#1-3-2-é¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼è¯¦è§£" class="headerlink" title="1.3.2 é¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼è¯¦è§£"></a>1.3.2 é¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼è¯¦è§£</h3><h4 id="é€šä¿—ç†è§£ï¼šåŸ¹å…»â€é€šæ‰â€å†åŸ¹å…»â€ä¸“æ‰â€"><a href="#é€šä¿—ç†è§£ï¼šåŸ¹å…»â€é€šæ‰â€å†åŸ¹å…»â€ä¸“æ‰â€" class="headerlink" title="é€šä¿—ç†è§£ï¼šåŸ¹å…»â€é€šæ‰â€å†åŸ¹å…»â€ä¸“æ‰â€"></a>é€šä¿—ç†è§£ï¼šåŸ¹å…»â€é€šæ‰â€å†åŸ¹å…»â€ä¸“æ‰â€</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">ä¼ ç»Ÿæ–¹å¼ï¼ˆä»é›¶å¼€å§‹ï¼‰ï¼š</span><br><span class="line">  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">  â”‚  ä»»åŠ¡ï¼šæƒ…æ„Ÿåˆ†æ                                   â”‚</span><br><span class="line">  â”‚  æ•°æ®ï¼š10ä¸‡æ¡ç”µå•†è¯„è®º                             â”‚</span><br><span class="line">  â”‚  è®­ç»ƒï¼šä»éšæœºåˆå§‹åŒ–å¼€å§‹ï¼Œè®­ç»ƒä¸€ä¸ªä¸“é—¨çš„æ¨¡å‹         â”‚</span><br><span class="line">  â”‚  è€—æ—¶ï¼š3å¤©ï¼Œéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®                       â”‚</span><br><span class="line">  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br><span class="line">  </span><br><span class="line">  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">  â”‚  ä»»åŠ¡ï¼šåƒåœ¾é‚®ä»¶æ£€æµ‹                               â”‚</span><br><span class="line">  â”‚  æ•°æ®ï¼š5ä¸‡å°é‚®ä»¶                                  â”‚</span><br><span class="line">  â”‚  è®­ç»ƒï¼šä»éšæœºåˆå§‹åŒ–å¼€å§‹ï¼Œå†è®­ç»ƒå¦ä¸€ä¸ªä¸“é—¨çš„æ¨¡å‹     â”‚</span><br><span class="line">  â”‚  è€—æ—¶ï¼š2å¤©ï¼Œåˆéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®                     â”‚</span><br><span class="line">  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br><span class="line">  </span><br><span class="line">  é—®é¢˜ï¼šæ¯ä¸ªä»»åŠ¡éƒ½è¦ä»å¤´è®­ç»ƒï¼Œé‡å¤åŠ³åŠ¨ï¼</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">å¾®è°ƒèŒƒå¼ï¼ˆç«™åœ¨å·¨äººè‚©è†€ä¸Šï¼‰ï¼š</span><br><span class="line"></span><br><span class="line">  ç¬¬ä¸€é˜¶æ®µï¼šé¢„è®­ç»ƒï¼ˆPre-trainingï¼‰â€”â€” åªåšä¸€æ¬¡ï¼</span><br><span class="line">  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">  â”‚  æ•°æ®ï¼šæ•´ä¸ªç»´åŸºç™¾ç§‘ + å¤§é‡ä¹¦ç± (æ•°åäº¿è¯)          â”‚</span><br><span class="line">  â”‚  ä»»åŠ¡ï¼šMLMï¼ˆå®Œå½¢å¡«ç©ºï¼‰                            â”‚</span><br><span class="line">  â”‚  ç›®çš„ï¼šè®©æ¨¡å‹å­¦ä¼š"ç†è§£è¯­è¨€"                       â”‚</span><br><span class="line">  â”‚  è€—æ—¶ï¼šæ•°å‘¨ (ä½†åªéœ€åšä¸€æ¬¡ï¼Œç”± Google å®Œæˆ)         â”‚</span><br><span class="line">  â”‚  äº§å‡ºï¼šBERT é¢„è®­ç»ƒæ¨¡å‹ âœ¨                         â”‚</span><br><span class="line">  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br><span class="line">                    â”‚</span><br><span class="line">                    â–¼ ä¸‹è½½ç°æˆçš„ BERT</span><br><span class="line">                    </span><br><span class="line">  ç¬¬äºŒé˜¶æ®µï¼šå¾®è°ƒï¼ˆFine-tuningï¼‰â€”â€” æ¯ä¸ªä»»åŠ¡åªéœ€å‡ å°æ—¶ï¼</span><br><span class="line">  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">  â”‚  ä»»åŠ¡Aï¼šæƒ…æ„Ÿåˆ†æ                                  â”‚</span><br><span class="line">  â”‚  æ•°æ®ï¼šä»…éœ€ 1000 æ¡æ ‡æ³¨æ•°æ®ï¼                     â”‚</span><br><span class="line">  â”‚  æ–¹æ³•ï¼šåœ¨ BERT ä¸ŠåŠ ä¸€ä¸ªåˆ†ç±»å±‚ï¼Œå¾®è°ƒå‡ è½®            â”‚</span><br><span class="line">  â”‚  è€—æ—¶ï¼š30åˆ†é’Ÿ                                    â”‚</span><br><span class="line">  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br><span class="line">  </span><br><span class="line">  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">  â”‚  ä»»åŠ¡Bï¼šåƒåœ¾é‚®ä»¶æ£€æµ‹                              â”‚</span><br><span class="line">  â”‚  æ•°æ®ï¼šä»…éœ€ 500 æ¡æ ‡æ³¨æ•°æ®ï¼                      â”‚</span><br><span class="line">  â”‚  æ–¹æ³•ï¼šåŒæ ·åŠ åˆ†ç±»å±‚ï¼Œå¾®è°ƒå‡ è½®                      â”‚</span><br><span class="line">  â”‚  è€—æ—¶ï¼š20åˆ†é’Ÿ                                    â”‚</span><br><span class="line">  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br></pre></td></tr></tbody></table></figure>

<h4 id="å½¢è±¡ç±»æ¯”æ€»ç»“"><a href="#å½¢è±¡ç±»æ¯”æ€»ç»“" class="headerlink" title="å½¢è±¡ç±»æ¯”æ€»ç»“"></a>å½¢è±¡ç±»æ¯”æ€»ç»“</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">â”‚                      ğŸ“ æ•™è‚²ç±»æ¯”                             â”‚</span><br><span class="line">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â”‚  ã€é¢„è®­ç»ƒ Pre-trainingã€‘= ä¸Šå¤§å­¦ï¼Œæ¥å—é€šè¯†æ•™è‚²               â”‚</span><br><span class="line">â”‚     â€¢ å­¦ä¹ è¯­æ–‡ã€æ•°å­¦ã€è‹±è¯­ã€ç‰©ç†...                          â”‚</span><br><span class="line">â”‚     â€¢ ç›®æ ‡ï¼šæˆä¸ºä¸€ä¸ªæœ‰åŸºç¡€çŸ¥è¯†çš„"é€šæ‰"                       â”‚</span><br><span class="line">â”‚     â€¢ æ—¶é—´ï¼š4å¹´                                             â”‚</span><br><span class="line">â”‚     â€¢ æˆæœ¬ï¼šé«˜                                              â”‚</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â”‚  ã€MLM ä»»åŠ¡ã€‘= å¤§å­¦é‡Œçš„å„ç§ç»ƒä¹ é¢˜                            â”‚</span><br><span class="line">â”‚     â€¢ å®Œå½¢å¡«ç©ºã€é˜…è¯»ç†è§£ã€è¯­æ³•ç»ƒä¹ ...                        â”‚</span><br><span class="line">â”‚     â€¢ ç›®æ ‡ï¼šé”»ç‚¼è¯­è¨€ç†è§£èƒ½åŠ›                                 â”‚</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â”‚  ã€å¾®è°ƒ Fine-tuningã€‘= å·¥ä½œåçš„å²—ä½åŸ¹è®­                      â”‚</span><br><span class="line">â”‚     â€¢ é’ˆå¯¹å…·ä½“å·¥ä½œï¼ˆæƒ…æ„Ÿåˆ†æ/é—®ç­”/ç¿»è¯‘ï¼‰å­¦ä¹                   â”‚</span><br><span class="line">â”‚     â€¢ ç›®æ ‡ï¼šæˆä¸ºæŸä¸ªé¢†åŸŸçš„"ä¸“æ‰"                             â”‚</span><br><span class="line">â”‚     â€¢ æ—¶é—´ï¼šå‡ å¤©                                            â”‚</span><br><span class="line">â”‚     â€¢ æˆæœ¬ï¼šä½ï¼ˆå› ä¸ºå·²ç»æœ‰åŸºç¡€äº†ï¼‰                           â”‚</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">â”‚                    ğŸ’¡ ä¸ºä»€ä¹ˆè¿™æ ·æ›´å¥½ï¼Ÿ                       â”‚</span><br><span class="line">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â”‚  ä¼ ç»Ÿæ–¹å¼ï¼šæ¯æ¢ä¸€ä¸ªå·¥ä½œå°±é‡æ–°ä¸Šä¸€æ¬¡å¤§å­¦                       â”‚</span><br><span class="line">â”‚     â†’ å¤ªæ…¢ã€å¤ªè´µã€å¤ªæµªè´¹                                    â”‚</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â”‚  å¾®è°ƒèŒƒå¼ï¼šå¤§å­¦åªä¸Šä¸€æ¬¡ï¼Œæ¢å·¥ä½œåªéœ€çŸ­æœŸåŸ¹è®­                   â”‚</span><br><span class="line">â”‚     â†’ å¿«é€Ÿã€ä¾¿å®œã€é«˜æ•ˆ                                      â”‚</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â”‚  æ•°æ®å¯¹æ¯”ï¼š                                                 â”‚</span><br><span class="line">â”‚     â€¢ ä¼ ç»Ÿï¼šéœ€è¦ 10ä¸‡+ æ ‡æ³¨æ•°æ®                             â”‚</span><br><span class="line">â”‚     â€¢ å¾®è°ƒï¼šåªéœ€ 1000 æ¡æ ‡æ³¨æ•°æ®å°±èƒ½è¾¾åˆ°ç›¸ä¼¼æ•ˆæœï¼            â”‚</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br></pre></td></tr></tbody></table></figure>

<h4 id="æƒ…æ„Ÿåˆ†æå¾®è°ƒå®Œæ•´ç¤ºä¾‹"><a href="#æƒ…æ„Ÿåˆ†æå¾®è°ƒå®Œæ•´ç¤ºä¾‹" class="headerlink" title="æƒ…æ„Ÿåˆ†æå¾®è°ƒå®Œæ•´ç¤ºä¾‹"></a>æƒ…æ„Ÿåˆ†æå¾®è°ƒå®Œæ•´ç¤ºä¾‹</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡å°‘é‡æ ‡æ³¨æ•°æ® ============</span></span><br><span class="line">train_data = {</span><br><span class="line">    <span class="string">"text"</span>: [</span><br><span class="line">        <span class="string">"è¿™ä¸ªäº§å“å¤ªæ£’äº†ï¼Œå¼ºçƒˆæ¨èï¼"</span>,</span><br><span class="line">        <span class="string">"è´¨é‡å¾ˆå·®ï¼Œç”¨äº†ä¸€å¤©å°±åäº†"</span>,</span><br><span class="line">        <span class="string">"ä¸€èˆ¬èˆ¬ï¼Œæ²¡æœ‰æƒŠå–œä¹Ÿæ²¡æœ‰å¤±æœ›"</span>,</span><br><span class="line">        <span class="string">"è¶…çº§å–œæ¬¢ï¼Œå·²ç»å›è´­ä¸‰æ¬¡"</span>,</span><br><span class="line">        <span class="string">"å®¢æœæ€åº¦å¾ˆå·®ï¼Œå†ä¹Ÿä¸ä¹°äº†"</span>,</span><br><span class="line">        <span class="string">"æ€§ä»·æ¯”å¾ˆé«˜ï¼Œå€¼å¾—è´­ä¹°"</span>,</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"label"</span>: [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]  <span class="comment"># 1=æ­£é¢, 0=è´Ÿé¢</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ ç¬¬äºŒæ­¥ï¼šåŠ è½½é¢„è®­ç»ƒçš„ BERT ============</span></span><br><span class="line"><span class="comment"># è¿™ä¸ª BERT å·²ç»é€šè¿‡ MLM ä»»åŠ¡å­¦ä¼šäº†"ç†è§£ä¸­æ–‡"</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-chinese'</span>)</span><br><span class="line">model = BertForSequenceClassification.from_pretrained(</span><br><span class="line">    <span class="string">'bert-base-chinese'</span>, </span><br><span class="line">    num_labels=<span class="number">2</span>  <span class="comment"># æ­£é¢/è´Ÿé¢ ä¸¤ä¸ªç±»åˆ«</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ ç¬¬ä¸‰æ­¥ï¼šæ•°æ®é¢„å¤„ç† ============</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_function</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">"text"</span>], padding=<span class="string">"max_length"</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">dataset = Dataset.from_dict(train_data)</span><br><span class="line">tokenized_dataset = dataset.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ ç¬¬å››æ­¥ï¼šå¾®è°ƒè®­ç»ƒ ============</span></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">"./results"</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,           <span class="comment"># åªéœ€è¦è®­ç»ƒ 3 è½®ï¼</span></span><br><span class="line">    per_device_train_batch_size=<span class="number">4</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,           <span class="comment"># å¾ˆå°çš„å­¦ä¹ ç‡ï¼Œè½»å¾®è°ƒæ•´</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=tokenized_dataset,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()  <span class="comment"># å‡ åˆ†é’Ÿå°±å®Œæˆï¼</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ ç¬¬äº”æ­¥ï¼šä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ ============</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">text</span>):</span><br><span class="line">    inputs = tokenizer(text, return_tensors=<span class="string">"pt"</span>, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>)</span><br><span class="line">    outputs = model(**inputs)</span><br><span class="line">    prediction = outputs.logits.argmax(dim=-<span class="number">1</span>).item()</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"æ­£é¢ ğŸ˜Š"</span> <span class="keyword">if</span> prediction == <span class="number">1</span> <span class="keyword">else</span> <span class="string">"è´Ÿé¢ ğŸ˜"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># æµ‹è¯•</span></span><br><span class="line"><span class="built_in">print</span>(predict(<span class="string">"è¿™æ¬¾æ‰‹æœºæ‹ç…§æ•ˆæœæƒŠè‰³"</span>))  <span class="comment"># â†’ æ­£é¢ ğŸ˜Š</span></span><br><span class="line"><span class="built_in">print</span>(predict(<span class="string">"ç‰©æµå¤ªæ…¢äº†ï¼Œç­‰äº†ä¸€å‘¨"</span>))   <span class="comment"># â†’ è´Ÿé¢ ğŸ˜</span></span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="1-4-è®ºæ–‡å…³é”®åˆ›æ–°ç‚¹æ·±åº¦å‰–æ"><a href="#1-4-è®ºæ–‡å…³é”®åˆ›æ–°ç‚¹æ·±åº¦å‰–æ" class="headerlink" title="1.4 è®ºæ–‡å…³é”®åˆ›æ–°ç‚¹æ·±åº¦å‰–æ"></a>1.4 è®ºæ–‡å…³é”®åˆ›æ–°ç‚¹æ·±åº¦å‰–æ</h3><h4 id="ğŸ”¬-åˆ›æ–°ç‚¹-1ï¼šæ·±åº¦åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡"><a href="#ğŸ”¬-åˆ›æ–°ç‚¹-1ï¼šæ·±åº¦åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡" class="headerlink" title="ğŸ”¬ åˆ›æ–°ç‚¹ 1ï¼šæ·±åº¦åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡"></a>ğŸ”¬ åˆ›æ–°ç‚¹ 1ï¼šæ·±åº¦åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡</h4><table>
<thead>
<tr>
<th>æ¨¡å‹</th>
<th>ä¸Šä¸‹æ–‡æ–¹å‘</th>
<th>é—®é¢˜</th>
</tr>
</thead>
<tbody><tr>
<td>GPT-1</td>
<td>å•å‘ (å·¦â†’å³)</td>
<td>æ— æ³•åˆ©ç”¨å³ä¾§ä¿¡æ¯</td>
</tr>
<tr>
<td>ELMo</td>
<td>æµ…å±‚åŒå‘ (æ‹¼æ¥)</td>
<td>ä¸¤ä¸ªæ–¹å‘ä»…åœ¨é¡¶å±‚èåˆ</td>
</tr>
<tr>
<td><strong>BERT</strong></td>
<td><strong>æ·±åº¦åŒå‘</strong></td>
<td>âœ… æ¯ä¸€å±‚éƒ½èƒ½çœ‹åˆ°å®Œæ•´ä¸Šä¸‹æ–‡</td>
</tr>
</tbody></table>
<p><strong>æŠ€æœ¯å®ç°å…³é”®</strong>ï¼šé€šè¿‡ MLM ä»»åŠ¡ï¼Œæ¨¡å‹å¯ä»¥â€ä½œå¼Šâ€åœ°çœ‹åˆ°è¢«é¢„æµ‹è¯çš„ä¸¤ä¾§ä¿¡æ¯ã€‚</p>
<h4 id="ğŸ”¬-åˆ›æ–°ç‚¹-2ï¼šé¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼"><a href="#ğŸ”¬-åˆ›æ–°ç‚¹-2ï¼šé¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼" class="headerlink" title="ğŸ”¬ åˆ›æ–°ç‚¹ 2ï¼šé¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼"></a>ğŸ”¬ åˆ›æ–°ç‚¹ 2ï¼šé¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">ä¼ ç»Ÿæ–¹æ³•: ä»»åŠ¡A â†’ ä»å¤´è®­ç»ƒæ¨¡å‹A</span><br><span class="line">         ä»»åŠ¡B â†’ ä»å¤´è®­ç»ƒæ¨¡å‹B  (é‡å¤åŠ³åŠ¨!)</span><br><span class="line"></span><br><span class="line">BERTèŒƒå¼: å¤§è§„æ¨¡è¯­æ–™ â†’ é¢„è®­ç»ƒBERT (ä¸€æ¬¡)</span><br><span class="line">                â†“</span><br><span class="line">         ä»»åŠ¡A â†’ å¾®è°ƒ (ä»…éœ€å°æ•°æ®)</span><br><span class="line">         ä»»åŠ¡B â†’ å¾®è°ƒ (ä»…éœ€å°æ•°æ®)</span><br><span class="line">         ä»»åŠ¡C â†’ å¾®è°ƒ (ä»…éœ€å°æ•°æ®)</span><br></pre></td></tr></tbody></table></figure>

<p><strong>é©å‘½æ€§å½±å“</strong>ï¼šå°å…¬å¸/ç ”ç©¶è€…æ— éœ€å¤§è§„æ¨¡è®¡ç®—èµ„æºï¼Œåªéœ€å¾®è°ƒå³å¯è·å¾—SOTAæ€§èƒ½ã€‚</p>
<h4 id="ğŸ”¬-åˆ›æ–°ç‚¹-3ï¼šç»Ÿä¸€çš„ç‰¹å¾æå–å™¨"><a href="#ğŸ”¬-åˆ›æ–°ç‚¹-3ï¼šç»Ÿä¸€çš„ç‰¹å¾æå–å™¨" class="headerlink" title="ğŸ”¬ åˆ›æ–°ç‚¹ 3ï¼šç»Ÿä¸€çš„ç‰¹å¾æå–å™¨"></a>ğŸ”¬ åˆ›æ–°ç‚¹ 3ï¼šç»Ÿä¸€çš„ç‰¹å¾æå–å™¨</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BERT å¯ä»¥é€‚é…å‡ ä¹æ‰€æœ‰ NLP ä»»åŠ¡</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. å•å¥åˆ†ç±» (æƒ…æ„Ÿåˆ†æ)</span></span><br><span class="line">[CLS] è¿™éƒ¨ç”µå½±å¤ªæ£’äº† [SEP] â†’ CLSå‘é‡ â†’ åˆ†ç±»å™¨ â†’ æ­£é¢</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. å¥å¯¹åˆ†ç±» (è‡ªç„¶è¯­è¨€æ¨ç†)</span></span><br><span class="line">[CLS] å¤©åœ¨ä¸‹é›¨ [SEP] åœ°é¢æ˜¯æ¹¿çš„ [SEP] â†’ CLSå‘é‡ â†’ è•´å«/çŸ›ç›¾/ä¸­æ€§</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. åºåˆ—æ ‡æ³¨ (å‘½åå®ä½“è¯†åˆ«)</span></span><br><span class="line">[CLS] é©¬äº‘ åˆ›åŠäº† é˜¿é‡Œå·´å·´ [SEP] â†’ æ¯ä¸ªtoken â†’ B-PER O O B-ORG</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. é—®ç­” (é˜…è¯»ç†è§£)</span></span><br><span class="line">[CLS] é—®é¢˜ [SEP] æ–‡ç«  [SEP] â†’ é¢„æµ‹ç­”æ¡ˆèµ·æ­¢ä½ç½®</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="1-5-æ¶ˆèå®éªŒè¯¦è§£-Ablation-Study"><a href="#1-5-æ¶ˆèå®éªŒè¯¦è§£-Ablation-Study" class="headerlink" title="1.5 æ¶ˆèå®éªŒè¯¦è§£ (Ablation Study)"></a>1.5 æ¶ˆèå®éªŒè¯¦è§£ (Ablation Study)</h3><p>è®ºæ–‡é€šè¿‡æ¶ˆèå®éªŒéªŒè¯äº†å„ç»„ä»¶çš„é‡è¦æ€§ï¼š</p>
<h4 id="å®éªŒ-1ï¼šé¢„è®­ç»ƒä»»åŠ¡çš„å½±å“"><a href="#å®éªŒ-1ï¼šé¢„è®­ç»ƒä»»åŠ¡çš„å½±å“" class="headerlink" title="å®éªŒ 1ï¼šé¢„è®­ç»ƒä»»åŠ¡çš„å½±å“"></a>å®éªŒ 1ï¼šé¢„è®­ç»ƒä»»åŠ¡çš„å½±å“</h4><table>
<thead>
<tr>
<th>é…ç½®</th>
<th>MNLI</th>
<th>QNLI</th>
<th>SST-2</th>
</tr>
</thead>
<tbody><tr>
<td>BERT (MLM + NSP)</td>
<td><strong>84.6</strong></td>
<td><strong>90.5</strong></td>
<td><strong>93.5</strong></td>
</tr>
<tr>
<td>ä»… MLM (æ—  NSP)</td>
<td>84.3</td>
<td>90.2</td>
<td>93.2</td>
</tr>
<tr>
<td>ä»… LTR (å·¦åˆ°å³)</td>
<td>82.1</td>
<td>87.4</td>
<td>91.3</td>
</tr>
<tr>
<td>LTR + BiLSTM</td>
<td>82.8</td>
<td>88.1</td>
<td>91.6</td>
</tr>
</tbody></table>
<p><strong>ç»“è®º</strong>ï¼š</p>
<ul>
<li>MLM æ¯”å•å‘ LTR æå‡çº¦ 2.5%</li>
<li>NSP æå‡æœ‰é™ (~0.3%)ï¼Œåç»­ RoBERTa ç§»é™¤äº†å®ƒ</li>
</ul>
<h4 id="å®éªŒ-2ï¼šæ¨¡å‹è§„æ¨¡çš„å½±å“"><a href="#å®éªŒ-2ï¼šæ¨¡å‹è§„æ¨¡çš„å½±å“" class="headerlink" title="å®éªŒ 2ï¼šæ¨¡å‹è§„æ¨¡çš„å½±å“"></a>å®éªŒ 2ï¼šæ¨¡å‹è§„æ¨¡çš„å½±å“</h4><table>
<thead>
<tr>
<th>æ¨¡å‹</th>
<th>å±‚æ•°</th>
<th>éšè—ç»´åº¦</th>
<th>å‚æ•°é‡</th>
<th>MNLI</th>
</tr>
</thead>
<tbody><tr>
<td>BERT-Base</td>
<td>12</td>
<td>768</td>
<td>110M</td>
<td>84.6</td>
</tr>
<tr>
<td>BERT-Large</td>
<td>24</td>
<td>1024</td>
<td>340M</td>
<td><strong>86.7</strong></td>
</tr>
</tbody></table>
<p><strong>ç»“è®º</strong>ï¼šæ›´å¤§çš„æ¨¡å‹ = æ›´å¥½çš„æ€§èƒ½ (Scaling Law çš„æ—©æœŸéªŒè¯)</p>
<h4 id="å®éªŒ-3ï¼šMask-ç­–ç•¥çš„å½±å“"><a href="#å®éªŒ-3ï¼šMask-ç­–ç•¥çš„å½±å“" class="headerlink" title="å®éªŒ 3ï¼šMask ç­–ç•¥çš„å½±å“"></a>å®éªŒ 3ï¼šMask ç­–ç•¥çš„å½±å“</h4><table>
<thead>
<tr>
<th>Mask ç­–ç•¥</th>
<th>æ•ˆæœ</th>
</tr>
</thead>
<tbody><tr>
<td>100% [MASK]</td>
<td>æ¬¡ä¼˜ï¼Œé¢„è®­ç»ƒä¸å¾®è°ƒåˆ†å¸ƒä¸ä¸€è‡´</td>
</tr>
<tr>
<td>80%/10%/10% (è®ºæ–‡æ–¹æ¡ˆ)</td>
<td><strong>æœ€ä¼˜</strong></td>
</tr>
<tr>
<td>éšæœºæ¯”ä¾‹</td>
<td>ä¸ç¨³å®š</td>
</tr>
</tbody></table>
<hr>
<h3 id="1-6-è®­ç»ƒç»†èŠ‚ä¸è¶…å‚æ•°"><a href="#1-6-è®­ç»ƒç»†èŠ‚ä¸è¶…å‚æ•°" class="headerlink" title="1.6 è®­ç»ƒç»†èŠ‚ä¸è¶…å‚æ•°"></a>1.6 è®­ç»ƒç»†èŠ‚ä¸è¶…å‚æ•°</h3><h4 id="é¢„è®­ç»ƒé…ç½®"><a href="#é¢„è®­ç»ƒé…ç½®" class="headerlink" title="é¢„è®­ç»ƒé…ç½®"></a>é¢„è®­ç»ƒé…ç½®</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># æ•°æ®é›†</span></span><br><span class="line">- BooksCorpus: 800M è¯ (<span class="number">11</span>,038 æœ¬ä¹¦)</span><br><span class="line">- English Wikipedia: <span class="number">2</span>,500M è¯ (ä»…æ–‡æœ¬ï¼Œå»é™¤è¡¨æ ¼/åˆ—è¡¨)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è®­ç»ƒé…ç½®</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">max_seq_length = <span class="number">512</span> (å‰<span class="number">90</span>%æ­¥ç”¨<span class="number">128</span>ï¼Œå<span class="number">10</span>%ç”¨<span class="number">512</span>)</span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line">warmup_steps = <span class="number">10</span>,<span class="number">000</span></span><br><span class="line">total_steps = <span class="number">1</span>,<span class="number">000</span>,<span class="number">000</span></span><br><span class="line">optimizer = Adam (Î²<span class="number">1</span>=<span class="number">0.9</span>, Î²<span class="number">2</span>=<span class="number">0.999</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ç¡¬ä»¶</span></span><br><span class="line">BERT-Base: <span class="number">4</span> TPU Pods (<span class="number">16</span> TPU chips), <span class="number">4</span>å¤©</span><br><span class="line">BERT-Large: <span class="number">16</span> TPU Pods (<span class="number">64</span> TPU chips), <span class="number">4</span>å¤©</span><br></pre></td></tr></tbody></table></figure>

<h4 id="å¾®è°ƒé…ç½®"><a href="#å¾®è°ƒé…ç½®" class="headerlink" title="å¾®è°ƒé…ç½®"></a>å¾®è°ƒé…ç½®</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># é€šç”¨å¾®è°ƒè¶…å‚æ•°</span></span><br><span class="line">batch_size = <span class="number">16</span> æˆ– <span class="number">32</span></span><br><span class="line">learning_rate = <span class="number">2e-5</span>, <span class="number">3e-5</span>, <span class="number">5e-5</span> (é€‰æœ€ä¼˜)</span><br><span class="line">epochs = <span class="number">2</span>-<span class="number">4</span></span><br><span class="line">dropout = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ä¸åŒä»»åŠ¡çš„å¾®è°ƒæ—¶é—´</span></span><br><span class="line">MRPC (<span class="number">3.5</span>kæ ·æœ¬): ~<span class="number">1</span>åˆ†é’Ÿ</span><br><span class="line">SST-<span class="number">2</span> (67kæ ·æœ¬): ~<span class="number">1</span>å°æ—¶  </span><br><span class="line">SQuAD (100kæ ·æœ¬): ~<span class="number">30</span>åˆ†é’Ÿ</span><br></pre></td></tr></tbody></table></figure>

<hr>
<hr>
<h3 id="1-4-MLM-ä¸å¾®è°ƒèŒƒå¼æ·±åº¦è§£æ"><a href="#1-4-MLM-ä¸å¾®è°ƒèŒƒå¼æ·±åº¦è§£æ" class="headerlink" title="1.4 MLM ä¸å¾®è°ƒèŒƒå¼æ·±åº¦è§£æ"></a>1.4 MLM ä¸å¾®è°ƒèŒƒå¼æ·±åº¦è§£æ</h3><h4 id="ğŸ¯-ä»€ä¹ˆæ˜¯-MLMï¼ˆMasked-Language-Modelï¼‰ï¼Ÿ"><a href="#ğŸ¯-ä»€ä¹ˆæ˜¯-MLMï¼ˆMasked-Language-Modelï¼‰ï¼Ÿ" class="headerlink" title="ğŸ¯ ä»€ä¹ˆæ˜¯ MLMï¼ˆMasked Language Modelï¼‰ï¼Ÿ"></a>ğŸ¯ ä»€ä¹ˆæ˜¯ MLMï¼ˆMasked Language Modelï¼‰ï¼Ÿ</h4><p><strong>é€šä¿—ç†è§£</strong>ï¼šè®© AI åšâ€å®Œå½¢å¡«ç©ºâ€æ¸¸æˆ</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">å°å­¦è¯­æ–‡é¢˜ï¼š</span><br><span class="line">  "å°æ˜ _____ å­¦æ ¡ä¸Šè¯¾"</span><br><span class="line">  </span><br><span class="line">ç­”æ¡ˆï¼šå»ã€åˆ°ã€åœ¨...</span><br><span class="line"></span><br><span class="line">BERT çš„ MLM å°±æ˜¯è®© AI åšè¿™ç§"å®Œå½¢å¡«ç©º"ï¼</span><br></pre></td></tr></tbody></table></figure>

<h4 id="MLM-å…·ä½“æ“ä½œæµç¨‹"><a href="#MLM-å…·ä½“æ“ä½œæµç¨‹" class="headerlink" title="MLM å…·ä½“æ“ä½œæµç¨‹"></a>MLM å…·ä½“æ“ä½œæµç¨‹</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># åŸå§‹å¥å­</span></span><br><span class="line">åŸå¥ = <span class="string">"æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: éšæœºé€‰æ‹© 15% çš„è¯è¿›è¡Œå¤„ç†</span></span><br><span class="line">é€‰ä¸­ = <span class="string">"åŒ—äº¬"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: å¯¹é€‰ä¸­çš„è¯è¿›è¡Œä¸‰ç§å¤„ç†ï¼ˆéšæœºé€‰ä¸€ç§ï¼‰</span></span><br><span class="line">å¤„ç†å = <span class="string">"æˆ‘çˆ± [MASK] å¤©å®‰é—¨"</span>   <span class="comment"># 80% æ¦‚ç‡ï¼šæ›¿æ¢ä¸º [MASK]</span></span><br><span class="line">æˆ–è€…   = <span class="string">"æˆ‘çˆ± ä¸Šæµ· å¤©å®‰é—¨"</span>     <span class="comment"># 10% æ¦‚ç‡ï¼šæ›¿æ¢ä¸ºéšæœºè¯</span></span><br><span class="line">æˆ–è€…   = <span class="string">"æˆ‘çˆ± åŒ—äº¬ å¤©å®‰é—¨"</span>     <span class="comment"># 10% æ¦‚ç‡ï¼šä¿æŒä¸å˜</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: è®©æ¨¡å‹é¢„æµ‹è¢«é®ä½çš„è¯æ˜¯ä»€ä¹ˆ</span></span><br><span class="line">æ¨¡å‹è¾“å…¥ = <span class="string">"æˆ‘çˆ± [MASK] å¤©å®‰é—¨"</span></span><br><span class="line">æ¨¡å‹è¾“å‡º = <span class="string">"åŒ—äº¬"</span> âœ…  (å¦‚æœé¢„æµ‹å¯¹äº†ï¼Œloss å¾ˆå°)</span><br><span class="line">         = <span class="string">"ä¸Šæµ·"</span> âŒ  (å¦‚æœé¢„æµ‹é”™äº†ï¼Œloss å¾ˆå¤§)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="ä¸ºä»€ä¹ˆ-MLM-èƒ½è®©-BERT-å­¦ä¼šâ€ç†è§£è¯­è¨€â€ï¼Ÿ-1"><a href="#ä¸ºä»€ä¹ˆ-MLM-èƒ½è®©-BERT-å­¦ä¼šâ€ç†è§£è¯­è¨€â€ï¼Ÿ-1" class="headerlink" title="ä¸ºä»€ä¹ˆ MLM èƒ½è®© BERT å­¦ä¼šâ€ç†è§£è¯­è¨€â€ï¼Ÿ"></a>ä¸ºä»€ä¹ˆ MLM èƒ½è®© BERT å­¦ä¼šâ€ç†è§£è¯­è¨€â€ï¼Ÿ</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">åœºæ™¯ 1ï¼š</span><br><span class="line">  è¾“å…¥ï¼š"æˆ‘å» [MASK] å­˜é’±"</span><br><span class="line">  æ¨¡å‹å­¦ä¼šï¼šçœ‹åˆ°"å­˜é’±" â†’ é¢„æµ‹"é“¶è¡Œ"ï¼ˆé‡‘èæœºæ„ï¼‰</span><br><span class="line"></span><br><span class="line">åœºæ™¯ 2ï¼š</span><br><span class="line">  è¾“å…¥ï¼š"æ²³è¾¹çš„ [MASK] å¾ˆé™¡å³­"  </span><br><span class="line">  æ¨¡å‹å­¦ä¼šï¼šçœ‹åˆ°"æ²³è¾¹""é™¡å³­" â†’ é¢„æµ‹"æ²³å²¸/å ¤å"</span><br><span class="line"></span><br><span class="line">é€šè¿‡æ•°åäº¿æ¬¡è¿™æ ·çš„"å®Œå½¢å¡«ç©º"è®­ç»ƒåï¼š</span><br><span class="line">  â†’ æ¨¡å‹å­¦ä¼šäº†è¯è¯­ä¹‹é—´çš„å…³ç³»</span><br><span class="line">  â†’ æ¨¡å‹å­¦ä¼šäº†è¯­æ³•ç»“æ„</span><br><span class="line">  â†’ æ¨¡å‹å­¦ä¼šäº†å¸¸è¯†çŸ¥è¯†</span><br></pre></td></tr></tbody></table></figure>

<h4 id="MLM-è®­ç»ƒä»£ç ç¤ºä¾‹-1"><a href="#MLM-è®­ç»ƒä»£ç ç¤ºä¾‹-1" class="headerlink" title="MLM è®­ç»ƒä»£ç ç¤ºä¾‹"></a>MLM è®­ç»ƒä»£ç ç¤ºä¾‹</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForMaskedLM</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># åŠ è½½æ¨¡å‹</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-chinese'</span>)</span><br><span class="line">model = BertForMaskedLM.from_pretrained(<span class="string">'bert-base-chinese'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è¾“å…¥ä¸€ä¸ªå¸¦ [MASK] çš„å¥å­</span></span><br><span class="line">text = <span class="string">"æˆ‘çˆ±[MASK]å¤©å®‰é—¨"</span></span><br><span class="line">inputs = tokenizer(text, return_tensors=<span class="string">'pt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨¡å‹é¢„æµ‹</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = model(**inputs)</span><br><span class="line">    predictions = outputs.logits</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ‰¾åˆ° [MASK] ä½ç½®çš„é¢„æµ‹ç»“æœ</span></span><br><span class="line">mask_index = (inputs[<span class="string">'input_ids'</span>] == tokenizer.mask_token_id).nonzero()[<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">predicted_token_id = predictions[<span class="number">0</span>, mask_index].argmax(dim=-<span class="number">1</span>)</span><br><span class="line">predicted_token = tokenizer.decode(predicted_token_id)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"åŸå¥: <span class="subst">{text}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"é¢„æµ‹: <span class="subst">{predicted_token}</span>"</span>)  <span class="comment"># è¾“å‡º: åŒ—äº¬</span></span><br></pre></td></tr></tbody></table></figure>

<hr>
<h4 id="ğŸ”§-ä»€ä¹ˆæ˜¯å¾®è°ƒèŒƒå¼ï¼ˆFine-tuning-Paradigmï¼‰ï¼Ÿ"><a href="#ğŸ”§-ä»€ä¹ˆæ˜¯å¾®è°ƒèŒƒå¼ï¼ˆFine-tuning-Paradigmï¼‰ï¼Ÿ" class="headerlink" title="ğŸ”§ ä»€ä¹ˆæ˜¯å¾®è°ƒèŒƒå¼ï¼ˆFine-tuning Paradigmï¼‰ï¼Ÿ"></a>ğŸ”§ ä»€ä¹ˆæ˜¯å¾®è°ƒèŒƒå¼ï¼ˆFine-tuning Paradigmï¼‰ï¼Ÿ</h4><p><strong>é€šä¿—ç†è§£</strong>ï¼šåŸ¹å…»â€é€šæ‰â€å†åŸ¹å…»â€ä¸“æ‰â€</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">ä¼ ç»Ÿæ–¹å¼ï¼ˆä»é›¶å¼€å§‹ï¼‰ï¼š</span><br><span class="line">  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">  â”‚  ä»»åŠ¡ï¼šæƒ…æ„Ÿåˆ†æ                                   â”‚</span><br><span class="line">  â”‚  æ•°æ®ï¼š10ä¸‡æ¡ç”µå•†è¯„è®º                             â”‚</span><br><span class="line">  â”‚  è®­ç»ƒï¼šä»éšæœºåˆå§‹åŒ–å¼€å§‹ï¼Œè®­ç»ƒä¸€ä¸ªä¸“é—¨çš„æ¨¡å‹         â”‚</span><br><span class="line">  â”‚  è€—æ—¶ï¼š3å¤©ï¼Œéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ®                       â”‚</span><br><span class="line">  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br><span class="line">  </span><br><span class="line">  é—®é¢˜ï¼šæ¯ä¸ªä»»åŠ¡éƒ½è¦ä»å¤´è®­ç»ƒï¼Œé‡å¤åŠ³åŠ¨ï¼</span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">å¾®è°ƒèŒƒå¼ï¼ˆç«™åœ¨å·¨äººè‚©è†€ä¸Šï¼‰ï¼š</span><br><span class="line"></span><br><span class="line">  ç¬¬ä¸€é˜¶æ®µï¼šé¢„è®­ç»ƒï¼ˆPre-trainingï¼‰â€”â€” åªåšä¸€æ¬¡ï¼</span><br><span class="line">  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">  â”‚  æ•°æ®ï¼šæ•´ä¸ªç»´åŸºç™¾ç§‘ + å¤§é‡ä¹¦ç± (æ•°åäº¿è¯)          â”‚</span><br><span class="line">  â”‚  ä»»åŠ¡ï¼šMLMï¼ˆå®Œå½¢å¡«ç©ºï¼‰                            â”‚</span><br><span class="line">  â”‚  ç›®çš„ï¼šè®©æ¨¡å‹å­¦ä¼š"ç†è§£è¯­è¨€"                       â”‚</span><br><span class="line">  â”‚  è€—æ—¶ï¼šæ•°å‘¨ (ä½†åªéœ€åšä¸€æ¬¡ï¼Œç”± Google å®Œæˆ)         â”‚</span><br><span class="line">  â”‚  äº§å‡ºï¼šBERT é¢„è®­ç»ƒæ¨¡å‹ âœ¨                         â”‚</span><br><span class="line">  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br><span class="line">                    â”‚</span><br><span class="line">                    â–¼ ä¸‹è½½ç°æˆçš„ BERT</span><br><span class="line">                    </span><br><span class="line">  ç¬¬äºŒé˜¶æ®µï¼šå¾®è°ƒï¼ˆFine-tuningï¼‰â€”â€” æ¯ä¸ªä»»åŠ¡åªéœ€å‡ å°æ—¶ï¼</span><br><span class="line">  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">  â”‚  ä»»åŠ¡Aï¼šæƒ…æ„Ÿåˆ†æ                                  â”‚</span><br><span class="line">  â”‚  æ•°æ®ï¼šä»…éœ€ 1000 æ¡æ ‡æ³¨æ•°æ®ï¼                     â”‚</span><br><span class="line">  â”‚  æ–¹æ³•ï¼šåœ¨ BERT ä¸ŠåŠ ä¸€ä¸ªåˆ†ç±»å±‚ï¼Œå¾®è°ƒå‡ è½®            â”‚</span><br><span class="line">  â”‚  è€—æ—¶ï¼š30åˆ†é’Ÿ                                    â”‚</span><br><span class="line">  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br></pre></td></tr></tbody></table></figure>

<h4 id="å¾®è°ƒç¤ºä¾‹ï¼šæƒ…æ„Ÿåˆ†æå®Œæ•´ä»£ç "><a href="#å¾®è°ƒç¤ºä¾‹ï¼šæƒ…æ„Ÿåˆ†æå®Œæ•´ä»£ç " class="headerlink" title="å¾®è°ƒç¤ºä¾‹ï¼šæƒ…æ„Ÿåˆ†æå®Œæ•´ä»£ç "></a>å¾®è°ƒç¤ºä¾‹ï¼šæƒ…æ„Ÿåˆ†æå®Œæ•´ä»£ç </h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡å°‘é‡æ ‡æ³¨æ•°æ® ============</span></span><br><span class="line">train_data = {</span><br><span class="line">    <span class="string">"text"</span>: [</span><br><span class="line">        <span class="string">"è¿™ä¸ªäº§å“å¤ªæ£’äº†ï¼Œå¼ºçƒˆæ¨èï¼"</span>,</span><br><span class="line">        <span class="string">"è´¨é‡å¾ˆå·®ï¼Œç”¨äº†ä¸€å¤©å°±åäº†"</span>,</span><br><span class="line">        <span class="string">"ä¸€èˆ¬èˆ¬ï¼Œæ²¡æœ‰æƒŠå–œä¹Ÿæ²¡æœ‰å¤±æœ›"</span>,</span><br><span class="line">        <span class="string">"è¶…çº§å–œæ¬¢ï¼Œå·²ç»å›è´­ä¸‰æ¬¡"</span>,</span><br><span class="line">        <span class="string">"å®¢æœæ€åº¦å¾ˆå·®ï¼Œå†ä¹Ÿä¸ä¹°äº†"</span>,</span><br><span class="line">        <span class="string">"æ€§ä»·æ¯”å¾ˆé«˜ï¼Œå€¼å¾—è´­ä¹°"</span>,</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"label"</span>: [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]  <span class="comment"># 1=æ­£é¢, 0=è´Ÿé¢</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ ç¬¬äºŒæ­¥ï¼šåŠ è½½é¢„è®­ç»ƒçš„ BERT ============</span></span><br><span class="line"><span class="comment"># è¿™ä¸ª BERT å·²ç»é€šè¿‡ MLM ä»»åŠ¡å­¦ä¼šäº†"ç†è§£ä¸­æ–‡"</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-chinese'</span>)</span><br><span class="line">model = BertForSequenceClassification.from_pretrained(</span><br><span class="line">    <span class="string">'bert-base-chinese'</span>, </span><br><span class="line">    num_labels=<span class="number">2</span>  <span class="comment"># æ­£é¢/è´Ÿé¢ ä¸¤ä¸ªç±»åˆ«</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ ç¬¬ä¸‰æ­¥ï¼šæ•°æ®é¢„å¤„ç† ============</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_function</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">"text"</span>], padding=<span class="string">"max_length"</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">dataset = Dataset.from_dict(train_data)</span><br><span class="line">tokenized_dataset = dataset.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ ç¬¬å››æ­¥ï¼šå¾®è°ƒè®­ç»ƒ ============</span></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">"./results"</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,           <span class="comment"># åªéœ€è¦è®­ç»ƒ 3 è½®ï¼</span></span><br><span class="line">    per_device_train_batch_size=<span class="number">4</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,           <span class="comment"># å¾ˆå°çš„å­¦ä¹ ç‡ï¼Œè½»å¾®è°ƒæ•´</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=tokenized_dataset,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()  <span class="comment"># å‡ åˆ†é’Ÿå°±å®Œæˆï¼</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ ç¬¬äº”æ­¥ï¼šä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ ============</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">text</span>):</span><br><span class="line">    inputs = tokenizer(text, return_tensors=<span class="string">"pt"</span>, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>)</span><br><span class="line">    outputs = model(**inputs)</span><br><span class="line">    prediction = outputs.logits.argmax(dim=-<span class="number">1</span>).item()</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"æ­£é¢ ğŸ˜Š"</span> <span class="keyword">if</span> prediction == <span class="number">1</span> <span class="keyword">else</span> <span class="string">"è´Ÿé¢ ğŸ˜"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># æµ‹è¯•</span></span><br><span class="line"><span class="built_in">print</span>(predict(<span class="string">"è¿™æ¬¾æ‰‹æœºæ‹ç…§æ•ˆæœæƒŠè‰³"</span>))  <span class="comment"># â†’ æ­£é¢ ğŸ˜Š</span></span><br><span class="line"><span class="built_in">print</span>(predict(<span class="string">"ç‰©æµå¤ªæ…¢äº†ï¼Œç­‰äº†ä¸€å‘¨"</span>))   <span class="comment"># â†’ è´Ÿé¢ ğŸ˜</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="å½¢è±¡ç±»æ¯”æ€»ç»“-1"><a href="#å½¢è±¡ç±»æ¯”æ€»ç»“-1" class="headerlink" title="å½¢è±¡ç±»æ¯”æ€»ç»“"></a>å½¢è±¡ç±»æ¯”æ€»ç»“</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">â”‚                      ğŸ“ æ•™è‚²ç±»æ¯”                             â”‚</span><br><span class="line">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â”‚  ã€é¢„è®­ç»ƒ Pre-trainingã€‘= ä¸Šå¤§å­¦ï¼Œæ¥å—é€šè¯†æ•™è‚²               â”‚</span><br><span class="line">â”‚     â€¢ å­¦ä¹ è¯­æ–‡ã€æ•°å­¦ã€è‹±è¯­ã€ç‰©ç†...                          â”‚</span><br><span class="line">â”‚     â€¢ ç›®æ ‡ï¼šæˆä¸ºä¸€ä¸ªæœ‰åŸºç¡€çŸ¥è¯†çš„"é€šæ‰"                       â”‚</span><br><span class="line">â”‚     â€¢ æ—¶é—´ï¼š4å¹´                                             â”‚</span><br><span class="line">â”‚     â€¢ æˆæœ¬ï¼šé«˜                                              â”‚</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â”‚  ã€MLM ä»»åŠ¡ã€‘= å¤§å­¦é‡Œçš„å„ç§ç»ƒä¹ é¢˜                            â”‚</span><br><span class="line">â”‚     â€¢ å®Œå½¢å¡«ç©ºã€é˜…è¯»ç†è§£ã€è¯­æ³•ç»ƒä¹ ...                        â”‚</span><br><span class="line">â”‚     â€¢ ç›®æ ‡ï¼šé”»ç‚¼è¯­è¨€ç†è§£èƒ½åŠ›                                 â”‚</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â”‚  ã€å¾®è°ƒ Fine-tuningã€‘= å·¥ä½œåçš„å²—ä½åŸ¹è®­                      â”‚</span><br><span class="line">â”‚     â€¢ é’ˆå¯¹å…·ä½“å·¥ä½œï¼ˆæƒ…æ„Ÿåˆ†æ/é—®ç­”/ç¿»è¯‘ï¼‰å­¦ä¹                   â”‚</span><br><span class="line">â”‚     â€¢ ç›®æ ‡ï¼šæˆä¸ºæŸä¸ªé¢†åŸŸçš„"ä¸“æ‰"                             â”‚</span><br><span class="line">â”‚     â€¢ æ—¶é—´ï¼šå‡ å¤©                                            â”‚</span><br><span class="line">â”‚     â€¢ æˆæœ¬ï¼šä½ï¼ˆå› ä¸ºå·²ç»æœ‰åŸºç¡€äº†ï¼‰                           â”‚</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="1-5-MASK-Token-æœºåˆ¶æ·±åº¦è§£æ"><a href="#1-5-MASK-Token-æœºåˆ¶æ·±åº¦è§£æ" class="headerlink" title="1.5 [MASK] Token æœºåˆ¶æ·±åº¦è§£æ"></a>1.5 [MASK] Token æœºåˆ¶æ·±åº¦è§£æ</h3><h4 id="MASK-åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ"><a href="#MASK-åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ" class="headerlink" title="[MASK] åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ"></a>[MASK] åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-chinese'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [MASK] å°±æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„ Token ID</span></span><br><span class="line"><span class="built_in">print</span>(tokenizer.mask_token)     <span class="comment"># è¾“å‡º: [MASK]</span></span><br><span class="line"><span class="built_in">print</span>(tokenizer.mask_token_id)  <span class="comment"># è¾“å‡º: 103</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="å®Œæ•´è¾“å…¥å¯¹æ¯”"><a href="#å®Œæ•´è¾“å…¥å¯¹æ¯”" class="headerlink" title="å®Œæ•´è¾“å…¥å¯¹æ¯”"></a>å®Œæ•´è¾“å…¥å¯¹æ¯”</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># åŸå§‹å¥å­</span></span><br><span class="line">åŸå¥ = <span class="string">"æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨"</span></span><br><span class="line">tokenizer(åŸå¥)</span><br><span class="line"></span><br><span class="line">Token:    [<span class="string">"[CLS]"</span>, <span class="string">"æˆ‘"</span>, <span class="string">"çˆ±"</span>, <span class="string">"åŒ—äº¬"</span>, <span class="string">"å¤©"</span>, <span class="string">"å®‰"</span>, <span class="string">"é—¨"</span>, <span class="string">"[SEP]"</span>]</span><br><span class="line">Token ID: [  <span class="number">101</span>,  <span class="number">2769</span>, <span class="number">4263</span>,  <span class="number">1266</span>, <span class="number">1921</span>, <span class="number">2128</span>, <span class="number">7305</span>,   <span class="number">102</span>  ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># åš MLM æ—¶æŠŠ "åŒ—äº¬" æ›¿æ¢æˆ [MASK]</span></span><br><span class="line">é®è”½å¥ = <span class="string">"æˆ‘çˆ±[MASK]å¤©å®‰é—¨"</span></span><br><span class="line">tokenizer(é®è”½å¥)</span><br><span class="line"></span><br><span class="line">Token:    [<span class="string">"[CLS]"</span>, <span class="string">"æˆ‘"</span>, <span class="string">"çˆ±"</span>, <span class="string">"[MASK]"</span>, <span class="string">"å¤©"</span>, <span class="string">"å®‰"</span>, <span class="string">"é—¨"</span>, <span class="string">"[SEP]"</span>]</span><br><span class="line">Token ID: [  <span class="number">101</span>,  <span class="number">2769</span>, <span class="number">4263</span>,    <span class="number">103</span>,   <span class="number">1921</span>, <span class="number">2128</span>, <span class="number">7305</span>,   <span class="number">102</span>  ]</span><br><span class="line">                                  â†‘</span><br><span class="line">                          å°±æ˜¯æŠŠ <span class="number">1266</span> æ¢æˆäº† <span class="number">103</span>ï¼</span><br></pre></td></tr></tbody></table></figure>

<h4 id="æ¨¡å‹å®é™…çœ‹åˆ°çš„æ˜¯ä»€ä¹ˆï¼Ÿ"><a href="#æ¨¡å‹å®é™…çœ‹åˆ°çš„æ˜¯ä»€ä¹ˆï¼Ÿ" class="headerlink" title="æ¨¡å‹å®é™…çœ‹åˆ°çš„æ˜¯ä»€ä¹ˆï¼Ÿ"></a>æ¨¡å‹å®é™…çœ‹åˆ°çš„æ˜¯ä»€ä¹ˆï¼Ÿ</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># æ¨¡å‹è¾“å…¥çš„å°±æ˜¯ä¸€ä¸²æ•°å­—ï¼ˆToken IDsï¼‰</span></span><br><span class="line">input_ids = [<span class="number">101</span>, <span class="number">2769</span>, <span class="number">4263</span>, <span class="number">103</span>, <span class="number">1921</span>, <span class="number">2128</span>, <span class="number">7305</span>, <span class="number">102</span>]</span><br><span class="line">                              â†‘</span><br><span class="line">                     <span class="number">103</span> = [MASK] çš„ç¼–å·</span><br><span class="line"></span><br><span class="line"><span class="comment"># ç»è¿‡ Embedding å±‚å</span></span><br><span class="line"><span class="comment"># 103 ä¼šè¢«æŸ¥è¡¨è½¬æ¢æˆä¸€ä¸ª 768 ç»´çš„å‘é‡</span></span><br><span class="line"></span><br><span class="line">embedding_table = model.embeddings.word_embeddings.weight</span><br><span class="line"><span class="comment"># å½¢çŠ¶: [21128, 768]  (è¯è¡¨å¤§å° Ã— éšè—ç»´åº¦)</span></span><br><span class="line"></span><br><span class="line">mask_embedding = embedding_table[<span class="number">103</span>]  <span class="comment"># [MASK] çš„å‘é‡è¡¨ç¤º</span></span><br><span class="line"><span class="comment"># å½¢çŠ¶: [768]</span></span><br><span class="line"><span class="comment"># å€¼: [0.023, -0.156, 0.234, ..., 0.089]  â† è¿™æ˜¯å¯å­¦ä¹ çš„å‚æ•°ï¼</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="ä¸ºä»€ä¹ˆ-103-åˆå§‹æ—¶å•¥ä¹Ÿä¸ä»£è¡¨ï¼Ÿ"><a href="#ä¸ºä»€ä¹ˆ-103-åˆå§‹æ—¶å•¥ä¹Ÿä¸ä»£è¡¨ï¼Ÿ" class="headerlink" title="ä¸ºä»€ä¹ˆ 103 åˆå§‹æ—¶å•¥ä¹Ÿä¸ä»£è¡¨ï¼Ÿ"></a>ä¸ºä»€ä¹ˆ 103 åˆå§‹æ—¶å•¥ä¹Ÿä¸ä»£è¡¨ï¼Ÿ</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Embedding å±‚è¾“å‡º</span></span><br><span class="line">embeddings = [</span><br><span class="line">    E_cls,    <span class="comment"># [CLS] çš„å‘é‡</span></span><br><span class="line">    E_æˆ‘,     <span class="comment"># "æˆ‘" çš„è¯­ä¹‰å‘é‡</span></span><br><span class="line">    E_çˆ±,     <span class="comment"># "çˆ±" çš„è¯­ä¹‰å‘é‡  </span></span><br><span class="line">    E_mask,   <span class="comment"># 103 å¯¹åº”çš„å‘é‡ â† è¿™ä¸ªå‘é‡æœ¬èº«æ²¡æœ‰è¯­ä¹‰ï¼</span></span><br><span class="line">    E_å¤©,     <span class="comment"># "å¤©" çš„è¯­ä¹‰å‘é‡</span></span><br><span class="line">    E_å®‰,     <span class="comment"># "å®‰" çš„è¯­ä¹‰å‘é‡</span></span><br><span class="line">    E_é—¨,     <span class="comment"># "é—¨" çš„è¯­ä¹‰å‘é‡</span></span><br><span class="line">    E_sep,    <span class="comment"># [SEP] çš„å‘é‡</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># E_mask åˆå§‹å€¼å¯èƒ½æ˜¯ [0.01, -0.02, 0.03, ...]</span></span><br><span class="line"><span class="comment"># å®ƒæœ¬èº«ä¸ä»£è¡¨ä»»ä½•è¯ä¹‰ï¼Œåªæ˜¯ä¸€ä¸ª"å ä½ç¬¦"</span></span><br></pre></td></tr></tbody></table></figure>

<p><strong>å…³é”®</strong>ï¼š103 æœ¬èº«ç¡®å®å•¥ä¹Ÿä¸ä»£è¡¨ï¼Œä½†ç»è¿‡ Self-Attention åä¼šå¸æ”¶å‘¨å›´è¯çš„ä¿¡æ¯ï¼</p>
<hr>
<h3 id="1-6-BERT-æ¨¡å‹æ¶æ„è¯¦è§£"><a href="#1-6-BERT-æ¨¡å‹æ¶æ„è¯¦è§£" class="headerlink" title="1.6 BERT æ¨¡å‹æ¶æ„è¯¦è§£"></a>1.6 BERT æ¨¡å‹æ¶æ„è¯¦è§£</h3><h4 id="æ¶æ„å‚æ•°"><a href="#æ¶æ„å‚æ•°" class="headerlink" title="æ¶æ„å‚æ•°"></a>æ¶æ„å‚æ•°</h4><table>
<thead>
<tr>
<th>é…ç½®</th>
<th>BERT-Base</th>
<th>BERT-Large</th>
</tr>
</thead>
<tbody><tr>
<td>Transformer å±‚æ•° (L)</td>
<td>12</td>
<td>24</td>
</tr>
<tr>
<td>éšè—å±‚ç»´åº¦ (H)</td>
<td>768</td>
<td>1024</td>
</tr>
<tr>
<td>æ³¨æ„åŠ›å¤´æ•° (A)</td>
<td>12</td>
<td>16</td>
</tr>
<tr>
<td>æ¯ä¸ªå¤´çš„ç»´åº¦ (H/A)</td>
<td>64</td>
<td>64</td>
</tr>
<tr>
<td>æ€»å‚æ•°é‡</td>
<td>110M</td>
<td>340M</td>
</tr>
<tr>
<td>FFN ä¸­é—´å±‚ç»´åº¦</td>
<td>3072</td>
<td>4096</td>
</tr>
</tbody></table>
<h4 id="è¾“å…¥è¡¨ç¤º-Input-Representation"><a href="#è¾“å…¥è¡¨ç¤º-Input-Representation" class="headerlink" title="è¾“å…¥è¡¨ç¤º (Input Representation)"></a>è¾“å…¥è¡¨ç¤º (Input Representation)</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Token Embedding: æ¯ä¸ªè¯çš„å‘é‡è¡¨ç¤º [vocab_size, 768]</span><br><span class="line">  + </span><br><span class="line">Position Embedding: ä½ç½®ç¼–ç  [512, 768] (æœ€å¤§åºåˆ—é•¿åº¦ 512)</span><br><span class="line">  +</span><br><span class="line">Segment Embedding: å¥å­åˆ†éš” [2, 768] (Sentence A æˆ– B)</span><br><span class="line">  =</span><br><span class="line">Input Representation: [batch, seq_len, 768]</span><br></pre></td></tr></tbody></table></figure>

<p><strong>å…·ä½“ä¾‹å­</strong>:</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">è¾“å…¥: <span class="string">"[CLS] æˆ‘ çˆ± NLP [SEP] å®ƒ å¾ˆ æœ‰è¶£ [SEP]"</span></span><br><span class="line"></span><br><span class="line">Token IDs:    [<span class="number">101</span>, <span class="number">2769</span>, <span class="number">4263</span>, <span class="number">21128</span>, <span class="number">102</span>, <span class="number">1045</span>, <span class="number">1447</span>, <span class="number">3300</span>, <span class="number">4638</span>, <span class="number">102</span>]</span><br><span class="line">Segment IDs:  [<span class="number">0</span>,   <span class="number">0</span>,    <span class="number">0</span>,    <span class="number">0</span>,     <span class="number">0</span>,   <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>]</span><br><span class="line">Position IDs: [<span class="number">0</span>,   <span class="number">1</span>,    <span class="number">2</span>,    <span class="number">3</span>,     <span class="number">4</span>,   <span class="number">5</span>,    <span class="number">6</span>,    <span class="number">7</span>,    <span class="number">8</span>,    <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line">ä¸‰ä¸ª embedding ç›¸åŠ å: [<span class="number">10</span>, <span class="number">768</span>]</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="1-5-è®ºæ–‡å®éªŒç»“æœä¸å½±å“"><a href="#1-5-è®ºæ–‡å®éªŒç»“æœä¸å½±å“" class="headerlink" title="1.5 è®ºæ–‡å®éªŒç»“æœä¸å½±å“"></a>1.5 è®ºæ–‡å®éªŒç»“æœä¸å½±å“</h3><h4 id="å…³é”®æ€§èƒ½æå‡"><a href="#å…³é”®æ€§èƒ½æå‡" class="headerlink" title="å…³é”®æ€§èƒ½æå‡"></a>å…³é”®æ€§èƒ½æå‡</h4><p><strong>GLUE åŸºå‡†æµ‹è¯•</strong> (11 ä¸ª NLP ä»»åŠ¡):</p>
<ul>
<li>BERT-Base: 78.6% â†’ æå‡ 7%</li>
<li>BERT-Large: 80.5% â†’ æå‡ 9%</li>
</ul>
<p><strong>SQuAD é—®ç­”ä»»åŠ¡</strong>:</p>
<ul>
<li>BERT-Large: F1 = 93.2% (è¶…è¶Šäººç±»æ°´å¹³ 91.2%)</li>
</ul>
<p><strong>ä¸ºä»€ä¹ˆæ•ˆæœè¿™ä¹ˆå¥½ï¼Ÿ</strong></p>
<ol>
<li><strong>åŒå‘ä¸Šä¸‹æ–‡</strong>: æ¯ä¸ªè¯éƒ½èƒ½çœ‹åˆ°å®Œæ•´å¥å­</li>
<li><strong>æ·±åº¦äº¤äº’</strong>: 12/24 å±‚é€å±‚ç²¾ç‚¼è¡¨ç¤º</li>
<li><strong>å¤§è§„æ¨¡é¢„è®­ç»ƒ</strong>: BooksCorpus (800M è¯) + Wikipedia (2500M è¯)</li>
<li><strong>è¿ç§»å­¦ä¹ </strong>: é¢„è®­ç»ƒ + å¾®è°ƒèŒƒå¼</li>
</ol>
<hr>
<hr>
<hr>
<h2 id="ğŸ”¬-Part-2-BERT-å‰ä¸¤å±‚å®Œæ•´æ¨æ¼”"><a href="#ğŸ”¬-Part-2-BERT-å‰ä¸¤å±‚å®Œæ•´æ¨æ¼”" class="headerlink" title="ğŸ”¬ Part 2: BERT å‰ä¸¤å±‚å®Œæ•´æ¨æ¼”"></a>ğŸ”¬ Part 2: BERT å‰ä¸¤å±‚å®Œæ•´æ¨æ¼”</h2><h3 id="è¾“å…¥å‡†å¤‡ï¼šMLM-ä»»åŠ¡"><a href="#è¾“å…¥å‡†å¤‡ï¼šMLM-ä»»åŠ¡" class="headerlink" title="è¾“å…¥å‡†å¤‡ï¼šMLM ä»»åŠ¡"></a>è¾“å…¥å‡†å¤‡ï¼šMLM ä»»åŠ¡</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">å¥å­: "æˆ‘ çˆ± [MASK] å¤©å®‰é—¨"</span><br><span class="line"></span><br><span class="line">Token IDs: [101, 2769, 4263, 103, 1921, 102]</span><br><span class="line">           [CLS]  æˆ‘    çˆ±   MASK  å¤©å®‰é—¨ [SEP]</span><br><span class="line"></span><br><span class="line">Embedding å: Xâ‚€ = [6, 768]</span><br><span class="line"></span><br><span class="line">ä½ç½®0 [CLS]:  [0.12, -0.05, 0.33, ..., 0.45]</span><br><span class="line">ä½ç½®1  æˆ‘:    [0.45, 0.23, -0.12, ..., 0.12]</span><br><span class="line">ä½ç½®2  çˆ±:    [0.33, 0.56, 0.78, ..., 0.89]</span><br><span class="line">ä½ç½®3 MASK:   [0.01, 0.02, 0.01, ..., 0.03]  â† å‡ ä¹æ˜¯ç©ºçš„ï¼</span><br><span class="line">ä½ç½®4 å¤©å®‰é—¨: [0.67, -0.34, 0.45, ..., 0.56]</span><br><span class="line">ä½ç½®5 [SEP]:  [0.78, 0.34, -0.12, ..., 0.12]</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="Layer-1ï¼šç¬¬ä¸€å±‚è¯¦ç»†è®¡ç®—"><a href="#Layer-1ï¼šç¬¬ä¸€å±‚è¯¦ç»†è®¡ç®—" class="headerlink" title="Layer 1ï¼šç¬¬ä¸€å±‚è¯¦ç»†è®¡ç®—"></a>Layer 1ï¼šç¬¬ä¸€å±‚è¯¦ç»†è®¡ç®—</h3><h4 id="Step-1-1-è®¡ç®—-Qã€Kã€V"><a href="#Step-1-1-è®¡ç®—-Qã€Kã€V" class="headerlink" title="Step 1.1: è®¡ç®— Qã€Kã€V"></a>Step 1.1: è®¡ç®— Qã€Kã€V</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Xâ‚€ = [<span class="number">6</span>, <span class="number">768</span>]  <span class="comment"># è¾“å…¥</span></span><br><span class="line"></span><br><span class="line">Qâ‚ = Xâ‚€ @ W_Q  <span class="comment"># [6, 768] @ [768, 768] = [6, 768]</span></span><br><span class="line">Kâ‚ = Xâ‚€ @ W_K  <span class="comment"># [6, 768] @ [768, 768] = [6, 768]</span></span><br><span class="line">Vâ‚ = Xâ‚€ @ W_V  <span class="comment"># [6, 768] @ [768, 768] = [6, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¯ä¸ªä½ç½®éƒ½æœ‰è‡ªå·±çš„ Qã€Kã€V å‘é‡</span></span><br><span class="line">Qâ‚ = [Q_cls, Q_æˆ‘, Q_çˆ±, Q_mask, Q_å¤©å®‰é—¨, Q_sep]</span><br><span class="line">Kâ‚ = [K_cls, K_æˆ‘, K_çˆ±, K_mask, K_å¤©å®‰é—¨, K_sep]</span><br><span class="line">Vâ‚ = [V_cls, V_æˆ‘, V_çˆ±, V_mask, V_å¤©å®‰é—¨, V_sep]</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-1-2-è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°"><a href="#Step-1-2-è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°" class="headerlink" title="Step 1.2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°"></a>Step 1.2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scores = Qâ‚ @ Kâ‚.T / sqrt(<span class="number">64</span>)  <span class="comment"># [6, 768] @ [768, 6] = [6, 6]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         [CLS]   æˆ‘    çˆ±   MASK  å¤©å®‰é—¨  [SEP]</span></span><br><span class="line"><span class="comment"># [CLS]  [ 2.1   1.3   1.5   0.2   1.8    1.2 ]</span></span><br><span class="line"><span class="comment">#   æˆ‘   [ 1.2   3.1   2.3   0.3   1.5    0.8 ]</span></span><br><span class="line"><span class="comment">#   çˆ±   [ 1.4   2.5   2.8   0.4   2.1    0.9 ]</span></span><br><span class="line"><span class="comment"># MASK   [ 0.8   1.9   2.4   0.1   2.6    0.7 ]  â† MASK è¡Œ</span></span><br><span class="line"><span class="comment"># å¤©å®‰é—¨ [ 1.6   1.4   2.0   0.3   3.2    1.1 ]</span></span><br><span class="line"><span class="comment"># [SEP]  [ 1.3   0.9   1.1   0.2   1.3    2.5 ]</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-1-3-Softmax-å½’ä¸€åŒ–"><a href="#Step-1-3-Softmax-å½’ä¸€åŒ–" class="headerlink" title="Step 1.3: Softmax å½’ä¸€åŒ–"></a>Step 1.3: Softmax å½’ä¸€åŒ–</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">weights = softmax(scores, dim=-<span class="number">1</span>)  <span class="comment"># æ¯è¡Œå’Œä¸º1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         [CLS]   æˆ‘     çˆ±    MASK  å¤©å®‰é—¨  [SEP]</span></span><br><span class="line"><span class="comment"># [CLS]  [0.18  0.12   0.15   0.03   0.40   0.12]</span></span><br><span class="line"><span class="comment">#   æˆ‘   [0.10  0.35   0.25   0.02   0.20   0.08]</span></span><br><span class="line"><span class="comment">#   çˆ±   [0.12  0.22   0.28   0.03   0.28   0.07]</span></span><br><span class="line"><span class="comment"># MASK   [0.08  0.18   0.28   0.01   0.38   0.07]  â† é‡ç‚¹çœ‹è¿™è¡Œï¼</span></span><br><span class="line"><span class="comment"># å¤©å®‰é—¨ [0.11  0.09   0.16   0.02   0.55   0.07]</span></span><br><span class="line"><span class="comment"># [SEP]  [0.15  0.10   0.12   0.03   0.15   0.45]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MASK ä½ç½®: 28% çœ‹"çˆ±"ï¼Œ38% çœ‹"å¤©å®‰é—¨"ï¼Œåªæœ‰ 1% çœ‹è‡ªå·±ï¼</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-1-4-åŠ æƒæ±‚å’Œå¾—åˆ°æ–°è¡¨ç¤º"><a href="#Step-1-4-åŠ æƒæ±‚å’Œå¾—åˆ°æ–°è¡¨ç¤º" class="headerlink" title="Step 1.4: åŠ æƒæ±‚å’Œå¾—åˆ°æ–°è¡¨ç¤º"></a>Step 1.4: åŠ æƒæ±‚å’Œå¾—åˆ°æ–°è¡¨ç¤º</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Hâ‚ = weights @ Vâ‚  <span class="comment"># [6, 6] @ [6, 768] = [6, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MASK ä½ç½®çš„æ–°å‘é‡:</span></span><br><span class="line">Hâ‚[<span class="number">3</span>] = <span class="number">0.08</span>Ã—V_cls + <span class="number">0.18</span>Ã—V_æˆ‘ + <span class="number">0.28</span>Ã—V_çˆ± + <span class="number">0.01</span>Ã—V_mask + <span class="number">0.38</span>Ã—V_å¤©å®‰é—¨ + <span class="number">0.07</span>Ã—V_sep</span><br><span class="line">        â†‘                        â†‘                          â†‘</span><br><span class="line">      å‡ ä¹å¿½ç•¥                ä¸»è¦æ¥è‡ª<span class="string">"çˆ±"</span>              ä¸»è¦æ¥è‡ª<span class="string">"å¤©å®‰é—¨"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ç»“æœ: MASK ä½ç½®ç°åœ¨èåˆäº† "çˆ±" å’Œ "å¤©å®‰é—¨" çš„ä¿¡æ¯ï¼</span></span><br><span class="line">Hâ‚[<span class="number">3</span>] = [<span class="number">0.45</span>, <span class="number">0.67</span>, <span class="number">0.23</span>, ..., <span class="number">0.78</span>]  â† ä¸å†æ˜¯ç©ºå£³äº†ï¼</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-1-5-Feed-Forward-Network-FFN"><a href="#Step-1-5-Feed-Forward-Network-FFN" class="headerlink" title="Step 1.5: Feed-Forward Network (FFN)"></a>Step 1.5: Feed-Forward Network (FFN)</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># è¾“å…¥</span></span><br><span class="line">Hâ‚ = [<span class="number">6</span>, <span class="number">768</span>]  <span class="comment"># Attention çš„è¾“å‡º</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># FFN: ä¸¤å±‚çº¿æ€§å˜æ¢</span></span><br><span class="line">step1 = Hâ‚ @ Wâ‚        <span class="comment"># [6, 768] @ [768, 3072] = [6, 3072]  å…ˆæ‰©å¤§4å€</span></span><br><span class="line">step2 = ReLU(step1)    <span class="comment"># [6, 3072]  æ¿€æ´»å‡½æ•°</span></span><br><span class="line">step3 = step2 @ Wâ‚‚     <span class="comment"># [6, 3072] @ [3072, 768] = [6, 768]  å†å‹å›å»</span></span><br><span class="line"></span><br><span class="line">FFN_out = step3        <span class="comment"># [6, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ç»´åº¦å˜åŒ–å›¾ç¤º</span></span><br><span class="line"><span class="number">768</span> â”€â”€æ‰©å¤§â”€â”€â†’ <span class="number">3072</span> â”€â”€å‹ç¼©â”€â”€â†’ <span class="number">768</span></span><br><span class="line">      Wâ‚           Wâ‚‚</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-1-6-æ®‹å·®è¿æ¥-LayerNorm"><a href="#Step-1-6-æ®‹å·®è¿æ¥-LayerNorm" class="headerlink" title="Step 1.6: æ®‹å·®è¿æ¥ + LayerNorm"></a>Step 1.6: æ®‹å·®è¿æ¥ + LayerNorm</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># æ®‹å·®è¿æ¥ï¼šæŠŠè¾“å…¥åŠ å›æ¥</span></span><br><span class="line">output = Hâ‚ + FFN_out  <span class="comment"># [6, 768] + [6, 768] = [6, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># LayerNorm å½’ä¸€åŒ–</span></span><br><span class="line">Xâ‚ = LayerNorm(output)  <span class="comment"># [6, 768]</span></span><br></pre></td></tr></tbody></table></figure>

<p><strong>æ®‹å·®è¿æ¥ç¤ºæ„å›¾ï¼š</strong></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Hâ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line"> â”‚                          â”‚</span><br><span class="line"> â†“                          â”‚ (è·³è·ƒè¿æ¥)</span><br><span class="line">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚</span><br><span class="line">â”‚   FFN   â”‚                 â”‚</span><br><span class="line">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚</span><br><span class="line"> â”‚                          â”‚</span><br><span class="line"> â†“                          â†“</span><br><span class="line">FFN_out â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ (+) ç›¸åŠ  â”€â”€â†’ LayerNorm â”€â”€â†’ Xâ‚</span><br><span class="line">                       â†‘</span><br><span class="line">              æ®‹å·® = Hâ‚ + FFN_out</span><br></pre></td></tr></tbody></table></figure>

<p><strong>ä¸ºä»€ä¹ˆéœ€è¦æ®‹å·®è¿æ¥ï¼Ÿ</strong></p>
<table>
<thead>
<tr>
<th>é—®é¢˜</th>
<th>æ®‹å·®è§£å†³æ–¹æ¡ˆ</th>
</tr>
</thead>
<tbody><tr>
<td>æ¢¯åº¦æ¶ˆå¤±</td>
<td>æ¢¯åº¦å¯ä»¥ç›´æ¥é€šè¿‡â€è·³è·ƒè¿æ¥â€å›ä¼ </td>
</tr>
<tr>
<td>ä¿¡æ¯ä¸¢å¤±</td>
<td>åŸå§‹ä¿¡æ¯ Hâ‚ è¢«ä¿ç•™ï¼Œä¸ä¼šå®Œå…¨è¢«è¦†ç›–</td>
</tr>
<tr>
<td>è®­ç»ƒå›°éš¾</td>
<td>ç½‘ç»œåªéœ€å­¦ä¹ â€å·®å¼‚â€ï¼Œæ›´å®¹æ˜“ä¼˜åŒ–</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># æœ¬è´¨ï¼šFFN åªå­¦ä¹ "å¢é‡"</span></span><br><span class="line">Xâ‚ = Hâ‚ + FFN(Hâ‚)</span><br><span class="line">   = Hâ‚ + Î”H     <span class="comment"># åŸå§‹ + ä¿®æ­£é‡</span></span><br></pre></td></tr></tbody></table></figure>

<p><strong>Layer 1 è¾“å‡ºï¼š</strong></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Xâ‚ = [6, 768]</span><br><span class="line"></span><br><span class="line">ä½ç½®3 (MASK): [0.52, 0.71, 0.34, ..., 0.82]</span><br><span class="line">              â†‘</span><br><span class="line">        å·²ç»åŒ…å«äº† "çˆ±___å¤©å®‰é—¨" çš„æ¨¡å¼ä¿¡æ¯</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="Layer-2ï¼šç¬¬äºŒå±‚è¯¦ç»†è®¡ç®—"><a href="#Layer-2ï¼šç¬¬äºŒå±‚è¯¦ç»†è®¡ç®—" class="headerlink" title="Layer 2ï¼šç¬¬äºŒå±‚è¯¦ç»†è®¡ç®—"></a>Layer 2ï¼šç¬¬äºŒå±‚è¯¦ç»†è®¡ç®—</h3><h4 id="Step-2-1-è®¡ç®—æ–°çš„-Qã€Kã€V"><a href="#Step-2-1-è®¡ç®—æ–°çš„-Qã€Kã€V" class="headerlink" title="Step 2.1: è®¡ç®—æ–°çš„ Qã€Kã€V"></a>Step 2.1: è®¡ç®—æ–°çš„ Qã€Kã€V</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># è¾“å…¥æ˜¯ Layer 1 çš„è¾“å‡º</span></span><br><span class="line">Xâ‚ = [<span class="number">6</span>, <span class="number">768</span>]</span><br><span class="line"></span><br><span class="line">Qâ‚‚ = Xâ‚ @ W_Q  <span class="comment"># æ–°çš„ Qï¼ˆæƒé‡çŸ©é˜µå’Œ Layer1 ä¸åŒï¼ï¼‰</span></span><br><span class="line">Kâ‚‚ = Xâ‚ @ W_K  <span class="comment"># æ–°çš„ K</span></span><br><span class="line">Vâ‚‚ = Xâ‚ @ W_V  <span class="comment"># æ–°çš„ V</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-2-2-è®¡ç®—æ³¨æ„åŠ›ï¼ˆåŸºäºæ›´æ–°åçš„è¡¨ç¤ºï¼‰"><a href="#Step-2-2-è®¡ç®—æ³¨æ„åŠ›ï¼ˆåŸºäºæ›´æ–°åçš„è¡¨ç¤ºï¼‰" class="headerlink" title="Step 2.2: è®¡ç®—æ³¨æ„åŠ›ï¼ˆåŸºäºæ›´æ–°åçš„è¡¨ç¤ºï¼‰"></a>Step 2.2: è®¡ç®—æ³¨æ„åŠ›ï¼ˆåŸºäºæ›´æ–°åçš„è¡¨ç¤ºï¼‰</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scores = Qâ‚‚ @ Kâ‚‚.T / sqrt(<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ç°åœ¨ MASK ä½ç½®å·²ç»æœ‰äº†ä¸Šä¸‹æ–‡ä¿¡æ¯</span></span><br><span class="line"><span class="comment"># å®ƒçš„ Q å‘é‡æ›´"èªæ˜"äº†ï¼Œèƒ½æ‰¾åˆ°æ›´ç›¸å…³çš„è¯</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         [CLS]   æˆ‘     çˆ±    MASK  å¤©å®‰é—¨  [SEP]</span></span><br><span class="line"><span class="comment"># MASK   [ 0.5   1.5   2.8    0.2   3.5    0.4 ]</span></span><br><span class="line"><span class="comment">#                       â†‘            â†‘</span></span><br><span class="line"><span class="comment">#                   æ›´å…³æ³¨"çˆ±"   æ›´å…³æ³¨"å¤©å®‰é—¨"</span></span><br><span class="line"></span><br><span class="line">weights[<span class="number">3</span>] = softmax([<span class="number">0.5</span>, <span class="number">1.5</span>, <span class="number">2.8</span>, <span class="number">0.2</span>, <span class="number">3.5</span>, <span class="number">0.4</span>])</span><br><span class="line">           = [<span class="number">0.04</span>, <span class="number">0.10</span>, <span class="number">0.28</span>, <span class="number">0.02</span>, <span class="number">0.52</span>, <span class="number">0.04</span>]</span><br><span class="line"><span class="comment">#                          â†‘           â†‘</span></span><br><span class="line"><span class="comment">#                      28%çœ‹çˆ±      52%çœ‹å¤©å®‰é—¨</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-2-3-åŠ æƒæ±‚å’Œ"><a href="#Step-2-3-åŠ æƒæ±‚å’Œ" class="headerlink" title="Step 2.3: åŠ æƒæ±‚å’Œ"></a>Step 2.3: åŠ æƒæ±‚å’Œ</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Hâ‚‚[<span class="number">3</span>] = <span class="number">0.04</span>Ã—V_cls + <span class="number">0.10</span>Ã—V_æˆ‘ + <span class="number">0.28</span>Ã—V_çˆ± + <span class="number">0.02</span>Ã—V_mask + <span class="number">0.52</span>Ã—V_å¤©å®‰é—¨ + <span class="number">0.04</span>Ã—V_sep</span><br><span class="line"></span><br><span class="line"><span class="comment"># è¿™æ¬¡ V_å¤©å®‰é—¨ å·²ç»ä¸æ˜¯åŸå§‹çš„äº†</span></span><br><span class="line"><span class="comment"># å®ƒåœ¨ Layer1 ä¸­ä¹Ÿèåˆäº†ä¸Šä¸‹æ–‡ï¼ŒçŸ¥é“"å¤©å®‰é—¨åœ¨åŒ—äº¬"</span></span><br><span class="line"><span class="comment"># æ‰€ä»¥ MASK é—´æ¥è·å¾—äº†"åŒ—äº¬"çš„ä¿¡æ¯ï¼</span></span><br><span class="line"></span><br><span class="line">Hâ‚‚[<span class="number">3</span>] = [<span class="number">0.68</span>, <span class="number">0.82</span>, <span class="number">0.45</span>, ..., <span class="number">0.91</span>]</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-2-4-FFN-æ®‹å·®-LayerNorm"><a href="#Step-2-4-FFN-æ®‹å·®-LayerNorm" class="headerlink" title="Step 2.4: FFN + æ®‹å·® + LayerNorm"></a>Step 2.4: FFN + æ®‹å·® + LayerNorm</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Xâ‚‚ = LayerNorm(Hâ‚‚ + FFN(Hâ‚‚))  <span class="comment"># [6, 768]</span></span><br></pre></td></tr></tbody></table></figure>

<p><strong>Layer 2 è¾“å‡ºï¼š</strong></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Xâ‚‚ = [6, 768]</span><br><span class="line"></span><br><span class="line">ä½ç½®3 (MASK): [0.71, 0.85, 0.52, ..., 0.93]</span><br><span class="line">              â†‘</span><br><span class="line">        ç°åœ¨çŸ¥é“: "æˆ‘çˆ±___å¤©å®‰é—¨" â†’ è¿™ä¸ªç©ºåº”è¯¥å¡«åœ°å</span><br><span class="line">                  å¤©å®‰é—¨ç›¸å…³ â†’ å¯èƒ½æ˜¯"åŒ—äº¬"</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="ä¸¤å±‚å¯¹æ¯”æ€»ç»“"><a href="#ä¸¤å±‚å¯¹æ¯”æ€»ç»“" class="headerlink" title="ä¸¤å±‚å¯¹æ¯”æ€»ç»“"></a>ä¸¤å±‚å¯¹æ¯”æ€»ç»“</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">â”‚  MASK ä½ç½®å‘é‡çš„å˜åŒ–                                        â”‚</span><br><span class="line">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span><br><span class="line">â”‚                                                            â”‚</span><br><span class="line">â”‚  Embedding: [0.01, 0.02, 0.01, ...]   â† ç©ºå£³ï¼Œæ— è¯­ä¹‰        â”‚</span><br><span class="line">â”‚      â†“                                                     â”‚</span><br><span class="line">â”‚  Layer 1:   [0.52, 0.71, 0.34, ...]   â† èåˆäº†"çˆ±""å¤©å®‰é—¨"  â”‚</span><br><span class="line">â”‚      â†“                                                     â”‚</span><br><span class="line">â”‚  Layer 2:   [0.71, 0.85, 0.52, ...]   â† æ›´æ·±å±‚ç†è§£          â”‚</span><br><span class="line">â”‚      â†“                                                     â”‚</span><br><span class="line">â”‚    ...      (ç»§ç»­ 10 å±‚)                                    â”‚</span><br><span class="line">â”‚      â†“                                                     â”‚</span><br><span class="line">â”‚  Layer 12:  [0.93, 0.87, 0.76, ...]   â† ç¡®å®šæ˜¯"åŒ—äº¬"        â”‚</span><br><span class="line">â”‚                                                            â”‚</span><br><span class="line">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br></pre></td></tr></tbody></table></figure>

<table>
<thead>
<tr>
<th>å±‚</th>
<th>MASK å­¦åˆ°äº†ä»€ä¹ˆ</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Embedding</strong></td>
<td>ç©ºçš„å ä½ç¬¦ï¼Œ103åªæ˜¯ä¸ªID</td>
</tr>
<tr>
<td><strong>Layer 1</strong></td>
<td>â€œçˆ±â€åé¢æ˜¯å®¾è¯­ï¼Œâ€å¤©å®‰é—¨â€æ˜¯åœ°æ ‡</td>
</tr>
<tr>
<td><strong>Layer 2</strong></td>
<td>â€œæˆ‘çˆ±Xå¤©å®‰é—¨â€æ˜¯å›ºå®šæ­é…ï¼ŒXæ˜¯åœ°å</td>
</tr>
<tr>
<td><strong>Layer 3-6</strong></td>
<td>å¤©å®‰é—¨åœ¨åŒ—äº¬ï¼Œè¿™æ˜¯å¸¸è¯†</td>
</tr>
<tr>
<td><strong>Layer 7-12</strong></td>
<td>ç¡®å®šç­”æ¡ˆæ˜¯â€åŒ—äº¬â€ï¼Œæ’é™¤å…¶ä»–å¯èƒ½</td>
</tr>
</tbody></table>
<hr>
<h2 id="ğŸ”-Part-3-Self-Attention-æ ¸å¿ƒæœºåˆ¶"><a href="#ğŸ”-Part-3-Self-Attention-æ ¸å¿ƒæœºåˆ¶" class="headerlink" title="ğŸ” Part 3: Self-Attention æ ¸å¿ƒæœºåˆ¶"></a>ğŸ” Part 3: Self-Attention æ ¸å¿ƒæœºåˆ¶</h2><h3 id="2-0-è¾“å…¥å‡†å¤‡"><a href="#2-0-è¾“å…¥å‡†å¤‡" class="headerlink" title="2.0 è¾“å…¥å‡†å¤‡"></a>2.0 è¾“å…¥å‡†å¤‡</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">å¥å­: "æˆ‘ çˆ± [MASK] å¤©å®‰é—¨"</span><br><span class="line"></span><br><span class="line">Token IDs: [101, 2769, 4263, 103, 1921, 102]</span><br><span class="line">           [CLS]  æˆ‘    çˆ±   MASK  å¤©å®‰é—¨ [SEP]</span><br><span class="line"></span><br><span class="line">Embedding å: Xâ‚€ = [6, 768]</span><br><span class="line"></span><br><span class="line">ä½ç½®0 [CLS]:  [0.12, -0.05, 0.33, ..., 0.45]  â† 768ç»´å‘é‡</span><br><span class="line">ä½ç½®1  æˆ‘:    [0.45, 0.23, -0.12, ..., 0.12]</span><br><span class="line">ä½ç½®2  çˆ±:    [0.33, 0.56, 0.78, ..., 0.89]</span><br><span class="line">ä½ç½®3 MASK:   [0.01, 0.02, 0.01, ..., 0.03]  â† å‡ ä¹æ˜¯ç©ºçš„ï¼</span><br><span class="line">ä½ç½®4 å¤©å®‰é—¨: [0.67, -0.34, 0.45, ..., 0.56]</span><br><span class="line">ä½ç½®5 [SEP]:  [0.78, 0.34, -0.12, ..., 0.12]</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="2-1-Layer-1ï¼šç¬¬ä¸€å±‚è¯¦ç»†è®¡ç®—"><a href="#2-1-Layer-1ï¼šç¬¬ä¸€å±‚è¯¦ç»†è®¡ç®—" class="headerlink" title="2.1 Layer 1ï¼šç¬¬ä¸€å±‚è¯¦ç»†è®¡ç®—"></a>2.1 Layer 1ï¼šç¬¬ä¸€å±‚è¯¦ç»†è®¡ç®—</h3><h4 id="Step-1-1-è®¡ç®—-Qã€Kã€V-1"><a href="#Step-1-1-è®¡ç®—-Qã€Kã€V-1" class="headerlink" title="Step 1.1: è®¡ç®— Qã€Kã€V"></a>Step 1.1: è®¡ç®— Qã€Kã€V</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Xâ‚€ = [<span class="number">6</span>, <span class="number">768</span>]  <span class="comment"># è¾“å…¥</span></span><br><span class="line"></span><br><span class="line">Qâ‚ = Xâ‚€ @ W_Q  <span class="comment"># [6, 768] @ [768, 768] = [6, 768]</span></span><br><span class="line">Kâ‚ = Xâ‚€ @ W_K  <span class="comment"># [6, 768] @ [768, 768] = [6, 768]</span></span><br><span class="line">Vâ‚ = Xâ‚€ @ W_V  <span class="comment"># [6, 768] @ [768, 768] = [6, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¯ä¸ªä½ç½®éƒ½æœ‰è‡ªå·±çš„ Qã€Kã€V å‘é‡</span></span><br><span class="line">Qâ‚ = [Q_cls, Q_æˆ‘, Q_çˆ±, Q_mask, Q_å¤©å®‰é—¨, Q_sep]</span><br><span class="line">Kâ‚ = [K_cls, K_æˆ‘, K_çˆ±, K_mask, K_å¤©å®‰é—¨, K_sep]</span><br><span class="line">Vâ‚ = [V_cls, V_æˆ‘, V_çˆ±, V_mask, V_å¤©å®‰é—¨, V_sep]</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-1-2-è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°-1"><a href="#Step-1-2-è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°-1" class="headerlink" title="Step 1.2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°"></a>Step 1.2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scores = Qâ‚ @ Kâ‚.T / sqrt(<span class="number">64</span>)  <span class="comment"># [6, 768] @ [768, 6] = [6, 6]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># æ³¨æ„åŠ›çŸ©é˜µï¼ˆæœªå½’ä¸€åŒ–ï¼‰</span></span><br><span class="line"><span class="comment">#         [CLS]   æˆ‘    çˆ±   MASK  å¤©å®‰é—¨  [SEP]</span></span><br><span class="line"><span class="comment"># [CLS]  [ 2.1   1.3   1.5   0.2   1.8    1.2 ]</span></span><br><span class="line"><span class="comment">#   æˆ‘   [ 1.2   3.1   2.3   0.3   1.5    0.8 ]</span></span><br><span class="line"><span class="comment">#   çˆ±   [ 1.4   2.5   2.8   0.4   2.1    0.9 ]</span></span><br><span class="line"><span class="comment"># MASK   [ 0.8   1.9   2.4   0.1   2.6    0.7 ]  â† MASK è¡Œ</span></span><br><span class="line"><span class="comment"># å¤©å®‰é—¨ [ 1.6   1.4   2.0   0.3   3.2    1.1 ]</span></span><br><span class="line"><span class="comment"># [SEP]  [ 1.3   0.9   1.1   0.2   1.3    2.5 ]</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-1-3-Softmax-å½’ä¸€åŒ–-1"><a href="#Step-1-3-Softmax-å½’ä¸€åŒ–-1" class="headerlink" title="Step 1.3: Softmax å½’ä¸€åŒ–"></a>Step 1.3: Softmax å½’ä¸€åŒ–</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">weights = softmax(scores, dim=-<span class="number">1</span>)  <span class="comment"># æ¯è¡Œå’Œä¸º1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># æ³¨æ„åŠ›æƒé‡çŸ©é˜µ</span></span><br><span class="line"><span class="comment">#         [CLS]   æˆ‘     çˆ±    MASK  å¤©å®‰é—¨  [SEP]</span></span><br><span class="line"><span class="comment"># [CLS]  [0.18  0.12   0.15   0.03   0.40   0.12]</span></span><br><span class="line"><span class="comment">#   æˆ‘   [0.10  0.35   0.25   0.02   0.20   0.08]</span></span><br><span class="line"><span class="comment">#   çˆ±   [0.12  0.22   0.28   0.03   0.28   0.07]</span></span><br><span class="line"><span class="comment"># MASK   [0.08  0.18   0.28   0.01   0.38   0.07]  â† é‡ç‚¹çœ‹è¿™è¡Œï¼</span></span><br><span class="line"><span class="comment"># å¤©å®‰é—¨ [0.11  0.09   0.16   0.02   0.55   0.07]</span></span><br><span class="line"><span class="comment"># [SEP]  [0.15  0.10   0.12   0.03   0.15   0.45]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MASK ä½ç½®è§£è¯»:</span></span><br><span class="line"><span class="comment"># - 28% çš„æ³¨æ„åŠ›ç»™ "çˆ±"</span></span><br><span class="line"><span class="comment"># - 38% çš„æ³¨æ„åŠ›ç»™ "å¤©å®‰é—¨"</span></span><br><span class="line"><span class="comment"># - åªæœ‰ 1% çœ‹è‡ªå·±ï¼ˆå› ä¸ºè‡ªå·±æ˜¯ç©ºå£³ï¼‰</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-1-4-åŠ æƒæ±‚å’Œå¾—åˆ°æ–°è¡¨ç¤º-1"><a href="#Step-1-4-åŠ æƒæ±‚å’Œå¾—åˆ°æ–°è¡¨ç¤º-1" class="headerlink" title="Step 1.4: åŠ æƒæ±‚å’Œå¾—åˆ°æ–°è¡¨ç¤º"></a>Step 1.4: åŠ æƒæ±‚å’Œå¾—åˆ°æ–°è¡¨ç¤º</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Hâ‚ = weights @ Vâ‚  <span class="comment"># [6, 6] @ [6, 768] = [6, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MASK ä½ç½®çš„æ–°å‘é‡è®¡ç®—:</span></span><br><span class="line">Hâ‚[<span class="number">3</span>] = <span class="number">0.08</span>Ã—V_cls + <span class="number">0.18</span>Ã—V_æˆ‘ + <span class="number">0.28</span>Ã—V_çˆ± + <span class="number">0.01</span>Ã—V_mask </span><br><span class="line">      + <span class="number">0.38</span>Ã—V_å¤©å®‰é—¨ + <span class="number">0.07</span>Ã—V_sep</span><br><span class="line"></span><br><span class="line"><span class="comment"># å…·ä½“æ•°å€¼ç¤ºä¾‹ï¼ˆå‡è®¾ç®€åŒ–åˆ°4ç»´ï¼‰:</span></span><br><span class="line">V_cls    = [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">0.3</span>]</span><br><span class="line">V_æˆ‘     = [<span class="number">0.8</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.4</span>]</span><br><span class="line">V_çˆ±     = [<span class="number">0.3</span>, <span class="number">0.7</span>, <span class="number">0.5</span>, <span class="number">0.2</span>]</span><br><span class="line">V_mask   = [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.1</span>]  <span class="comment"># å‡ ä¹ä¸ºç©º</span></span><br><span class="line">V_å¤©å®‰é—¨ = [<span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.8</span>, <span class="number">0.6</span>]</span><br><span class="line">V_sep    = [<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.2</span>]</span><br><span class="line"></span><br><span class="line">Hâ‚[<span class="number">3</span>] = <span class="number">0.08</span>Ã—[<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.1</span>,<span class="number">0.3</span>] + <span class="number">0.18</span>Ã—[<span class="number">0.8</span>,<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.4</span>]</span><br><span class="line">      + <span class="number">0.28</span>Ã—[<span class="number">0.3</span>,<span class="number">0.7</span>,<span class="number">0.5</span>,<span class="number">0.2</span>] + <span class="number">0.01</span>Ã—[<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.1</span>]</span><br><span class="line">      + <span class="number">0.38</span>Ã—[<span class="number">0.2</span>,<span class="number">0.4</span>,<span class="number">0.8</span>,<span class="number">0.6</span>] + <span class="number">0.07</span>Ã—[<span class="number">0.1</span>,<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.2</span>]</span><br><span class="line"></span><br><span class="line">      = [<span class="number">0.008</span>+<span class="number">0.144</span>+<span class="number">0.084</span>+<span class="number">0</span>+<span class="number">0.076</span>+<span class="number">0.007</span>,</span><br><span class="line">         <span class="number">0.016</span>+<span class="number">0.018</span>+<span class="number">0.196</span>+<span class="number">0</span>+<span class="number">0.152</span>+<span class="number">0.007</span>,</span><br><span class="line">         <span class="number">0.008</span>+<span class="number">0.036</span>+<span class="number">0.14</span>+<span class="number">0</span>+<span class="number">0.304</span>+<span class="number">0.014</span>,</span><br><span class="line">         <span class="number">0.024</span>+<span class="number">0.072</span>+<span class="number">0.056</span>+<span class="number">0.001</span>+<span class="number">0.228</span>+<span class="number">0.014</span>]</span><br><span class="line"></span><br><span class="line">      = [<span class="number">0.319</span>, <span class="number">0.389</span>, <span class="number">0.502</span>, <span class="number">0.395</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># ç»“æœ: MASK ä½ç½®ç°åœ¨èåˆäº† "çˆ±" å’Œ "å¤©å®‰é—¨" çš„ä¿¡æ¯ï¼</span></span><br><span class="line"><span class="comment"># ä¸å†æ˜¯ç©ºå£³ [0,0,0,0.1] äº†ï¼</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-1-5-Feed-Forward-Network-FFN-1"><a href="#Step-1-5-Feed-Forward-Network-FFN-1" class="headerlink" title="Step 1.5: Feed-Forward Network (FFN)"></a>Step 1.5: Feed-Forward Network (FFN)</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># FFN: 768 â†’ 3072 â†’ 768</span></span><br><span class="line">FFN_input = Hâ‚  <span class="comment"># [6, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ç¬¬ä¸€å±‚ï¼šå‡ç»´ + æ¿€æ´»</span></span><br><span class="line">hidden = FFN_input @ Wâ‚  <span class="comment"># [6, 768] @ [768, 3072] = [6, 3072]</span></span><br><span class="line">hidden = ReLU(hidden)    <span class="comment"># è´Ÿæ•°å˜0ï¼Œæ­£æ•°ä¸å˜</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ç¬¬äºŒå±‚ï¼šé™ç»´</span></span><br><span class="line">FFN_out = hidden @ Wâ‚‚    <span class="comment"># [6, 3072] @ [3072, 768] = [6, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MASK ä½ç½®ç¤ºä¾‹</span></span><br><span class="line">FFN_out[<span class="number">3</span>] = [<span class="number">0.07</span>, <span class="number">0.12</span>, <span class="number">0.09</span>, ..., <span class="number">0.15</span>]</span><br></pre></td></tr></tbody></table></figure>

<p><strong>ä¸ºä»€ä¹ˆè¦ FFNï¼Ÿ</strong></p>
<ul>
<li>Attention åªåšçº¿æ€§ç»„åˆ</li>
<li>FFN å¼•å…¥éçº¿æ€§ï¼Œè®©æ¨¡å‹å­¦ä¹ æ›´å¤æ‚çš„æ¨¡å¼</li>
</ul>
<h4 id="Step-1-6-æ®‹å·®è¿æ¥-LayerNorm-1"><a href="#Step-1-6-æ®‹å·®è¿æ¥-LayerNorm-1" class="headerlink" title="Step 1.6: æ®‹å·®è¿æ¥ + LayerNorm"></a>Step 1.6: æ®‹å·®è¿æ¥ + LayerNorm</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># æ®‹å·®è¿æ¥ï¼šæŠŠè¾“å…¥åŠ å›æ¥</span></span><br><span class="line">residual = Hâ‚ + FFN_out  <span class="comment"># [6, 768] + [6, 768] = [6, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MASK ä½ç½®</span></span><br><span class="line">residual[<span class="number">3</span>] = Hâ‚[<span class="number">3</span>] + FFN_out[<span class="number">3</span>]</span><br><span class="line">            = [<span class="number">0.319</span>, <span class="number">0.389</span>, <span class="number">0.502</span>, <span class="number">0.395</span>] + [<span class="number">0.07</span>, <span class="number">0.12</span>, <span class="number">0.09</span>, <span class="number">0.15</span>]</span><br><span class="line">            = [<span class="number">0.389</span>, <span class="number">0.509</span>, <span class="number">0.592</span>, <span class="number">0.545</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># LayerNorm: å½’ä¸€åŒ–</span></span><br><span class="line">Xâ‚ = LayerNorm(residual)  <span class="comment"># [6, 768]</span></span><br></pre></td></tr></tbody></table></figure>

<p><strong>æ®‹å·®è¿æ¥ç¤ºæ„å›¾ï¼š</strong></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">Hâ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line"> â”‚                          â”‚</span><br><span class="line"> â†“                          â”‚ (è·³è·ƒè¿æ¥ï¼Œé˜²æ­¢ä¿¡æ¯ä¸¢å¤±)</span><br><span class="line">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚</span><br><span class="line">â”‚   FFN   â”‚                 â”‚</span><br><span class="line">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚</span><br><span class="line"> â”‚                          â”‚</span><br><span class="line"> â†“                          â†“</span><br><span class="line">FFN_out â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ (+) ç›¸åŠ  â”€â”€â†’ LayerNorm â”€â”€â†’ Xâ‚</span><br></pre></td></tr></tbody></table></figure>

<p><strong>Layer 1 è¾“å‡ºï¼š</strong></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Xâ‚ = [6, 768]</span><br><span class="line"></span><br><span class="line">ä½ç½®3 (MASK): [0.52, 0.71, 0.34, ..., 0.82]</span><br><span class="line">              â†‘</span><br><span class="line">        å·²ç»åŒ…å«äº† "çˆ±___å¤©å®‰é—¨" çš„æ¨¡å¼ä¿¡æ¯</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="2-2-Layer-2ï¼šç¬¬äºŒå±‚è¯¦ç»†è®¡ç®—"><a href="#2-2-Layer-2ï¼šç¬¬äºŒå±‚è¯¦ç»†è®¡ç®—" class="headerlink" title="2.2 Layer 2ï¼šç¬¬äºŒå±‚è¯¦ç»†è®¡ç®—"></a>2.2 Layer 2ï¼šç¬¬äºŒå±‚è¯¦ç»†è®¡ç®—</h3><h4 id="Step-2-1-è®¡ç®—æ–°çš„-Qã€Kã€V-1"><a href="#Step-2-1-è®¡ç®—æ–°çš„-Qã€Kã€V-1" class="headerlink" title="Step 2.1: è®¡ç®—æ–°çš„ Qã€Kã€V"></a>Step 2.1: è®¡ç®—æ–°çš„ Qã€Kã€V</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># è¾“å…¥æ˜¯ Layer 1 çš„è¾“å‡ºï¼ˆå·²ç»èåˆè¿‡ä¸€æ¬¡ä¸Šä¸‹æ–‡ï¼‰</span></span><br><span class="line">Xâ‚ = [<span class="number">6</span>, <span class="number">768</span>]</span><br><span class="line"></span><br><span class="line">Qâ‚‚ = Xâ‚ @ W_Q  <span class="comment"># æ–°çš„æƒé‡çŸ©é˜µï¼ä¸ Layer1 ä¸åŒ</span></span><br><span class="line">Kâ‚‚ = Xâ‚ @ W_K</span><br><span class="line">Vâ‚‚ = Xâ‚ @ W_V</span><br><span class="line"></span><br><span class="line"><span class="comment"># ç°åœ¨ MASK ä½ç½®çš„ Q å·²ç»ä¸æ˜¯ç©ºå£³äº†</span></span><br><span class="line">Q_mask_new = Xâ‚[<span class="number">3</span>] @ W_Q  <span class="comment"># åŸºäºèåˆåçš„å‘é‡è®¡ç®—</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-2-2-è®¡ç®—æ³¨æ„åŠ›ï¼ˆåŸºäºæ›´æ–°åçš„è¡¨ç¤ºï¼‰-1"><a href="#Step-2-2-è®¡ç®—æ³¨æ„åŠ›ï¼ˆåŸºäºæ›´æ–°åçš„è¡¨ç¤ºï¼‰-1" class="headerlink" title="Step 2.2: è®¡ç®—æ³¨æ„åŠ›ï¼ˆåŸºäºæ›´æ–°åçš„è¡¨ç¤ºï¼‰"></a>Step 2.2: è®¡ç®—æ³¨æ„åŠ›ï¼ˆåŸºäºæ›´æ–°åçš„è¡¨ç¤ºï¼‰</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scores = Qâ‚‚ @ Kâ‚‚.T / sqrt(<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ç°åœ¨ MASK ä½ç½®å·²ç»æœ‰äº†ä¸Šä¸‹æ–‡ä¿¡æ¯</span></span><br><span class="line"><span class="comment"># å®ƒçš„ Q å‘é‡æ›´"èªæ˜"äº†ï¼Œèƒ½æ‰¾åˆ°æ›´ç›¸å…³çš„è¯</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         [CLS]   æˆ‘     çˆ±    MASK  å¤©å®‰é—¨  [SEP]</span></span><br><span class="line"><span class="comment"># MASK   [ 0.5   1.5   2.8    0.2   3.5    0.4 ]</span></span><br><span class="line"><span class="comment">#                       â†‘            â†‘</span></span><br><span class="line"><span class="comment">#                   æ›´å…³æ³¨"çˆ±"   æ›´å…³æ³¨"å¤©å®‰é—¨"</span></span><br><span class="line"></span><br><span class="line">weights[<span class="number">3</span>] = softmax([<span class="number">0.5</span>, <span class="number">1.5</span>, <span class="number">2.8</span>, <span class="number">0.2</span>, <span class="number">3.5</span>, <span class="number">0.4</span>])</span><br><span class="line">           = [<span class="number">0.04</span>, <span class="number">0.10</span>, <span class="number">0.28</span>, <span class="number">0.02</span>, <span class="number">0.52</span>, <span class="number">0.04</span>]</span><br><span class="line"><span class="comment">#                          â†‘           â†‘</span></span><br><span class="line"><span class="comment">#                      28%çœ‹çˆ±      52%çœ‹å¤©å®‰é—¨ï¼ˆæƒé‡æ›´é«˜äº†ï¼ï¼‰</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-2-3-åŠ æƒæ±‚å’Œ-1"><a href="#Step-2-3-åŠ æƒæ±‚å’Œ-1" class="headerlink" title="Step 2.3: åŠ æƒæ±‚å’Œ"></a>Step 2.3: åŠ æƒæ±‚å’Œ</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">Hâ‚‚[<span class="number">3</span>] = <span class="number">0.04</span>Ã—V_cls + <span class="number">0.10</span>Ã—V_æˆ‘ + <span class="number">0.28</span>Ã—V_çˆ± </span><br><span class="line">      + <span class="number">0.02</span>Ã—V_mask + <span class="number">0.52</span>Ã—V_å¤©å®‰é—¨ + <span class="number">0.04</span>Ã—V_sep</span><br><span class="line"></span><br><span class="line"><span class="comment"># å…³é”®ï¼šè¿™æ¬¡çš„ V_å¤©å®‰é—¨ å·²ç»ä¸æ˜¯åŸå§‹çš„äº†</span></span><br><span class="line"><span class="comment"># å®ƒåœ¨ Layer1 ä¸­ä¹Ÿèåˆäº†ä¸Šä¸‹æ–‡ï¼ŒçŸ¥é“"å¤©å®‰é—¨åœ¨åŒ—äº¬"</span></span><br><span class="line"><span class="comment"># æ‰€ä»¥ MASK é—´æ¥è·å¾—äº†"åŒ—äº¬"çš„ä¿¡æ¯ï¼</span></span><br><span class="line"></span><br><span class="line">Hâ‚‚[<span class="number">3</span>] = [<span class="number">0.68</span>, <span class="number">0.82</span>, <span class="number">0.45</span>, ..., <span class="number">0.91</span>]</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-2-4-FFN-æ®‹å·®-LayerNorm-1"><a href="#Step-2-4-FFN-æ®‹å·®-LayerNorm-1" class="headerlink" title="Step 2.4: FFN + æ®‹å·® + LayerNorm"></a>Step 2.4: FFN + æ®‹å·® + LayerNorm</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FFN_out = FFN(Hâ‚‚)</span><br><span class="line">Xâ‚‚ = LayerNorm(Hâ‚‚ + FFN_out)  <span class="comment"># [6, 768]</span></span><br></pre></td></tr></tbody></table></figure>

<p><strong>Layer 2 è¾“å‡ºï¼š</strong></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Xâ‚‚ = [6, 768]</span><br><span class="line"></span><br><span class="line">ä½ç½®3 (MASK): [0.71, 0.85, 0.52, ..., 0.93]</span><br><span class="line">              â†‘</span><br><span class="line">        ç°åœ¨çŸ¥é“: "æˆ‘çˆ±___å¤©å®‰é—¨" â†’ è¿™ä¸ªç©ºåº”è¯¥å¡«åœ°å</span><br><span class="line">                  å¤©å®‰é—¨ç›¸å…³ â†’ å¯èƒ½æ˜¯"åŒ—äº¬"</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="2-3-ä¸¤å±‚å¯¹æ¯”æ€»ç»“"><a href="#2-3-ä¸¤å±‚å¯¹æ¯”æ€»ç»“" class="headerlink" title="2.3 ä¸¤å±‚å¯¹æ¯”æ€»ç»“"></a>2.3 ä¸¤å±‚å¯¹æ¯”æ€»ç»“</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">â”‚  MASK ä½ç½®å‘é‡çš„æ¼”åŒ–                                        â”‚</span><br><span class="line">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span><br><span class="line">â”‚                                                            â”‚</span><br><span class="line">â”‚  Embedding: [0.01, 0.02, 0.01, ...]   â† ç©ºå£³ï¼Œæ— è¯­ä¹‰        â”‚</span><br><span class="line">â”‚      â†“ Layer 1 Self-Attention                             â”‚</span><br><span class="line">â”‚  Layer 1:   [0.52, 0.71, 0.34, ...]   â† èåˆäº†"çˆ±""å¤©å®‰é—¨"  â”‚</span><br><span class="line">â”‚      â†“ Layer 2 Self-Attention                             â”‚</span><br><span class="line">â”‚  Layer 2:   [0.71, 0.85, 0.52, ...]   â† æ›´æ·±å±‚ç†è§£          â”‚</span><br><span class="line">â”‚      â†“                                                     â”‚</span><br><span class="line">â”‚    ...      (ç»§ç»­ 10 å±‚)                                    â”‚</span><br><span class="line">â”‚      â†“                                                     â”‚</span><br><span class="line">â”‚  Layer 12:  [0.93, 0.87, 0.76, ...]   â† ç¡®å®šæ˜¯"åŒ—äº¬"        â”‚</span><br><span class="line">â”‚                                                            â”‚</span><br><span class="line">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br></pre></td></tr></tbody></table></figure>

<table>
<thead>
<tr>
<th>å±‚</th>
<th>MASK å­¦åˆ°äº†ä»€ä¹ˆ</th>
<th>æ³¨æ„åŠ›æƒé‡å˜åŒ–</th>
</tr>
</thead>
<tbody><tr>
<td><strong>Embedding</strong></td>
<td>ç©ºçš„å ä½ç¬¦</td>
<td>-</td>
</tr>
<tr>
<td><strong>Layer 1</strong></td>
<td>â€œçˆ±â€åé¢æ˜¯å®¾è¯­ï¼Œâ€å¤©å®‰é—¨â€æ˜¯åœ°æ ‡</td>
<td>28%çœ‹â€çˆ±â€ï¼Œ38%çœ‹â€å¤©å®‰é—¨â€</td>
</tr>
<tr>
<td><strong>Layer 2</strong></td>
<td>â€œæˆ‘çˆ±Xå¤©å®‰é—¨â€æ˜¯å›ºå®šæ­é…ï¼ŒXæ˜¯åœ°å</td>
<td>28%çœ‹â€çˆ±â€ï¼Œ52%çœ‹â€å¤©å®‰é—¨â€</td>
</tr>
<tr>
<td><strong>Layer 3-6</strong></td>
<td>å¤©å®‰é—¨åœ¨åŒ—äº¬ï¼Œè¿™æ˜¯å¸¸è¯†</td>
<td>é€æ¸èšç„¦åˆ°ç›¸å…³è¯</td>
</tr>
<tr>
<td><strong>Layer 7-12</strong></td>
<td>ç¡®å®šç­”æ¡ˆæ˜¯â€åŒ—äº¬â€ï¼Œæ’é™¤å…¶ä»–å¯èƒ½</td>
<td>é«˜åº¦ç¡®ä¿¡</td>
</tr>
</tbody></table>
<hr>
<h2 id="ğŸ”-Part-3-Self-Attention-æ ¸å¿ƒæœºåˆ¶-1"><a href="#ğŸ”-Part-3-Self-Attention-æ ¸å¿ƒæœºåˆ¶-1" class="headerlink" title="ğŸ” Part 3: Self-Attention æ ¸å¿ƒæœºåˆ¶"></a>ğŸ” Part 3: Self-Attention æ ¸å¿ƒæœºåˆ¶</h2><h3 id="3-1-Q-x2F-K-x2F-V-çš„æœ¬è´¨ï¼šä¿¡æ¯æ£€ç´¢ç³»ç»Ÿ"><a href="#3-1-Q-x2F-K-x2F-V-çš„æœ¬è´¨ï¼šä¿¡æ¯æ£€ç´¢ç³»ç»Ÿ" class="headerlink" title="3.1 Q/K/V çš„æœ¬è´¨ï¼šä¿¡æ¯æ£€ç´¢ç³»ç»Ÿ"></a>3.1 Q/K/V çš„æœ¬è´¨ï¼šä¿¡æ¯æ£€ç´¢ç³»ç»Ÿ</h3><h4 id="ç›´è§‰ç±»æ¯”"><a href="#ç›´è§‰ç±»æ¯”" class="headerlink" title="ç›´è§‰ç±»æ¯”"></a>ç›´è§‰ç±»æ¯”</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">åœºæ™¯: åœ¨å›¾ä¹¦é¦†æ‰¾ä¹¦</span><br><span class="line"></span><br><span class="line">Q (Query):  "æˆ‘æƒ³æ‰¾å…³äºæ·±åº¦å­¦ä¹ çš„ä¹¦"</span><br><span class="line">            â†“ è®¡ç®—ç›¸ä¼¼åº¦</span><br><span class="line">K (Key):    æ¯æœ¬ä¹¦çš„æ ‡ç­¾ ["æœºå™¨å­¦ä¹ ", "çƒ¹é¥ª", "å†å²", ...]</span><br><span class="line">            â†“ Softmax å½’ä¸€åŒ–</span><br><span class="line">Attention:  [0.7, 0.05, 0.05, ...] (å¯¹"æœºå™¨å­¦ä¹ "ä¹¦çš„æ³¨æ„åŠ›æœ€é«˜)</span><br><span class="line">            â†“ åŠ æƒæ±‚å’Œ</span><br><span class="line">V (Value):  ä¹¦çš„å®é™…å†…å®¹</span><br><span class="line">            â†“</span><br><span class="line">Output:     æ ¹æ®æ³¨æ„åŠ›æƒé‡ï¼Œèåˆæœ€ç›¸å…³ä¹¦ç±çš„çŸ¥è¯†</span><br></pre></td></tr></tbody></table></figure>

<h4 id="æ•°å­¦å®šä¹‰"><a href="#æ•°å­¦å®šä¹‰" class="headerlink" title="æ•°å­¦å®šä¹‰"></a>æ•°å­¦å®šä¹‰</h4><p>$$<br>\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V<br>$$</p>
<hr>
<h3 id="2-2-å®Œæ•´æ•°æ®æµä¸ç»´åº¦å˜åŒ–"><a href="#2-2-å®Œæ•´æ•°æ®æµä¸ç»´åº¦å˜åŒ–" class="headerlink" title="2.2 å®Œæ•´æ•°æ®æµä¸ç»´åº¦å˜åŒ–"></a>2.2 å®Œæ•´æ•°æ®æµä¸ç»´åº¦å˜åŒ–</h3><p><strong>å‡è®¾</strong>: <code>batch=2, seq=4, d_model=768, heads=12, d_k=64</code></p>
<h4 id="Step-1-çº¿æ€§æŠ•å½±"><a href="#Step-1-çº¿æ€§æŠ•å½±" class="headerlink" title="Step 1: çº¿æ€§æŠ•å½±"></a>Step 1: çº¿æ€§æŠ•å½±</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X: [<span class="number">2</span>, <span class="number">4</span>, <span class="number">768</span>]  <span class="comment"># è¾“å…¥</span></span><br><span class="line"></span><br><span class="line">Q = X @ W_Q  <span class="comment"># [2, 4, 768] @ [768, 768] = [2, 4, 768]</span></span><br><span class="line">K = X @ W_K  <span class="comment"># [2, 4, 768]</span></span><br><span class="line">V = X @ W_V  <span class="comment"># [2, 4, 768]</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-2-æ‹†åˆ†æˆå¤šå¤´"><a href="#Step-2-æ‹†åˆ†æˆå¤šå¤´" class="headerlink" title="Step 2: æ‹†åˆ†æˆå¤šå¤´"></a>Step 2: æ‹†åˆ†æˆå¤šå¤´</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Q: [<span class="number">2</span>, <span class="number">4</span>, <span class="number">768</span>] â†’ reshape â†’ [<span class="number">2</span>, <span class="number">4</span>, <span class="number">12</span>, <span class="number">64</span>] â†’ transpose â†’ [<span class="number">2</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">64</span>]</span><br><span class="line">K: [<span class="number">2</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">64</span>]</span><br><span class="line">V: [<span class="number">2</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">64</span>]</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-3-è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°"><a href="#Step-3-è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°" class="headerlink" title="Step 3: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°"></a>Step 3: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Scores = Q @ K^T</span><br><span class="line">       = [<span class="number">2</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">64</span>] @ [<span class="number">2</span>, <span class="number">12</span>, <span class="number">64</span>, <span class="number">4</span>]</span><br><span class="line">       = [<span class="number">2</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">4</span>]  â† è¿™å°±æ˜¯æ³¨æ„åŠ›çŸ©é˜µï¼</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¯ä¸ª token å¯¹å…¶ä»– token çš„å…³æ³¨ç¨‹åº¦</span></span><br><span class="line">ä¾‹å¦‚ Scores[<span class="number">0</span>, <span class="number">0</span>, :, :] =</span><br><span class="line">       I    love   NLP    !</span><br><span class="line">  I   [<span class="number">9.2</span>   <span class="number">1.3</span>   <span class="number">2.1</span>  <span class="number">0.8</span>]</span><br><span class="line"> love [<span class="number">2.4</span>   <span class="number">8.7</span>   <span class="number">3.2</span>  <span class="number">1.1</span>]</span><br><span class="line"> NLP  [<span class="number">1.8</span>   <span class="number">3.5</span>   <span class="number">9.1</span>  <span class="number">0.9</span>]</span><br><span class="line">  !   [<span class="number">0.5</span>   <span class="number">1.2</span>   <span class="number">0.7</span>  <span class="number">8.9</span>]</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-4-Scale-å’Œ-Softmax"><a href="#Step-4-Scale-å’Œ-Softmax" class="headerlink" title="Step 4: Scale å’Œ Softmax"></a>Step 4: Scale å’Œ Softmax</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Scores = Scores / sqrt(<span class="number">64</span>) â‰ˆ Scores / <span class="number">8</span></span><br><span class="line"></span><br><span class="line">Weights = softmax(Scores, dim=-<span class="number">1</span>)  <span class="comment"># [2, 12, 4, 4]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¯è¡Œå’Œä¸º 1</span></span><br><span class="line">ä¾‹å¦‚ Weights[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, :] = [<span class="number">0.15</span>, <span class="number">0.52</span>, <span class="number">0.28</span>, <span class="number">0.05</span>]</span><br><span class="line">è¡¨ç¤º <span class="string">"love"</span> å¯¹ [<span class="string">"I"</span>, <span class="string">"love"</span>, <span class="string">"NLP"</span>, <span class="string">"!"</span>] çš„æ³¨æ„åŠ›åˆ†å¸ƒ</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-5-åŠ æƒæ±‚å’Œ"><a href="#Step-5-åŠ æƒæ±‚å’Œ" class="headerlink" title="Step 5: åŠ æƒæ±‚å’Œ"></a>Step 5: åŠ æƒæ±‚å’Œ</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Output = Weights @ V</span><br><span class="line">       = [<span class="number">2</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">4</span>] @ [<span class="number">2</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">64</span>]</span><br><span class="line">       = [<span class="number">2</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">64</span>]</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Step-6-åˆå¹¶å¤šå¤´"><a href="#Step-6-åˆå¹¶å¤šå¤´" class="headerlink" title="Step 6: åˆå¹¶å¤šå¤´"></a>Step 6: åˆå¹¶å¤šå¤´</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Output: [<span class="number">2</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">64</span>] â†’ transpose â†’ [<span class="number">2</span>, <span class="number">4</span>, <span class="number">12</span>, <span class="number">64</span>] â†’ reshape â†’ [<span class="number">2</span>, <span class="number">4</span>, <span class="number">768</span>]</span><br></pre></td></tr></tbody></table></figure>

<p><strong>å…³é”®</strong>: è¾“å…¥å’Œè¾“å‡ºç»´åº¦å®Œå…¨ç›¸åŒï¼<code>[2, 4, 768]</code></p>
<hr>
<h3 id="2-3-ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´æ³¨æ„åŠ›ï¼Ÿ"><a href="#2-3-ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´æ³¨æ„åŠ›ï¼Ÿ" class="headerlink" title="2.3 ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´æ³¨æ„åŠ›ï¼Ÿ"></a>2.3 ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´æ³¨æ„åŠ›ï¼Ÿ</h3><h4 id="å•å¤´çš„å±€é™"><a href="#å•å¤´çš„å±€é™" class="headerlink" title="å•å¤´çš„å±€é™"></a>å•å¤´çš„å±€é™</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">å•å¤´: åªæœ‰ä¸€ç»„ Q/K/V</span><br><span class="line">     åªèƒ½å­¦ä¹ ä¸€ç§"æŸ¥è¯¢-åŒ¹é…"æ¨¡å¼</span><br></pre></td></tr></tbody></table></figure>

<h4 id="å¤šå¤´çš„ä¼˜åŠ¿"><a href="#å¤šå¤´çš„ä¼˜åŠ¿" class="headerlink" title="å¤šå¤´çš„ä¼˜åŠ¿"></a>å¤šå¤´çš„ä¼˜åŠ¿</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">12 ä¸ªå¤´ = 12 ç§ä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼</span><br><span class="line"></span><br><span class="line">Head 1: å…³æ³¨è¯­æ³•å…³ç³» (ä¸»è°“å®¾)</span><br><span class="line">Head 2: å…³æ³¨è¯­ä¹‰ç›¸ä¼¼</span><br><span class="line">Head 3: å…³æ³¨ä½ç½®é‚»è¿‘</span><br><span class="line">...</span><br><span class="line">Head 12: å…³æ³¨é•¿è·ç¦»ä¾èµ–</span><br><span class="line"></span><br><span class="line">æœ€ç»ˆèåˆ 12 ä¸ªå¤´çš„ä¿¡æ¯ â†’ æ›´ä¸°å¯Œçš„è¡¨ç¤º</span><br></pre></td></tr></tbody></table></figure>

<hr>
<hr>
<h2 id="âš¡-Part-4-KV-Cache-æ·±åº¦è§£æ"><a href="#âš¡-Part-4-KV-Cache-æ·±åº¦è§£æ" class="headerlink" title="âš¡ Part 4: KV Cache æ·±åº¦è§£æ"></a>âš¡ Part 4: KV Cache æ·±åº¦è§£æ</h2><h3 id="4-1-BERT-æœ‰-Q-x2F-K-x2F-V-å—ï¼Ÿæœ‰-KV-Cache-å—ï¼Ÿ"><a href="#4-1-BERT-æœ‰-Q-x2F-K-x2F-V-å—ï¼Ÿæœ‰-KV-Cache-å—ï¼Ÿ" class="headerlink" title="4.1 BERT æœ‰ Q/K/V å—ï¼Ÿæœ‰ KV Cache å—ï¼Ÿ"></a>4.1 BERT æœ‰ Q/K/V å—ï¼Ÿæœ‰ KV Cache å—ï¼Ÿ</h3><table>
<thead>
<tr>
<th>é—®é¢˜</th>
<th>ç­”æ¡ˆ</th>
<th>åŸå› </th>
</tr>
</thead>
<tbody><tr>
<td>BERT æœ‰ Qã€Kã€V å—ï¼Ÿ</td>
<td>âœ… <strong>æœ‰</strong></td>
<td>æ¯å±‚ Self-Attention éƒ½è¦è®¡ç®—</td>
</tr>
<tr>
<td>BERT æœ‰ KV Cache å—ï¼Ÿ</td>
<td>âŒ <strong>æ²¡æœ‰</strong></td>
<td>ä¸€æ¬¡æ€§å¤„ç†ï¼Œä¸éœ€è¦ç¼“å­˜</td>
</tr>
<tr>
<td>GPT æœ‰ KV Cache å—ï¼Ÿ</td>
<td>âœ… <strong>å¿…é¡»æœ‰</strong></td>
<td>é€ä¸ªç”Ÿæˆï¼Œå¿…é¡»ç¼“å­˜å†å²</td>
</tr>
</tbody></table>
<hr>
<h3 id="4-2-BERT-çš„-Q-x2F-K-x2F-V-è®¡ç®—"><a href="#4-2-BERT-çš„-Q-x2F-K-x2F-V-è®¡ç®—" class="headerlink" title="4.2 BERT çš„ Q/K/V è®¡ç®—"></a>4.2 BERT çš„ Q/K/V è®¡ç®—</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BERT æ¯ä¸€å±‚éƒ½è®¡ç®— Q/K/V</span></span><br><span class="line"><span class="comment"># è¾“å…¥: "æˆ‘ çˆ± [MASK] å¤©å®‰é—¨"</span></span><br><span class="line"><span class="comment"># X = [5, 768]  (5ä¸ªtokenï¼Œæ¯ä¸ª768ç»´)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¯å±‚éƒ½è¦è®¡ç®— Qã€Kã€V</span></span><br><span class="line">Q = X @ W_Q  <span class="comment"># [5, 768] @ [768, 768] = [5, 768]</span></span><br><span class="line">K = X @ W_K  <span class="comment"># [5, 768] @ [768, 768] = [5, 768]</span></span><br><span class="line">V = X @ W_V  <span class="comment"># [5, 768] @ [768, 768] = [5, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># è®¡ç®—æ³¨æ„åŠ›</span></span><br><span class="line">scores = Q @ K.T  <span class="comment"># [5, 768] @ [768, 5] = [5, 5]</span></span><br><span class="line">weights = softmax(scores)  <span class="comment"># [5, 5]</span></span><br><span class="line">output = weights @ V  <span class="comment"># [5, 5] @ [5, 768] = [5, 768]</span></span><br></pre></td></tr></tbody></table></figure>

<p><strong>æ³¨æ„åŠ›çŸ©é˜µç¤ºä¾‹ï¼š</strong></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">        æˆ‘     çˆ±    [MASK]  å¤©å®‰é—¨</span><br><span class="line">æˆ‘     [0.3   0.2    0.2    0.3 ]</span><br><span class="line">çˆ±     [0.2   0.3    0.2    0.3 ]     â† æ¯ä¸ªè¯çœ‹æ‰€æœ‰è¯</span><br><span class="line">[MASK] [0.15  0.25   0.05   0.55]     â† åŒå‘æ³¨æ„åŠ›ï¼</span><br><span class="line">å¤©å®‰é—¨ [0.2   0.2    0.2    0.4 ]</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="4-3-ä¸ºä»€ä¹ˆ-BERT-ä¸éœ€è¦-KV-Cacheï¼Ÿ"><a href="#4-3-ä¸ºä»€ä¹ˆ-BERT-ä¸éœ€è¦-KV-Cacheï¼Ÿ" class="headerlink" title="4.3 ä¸ºä»€ä¹ˆ BERT ä¸éœ€è¦ KV Cacheï¼Ÿ"></a>4.3 ä¸ºä»€ä¹ˆ BERT ä¸éœ€è¦ KV Cacheï¼Ÿ</h3><h4 id="å…³é”®åŒºåˆ«ï¼šä¸€æ¬¡æ€§å¤„ç†-vs-é€ä¸ªç”Ÿæˆ"><a href="#å…³é”®åŒºåˆ«ï¼šä¸€æ¬¡æ€§å¤„ç†-vs-é€ä¸ªç”Ÿæˆ" class="headerlink" title="å…³é”®åŒºåˆ«ï¼šä¸€æ¬¡æ€§å¤„ç† vs é€ä¸ªç”Ÿæˆ"></a>å…³é”®åŒºåˆ«ï¼šä¸€æ¬¡æ€§å¤„ç† vs é€ä¸ªç”Ÿæˆ</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">â”‚  BERTï¼ˆEncoder - ç†è§£ä»»åŠ¡ï¼‰ï¼šä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªå¥å­              â”‚</span><br><span class="line">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â”‚  è¾“å…¥: "æˆ‘ çˆ± [MASK] å¤©å®‰é—¨"   ï¼ˆä¸€æ¬¡æ€§å…¨éƒ¨è¾“å…¥ï¼‰              â”‚</span><br><span class="line">â”‚         â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“                                   â”‚</span><br><span class="line">â”‚        åŒæ—¶è®¡ç®—æ‰€æœ‰ä½ç½®çš„ Qã€Kã€V                             â”‚</span><br><span class="line">â”‚         â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“                                   â”‚</span><br><span class="line">â”‚  è¾“å‡º: åŒæ—¶å¾—åˆ°æ‰€æœ‰ä½ç½®çš„è¡¨ç¤º                                 â”‚</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â”‚  âœ… åªéœ€è¦ 1 æ¬¡å‰å‘ä¼ æ’­                                      â”‚</span><br><span class="line">â”‚  âŒ ä¸éœ€è¦ç¼“å­˜ä»»ä½•ä¸œè¥¿ï¼                                     â”‚</span><br><span class="line">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br><span class="line"></span><br><span class="line">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">â”‚  GPTï¼ˆDecoder - ç”Ÿæˆä»»åŠ¡ï¼‰ï¼šé€ä¸ª token ç”Ÿæˆ                  â”‚</span><br><span class="line">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â”‚  Step 1: è¾“å…¥ "æˆ‘"                                          â”‚</span><br><span class="line">â”‚          è®¡ç®— Kâ‚, Vâ‚ â†’ ğŸ’¾ éœ€è¦ä¿å­˜ï¼                         â”‚</span><br><span class="line">â”‚          è¾“å‡º "çˆ±"                                          â”‚</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â”‚  Step 2: è¾“å…¥ "çˆ±"                                          â”‚</span><br><span class="line">â”‚          è®¡ç®— Kâ‚‚, Vâ‚‚ â†’ ğŸ’¾ éœ€è¦ä¿å­˜ï¼                         â”‚</span><br><span class="line">â”‚          éœ€è¦ç”¨åˆ°ä¹‹å‰çš„ Kâ‚, Vâ‚                               â”‚</span><br><span class="line">â”‚          è¾“å‡º "åŒ—äº¬"                                         â”‚</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â”‚  Step 3: è¾“å…¥ "åŒ—äº¬"                                         â”‚</span><br><span class="line">â”‚          è®¡ç®— Kâ‚ƒ, Vâ‚ƒ â†’ ğŸ’¾ éœ€è¦ä¿å­˜ï¼                         â”‚</span><br><span class="line">â”‚          éœ€è¦ç”¨åˆ°ä¹‹å‰çš„ Kâ‚, Kâ‚‚, Vâ‚, Vâ‚‚                       â”‚</span><br><span class="line">â”‚          è¾“å‡º "å¤©å®‰é—¨"                                       â”‚</span><br><span class="line">â”‚                                                             â”‚</span><br><span class="line">â”‚  âœ… æ¯ä¸€æ­¥éƒ½éœ€è¦ä¹‹å‰æ‰€æœ‰çš„ Kã€V â†’ å¿…é¡»ç¼“å­˜ï¼                  â”‚</span><br><span class="line">â”‚  è¿™å°±æ˜¯ KV Cache çš„ä½œç”¨ï¼                                    â”‚</span><br><span class="line">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="4-4-ä»£ç å¯¹æ¯”ï¼šBERT-vs-GPT"><a href="#4-4-ä»£ç å¯¹æ¯”ï¼šBERT-vs-GPT" class="headerlink" title="4.4 ä»£ç å¯¹æ¯”ï¼šBERT vs GPT"></a>4.4 ä»£ç å¯¹æ¯”ï¼šBERT vs GPT</h3><h4 id="BERTï¼šæ— éœ€ç¼“å­˜"><a href="#BERTï¼šæ— éœ€ç¼“å­˜" class="headerlink" title="BERTï¼šæ— éœ€ç¼“å­˜"></a>BERTï¼šæ— éœ€ç¼“å­˜</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BERT: ä¸€æ¬¡æå®šï¼Œä¸éœ€è¦ç¼“å­˜</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bert_forward</span>(<span class="params">input_ids</span>):</span><br><span class="line">    <span class="comment"># input_ids = [101, 2769, 4263, 103, 1921, 102]  ä¸€æ¬¡æ€§è¾“å…¥</span></span><br><span class="line">    </span><br><span class="line">    X = embedding(input_ids)  <span class="comment"># [6, 768]</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> encoder_layers:</span><br><span class="line">        Q = X @ W_Q  <span class="comment"># åŒæ—¶ç®—æ‰€æœ‰ä½ç½®</span></span><br><span class="line">        K = X @ W_K  <span class="comment"># åŒæ—¶ç®—æ‰€æœ‰ä½ç½®</span></span><br><span class="line">        V = X @ W_V  <span class="comment"># åŒæ—¶ç®—æ‰€æœ‰ä½ç½®</span></span><br><span class="line">        </span><br><span class="line">        scores = Q @ K.T / sqrt(d_k)</span><br><span class="line">        weights = softmax(scores)</span><br><span class="line">        X = weights @ V  <span class="comment"># ä¸€æ¬¡å®Œæˆ</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X  <span class="comment"># ç›´æ¥è¿”å›ï¼Œä¸éœ€è¦ä¿å­˜ä»»ä½•ä¸­é—´ç»“æœ</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="GPTï¼šå¿…é¡»ç¼“å­˜"><a href="#GPTï¼šå¿…é¡»ç¼“å­˜" class="headerlink" title="GPTï¼šå¿…é¡»ç¼“å­˜"></a>GPTï¼šå¿…é¡»ç¼“å­˜</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GPT: é€ä¸ªç”Ÿæˆï¼Œå¿…é¡»ç¼“å­˜ Kã€V</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gpt_generate</span>(<span class="params">prompt</span>):</span><br><span class="line">    kv_cache = []  <span class="comment"># å¿…é¡»æœ‰è¿™ä¸ªï¼</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(max_tokens):</span><br><span class="line">        <span class="keyword">if</span> step == <span class="number">0</span>:</span><br><span class="line">            X = embedding(prompt)  <span class="comment"># ç¬¬ä¸€æ­¥å¤„ç†æ•´ä¸ª prompt</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X = embedding(new_token)  <span class="comment"># ä¹‹åæ¯æ­¥åªå¤„ç†æ–° token</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> layer_idx, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(decoder_layers):</span><br><span class="line">            Q = X @ W_Q  <span class="comment"># åªç®—å½“å‰ token çš„ Q</span></span><br><span class="line">            K_new = X @ W_K</span><br><span class="line">            V_new = X @ W_V</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># æ‹¼æ¥å†å²çš„ Kã€V</span></span><br><span class="line">            <span class="keyword">if</span> step &gt; <span class="number">0</span>:</span><br><span class="line">                K = concat(kv_cache[layer_idx][<span class="number">0</span>], K_new)  <span class="comment"># æ‹¼æ¥ï¼</span></span><br><span class="line">                V = concat(kv_cache[layer_idx][<span class="number">1</span>], V_new)  <span class="comment"># æ‹¼æ¥ï¼</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                K, V = K_new, V_new</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># æ›´æ–°ç¼“å­˜</span></span><br><span class="line">            <span class="keyword">if</span> layer_idx &gt;= <span class="built_in">len</span>(kv_cache):</span><br><span class="line">                kv_cache.append((K, V))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                kv_cache[layer_idx] = (K, V)  <span class="comment"># ä¿å­˜ï¼</span></span><br><span class="line">            </span><br><span class="line">            scores = Q @ K.T / sqrt(d_k)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Causal Mask: åªèƒ½çœ‹å·¦è¾¹</span></span><br><span class="line">            mask = causal_mask(Q.size(<span class="number">1</span>), K.size(<span class="number">1</span>))</span><br><span class="line">            scores = scores.masked_fill(mask, -inf)</span><br><span class="line">            </span><br><span class="line">            weights = softmax(scores)</span><br><span class="line">            X = weights @ V</span><br><span class="line">        </span><br><span class="line">        new_token = predict_next(X)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> generated_text</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="4-5-ä¸ºä»€ä¹ˆ-GPT-å¿…é¡»ç¼“å­˜-Kã€Vï¼Ÿ"><a href="#4-5-ä¸ºä»€ä¹ˆ-GPT-å¿…é¡»ç¼“å­˜-Kã€Vï¼Ÿ" class="headerlink" title="4.5 ä¸ºä»€ä¹ˆ GPT å¿…é¡»ç¼“å­˜ Kã€Vï¼Ÿ"></a>4.5 ä¸ºä»€ä¹ˆ GPT å¿…é¡»ç¼“å­˜ Kã€Vï¼Ÿ</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">ç”Ÿæˆ "æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨" çš„è¿‡ç¨‹ï¼š</span><br><span class="line"></span><br><span class="line">Step 1: è¾“å…¥ "æˆ‘"</span><br><span class="line">        Qâ‚ @ Kâ‚.T â†’ åªæœ‰è‡ªå·±çœ‹è‡ªå·±</span><br><span class="line">        è¾“å‡º: "çˆ±"</span><br><span class="line">        ğŸ’¾ ç¼“å­˜: Kâ‚, Vâ‚</span><br><span class="line"></span><br><span class="line">Step 2: è¾“å…¥ "çˆ±"  </span><br><span class="line">        éœ€è¦è®¡ç®—: Qâ‚‚ @ [Kâ‚, Kâ‚‚].T  â† éœ€è¦ä¹‹å‰çš„ Kâ‚ï¼</span><br><span class="line">        å¦‚æœä¸ç¼“å­˜ Kâ‚ï¼Œå°±è¦é‡æ–°ç®— â†’ æµªè´¹ï¼</span><br><span class="line">        è¾“å‡º: "åŒ—äº¬"</span><br><span class="line">        ğŸ’¾ ç¼“å­˜: Kâ‚, Kâ‚‚, Vâ‚, Vâ‚‚</span><br><span class="line">        </span><br><span class="line">Step 3: è¾“å…¥ "åŒ—äº¬"</span><br><span class="line">        éœ€è¦è®¡ç®—: Qâ‚ƒ @ [Kâ‚, Kâ‚‚, Kâ‚ƒ].T  â† éœ€è¦ Kâ‚, Kâ‚‚ï¼</span><br><span class="line">        å¦‚æœä¸ç¼“å­˜ï¼Œè¦é‡æ–°ç®— Kâ‚, Kâ‚‚ â†’ æ›´æµªè´¹ï¼</span><br><span class="line">        è¾“å‡º: "å¤©å®‰é—¨"</span><br><span class="line">        ğŸ’¾ ç¼“å­˜: Kâ‚, Kâ‚‚, Kâ‚ƒ, Vâ‚, Vâ‚‚, Vâ‚ƒ</span><br><span class="line"></span><br><span class="line">Step N: </span><br><span class="line">        éœ€è¦è®¡ç®—: Qâ‚™ @ [Kâ‚, Kâ‚‚, ..., Kâ‚™].T</span><br><span class="line">        </span><br><span class="line">ä¸ç¼“å­˜: è®¡ç®—é‡ = 1+2+3+...+N = O(NÂ²)</span><br><span class="line">ç¼“å­˜:   è®¡ç®—é‡ = 1+1+1+...+1 = O(N)</span><br><span class="line">åŠ é€Ÿæ¯”: N å€ï¼</span><br></pre></td></tr></tbody></table></figure>

<p><strong>ä¸ºä»€ä¹ˆè¦åŠ åˆ°å½“å‰è¯ä¸Šï¼Ÿ</strong></p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Step 2 ç”Ÿæˆ "çˆ±" æ—¶</span></span><br><span class="line">Q_çˆ± @ [K_æˆ‘, K_çˆ±].T  <span class="comment"># "çˆ±" éœ€è¦çœ‹åˆ° "æˆ‘"</span></span><br><span class="line">      â†‘</span><br><span class="line">  å¿…é¡»åŒ…å«ä¹‹å‰çš„ K_æˆ‘ï¼</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3 ç”Ÿæˆ "åŒ—äº¬" æ—¶  </span></span><br><span class="line">Q_åŒ—äº¬ @ [K_æˆ‘, K_çˆ±, K_åŒ—äº¬].T  <span class="comment"># "åŒ—äº¬" éœ€è¦çœ‹åˆ° "æˆ‘" å’Œ "çˆ±"</span></span><br><span class="line">       â†‘</span><br><span class="line">   å¿…é¡»åŒ…å«æ‰€æœ‰å†å² Kï¼</span><br><span class="line"></span><br><span class="line"><span class="comment"># è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¦æŠŠå‰é¢çš„ Kã€V åŠ åˆ°å½“å‰çš„è®¡ç®—ä¸­</span></span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="4-6-æ¶æ„æ€»ç»“å¯¹æ¯”"><a href="#4-6-æ¶æ„æ€»ç»“å¯¹æ¯”" class="headerlink" title="4.6 æ¶æ„æ€»ç»“å¯¹æ¯”"></a>4.6 æ¶æ„æ€»ç»“å¯¹æ¯”</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">â”‚                    BERT vs GPT                              â”‚</span><br><span class="line">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span><br><span class="line">â”‚         BERT             â”‚            GPT                   â”‚</span><br><span class="line">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span><br><span class="line">â”‚  Encoder-only æ¶æ„        â”‚  Decoder-only æ¶æ„                â”‚</span><br><span class="line">â”‚  åŒå‘æ³¨æ„åŠ›               â”‚  å•å‘æ³¨æ„åŠ›ï¼ˆCausal Maskï¼‰         â”‚</span><br><span class="line">â”‚  ä¸€æ¬¡æ€§è¾“å…¥æ•´ä¸ªå¥å­        â”‚  é€ä¸ª token ç”Ÿæˆ                 â”‚</span><br><span class="line">â”‚  è¾“å…¥è¾“å‡ºé•¿åº¦ç›¸åŒ          â”‚  è¾“å‡ºæ¯”è¾“å…¥é•¿                    â”‚</span><br><span class="line">â”‚  å¹¶è¡Œè®¡ç®—æ‰€æœ‰ä½ç½®          â”‚  ä¸²è¡Œç”Ÿæˆ                        â”‚</span><br><span class="line">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span><br><span class="line">â”‚  âœ… æœ‰ Qã€Kã€V            â”‚  âœ… æœ‰ Qã€Kã€V                   â”‚</span><br><span class="line">â”‚  âŒ ä¸éœ€è¦ KV Cache       â”‚  âœ… å¿…é¡» KV Cache                â”‚</span><br><span class="line">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span><br><span class="line">â”‚  ç”¨é€”: ç†è§£               â”‚  ç”¨é€”: ç”Ÿæˆ                      â”‚</span><br><span class="line">â”‚  ä»»åŠ¡: åˆ†ç±»ã€NERã€é—®ç­”    â”‚  ä»»åŠ¡: èŠå¤©ã€å†™ä½œã€ä»£ç ç”Ÿæˆ       â”‚</span><br><span class="line">â”‚  ä¾‹å­: "è¿™æ˜¯___è¯„è®º"â†’æ­£é¢ â”‚  ä¾‹å­: "ä»å‰æœ‰" â†’ "åº§å±±"         â”‚</span><br><span class="line">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="4-7-æ··åˆæ¶æ„ï¼šT5-x2F-BART"><a href="#4-7-æ··åˆæ¶æ„ï¼šT5-x2F-BART" class="headerlink" title="4.7 æ··åˆæ¶æ„ï¼šT5/BART"></a>4.7 æ··åˆæ¶æ„ï¼šT5/BART</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">T5/BART = Encoder + Decoder</span><br><span class="line"></span><br><span class="line">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">â”‚  Encoder éƒ¨åˆ†ï¼ˆç±»ä¼¼ BERTï¼‰                               â”‚</span><br><span class="line">â”‚  è¾“å…¥: "Translate to English: æˆ‘çˆ±ä½ "                    â”‚</span><br><span class="line">â”‚  å¤„ç†: ä¸€æ¬¡æ€§ç¼–ç                                         â”‚</span><br><span class="line">â”‚  âŒ ä¸éœ€è¦ KV Cache                                     â”‚</span><br><span class="line">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br><span class="line">              â†“ ä¼ é€’ç¼–ç ç»“æœ</span><br><span class="line">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">â”‚  Decoder éƒ¨åˆ†ï¼ˆç±»ä¼¼ GPTï¼‰                                â”‚</span><br><span class="line">â”‚  ç”Ÿæˆ: "I" â†’ "love" â†’ "you"                             â”‚</span><br><span class="line">â”‚  å¤„ç†: é€ä¸ªç”Ÿæˆ                                          â”‚</span><br><span class="line">â”‚  âœ… éœ€è¦ KV Cacheï¼ˆä»… Decoder éƒ¨åˆ†ï¼‰                     â”‚</span><br><span class="line">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="4-8-è®°å¿†å£è¯€"><a href="#4-8-è®°å¿†å£è¯€" class="headerlink" title="4.8 è®°å¿†å£è¯€"></a>4.8 è®°å¿†å£è¯€</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Encoder (BERT) = é˜…è¯»ç†è§£ = ä¸€çœ¼çœ‹å®Œå…¨æ–‡ = ä¸éœ€è¦ç¼“å­˜</span><br><span class="line">Decoder (GPT)  = å†™ä½œæ–‡   = ä¸€å­—ä¸€å­—å†™   = éœ€è¦ç¼“å­˜å†å²</span><br><span class="line"></span><br><span class="line">åˆ¤æ–­æ ‡å‡†:</span><br><span class="line">  - è¾“å…¥è¾“å‡ºåŒæ—¶å­˜åœ¨ï¼Ÿ â†’ Encoder â†’ æ—  KV Cache</span><br><span class="line">  - é€æ­¥ç”Ÿæˆæ–°å†…å®¹ï¼Ÿ   â†’ Decoder â†’ æœ‰ KV Cache</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="âš¡-Part-5-KV-Cache-æ·±åº¦è§£æ"><a href="#âš¡-Part-5-KV-Cache-æ·±åº¦è§£æ" class="headerlink" title="âš¡ Part 5: KV Cache æ·±åº¦è§£æ"></a>âš¡ Part 5: KV Cache æ·±åº¦è§£æ</h2><h3 id="3-1-é—®é¢˜åœºæ™¯ï¼šè‡ªå›å½’ç”Ÿæˆ"><a href="#3-1-é—®é¢˜åœºæ™¯ï¼šè‡ªå›å½’ç”Ÿæˆ" class="headerlink" title="3.1 é—®é¢˜åœºæ™¯ï¼šè‡ªå›å½’ç”Ÿæˆ"></a>3.1 é—®é¢˜åœºæ™¯ï¼šè‡ªå›å½’ç”Ÿæˆ</h3><p>GPT ç”Ÿæˆæ–‡æœ¬ â€œI love NLP so muchâ€ï¼š</p>
<p><strong>æ—  KV Cache (ä½æ•ˆ)</strong>:</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Step 1: è¾“å…¥ "I"</span><br><span class="line">        è®¡ç®— Qâ‚, Kâ‚, Vâ‚ â†’ è¾“å‡º "love"</span><br><span class="line">        </span><br><span class="line">Step 2: è¾“å…¥ "I love"</span><br><span class="line">        é‡æ–°è®¡ç®— Qâ‚, Kâ‚, Vâ‚  â† æµªè´¹ï¼</span><br><span class="line">        é‡æ–°è®¡ç®— Qâ‚‚, Kâ‚‚, Vâ‚‚  â† æµªè´¹ï¼</span><br><span class="line">        â†’ è¾“å‡º "NLP"</span><br><span class="line">        </span><br><span class="line">Step 3: è¾“å…¥ "I love NLP"</span><br><span class="line">        é‡æ–°è®¡ç®— Qâ‚, Kâ‚, Vâ‚  â† æµªè´¹ï¼</span><br><span class="line">        é‡æ–°è®¡ç®— Qâ‚‚, Kâ‚‚, Vâ‚‚  â† æµªè´¹ï¼</span><br><span class="line">        é‡æ–°è®¡ç®— Qâ‚ƒ, Kâ‚ƒ, Vâ‚ƒ  â† æµªè´¹ï¼</span><br><span class="line">        â†’ è¾“å‡º "so"</span><br></pre></td></tr></tbody></table></figure>

<p><strong>è®¡ç®—é‡</strong>: 1 + 2 + 3 + â€¦ + n = <strong>O(nÂ²)</strong></p>
<hr>
<h3 id="3-2-KV-Cache-è§£å†³æ–¹æ¡ˆ"><a href="#3-2-KV-Cache-è§£å†³æ–¹æ¡ˆ" class="headerlink" title="3.2 KV Cache è§£å†³æ–¹æ¡ˆ"></a>3.2 KV Cache è§£å†³æ–¹æ¡ˆ</h3><p><strong>æ ¸å¿ƒæ´å¯Ÿ</strong>: K å’Œ V åªä¾èµ–è¾“å…¥ï¼Œä¸â€å½“å‰è¦ç”Ÿæˆä»€ä¹ˆâ€æ— å…³ â†’ <strong>å¯ä»¥ç¼“å­˜ï¼</strong></p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Step 1: è¾“å…¥ "I"</span><br><span class="line">        è®¡ç®— Kâ‚, Vâ‚ â†’ å­˜å…¥ Cache</span><br><span class="line">        è®¡ç®— Qâ‚ â†’ Attention(Qâ‚, [Kâ‚], [Vâ‚]) â†’ è¾“å‡º "love"</span><br><span class="line">        </span><br><span class="line">Step 2: è¾“å…¥ "love"</span><br><span class="line">        è®¡ç®— Kâ‚‚, Vâ‚‚ â†’ è¿½åŠ åˆ° Cache</span><br><span class="line">        åªè®¡ç®— Qâ‚‚ â†’ Attention(Qâ‚‚, [Kâ‚,Kâ‚‚], [Vâ‚,Vâ‚‚]) â†’ è¾“å‡º "NLP"</span><br><span class="line">        </span><br><span class="line">Step 3: è¾“å…¥ "NLP"</span><br><span class="line">        è®¡ç®— Kâ‚ƒ, Vâ‚ƒ â†’ è¿½åŠ åˆ° Cache</span><br><span class="line">        åªè®¡ç®— Qâ‚ƒ â†’ Attention(Qâ‚ƒ, [Kâ‚,Kâ‚‚,Kâ‚ƒ], [Vâ‚,Vâ‚‚,Vâ‚ƒ]) â†’ è¾“å‡º "so"</span><br></pre></td></tr></tbody></table></figure>

<p><strong>è®¡ç®—é‡</strong>: 1 + 1 + 1 + â€¦ + 1 = <strong>O(n)</strong></p>
<p><strong>åŠ é€Ÿæ¯”</strong>: nÂ²/n = <strong>n å€åŠ é€Ÿ</strong>ï¼</p>
<hr>
<h3 id="3-3-å†…å­˜æ¶ˆè€—åˆ†æ"><a href="#3-3-å†…å­˜æ¶ˆè€—åˆ†æ" class="headerlink" title="3.3 å†…å­˜æ¶ˆè€—åˆ†æ"></a>3.3 å†…å­˜æ¶ˆè€—åˆ†æ</h3><h4 id="å…¬å¼"><a href="#å…¬å¼" class="headerlink" title="å…¬å¼"></a>å…¬å¼</h4><p>$$<br>\text{KV Cache Size} = 2 \times n_{\text{layers}} \times d_{\text{model}} \times \text{seq_len} \times \text{dtype_size}<br>$$</p>
<h4 id="å®é™…æ¡ˆä¾‹ï¼šLLaMA-7B"><a href="#å®é™…æ¡ˆä¾‹ï¼šLLaMA-7B" class="headerlink" title="å®é™…æ¡ˆä¾‹ï¼šLLaMA-7B"></a>å®é™…æ¡ˆä¾‹ï¼šLLaMA-7B</h4><ul>
<li><code>n_layers = 32</code></li>
<li><code>d_model = 4096</code></li>
<li><code>dtype = float16</code> (2 bytes)</li>
</ul>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">æ¯ä¸ª token çš„ KV Cache:</span><br><span class="line">= <span class="number">2</span> Ã— <span class="number">32</span> Ã— <span class="number">4096</span> Ã— <span class="number">2</span> <span class="built_in">bytes</span></span><br><span class="line">= <span class="number">524</span>,<span class="number">288</span> <span class="built_in">bytes</span></span><br><span class="line">= <span class="number">512</span> KB / token</span><br><span class="line"></span><br><span class="line">ä¸åŒåºåˆ—é•¿åº¦çš„æ˜¾å­˜å ç”¨:</span><br><span class="line">- 1K tokens:   <span class="number">512</span> MB</span><br><span class="line">- 4K tokens:   <span class="number">2</span> GB</span><br><span class="line">- 32K tokens:  <span class="number">16</span> GB  â† é•¿ä¸Šä¸‹æ–‡çš„æŒ‘æˆ˜ï¼</span><br><span class="line">- 128K tokens: <span class="number">64</span> GB  â† éœ€è¦å¤šå¡æˆ–ä¼˜åŒ–æŠ€æœ¯</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="3-4-KV-Cache-ä¼˜åŒ–æŠ€æœ¯"><a href="#3-4-KV-Cache-ä¼˜åŒ–æŠ€æœ¯" class="headerlink" title="3.4 KV Cache ä¼˜åŒ–æŠ€æœ¯"></a>3.4 KV Cache ä¼˜åŒ–æŠ€æœ¯</h3><h4 id="1-Multi-Query-Attention-MQA"><a href="#1-Multi-Query-Attention-MQA" class="headerlink" title="1. Multi-Query Attention (MQA)"></a>1. Multi-Query Attention (MQA)</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">æ ‡å‡† Multi-Head: æ¯ä¸ªå¤´éƒ½æœ‰ç‹¬ç«‹çš„ K, V</span><br><span class="line">MQA: æ‰€æœ‰å¤´å…±äº«åŒä¸€ç»„ K, V</span><br><span class="line"></span><br><span class="line">æ˜¾å­˜èŠ‚çœ: heads å€ (ä¾‹å¦‚ 12 å¤´ â†’ èŠ‚çœ 12 å€)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="2-Grouped-Query-Attention-GQA"><a href="#2-Grouped-Query-Attention-GQA" class="headerlink" title="2. Grouped-Query Attention (GQA)"></a>2. Grouped-Query Attention (GQA)</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">æŠ˜ä¸­æ–¹æ¡ˆ: å°† 12 ä¸ªå¤´åˆ†æˆ 3 ç»„ï¼Œæ¯ç»„å…±äº« K, V</span><br><span class="line"></span><br><span class="line">LLaMA-2 ä½¿ç”¨ GQA:</span><br><span class="line">- 32 ä¸ªå¤´ â†’ 8 ç»„</span><br><span class="line">- æ˜¾å­˜èŠ‚çœ: 4 å€</span><br></pre></td></tr></tbody></table></figure>

<h4 id="3-PagedAttention-vLLM"><a href="#3-PagedAttention-vLLM" class="headerlink" title="3. PagedAttention (vLLM)"></a>3. PagedAttention (vLLM)</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">ç±»ä¼¼æ“ä½œç³»ç»Ÿçš„åˆ†é¡µæœºåˆ¶</span><br><span class="line">å°† KV Cache åˆ†æˆå›ºå®šå¤§å°çš„å— (Page)</span><br><span class="line">åŠ¨æ€åˆ†é…å’Œå›æ”¶ï¼Œå‡å°‘ç¢ç‰‡</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="ğŸ­-Part-4-Causal-Attention-vs-Bidirectional-Attention"><a href="#ğŸ­-Part-4-Causal-Attention-vs-Bidirectional-Attention" class="headerlink" title="ğŸ­ Part 4: Causal Attention vs Bidirectional Attention"></a>ğŸ­ Part 4: Causal Attention vs Bidirectional Attention</h2><h3 id="4-1-æ ¸å¿ƒåŒºåˆ«"><a href="#4-1-æ ¸å¿ƒåŒºåˆ«" class="headerlink" title="4.1 æ ¸å¿ƒåŒºåˆ«"></a>4.1 æ ¸å¿ƒåŒºåˆ«</h3><table>
<thead>
<tr>
<th>ç»´åº¦</th>
<th>Bidirectional (BERT)</th>
<th>Causal (GPT)</th>
</tr>
</thead>
<tbody><tr>
<td><strong>å¯è§èŒƒå›´</strong></td>
<td>å…¨å±€å¯è§</td>
<td>ä»…å·¦ä¾§å¯è§</td>
</tr>
<tr>
<td><strong>Mask å½¢çŠ¶</strong></td>
<td>å…¨ 1 (æˆ–ä»… mask padding)</td>
<td>ä¸‹ä¸‰è§’çŸ©é˜µ</td>
</tr>
<tr>
<td><strong>é€‚ç”¨ä»»åŠ¡</strong></td>
<td>ç†è§£ (åˆ†ç±»ã€NERã€QA)</td>
<td>ç”Ÿæˆ (æ–‡æœ¬ã€å¯¹è¯)</td>
</tr>
<tr>
<td><strong>è®­ç»ƒæ•ˆç‡</strong></td>
<td>é«˜ (å¹¶è¡Œè®¡ç®—æ‰€æœ‰ä½ç½®)</td>
<td>é«˜ (teacher forcing)</td>
</tr>
<tr>
<td><strong>æ¨ç†æ¨¡å¼</strong></td>
<td>ä¸€æ¬¡æ€§è¾“å‡º</td>
<td>é€ token ç”Ÿæˆ</td>
</tr>
</tbody></table>
<hr>
<h3 id="4-2-Causal-Mask-çš„æ•°å­¦å®ç°"><a href="#4-2-Causal-Mask-çš„æ•°å­¦å®ç°" class="headerlink" title="4.2 Causal Mask çš„æ•°å­¦å®ç°"></a>4.2 Causal Mask çš„æ•°å­¦å®ç°</h3><h4 id="Mask-çŸ©é˜µ"><a href="#Mask-çŸ©é˜µ" class="headerlink" title="Mask çŸ©é˜µ"></a>Mask çŸ©é˜µ</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">seq_len = <span class="number">4</span></span><br><span class="line">mask = np.tril(np.ones((<span class="number">4</span>, <span class="number">4</span>)))</span><br><span class="line"></span><br><span class="line">       I   love  NLP   !</span><br><span class="line">  I   [<span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>]</span><br><span class="line"> love [<span class="number">1</span>    <span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>]</span><br><span class="line"> NLP  [<span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">0</span>]</span><br><span class="line">  !   [<span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">è§£é‡Š:</span><br><span class="line">- <span class="string">"I"</span> åªèƒ½çœ‹åˆ°è‡ªå·±</span><br><span class="line">- <span class="string">"love"</span> èƒ½çœ‹åˆ° <span class="string">"I"</span> å’Œè‡ªå·±</span><br><span class="line">- <span class="string">"NLP"</span> èƒ½çœ‹åˆ° <span class="string">"I"</span>, <span class="string">"love"</span>, <span class="string">"NLP"</span></span><br><span class="line">- <span class="string">"!"</span> èƒ½çœ‹åˆ°æ‰€æœ‰</span><br></pre></td></tr></tbody></table></figure>

<h4 id="åº”ç”¨åˆ°-Attention-Scores"><a href="#åº”ç”¨åˆ°-Attention-Scores" class="headerlink" title="åº”ç”¨åˆ° Attention Scores"></a>åº”ç”¨åˆ° Attention Scores</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">scores = Q @ K.T / sqrt(d_k)  <span class="comment"># [4, 4]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># å°†ä¸Šä¸‰è§’è®¾ä¸º -inf</span></span><br><span class="line">masked_scores = np.where(mask == <span class="number">1</span>, scores, -np.inf)</span><br><span class="line"></span><br><span class="line">       I     love    NLP     !</span><br><span class="line">  I   [<span class="number">2.1</span>   -inf   -inf   -inf]</span><br><span class="line"> love [<span class="number">1.3</span>   <span class="number">3.2</span>    -inf   -inf]</span><br><span class="line"> NLP  [<span class="number">0.8</span>   <span class="number">1.9</span>    <span class="number">2.7</span>    -inf]</span><br><span class="line">  !   [<span class="number">0.5</span>   <span class="number">1.1</span>    <span class="number">0.9</span>    <span class="number">3.5</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Softmax å, -inf ä½ç½®å˜æˆ 0</span></span><br><span class="line">weights = softmax(masked_scores)</span><br><span class="line"></span><br><span class="line">       I     love    NLP     !</span><br><span class="line">  I   [<span class="number">1.0</span>   <span class="number">0.0</span>    <span class="number">0.0</span>    <span class="number">0.0</span>]  â† åªçœ‹è‡ªå·±</span><br><span class="line"> love [<span class="number">0.3</span>   <span class="number">0.7</span>    <span class="number">0.0</span>    <span class="number">0.0</span>]  â† <span class="number">70</span>% çœ‹è‡ªå·±, <span class="number">30</span>% çœ‹ I</span><br><span class="line"> NLP  [<span class="number">0.1</span>   <span class="number">0.3</span>    <span class="number">0.6</span>    <span class="number">0.0</span>]  â† ä¸»è¦çœ‹è‡ªå·±å’Œ love</span><br><span class="line">  !   [<span class="number">0.05</span>  <span class="number">0.2</span>    <span class="number">0.15</span>   <span class="number">0.6</span>]  â† <span class="number">60</span>% çœ‹è‡ªå·±</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="ğŸ”„-Part-5-è®­ç»ƒé—­ç¯-Label-x2F-Loss-x2F-æ¢¯åº¦æµ"><a href="#ğŸ”„-Part-5-è®­ç»ƒé—­ç¯-Label-x2F-Loss-x2F-æ¢¯åº¦æµ" class="headerlink" title="ğŸ”„ Part 5: è®­ç»ƒé—­ç¯ - Label / Loss / æ¢¯åº¦æµ"></a>ğŸ”„ Part 5: è®­ç»ƒé—­ç¯ - Label / Loss / æ¢¯åº¦æµ</h2><h3 id="5-1-MLM-ä»»åŠ¡çš„å®Œæ•´è®­ç»ƒæµç¨‹"><a href="#5-1-MLM-ä»»åŠ¡çš„å®Œæ•´è®­ç»ƒæµç¨‹" class="headerlink" title="5.1 MLM ä»»åŠ¡çš„å®Œæ•´è®­ç»ƒæµç¨‹"></a>5.1 MLM ä»»åŠ¡çš„å®Œæ•´è®­ç»ƒæµç¨‹</h3><h4 id="æ•°æ®å‡†å¤‡"><a href="#æ•°æ®å‡†å¤‡" class="headerlink" title="æ•°æ®å‡†å¤‡"></a>æ•°æ®å‡†å¤‡</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">åŸå§‹æ–‡æœ¬: <span class="string">"æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨"</span></span><br><span class="line">å¤„ç†å:   <span class="string">"æˆ‘çˆ±[MASK]å¤©å®‰é—¨"</span></span><br><span class="line"></span><br><span class="line">input_ids = [<span class="number">101</span>, <span class="number">2769</span>, <span class="number">4263</span>, <span class="number">103</span>, <span class="number">1921</span>, <span class="number">2128</span>, <span class="number">7305</span>, <span class="number">102</span>]</span><br><span class="line">             [CLS]  æˆ‘    çˆ±   MASK  å¤©   å®‰    é—¨  [SEP]</span><br><span class="line">labels    = [-<span class="number">100</span>, -<span class="number">100</span>, -<span class="number">100</span>, <span class="number">1266</span>, -<span class="number">100</span>, -<span class="number">100</span>, -<span class="number">100</span>, -<span class="number">100</span>]</span><br><span class="line">                                 â†‘</span><br><span class="line">                               <span class="string">"åŒ—äº¬"</span> çš„ ID</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Forward-Pass"><a href="#Forward-Pass" class="headerlink" title="Forward Pass"></a>Forward Pass</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Embedding</span></span><br><span class="line">embeddings = token_emb + pos_emb + seg_emb  <span class="comment"># [1, 8, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 12 å±‚ Transformer</span></span><br><span class="line">hidden = embeddings</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> encoder_layers:</span><br><span class="line">    hidden = layer(hidden)  <span class="comment"># [1, 8, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. MLM Head</span></span><br><span class="line">logits = mlm_head(hidden)  <span class="comment"># [1, 8, 21128]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># åªå– [MASK] ä½ç½® (index=3)</span></span><br><span class="line">masked_logits = logits[<span class="number">0</span>, <span class="number">3</span>, :]  <span class="comment"># [21128]</span></span><br></pre></td></tr></tbody></table></figure>

<h4 id="Loss-è®¡ç®—"><a href="#Loss-è®¡ç®—" class="headerlink" title="Loss è®¡ç®—"></a>Loss è®¡ç®—</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">true_label = <span class="number">1266</span>  <span class="comment"># "åŒ—äº¬"</span></span><br><span class="line"></span><br><span class="line">loss = CrossEntropyLoss(masked_logits, true_label)</span><br><span class="line">     = -log(softmax(masked_logits)[<span class="number">1266</span>])</span><br><span class="line">     </span><br><span class="line">å¦‚æœæ¨¡å‹é¢„æµ‹:</span><br><span class="line">  P(<span class="string">"åŒ—äº¬"</span>) = <span class="number">0.8</span>  â†’ loss = -log(<span class="number">0.8</span>) = <span class="number">0.22</span> (å¥½)</span><br><span class="line">  P(<span class="string">"åŒ—äº¬"</span>) = <span class="number">0.1</span>  â†’ loss = -log(<span class="number">0.1</span>) = <span class="number">2.30</span> (å·®)</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="5-2-æ¢¯åº¦åå‘ä¼ æ’­"><a href="#5-2-æ¢¯åº¦åå‘ä¼ æ’­" class="headerlink" title="5.2 æ¢¯åº¦åå‘ä¼ æ’­"></a>5.2 æ¢¯åº¦åå‘ä¼ æ’­</h3><h4 id="æ¢¯åº¦æµåŠ¨è·¯å¾„"><a href="#æ¢¯åº¦æµåŠ¨è·¯å¾„" class="headerlink" title="æ¢¯åº¦æµåŠ¨è·¯å¾„"></a>æ¢¯åº¦æµåŠ¨è·¯å¾„</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">      Loss (æ ‡é‡)</span><br><span class="line">        â†“ âˆ‚L/âˆ‚logits</span><br><span class="line">   MLM Head (çº¿æ€§å±‚)</span><br><span class="line">        â†“ âˆ‚L/âˆ‚hâ‚â‚‚</span><br><span class="line">Transformer Layer 12</span><br><span class="line">        â†“</span><br><span class="line">      ...</span><br><span class="line">        â†“</span><br><span class="line">Transformer Layer 1</span><br><span class="line">  â†™    â†“    â†˜</span><br><span class="line">âˆ‚L/âˆ‚Q  âˆ‚L/âˆ‚K  âˆ‚L/âˆ‚V</span><br><span class="line">  â†“     â†“     â†“</span><br><span class="line">W_Q   W_K   W_V  â† è¿™äº›æƒé‡è¢«æ›´æ–°ï¼</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Attention-ä¸­çš„æ¢¯åº¦åˆ†å‰"><a href="#Attention-ä¸­çš„æ¢¯åº¦åˆ†å‰" class="headerlink" title="Attention ä¸­çš„æ¢¯åº¦åˆ†å‰"></a>Attention ä¸­çš„æ¢¯åº¦åˆ†å‰</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Forward</span></span><br><span class="line">attn_weights = softmax(Q @ K.T / sqrt(d_k))  <span class="comment"># [seq, seq]</span></span><br><span class="line">output = attn_weights @ V                     <span class="comment"># [seq, dim]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Backward</span></span><br><span class="line">âˆ‚L/âˆ‚V = attn_weights.T @ âˆ‚L/âˆ‚output  â† V çš„æ¢¯åº¦</span><br><span class="line">âˆ‚L/âˆ‚attn = âˆ‚L/âˆ‚output @ V.T          â† æ³¨æ„åŠ›æƒé‡çš„æ¢¯åº¦</span><br><span class="line">âˆ‚L/âˆ‚scores = âˆ‚softmax(âˆ‚L/âˆ‚attn)     â† Softmax åå‘</span><br><span class="line">âˆ‚L/âˆ‚Q = âˆ‚L/âˆ‚scores @ K               â† Q çš„æ¢¯åº¦</span><br><span class="line">âˆ‚L/âˆ‚K = âˆ‚L/âˆ‚scores.T @ Q             â† K çš„æ¢¯åº¦</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="5-3-ä¸ºä»€ä¹ˆ-Attention-èƒ½å­¦åˆ°è¯­ä¹‰ï¼Ÿ"><a href="#5-3-ä¸ºä»€ä¹ˆ-Attention-èƒ½å­¦åˆ°è¯­ä¹‰ï¼Ÿ" class="headerlink" title="5.3 ä¸ºä»€ä¹ˆ Attention èƒ½å­¦åˆ°è¯­ä¹‰ï¼Ÿ"></a>5.3 ä¸ºä»€ä¹ˆ Attention èƒ½å­¦åˆ°è¯­ä¹‰ï¼Ÿ</h3><h4 id="æ¢¯åº¦çš„â€æŒ‡å¯¼ä½œç”¨â€"><a href="#æ¢¯åº¦çš„â€æŒ‡å¯¼ä½œç”¨â€" class="headerlink" title="æ¢¯åº¦çš„â€æŒ‡å¯¼ä½œç”¨â€"></a>æ¢¯åº¦çš„â€æŒ‡å¯¼ä½œç”¨â€</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">å‡è®¾å½“å‰é¢„æµ‹:</span><br><span class="line">  [MASK] ä½ç½®é¢„æµ‹ "ä¸Šæµ·" (é”™è¯¯, åº”è¯¥æ˜¯ "åŒ—äº¬")</span><br><span class="line">  </span><br><span class="line">Loss å¾ˆå¤§ â†’ æ¢¯åº¦å›ä¼ :</span><br><span class="line">  </span><br><span class="line">1. æµå‘ V:</span><br><span class="line">   "ä½ ä»¬æä¾›çš„å†…å®¹ä¸å¯¹ï¼'ä¸Šæµ·' çš„è¯­ä¹‰ç‰¹å¾å¤ªå¼ºäº†"</span><br><span class="line">   â†’ è°ƒæ•´ Vï¼Œè®© "åŒ—äº¬" ç›¸å…³çš„ token æä¾›æ›´å¤šä¿¡æ¯</span><br><span class="line">   </span><br><span class="line">2. æµå‘ Q å’Œ K:</span><br><span class="line">   "'çˆ±' å’Œ 'å¤©å®‰é—¨' çš„æ³¨æ„åŠ›æƒé‡ä¸å¯¹ï¼"</span><br><span class="line">   â†’ è°ƒæ•´ Q/Kï¼Œè®© [MASK] æ›´å¤šå…³æ³¨ "å¤©å®‰é—¨"</span><br><span class="line">   â†’ å› ä¸º "åŒ—äº¬ + å¤©å®‰é—¨" å…±ç°é¢‘ç‡é«˜</span><br><span class="line">   </span><br><span class="line">3. å¤šè½®è¿­ä»£å:</span><br><span class="line">   æ¨¡å‹å­¦ä¼š: "çœ‹åˆ°'å¤©å®‰é—¨' â†’ è”æƒ³åˆ°'åŒ—äº¬'"</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="ğŸ“š-æ€»ç»“ï¼šæ ¸å¿ƒè¦ç‚¹å›é¡¾"><a href="#ğŸ“š-æ€»ç»“ï¼šæ ¸å¿ƒè¦ç‚¹å›é¡¾" class="headerlink" title="ğŸ“š æ€»ç»“ï¼šæ ¸å¿ƒè¦ç‚¹å›é¡¾"></a>ğŸ“š æ€»ç»“ï¼šæ ¸å¿ƒè¦ç‚¹å›é¡¾</h2><h3 id="BERT-è®ºæ–‡æ ¸å¿ƒè´¡çŒ®"><a href="#BERT-è®ºæ–‡æ ¸å¿ƒè´¡çŒ®" class="headerlink" title="BERT è®ºæ–‡æ ¸å¿ƒè´¡çŒ®"></a>BERT è®ºæ–‡æ ¸å¿ƒè´¡çŒ®</h3><ol>
<li><strong>Masked Language Model</strong>: å®ç°æ·±åº¦åŒå‘å»ºæ¨¡</li>
<li><strong>å¤§è§„æ¨¡é¢„è®­ç»ƒ + å¾®è°ƒ</strong>: å¼€åˆ›é¢„è®­ç»ƒèŒƒå¼</li>
<li><strong>SOTA æ€§èƒ½</strong>: åœ¨ 11 ä¸ªä»»åŠ¡ä¸Šåˆ·æ–°è®°å½•</li>
</ol>
<h3 id="Q-x2F-K-x2F-V-æœºåˆ¶æœ¬è´¨"><a href="#Q-x2F-K-x2F-V-æœºåˆ¶æœ¬è´¨" class="headerlink" title="Q/K/V æœºåˆ¶æœ¬è´¨"></a>Q/K/V æœºåˆ¶æœ¬è´¨</h3><ul>
<li><strong>Q</strong>: æé—® â€œæˆ‘æƒ³æ‰¾ä»€ä¹ˆä¿¡æ¯ï¼Ÿâ€</li>
<li><strong>K</strong>: ç´¢å¼• â€œæˆ‘è¿™é‡Œæœ‰ä»€ä¹ˆä¿¡æ¯ï¼Ÿâ€</li>
<li><strong>V</strong>: å†…å®¹ â€œå®é™…çš„ä¿¡æ¯æ˜¯ä»€ä¹ˆï¼Ÿâ€</li>
<li><strong>Attention</strong>: æ ¹æ® Q-K ç›¸ä¼¼åº¦ï¼ŒåŠ æƒèšåˆ V</li>
</ul>
<h3 id="KV-Cache-ä¼˜åŒ–"><a href="#KV-Cache-ä¼˜åŒ–" class="headerlink" title="KV Cache ä¼˜åŒ–"></a>KV Cache ä¼˜åŒ–</h3><ul>
<li><strong>é—®é¢˜</strong>: è‡ªå›å½’ç”Ÿæˆæ—¶é‡å¤è®¡ç®— K/V â†’ O(nÂ²)</li>
<li><strong>æ–¹æ¡ˆ</strong>: ç¼“å­˜å†å² K/V â†’ O(n)</li>
<li><strong>ä»£ä»·</strong>: æ˜¾å­˜å ç”¨ (LLaMA-7B: 512KB/token)</li>
</ul>
<h3 id="Causal-vs-Bidirectional"><a href="#Causal-vs-Bidirectional" class="headerlink" title="Causal vs Bidirectional"></a>Causal vs Bidirectional</h3><ul>
<li><strong>Causal</strong>: ä¸‹ä¸‰è§’ mask, ç”¨äºç”Ÿæˆ</li>
<li><strong>Bidirectional</strong>: å…¨å±€å¯è§, ç”¨äºç†è§£</li>
</ul>
<h3 id="è®­ç»ƒé—­ç¯"><a href="#è®­ç»ƒé—­ç¯" class="headerlink" title="è®­ç»ƒé—­ç¯"></a>è®­ç»ƒé—­ç¯</h3><ul>
<li><strong>Label</strong> â†’ <strong>Loss</strong> â†’ <strong>Gradient</strong> â†’ <strong>Update</strong></li>
<li>æ¢¯åº¦æµè¿‡ Attention æ—¶åˆ†å‰åˆ° Q/K/V</li>
<li>é€šè¿‡åå‘ä¼ æ’­ï¼Œæ¨¡å‹å­¦ä¼šâ€åœ¨å“ªé‡Œæ‰¾ä¿¡æ¯â€å’Œâ€æ‰¾ä»€ä¹ˆä¿¡æ¯â€</li>
</ul>
<hr>
<h2 id="ğŸ†š-Part-8-Encoder-vs-Decoder-KV-Cache-å¯¹æ¯”"><a href="#ğŸ†š-Part-8-Encoder-vs-Decoder-KV-Cache-å¯¹æ¯”" class="headerlink" title="ğŸ†š Part 8: Encoder vs Decoder - KV Cache å¯¹æ¯”"></a>ğŸ†š Part 8: Encoder vs Decoder - KV Cache å¯¹æ¯”</h2><h3 id="æ ¸å¿ƒç»“è®º"><a href="#æ ¸å¿ƒç»“è®º" class="headerlink" title="æ ¸å¿ƒç»“è®º"></a>æ ¸å¿ƒç»“è®º</h3><table>
<thead>
<tr>
<th>æ¨¡å‹</th>
<th>æ¶æ„</th>
<th>æœ‰ KV Cacheï¼Ÿ</th>
<th>åŸå› </th>
</tr>
</thead>
<tbody><tr>
<td><strong>BERT</strong></td>
<td>Encoder-only</td>
<td>âŒ æ²¡æœ‰</td>
<td>ä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªè¾“å…¥</td>
</tr>
<tr>
<td><strong>GPT</strong></td>
<td>Decoder-only</td>
<td>âœ… æœ‰</td>
<td>é€ä¸ª token ç”Ÿæˆ</td>
</tr>
<tr>
<td><strong>T5/BART</strong></td>
<td>Encoder + Decoder</td>
<td>âš ï¸ Decoder éƒ¨åˆ†æœ‰</td>
<td>Encoder ä¸éœ€è¦ï¼ŒDecoder éœ€è¦</td>
</tr>
</tbody></table>
<hr>
<h3 id="BERT-Encoder-only-ä¸éœ€è¦-KV-Cache"><a href="#BERT-Encoder-only-ä¸éœ€è¦-KV-Cache" class="headerlink" title="BERT (Encoder-only) - ä¸éœ€è¦ KV Cache"></a>BERT (Encoder-only) - ä¸éœ€è¦ KV Cache</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># è¾“å…¥å®Œæ•´å¥å­</span></span><br><span class="line"><span class="built_in">input</span> = <span class="string">"æˆ‘ çˆ± [MASK] å¤©å®‰é—¨"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ä¸€æ¬¡æ€§å…¨éƒ¨å¤„ç†</span></span><br><span class="line">output = bert(<span class="built_in">input</span>)  <span class="comment"># åŒæ—¶è®¡ç®—æ‰€æœ‰ä½ç½®çš„ Qã€Kã€V</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># æµç¨‹</span></span><br><span class="line">Step <span class="number">1</span>: è¾“å…¥å…¨éƒ¨ token [<span class="number">6</span>ä¸ª]</span><br><span class="line">Step <span class="number">2</span>: åŒæ—¶è®¡ç®—æ‰€æœ‰ä½ç½®çš„ Kã€V</span><br><span class="line">Step <span class="number">3</span>: åŒæ—¶è®¡ç®—æ‰€æœ‰ä½ç½®çš„ Attention</span><br><span class="line">Step <span class="number">4</span>: åŒæ—¶è¾“å‡ºæ‰€æœ‰ä½ç½®çš„ç»“æœ</span><br><span class="line"></span><br><span class="line"><span class="comment"># è®¡ç®—é‡: O(n) - åªè®¡ç®—ä¸€æ¬¡</span></span><br><span class="line"><span class="comment"># ä¸éœ€è¦ KV Cacheï¼</span></span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="GPT-Decoder-only-å¿…é¡»æœ‰-KV-Cache"><a href="#GPT-Decoder-only-å¿…é¡»æœ‰-KV-Cache" class="headerlink" title="GPT (Decoder-only) - å¿…é¡»æœ‰ KV Cache"></a>GPT (Decoder-only) - å¿…é¡»æœ‰ KV Cache</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># é€ä¸ªç”Ÿæˆ</span></span><br><span class="line">Step <span class="number">1</span>: <span class="built_in">input</span>=<span class="string">"æˆ‘"</span>     </span><br><span class="line">        è®¡ç®— Kâ‚, Vâ‚ â†’ ç¼“å­˜</span><br><span class="line">        ç”Ÿæˆ <span class="string">"çˆ±"</span></span><br><span class="line"></span><br><span class="line">Step <span class="number">2</span>: <span class="built_in">input</span>=<span class="string">"çˆ±"</span>     </span><br><span class="line">        è®¡ç®— Kâ‚‚, Vâ‚‚ â†’ ç¼“å­˜</span><br><span class="line">        ä½¿ç”¨ Kâ‚, Vâ‚ + Kâ‚‚, Vâ‚‚</span><br><span class="line">        ç”Ÿæˆ <span class="string">"åŒ—äº¬"</span></span><br><span class="line"></span><br><span class="line">Step <span class="number">3</span>: <span class="built_in">input</span>=<span class="string">"åŒ—äº¬"</span>   </span><br><span class="line">        è®¡ç®— Kâ‚ƒ, Vâ‚ƒ â†’ ç¼“å­˜</span><br><span class="line">        ä½¿ç”¨ Kâ‚, Vâ‚ + Kâ‚‚, Vâ‚‚ + Kâ‚ƒ, Vâ‚ƒ</span><br><span class="line">        ç”Ÿæˆ <span class="string">"å¤©å®‰é—¨"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ä¸ç”¨ç¼“å­˜: è®¡ç®—é‡ = 1+2+3+...+N = O(NÂ²)</span></span><br><span class="line"><span class="comment"># ç”¨ç¼“å­˜: è®¡ç®—é‡ = 1+1+1+...+1 = O(N)</span></span><br><span class="line"><span class="comment"># å¿…é¡»æœ‰ KV Cacheï¼</span></span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="ä¸ºä»€ä¹ˆ-Decoder-å¿…é¡»ç¼“å­˜-Kã€Vï¼Ÿ"><a href="#ä¸ºä»€ä¹ˆ-Decoder-å¿…é¡»ç¼“å­˜-Kã€Vï¼Ÿ" class="headerlink" title="ä¸ºä»€ä¹ˆ Decoder å¿…é¡»ç¼“å­˜ Kã€Vï¼Ÿ"></a>ä¸ºä»€ä¹ˆ Decoder å¿…é¡»ç¼“å­˜ Kã€Vï¼Ÿ</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">ç”Ÿæˆ "æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨" çš„è¿‡ç¨‹ï¼š</span><br><span class="line"></span><br><span class="line">Step 1: è¾“å…¥ "æˆ‘"</span><br><span class="line">        Qâ‚ @ Kâ‚.T â†’ åªæœ‰è‡ªå·±çœ‹è‡ªå·±</span><br><span class="line">        è¾“å‡º: "çˆ±"</span><br><span class="line"></span><br><span class="line">Step 2: è¾“å…¥ "çˆ±"  </span><br><span class="line">        éœ€è¦è®¡ç®—: Qâ‚‚ @ [Kâ‚, Kâ‚‚].T  â† éœ€è¦ä¹‹å‰çš„ Kâ‚ï¼</span><br><span class="line">        å¦‚æœä¸ç¼“å­˜ï¼Œå°±è¦é‡æ–°è®¡ç®— Kâ‚ â†’ æµªè´¹ï¼</span><br><span class="line">        </span><br><span class="line">Step 3: è¾“å…¥ "åŒ—äº¬"</span><br><span class="line">        éœ€è¦è®¡ç®—: Qâ‚ƒ @ [Kâ‚, Kâ‚‚, Kâ‚ƒ].T  â† éœ€è¦ Kâ‚, Kâ‚‚ï¼</span><br><span class="line">        å¦‚æœä¸ç¼“å­˜ï¼Œè¦é‡æ–°è®¡ç®— Kâ‚, Kâ‚‚ â†’ æ›´æµªè´¹ï¼</span><br><span class="line"></span><br><span class="line">Step N: </span><br><span class="line">        éœ€è¦è®¡ç®—: Qâ‚™ @ [Kâ‚, Kâ‚‚, ..., Kâ‚™].T</span><br><span class="line">        ä¸ç¼“å­˜çš„è¯ï¼Œè®¡ç®—é‡ = 1+2+3+...+N = O(NÂ²)</span><br><span class="line">        ç¼“å­˜çš„è¯ï¼Œæ¯æ­¥åªç®—æ–°çš„ï¼Œè®¡ç®—é‡ = O(N)</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="BERT-æœ‰-Q-x2F-K-x2F-Vï¼Œä½†ä¸ºä»€ä¹ˆä¸éœ€è¦ç¼“å­˜ï¼Ÿ"><a href="#BERT-æœ‰-Q-x2F-K-x2F-Vï¼Œä½†ä¸ºä»€ä¹ˆä¸éœ€è¦ç¼“å­˜ï¼Ÿ" class="headerlink" title="BERT æœ‰ Q/K/Vï¼Œä½†ä¸ºä»€ä¹ˆä¸éœ€è¦ç¼“å­˜ï¼Ÿ"></a>BERT æœ‰ Q/K/Vï¼Œä½†ä¸ºä»€ä¹ˆä¸éœ€è¦ç¼“å­˜ï¼Ÿ</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BERT: ä¸€æ¬¡æå®šï¼Œä¸éœ€è¦ç¼“å­˜</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bert_forward</span>(<span class="params">input_ids</span>):</span><br><span class="line">    <span class="comment"># input_ids = [101, 2769, 4263, 103, 1921, 102]  ä¸€æ¬¡æ€§è¾“å…¥</span></span><br><span class="line">    </span><br><span class="line">    X = embedding(input_ids)  <span class="comment"># [6, 768]</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> encoder_layers:</span><br><span class="line">        Q = X @ W_Q  <span class="comment"># åŒæ—¶ç®—æ‰€æœ‰ä½ç½®çš„ Q</span></span><br><span class="line">        K = X @ W_K  <span class="comment"># åŒæ—¶ç®—æ‰€æœ‰ä½ç½®çš„ K</span></span><br><span class="line">        V = X @ W_V  <span class="comment"># åŒæ—¶ç®—æ‰€æœ‰ä½ç½®çš„ V</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># æ‰€æœ‰ä½ç½®çš„ Attention åŒæ—¶è®¡ç®—</span></span><br><span class="line">        scores = Q @ K.T  <span class="comment"># [6, 6]</span></span><br><span class="line">        weights = softmax(scores)</span><br><span class="line">        X = weights @ V  <span class="comment"># [6, 768]</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X  <span class="comment"># ç›´æ¥è¿”å›ï¼Œä¸éœ€è¦ä¿å­˜ä»»ä½•ä¸­é—´ç»“æœ</span></span><br></pre></td></tr></tbody></table></figure>

<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GPT: é€ä¸ªç”Ÿæˆï¼Œå¿…é¡»ç¼“å­˜ Kã€V</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gpt_generate</span>(<span class="params">prompt</span>):</span><br><span class="line">    kv_cache = []  <span class="comment"># å¿…é¡»æœ‰è¿™ä¸ªï¼</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(max_tokens):</span><br><span class="line">        <span class="keyword">if</span> step == <span class="number">0</span>:</span><br><span class="line">            X = embedding(prompt)  <span class="comment"># ç¬¬ä¸€æ­¥å¤„ç†æ•´ä¸ª prompt</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X = embedding(new_token)  <span class="comment"># ä¹‹åæ¯æ­¥åªå¤„ç†æ–° token</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> layer_idx, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(decoder_layers):</span><br><span class="line">            Q = X @ W_Q  <span class="comment"># åªç®—å½“å‰ token çš„ Q</span></span><br><span class="line">            K_new = X @ W_K</span><br><span class="line">            V_new = X @ W_V</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># æ‹¼æ¥å†å²çš„ Kã€V</span></span><br><span class="line">            <span class="keyword">if</span> kv_cache[layer_idx] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                K = concat(kv_cache[layer_idx][<span class="number">0</span>], K_new)  <span class="comment"># æ‹¼æ¥ï¼</span></span><br><span class="line">                V = concat(kv_cache[layer_idx][<span class="number">1</span>], V_new)  <span class="comment"># æ‹¼æ¥ï¼</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                K = K_new</span><br><span class="line">                V = V_new</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># æ›´æ–°ç¼“å­˜</span></span><br><span class="line">            kv_cache[layer_idx] = (K, V)  <span class="comment"># ä¿å­˜ï¼</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Causal Attention (åªçœ‹å·¦è¾¹)</span></span><br><span class="line">            scores = Q @ K.T  <span class="comment"># [1, seq_len]</span></span><br><span class="line">            weights = softmax(scores)</span><br><span class="line">            X = weights @ V</span><br><span class="line">        </span><br><span class="line">        new_token = predict_next(X)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> generated_text</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="ä¸€å›¾æ€»ç»“"><a href="#ä¸€å›¾æ€»ç»“" class="headerlink" title="ä¸€å›¾æ€»ç»“"></a>ä¸€å›¾æ€»ç»“</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”</span><br><span class="line">â”‚                    BERT vs GPT                              â”‚</span><br><span class="line">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span><br><span class="line">â”‚         BERT             â”‚            GPT                   â”‚</span><br><span class="line">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span><br><span class="line">â”‚  Encoder æ¶æ„             â”‚  Decoder æ¶æ„                    â”‚</span><br><span class="line">â”‚  åŒå‘æ³¨æ„åŠ›               â”‚  å•å‘æ³¨æ„åŠ›ï¼ˆCausalï¼‰             â”‚</span><br><span class="line">â”‚  ä¸€æ¬¡æ€§è¾“å…¥æ•´ä¸ªå¥å­        â”‚  é€ä¸ª token ç”Ÿæˆ                 â”‚</span><br><span class="line">â”‚  è¾“å…¥è¾“å‡ºé•¿åº¦ç›¸åŒ          â”‚  è¾“å‡ºæ¯”è¾“å…¥é•¿                    â”‚</span><br><span class="line">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span><br><span class="line">â”‚  âœ… æœ‰ Qã€Kã€V            â”‚  âœ… æœ‰ Qã€Kã€V                   â”‚</span><br><span class="line">â”‚  âŒ ä¸éœ€è¦ KV Cache       â”‚  âœ… å¿…é¡» KV Cache                â”‚</span><br><span class="line">â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤</span><br><span class="line">â”‚  ç”¨é€”: ç†è§£               â”‚  ç”¨é€”: ç”Ÿæˆ                      â”‚</span><br><span class="line">â”‚  åˆ†ç±»ã€NERã€é—®ç­”          â”‚  èŠå¤©ã€å†™ä½œã€ä»£ç ç”Ÿæˆ             â”‚</span><br><span class="line">â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="è®°å¿†å£è¯€"><a href="#è®°å¿†å£è¯€" class="headerlink" title="è®°å¿†å£è¯€"></a>è®°å¿†å£è¯€</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BERT = é˜…è¯»ç†è§£ = ä¸€çœ¼çœ‹å®Œå…¨æ–‡ = ä¸éœ€è¦ç¼“å­˜</span><br><span class="line">GPT  = å†™ä½œæ–‡   = ä¸€ä¸ªå­—ä¸€ä¸ªå­—å†™ = éœ€è¦è®°ä½å‰é¢å†™äº†ä»€ä¹ˆ</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="â“-Part-9-å¸¸è§é—®ç­”-FAQ"><a href="#â“-Part-9-å¸¸è§é—®ç­”-FAQ" class="headerlink" title="â“ Part 9: å¸¸è§é—®ç­” FAQ"></a>â“ Part 9: å¸¸è§é—®ç­” FAQ</h2><h3 id="Q1-MASK-Token-ä¸ºä»€ä¹ˆèƒ½é¢„æµ‹å‡ºæ­£ç¡®ç­”æ¡ˆï¼Ÿ"><a href="#Q1-MASK-Token-ä¸ºä»€ä¹ˆèƒ½é¢„æµ‹å‡ºæ­£ç¡®ç­”æ¡ˆï¼Ÿ" class="headerlink" title="Q1: [MASK] Token ä¸ºä»€ä¹ˆèƒ½é¢„æµ‹å‡ºæ­£ç¡®ç­”æ¡ˆï¼Ÿ"></a>Q1: [MASK] Token ä¸ºä»€ä¹ˆèƒ½é¢„æµ‹å‡ºæ­£ç¡®ç­”æ¡ˆï¼Ÿ</h3><p><strong>A</strong>: [MASK] çš„ Token ID (103) æœ¬èº«ä¸ä»£è¡¨ä»»ä½•è¯­ä¹‰ï¼Œä½†ç»è¿‡ 12 å±‚ Self-Attention åï¼š</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Embedding:  103 â†’ [0.01, 0.02, ...]  ç©ºå£³</span><br><span class="line">    â†“ Layer 1: å¸æ”¶ "çˆ±" å’Œ "å¤©å®‰é—¨" çš„ä¿¡æ¯</span><br><span class="line">Layer 1:    [0.52, 0.71, ...]  å¼€å§‹æœ‰è¯­ä¹‰</span><br><span class="line">    â†“ Layer 2-12: ä¸æ–­ç²¾ç‚¼</span><br><span class="line">Layer 12:   [0.93, 0.87, ...]  å®Œå…¨ç†è§£ä¸Šä¸‹æ–‡</span><br><span class="line"></span><br><span class="line">æœ€ç»ˆè¿™ä¸ªå‘é‡åœ¨è¯­ä¹‰ç©ºé—´ä¸­æ¥è¿‘ "åŒ—äº¬"ï¼</span><br></pre></td></tr></tbody></table></figure>

<p><strong>å…³é”®</strong>: Self-Attention è®© [MASK] ä»å‘¨å›´è¯â€å·â€ä¿¡æ¯ï¼</p>
<hr>
<h3 id="Q2-BERT-ä¸ºä»€ä¹ˆåªç”¨-Encoderï¼Œä¸ç”¨-Decoderï¼Ÿ"><a href="#Q2-BERT-ä¸ºä»€ä¹ˆåªç”¨-Encoderï¼Œä¸ç”¨-Decoderï¼Ÿ" class="headerlink" title="Q2: BERT ä¸ºä»€ä¹ˆåªç”¨ Encoderï¼Œä¸ç”¨ Decoderï¼Ÿ"></a>Q2: BERT ä¸ºä»€ä¹ˆåªç”¨ Encoderï¼Œä¸ç”¨ Decoderï¼Ÿ</h3><p><strong>A</strong>: å› ä¸º BERT çš„ä»»åŠ¡æ˜¯â€ç†è§£â€ï¼Œä¸æ˜¯â€ç”Ÿæˆâ€</p>
<table>
<thead>
<tr>
<th>ä»»åŠ¡ç±»å‹</th>
<th>éœ€è¦ Decoderï¼Ÿ</th>
<th>åŸå› </th>
</tr>
</thead>
<tbody><tr>
<td><strong>æœºå™¨ç¿»è¯‘</strong></td>
<td>âœ… éœ€è¦</td>
<td>è¾“å…¥ä¸­æ–‡ï¼Œè¾“å‡ºè‹±æ–‡ï¼Œæ˜¯ä¸åŒçš„åºåˆ—</td>
</tr>
<tr>
<td><strong>æ–‡æœ¬åˆ†ç±»</strong></td>
<td>âŒ ä¸éœ€è¦</td>
<td>åªéœ€ç†è§£è¾“å…¥ï¼Œè¾“å‡ºä¸€ä¸ªç±»åˆ«</td>
</tr>
<tr>
<td><strong>NER</strong></td>
<td>âŒ ä¸éœ€è¦</td>
<td>åªéœ€ç»™æ¯ä¸ªè¾“å…¥è¯æ‰“æ ‡ç­¾</td>
</tr>
<tr>
<td><strong>é—®ç­”</strong></td>
<td>âŒ ä¸éœ€è¦</td>
<td>ç­”æ¡ˆåœ¨åŸæ–‡ä¸­ï¼Œåªéœ€æ‰¾ä½ç½®</td>
</tr>
</tbody></table>
<p><strong>ç®€å•è¯´</strong>: Encoder ç†è§£è¾“å…¥ï¼ŒDecoder ç”Ÿæˆè¾“å‡ºã€‚BERT åªéœ€è¦ç†è§£ï¼</p>
<hr>
<h3 id="Q3-æ®‹å·®è¿æ¥æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ"><a href="#Q3-æ®‹å·®è¿æ¥æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ" class="headerlink" title="Q3: æ®‹å·®è¿æ¥æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ"></a>Q3: æ®‹å·®è¿æ¥æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ</h3><p><strong>A</strong>: æ®‹å·®è¿æ¥å°±æ˜¯æŠŠè¾“å…¥ç›´æ¥åŠ åˆ°è¾“å‡ºä¸Š</p>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ä¸ç”¨æ®‹å·®</span></span><br><span class="line">output = FFN(<span class="built_in">input</span>)  <span class="comment"># å¯èƒ½ä¸¢å¤±åŸå§‹ä¿¡æ¯</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ç”¨æ®‹å·®</span></span><br><span class="line">output = <span class="built_in">input</span> + FFN(<span class="built_in">input</span>)  <span class="comment"># ä¿åº• + å¢é‡</span></span><br></pre></td></tr></tbody></table></figure>

<p><strong>å¥½å¤„</strong>:</p>
<ol>
<li>æ¢¯åº¦å¯ä»¥ç›´æ¥è·³è¿‡ FFN å›ä¼  â†’ è§£å†³æ¢¯åº¦æ¶ˆå¤±</li>
<li>åŸå§‹ä¿¡æ¯ä¸ä¼šä¸¢å¤± â†’ ç½‘ç»œå¯ä»¥æ›´æ·±</li>
<li>FFN åªéœ€å­¦ä¹ â€å·®å¼‚â€ â†’ æ›´å®¹æ˜“è®­ç»ƒ</li>
</ol>
<hr>
<h3 id="Q4-é¢„è®­ç»ƒå’Œå¾®è°ƒæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"><a href="#Q4-é¢„è®­ç»ƒå’Œå¾®è°ƒæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ" class="headerlink" title="Q4: é¢„è®­ç»ƒå’Œå¾®è°ƒæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"></a>Q4: é¢„è®­ç»ƒå’Œå¾®è°ƒæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ</h3><p><strong>A</strong>: </p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">é¢„è®­ç»ƒï¼ˆPre-trainingï¼‰:</span><br><span class="line">  - æ•°æ®: æµ·é‡æ— æ ‡æ³¨æ–‡æœ¬ (æ•°åäº¿è¯)</span><br><span class="line">  - ä»»åŠ¡: MLM å®Œå½¢å¡«ç©º</span><br><span class="line">  - ç›®çš„: å­¦ä¼š"ç†è§£è¯­è¨€"</span><br><span class="line">  - è€—æ—¶: æ•°å‘¨ (åªåšä¸€æ¬¡)</span><br><span class="line">  </span><br><span class="line">å¾®è°ƒï¼ˆFine-tuningï¼‰:</span><br><span class="line">  - æ•°æ®: å°‘é‡æ ‡æ³¨æ•°æ® (1000æ¡)</span><br><span class="line">  - ä»»åŠ¡: å…·ä½“ä»»åŠ¡ (æƒ…æ„Ÿåˆ†æã€NER...)</span><br><span class="line">  - ç›®çš„: é€‚é…ç‰¹å®šä»»åŠ¡</span><br><span class="line">  - è€—æ—¶: å‡ å°æ—¶ (æ¯ä¸ªä»»åŠ¡éƒ½è¦)</span><br></pre></td></tr></tbody></table></figure>

<p><strong>ç±»æ¯”</strong>: é¢„è®­ç»ƒ = ä¸Šå¤§å­¦ï¼Œå¾®è°ƒ = å²—ä½åŸ¹è®­</p>
<hr>
<h3 id="Q5-MLM-ä¸ºä»€ä¹ˆè¦-80-x2F-10-x2F-10-çš„ç­–ç•¥ï¼Ÿ"><a href="#Q5-MLM-ä¸ºä»€ä¹ˆè¦-80-x2F-10-x2F-10-çš„ç­–ç•¥ï¼Ÿ" class="headerlink" title="Q5: MLM ä¸ºä»€ä¹ˆè¦ 80%/10%/10% çš„ç­–ç•¥ï¼Ÿ"></a>Q5: MLM ä¸ºä»€ä¹ˆè¦ 80%/10%/10% çš„ç­–ç•¥ï¼Ÿ</h3><p><strong>A</strong>: </p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">100% [MASK]: é¢„è®­ç»ƒå’Œå¾®è°ƒåˆ†å¸ƒä¸ä¸€è‡´</span><br><span class="line">  é¢„è®­ç»ƒ: "æˆ‘çˆ±[MASK]å¤©å®‰é—¨"</span><br><span class="line">  å¾®è°ƒ:   "æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨"  â† æ²¡æœ‰ [MASK]ï¼</span><br><span class="line">  é—®é¢˜: æ¨¡å‹è¿‡åº¦ä¾èµ– [MASK] ç¬¦å·</span><br><span class="line"></span><br><span class="line">80% [MASK] + 10% éšæœº + 10% ä¸å˜:</span><br><span class="line">  - 80% [MASK]: ä¸»è¦å­¦ä¹ ç›®æ ‡</span><br><span class="line">  - 10% éšæœº: è®©æ¨¡å‹å­¦ä¼šçº é”™</span><br><span class="line">  - 10% ä¸å˜: é€‚é…çœŸå®åˆ†å¸ƒ</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="Q6-BERT-çš„-Qã€Kã€V-æ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ"><a href="#Q6-BERT-çš„-Qã€Kã€V-æ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ" class="headerlink" title="Q6: BERT çš„ Qã€Kã€V æ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ"></a>Q6: BERT çš„ Qã€Kã€V æ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ</h3><p><strong>A</strong>: </p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Q (Query):  "æˆ‘æƒ³æ‰¾ä»€ä¹ˆä¿¡æ¯ï¼Ÿ"</span><br><span class="line">K (Key):    "æˆ‘è¿™é‡Œæœ‰ä»€ä¹ˆä¿¡æ¯ï¼Ÿ"</span><br><span class="line">V (Value):  "å®é™…çš„ä¿¡æ¯å†…å®¹"</span><br><span class="line"></span><br><span class="line">Attention = æ ¹æ® Q å’Œ K çš„ç›¸ä¼¼åº¦ï¼ŒåŠ æƒæ±‚å’Œ V</span><br><span class="line"></span><br><span class="line">ä¾‹å­: [MASK] ä½ç½®</span><br><span class="line">  Q_mask: "æˆ‘éœ€è¦çŸ¥é“è¿™ä¸ªç©ºå¡«ä»€ä¹ˆ"</span><br><span class="line">  K_å¤©å®‰é—¨: "æˆ‘æ˜¯å¤©å®‰é—¨"</span><br><span class="line">  ç›¸ä¼¼åº¦é«˜ â†’ [MASK] å¤šçœ‹ V_å¤©å®‰é—¨</span><br><span class="line">  â†’ [MASK] è·å¾—"å¤©å®‰é—¨åœ¨åŒ—äº¬"çš„ä¿¡æ¯</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="Q7-Encoder-å’Œ-Decoder-çš„æ³¨æ„åŠ›æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"><a href="#Q7-Encoder-å’Œ-Decoder-çš„æ³¨æ„åŠ›æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ" class="headerlink" title="Q7: Encoder å’Œ Decoder çš„æ³¨æ„åŠ›æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ"></a>Q7: Encoder å’Œ Decoder çš„æ³¨æ„åŠ›æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ</h3><p><strong>A</strong>: </p>
<table>
<thead>
<tr>
<th>ç»´åº¦</th>
<th>Encoder</th>
<th>Decoder</th>
</tr>
</thead>
<tbody><tr>
<td><strong>å¯è§èŒƒå›´</strong></td>
<td>å…¨å±€å¯è§ï¼ˆåŒå‘ï¼‰</td>
<td>åªçœ‹å·¦è¾¹ï¼ˆå•å‘ï¼‰</td>
</tr>
<tr>
<td><strong>Mask</strong></td>
<td>æ—  mask æˆ–ä»… padding</td>
<td>Causal mask (ä¸‹ä¸‰è§’)</td>
</tr>
<tr>
<td><strong>ç”¨é€”</strong></td>
<td>ç†è§£æ•´ä¸ªå¥å­</td>
<td>ç”Ÿæˆä¸‹ä¸€ä¸ªè¯</td>
</tr>
<tr>
<td><strong>KV Cache</strong></td>
<td>âŒ ä¸éœ€è¦</td>
<td>âœ… éœ€è¦</td>
</tr>
</tbody></table>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Encoder: [CLS] å¯ä»¥çœ‹åˆ° "æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨" æ‰€æœ‰è¯</span><br><span class="line">Decoder: "åŒ—äº¬" åªèƒ½çœ‹åˆ° "æˆ‘çˆ±åŒ—äº¬"ï¼Œä¸èƒ½çœ‹åˆ° "å¤©å®‰é—¨"</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="Q8-ä¸ºä»€ä¹ˆ-BERT-éœ€è¦-CLS-å’Œ-SEP-Tokenï¼Ÿ"><a href="#Q8-ä¸ºä»€ä¹ˆ-BERT-éœ€è¦-CLS-å’Œ-SEP-Tokenï¼Ÿ" class="headerlink" title="Q8: ä¸ºä»€ä¹ˆ BERT éœ€è¦ [CLS] å’Œ [SEP] Tokenï¼Ÿ"></a>Q8: ä¸ºä»€ä¹ˆ BERT éœ€è¦ [CLS] å’Œ [SEP] Tokenï¼Ÿ</h3><p><strong>A</strong>: </p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[CLS] (Classification):</span><br><span class="line">  - ä½ç½®: å¥å­å¼€å¤´</span><br><span class="line">  - ä½œç”¨: æ±‡èšæ•´ä¸ªå¥å­çš„è¯­ä¹‰</span><br><span class="line">  - ç”¨é€”: åˆ†ç±»ä»»åŠ¡å– [CLS] çš„å‘é‡</span><br><span class="line"></span><br><span class="line">[SEP] (Separator):</span><br><span class="line">  - ä½ç½®: å¥å­ç»“å°¾ï¼Œæˆ–ä¸¤ä¸ªå¥å­ä¹‹é—´</span><br><span class="line">  - ä½œç”¨: åˆ†éš”ä¸åŒå¥å­</span><br><span class="line">  - ç”¨é€”: è®©æ¨¡å‹çŸ¥é“å¥å­è¾¹ç•Œ</span><br><span class="line"></span><br><span class="line">ä¾‹å­:</span><br><span class="line">  å•å¥: [CLS] è¿™ä¸ªäº§å“å¾ˆå¥½ [SEP]</span><br><span class="line">  å¥å¯¹: [CLS] å¤©åœ¨ä¸‹é›¨ [SEP] åœ°é¢æ˜¯æ¹¿çš„ [SEP]</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="Q9-BERT-çš„å±‚æ•°è¶Šå¤šè¶Šå¥½å—ï¼Ÿ"><a href="#Q9-BERT-çš„å±‚æ•°è¶Šå¤šè¶Šå¥½å—ï¼Ÿ" class="headerlink" title="Q9: BERT çš„å±‚æ•°è¶Šå¤šè¶Šå¥½å—ï¼Ÿ"></a>Q9: BERT çš„å±‚æ•°è¶Šå¤šè¶Šå¥½å—ï¼Ÿ</h3><p><strong>A</strong>: ä¸ä¸€å®šï¼</p>
<table>
<thead>
<tr>
<th>æ¨¡å‹</th>
<th>å±‚æ•°</th>
<th>å‚æ•°é‡</th>
<th>æ€§èƒ½</th>
<th>é—®é¢˜</th>
</tr>
</thead>
<tbody><tr>
<td>BERT-Base</td>
<td>12</td>
<td>110M</td>
<td>84.6%</td>
<td>-</td>
</tr>
<tr>
<td>BERT-Large</td>
<td>24</td>
<td>340M</td>
<td>86.7%</td>
<td>è®­ç»ƒæ…¢ã€éœ€è¦æ›´å¤šæ•°æ®</td>
</tr>
<tr>
<td>BERT-è¶…å¤§</td>
<td>48+</td>
<td>1B+</td>
<td>æå‡æœ‰é™</td>
<td>è¿‡æ‹Ÿåˆã€æ¨ç†æ…¢</td>
</tr>
</tbody></table>
<p><strong>ç»“è®º</strong>: 12-24 å±‚æ˜¯ç”œèœœç‚¹ï¼Œæ›´å¤šå±‚è¾¹é™…æ”¶ç›Šé€’å‡</p>
<hr>
<h3 id="Q10-BERT-å’Œ-GPT-å¯ä»¥ç»“åˆä½¿ç”¨å—ï¼Ÿ"><a href="#Q10-BERT-å’Œ-GPT-å¯ä»¥ç»“åˆä½¿ç”¨å—ï¼Ÿ" class="headerlink" title="Q10: BERT å’Œ GPT å¯ä»¥ç»“åˆä½¿ç”¨å—ï¼Ÿ"></a>Q10: BERT å’Œ GPT å¯ä»¥ç»“åˆä½¿ç”¨å—ï¼Ÿ</h3><p><strong>A</strong>: å¯ä»¥ï¼è¿™å°±æ˜¯ Encoder-Decoder æ¶æ„</p>
<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">T5 / BART:</span><br><span class="line">  è¾“å…¥ â†’ BERT-like Encoder (ç†è§£)</span><br><span class="line">      â†’ GPT-like Decoder (ç”Ÿæˆ)</span><br><span class="line">      â†’ è¾“å‡º</span><br><span class="line"></span><br><span class="line">é€‚ç”¨åœºæ™¯:</span><br><span class="line">  - æœºå™¨ç¿»è¯‘</span><br><span class="line">  - æ–‡æœ¬æ‘˜è¦</span><br><span class="line">  - å¯¹è¯ç”Ÿæˆ</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="ğŸ”—-å»¶ä¼¸é˜…è¯»"><a href="#ğŸ”—-å»¶ä¼¸é˜…è¯»" class="headerlink" title="ğŸ”— å»¶ä¼¸é˜…è¯»"></a>ğŸ”— å»¶ä¼¸é˜…è¯»</h2><ol>
<li><strong>è®ºæ–‡åŸæ–‡</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1810.04805">BERT (arxiv.org/abs/1810.04805)</a></li>
<li><strong>åç»­æ”¹è¿›</strong>:<ul>
<li>RoBERTa: ç§»é™¤ NSP, æ›´å¤§æ‰¹æ¬¡è®­ç»ƒ</li>
<li>ALBERT: å‚æ•°å…±äº«, å‡å°‘æ¨¡å‹å¤§å°</li>
<li>ELECTRA: åˆ¤åˆ«å¼é¢„è®­ç»ƒ, æ›´é«˜æ•ˆ</li>
</ul>
</li>
<li><strong>KV Cache ä¼˜åŒ–</strong>:<ul>
<li>PagedAttention: vLLM çš„æ ¸å¿ƒæŠ€æœ¯</li>
<li>FlashAttention: IO ä¼˜åŒ–çš„æ³¨æ„åŠ›ç®—æ³•</li>
</ul>
</li>
<li><strong>Transformer åŸæ–‡</strong>: <a target="_blank" rel="noopener" href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li>
</ol>
<hr>
<h2 id="ğŸš€-Part-6-BERT-å®é™…åº”ç”¨åœºæ™¯ä¸ä»£ç å®ç°"><a href="#ğŸš€-Part-6-BERT-å®é™…åº”ç”¨åœºæ™¯ä¸ä»£ç å®ç°" class="headerlink" title="ğŸš€ Part 6: BERT å®é™…åº”ç”¨åœºæ™¯ä¸ä»£ç å®ç°"></a>ğŸš€ Part 6: BERT å®é™…åº”ç”¨åœºæ™¯ä¸ä»£ç å®ç°</h2><h3 id="6-1-åº”ç”¨åœºæ™¯æ€»è§ˆ"><a href="#6-1-åº”ç”¨åœºæ™¯æ€»è§ˆ" class="headerlink" title="6.1 åº”ç”¨åœºæ™¯æ€»è§ˆ"></a>6.1 åº”ç”¨åœºæ™¯æ€»è§ˆ</h3><table>
<thead>
<tr>
<th>åº”ç”¨åœºæ™¯</th>
<th>ä»»åŠ¡ç±»å‹</th>
<th>è¾“å…¥æ ¼å¼</th>
<th>è¾“å‡º</th>
<th>å…¸å‹åº”ç”¨</th>
</tr>
</thead>
<tbody><tr>
<td>æƒ…æ„Ÿåˆ†æ</td>
<td>å•å¥åˆ†ç±»</td>
<td><code>[CLS] æ–‡æœ¬ [SEP]</code></td>
<td>ç±»åˆ«æ ‡ç­¾</td>
<td>å•†å“è¯„è®ºã€èˆ†æƒ…ç›‘æ§</td>
</tr>
<tr>
<td>æ–‡æœ¬åŒ¹é…</td>
<td>å¥å¯¹åˆ†ç±»</td>
<td><code>[CLS] å¥A [SEP] å¥B [SEP]</code></td>
<td>ç›¸ä¼¼åº¦/å…³ç³»</td>
<td>æ™ºèƒ½å®¢æœã€é—®ç­”åŒ¹é…</td>
</tr>
<tr>
<td>å‘½åå®ä½“è¯†åˆ«</td>
<td>åºåˆ—æ ‡æ³¨</td>
<td><code>[CLS] æ–‡æœ¬ [SEP]</code></td>
<td>æ¯ä¸ªtokenæ ‡ç­¾</td>
<td>ä¿¡æ¯æŠ½å–ã€çŸ¥è¯†å›¾è°±</td>
</tr>
<tr>
<td>é˜…è¯»ç†è§£</td>
<td>æŠ½å–å¼QA</td>
<td><code>[CLS] é—®é¢˜ [SEP] æ–‡ç«  [SEP]</code></td>
<td>ç­”æ¡ˆèµ·æ­¢ä½ç½®</td>
<td>æ™ºèƒ½é—®ç­”ã€å®¢æœæœºå™¨äºº</td>
</tr>
<tr>
<td>æ–‡æœ¬ç”Ÿæˆ</td>
<td>Seq2Seq</td>
<td>éœ€é…åˆDecoder</td>
<td>ç”Ÿæˆæ–‡æœ¬</td>
<td>æ‘˜è¦ã€ç¿»è¯‘ (éœ€BART/T5)</td>
</tr>
</tbody></table>
<hr>
<h3 id="6-2-æƒ…æ„Ÿåˆ†æå®Œæ•´å®ç°"><a href="#6-2-æƒ…æ„Ÿåˆ†æå®Œæ•´å®ç°" class="headerlink" title="6.2 æƒ…æ„Ÿåˆ†æå®Œæ•´å®ç°"></a>6.2 æƒ…æ„Ÿåˆ†æå®Œæ•´å®ç°</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">åœºæ™¯: ç”µå•†è¯„è®ºæƒ…æ„Ÿåˆ†æ</span></span><br><span class="line"><span class="string">è¾“å…¥: "è¿™ä¸ªæ‰‹æœºæ‹ç…§æ•ˆæœå¾ˆæ£’ï¼Œç”µæ± ä¹Ÿè€ç”¨"</span></span><br><span class="line"><span class="string">è¾“å‡º: æ­£é¢ (0.95)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForSequenceClassification</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. åŠ è½½æ¨¡å‹</span></span><br><span class="line">model_name = <span class="string">"bert-base-chinese"</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(model_name)</span><br><span class="line">model = BertForSequenceClassification.from_pretrained(</span><br><span class="line">    model_name, </span><br><span class="line">    num_labels=<span class="number">2</span>  <span class="comment"># æ­£é¢/è´Ÿé¢</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. æ•°æ®å‡†å¤‡</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_data</span>(<span class="params">texts, labels</span>):</span><br><span class="line">    encodings = tokenizer(</span><br><span class="line">        texts,</span><br><span class="line">        padding=<span class="literal">True</span>,</span><br><span class="line">        truncation=<span class="literal">True</span>,</span><br><span class="line">        max_length=<span class="number">128</span>,</span><br><span class="line">        return_tensors=<span class="string">"pt"</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> encodings, torch.tensor(labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ç¤ºä¾‹æ•°æ®</span></span><br><span class="line">train_texts = [</span><br><span class="line">    <span class="string">"è¿™ä¸ªæ‰‹æœºæ‹ç…§æ•ˆæœå¾ˆæ£’ï¼Œç”µæ± ä¹Ÿè€ç”¨"</span>,</span><br><span class="line">    <span class="string">"è´¨é‡å¤ªå·®äº†ï¼Œç”¨äº†ä¸€å¤©å°±åäº†"</span>,</span><br><span class="line">    <span class="string">"ç‰©æµå¾ˆå¿«ï¼ŒåŒ…è£…å®Œå¥½ï¼Œå¥½è¯„"</span>,</span><br><span class="line">    <span class="string">"å®¢æœæ€åº¦æ¶åŠ£ï¼Œå†ä¹Ÿä¸ä¹°äº†"</span></span><br><span class="line">]</span><br><span class="line">train_labels = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]  <span class="comment"># 1=æ­£é¢, 0=è´Ÿé¢</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. è®­ç»ƒå¾ªç¯</span></span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> AdamW</span><br><span class="line"></span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">2e-5</span>)</span><br><span class="line">model.train()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    encodings, labels = prepare_data(train_texts, train_labels)</span><br><span class="line">    </span><br><span class="line">    outputs = model(</span><br><span class="line">        input_ids=encodings[<span class="string">"input_ids"</span>],</span><br><span class="line">        attention_mask=encodings[<span class="string">"attention_mask"</span>],</span><br><span class="line">        labels=labels</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    loss = outputs.loss</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Epoch <span class="subst">{epoch+<span class="number">1</span>}</span>, Loss: <span class="subst">{loss.item():<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. æ¨ç†é¢„æµ‹</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">test_text = <span class="string">"è¿™æ¬¾äº§å“æ€§ä»·æ¯”è¶…é«˜ï¼Œå¼ºçƒˆæ¨èï¼"</span></span><br><span class="line">inputs = tokenizer(test_text, return_tensors=<span class="string">"pt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = model(**inputs)</span><br><span class="line">    probs = torch.softmax(outputs.logits, dim=-<span class="number">1</span>)</span><br><span class="line">    pred = torch.argmax(probs, dim=-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"é¢„æµ‹: <span class="subst">{<span class="string">'æ­£é¢'</span> <span class="keyword">if</span> pred==<span class="number">1</span> <span class="keyword">else</span> <span class="string">'è´Ÿé¢'</span>}</span>, ç½®ä¿¡åº¦: <span class="subst">{probs[<span class="number">0</span>][pred].item():<span class="number">.2</span>%}</span>"</span>)</span><br><span class="line"><span class="comment"># è¾“å‡º: é¢„æµ‹: æ­£é¢, ç½®ä¿¡åº¦: 94.32%</span></span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="6-3-å‘½åå®ä½“è¯†åˆ«-NER-å®ç°"><a href="#6-3-å‘½åå®ä½“è¯†åˆ«-NER-å®ç°" class="headerlink" title="6.3 å‘½åå®ä½“è¯†åˆ« (NER) å®ç°"></a>6.3 å‘½åå®ä½“è¯†åˆ« (NER) å®ç°</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">åœºæ™¯: ä»æ–°é—»ä¸­æå–äººåã€åœ°åã€æœºæ„å</span></span><br><span class="line"><span class="string">è¾“å…¥: "é©¬äº‘åœ¨æ­å·åˆ›åŠäº†é˜¿é‡Œå·´å·´å…¬å¸"</span></span><br><span class="line"><span class="string">è¾“å‡º: [("é©¬äº‘", "PER"), ("æ­å·", "LOC"), ("é˜¿é‡Œå·´å·´å…¬å¸", "ORG")]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizerFast, BertForTokenClassification</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># NER æ ‡ç­¾å®šä¹‰ (BIOæ ¼å¼)</span></span><br><span class="line">label_list = [<span class="string">"O"</span>, <span class="string">"B-PER"</span>, <span class="string">"I-PER"</span>, <span class="string">"B-LOC"</span>, <span class="string">"I-LOC"</span>, <span class="string">"B-ORG"</span>, <span class="string">"I-ORG"</span>]</span><br><span class="line">label2id = {l: i <span class="keyword">for</span> i, l <span class="keyword">in</span> <span class="built_in">enumerate</span>(label_list)}</span><br><span class="line">id2label = {i: l <span class="keyword">for</span> i, l <span class="keyword">in</span> <span class="built_in">enumerate</span>(label_list)}</span><br><span class="line"></span><br><span class="line"><span class="comment"># åŠ è½½æ¨¡å‹</span></span><br><span class="line">tokenizer = BertTokenizerFast.from_pretrained(<span class="string">"bert-base-chinese"</span>)</span><br><span class="line">model = BertForTokenClassification.from_pretrained(</span><br><span class="line">    <span class="string">"bert-base-chinese"</span>,</span><br><span class="line">    num_labels=<span class="built_in">len</span>(label_list),</span><br><span class="line">    id2label=id2label,</span><br><span class="line">    label2id=label2id</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨ç†å‡½æ•°</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_entities</span>(<span class="params">text</span>):</span><br><span class="line">    inputs = tokenizer(text, return_tensors=<span class="string">"pt"</span>, return_offsets_mapping=<span class="literal">True</span>)</span><br><span class="line">    offset_mapping = inputs.pop(<span class="string">"offset_mapping"</span>)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">        predictions = torch.argmax(outputs.logits, dim=-<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    entities = []</span><br><span class="line">    current_entity = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> idx, (pred, offset) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(predictions, offset_mapping)):</span><br><span class="line">        <span class="keyword">if</span> offset[<span class="number">0</span>] == offset[<span class="number">1</span>]:  <span class="comment"># è·³è¿‡ç‰¹æ®Štoken</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">        label = id2label[pred.item()]</span><br><span class="line">        char = text[offset[<span class="number">0</span>]:offset[<span class="number">1</span>]]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> label.startswith(<span class="string">"B-"</span>):</span><br><span class="line">            <span class="keyword">if</span> current_entity:</span><br><span class="line">                entities.append(current_entity)</span><br><span class="line">            current_entity = {<span class="string">"text"</span>: char, <span class="string">"type"</span>: label[<span class="number">2</span>:]}</span><br><span class="line">        <span class="keyword">elif</span> label.startswith(<span class="string">"I-"</span>) <span class="keyword">and</span> current_entity:</span><br><span class="line">            current_entity[<span class="string">"text"</span>] += char</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> current_entity:</span><br><span class="line">                entities.append(current_entity)</span><br><span class="line">                current_entity = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> current_entity:</span><br><span class="line">        entities.append(current_entity)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> [(e[<span class="string">"text"</span>], e[<span class="string">"type"</span>]) <span class="keyword">for</span> e <span class="keyword">in</span> entities]</span><br><span class="line"></span><br><span class="line"><span class="comment"># æµ‹è¯•</span></span><br><span class="line">text = <span class="string">"é©¬äº‘åœ¨æ­å·åˆ›åŠäº†é˜¿é‡Œå·´å·´å…¬å¸"</span></span><br><span class="line"><span class="built_in">print</span>(extract_entities(text))</span><br><span class="line"><span class="comment"># è¾“å‡º: [("é©¬äº‘", "PER"), ("æ­å·", "LOC"), ("é˜¿é‡Œå·´å·´å…¬å¸", "ORG")]</span></span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="6-4-è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…"><a href="#6-4-è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…" class="headerlink" title="6.4 è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…"></a>6.4 è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">åœºæ™¯: æ™ºèƒ½å®¢æœFAQåŒ¹é…</span></span><br><span class="line"><span class="string">è¾“å…¥: ç”¨æˆ·é—®é¢˜ + FAQåº“</span></span><br><span class="line"><span class="string">è¾“å‡º: æœ€ç›¸ä¼¼çš„FAQåŠç­”æ¡ˆ</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">"bert-base-chinese"</span>)</span><br><span class="line">model = BertModel.from_pretrained(<span class="string">"bert-base-chinese"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_sentence_embedding</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">"""è·å–å¥å­çš„BERTè¡¨ç¤º (ä½¿ç”¨[CLS]å‘é‡)"""</span></span><br><span class="line">    inputs = tokenizer(text, return_tensors=<span class="string">"pt"</span>, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">    <span class="comment"># ä½¿ç”¨ [CLS] token çš„è¾“å‡ºä½œä¸ºå¥å­è¡¨ç¤º</span></span><br><span class="line">    <span class="keyword">return</span> outputs.last_hidden_state[:, <span class="number">0</span>, :]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_similarity</span>(<span class="params">text1, text2</span>):</span><br><span class="line">    <span class="string">"""è®¡ç®—ä¸¤ä¸ªå¥å­çš„ä½™å¼¦ç›¸ä¼¼åº¦"""</span></span><br><span class="line">    emb1 = get_sentence_embedding(text1)</span><br><span class="line">    emb2 = get_sentence_embedding(text2)</span><br><span class="line">    <span class="keyword">return</span> F.cosine_similarity(emb1, emb2).item()</span><br><span class="line"></span><br><span class="line"><span class="comment"># FAQåº“</span></span><br><span class="line">faq_database = [</span><br><span class="line">    {<span class="string">"question"</span>: <span class="string">"å¦‚ä½•ä¿®æ”¹å¯†ç ï¼Ÿ"</span>, <span class="string">"answer"</span>: <span class="string">"è¯·è¿›å…¥è®¾ç½®-è´¦æˆ·å®‰å…¨-ä¿®æ”¹å¯†ç "</span>},</span><br><span class="line">    {<span class="string">"question"</span>: <span class="string">"æ€ä¹ˆç”³è¯·é€€æ¬¾ï¼Ÿ"</span>, <span class="string">"answer"</span>: <span class="string">"åœ¨è®¢å•è¯¦æƒ…é¡µç‚¹å‡»ç”³è¯·é€€æ¬¾æŒ‰é’®"</span>},</span><br><span class="line">    {<span class="string">"question"</span>: <span class="string">"é…é€éœ€è¦å¤šä¹…ï¼Ÿ"</span>, <span class="string">"answer"</span>: <span class="string">"ä¸€èˆ¬3-5ä¸ªå·¥ä½œæ—¥é€è¾¾"</span>},</span><br><span class="line">    {<span class="string">"question"</span>: <span class="string">"æ”¯æŒå“ªäº›æ”¯ä»˜æ–¹å¼ï¼Ÿ"</span>, <span class="string">"answer"</span>: <span class="string">"æ”¯æŒå¾®ä¿¡ã€æ”¯ä»˜å®ã€é“¶è¡Œå¡æ”¯ä»˜"</span>},</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">find_best_match</span>(<span class="params">user_query</span>):</span><br><span class="line">    <span class="string">"""æ‰¾åˆ°æœ€åŒ¹é…çš„FAQ"""</span></span><br><span class="line">    best_score = -<span class="number">1</span></span><br><span class="line">    best_faq = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> faq <span class="keyword">in</span> faq_database:</span><br><span class="line">        score = compute_similarity(user_query, faq[<span class="string">"question"</span>])</span><br><span class="line">        <span class="keyword">if</span> score &gt; best_score:</span><br><span class="line">            best_score = score</span><br><span class="line">            best_faq = faq</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> best_faq, best_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># æµ‹è¯•</span></span><br><span class="line">query = <span class="string">"æˆ‘æƒ³æ”¹ä¸€ä¸‹ç™»å½•å¯†ç "</span></span><br><span class="line">faq, score = find_best_match(query)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"ç”¨æˆ·é—®é¢˜: <span class="subst">{query}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"åŒ¹é…FAQ: <span class="subst">{faq[<span class="string">'question'</span>]}</span> (ç›¸ä¼¼åº¦: <span class="subst">{score:<span class="number">.2</span>%}</span>)"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"å›ç­”: <span class="subst">{faq[<span class="string">'answer'</span>]}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è¾“å‡º:</span></span><br><span class="line"><span class="comment"># ç”¨æˆ·é—®é¢˜: æˆ‘æƒ³æ”¹ä¸€ä¸‹ç™»å½•å¯†ç </span></span><br><span class="line"><span class="comment"># åŒ¹é…FAQ: å¦‚ä½•ä¿®æ”¹å¯†ç ï¼Ÿ (ç›¸ä¼¼åº¦: 89.34%)</span></span><br><span class="line"><span class="comment"># å›ç­”: è¯·è¿›å…¥è®¾ç½®-è´¦æˆ·å®‰å…¨-ä¿®æ”¹å¯†ç </span></span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="6-5-é˜…è¯»ç†è§£é—®ç­”ç³»ç»Ÿ"><a href="#6-5-é˜…è¯»ç†è§£é—®ç­”ç³»ç»Ÿ" class="headerlink" title="6.5 é˜…è¯»ç†è§£é—®ç­”ç³»ç»Ÿ"></a>6.5 é˜…è¯»ç†è§£é—®ç­”ç³»ç»Ÿ</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">åœºæ™¯: ä»æ–‡æ¡£ä¸­æ‰¾ç­”æ¡ˆ</span></span><br><span class="line"><span class="string">è¾“å…¥: é—®é¢˜ + æ–‡ç« </span></span><br><span class="line"><span class="string">è¾“å‡º: ç­”æ¡ˆæ–‡æœ¬åŠä½ç½®</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizerFast, BertForQuestionAnswering</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizerFast.from_pretrained(<span class="string">"bert-base-chinese"</span>)</span><br><span class="line">model = BertForQuestionAnswering.from_pretrained(</span><br><span class="line">    <span class="string">"luhua/chinese_pretrain_mrc_roberta_wwm_ext_large"</span>  <span class="comment"># ä¸­æ–‡QAæ¨¡å‹</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">answer_question</span>(<span class="params">question, context</span>):</span><br><span class="line">    <span class="string">"""ä»æ–‡ç« ä¸­æŠ½å–ç­”æ¡ˆ"""</span></span><br><span class="line">    inputs = tokenizer(</span><br><span class="line">        question, </span><br><span class="line">        context, </span><br><span class="line">        return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">        max_length=<span class="number">512</span>,</span><br><span class="line">        truncation=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># è·å–ç­”æ¡ˆèµ·æ­¢ä½ç½®</span></span><br><span class="line">    start_idx = torch.argmax(outputs.start_logits)</span><br><span class="line">    end_idx = torch.argmax(outputs.end_logits)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># è§£ç ç­”æ¡ˆ</span></span><br><span class="line">    tokens = tokenizer.convert_ids_to_tokens(inputs[<span class="string">"input_ids"</span>][<span class="number">0</span>])</span><br><span class="line">    answer = tokenizer.convert_tokens_to_string(tokens[start_idx:end_idx+<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># è®¡ç®—ç½®ä¿¡åº¦</span></span><br><span class="line">    start_prob = torch.softmax(outputs.start_logits, dim=-<span class="number">1</span>)[<span class="number">0</span>][start_idx].item()</span><br><span class="line">    end_prob = torch.softmax(outputs.end_logits, dim=-<span class="number">1</span>)[<span class="number">0</span>][end_idx].item()</span><br><span class="line">    confidence = (start_prob + end_prob) / <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> answer, confidence</span><br><span class="line"></span><br><span class="line"><span class="comment"># æµ‹è¯•</span></span><br><span class="line">context = <span class="string">"""</span></span><br><span class="line"><span class="string">é˜¿é‡Œå·´å·´é›†å›¢ç”±é©¬äº‘äº1999å¹´åœ¨ä¸­å›½æ­å·åˆ›ç«‹ã€‚</span></span><br><span class="line"><span class="string">å…¬å¸æœ€åˆæ˜¯ä¸€ä¸ªB2Bç½‘ä¸Šäº¤æ˜“å¸‚åœºï¼Œåæ¥å‘å±•æˆä¸ºä¸€ä¸ªå¤šå…ƒåŒ–çš„ç§‘æŠ€å…¬å¸ã€‚</span></span><br><span class="line"><span class="string">2014å¹´ï¼Œé˜¿é‡Œå·´å·´åœ¨çº½çº¦è¯åˆ¸äº¤æ˜“æ‰€ä¸Šå¸‚ï¼Œåˆ›é€ äº†å½“æ—¶å…¨çƒæœ€å¤§çš„IPOçºªå½•ã€‚</span></span><br><span class="line"><span class="string">ç›®å‰é˜¿é‡Œå·´å·´çš„æ ¸å¿ƒä¸šåŠ¡åŒ…æ‹¬ç”µå­å•†åŠ¡ã€äº‘è®¡ç®—ã€æ•°å­—åª’ä½“å’Œå¨±ä¹ã€‚</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">questions = [</span><br><span class="line">    <span class="string">"é˜¿é‡Œå·´å·´æ˜¯è°åˆ›ç«‹çš„ï¼Ÿ"</span>,</span><br><span class="line">    <span class="string">"é˜¿é‡Œå·´å·´æ˜¯å“ªä¸€å¹´ä¸Šå¸‚çš„ï¼Ÿ"</span>,</span><br><span class="line">    <span class="string">"é˜¿é‡Œå·´å·´çš„æ ¸å¿ƒä¸šåŠ¡æœ‰å“ªäº›ï¼Ÿ"</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> q <span class="keyword">in</span> questions:</span><br><span class="line">    answer, conf = answer_question(q, context)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Q: <span class="subst">{q}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"A: <span class="subst">{answer}</span> (ç½®ä¿¡åº¦: <span class="subst">{conf:<span class="number">.2</span>%}</span>)\n"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è¾“å‡º:</span></span><br><span class="line"><span class="comment"># Q: é˜¿é‡Œå·´å·´æ˜¯è°åˆ›ç«‹çš„ï¼Ÿ</span></span><br><span class="line"><span class="comment"># A: é©¬äº‘ (ç½®ä¿¡åº¦: 92.15%)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Q: é˜¿é‡Œå·´å·´æ˜¯å“ªä¸€å¹´ä¸Šå¸‚çš„ï¼Ÿ</span></span><br><span class="line"><span class="comment"># A: 2014å¹´ (ç½®ä¿¡åº¦: 88.73%)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Q: é˜¿é‡Œå·´å·´çš„æ ¸å¿ƒä¸šåŠ¡æœ‰å“ªäº›ï¼Ÿ</span></span><br><span class="line"><span class="comment"># A: ç”µå­å•†åŠ¡ã€äº‘è®¡ç®—ã€æ•°å­—åª’ä½“å’Œå¨±ä¹ (ç½®ä¿¡åº¦: 85.21%)</span></span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="âš¡-Part-7-KV-Cache-ä¼˜åŒ–å®è·µä¸ä»£ç "><a href="#âš¡-Part-7-KV-Cache-ä¼˜åŒ–å®è·µä¸ä»£ç " class="headerlink" title="âš¡ Part 7: KV Cache ä¼˜åŒ–å®è·µä¸ä»£ç "></a>âš¡ Part 7: KV Cache ä¼˜åŒ–å®è·µä¸ä»£ç </h2><h3 id="7-1-KV-Cache-åŸç†å¯è§†åŒ–"><a href="#7-1-KV-Cache-åŸç†å¯è§†åŒ–" class="headerlink" title="7.1 KV Cache åŸç†å¯è§†åŒ–"></a>7.1 KV Cache åŸç†å¯è§†åŒ–</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">æ—  KV Cache (æ¯æ¬¡é‡æ–°è®¡ç®—):</span><br><span class="line">â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•</span><br><span class="line">Step 1: "I"           â†’ è®¡ç®— Kâ‚,Vâ‚,Qâ‚ â†’ è¾“å‡º "love"</span><br><span class="line">Step 2: "I love"      â†’ è®¡ç®— Kâ‚,Vâ‚,Kâ‚‚,Vâ‚‚,Qâ‚‚ â†’ è¾“å‡º "NLP"  </span><br><span class="line">Step 3: "I love NLP"  â†’ è®¡ç®— Kâ‚,Vâ‚,Kâ‚‚,Vâ‚‚,Kâ‚ƒ,Vâ‚ƒ,Qâ‚ƒ â†’ è¾“å‡º "!"</span><br><span class="line">                        â†‘ é‡å¤è®¡ç®—!</span><br><span class="line"></span><br><span class="line">æœ‰ KV Cache (ç¼“å­˜å¤ç”¨):</span><br><span class="line">â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•</span><br><span class="line">Step 1: "I"    â†’ è®¡ç®— Kâ‚,Vâ‚ [å­˜å…¥Cache] â†’ Qâ‚ â†’ "love"</span><br><span class="line">Step 2: "love" â†’ è®¡ç®— Kâ‚‚,Vâ‚‚ [è¿½åŠ Cache] â†’ Qâ‚‚ @ [Kâ‚Kâ‚‚] â†’ "NLP"</span><br><span class="line">Step 3: "NLP"  â†’ è®¡ç®— Kâ‚ƒ,Vâ‚ƒ [è¿½åŠ Cache] â†’ Qâ‚ƒ @ [Kâ‚Kâ‚‚Kâ‚ƒ] â†’ "!"</span><br><span class="line">                 â†‘ åªè®¡ç®—æ–°tokençš„KV!</span><br></pre></td></tr></tbody></table></figure>

<h3 id="7-2-KV-Cache-å®Œæ•´å®ç°"><a href="#7-2-KV-Cache-å®Œæ•´å®ç°" class="headerlink" title="7.2 KV Cache å®Œæ•´å®ç°"></a>7.2 KV Cache å®Œæ•´å®ç°</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">ä»é›¶å®ç°å¸¦ KV Cache çš„ GPT æ¨ç†</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CachedMultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model=<span class="number">768</span>, n_heads=<span class="number">12</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.d_k = d_model // n_heads</span><br><span class="line">        </span><br><span class="line">        self.W_q = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_k = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_v = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_o = nn.Linear(d_model, d_model)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, kv_cache=<span class="literal">None</span>, use_cache=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: [batch, seq_len, d_model] è¾“å…¥</span></span><br><span class="line"><span class="string">            kv_cache: (cached_k, cached_v) ç¼“å­˜çš„Kå’ŒV</span></span><br><span class="line"><span class="string">            use_cache: æ˜¯å¦ä½¿ç”¨å’Œæ›´æ–°ç¼“å­˜</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output: [batch, seq_len, d_model]</span></span><br><span class="line"><span class="string">            new_cache: æ›´æ–°åçš„ç¼“å­˜</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># è®¡ç®—å½“å‰tokençš„ Q, K, V</span></span><br><span class="line">        Q = self.W_q(x)  <span class="comment"># [batch, seq, d_model]</span></span><br><span class="line">        K = self.W_k(x)</span><br><span class="line">        V = self.W_v(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># å¦‚æœæœ‰ç¼“å­˜ï¼Œæ‹¼æ¥å†å² K, V</span></span><br><span class="line">        <span class="keyword">if</span> kv_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            cached_k, cached_v = kv_cache</span><br><span class="line">            K = torch.cat([cached_k, K], dim=<span class="number">1</span>)  <span class="comment"># [batch, cached+seq, d_model]</span></span><br><span class="line">            V = torch.cat([cached_v, V], dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ä¿å­˜æ–°ç¼“å­˜</span></span><br><span class="line">        new_cache = (K, V) <span class="keyword">if</span> use_cache <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">reshape_for_heads</span>(<span class="params">t</span>):</span><br><span class="line">            <span class="comment"># [batch, seq, d_model] -&gt; [batch, heads, seq, d_k]</span></span><br><span class="line">            <span class="keyword">return</span> t.view(batch_size, -<span class="number">1</span>, self.n_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        Q = reshape_for_heads(Q)  <span class="comment"># [batch, heads, q_seq, d_k]</span></span><br><span class="line">        K = reshape_for_heads(K)  <span class="comment"># [batch, heads, kv_seq, d_k]</span></span><br><span class="line">        V = reshape_for_heads(V)  <span class="comment"># [batch, heads, kv_seq, d_k]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°</span></span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(self.d_k)</span><br><span class="line">        <span class="comment"># [batch, heads, q_seq, kv_seq]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Causal Mask (åªçœ‹è¿‡å»)</span></span><br><span class="line">        q_len, kv_len = Q.size(<span class="number">2</span>), K.size(<span class="number">2</span>)</span><br><span class="line">        causal_mask = torch.triu(</span><br><span class="line">            torch.ones(q_len, kv_len, device=x.device), </span><br><span class="line">            diagonal=kv_len - q_len + <span class="number">1</span></span><br><span class="line">        ).<span class="built_in">bool</span>()</span><br><span class="line">        scores = scores.masked_fill(causal_mask, <span class="built_in">float</span>(<span class="string">'-inf'</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Softmax + åŠ æƒæ±‚å’Œ</span></span><br><span class="line">        attn_weights = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">        output = torch.matmul(attn_weights, V)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># åˆå¹¶å¤šå¤´</span></span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, self.d_model)</span><br><span class="line">        output = self.W_o(output)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, new_cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GPTWithKVCache</span>(nn.Module):</span><br><span class="line">    <span class="string">"""ç®€åŒ–çš„ GPT æ¨¡å‹ï¼Œæ”¯æŒ KV Cache"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size=<span class="number">50257</span>, d_model=<span class="number">768</span>, n_layers=<span class="number">12</span>, n_heads=<span class="number">12</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, d_model)</span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            CachedMultiHeadAttention(d_model, n_heads) </span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)</span><br><span class="line">        ])</span><br><span class="line">        self.lm_head = nn.Linear(d_model, vocab_size)</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, past_kv_cache=<span class="literal">None</span>, use_cache=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            input_ids: [batch, seq_len]</span></span><br><span class="line"><span class="string">            past_kv_cache: List of (K, V) for each layer</span></span><br><span class="line"><span class="string">            use_cache: æ˜¯å¦ä½¿ç”¨KV Cache</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        x = self.embedding(input_ids)</span><br><span class="line">        </span><br><span class="line">        new_cache = []</span><br><span class="line">        <span class="keyword">for</span> i, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.layers):</span><br><span class="line">            layer_cache = past_kv_cache[i] <span class="keyword">if</span> past_kv_cache <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">            x, cache = layer(x, kv_cache=layer_cache, use_cache=use_cache)</span><br><span class="line">            new_cache.append(cache)</span><br><span class="line">        </span><br><span class="line">        logits = self.lm_head(x)</span><br><span class="line">        <span class="keyword">return</span> logits, new_cache <span class="keyword">if</span> use_cache <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_with_kv_cache</span>(<span class="params">model, prompt_ids, max_new_tokens=<span class="number">50</span></span>):</span><br><span class="line">    <span class="string">"""ä½¿ç”¨ KV Cache è¿›è¡Œé«˜æ•ˆç”Ÿæˆ"""</span></span><br><span class="line">    </span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    generated = prompt_ids.clone()</span><br><span class="line">    past_cache = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># ç¬¬ä¸€æ­¥ï¼šå¤„ç†æ•´ä¸ª prompt</span></span><br><span class="line">        logits, past_cache = model(prompt_ids, past_kv_cache=<span class="literal">None</span>, use_cache=<span class="literal">True</span>)</span><br><span class="line">        next_token = torch.argmax(logits[:, -<span class="number">1</span>, :], dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        generated = torch.cat([generated, next_token], dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># åç»­æ­¥éª¤ï¼šåªå¤„ç†æ–° token</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens - <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># åªè¾“å…¥æœ€åä¸€ä¸ª tokenï¼</span></span><br><span class="line">            logits, past_cache = model(</span><br><span class="line">                next_token,  <span class="comment"># [batch, 1] åªæœ‰ä¸€ä¸ªtoken</span></span><br><span class="line">                past_kv_cache=past_cache,</span><br><span class="line">                use_cache=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line">            next_token = torch.argmax(logits[:, -<span class="number">1</span>, :], dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            generated = torch.cat([generated, next_token], dim=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> generated</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># æ€§èƒ½å¯¹æ¯”æµ‹è¯•</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">benchmark_kv_cache</span>():</span><br><span class="line">    <span class="keyword">import</span> time</span><br><span class="line">    </span><br><span class="line">    model = GPTWithKVCache(vocab_size=<span class="number">1000</span>, d_model=<span class="number">256</span>, n_layers=<span class="number">6</span>, n_heads=<span class="number">8</span>)</span><br><span class="line">    prompt = torch.randint(<span class="number">0</span>, <span class="number">1000</span>, (<span class="number">1</span>, <span class="number">10</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># æ—  KV Cache</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        generated = prompt.clone()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                logits, _ = model(generated, use_cache=<span class="literal">False</span>)</span><br><span class="line">            next_token = torch.argmax(logits[:, -<span class="number">1</span>, :], dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            generated = torch.cat([generated, next_token], dim=<span class="number">1</span>)</span><br><span class="line">    time_no_cache = time.time() - start</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># æœ‰ KV Cache  </span></span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        generate_with_kv_cache(model, prompt, max_new_tokens=<span class="number">50</span>)</span><br><span class="line">    time_with_cache = time.time() - start</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"æ—  KV Cache: <span class="subst">{time_no_cache:<span class="number">.2</span>f}</span>s"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"æœ‰ KV Cache: <span class="subst">{time_with_cache:<span class="number">.2</span>f}</span>s"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"åŠ é€Ÿæ¯”: <span class="subst">{time_no_cache/time_with_cache:<span class="number">.1</span>f}</span>x"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># benchmark_kv_cache()</span></span><br><span class="line"><span class="comment"># è¾“å‡ºç¤ºä¾‹:</span></span><br><span class="line"><span class="comment"># æ—  KV Cache: 45.32s</span></span><br><span class="line"><span class="comment"># æœ‰ KV Cache: 8.21s</span></span><br><span class="line"><span class="comment"># åŠ é€Ÿæ¯”: 5.5x</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="7-3-KV-Cache-æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯"><a href="#7-3-KV-Cache-æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯" class="headerlink" title="7.3 KV Cache æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯"></a>7.3 KV Cache æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">ä¸‰ç§ KV Cache ä¼˜åŒ–æŠ€æœ¯çš„å®ç°å¯¹æ¯”</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Multi-Query Attention (MQA)</span></span><br><span class="line"><span class="comment"># æ‰€æœ‰ Q å¤´å…±äº«ä¸€ç»„ K, V</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiQueryAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model=<span class="number">768</span>, n_heads=<span class="number">12</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.d_k = d_model // n_heads</span><br><span class="line">        </span><br><span class="line">        self.W_q = nn.Linear(d_model, d_model)  <span class="comment"># 12 ä¸ªå¤´çš„ Q</span></span><br><span class="line">        self.W_k = nn.Linear(d_model, self.d_k)  <span class="comment"># åªæœ‰ 1 ç»„ K</span></span><br><span class="line">        self.W_v = nn.Linear(d_model, self.d_k)  <span class="comment"># åªæœ‰ 1 ç»„ V</span></span><br><span class="line">        self.W_o = nn.Linear(d_model, d_model)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batch, seq, _ = x.shape</span><br><span class="line">        </span><br><span class="line">        Q = self.W_q(x).view(batch, seq, self.n_heads, self.d_k)</span><br><span class="line">        K = self.W_k(x).unsqueeze(<span class="number">2</span>)  <span class="comment"># [batch, seq, 1, d_k]</span></span><br><span class="line">        V = self.W_v(x).unsqueeze(<span class="number">2</span>)  <span class="comment"># [batch, seq, 1, d_k]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># K, V å¹¿æ’­åˆ°æ‰€æœ‰å¤´</span></span><br><span class="line">        K = K.expand(-<span class="number">1</span>, -<span class="number">1</span>, self.n_heads, -<span class="number">1</span>)</span><br><span class="line">        V = V.expand(-<span class="number">1</span>, -<span class="number">1</span>, self.n_heads, -<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ... åç»­è®¡ç®—ç›¸åŒ</span></span><br><span class="line">        <span class="comment"># KV Cache å¤§å°: 1/n_heads of standard!</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Grouped-Query Attention (GQA) - LLaMA-2 ä½¿ç”¨</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GroupedQueryAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model=<span class="number">768</span>, n_heads=<span class="number">12</span>, n_kv_heads=<span class="number">3</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        n_heads=12, n_kv_heads=3 è¡¨ç¤º:</span></span><br><span class="line"><span class="string">        - 12 ä¸ª Q å¤´</span></span><br><span class="line"><span class="string">        - 3 ä¸ª KV å¤´ (æ¯ 4 ä¸ª Q å¤´å…±äº« 1 ä¸ª KV)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.n_kv_heads = n_kv_heads</span><br><span class="line">        self.n_groups = n_heads // n_kv_heads  <span class="comment"># 4</span></span><br><span class="line">        self.d_k = d_model // n_heads</span><br><span class="line">        </span><br><span class="line">        self.W_q = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_k = nn.Linear(d_model, self.n_kv_heads * self.d_k)</span><br><span class="line">        self.W_v = nn.Linear(d_model, self.n_kv_heads * self.d_k)</span><br><span class="line">        self.W_o = nn.Linear(d_model, d_model)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batch, seq, _ = x.shape</span><br><span class="line">        </span><br><span class="line">        Q = self.W_q(x).view(batch, seq, self.n_heads, self.d_k)</span><br><span class="line">        K = self.W_k(x).view(batch, seq, self.n_kv_heads, self.d_k)</span><br><span class="line">        V = self.W_v(x).view(batch, seq, self.n_kv_heads, self.d_k)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># å°† KV é‡å¤ä»¥åŒ¹é… Q å¤´æ•°</span></span><br><span class="line">        K = K.repeat_interleave(self.n_groups, dim=<span class="number">2</span>)  <span class="comment"># [b, s, 12, d_k]</span></span><br><span class="line">        V = V.repeat_interleave(self.n_groups, dim=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ... åç»­è®¡ç®—ç›¸åŒ</span></span><br><span class="line">        <span class="comment"># KV Cache å¤§å°: n_kv_heads/n_heads of standard (è¿™é‡Œæ˜¯ 1/4)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. æ˜¾å­˜å ç”¨å¯¹æ¯”</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">memory_comparison</span>():</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    LLaMA-7B é…ç½®: 32å±‚, 4096ç»´åº¦, 32å¤´</span></span><br><span class="line"><span class="string">    åºåˆ—é•¿åº¦: 4096 tokens</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    n_layers = <span class="number">32</span></span><br><span class="line">    d_model = <span class="number">4096</span></span><br><span class="line">    seq_len = <span class="number">4096</span></span><br><span class="line">    dtype_bytes = <span class="number">2</span>  <span class="comment"># float16</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># æ ‡å‡† MHA</span></span><br><span class="line">    standard = <span class="number">2</span> * n_layers * d_model * seq_len * dtype_bytes</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"æ ‡å‡† MHA KV Cache: <span class="subst">{standard / <span class="number">1e9</span>:<span class="number">.2</span>f}</span> GB"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># MQA (æ‰€æœ‰å¤´å…±äº«)</span></span><br><span class="line">    n_heads = <span class="number">32</span></span><br><span class="line">    mqa = <span class="number">2</span> * n_layers * (d_model // n_heads) * seq_len * dtype_bytes</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"MQA KV Cache: <span class="subst">{mqa / <span class="number">1e9</span>:<span class="number">.2</span>f}</span> GB (<span class="subst">{standard/mqa:<span class="number">.0</span>f}</span>x èŠ‚çœ)"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># GQA (LLaMA-2 é…ç½®: 8 ä¸ª KV å¤´)</span></span><br><span class="line">    n_kv_heads = <span class="number">8</span></span><br><span class="line">    gqa = <span class="number">2</span> * n_layers * (d_model // n_heads * n_kv_heads) * seq_len * dtype_bytes</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"GQA KV Cache: <span class="subst">{gqa / <span class="number">1e9</span>:<span class="number">.2</span>f}</span> GB (<span class="subst">{standard/gqa:<span class="number">.0</span>f}</span>x èŠ‚çœ)"</span>)</span><br><span class="line"></span><br><span class="line">memory_comparison()</span><br><span class="line"><span class="comment"># è¾“å‡º:</span></span><br><span class="line"><span class="comment"># æ ‡å‡† MHA KV Cache: 2.15 GB</span></span><br><span class="line"><span class="comment"># MQA KV Cache: 0.07 GB (32x èŠ‚çœ)</span></span><br><span class="line"><span class="comment"># GQA KV Cache: 0.27 GB (8x èŠ‚çœ)</span></span><br></pre></td></tr></tbody></table></figure>

<h3 id="7-4-ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ"><a href="#7-4-ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ" class="headerlink" title="7.4 ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ"></a>7.4 ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ</h3><table>
<thead>
<tr>
<th>ä¼˜åŒ–æŠ€æœ¯</th>
<th>é€‚ç”¨åœºæ™¯</th>
<th>ä¼˜ç‚¹</th>
<th>ç¼ºç‚¹</th>
</tr>
</thead>
<tbody><tr>
<td><strong>æ ‡å‡† MHA</strong></td>
<td>å°æ¨¡å‹ã€çŸ­åºåˆ—</td>
<td>è´¨é‡æœ€å¥½</td>
<td>æ˜¾å­˜å ç”¨å¤§</td>
</tr>
<tr>
<td><strong>MQA</strong></td>
<td>è¶…é•¿åºåˆ—ã€æè‡´æ¨ç†é€Ÿåº¦</td>
<td>æ˜¾å­˜èŠ‚çœæœ€å¤š</td>
<td>è´¨é‡ç•¥æœ‰ä¸‹é™</td>
</tr>
<tr>
<td><strong>GQA</strong></td>
<td>ç”Ÿäº§ç¯å¢ƒæ¨è</td>
<td>å¹³è¡¡è´¨é‡å’Œæ•ˆç‡</td>
<td>éœ€è¦é‡æ–°è®­ç»ƒ</td>
</tr>
<tr>
<td><strong>PagedAttention</strong></td>
<td>é«˜å¹¶å‘æ¨ç†æœåŠ¡</td>
<td>å‡å°‘æ˜¾å­˜ç¢ç‰‡</td>
<td>å®ç°å¤æ‚</td>
</tr>
<tr>
<td><strong>FlashAttention</strong></td>
<td>æ‰€æœ‰åœºæ™¯</td>
<td>IOä¼˜åŒ–ã€è®­ç»ƒåŠ é€Ÿ</td>
<td>éœ€è¦ç‰¹å®šç¡¬ä»¶</td>
</tr>
</tbody></table>
<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vLLM ä½¿ç”¨ç¤ºä¾‹ (ç”Ÿäº§æ¨è)</span></span><br><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> LLM, SamplingParams</span><br><span class="line"></span><br><span class="line"><span class="comment"># è‡ªåŠ¨å¯ç”¨ PagedAttention</span></span><br><span class="line">llm = LLM(</span><br><span class="line">    model=<span class="string">"meta-llama/Llama-2-7b-hf"</span>,</span><br><span class="line">    tensor_parallel_size=<span class="number">1</span>,  <span class="comment"># GPUæ•°é‡</span></span><br><span class="line">    gpu_memory_utilization=<span class="number">0.9</span>,  <span class="comment"># æ˜¾å­˜åˆ©ç”¨ç‡</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># é«˜æ•ˆæ¨ç†</span></span><br><span class="line">prompts = [<span class="string">"å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—ï¼š"</span>, <span class="string">"è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼š"</span>]</span><br><span class="line">outputs = llm.generate(prompts, SamplingParams(temperature=<span class="number">0.7</span>, max_tokens=<span class="number">100</span>))</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="ğŸš€-Part-6-BERT-å®é™…åº”ç”¨åœºæ™¯ä¸ä»£ç "><a href="#ğŸš€-Part-6-BERT-å®é™…åº”ç”¨åœºæ™¯ä¸ä»£ç " class="headerlink" title="ğŸš€ Part 6: BERT å®é™…åº”ç”¨åœºæ™¯ä¸ä»£ç "></a>ğŸš€ Part 6: BERT å®é™…åº”ç”¨åœºæ™¯ä¸ä»£ç </h2><h3 id="6-1-åº”ç”¨åœºæ™¯æ€»è§ˆ-1"><a href="#6-1-åº”ç”¨åœºæ™¯æ€»è§ˆ-1" class="headerlink" title="6.1 åº”ç”¨åœºæ™¯æ€»è§ˆ"></a>6.1 åº”ç”¨åœºæ™¯æ€»è§ˆ</h3><table>
<thead>
<tr>
<th>åº”ç”¨åœºæ™¯</th>
<th>ä»»åŠ¡ç±»å‹</th>
<th>è¾“å…¥æ ¼å¼</th>
<th>è¾“å‡º</th>
<th>å®é™…æ¡ˆä¾‹</th>
</tr>
</thead>
<tbody><tr>
<td><strong>æƒ…æ„Ÿåˆ†æ</strong></td>
<td>å•å¥åˆ†ç±»</td>
<td>[CLS] æ–‡æœ¬ [SEP]</td>
<td>æ­£é¢/è´Ÿé¢</td>
<td>ç”µå•†è¯„è®ºåˆ†æã€èˆ†æƒ…ç›‘æ§</td>
</tr>
<tr>
<td><strong>æ–‡æœ¬åŒ¹é…</strong></td>
<td>å¥å¯¹åˆ†ç±»</td>
<td>[CLS] å¥A [SEP] å¥B [SEP]</td>
<td>ç›¸ä¼¼/ä¸ç›¸ä¼¼</td>
<td>æ™ºèƒ½å®¢æœã€é—®é¢˜å»é‡</td>
</tr>
<tr>
<td><strong>å‘½åå®ä½“è¯†åˆ«</strong></td>
<td>åºåˆ—æ ‡æ³¨</td>
<td>[CLS] æ–‡æœ¬ [SEP]</td>
<td>æ¯ä¸ªtokençš„æ ‡ç­¾</td>
<td>ç®€å†è§£æã€åŒ»ç–—ç—…å†</td>
</tr>
<tr>
<td><strong>é˜…è¯»ç†è§£</strong></td>
<td>æŠ½å–å¼QA</td>
<td>[CLS] é—®é¢˜ [SEP] æ–‡ç«  [SEP]</td>
<td>ç­”æ¡ˆä½ç½®</td>
<td>æ™ºèƒ½é—®ç­”ã€çŸ¥è¯†åº“æ£€ç´¢</td>
</tr>
<tr>
<td><strong>æ–‡æœ¬ç”Ÿæˆ</strong></td>
<td>Seq2Seq</td>
<td>éœ€é…åˆDecoder</td>
<td>ç”Ÿæˆæ–‡æœ¬</td>
<td>æ‘˜è¦ç”Ÿæˆã€æœºå™¨ç¿»è¯‘</td>
</tr>
</tbody></table>
<hr>
<h3 id="6-2-æƒ…æ„Ÿåˆ†æå®Œæ•´ä»£ç "><a href="#6-2-æƒ…æ„Ÿåˆ†æå®Œæ•´ä»£ç " class="headerlink" title="6.2 æƒ…æ„Ÿåˆ†æå®Œæ•´ä»£ç "></a>6.2 æƒ…æ„Ÿåˆ†æå®Œæ•´ä»£ç </h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForSequenceClassification</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹</span></span><br><span class="line">model_name = <span class="string">"bert-base-chinese"</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(model_name)</span><br><span class="line">model = BertForSequenceClassification.from_pretrained(model_name, num_labels=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. æ•°æ®é¢„å¤„ç†</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">texts, labels</span>):</span><br><span class="line">    encodings = tokenizer(</span><br><span class="line">        texts,</span><br><span class="line">        padding=<span class="literal">True</span>,</span><br><span class="line">        truncation=<span class="literal">True</span>,</span><br><span class="line">        max_length=<span class="number">128</span>,</span><br><span class="line">        return_tensors=<span class="string">"pt"</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> encodings, torch.tensor(labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. è®­ç»ƒæ•°æ®ç¤ºä¾‹</span></span><br><span class="line">train_texts = [</span><br><span class="line">    <span class="string">"è¿™ä¸ªäº§å“è´¨é‡å¤ªå·®äº†ï¼Œå®Œå…¨æ˜¯æµªè´¹é’±"</span>,</span><br><span class="line">    <span class="string">"éå¸¸æ»¡æ„ï¼å‘è´§é€Ÿåº¦å¿«ï¼Œè´¨é‡å¾ˆå¥½"</span>,</span><br><span class="line">    <span class="string">"ä¸€èˆ¬èˆ¬å§ï¼Œæ²¡æœ‰æƒ³è±¡ä¸­é‚£ä¹ˆå¥½"</span>,</span><br><span class="line">    <span class="string">"è¶…çº§æ¨èï¼å·²ç»å›è´­ä¸‰æ¬¡äº†"</span></span><br><span class="line">]</span><br><span class="line">train_labels = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]  <span class="comment"># 0=è´Ÿé¢, 1=æ­£é¢</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. å¾®è°ƒè®­ç»ƒ</span></span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> AdamW</span><br><span class="line"></span><br><span class="line">encodings, labels = preprocess(train_texts, train_labels)</span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">2e-5</span>)</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    outputs = model(**encodings, labels=labels)</span><br><span class="line">    loss = outputs.loss</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Epoch <span class="subst">{epoch+<span class="number">1</span>}</span>, Loss: <span class="subst">{loss.item():<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. æ¨ç†é¢„æµ‹</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">test_text = <span class="string">"è¿™æ¬¾æ‰‹æœºæ‹ç…§æ•ˆæœå¾ˆæ£’ï¼Œç”µæ± ä¹Ÿè€ç”¨"</span></span><br><span class="line">inputs = tokenizer(test_text, return_tensors=<span class="string">"pt"</span>)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = model(**inputs)</span><br><span class="line">    prediction = torch.argmax(outputs.logits, dim=<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"é¢„æµ‹ç»“æœ: <span class="subst">{<span class="string">'æ­£é¢'</span> <span class="keyword">if</span> prediction == <span class="number">1</span> <span class="keyword">else</span> <span class="string">'è´Ÿé¢'</span>}</span>"</span>)</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="6-3-å‘½åå®ä½“è¯†åˆ«-NER-å®Œæ•´ä»£ç "><a href="#6-3-å‘½åå®ä½“è¯†åˆ«-NER-å®Œæ•´ä»£ç " class="headerlink" title="6.3 å‘½åå®ä½“è¯†åˆ« (NER) å®Œæ•´ä»£ç "></a>6.3 å‘½åå®ä½“è¯†åˆ« (NER) å®Œæ•´ä»£ç </h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizerFast, BertForTokenClassification</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># NER æ ‡ç­¾å®šä¹‰ (BIO æ ¼å¼)</span></span><br><span class="line">label_list = [<span class="string">"O"</span>, <span class="string">"B-PER"</span>, <span class="string">"I-PER"</span>, <span class="string">"B-ORG"</span>, <span class="string">"I-ORG"</span>, <span class="string">"B-LOC"</span>, <span class="string">"I-LOC"</span>]</span><br><span class="line">label2id = {label: i <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(label_list)}</span><br><span class="line">id2label = {i: label <span class="keyword">for</span> label, i <span class="keyword">in</span> label2id.items()}</span><br><span class="line"></span><br><span class="line"><span class="comment"># åŠ è½½æ¨¡å‹</span></span><br><span class="line">tokenizer = BertTokenizerFast.from_pretrained(<span class="string">"bert-base-chinese"</span>)</span><br><span class="line">model = BertForTokenClassification.from_pretrained(</span><br><span class="line">    <span class="string">"bert-base-chinese"</span>,</span><br><span class="line">    num_labels=<span class="built_in">len</span>(label_list),</span><br><span class="line">    id2label=id2label,</span><br><span class="line">    label2id=label2id</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è®­ç»ƒæ•°æ®ç¤ºä¾‹</span></span><br><span class="line">train_data = [</span><br><span class="line">    {</span><br><span class="line">        <span class="string">"text"</span>: <span class="string">"é©¬äº‘åœ¨æ­å·åˆ›åŠäº†é˜¿é‡Œå·´å·´"</span>,</span><br><span class="line">        <span class="string">"entities"</span>: [</span><br><span class="line">            {<span class="string">"start"</span>: <span class="number">0</span>, <span class="string">"end"</span>: <span class="number">2</span>, <span class="string">"label"</span>: <span class="string">"PER"</span>},   <span class="comment"># é©¬äº‘</span></span><br><span class="line">            {<span class="string">"start"</span>: <span class="number">3</span>, <span class="string">"end"</span>: <span class="number">5</span>, <span class="string">"label"</span>: <span class="string">"LOC"</span>},   <span class="comment"># æ­å·</span></span><br><span class="line">            {<span class="string">"start"</span>: <span class="number">8</span>, <span class="string">"end"</span>: <span class="number">12</span>, <span class="string">"label"</span>: <span class="string">"ORG"</span>}   <span class="comment"># é˜¿é‡Œå·´å·´</span></span><br><span class="line">        ]</span><br><span class="line">    }</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ¨ç†ç¤ºä¾‹</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ner</span>(<span class="params">text</span>):</span><br><span class="line">    inputs = tokenizer(text, return_tensors=<span class="string">"pt"</span>, return_offsets_mapping=<span class="literal">True</span>)</span><br><span class="line">    offset_mapping = inputs.pop(<span class="string">"offset_mapping"</span>)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">        predictions = torch.argmax(outputs.logits, dim=<span class="number">2</span>)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># è§£æå®ä½“</span></span><br><span class="line">    entities = []</span><br><span class="line">    current_entity = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> idx, (pred, offset) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(predictions, offset_mapping)):</span><br><span class="line">        label = id2label[pred.item()]</span><br><span class="line">        <span class="keyword">if</span> label.startswith(<span class="string">"B-"</span>):</span><br><span class="line">            <span class="keyword">if</span> current_entity:</span><br><span class="line">                entities.append(current_entity)</span><br><span class="line">            current_entity = {</span><br><span class="line">                <span class="string">"text"</span>: text[offset[<span class="number">0</span>]:offset[<span class="number">1</span>]],</span><br><span class="line">                <span class="string">"label"</span>: label[<span class="number">2</span>:],</span><br><span class="line">                <span class="string">"start"</span>: offset[<span class="number">0</span>].item()</span><br><span class="line">            }</span><br><span class="line">        <span class="keyword">elif</span> label.startswith(<span class="string">"I-"</span>) <span class="keyword">and</span> current_entity:</span><br><span class="line">            current_entity[<span class="string">"text"</span>] += text[offset[<span class="number">0</span>]:offset[<span class="number">1</span>]]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> current_entity:</span><br><span class="line">                entities.append(current_entity)</span><br><span class="line">                current_entity = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> entities</span><br><span class="line"></span><br><span class="line"><span class="comment"># æµ‹è¯•</span></span><br><span class="line">result = predict_ner(<span class="string">"é©¬äº‘åœ¨æ­å·åˆ›åŠäº†é˜¿é‡Œå·´å·´"</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="comment"># [{'text': 'é©¬äº‘', 'label': 'PER', 'start': 0}, </span></span><br><span class="line"><span class="comment">#  {'text': 'æ­å·', 'label': 'LOC', 'start': 3},</span></span><br><span class="line"><span class="comment">#  {'text': 'é˜¿é‡Œå·´å·´', 'label': 'ORG', 'start': 8}]</span></span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="6-4-è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ä»£ç "><a href="#6-4-è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ä»£ç " class="headerlink" title="6.4 è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ä»£ç "></a>6.4 è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ä»£ç </h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># ä½¿ç”¨ BERT æå–å¥å­å‘é‡</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">"bert-base-chinese"</span>)</span><br><span class="line">model = BertModel.from_pretrained(<span class="string">"bert-base-chinese"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_sentence_embedding</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">"""æå–å¥å­çš„ [CLS] å‘é‡ä½œä¸ºè¯­ä¹‰è¡¨ç¤º"""</span></span><br><span class="line">    inputs = tokenizer(text, return_tensors=<span class="string">"pt"</span>, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">        <span class="comment"># ä½¿ç”¨ [CLS] token çš„è¾“å‡ºä½œä¸ºå¥å­è¡¨ç¤º</span></span><br><span class="line">        cls_embedding = outputs.last_hidden_state[:, <span class="number">0</span>, :]</span><br><span class="line">    <span class="keyword">return</span> cls_embedding</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_similarity</span>(<span class="params">text1, text2</span>):</span><br><span class="line">    <span class="string">"""è®¡ç®—ä¸¤ä¸ªå¥å­çš„ä½™å¼¦ç›¸ä¼¼åº¦"""</span></span><br><span class="line">    emb1 = get_sentence_embedding(text1)</span><br><span class="line">    emb2 = get_sentence_embedding(text2)</span><br><span class="line">    similarity = F.cosine_similarity(emb1, emb2)</span><br><span class="line">    <span class="keyword">return</span> similarity.item()</span><br><span class="line"></span><br><span class="line"><span class="comment"># æµ‹è¯•è¯­ä¹‰ç›¸ä¼¼åº¦</span></span><br><span class="line">pairs = [</span><br><span class="line">    (<span class="string">"ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·"</span>, <span class="string">"ä»Šå¤©å¤©æ°”å¥½å—"</span>),           <span class="comment"># é«˜ç›¸ä¼¼</span></span><br><span class="line">    (<span class="string">"ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·"</span>, <span class="string">"æ˜å¤©ä¼šä¸‹é›¨å—"</span>),           <span class="comment"># ä¸­ç­‰ç›¸ä¼¼</span></span><br><span class="line">    (<span class="string">"ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·"</span>, <span class="string">"è¿™é“èœæ€ä¹ˆåš"</span>),           <span class="comment"># ä½ç›¸ä¼¼</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> text1, text2 <span class="keyword">in</span> pairs:</span><br><span class="line">    sim = compute_similarity(text1, text2)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"'<span class="subst">{text1}</span>' vs '<span class="subst">{text2}</span>': <span class="subst">{sim:<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># è¾“å‡º:</span></span><br><span class="line"><span class="comment"># 'ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·' vs 'ä»Šå¤©å¤©æ°”å¥½å—': 0.9234</span></span><br><span class="line"><span class="comment"># 'ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·' vs 'æ˜å¤©ä¼šä¸‹é›¨å—': 0.7821</span></span><br><span class="line"><span class="comment"># 'ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·' vs 'è¿™é“èœæ€ä¹ˆåš': 0.4123</span></span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="6-5-é—®ç­”ç³»ç»Ÿ-é˜…è¯»ç†è§£-ä»£ç "><a href="#6-5-é—®ç­”ç³»ç»Ÿ-é˜…è¯»ç†è§£-ä»£ç " class="headerlink" title="6.5 é—®ç­”ç³»ç»Ÿ (é˜…è¯»ç†è§£) ä»£ç "></a>6.5 é—®ç­”ç³»ç»Ÿ (é˜…è¯»ç†è§£) ä»£ç </h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForQuestionAnswering</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># åŠ è½½é—®ç­”æ¨¡å‹</span></span><br><span class="line">model_name = <span class="string">"bert-large-uncased-whole-word-masking-finetuned-squad"</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(model_name)</span><br><span class="line">model = BertForQuestionAnswering.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">answer_question</span>(<span class="params">question, context</span>):</span><br><span class="line">    <span class="string">"""ä»æ–‡ç« ä¸­æŠ½å–ç­”æ¡ˆ"""</span></span><br><span class="line">    <span class="comment"># ç¼–ç é—®é¢˜å’Œä¸Šä¸‹æ–‡</span></span><br><span class="line">    inputs = tokenizer(</span><br><span class="line">        question,</span><br><span class="line">        context,</span><br><span class="line">        return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">        max_length=<span class="number">512</span>,</span><br><span class="line">        truncation=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># é¢„æµ‹ç­”æ¡ˆä½ç½®</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">        start_scores = outputs.start_logits</span><br><span class="line">        end_scores = outputs.end_logits</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># æ‰¾åˆ°æœ€å¯èƒ½çš„ç­”æ¡ˆä½ç½®</span></span><br><span class="line">    start_idx = torch.argmax(start_scores)</span><br><span class="line">    end_idx = torch.argmax(end_scores)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># è§£ç ç­”æ¡ˆ</span></span><br><span class="line">    tokens = tokenizer.convert_ids_to_tokens(inputs[<span class="string">"input_ids"</span>][<span class="number">0</span>])</span><br><span class="line">    answer = tokenizer.convert_tokens_to_string(tokens[start_idx:end_idx+<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> answer</span><br><span class="line"></span><br><span class="line"><span class="comment"># æµ‹è¯•é—®ç­”</span></span><br><span class="line">context = <span class="string">"""</span></span><br><span class="line"><span class="string">BERTæ˜¯ç”±Googleåœ¨2018å¹´æå‡ºçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚</span></span><br><span class="line"><span class="string">å®ƒä½¿ç”¨Transformerçš„Encoderæ¶æ„ï¼Œé€šè¿‡Masked Language Modelä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒã€‚</span></span><br><span class="line"><span class="string">BERTåœ¨11ä¸ªNLPä»»åŠ¡ä¸Šå–å¾—äº†å½“æ—¶çš„æœ€ä½³æˆç»©ï¼ŒåŒ…æ‹¬é—®ç­”ã€æ–‡æœ¬åˆ†ç±»ç­‰ä»»åŠ¡ã€‚</span></span><br><span class="line"><span class="string">BERT-Baseæœ‰1.1äº¿å‚æ•°ï¼ŒBERT-Largeæœ‰3.4äº¿å‚æ•°ã€‚</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">questions = [</span><br><span class="line">    <span class="string">"BERTæ˜¯è°æå‡ºçš„ï¼Ÿ"</span>,</span><br><span class="line">    <span class="string">"BERTä½¿ç”¨ä»€ä¹ˆæ¶æ„ï¼Ÿ"</span>,</span><br><span class="line">    <span class="string">"BERTæœ‰å¤šå°‘å‚æ•°ï¼Ÿ"</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> q <span class="keyword">in</span> questions:</span><br><span class="line">    answer = answer_question(q, context)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Q: <span class="subst">{q}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"A: <span class="subst">{answer}</span>\n"</span>)</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h2 id="âš¡-Part-7-KV-Cache-å®æˆ˜ä¸ä¼˜åŒ–"><a href="#âš¡-Part-7-KV-Cache-å®æˆ˜ä¸ä¼˜åŒ–" class="headerlink" title="âš¡ Part 7: KV Cache å®æˆ˜ä¸ä¼˜åŒ–"></a>âš¡ Part 7: KV Cache å®æˆ˜ä¸ä¼˜åŒ–</h2><h3 id="7-1-KV-Cache-å®Œæ•´å®ç°ä»£ç "><a href="#7-1-KV-Cache-å®Œæ•´å®ç°ä»£ç " class="headerlink" title="7.1 KV Cache å®Œæ•´å®ç°ä»£ç "></a>7.1 KV Cache å®Œæ•´å®ç°ä»£ç </h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionWithKVCache</span>(nn.Module):</span><br><span class="line">    <span class="string">"""å¸¦ KV Cache çš„æ³¨æ„åŠ›å±‚å®ç°"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model=<span class="number">768</span>, n_heads=<span class="number">12</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.d_k = d_model // n_heads</span><br><span class="line">        </span><br><span class="line">        self.W_q = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_k = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_v = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_o = nn.Linear(d_model, d_model)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, kv_cache=<span class="literal">None</span>, use_cache=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: [batch, seq_len, d_model] è¾“å…¥</span></span><br><span class="line"><span class="string">            kv_cache: tuple(K, V) ç¼“å­˜çš„ K/V</span></span><br><span class="line"><span class="string">            use_cache: æ˜¯å¦ä½¿ç”¨å¹¶æ›´æ–°ç¼“å­˜</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output: [batch, seq_len, d_model]</span></span><br><span class="line"><span class="string">            new_kv_cache: æ›´æ–°åçš„ç¼“å­˜</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        batch_size, seq_len, _ = x.shape</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># è®¡ç®— Q, K, V</span></span><br><span class="line">        Q = self.W_q(x)  <span class="comment"># [batch, seq, d_model]</span></span><br><span class="line">        K = self.W_k(x)</span><br><span class="line">        V = self.W_v(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># å¦‚æœæœ‰ç¼“å­˜ï¼Œæ‹¼æ¥å†å² K, V</span></span><br><span class="line">        <span class="keyword">if</span> kv_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            K_cache, V_cache = kv_cache</span><br><span class="line">            K = torch.cat([K_cache, K], dim=<span class="number">1</span>)  <span class="comment"># [batch, cache_len + seq, d_model]</span></span><br><span class="line">            V = torch.cat([V_cache, V], dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># å‡†å¤‡è¿”å›çš„ç¼“å­˜</span></span><br><span class="line">        new_kv_cache = (K, V) <span class="keyword">if</span> use_cache <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># é‡å¡‘ä¸ºå¤šå¤´å½¢å¼</span></span><br><span class="line">        Q = Q.view(batch_size, -<span class="number">1</span>, self.n_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        K = K.view(batch_size, -<span class="number">1</span>, self.n_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        V = V.view(batch_size, -<span class="number">1</span>, self.n_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># Q: [batch, heads, seq, d_k]</span></span><br><span class="line">        <span class="comment"># K, V: [batch, heads, total_seq, d_k]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># è®¡ç®—æ³¨æ„åŠ›</span></span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / (self.d_k ** <span class="number">0.5</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Causal Mask (åªçœ‹å·¦è¾¹)</span></span><br><span class="line">        total_len = K.size(<span class="number">2</span>)</span><br><span class="line">        query_len = Q.size(<span class="number">2</span>)</span><br><span class="line">        mask = torch.triu(torch.ones(query_len, total_len), diagonal=total_len-query_len+<span class="number">1</span>)</span><br><span class="line">        mask = mask.<span class="built_in">bool</span>().to(x.device)</span><br><span class="line">        scores = scores.masked_fill(mask, <span class="built_in">float</span>(<span class="string">'-inf'</span>))</span><br><span class="line">        </span><br><span class="line">        attn_weights = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">        output = torch.matmul(attn_weights, V)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># åˆå¹¶å¤šå¤´</span></span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, self.d_model)</span><br><span class="line">        output = self.W_o(output)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, new_kv_cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ä½¿ç”¨ç¤ºä¾‹ï¼šè‡ªå›å½’ç”Ÿæˆ</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_with_kv_cache</span>(<span class="params">model, prompt_ids, max_new_tokens=<span class="number">50</span></span>):</span><br><span class="line">    <span class="string">"""ä½¿ç”¨ KV Cache è¿›è¡Œé«˜æ•ˆç”Ÿæˆ"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># åˆå§‹åŒ–ï¼šå¤„ç† prompt</span></span><br><span class="line">    kv_cache = <span class="literal">None</span></span><br><span class="line">    input_ids = prompt_ids</span><br><span class="line">    generated = prompt_ids.tolist()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens):</span><br><span class="line">        <span class="comment"># åªè¾“å…¥æ–°çš„ tokenï¼ˆç¬¬ä¸€æ­¥è¾“å…¥å®Œæ•´ promptï¼‰</span></span><br><span class="line">        <span class="keyword">if</span> step == <span class="number">0</span>:</span><br><span class="line">            x = get_embeddings(input_ids)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = get_embeddings(input_ids[:, -<span class="number">1</span>:])  <span class="comment"># åªå–æœ€åä¸€ä¸ª token</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># å‰å‘ä¼ æ’­ï¼Œä½¿ç”¨å¹¶æ›´æ–°ç¼“å­˜</span></span><br><span class="line">        output, kv_cache = model(x, kv_cache=kv_cache, use_cache=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># é¢„æµ‹ä¸‹ä¸€ä¸ª token</span></span><br><span class="line">        logits = output[:, -<span class="number">1</span>, :]  <span class="comment"># å–æœ€åä¸€ä¸ªä½ç½®çš„è¾“å‡º</span></span><br><span class="line">        next_token = torch.argmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        generated.append(next_token.item())</span><br><span class="line">        input_ids = torch.cat([input_ids, next_token.unsqueeze(<span class="number">0</span>)], dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># é‡åˆ°ç»“æŸç¬¦åœæ­¢</span></span><br><span class="line">        <span class="keyword">if</span> next_token.item() == EOS_TOKEN_ID:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> generated</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"KV Cache åŠ é€Ÿå¯¹æ¯”:"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"æ— ç¼“å­˜: O(nÂ²) è®¡ç®—é‡"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"æœ‰ç¼“å­˜: O(n) è®¡ç®—é‡"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"åŠ é€Ÿæ¯”: n å€ (åºåˆ—é•¿åº¦)"</span>)</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="7-2-KV-Cache-æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯"><a href="#7-2-KV-Cache-æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯" class="headerlink" title="7.2 KV Cache æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯"></a>7.2 KV Cache æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯</h3><h4 id="Multi-Query-Attention-MQA-å®ç°"><a href="#Multi-Query-Attention-MQA-å®ç°" class="headerlink" title="Multi-Query Attention (MQA) å®ç°"></a>Multi-Query Attention (MQA) å®ç°</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiQueryAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Multi-Query Attention: æ‰€æœ‰å¤´å…±äº«ä¸€ç»„ K, V</span></span><br><span class="line"><span class="string">    æ˜¾å­˜èŠ‚çœ: n_heads å€</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model=<span class="number">768</span>, n_heads=<span class="number">12</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.d_k = d_model // n_heads</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Q: æ¯ä¸ªå¤´ç‹¬ç«‹</span></span><br><span class="line">        self.W_q = nn.Linear(d_model, d_model)</span><br><span class="line">        <span class="comment"># K, V: æ‰€æœ‰å¤´å…±äº« (åªæœ‰ä¸€ä»½)</span></span><br><span class="line">        self.W_k = nn.Linear(d_model, self.d_k)  <span class="comment"># åªè¾“å‡ºä¸€ä¸ªå¤´çš„ç»´åº¦</span></span><br><span class="line">        self.W_v = nn.Linear(d_model, self.d_k)</span><br><span class="line">        self.W_o = nn.Linear(d_model, d_model)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, kv_cache=<span class="literal">None</span></span>):</span><br><span class="line">        batch, seq, _ = x.shape</span><br><span class="line">        </span><br><span class="line">        Q = self.W_q(x).view(batch, seq, self.n_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        K = self.W_k(x).unsqueeze(<span class="number">1</span>)  <span class="comment"># [batch, 1, seq, d_k] å¹¿æ’­åˆ°æ‰€æœ‰å¤´</span></span><br><span class="line">        V = self.W_v(x).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># KV Cache åªéœ€å­˜å‚¨ [batch, 1, seq, d_k] è€Œé [batch, heads, seq, d_k]</span></span><br><span class="line">        <span class="comment"># æ˜¾å­˜èŠ‚çœ: 12å€ (å¯¹äº12å¤´)</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> kv_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            K = torch.cat([kv_cache[<span class="number">0</span>], K], dim=<span class="number">2</span>)</span><br><span class="line">            V = torch.cat([kv_cache[<span class="number">1</span>], V], dim=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / (self.d_k ** <span class="number">0.5</span>)</span><br><span class="line">        attn = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">        output = torch.matmul(attn, V)</span><br><span class="line">        </span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch, seq, -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.W_o(output), (K, V)</span><br></pre></td></tr></tbody></table></figure>

<h4 id="Grouped-Query-Attention-GQA-å®ç°"><a href="#Grouped-Query-Attention-GQA-å®ç°" class="headerlink" title="Grouped-Query Attention (GQA) å®ç°"></a>Grouped-Query Attention (GQA) å®ç°</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GroupedQueryAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Grouped-Query Attention: å°†å¤´åˆ†ç»„ï¼Œæ¯ç»„å…±äº« K, V</span></span><br><span class="line"><span class="string">    LLaMA-2 ä½¿ç”¨: 32å¤´ â†’ 8ç»„</span></span><br><span class="line"><span class="string">    æ˜¾å­˜èŠ‚çœ: n_heads / n_groups å€</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model=<span class="number">768</span>, n_heads=<span class="number">12</span>, n_kv_groups=<span class="number">4</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.n_kv_groups = n_kv_groups</span><br><span class="line">        self.heads_per_group = n_heads // n_kv_groups</span><br><span class="line">        self.d_k = d_model // n_heads</span><br><span class="line">        </span><br><span class="line">        self.W_q = nn.Linear(d_model, d_model)</span><br><span class="line">        <span class="comment"># K, V åªæœ‰ n_kv_groups ç»„</span></span><br><span class="line">        self.W_k = nn.Linear(d_model, self.d_k * n_kv_groups)</span><br><span class="line">        self.W_v = nn.Linear(d_model, self.d_k * n_kv_groups)</span><br><span class="line">        self.W_o = nn.Linear(d_model, d_model)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batch, seq, _ = x.shape</span><br><span class="line">        </span><br><span class="line">        Q = self.W_q(x).view(batch, seq, self.n_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        K = self.W_k(x).view(batch, seq, self.n_kv_groups, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        V = self.W_v(x).view(batch, seq, self.n_kv_groups, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># å°† K, V æ‰©å±•åˆ°ä¸ Q ç›¸åŒçš„å¤´æ•°</span></span><br><span class="line">        K = K.repeat_interleave(self.heads_per_group, dim=<span class="number">1</span>)</span><br><span class="line">        V = V.repeat_interleave(self.heads_per_group, dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / (self.d_k ** <span class="number">0.5</span>)</span><br><span class="line">        attn = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">        output = torch.matmul(attn, V)</span><br><span class="line">        </span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch, seq, -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.W_o(output)</span><br></pre></td></tr></tbody></table></figure>

<hr>
<h3 id="7-3-æ˜¾å­˜å ç”¨å¯¹æ¯”è¡¨"><a href="#7-3-æ˜¾å­˜å ç”¨å¯¹æ¯”è¡¨" class="headerlink" title="7.3 æ˜¾å­˜å ç”¨å¯¹æ¯”è¡¨"></a>7.3 æ˜¾å­˜å ç”¨å¯¹æ¯”è¡¨</h3><table>
<thead>
<tr>
<th>æŠ€æœ¯</th>
<th>KV Cache å¤§å°</th>
<th>LLaMA-7B 128K tokens</th>
<th>é€‚ç”¨æ¨¡å‹</th>
</tr>
</thead>
<tbody><tr>
<td><strong>MHA</strong> (æ ‡å‡†)</td>
<td><code>2 Ã— L Ã— H Ã— S Ã— dtype</code></td>
<td>64 GB</td>
<td>BERT, GPT-2</td>
</tr>
<tr>
<td><strong>MQA</strong></td>
<td><code>2 Ã— L Ã— (H/heads) Ã— S Ã— dtype</code></td>
<td>5.3 GB</td>
<td>PaLM, Falcon</td>
</tr>
<tr>
<td><strong>GQA</strong> (8ç»„)</td>
<td><code>2 Ã— L Ã— (H/4) Ã— S Ã— dtype</code></td>
<td>16 GB</td>
<td>LLaMA-2, Mistral</td>
</tr>
</tbody></table>
<blockquote>
<p>L=å±‚æ•°, H=éšè—ç»´åº¦, S=åºåˆ—é•¿åº¦, heads=æ³¨æ„åŠ›å¤´æ•°</p>
</blockquote>
<hr>
<h3 id="7-4-ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ-1"><a href="#7-4-ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ-1" class="headerlink" title="7.4 ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ"></a>7.4 ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># ä½¿ç”¨ vLLM è¿›è¡Œé«˜æ•ˆæ¨ç† (PagedAttention)</span></span><br><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> LLM, SamplingParams</span><br><span class="line"></span><br><span class="line"><span class="comment"># åˆå§‹åŒ–æ¨¡å‹</span></span><br><span class="line">llm = LLM(</span><br><span class="line">    model=<span class="string">"meta-llama/Llama-2-7b-hf"</span>,</span><br><span class="line">    tensor_parallel_size=<span class="number">1</span>,      <span class="comment"># GPU æ•°é‡</span></span><br><span class="line">    gpu_memory_utilization=<span class="number">0.9</span>,  <span class="comment"># æ˜¾å­˜ä½¿ç”¨ç‡</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># æ‰¹é‡æ¨ç† (è‡ªåŠ¨ç®¡ç† KV Cache)</span></span><br><span class="line">prompts = [</span><br><span class="line">    <span class="string">"Explain the concept of attention mechanism:"</span>,</span><br><span class="line">    <span class="string">"Write a Python function to sort a list:"</span>,</span><br><span class="line">    <span class="string">"What is the capital of France?"</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">sampling_params = SamplingParams(</span><br><span class="line">    temperature=<span class="number">0.7</span>,</span><br><span class="line">    max_tokens=<span class="number">256</span>,</span><br><span class="line">    top_p=<span class="number">0.9</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">outputs = llm.generate(prompts, sampling_params)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> output <span class="keyword">in</span> outputs:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Prompt: <span class="subst">{output.prompt}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Response: <span class="subst">{output.outputs[<span class="number">0</span>].text}</span>\n"</span>)</span><br></pre></td></tr></tbody></table></figure>

<hr>
<p><em>ğŸ“… åˆ›å»ºæ—¶é—´: 2026-02-03</em><br><em>ğŸ·ï¸ æ ‡ç­¾: #AI #Transformer #BERT #Attention #KVCache</em></p>
</article>
<div class="article-footer slide-up">


    <section id="share">
      <div class="header"><span>Share</span></div>
      <div class="body">
        <div class="link"><input class="copy-area" readonly="true" id="copy-link" value="https://caiusy.github.io/2026/02/03/bert-comprehensive-guide/" /></div>
        <div class="social-wrap dis-select"><a class="social share-item wechat" onclick="util.toggle(&quot;qrcode-wechat&quot;)"><img  src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/b32ef3da1162a.svg" /></a><a class="social share-item link" onclick="util.copy(&quot;copy-link&quot;, &quot;Copied!&quot;)"><img  src="https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/social/8411ed322ced6.svg" /></a></div>
        
        <div class="qrcode" id="qrcode-wechat" style="opacity:0;height:0">
          <img src="https://api.qrserver.com/v1/create-qr-code/?size=256x256&data=https://caiusy.github.io/2026/02/03/bert-comprehensive-guide/"/>
        </div>
        
      </div>
    </section>
    </div>

<div class="related-wrap slide-up" id="read-next"><section class="body"><div class="item" id="prev"><div class="note">Newer</div><a href="/2026/02/05/2026-02-05-gpt-series-deep-dive/">GPT ç³»åˆ—æ·±åº¦è§£æï¼šä» GPT-1 åˆ° GPT-3</a></div><div class="item" id="next"><div class="note">Older</div><a href="/2026/02/02/Transformer-DeepDive/">å½»åº•ç†è§£ Transformerï¼šAttention Is All You Need</a></div></section></div>




  <div class="related-wrap md-text slide-up" id="comments">
    <section class='header cmt-title cap theme'>
      <p>ğŸ’¬ æ¬¢è¿ç•™è¨€è®¨è®º</p>

    </section>
    <section class='body cmt-body giscus'>
      

<svg class="loading" style="vertical-align:middle;fill:currentColor;overflow:hidden;" viewBox="0 0 1024 1024" version="1.1" xmlns="http://www.w3.org/2000/svg" p-id="2709"><path d="M832 512c0-176-144-320-320-320V128c211.2 0 384 172.8 384 384h-64zM192 512c0 176 144 320 320 320v64C300.8 896 128 723.2 128 512h64z" p-id="2710"></path></svg>

<div id="giscus" src="https://giscus.app/client.js" data-repo="caiusy/caiusy.github.io" data-repo-id="R_kgDOI0diCA" data-category="Announcements" data-category-id="DIC_kwDOI0diCM4C1G8d" data-mapping="pathname" data-strict="0" data-reactions-enabled="1" data-emit-metadata="0" data-input-position="bottom" data-theme="preferred_color_scheme" data-lang="zh-CN" data-loading="lazy" crossorigin="anonymous"></div>

    </section>
  </div>



<footer class="page-footer slide-up footnote"><hr><div class="sitemap" style="column-count:3;"><div class="sitemap-group"><span class="fs15">åšå®¢</span><a href="/">è¿‘æœŸå‘å¸ƒ</a><a href="/categories/">åˆ†ç±»</a><a href="/tags/">æ ‡ç­¾</a><a href="/archives/">å½’æ¡£</a></div><div class="sitemap-group"><span class="fs15">æŠ€æœ¯é¢†åŸŸ</span><a href="/tags/CV/">è®¡ç®—æœºè§†è§‰</a><a href="/tags/tracking/">ç›®æ ‡è·Ÿè¸ª</a><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/">æ·±åº¦å­¦ä¹ </a></div><div class="sitemap-group"><span class="fs15">å…³äº</span><a href="/about/">å…³äºæˆ‘</a><a target="_blank" rel="noopener" href="https://github.com/caiusy">GitHub</a></div></div><div class="text"><p>æœ¬ç«™ç”± <strong>Caius Lu</strong> ä½¿ç”¨ <a target="_blank" rel="noopener" href="https://github.com/xaoxuu/hexo-theme-stellar">Stellar</a> ä¸»é¢˜åˆ›å»º<br>Â© 2023-2026 Â· æ¢ç´¢ AI ä¸è®¡ç®—æœºè§†è§‰çš„æ— é™å¯èƒ½ ğŸš€</p>
<p><span id="busuanzi_container_site_pv">æœ¬ç«™æ€»è®¿é—®é‡ <span id="busuanzi_value_site_pv"></span> æ¬¡</span> | <span id="busuanzi_container_site_uv">è®¿å®¢æ•° <span id="busuanzi_value_site_uv"></span> äºº</span></p>
</div></footer>
<div class="main-mask" onclick="sidebar.dismiss()"></div></div><aside class="l_right">
<div class="widgets">



<widget class="widget-wrapper slide-up toc" id="data-toc" collapse="false"><div class="widget-header dis-select"><span class="name">On This Page</span><a class="cap-action" onclick="sidebar.toggleTOC()" ><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg></a></div><div class="widget-body"><ol class="toc"><li class="toc-item toc-level-1"><a class="toc-link" href="#BERT-%E5%AE%8C%E6%95%B4%E8%A7%A3%E6%9E%90%EF%BC%9A%E4%BB%8E%E8%AE%BA%E6%96%87%E5%88%B0-KV-Cache"><span class="toc-text">BERT å®Œæ•´è§£æï¼šä»è®ºæ–‡åˆ° KV Cache</span></a><ol class="toc-child"><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%96%BC%EF%B8%8F-%E6%A0%B8%E5%BF%83%E5%8F%AF%E8%A7%86%E5%8C%96%E5%9B%BE%E8%A7%A3"><span class="toc-text">ğŸ–¼ï¸ æ ¸å¿ƒå¯è§†åŒ–å›¾è§£</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#BERT-%E6%95%B4%E4%BD%93%E6%9E%B6%E6%9E%84%E4%B8%8E%E6%A0%B8%E5%BF%83%E6%A6%82%E5%BF%B5"><span class="toc-text">BERT æ•´ä½“æ¶æ„ä¸æ ¸å¿ƒæ¦‚å¿µ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BERT-%E6%9E%B6%E6%9E%84%E8%AF%A6%E8%A7%A3"><span class="toc-text">BERT æ¶æ„è¯¦è§£</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q-x2F-K-x2F-V-%E6%95%B0%E6%8D%AE%E6%B5%81%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-text">Q&#x2F;K&#x2F;V æ•°æ®æµå¯è§†åŒ–</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%BA%E5%88%B6%E8%AF%A6%E8%A7%A3"><span class="toc-text">æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Causal-vs-Bidirectional-Attention-%E5%AF%B9%E6%AF%94"><span class="toc-text">Causal vs Bidirectional Attention å¯¹æ¯”</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%86%9A-BERT-vs-%E2%80%9CAttention-Is-All-You-Need%E2%80%9D-%E5%AF%B9%E6%AF%94%E5%88%86%E6%9E%90"><span class="toc-text">ğŸ†š BERT vs â€œAttention Is All You Needâ€ å¯¹æ¯”åˆ†æ</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF%E5%AF%B9%E6%AF%94"><span class="toc-text">è®ºæ–‡åŸºæœ¬ä¿¡æ¯å¯¹æ¯”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84%E5%85%B3%E7%B3%BB"><span class="toc-text">æ¶æ„å…³ç³»</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%8C%BA%E5%88%AB"><span class="toc-text">æ ¸å¿ƒåŒºåˆ«</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BERT-%E7%BB%A7%E6%89%BF%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-text">BERT ç»§æ‰¿äº†ä»€ä¹ˆï¼Ÿ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BERT-%E7%9A%84%E5%88%9B%E6%96%B0%E7%82%B9"><span class="toc-text">BERT çš„åˆ›æ–°ç‚¹</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%86%9A-BERT-vs-Attention-Is-All-You-Need%EF%BC%9A%E8%AE%BA%E6%96%87%E5%AF%B9%E6%AF%94"><span class="toc-text">ğŸ†š BERT vs Attention Is All You Needï¼šè®ºæ–‡å¯¹æ¯”</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF%E5%AF%B9%E6%AF%94-1"><span class="toc-text">è®ºæ–‡åŸºæœ¬ä¿¡æ¯å¯¹æ¯”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%85%B3%E7%B3%BB%EF%BC%9A%E7%BB%A7%E6%89%BF%E4%B8%8E%E5%88%9B%E6%96%B0"><span class="toc-text">æ ¸å¿ƒå…³ç³»ï¼šç»§æ‰¿ä¸åˆ›æ–°</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84%E5%AF%B9%E6%AF%94"><span class="toc-text">æ¶æ„å¯¹æ¯”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BERT-%E7%BB%A7%E6%89%BF%E4%BA%86%E4%BB%80%E4%B9%88%EF%BC%9F-1"><span class="toc-text">BERT ç»§æ‰¿äº†ä»€ä¹ˆï¼Ÿ</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%86%9A-BERT-vs-%E2%80%9CAttention-Is-All-You-Need%E2%80%9D-%E5%AF%B9%E6%AF%94"><span class="toc-text">ğŸ†š BERT vs â€œAttention Is All You Needâ€ å¯¹æ¯”</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF"><span class="toc-text">è®ºæ–‡åŸºæœ¬ä¿¡æ¯</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84%E5%AF%B9%E6%AF%94-1"><span class="toc-text">æ¶æ„å¯¹æ¯”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BERT-%E7%BB%A7%E6%89%BF%E4%BA%86-Transformer-%E7%9A%84%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-text">BERT ç»§æ‰¿äº† Transformer çš„ä»€ä¹ˆï¼Ÿ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BERT-%E7%9A%84%E5%88%9B%E6%96%B0%E7%82%B9-1"><span class="toc-text">BERT çš„åˆ›æ–°ç‚¹</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%81%94%E7%B3%BB%EF%BC%9ABERT-%E7%AB%99%E5%9C%A8-Transformer-%E8%82%A9%E8%86%80%E4%B8%8A"><span class="toc-text">è”ç³»ï¼šBERT ç«™åœ¨ Transformer è‚©è†€ä¸Š</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%86%9A-BERT-vs-%E2%80%9CAttention-Is-All-You-Need%E2%80%9D-%E5%AF%B9%E6%AF%94-1"><span class="toc-text">ğŸ†š BERT vs â€œAttention Is All You Needâ€ å¯¹æ¯”</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%BA%E6%96%87%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF%E5%AF%B9%E6%AF%94-2"><span class="toc-text">è®ºæ–‡åŸºæœ¬ä¿¡æ¯å¯¹æ¯”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E7%BB%A7%E6%89%BF%E5%85%B3%E7%B3%BB"><span class="toc-text">ç»§æ‰¿å…³ç³»</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84%E5%AF%B9%E6%AF%94-2"><span class="toc-text">æ¶æ„å¯¹æ¯”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E5%8C%BA%E5%88%AB%E6%80%BB%E7%BB%93"><span class="toc-text">æ ¸å¿ƒåŒºåˆ«æ€»ç»“</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%93%84-Part-1-BERT-%E8%AE%BA%E6%96%87%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90"><span class="toc-text">ğŸ“„ Part 1: BERT è®ºæ–‡æ·±åº¦è§£æ</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#1-1-%E8%AE%BA%E6%96%87%E5%9F%BA%E6%9C%AC%E4%BF%A1%E6%81%AF"><span class="toc-text">1.1 è®ºæ–‡åŸºæœ¬ä¿¡æ¯</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-2-%E7%A0%94%E7%A9%B6%E5%8A%A8%E6%9C%BA%EF%BC%9A%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81-BERT%EF%BC%9F"><span class="toc-text">1.2 ç ”ç©¶åŠ¨æœºï¼šä¸ºä»€ä¹ˆéœ€è¦ BERTï¼Ÿ</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%97%AE%E9%A2%98-1%EF%BC%9A%E5%8D%95%E5%90%91%E8%AF%AD%E8%A8%80%E6%A8%A1%E5%9E%8B%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-text">é—®é¢˜ 1ï¼šå•å‘è¯­è¨€æ¨¡å‹çš„å±€é™æ€§</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%97%AE%E9%A2%98-2%EF%BC%9A%E6%B5%85%E5%B1%82%E5%8F%8C%E5%90%91%E7%9A%84%E5%B1%80%E9%99%90%E6%80%A7"><span class="toc-text">é—®é¢˜ 2ï¼šæµ…å±‚åŒå‘çš„å±€é™æ€§</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-%E6%A0%B8%E5%BF%83%E6%96%B9%E6%B3%95%EF%BC%9A%E4%B8%A4%E4%B8%AA%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1"><span class="toc-text">1.3 æ ¸å¿ƒæ–¹æ³•ï¼šä¸¤ä¸ªé¢„è®­ç»ƒä»»åŠ¡</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1-1-Masked-Language-Model-MLM-%E6%A0%B8%E5%BF%83"><span class="toc-text">ä»»åŠ¡ 1: Masked Language Model (MLM) - æ ¸å¿ƒ</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%BB%BB%E5%8A%A1-2-Next-Sentence-Prediction-NSP"><span class="toc-text">ä»»åŠ¡ 2: Next Sentence Prediction (NSP)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-1-MLM-%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90%EF%BC%9A%E5%AE%8C%E5%BD%A2%E5%A1%AB%E7%A9%BA%E7%9A%84%E8%89%BA%E6%9C%AF"><span class="toc-text">1.3.1 MLM æ·±åº¦è§£æï¼šå®Œå½¢å¡«ç©ºçš„è‰ºæœ¯</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%9A%E4%BF%97%E7%90%86%E8%A7%A3"><span class="toc-text">é€šä¿—ç†è§£</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B7%E4%BD%93%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-text">å…·ä½“æ“ä½œæµç¨‹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88-MLM-%E8%83%BD%E8%AE%A9-BERT-%E5%AD%A6%E4%BC%9A%E2%80%9D%E7%90%86%E8%A7%A3%E8%AF%AD%E8%A8%80%E2%80%9D%EF%BC%9F"><span class="toc-text">ä¸ºä»€ä¹ˆ MLM èƒ½è®© BERT å­¦ä¼šâ€ç†è§£è¯­è¨€â€ï¼Ÿ</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MLM-%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B"><span class="toc-text">MLM è®­ç»ƒä»£ç ç¤ºä¾‹</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-3-2-%E9%A2%84%E8%AE%AD%E7%BB%83-%E5%BE%AE%E8%B0%83%E8%8C%83%E5%BC%8F%E8%AF%A6%E8%A7%A3"><span class="toc-text">1.3.2 é¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼è¯¦è§£</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%80%9A%E4%BF%97%E7%90%86%E8%A7%A3%EF%BC%9A%E5%9F%B9%E5%85%BB%E2%80%9D%E9%80%9A%E6%89%8D%E2%80%9D%E5%86%8D%E5%9F%B9%E5%85%BB%E2%80%9D%E4%B8%93%E6%89%8D%E2%80%9D"><span class="toc-text">é€šä¿—ç†è§£ï¼šåŸ¹å…»â€é€šæ‰â€å†åŸ¹å…»â€ä¸“æ‰â€</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BD%A2%E8%B1%A1%E7%B1%BB%E6%AF%94%E6%80%BB%E7%BB%93"><span class="toc-text">å½¢è±¡ç±»æ¯”æ€»ç»“</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E5%BE%AE%E8%B0%83%E5%AE%8C%E6%95%B4%E7%A4%BA%E4%BE%8B"><span class="toc-text">æƒ…æ„Ÿåˆ†æå¾®è°ƒå®Œæ•´ç¤ºä¾‹</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-%E8%AE%BA%E6%96%87%E5%85%B3%E9%94%AE%E5%88%9B%E6%96%B0%E7%82%B9%E6%B7%B1%E5%BA%A6%E5%89%96%E6%9E%90"><span class="toc-text">1.4 è®ºæ–‡å…³é”®åˆ›æ–°ç‚¹æ·±åº¦å‰–æ</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%AC-%E5%88%9B%E6%96%B0%E7%82%B9-1%EF%BC%9A%E6%B7%B1%E5%BA%A6%E5%8F%8C%E5%90%91%E4%B8%8A%E4%B8%8B%E6%96%87%E5%BB%BA%E6%A8%A1"><span class="toc-text">ğŸ”¬ åˆ›æ–°ç‚¹ 1ï¼šæ·±åº¦åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%AC-%E5%88%9B%E6%96%B0%E7%82%B9-2%EF%BC%9A%E9%A2%84%E8%AE%AD%E7%BB%83-%E5%BE%AE%E8%B0%83%E8%8C%83%E5%BC%8F"><span class="toc-text">ğŸ”¬ åˆ›æ–°ç‚¹ 2ï¼šé¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%AC-%E5%88%9B%E6%96%B0%E7%82%B9-3%EF%BC%9A%E7%BB%9F%E4%B8%80%E7%9A%84%E7%89%B9%E5%BE%81%E6%8F%90%E5%8F%96%E5%99%A8"><span class="toc-text">ğŸ”¬ åˆ›æ–°ç‚¹ 3ï¼šç»Ÿä¸€çš„ç‰¹å¾æå–å™¨</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-%E6%B6%88%E8%9E%8D%E5%AE%9E%E9%AA%8C%E8%AF%A6%E8%A7%A3-Ablation-Study"><span class="toc-text">1.5 æ¶ˆèå®éªŒè¯¦è§£ (Ablation Study)</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C-1%EF%BC%9A%E9%A2%84%E8%AE%AD%E7%BB%83%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text">å®éªŒ 1ï¼šé¢„è®­ç»ƒä»»åŠ¡çš„å½±å“</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C-2%EF%BC%9A%E6%A8%A1%E5%9E%8B%E8%A7%84%E6%A8%A1%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text">å®éªŒ 2ï¼šæ¨¡å‹è§„æ¨¡çš„å½±å“</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E9%AA%8C-3%EF%BC%9AMask-%E7%AD%96%E7%95%A5%E7%9A%84%E5%BD%B1%E5%93%8D"><span class="toc-text">å®éªŒ 3ï¼šMask ç­–ç•¥çš„å½±å“</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-6-%E8%AE%AD%E7%BB%83%E7%BB%86%E8%8A%82%E4%B8%8E%E8%B6%85%E5%8F%82%E6%95%B0"><span class="toc-text">1.6 è®­ç»ƒç»†èŠ‚ä¸è¶…å‚æ•°</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E9%A2%84%E8%AE%AD%E7%BB%83%E9%85%8D%E7%BD%AE"><span class="toc-text">é¢„è®­ç»ƒé…ç½®</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E9%85%8D%E7%BD%AE"><span class="toc-text">å¾®è°ƒé…ç½®</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-4-MLM-%E4%B8%8E%E5%BE%AE%E8%B0%83%E8%8C%83%E5%BC%8F%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90"><span class="toc-text">1.4 MLM ä¸å¾®è°ƒèŒƒå¼æ·±åº¦è§£æ</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%8E%AF-%E4%BB%80%E4%B9%88%E6%98%AF-MLM%EF%BC%88Masked-Language-Model%EF%BC%89%EF%BC%9F"><span class="toc-text">ğŸ¯ ä»€ä¹ˆæ˜¯ MLMï¼ˆMasked Language Modelï¼‰ï¼Ÿ</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MLM-%E5%85%B7%E4%BD%93%E6%93%8D%E4%BD%9C%E6%B5%81%E7%A8%8B"><span class="toc-text">MLM å…·ä½“æ“ä½œæµç¨‹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88-MLM-%E8%83%BD%E8%AE%A9-BERT-%E5%AD%A6%E4%BC%9A%E2%80%9D%E7%90%86%E8%A7%A3%E8%AF%AD%E8%A8%80%E2%80%9D%EF%BC%9F-1"><span class="toc-text">ä¸ºä»€ä¹ˆ MLM èƒ½è®© BERT å­¦ä¼šâ€ç†è§£è¯­è¨€â€ï¼Ÿ</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#MLM-%E8%AE%AD%E7%BB%83%E4%BB%A3%E7%A0%81%E7%A4%BA%E4%BE%8B-1"><span class="toc-text">MLM è®­ç»ƒä»£ç ç¤ºä¾‹</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%F0%9F%94%A7-%E4%BB%80%E4%B9%88%E6%98%AF%E5%BE%AE%E8%B0%83%E8%8C%83%E5%BC%8F%EF%BC%88Fine-tuning-Paradigm%EF%BC%89%EF%BC%9F"><span class="toc-text">ğŸ”§ ä»€ä¹ˆæ˜¯å¾®è°ƒèŒƒå¼ï¼ˆFine-tuning Paradigmï¼‰ï¼Ÿ</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BE%AE%E8%B0%83%E7%A4%BA%E4%BE%8B%EF%BC%9A%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="toc-text">å¾®è°ƒç¤ºä¾‹ï¼šæƒ…æ„Ÿåˆ†æå®Œæ•´ä»£ç </span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BD%A2%E8%B1%A1%E7%B1%BB%E6%AF%94%E6%80%BB%E7%BB%93-1"><span class="toc-text">å½¢è±¡ç±»æ¯”æ€»ç»“</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-MASK-Token-%E6%9C%BA%E5%88%B6%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90"><span class="toc-text">1.5 [MASK] Token æœºåˆ¶æ·±åº¦è§£æ</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#MASK-%E5%88%B0%E5%BA%95%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-text">[MASK] åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿ</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%8C%E6%95%B4%E8%BE%93%E5%85%A5%E5%AF%B9%E6%AF%94"><span class="toc-text">å®Œæ•´è¾“å…¥å¯¹æ¯”</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A8%A1%E5%9E%8B%E5%AE%9E%E9%99%85%E7%9C%8B%E5%88%B0%E7%9A%84%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F"><span class="toc-text">æ¨¡å‹å®é™…çœ‹åˆ°çš„æ˜¯ä»€ä¹ˆï¼Ÿ</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88-103-%E5%88%9D%E5%A7%8B%E6%97%B6%E5%95%A5%E4%B9%9F%E4%B8%8D%E4%BB%A3%E8%A1%A8%EF%BC%9F"><span class="toc-text">ä¸ºä»€ä¹ˆ 103 åˆå§‹æ—¶å•¥ä¹Ÿä¸ä»£è¡¨ï¼Ÿ</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-6-BERT-%E6%A8%A1%E5%9E%8B%E6%9E%B6%E6%9E%84%E8%AF%A6%E8%A7%A3"><span class="toc-text">1.6 BERT æ¨¡å‹æ¶æ„è¯¦è§£</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%9E%B6%E6%9E%84%E5%8F%82%E6%95%B0"><span class="toc-text">æ¶æ„å‚æ•°</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E8%A1%A8%E7%A4%BA-Input-Representation"><span class="toc-text">è¾“å…¥è¡¨ç¤º (Input Representation)</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#1-5-%E8%AE%BA%E6%96%87%E5%AE%9E%E9%AA%8C%E7%BB%93%E6%9E%9C%E4%B8%8E%E5%BD%B1%E5%93%8D"><span class="toc-text">1.5 è®ºæ–‡å®éªŒç»“æœä¸å½±å“</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E6%80%A7%E8%83%BD%E6%8F%90%E5%8D%87"><span class="toc-text">å…³é”®æ€§èƒ½æå‡</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%94%AC-Part-2-BERT-%E5%89%8D%E4%B8%A4%E5%B1%82%E5%AE%8C%E6%95%B4%E6%8E%A8%E6%BC%94"><span class="toc-text">ğŸ”¬ Part 2: BERT å‰ä¸¤å±‚å®Œæ•´æ¨æ¼”</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%BE%93%E5%85%A5%E5%87%86%E5%A4%87%EF%BC%9AMLM-%E4%BB%BB%E5%8A%A1"><span class="toc-text">è¾“å…¥å‡†å¤‡ï¼šMLM ä»»åŠ¡</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Layer-1%EF%BC%9A%E7%AC%AC%E4%B8%80%E5%B1%82%E8%AF%A6%E7%BB%86%E8%AE%A1%E7%AE%97"><span class="toc-text">Layer 1ï¼šç¬¬ä¸€å±‚è¯¦ç»†è®¡ç®—</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-1-1-%E8%AE%A1%E7%AE%97-Q%E3%80%81K%E3%80%81V"><span class="toc-text">Step 1.1: è®¡ç®— Qã€Kã€V</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-1-2-%E8%AE%A1%E7%AE%97%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0"><span class="toc-text">Step 1.2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-1-3-Softmax-%E5%BD%92%E4%B8%80%E5%8C%96"><span class="toc-text">Step 1.3: Softmax å½’ä¸€åŒ–</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-1-4-%E5%8A%A0%E6%9D%83%E6%B1%82%E5%92%8C%E5%BE%97%E5%88%B0%E6%96%B0%E8%A1%A8%E7%A4%BA"><span class="toc-text">Step 1.4: åŠ æƒæ±‚å’Œå¾—åˆ°æ–°è¡¨ç¤º</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-1-5-Feed-Forward-Network-FFN"><span class="toc-text">Step 1.5: Feed-Forward Network (FFN)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-1-6-%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5-LayerNorm"><span class="toc-text">Step 1.6: æ®‹å·®è¿æ¥ + LayerNorm</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Layer-2%EF%BC%9A%E7%AC%AC%E4%BA%8C%E5%B1%82%E8%AF%A6%E7%BB%86%E8%AE%A1%E7%AE%97"><span class="toc-text">Layer 2ï¼šç¬¬äºŒå±‚è¯¦ç»†è®¡ç®—</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-2-1-%E8%AE%A1%E7%AE%97%E6%96%B0%E7%9A%84-Q%E3%80%81K%E3%80%81V"><span class="toc-text">Step 2.1: è®¡ç®—æ–°çš„ Qã€Kã€V</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-2-2-%E8%AE%A1%E7%AE%97%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%88%E5%9F%BA%E4%BA%8E%E6%9B%B4%E6%96%B0%E5%90%8E%E7%9A%84%E8%A1%A8%E7%A4%BA%EF%BC%89"><span class="toc-text">Step 2.2: è®¡ç®—æ³¨æ„åŠ›ï¼ˆåŸºäºæ›´æ–°åçš„è¡¨ç¤ºï¼‰</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-2-3-%E5%8A%A0%E6%9D%83%E6%B1%82%E5%92%8C"><span class="toc-text">Step 2.3: åŠ æƒæ±‚å’Œ</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-2-4-FFN-%E6%AE%8B%E5%B7%AE-LayerNorm"><span class="toc-text">Step 2.4: FFN + æ®‹å·® + LayerNorm</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%A4%E5%B1%82%E5%AF%B9%E6%AF%94%E6%80%BB%E7%BB%93"><span class="toc-text">ä¸¤å±‚å¯¹æ¯”æ€»ç»“</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%94%8D-Part-3-Self-Attention-%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6"><span class="toc-text">ğŸ” Part 3: Self-Attention æ ¸å¿ƒæœºåˆ¶</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#2-0-%E8%BE%93%E5%85%A5%E5%87%86%E5%A4%87"><span class="toc-text">2.0 è¾“å…¥å‡†å¤‡</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-1-Layer-1%EF%BC%9A%E7%AC%AC%E4%B8%80%E5%B1%82%E8%AF%A6%E7%BB%86%E8%AE%A1%E7%AE%97"><span class="toc-text">2.1 Layer 1ï¼šç¬¬ä¸€å±‚è¯¦ç»†è®¡ç®—</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-1-1-%E8%AE%A1%E7%AE%97-Q%E3%80%81K%E3%80%81V-1"><span class="toc-text">Step 1.1: è®¡ç®— Qã€Kã€V</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-1-2-%E8%AE%A1%E7%AE%97%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0-1"><span class="toc-text">Step 1.2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-1-3-Softmax-%E5%BD%92%E4%B8%80%E5%8C%96-1"><span class="toc-text">Step 1.3: Softmax å½’ä¸€åŒ–</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-1-4-%E5%8A%A0%E6%9D%83%E6%B1%82%E5%92%8C%E5%BE%97%E5%88%B0%E6%96%B0%E8%A1%A8%E7%A4%BA-1"><span class="toc-text">Step 1.4: åŠ æƒæ±‚å’Œå¾—åˆ°æ–°è¡¨ç¤º</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-1-5-Feed-Forward-Network-FFN-1"><span class="toc-text">Step 1.5: Feed-Forward Network (FFN)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-1-6-%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5-LayerNorm-1"><span class="toc-text">Step 1.6: æ®‹å·®è¿æ¥ + LayerNorm</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-Layer-2%EF%BC%9A%E7%AC%AC%E4%BA%8C%E5%B1%82%E8%AF%A6%E7%BB%86%E8%AE%A1%E7%AE%97"><span class="toc-text">2.2 Layer 2ï¼šç¬¬äºŒå±‚è¯¦ç»†è®¡ç®—</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-2-1-%E8%AE%A1%E7%AE%97%E6%96%B0%E7%9A%84-Q%E3%80%81K%E3%80%81V-1"><span class="toc-text">Step 2.1: è®¡ç®—æ–°çš„ Qã€Kã€V</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-2-2-%E8%AE%A1%E7%AE%97%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%88%E5%9F%BA%E4%BA%8E%E6%9B%B4%E6%96%B0%E5%90%8E%E7%9A%84%E8%A1%A8%E7%A4%BA%EF%BC%89-1"><span class="toc-text">Step 2.2: è®¡ç®—æ³¨æ„åŠ›ï¼ˆåŸºäºæ›´æ–°åçš„è¡¨ç¤ºï¼‰</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-2-3-%E5%8A%A0%E6%9D%83%E6%B1%82%E5%92%8C-1"><span class="toc-text">Step 2.3: åŠ æƒæ±‚å’Œ</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-2-4-FFN-%E6%AE%8B%E5%B7%AE-LayerNorm-1"><span class="toc-text">Step 2.4: FFN + æ®‹å·® + LayerNorm</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E4%B8%A4%E5%B1%82%E5%AF%B9%E6%AF%94%E6%80%BB%E7%BB%93"><span class="toc-text">2.3 ä¸¤å±‚å¯¹æ¯”æ€»ç»“</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%94%8D-Part-3-Self-Attention-%E6%A0%B8%E5%BF%83%E6%9C%BA%E5%88%B6-1"><span class="toc-text">ğŸ” Part 3: Self-Attention æ ¸å¿ƒæœºåˆ¶</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-Q-x2F-K-x2F-V-%E7%9A%84%E6%9C%AC%E8%B4%A8%EF%BC%9A%E4%BF%A1%E6%81%AF%E6%A3%80%E7%B4%A2%E7%B3%BB%E7%BB%9F"><span class="toc-text">3.1 Q&#x2F;K&#x2F;V çš„æœ¬è´¨ï¼šä¿¡æ¯æ£€ç´¢ç³»ç»Ÿ</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E7%9B%B4%E8%A7%89%E7%B1%BB%E6%AF%94"><span class="toc-text">ç›´è§‰ç±»æ¯”</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E5%AD%A6%E5%AE%9A%E4%B9%89"><span class="toc-text">æ•°å­¦å®šä¹‰</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-2-%E5%AE%8C%E6%95%B4%E6%95%B0%E6%8D%AE%E6%B5%81%E4%B8%8E%E7%BB%B4%E5%BA%A6%E5%8F%98%E5%8C%96"><span class="toc-text">2.2 å®Œæ•´æ•°æ®æµä¸ç»´åº¦å˜åŒ–</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-1-%E7%BA%BF%E6%80%A7%E6%8A%95%E5%BD%B1"><span class="toc-text">Step 1: çº¿æ€§æŠ•å½±</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-2-%E6%8B%86%E5%88%86%E6%88%90%E5%A4%9A%E5%A4%B4"><span class="toc-text">Step 2: æ‹†åˆ†æˆå¤šå¤´</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-3-%E8%AE%A1%E7%AE%97%E6%B3%A8%E6%84%8F%E5%8A%9B%E5%88%86%E6%95%B0"><span class="toc-text">Step 3: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-4-Scale-%E5%92%8C-Softmax"><span class="toc-text">Step 4: Scale å’Œ Softmax</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-5-%E5%8A%A0%E6%9D%83%E6%B1%82%E5%92%8C"><span class="toc-text">Step 5: åŠ æƒæ±‚å’Œ</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Step-6-%E5%90%88%E5%B9%B6%E5%A4%9A%E5%A4%B4"><span class="toc-text">Step 6: åˆå¹¶å¤šå¤´</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#2-3-%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E5%A4%9A%E5%A4%B4%E6%B3%A8%E6%84%8F%E5%8A%9B%EF%BC%9F"><span class="toc-text">2.3 ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´æ³¨æ„åŠ›ï¼Ÿ</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%8D%95%E5%A4%B4%E7%9A%84%E5%B1%80%E9%99%90"><span class="toc-text">å•å¤´çš„å±€é™</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%A4%9A%E5%A4%B4%E7%9A%84%E4%BC%98%E5%8A%BF"><span class="toc-text">å¤šå¤´çš„ä¼˜åŠ¿</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%9A%A1-Part-4-KV-Cache-%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90"><span class="toc-text">âš¡ Part 4: KV Cache æ·±åº¦è§£æ</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-BERT-%E6%9C%89-Q-x2F-K-x2F-V-%E5%90%97%EF%BC%9F%E6%9C%89-KV-Cache-%E5%90%97%EF%BC%9F"><span class="toc-text">4.1 BERT æœ‰ Q&#x2F;K&#x2F;V å—ï¼Ÿæœ‰ KV Cache å—ï¼Ÿ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-BERT-%E7%9A%84-Q-x2F-K-x2F-V-%E8%AE%A1%E7%AE%97"><span class="toc-text">4.2 BERT çš„ Q&#x2F;K&#x2F;V è®¡ç®—</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-3-%E4%B8%BA%E4%BB%80%E4%B9%88-BERT-%E4%B8%8D%E9%9C%80%E8%A6%81-KV-Cache%EF%BC%9F"><span class="toc-text">4.3 ä¸ºä»€ä¹ˆ BERT ä¸éœ€è¦ KV Cacheï¼Ÿ</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%B3%E9%94%AE%E5%8C%BA%E5%88%AB%EF%BC%9A%E4%B8%80%E6%AC%A1%E6%80%A7%E5%A4%84%E7%90%86-vs-%E9%80%90%E4%B8%AA%E7%94%9F%E6%88%90"><span class="toc-text">å…³é”®åŒºåˆ«ï¼šä¸€æ¬¡æ€§å¤„ç† vs é€ä¸ªç”Ÿæˆ</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-4-%E4%BB%A3%E7%A0%81%E5%AF%B9%E6%AF%94%EF%BC%9ABERT-vs-GPT"><span class="toc-text">4.4 ä»£ç å¯¹æ¯”ï¼šBERT vs GPT</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#BERT%EF%BC%9A%E6%97%A0%E9%9C%80%E7%BC%93%E5%AD%98"><span class="toc-text">BERTï¼šæ— éœ€ç¼“å­˜</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#GPT%EF%BC%9A%E5%BF%85%E9%A1%BB%E7%BC%93%E5%AD%98"><span class="toc-text">GPTï¼šå¿…é¡»ç¼“å­˜</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-5-%E4%B8%BA%E4%BB%80%E4%B9%88-GPT-%E5%BF%85%E9%A1%BB%E7%BC%93%E5%AD%98-K%E3%80%81V%EF%BC%9F"><span class="toc-text">4.5 ä¸ºä»€ä¹ˆ GPT å¿…é¡»ç¼“å­˜ Kã€Vï¼Ÿ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-6-%E6%9E%B6%E6%9E%84%E6%80%BB%E7%BB%93%E5%AF%B9%E6%AF%94"><span class="toc-text">4.6 æ¶æ„æ€»ç»“å¯¹æ¯”</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-7-%E6%B7%B7%E5%90%88%E6%9E%B6%E6%9E%84%EF%BC%9AT5-x2F-BART"><span class="toc-text">4.7 æ··åˆæ¶æ„ï¼šT5&#x2F;BART</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-8-%E8%AE%B0%E5%BF%86%E5%8F%A3%E8%AF%80"><span class="toc-text">4.8 è®°å¿†å£è¯€</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%9A%A1-Part-5-KV-Cache-%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90"><span class="toc-text">âš¡ Part 5: KV Cache æ·±åº¦è§£æ</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#3-1-%E9%97%AE%E9%A2%98%E5%9C%BA%E6%99%AF%EF%BC%9A%E8%87%AA%E5%9B%9E%E5%BD%92%E7%94%9F%E6%88%90"><span class="toc-text">3.1 é—®é¢˜åœºæ™¯ï¼šè‡ªå›å½’ç”Ÿæˆ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-2-KV-Cache-%E8%A7%A3%E5%86%B3%E6%96%B9%E6%A1%88"><span class="toc-text">3.2 KV Cache è§£å†³æ–¹æ¡ˆ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-3-%E5%86%85%E5%AD%98%E6%B6%88%E8%80%97%E5%88%86%E6%9E%90"><span class="toc-text">3.3 å†…å­˜æ¶ˆè€—åˆ†æ</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%85%AC%E5%BC%8F"><span class="toc-text">å…¬å¼</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%AE%9E%E9%99%85%E6%A1%88%E4%BE%8B%EF%BC%9ALLaMA-7B"><span class="toc-text">å®é™…æ¡ˆä¾‹ï¼šLLaMA-7B</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#3-4-KV-Cache-%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="toc-text">3.4 KV Cache ä¼˜åŒ–æŠ€æœ¯</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#1-Multi-Query-Attention-MQA"><span class="toc-text">1. Multi-Query Attention (MQA)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#2-Grouped-Query-Attention-GQA"><span class="toc-text">2. Grouped-Query Attention (GQA)</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#3-PagedAttention-vLLM"><span class="toc-text">3. PagedAttention (vLLM)</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%8E%AD-Part-4-Causal-Attention-vs-Bidirectional-Attention"><span class="toc-text">ğŸ­ Part 4: Causal Attention vs Bidirectional Attention</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#4-1-%E6%A0%B8%E5%BF%83%E5%8C%BA%E5%88%AB"><span class="toc-text">4.1 æ ¸å¿ƒåŒºåˆ«</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#4-2-Causal-Mask-%E7%9A%84%E6%95%B0%E5%AD%A6%E5%AE%9E%E7%8E%B0"><span class="toc-text">4.2 Causal Mask çš„æ•°å­¦å®ç°</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Mask-%E7%9F%A9%E9%98%B5"><span class="toc-text">Mask çŸ©é˜µ</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#%E5%BA%94%E7%94%A8%E5%88%B0-Attention-Scores"><span class="toc-text">åº”ç”¨åˆ° Attention Scores</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%94%84-Part-5-%E8%AE%AD%E7%BB%83%E9%97%AD%E7%8E%AF-Label-x2F-Loss-x2F-%E6%A2%AF%E5%BA%A6%E6%B5%81"><span class="toc-text">ğŸ”„ Part 5: è®­ç»ƒé—­ç¯ - Label &#x2F; Loss &#x2F; æ¢¯åº¦æµ</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#5-1-MLM-%E4%BB%BB%E5%8A%A1%E7%9A%84%E5%AE%8C%E6%95%B4%E8%AE%AD%E7%BB%83%E6%B5%81%E7%A8%8B"><span class="toc-text">5.1 MLM ä»»åŠ¡çš„å®Œæ•´è®­ç»ƒæµç¨‹</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%95%B0%E6%8D%AE%E5%87%86%E5%A4%87"><span class="toc-text">æ•°æ®å‡†å¤‡</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Forward-Pass"><span class="toc-text">Forward Pass</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Loss-%E8%AE%A1%E7%AE%97"><span class="toc-text">Loss è®¡ç®—</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-2-%E6%A2%AF%E5%BA%A6%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD"><span class="toc-text">5.2 æ¢¯åº¦åå‘ä¼ æ’­</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E6%B5%81%E5%8A%A8%E8%B7%AF%E5%BE%84"><span class="toc-text">æ¢¯åº¦æµåŠ¨è·¯å¾„</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Attention-%E4%B8%AD%E7%9A%84%E6%A2%AF%E5%BA%A6%E5%88%86%E5%8F%89"><span class="toc-text">Attention ä¸­çš„æ¢¯åº¦åˆ†å‰</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#5-3-%E4%B8%BA%E4%BB%80%E4%B9%88-Attention-%E8%83%BD%E5%AD%A6%E5%88%B0%E8%AF%AD%E4%B9%89%EF%BC%9F"><span class="toc-text">5.3 ä¸ºä»€ä¹ˆ Attention èƒ½å­¦åˆ°è¯­ä¹‰ï¼Ÿ</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#%E6%A2%AF%E5%BA%A6%E7%9A%84%E2%80%9D%E6%8C%87%E5%AF%BC%E4%BD%9C%E7%94%A8%E2%80%9D"><span class="toc-text">æ¢¯åº¦çš„â€æŒ‡å¯¼ä½œç”¨â€</span></a></li></ol></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%93%9A-%E6%80%BB%E7%BB%93%EF%BC%9A%E6%A0%B8%E5%BF%83%E8%A6%81%E7%82%B9%E5%9B%9E%E9%A1%BE"><span class="toc-text">ğŸ“š æ€»ç»“ï¼šæ ¸å¿ƒè¦ç‚¹å›é¡¾</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#BERT-%E8%AE%BA%E6%96%87%E6%A0%B8%E5%BF%83%E8%B4%A1%E7%8C%AE"><span class="toc-text">BERT è®ºæ–‡æ ¸å¿ƒè´¡çŒ®</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q-x2F-K-x2F-V-%E6%9C%BA%E5%88%B6%E6%9C%AC%E8%B4%A8"><span class="toc-text">Q&#x2F;K&#x2F;V æœºåˆ¶æœ¬è´¨</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#KV-Cache-%E4%BC%98%E5%8C%96"><span class="toc-text">KV Cache ä¼˜åŒ–</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Causal-vs-Bidirectional"><span class="toc-text">Causal vs Bidirectional</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%AD%E7%BB%83%E9%97%AD%E7%8E%AF"><span class="toc-text">è®­ç»ƒé—­ç¯</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%86%9A-Part-8-Encoder-vs-Decoder-KV-Cache-%E5%AF%B9%E6%AF%94"><span class="toc-text">ğŸ†š Part 8: Encoder vs Decoder - KV Cache å¯¹æ¯”</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#%E6%A0%B8%E5%BF%83%E7%BB%93%E8%AE%BA"><span class="toc-text">æ ¸å¿ƒç»“è®º</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BERT-Encoder-only-%E4%B8%8D%E9%9C%80%E8%A6%81-KV-Cache"><span class="toc-text">BERT (Encoder-only) - ä¸éœ€è¦ KV Cache</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#GPT-Decoder-only-%E5%BF%85%E9%A1%BB%E6%9C%89-KV-Cache"><span class="toc-text">GPT (Decoder-only) - å¿…é¡»æœ‰ KV Cache</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88-Decoder-%E5%BF%85%E9%A1%BB%E7%BC%93%E5%AD%98-K%E3%80%81V%EF%BC%9F"><span class="toc-text">ä¸ºä»€ä¹ˆ Decoder å¿…é¡»ç¼“å­˜ Kã€Vï¼Ÿ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#BERT-%E6%9C%89-Q-x2F-K-x2F-V%EF%BC%8C%E4%BD%86%E4%B8%BA%E4%BB%80%E4%B9%88%E4%B8%8D%E9%9C%80%E8%A6%81%E7%BC%93%E5%AD%98%EF%BC%9F"><span class="toc-text">BERT æœ‰ Q&#x2F;K&#x2F;Vï¼Œä½†ä¸ºä»€ä¹ˆä¸éœ€è¦ç¼“å­˜ï¼Ÿ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E4%B8%80%E5%9B%BE%E6%80%BB%E7%BB%93"><span class="toc-text">ä¸€å›¾æ€»ç»“</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#%E8%AE%B0%E5%BF%86%E5%8F%A3%E8%AF%80"><span class="toc-text">è®°å¿†å£è¯€</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%9D%93-Part-9-%E5%B8%B8%E8%A7%81%E9%97%AE%E7%AD%94-FAQ"><span class="toc-text">â“ Part 9: å¸¸è§é—®ç­” FAQ</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#Q1-MASK-Token-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%83%BD%E9%A2%84%E6%B5%8B%E5%87%BA%E6%AD%A3%E7%A1%AE%E7%AD%94%E6%A1%88%EF%BC%9F"><span class="toc-text">Q1: [MASK] Token ä¸ºä»€ä¹ˆèƒ½é¢„æµ‹å‡ºæ­£ç¡®ç­”æ¡ˆï¼Ÿ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q2-BERT-%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8F%AA%E7%94%A8-Encoder%EF%BC%8C%E4%B8%8D%E7%94%A8-Decoder%EF%BC%9F"><span class="toc-text">Q2: BERT ä¸ºä»€ä¹ˆåªç”¨ Encoderï¼Œä¸ç”¨ Decoderï¼Ÿ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q3-%E6%AE%8B%E5%B7%AE%E8%BF%9E%E6%8E%A5%E6%98%AF%E4%BB%80%E4%B9%88%EF%BC%9F%E4%B8%BA%E4%BB%80%E4%B9%88%E9%9C%80%E8%A6%81%E5%AE%83%EF%BC%9F"><span class="toc-text">Q3: æ®‹å·®è¿æ¥æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼Ÿ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q4-%E9%A2%84%E8%AE%AD%E7%BB%83%E5%92%8C%E5%BE%AE%E8%B0%83%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-text">Q4: é¢„è®­ç»ƒå’Œå¾®è°ƒæœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q5-MLM-%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81-80-x2F-10-x2F-10-%E7%9A%84%E7%AD%96%E7%95%A5%EF%BC%9F"><span class="toc-text">Q5: MLM ä¸ºä»€ä¹ˆè¦ 80%&#x2F;10%&#x2F;10% çš„ç­–ç•¥ï¼Ÿ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q6-BERT-%E7%9A%84-Q%E3%80%81K%E3%80%81V-%E6%98%AF%E5%B9%B2%E4%BB%80%E4%B9%88%E7%9A%84%EF%BC%9F"><span class="toc-text">Q6: BERT çš„ Qã€Kã€V æ˜¯å¹²ä»€ä¹ˆçš„ï¼Ÿ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q7-Encoder-%E5%92%8C-Decoder-%E7%9A%84%E6%B3%A8%E6%84%8F%E5%8A%9B%E6%9C%89%E4%BB%80%E4%B9%88%E5%8C%BA%E5%88%AB%EF%BC%9F"><span class="toc-text">Q7: Encoder å’Œ Decoder çš„æ³¨æ„åŠ›æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q8-%E4%B8%BA%E4%BB%80%E4%B9%88-BERT-%E9%9C%80%E8%A6%81-CLS-%E5%92%8C-SEP-Token%EF%BC%9F"><span class="toc-text">Q8: ä¸ºä»€ä¹ˆ BERT éœ€è¦ [CLS] å’Œ [SEP] Tokenï¼Ÿ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q9-BERT-%E7%9A%84%E5%B1%82%E6%95%B0%E8%B6%8A%E5%A4%9A%E8%B6%8A%E5%A5%BD%E5%90%97%EF%BC%9F"><span class="toc-text">Q9: BERT çš„å±‚æ•°è¶Šå¤šè¶Šå¥½å—ï¼Ÿ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#Q10-BERT-%E5%92%8C-GPT-%E5%8F%AF%E4%BB%A5%E7%BB%93%E5%90%88%E4%BD%BF%E7%94%A8%E5%90%97%EF%BC%9F"><span class="toc-text">Q10: BERT å’Œ GPT å¯ä»¥ç»“åˆä½¿ç”¨å—ï¼Ÿ</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%94%97-%E5%BB%B6%E4%BC%B8%E9%98%85%E8%AF%BB"><span class="toc-text">ğŸ”— å»¶ä¼¸é˜…è¯»</span></a></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%9A%80-Part-6-BERT-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E4%B8%8E%E4%BB%A3%E7%A0%81%E5%AE%9E%E7%8E%B0"><span class="toc-text">ğŸš€ Part 6: BERT å®é™…åº”ç”¨åœºæ™¯ä¸ä»£ç å®ç°</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E6%80%BB%E8%A7%88"><span class="toc-text">6.1 åº”ç”¨åœºæ™¯æ€»è§ˆ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E5%AE%8C%E6%95%B4%E5%AE%9E%E7%8E%B0"><span class="toc-text">6.2 æƒ…æ„Ÿåˆ†æå®Œæ•´å®ç°</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB-NER-%E5%AE%9E%E7%8E%B0"><span class="toc-text">6.3 å‘½åå®ä½“è¯†åˆ« (NER) å®ç°</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-%E8%AF%AD%E4%B9%89%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%8C%B9%E9%85%8D"><span class="toc-text">6.4 è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F"><span class="toc-text">6.5 é˜…è¯»ç†è§£é—®ç­”ç³»ç»Ÿ</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%9A%A1-Part-7-KV-Cache-%E4%BC%98%E5%8C%96%E5%AE%9E%E8%B7%B5%E4%B8%8E%E4%BB%A3%E7%A0%81"><span class="toc-text">âš¡ Part 7: KV Cache ä¼˜åŒ–å®è·µä¸ä»£ç </span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-KV-Cache-%E5%8E%9F%E7%90%86%E5%8F%AF%E8%A7%86%E5%8C%96"><span class="toc-text">7.1 KV Cache åŸç†å¯è§†åŒ–</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-KV-Cache-%E5%AE%8C%E6%95%B4%E5%AE%9E%E7%8E%B0"><span class="toc-text">7.2 KV Cache å®Œæ•´å®ç°</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-KV-Cache-%E6%98%BE%E5%AD%98%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="toc-text">7.3 KV Cache æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-4-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5"><span class="toc-text">7.4 ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ</span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%F0%9F%9A%80-Part-6-BERT-%E5%AE%9E%E9%99%85%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E4%B8%8E%E4%BB%A3%E7%A0%81"><span class="toc-text">ğŸš€ Part 6: BERT å®é™…åº”ç”¨åœºæ™¯ä¸ä»£ç </span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#6-1-%E5%BA%94%E7%94%A8%E5%9C%BA%E6%99%AF%E6%80%BB%E8%A7%88-1"><span class="toc-text">6.1 åº”ç”¨åœºæ™¯æ€»è§ˆ</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-2-%E6%83%85%E6%84%9F%E5%88%86%E6%9E%90%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="toc-text">6.2 æƒ…æ„Ÿåˆ†æå®Œæ•´ä»£ç </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-3-%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB-NER-%E5%AE%8C%E6%95%B4%E4%BB%A3%E7%A0%81"><span class="toc-text">6.3 å‘½åå®ä½“è¯†åˆ« (NER) å®Œæ•´ä»£ç </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-4-%E8%AF%AD%E4%B9%89%E7%9B%B8%E4%BC%BC%E5%BA%A6%E5%8C%B9%E9%85%8D%E4%BB%A3%E7%A0%81"><span class="toc-text">6.4 è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ä»£ç </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#6-5-%E9%97%AE%E7%AD%94%E7%B3%BB%E7%BB%9F-%E9%98%85%E8%AF%BB%E7%90%86%E8%A7%A3-%E4%BB%A3%E7%A0%81"><span class="toc-text">6.5 é—®ç­”ç³»ç»Ÿ (é˜…è¯»ç†è§£) ä»£ç </span></a></li></ol></li><li class="toc-item toc-level-2"><a class="toc-link" href="#%E2%9A%A1-Part-7-KV-Cache-%E5%AE%9E%E6%88%98%E4%B8%8E%E4%BC%98%E5%8C%96"><span class="toc-text">âš¡ Part 7: KV Cache å®æˆ˜ä¸ä¼˜åŒ–</span></a><ol class="toc-child"><li class="toc-item toc-level-3"><a class="toc-link" href="#7-1-KV-Cache-%E5%AE%8C%E6%95%B4%E5%AE%9E%E7%8E%B0%E4%BB%A3%E7%A0%81"><span class="toc-text">7.1 KV Cache å®Œæ•´å®ç°ä»£ç </span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-2-KV-Cache-%E6%98%BE%E5%AD%98%E4%BC%98%E5%8C%96%E6%8A%80%E6%9C%AF"><span class="toc-text">7.2 KV Cache æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯</span></a><ol class="toc-child"><li class="toc-item toc-level-4"><a class="toc-link" href="#Multi-Query-Attention-MQA-%E5%AE%9E%E7%8E%B0"><span class="toc-text">Multi-Query Attention (MQA) å®ç°</span></a></li><li class="toc-item toc-level-4"><a class="toc-link" href="#Grouped-Query-Attention-GQA-%E5%AE%9E%E7%8E%B0"><span class="toc-text">Grouped-Query Attention (GQA) å®ç°</span></a></li></ol></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-3-%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E5%AF%B9%E6%AF%94%E8%A1%A8"><span class="toc-text">7.3 æ˜¾å­˜å ç”¨å¯¹æ¯”è¡¨</span></a></li><li class="toc-item toc-level-3"><a class="toc-link" href="#7-4-%E7%94%9F%E4%BA%A7%E7%8E%AF%E5%A2%83%E6%9C%80%E4%BD%B3%E5%AE%9E%E8%B7%B5-1"><span class="toc-text">7.4 ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ</span></a></li></ol></li></ol></li></ol></div><div class="widget-footer"><a class="top" onclick="util.scrollTop()"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><!-- Icon from Solar by 480 Design - https://creativecommons.org/licenses/by/4.0/ --><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="1.5"><path stroke-linejoin="round" d="m9 15.5l3-3l3 3m-6-4l3-3l3 3"/><path d="M7 3.338A9.95 9.95 0 0 1 12 2c5.523 0 10 4.477 10 10s-4.477 10-10 10S2 17.523 2 12c0-1.821.487-3.53 1.338-5"/></g></svg><span>Scroll to Top</span></a><a class="buttom" onclick="util.scrollComment()"><svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><!-- Icon from Solar by 480 Design - https://creativecommons.org/licenses/by/4.0/ --><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="1.5" d="M8 10.5h8M8 14h5.5M17 3.338A9.95 9.95 0 0 0 12 2C6.477 2 2 6.477 2 12c0 1.6.376 3.112 1.043 4.453c.178.356.237.763.134 1.148l-.595 2.226a1.3 1.3 0 0 0 1.591 1.592l2.226-.596a1.63 1.63 0 0 1 1.149.133A9.96 9.96 0 0 0 12 22c5.523 0 10-4.477 10-10c0-1.821-.487-3.53-1.338-5"/></svg><span>Join Discussion</span></a></div></widget>
</div></aside><div class='float-panel'>
  <button type='button' style='display:none' class='laptop-only rightbar-toggle mobile' onclick='sidebar.rightbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><path fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2" d="M10 6h11m-11 6h11m-11 6h11M4 6h1v4m-1 0h2m0 8H4c0-1 2-2 2-3s-1-1.5-2-1"/></svg>
  </button>
  <button type='button' style='display:none' class='mobile-only leftbar-toggle mobile' onclick='sidebar.leftbar()'>
    <svg xmlns="http://www.w3.org/2000/svg" width="32" height="32" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-width="1.5"><path d="M2 11c0-3.771 0-5.657 1.172-6.828C4.343 3 6.229 3 10 3h4c3.771 0 5.657 0 6.828 1.172C22 5.343 22 7.229 22 11v2c0 3.771 0 5.657-1.172 6.828C19.657 21 17.771 21 14 21h-4c-3.771 0-5.657 0-6.828-1.172C2 18.657 2 16.771 2 13z"/><path id="sep" stroke-linecap="round" d="M5.5 10h6m-5 4h4m4.5 7V3"/></g></svg>
  </button>
</div>
</div><div class="scripts">


<script type="text/javascript">
  window.canonical = {"closeEnable":true,"closeText":"å…³é—­æç¤º","originalHost":null,"officialHosts":["localhost"],"encoded":""};
  window.canonical["param"] = {"permalink":"https://caiusy.github.io/2026/02/03/bert-comprehensive-guide/","checklink":"/js/plugins/video.js"};
  const ctx = {
    date_suffix: {
      just: `Just`,
      min: `minutes ago`,
      hour: `hours ago`,
      day: `days ago`,
    },
    root : `/`,
    tag_plugins: {
      chat: Object.assign({"api":"https://siteinfo.listentothewind.cn/api/v1"}),
    }
  };

  // required plugins (only load if needs)
  if (`local_search`) {
    ctx.search = {};
    ctx.search.service = `local_search`;
    if (ctx.search.service == 'local_search') {
      let service_obj = Object.assign({}, `{"field":"all","path":"/search.json","content":true,"skip_search":null,"sort":"-date"}`);
      ctx.search[ctx.search.service] = service_obj;
    }
  }
  const def = {
    avatar: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/avatar/round/3442075.svg`,
    cover: `https://gcore.jsdelivr.net/gh/cdn-x/placeholder@1.0.12/cover/76b86c0226ffd.svg`,
    loading: `https://api.iconify.design/eos-icons:three-dots-loading.svg?color=%231cd0fd`,
  };
  const deps = {
    jquery: `https://gcore.jsdelivr.net/npm/jquery@3.7/dist/jquery.min.js`,
    marked: `https://gcore.jsdelivr.net/npm/marked@13.0/lib/marked.umd.min.js`,
    lazyload: `/%5Bobject%20Object%5D`
  }
  

</script>

<script type="text/javascript">
  
  function RunItem() {
    this.list = []; // å­˜æ”¾å›è°ƒå‡½æ•°
    this.start = () => {
      for (var i = 0; i < this.list.length; i++) {
        this.list[i].run();
      }
    };
    this.push = (fn, name, setRequestAnimationFrame = true) => {
      let myfn = fn
      if (setRequestAnimationFrame) {
        myfn = () => {
          utils.requestAnimationFrame(fn)
        }
      }
      var f = new Item(myfn, name);
      this.list.push(f);
    };
    this.remove = (name) => {
      for (let index = 0; index < this.list.length; index++) {
        const e = this.list[index];
        if (e.name == name) {
          this.list.splice(index, 1);
        }
      }
    }
    // æ„é€ ä¸€ä¸ªå¯ä»¥runçš„å¯¹è±¡
    function Item(fn, name) {
      // å‡½æ•°åç§°
      this.name = name || fn.name;
      // runæ–¹æ³•
      this.run = () => {
        try {
          fn()
        } catch (error) {
          console.log(error);
        }
      };
    }
  }

  const utils = {
    // æ‡’åŠ è½½ css https://github.com/filamentgroup/loadCSS
    css: (href, before, media, attributes) => {
      var doc = window.document;
      var ss = doc.createElement("link");
      var ref;
      if (before) {
        ref = before;
      } else {
        var refs = (doc.body || doc.getElementsByTagName("head")[0]).childNodes;
        ref = refs[refs.length - 1];
      }
      var sheets = doc.styleSheets;
      if (attributes) {
        for (var attributeName in attributes) {
          if (attributes.hasOwnProperty(attributeName)) {
            ss.setAttribute(attributeName, attributes[attributeName]);
          }
        }
      }
      ss.rel = "stylesheet";
      ss.href = href;
      ss.media = "only x";
      function ready(cb) {
        if (doc.body) {
          return cb();
        }
        setTimeout(function () {
          ready(cb);
        });
      }
      ready(function () {
        ref.parentNode.insertBefore(ss, before ? ref : ref.nextSibling);
      });
      var onloadcssdefined = function (cb) {
        var resolvedHref = ss.href;
        var i = sheets.length;
        while (i--) {
          if (sheets[i].href === resolvedHref) {
            return cb();
          }
        }
        setTimeout(function () {
          onloadcssdefined(cb);
        });
      };
      function loadCB() {
        if (ss.addEventListener) {
          ss.removeEventListener("load", loadCB);
        }
        ss.media = media || "all";
      }
      if (ss.addEventListener) {
        ss.addEventListener("load", loadCB);
      }
      ss.onloadcssdefined = onloadcssdefined;
      onloadcssdefined(loadCB);
      return ss;
    },

    js: (src, opt) => new Promise((resolve, reject) => {
      var script = document.createElement('script');
      if (src.startsWith('/')) {
        src = ctx.root + src.substring(1);
      }
      script.src = src;
      if (opt) {
        for (let key of Object.keys(opt)) {
          script[key] = opt[key]
        }
      } else {
        // é»˜è®¤å¼‚æ­¥ï¼Œå¦‚æœéœ€è¦åŒæ­¥ï¼Œç¬¬äºŒä¸ªå‚æ•°ä¼ å…¥ {} å³å¯
        script.async = true
      }
      script.onerror = reject
      script.onload = script.onreadystatechange = function () {
        const loadState = this.readyState
        if (loadState && loadState !== 'loaded' && loadState !== 'complete') return
        script.onload = script.onreadystatechange = null
        resolve()
      }
      document.head.appendChild(script)
    }),

    jq: (fn) => {
      if (typeof jQuery === 'undefined') {
        utils.js(deps.jquery).then(fn)
      } else {
        fn()
      }
    },

    onLoading: (el) => {
      if (el) {
        if ($(el).find('.loading-wrap').length == 0){
          $(el).append('<div class="loading-wrap"><svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" stroke-opacity=".3" d="M12 3C16.9706 3 21 7.02944 21 12C21 16.9706 16.9706 21 12 21C7.02944 21 3 16.9706 3 12C3 7.02944 7.02944 3 12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="1.3s" values="60;0"/></path><path stroke-dasharray="15" stroke-dashoffset="15" d="M12 3C16.9706 3 21 7.02944 21 12"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.3s" values="15;0"/><animateTransform attributeName="transform" dur="1.5s" repeatCount="indefinite" type="rotate" values="0 12 12;360 12 12"/></path></g></svg></div>');
        }
      }
    },
    onLoadSuccess: (el) => {
      if (el) {
        $(el).find('.loading-wrap').remove();
      }
    },
    onLoadFailure: (el) => {
      if (el) {
        $(el).find('.loading-wrap svg').remove();
        $(el).find('.loading-wrap').append('<svg xmlns="http://www.w3.org/2000/svg" width="2em" height="2em" preserveAspectRatio="xMidYMid meet" viewBox="0 0 24 24"><g fill="none" stroke="currentColor" stroke-linecap="round" stroke-linejoin="round" stroke-width="2"><path stroke-dasharray="60" stroke-dashoffset="60" d="M12 3L21 20H3L12 3Z"><animate fill="freeze" attributeName="stroke-dashoffset" dur="0.5s" values="60;0"/></path><path stroke-dasharray="6" stroke-dashoffset="6" d="M12 10V14"><animate fill="freeze" attributeName="stroke-dashoffset" begin="0.6s" dur="0.2s" values="6;0"/></path></g><circle cx="12" cy="17" r="1" fill="currentColor" fill-opacity="0"><animate fill="freeze" attributeName="fill-opacity" begin="0.8s" dur="0.4s" values="0;1"/></circle></svg>');
        $(el).find('.loading-wrap').addClass('error');
      }
    },
    request: (el, url, callback, onFailure) => {
      const maxRetry = 3;
      let retryCount = 0;

      return new Promise((resolve, reject) => {
        const load = () => {
          utils.onLoading?.(el);

          let timedOut = false;
          const timeout = setTimeout(() => {
            timedOut = true;
            console.warn('[request] è¶…æ—¶:', url);

            if (++retryCount >= maxRetry) {
              utils.onLoadFailure?.(el);
              onFailure?.();
              reject('è¯·æ±‚è¶…æ—¶');
            } else {
              setTimeout(load, 1000);
            }
          }, 5000);

          fetch(url).then(resp => {
            if (timedOut) return;
            clearTimeout(timeout);

            if (!resp.ok) throw new Error('å“åº”å¤±è´¥');
            return resp;
          }).then(data => {
            if (timedOut) return;
            utils.onLoadSuccess?.(el);
            callback(data);
            resolve(data);
          }).catch(err => {
            clearTimeout(timeout);
            console.warn('[request] é”™è¯¯:', err);

            if (++retryCount >= maxRetry) {
              utils.onLoadFailure?.(el);
              onFailure?.();
              reject(err);
            } else {
              setTimeout(load, 1000);
            }
          });
        };

        load();
      });
    },
    requestWithoutLoading: (url, options = {}, maxRetry = 2, timeout = 5000) => {
      return new Promise((resolve, reject) => {
        let retryCount = 0;

        const tryRequest = () => {
          let timedOut = false;
          const timer = setTimeout(() => {
            timedOut = true;
            if (++retryCount > maxRetry) reject('timeout');
            else tryRequest();
          }, timeout);

          fetch(url, options)
            .then(resp => {
              clearTimeout(timer);
              if (!resp.ok) throw new Error('bad response');
              resolve(resp);
            })
            .catch(err => {
              clearTimeout(timer);
              if (++retryCount > maxRetry) reject(err);
              else setTimeout(tryRequest, 500);
            });
        };

        tryRequest();
      });
    },
    /********************** requestAnimationFrame ********************************/
    // 1ã€requestAnimationFrame ä¼šæŠŠæ¯ä¸€å¸§ä¸­çš„æ‰€æœ‰ DOM æ“ä½œé›†ä¸­èµ·æ¥ï¼Œåœ¨ä¸€æ¬¡é‡ç»˜æˆ–å›æµä¸­å°±å®Œæˆï¼Œå¹¶ä¸”é‡ç»˜æˆ–å›æµçš„æ—¶é—´é—´éš”ç´§ç´§è·Ÿéšæµè§ˆå™¨çš„åˆ·æ–°é¢‘ç‡ï¼Œä¸€èˆ¬æ¥è¯´ï¼Œè¿™ä¸ªé¢‘ç‡ä¸ºæ¯ç§’60å¸§ã€‚
    // 2ã€åœ¨éšè—æˆ–ä¸å¯è§çš„å…ƒç´ ä¸­ï¼ŒrequestAnimationFrame å°†ä¸ä¼šè¿›è¡Œé‡ç»˜æˆ–å›æµï¼Œè¿™å½“ç„¶å°±æ„å‘³ç€æ›´å°‘çš„çš„ cpuï¼Œgpu å’Œå†…å­˜ä½¿ç”¨é‡ã€‚
    requestAnimationFrame: (fn) => {
      if (!window.requestAnimationFrame) {
        window.requestAnimationFrame = window.requestAnimationFrame || window.mozRequestAnimationFrame || window.webkitRequestAnimationFrame;
      }
      window.requestAnimationFrame(fn)
    },
    dark: {},
  };

  // utils.dark.mode å½“å‰æ¨¡å¼ dark or light
  // utils.dark.toggle() æš—é»‘æ¨¡å¼è§¦å‘å™¨
  // utils.dark.push(callBack[,"callBackName"]) ä¼ å…¥è§¦å‘å™¨å›è°ƒå‡½æ•°
  utils.dark.method = {
    toggle: new RunItem(),
  };
  utils.dark = Object.assign(utils.dark, {
    push: utils.dark.method.toggle.push,
  });
</script>
<script>
  const sidebar = {
    leftbar: () => {
      if (l_body) {
        l_body.toggleAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    rightbar: () => {
      if (l_body) {
        l_body.toggleAttribute('rightbar');
        l_body.removeAttribute('leftbar');
      }
    },
    dismiss: () => {
      if (l_body) {
        l_body.removeAttribute('leftbar');
        l_body.removeAttribute('rightbar');
      }
    },
    toggleTOC: () => {
      document.querySelector('#data-toc').classList.toggle('collapse');
    }
  }
</script>
<script type="text/javascript">
  (() => {
    const tagSwitchers = document.querySelectorAll('.tag-subtree.parent-tag > a > .tag-switcher-wrapper')
    for (const tagSwitcher of tagSwitchers) {
      tagSwitcher.addEventListener('click', (e) => {
        const parent = e.target.closest('.tag-subtree.parent-tag')
        parent.classList.toggle('expanded')
        e.preventDefault()
      })
    }

    // Get active tag from query string, then activate it.
    const urlParams = new URLSearchParams(window.location.search)
    const activeTag = urlParams.get('tag')
    if (activeTag) {
      let tag = document.querySelector(`.tag-subtree[data-tag="${activeTag}"]`)
      if (tag) {
        tag.querySelector('a').classList.add('active')
        
        while (tag) {
          tag.classList.add('expanded')
          tag = tag.parentElement.closest('.tag-subtree.parent-tag')
        }
      }
    }
  })()
</script>

<script async src="https://gcore.jsdelivr.net/npm/vanilla-lazyload@19.1/dist/lazyload.min.js"></script>
<script>
  // https://www.npmjs.com/package/vanilla-lazyload
  // Set the options globally
  // to make LazyLoad self-initialize
  window.lazyLoadOptions = {
    elements_selector: ".lazy",
    callback_loaded: (el) => {
      el.classList.add('loaded');
      const wrapper = el.closest('.lazy-box');
      const icon = wrapper?.querySelector('.lazy-icon');
      if (icon) icon.remove();
    }
  };
  // Listen to the initialization event
  // and get the instance of LazyLoad
  window.addEventListener(
    "LazyLoad::Initialized",
    function (event) {
      window.lazyLoadInstance = event.detail.instance;
    },
    false
  );
  document.addEventListener('DOMContentLoaded', function () {
    window.lazyLoadInstance?.update();
  });

  window.wrapLazyloadImages = (container) => {
    if (typeof container === 'string') {
      container = document.querySelector(container);
    }
    if (!container) return;
    
    const images = container.querySelectorAll('img');
    images.forEach((img) => {
      if (img.classList.contains('lazy')) return;

      const src = img.getAttribute('src');
      if (!src) return;

      const wrapper = document.createElement('div');
      wrapper.className = 'lazy-box';

      const newImg = img.cloneNode();
      newImg.removeAttribute('src');
      newImg.setAttribute('data-src', src);
      newImg.classList.add('lazy');

      const icon = document.createElement('div');
      icon.className = 'lazy-icon';
      if (def.loading) {
        icon.style.backgroundImage = `url("${def.loading}")`;
      }

      wrapper.appendChild(newImg);
      wrapper.appendChild(icon);

      img.replaceWith(wrapper);
    });

    // é€šçŸ¥ LazyLoad æ›´æ–°
    if (window.lazyLoadInstance?.update) {
      window.lazyLoadInstance.update();
    }
  }
  
</script>

<!-- required -->
<script src="/js/main.js?v=1.33.1" defer></script>

<script type="text/javascript">
  const applyTheme = (theme) => {
    if (theme === 'auto') {
      document.documentElement.removeAttribute('data-theme')
    } else {
      document.documentElement.setAttribute('data-theme', theme)
    }

    // applyThemeToGiscus(theme)
  }

  // FIXME: è¿™ä¼šå¯¼è‡´æ— æ³•ä½¿ç”¨ preferred_color_scheme ä»¥å¤–çš„ä¸»é¢˜
  const applyThemeToGiscus = (theme) => {
    // theme = theme === 'auto' ? 'preferred_color_scheme' : theme
    const cmt = document.getElementById('giscus')
    if (cmt) {
      // This works before giscus load.
      cmt.setAttribute('data-theme', theme)
    }

    const iframe = document.querySelector('#comments > section.giscus > iframe')
    if (iframe) {
      // This works after giscus loaded.
      const src = iframe.src
      const newSrc = src.replace(/theme=[\w]+/, `theme=${theme}`)
      iframe.src = newSrc
    }
  }

  const switchTheme = () => {
    // light -> dark -> auto -> light -> ...
    const currentTheme = document.documentElement.getAttribute('data-theme')
    let newTheme;
    switch (currentTheme) {
      case 'light':
        newTheme = 'dark'
        break
      case 'dark':
        newTheme = 'auto'
        break
      default:
        newTheme = 'light'
    }
    applyTheme(newTheme)
    window.localStorage.setItem('Stellar.theme', newTheme)
    utils.dark.mode = newTheme === 'auto' ? (window.matchMedia("(prefers-color-scheme: dark)").matches ? "dark" : "light") : newTheme;
    utils.dark.method.toggle.start();

    const messages = {
      light: `Switched to Light Mode`,
      dark: `Switched to Dark Mode`,
      auto: `Switched to Auto Mode`,
    }
    hud?.toast?.(messages[newTheme])
  }

  (() => {
    // Apply user's preferred theme, if any.
    const theme = window.localStorage.getItem('Stellar.theme')
    if (theme !== null) {
      applyTheme(theme)
    } else {
      utils.dark.mode = window.matchMedia("(prefers-color-scheme: dark)").matches ? "dark" : "light";
    }
    utils.dark.method.toggle.start();
  })()
</script>


<!-- optional -->

  <script type="module">
  const el = document.querySelector('#comments #giscus');
  util.viewportLazyload(el, load_discus, true);

  function load_discus() {
    if (!el) return;
    try {
        el.innerHTML = '';
      } catch (error) {
        console.error(error);
      }
      const script = document.createElement('script');
      script.async = true;
      for (const key of Object.keys(el.attributes)) {
        const attr = el.attributes[key];
        if (['class', 'id'].includes(attr.name) === false) {
          script.setAttribute(attr.name, attr.value);
        }
      }
      el.appendChild(script);
  }
</script>




<script defer>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.services = Object.assign({}, JSON.parse(`{"mdrender":{"js":"/js/services/mdrender.js"},"siteinfo":{"js":"/js/services/siteinfo.js","api":null},"ghinfo":{"js":"/js/services/ghinfo.js"},"rating":{"js":"/js/services/rating.js","api":"https://star-vote.xaox.cc/api/rating"},"vote":{"js":"/js/services/vote.js","api":"https://star-vote.xaox.cc/api/vote"},"sites":{"js":"/js/services/sites.js"},"friends":{"js":"/js/services/friends.js"},"friends_and_posts":{"js":"/js/services/friends_and_posts.js"},"timeline":{"js":"/js/services/timeline.js"},"fcircle":{"js":"/js/services/fcircle.js"},"weibo":{"js":"/js/services/weibo.js"},"memos":{"js":"/js/services/memos.js"},"voice":{"js":"/js/plugins/voice.js"},"video":{"js":"/js/plugins/video.js"},"download-file":{"js":"/js/plugins/download-file.js"},"twikoo":{"js":"/js/services/twikoo_latest_comment.js"},"waline":{"js":"/js/services/waline_latest_comment.js"},"artalk":{"js":"/js/services/artalk_latest_comment.js"},"giscus":{"js":"/js/services/giscus_latest_comment.js"},"contributors":{"edit_this_page":{"_posts/":null,"wiki/stellar/":"https://github.com/xaoxuu/hexo-theme-stellar-docs/blob/main/"},"js":"/js/services/contributors.js"},"rss":{"js":"/js/services/rss.js"}}`));
    for (let id of Object.keys(ctx.services)) {
      const js = ctx.services[id].js;
      if (id == 'siteinfo') {
        ctx.cardlinks = document.querySelectorAll('a.link-card[cardlink]');
        if (ctx.cardlinks?.length > 0) {
          utils.js(js, { defer: true }).then(function () {
            setCardLink(ctx.cardlinks);
          });
        }
      } else if (id == 'voice') {
        ctx.voiceAudios = document.querySelectorAll('.voice>audio');
        if (ctx.voiceAudios?.length > 0) {
          utils.js(js, { defer: true }).then(function () {
            createVoiceDom(ctx.voiceAudios);
          });
        }
      } else if (id == 'video') {
        ctx.videos = document.querySelectorAll('.video>video');
        if (ctx.videos?.length > 0) {
          utils.js(js, { defer: true }).then(function () {
            videoEvents(ctx.videos);
          });
        }
      } else if (id == 'download-file') {
        ctx.files = document.querySelectorAll('.chat-file');
        if (ctx.files?.length > 0) {
          utils.js(js, { defer: true }).then(function () {
            downloadFileEvent(ctx.files);
          });
        }
      } else {
        const els = document.getElementsByClassName(`ds-${id}`);
        if (els?.length > 0) {
          utils.jq(() => {
            if (id == 'timeline' || 'memos' || 'marked') {
              utils.js(deps.marked).then(function () {
                utils.js(js, { defer: true });
              });
            } else {
              utils.js(js, { defer: true });
            }
          });
        }
      }
    }

    // chat iphone time
    let phoneTimes = document.querySelectorAll('.chat .status-bar .time');

    if (phoneTimes.length > 0) {
      NowTime();
      var date = new Date();
      var sec = date.getSeconds();
      var firstAdjustInterval = setInterval(firstAdjustTime, 1000 * (60 - sec));
    }

    function firstAdjustTime() {
      NowTime();
      clearInterval(firstAdjustInterval);
      setInterval(NowTime, 1000 * 60);
    }

    function NowTime() {
      for (let i = 0; i < phoneTimes.length; ++i) {
        var timeSpan = phoneTimes[i];
        var date = new Date();
        var hour = date.getHours();
        var min = date.getMinutes();
        timeSpan.innerHTML = check(hour) + ":" + check(min);
      }
    };

    function check(val) {
      if (val < 10) {
        return ("0" + val);
      }
      return (val);
    }

    // chat quote
    const chat_quote_obverser = new IntersectionObserver((entries, observer) => {
      entries.filter((entry) => { return entry.isIntersecting }).sort((a, b) => a.intersectionRect.y !== b.intersectionRect.y ? a.intersectionRect.y - b.intersectionRect.y : a.intersectionRect.x - b.intersectionRect.x).forEach((entry, index) => {
          observer.unobserve(entry.target);
          setTimeout(() => {
            entry.target.classList.add('quote-blink');
            setTimeout(() => {
              entry.target.classList.remove('quote-blink');
            }, 1000);
          }, Math.max(100, 16) * (index + 1));
        });
    });

    var chatQuotes = document.querySelectorAll(".chat .talk .quote");
    chatQuotes.forEach((quote) => {
      quote.addEventListener('click', function () {
        var chatCellDom = document.getElementById("quote-" + quote.getAttribute("quotedCellTag"));
        if (chatCellDom) {
          var chatDiv = chatCellDom.parentElement;
          var mid = chatDiv.clientHeight / 2;
          var offsetTop = chatCellDom.offsetTop;
          if (offsetTop > mid - chatCellDom.clientHeight / 2) {
            chatDiv.scrollTo({
              top: chatCellDom.offsetTop - mid + chatCellDom.clientHeight / 2,
              behavior: "smooth"
            });
          } else {
            chatDiv.scrollTo({
              top: 0,
              behavior: "smooth"
            });
          }
          chat_quote_obverser.observe(chatCellDom);
        }
      });
    });
  });
</script>

<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    ctx.search = {
      path: `/search.json`,
    }
    utils.js('/js/search/local-search.js', { defer: true });
  });
</script><script>
  window.FPConfig = {
    delay: 0,
    ignoreKeywords: [],
    maxRPS: 5,
    hoverDelay: 25
  };
</script>
<script defer src="https://gcore.jsdelivr.net/npm/flying-pages@2/flying-pages.min.js"></script><script>
  ctx.fancybox = {
    selector: `.md-text img:not([class]), .md-text .image img, .timenode p>img`,
    css: `https://gcore.jsdelivr.net/npm/@fancyapps/ui@5.0/dist/fancybox/fancybox.css`,
    js: `https://gcore.jsdelivr.net/npm/@fancyapps/ui@5.0/dist/fancybox/fancybox.umd.js`
  };
  var selector = '[data-fancybox]:not(.error), .with-fancybox .atk-content img:not([atk-emoticon])';
  if (ctx.fancybox.selector) {
    selector += `, ${ctx.fancybox.selector}`
  }
  var needFancybox = document.querySelectorAll(selector).length !== 0;
  if (!needFancybox) {
    const memos = document.getElementsByClassName('ds-memos');
    if (memos != undefined && memos.length > 0) {
      needFancybox = true;
    }
    const fancybox = document.getElementsByClassName('with-fancybox');
    if (fancybox != undefined && fancybox.length > 0) {
      needFancybox = true;
    }
  }
  if (needFancybox) {
    utils.css(ctx.fancybox.css);
    utils.js(ctx.fancybox.js, { defer: true }).then(function () {
      Fancybox.bind(selector, {
        hideScrollbar: false,
        Thumbs: {
          autoStart: false,
        },
        caption: (fancybox, slide) => {
          return slide.triggerEl.alt || slide.triggerEl.dataset.caption || null
        }
      });
    })
  }
</script>
<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    const swiper_api = document.getElementById('swiper-api');
    if (swiper_api != undefined) {
      utils.css(`https://unpkg.com/swiper@10.3/swiper-bundle.min.css`);
      utils.js(`https://unpkg.com/swiper@10.3/swiper-bundle.min.js`, { defer: true }).then(function () {
        const effect = swiper_api.getAttribute('effect') || '';
        var swiper = new Swiper('.swiper#swiper-api', {
          slidesPerView: 'auto',
          spaceBetween: 8,
          centeredSlides: true,
          effect: effect,
          rewind: true,
          pagination: {
            el: '.swiper-pagination',
            clickable: true,
          },
          navigation: {
            nextEl: '.swiper-button-next',
            prevEl: '.swiper-button-prev',
          },
        });
      })
    }
  });
</script>
<script defer src="https://gcore.jsdelivr.net/npm/scrollreveal@4.0/dist/scrollreveal.min.js"></script>
<script>
  window.addEventListener('DOMContentLoaded', (event) => {
    const slideUp = {
      distance: `16px`,
      duration: `600`,
      interval: `100`,
      scale: `1`,
      opacity: 0,
      easing: "ease-out"
    };
    ScrollReveal().reveal('.slide-up', { ...slideUp });
  });
</script>

<script>
  window.MathJax = {
    tex: {
      inlineMath: [
        ["$", "$"],
        ["\\(", "\\)"],
      ],
      processEscapes: true,
      skipTags: ["script", "noscript", "style", "textarea", "pre", "code"],
    },
    startup: {
      ready() {
        MathJax.startup.defaultReady();
        MathJax.typesetPromise().then(() => {
          const math = document.querySelectorAll("mjx-container");
          math.forEach((element) => {
            if (element.parentNode) {
              element.parentNode.classList.add("has-jax");
            }
          });
        });
      },
    },
  };
</script>

<script id="MathJax-script" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.2/es5/tex-mml-chtml.min.js" async></script>

<script>
  document.addEventListener('DOMContentLoaded', function () {
    window.codeElements = document.querySelectorAll('.code');
    if (window.codeElements.length > 0) {
      ctx.copycode = {
        default_text: `Copy`,
        success_text: `Copied`,
        toast: `å¤åˆ¶æˆåŠŸ`,
      };
      utils.js('/js/plugins/copycode.js');
    }
  });
</script>


<!-- inject -->

</div></body></html>
