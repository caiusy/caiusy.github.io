[{"title":"å›¾è®ºä¸æœç´¢ï¼šä»é›¶åˆ°ç²¾é€šçš„è´¹æ›¼å¼å®Œå…¨æŒ‡å—","path":"/2026/02/23/graph-theory-and-search/","content":"è´¹æ›¼è¯´ï¼š â€œå¦‚æœä½ ä¸èƒ½æŠŠä¸€ä¸ªæ¦‚å¿µè§£é‡Šç»™ä¸€ä¸ª 10 å²å°å­©å¬ï¼Œè¯´æ˜ä½ è‡ªå·±ä¹Ÿæ²¡çœŸæ­£ç†è§£å®ƒã€‚â€ è¿™ç¯‡æ–‡ç« ç”¨è´¹æ›¼å¼æ•™å­¦æ³•ï¼Œå¸¦ä½ ä»ã€Œå›¾æ˜¯ä»€ä¹ˆã€ä¸€è·¯æ‰“é€šåˆ°ã€ŒLeetCode å®æˆ˜ã€ã€‚æ¯ä¸ªæ¦‚å¿µéƒ½æœ‰å…·ä½“ä¾‹å­ã€å¯è§†åŒ–å›¾è§£ã€å®Œæ•´ä»£ç å’Œæ•°æ®æµè¿½è¸ªã€‚ä¸è®²åºŸè¯ï¼Œç›´å‡»æœ¬è´¨ã€‚ ç¬¬ä¸€ç« ï¼šåŸºç¡€ç¯‡ â€” å›¾çš„ä¸–ç•Œè§‚ 1.1 å›¾åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿè´¹æ›¼ä¸€å¥è¯ï¼š å›¾å°±æ˜¯ã€Œä¸€å †ä¸œè¥¿ã€åŠ ä¸Šã€Œå®ƒä»¬ä¹‹é—´çš„å…³ç³»ã€ã€‚å°±åƒä½ çš„æœ‹å‹åœˆâ€”â€”äººæ˜¯èŠ‚ç‚¹ï¼Œäº’ç›¸è®¤è¯†å°±è¿æ¡çº¿ã€‚ å½¢å¼åŒ–å®šä¹‰ï¼šå›¾ $G = (V, E)$ï¼Œå…¶ä¸­ $V$ æ˜¯èŠ‚ç‚¹é›†åˆï¼Œ$E$ æ˜¯è¾¹é›†åˆã€‚ ä½†å®šä¹‰ä¸é‡è¦ï¼Œé‡è¦çš„æ˜¯ï¼šä½ æ€ä¹ˆåœ¨ç”µè„‘é‡Œå­˜ä¸€å¼ å›¾ï¼Ÿ è¿™å°±å¼•å‡ºäº†ä¸‰ç§è¡¨ç¤ºæ³•ã€‚ 1.2 å›¾çš„ä¸‰ç§è¡¨ç¤ºæ³•æˆ‘ä»¬ç”¨è¿™å¼ å›¾ä½œä¸ºè´¯ç©¿å…¨æ–‡çš„ä¾‹å­ï¼š 5 ä¸ªèŠ‚ç‚¹ï¼ˆ0-4ï¼‰ï¼Œ6 æ¡è¾¹ã€‚ä¸‹é¢åˆ†åˆ«çœ‹ä¸‰ç§å­˜æ³•ã€‚ 1.2.1 é‚»æ¥çŸ©é˜µ (Adjacency Matrix)è´¹æ›¼ä¸€å¥è¯ï¼š ç”»ä¸€å¼ è¡¨æ ¼ï¼Œè¡Œå’Œåˆ—éƒ½æ˜¯èŠ‚ç‚¹ã€‚ä¸¤ä¸ªèŠ‚ç‚¹ä¹‹é—´æœ‰è¾¹å°±å¡« 1ï¼Œæ²¡æœ‰å°±å¡« 0ã€‚å°±åƒä¸€å¼ ã€Œè°è®¤è¯†è°ã€çš„å…³ç³»è¡¨ã€‚ # é‚»æ¥çŸ©é˜µè¡¨ç¤ºV = 5adj_matrix = [[0]*V for _ in range(V)]edges = [(0,1),(0,2),(1,2),(1,3),(2,4),(3,4)]for u, v in edges: adj_matrix[u][v] = 1 adj_matrix[v][u] = 1 # æ— å‘å›¾ï¼Œå¯¹ç§°# æŸ¥è¯¢: èŠ‚ç‚¹1å’ŒèŠ‚ç‚¹3ä¹‹é—´æœ‰è¾¹å—ï¼Ÿprint(adj_matrix[1][3]) # è¾“å‡º: 1, O(1)æ—¶é—´ æ•°æ®æµè¿½è¸ªï¼š åˆå§‹çŸ©é˜µ: 5x5 å…¨0åŠ è¾¹(0,1): matrix[0][1]=1, matrix[1][0]=1åŠ è¾¹(0,2): matrix[0][2]=1, matrix[2][0]=1...æœ€ç»ˆçŸ©é˜µ: 0 1 2 3 40 [ 0, 1, 1, 0, 0 ]1 [ 1, 0, 1, 1, 0 ]2 [ 1, 1, 0, 0, 1 ]3 [ 0, 1, 0, 0, 1 ]4 [ 0, 0, 1, 1, 0 ] ä»€ä¹ˆæ—¶å€™ç”¨ï¼Ÿ âœ… éœ€è¦å¿«é€ŸæŸ¥è¯¢ã€Œä¸¤ç‚¹ä¹‹é—´æœ‰æ²¡æœ‰è¾¹ã€â†’ O(1) âœ… ç¨ å¯†å›¾ï¼ˆè¾¹å¾ˆå¤šï¼Œæ¥è¿‘ $V^2$ï¼‰ âŒ ç¨€ç–å›¾æµªè´¹ç©ºé—´ â†’ O(VÂ²) ç©ºé—´ï¼Œå¤§éƒ¨åˆ†æ˜¯ 0 1.2.2 é‚»æ¥è¡¨ (Adjacency List)è´¹æ›¼ä¸€å¥è¯ï¼š æ¯ä¸ªäººç»´æŠ¤ä¸€ä¸ªã€Œå¥½å‹åˆ—è¡¨ã€ã€‚æƒ³çŸ¥é“æŸäººè®¤è¯†è°ï¼Œç›´æ¥ç¿»ä»–çš„åˆ—è¡¨å°±è¡Œã€‚ # é‚»æ¥è¡¨è¡¨ç¤º (æœ€å¸¸ç”¨!)from collections import defaultdictgraph = defaultdict(list)edges = [(0,1),(0,2),(1,2),(1,3),(2,4),(3,4)]for u, v in edges: graph[u].append(v) graph[v].append(u)# æŸ¥è¯¢: èŠ‚ç‚¹1çš„æ‰€æœ‰é‚»å±…ï¼Ÿprint(graph[1]) # è¾“å‡º: [0, 2, 3], O(degree)æ—¶é—´ æ•°æ®æµè¿½è¸ªï¼š åŠ è¾¹(0,1): graph = {0:[1], 1:[0]}åŠ è¾¹(0,2): graph = {0:[1,2], 1:[0], 2:[0]}åŠ è¾¹(1,2): graph = {0:[1,2], 1:[0,2], 2:[0,1]}åŠ è¾¹(1,3): graph = {0:[1,2], 1:[0,2,3], 2:[0,1], 3:[1]}åŠ è¾¹(2,4): graph = {0:[1,2], 1:[0,2,3], 2:[0,1,4], 3:[1], 4:[2]}åŠ è¾¹(3,4): graph = {0:[1,2], 1:[0,2,3], 2:[0,1,4], 3:[1,4], 4:[2,3]} ä»€ä¹ˆæ—¶å€™ç”¨ï¼Ÿ âœ… éå†é‚»å±…å¿« â†’ O(degree) âœ… ç©ºé—´é«˜æ•ˆ â†’ O(V+E) âœ… 90% çš„å›¾é¢˜éƒ½ç”¨è¿™ä¸ª âŒ æŸ¥è¯¢ç‰¹å®šè¾¹æ˜¯å¦å­˜åœ¨ â†’ O(degree)ï¼Œä¸å¦‚çŸ©é˜µå¿« 1.2.3 è¾¹åˆ—è¡¨ (Edge List)è´¹æ›¼ä¸€å¥è¯ï¼š æœ€ç¬¨ä½†æœ€ç›´æ¥â€”â€”æŠŠæ‰€æœ‰å…³ç³»ä¸€æ¡æ¡åˆ—å‡ºæ¥ã€‚å°±åƒä¸€ä»½ã€Œè°å’Œè°æ˜¯æœ‹å‹ã€çš„åå•ã€‚ # è¾¹åˆ—è¡¨è¡¨ç¤ºedges = [(0,1),(0,2),(1,2),(1,3),(2,4),(3,4)]# å¸¦æƒé‡çš„è¾¹åˆ—è¡¨ (Kruskalæœ€å°ç”Ÿæˆæ ‘å¸¸ç”¨)weighted_edges = [(0,1,4),(0,2,2),(1,2,1),(1,3,5),(2,4,3),(3,4,6)]weighted_edges.sort(key=lambda x: x[2]) # æŒ‰æƒé‡æ’åº ä»€ä¹ˆæ—¶å€™ç”¨ï¼Ÿ âœ… Kruskal æœ€å°ç”Ÿæˆæ ‘ï¼ˆéœ€è¦æŒ‰æƒé‡æ’åºæ‰€æœ‰è¾¹ï¼‰ âœ… å­˜å‚¨ç®€å•ï¼Œç©ºé—´ O(E) âŒ æŸ¥è¾¹ã€æŸ¥é‚»å±…éƒ½æ…¢ â†’ O(E) ä¸‰ç§è¡¨ç¤ºæ³•å¯¹æ¯”æ€»ç»“ ç‰¹æ€§ é‚»æ¥çŸ©é˜µ é‚»æ¥è¡¨ è¾¹åˆ—è¡¨ ç©ºé—´ O(VÂ²) O(V+E) O(E) æŸ¥è¾¹ O(1) O(degree) O(E) éå†é‚»å±… O(V) O(degree) O(E) åŠ è¾¹ O(1) O(1) O(1) é€‚ç”¨åœºæ™¯ ç¨ å¯†å›¾ é€šç”¨(é¦–é€‰) Kruskal ğŸ¯ ç»“è®ºï¼šåˆ·é¢˜é»˜è®¤ç”¨é‚»æ¥è¡¨ã€‚ é™¤éé¢˜ç›®æ˜ç¡®ç»™äº†çŸ©é˜µï¼ˆå¦‚å²›å±¿é—®é¢˜çš„ç½‘æ ¼ï¼‰ï¼Œæˆ–è€…éœ€è¦ Kruskalã€‚ 1.3 DFS æ·±åº¦ä¼˜å…ˆæœç´¢è´¹æ›¼ä¸€å¥è¯ï¼š DFS å°±æ˜¯èµ°è¿·å®«çš„ç­–ç•¥â€”â€”ä¸€æ¡è·¯èµ°åˆ°é»‘ï¼Œæ’å¢™äº†å†å›å¤´æ¢ä¸€æ¡ã€‚ç”¨ã€Œæ ˆã€è®°ä½å²”è·¯å£ï¼Œæ–¹ä¾¿å›å¤´ã€‚ æ ¸å¿ƒæœºåˆ¶DFS çš„æœ¬è´¨æ˜¯ï¼šä¼˜å…ˆæ¢ç´¢æ·±åº¦æ–¹å‘ã€‚å®ƒç”¨æ ˆï¼ˆé€’å½’è°ƒç”¨æ ˆæˆ–æ˜¾å¼æ ˆï¼‰æ¥è®°å½•ã€Œè¿˜æ²¡æ¢ç´¢å®Œçš„å²”è·¯ã€ã€‚ å…³é”®æ•°æ®ç»“æ„ï¼š stackï¼šå¾…æ¢ç´¢çš„èŠ‚ç‚¹ï¼ˆåè¿›å…ˆå‡ºï¼‰ visitedï¼šå·²ç»å»è¿‡çš„èŠ‚ç‚¹ï¼ˆé¿å…é‡å¤èµ°ï¼‰ é€’å½’å†™æ³•ï¼ˆåˆ©ç”¨ç³»ç»Ÿè°ƒç”¨æ ˆï¼‰def dfs_recursive(graph, node, visited=None): if visited is None: visited = set() visited.add(node) print(f\"è®¿é—®: {node}\") for neighbor in graph[node]: if neighbor not in visited: dfs_recursive(graph, neighbor, visited) return visited æ•°æ®æµè¿½è¸ªï¼ˆé€’å½’ç‰ˆï¼‰ï¼š è°ƒç”¨ dfs(graph, 0) visited={0}, æ‰“å°\"è®¿é—®: 0\" é‚»å±…: [1, 2] â”œâ”€ 1 æœªè®¿é—® â†’ è°ƒç”¨ dfs(graph, 1) â”‚ visited={0,1}, æ‰“å°\"è®¿é—®: 1\" â”‚ é‚»å±…: [0, 2, 3] â”‚ â”œâ”€ 0 å·²è®¿é—® â†’ è·³è¿‡ â”‚ â”œâ”€ 2 æœªè®¿é—® â†’ è°ƒç”¨ dfs(graph, 2) â”‚ â”‚ visited={0,1,2}, æ‰“å°\"è®¿é—®: 2\" â”‚ â”‚ é‚»å±…: [0, 1, 4] â”‚ â”‚ â”œâ”€ 0 å·²è®¿é—® â†’ è·³è¿‡ â”‚ â”‚ â”œâ”€ 1 å·²è®¿é—® â†’ è·³è¿‡ â”‚ â”‚ â””â”€ 4 æœªè®¿é—® â†’ è°ƒç”¨ dfs(graph, 4) â”‚ â”‚ visited={0,1,2,4}, æ‰“å°\"è®¿é—®: 4\" â”‚ â”‚ é‚»å±…: [2, 3] â”‚ â”‚ â”œâ”€ 2 å·²è®¿é—® â†’ è·³è¿‡ â”‚ â”‚ â””â”€ 3 æœªè®¿é—® â†’ è°ƒç”¨ dfs(graph, 3) â”‚ â”‚ visited={0,1,2,3,4}, æ‰“å°\"è®¿é—®: 3\" â”‚ â”‚ é‚»å±…: [1, 4] â†’ å…¨éƒ¨å·²è®¿é—® â”‚ â”‚ è¿”å› â†© â”‚ â”‚ è¿”å› â†© â”‚ â”‚ è¿”å› â†© â”‚ â””â”€ 3 å·²è®¿é—® â†’ è·³è¿‡ â”‚ è¿”å› â†© â””â”€ 2 å·²è®¿é—® â†’ è·³è¿‡ è¿”å› â†©æœ€ç»ˆéå†é¡ºåº: 0 â†’ 1 â†’ 2 â†’ 4 â†’ 3 è¿­ä»£å†™æ³•ï¼ˆæ˜¾å¼æ ˆï¼‰def dfs_iterative(graph, start): visited = set() stack = [start] order = [] while stack: node = stack.pop() if node in visited: continue visited.add(node) order.append(node) # é€†åºå‹æ ˆï¼Œä¿è¯å°ç¼–å·å…ˆè¢«å¼¹å‡º for neighbor in reversed(sorted(graph[node])): if neighbor not in visited: stack.append(neighbor) return order æ ˆçš„å˜åŒ–è¿‡ç¨‹ï¼ˆé€æ­¥å›¾è§£ï¼‰ è¿­ä»£ç‰ˆæ•°æ®æµè¿½è¸ªï¼š åˆå§‹: stack=[0], visited={}Step1: pop 0 â†’ visited={0}, å‹å…¥é‚»å±…[2,1] stack=[2,1]Step2: pop 1 â†’ visited={0,1}, å‹å…¥é‚»å±…[3,2] stack=[2,3,2]Step3: pop 2 â†’ visited={0,1,2}, å‹å…¥é‚»å±…[4] stack=[2,3,4]Step4: pop 4 â†’ visited={0,1,2,4}, å‹å…¥é‚»å±…[3] stack=[2,3,3]Step5: pop 3 â†’ visited={0,1,2,3,4} stack=[2,3]Step6: pop 3 â†’ å·²è®¿é—®,è·³è¿‡. pop 2 â†’ å·²è®¿é—®,è·³è¿‡ stack=[] â†’ ç»“æŸéå†é¡ºåº: 0 â†’ 1 â†’ 2 â†’ 4 â†’ 3 âš ï¸ é€’å½’ vs è¿­ä»£çš„åŒºåˆ«ï¼š é€’å½’ç‰ˆéå†é¡ºåºå–å†³äºé‚»å±…çš„éå†é¡ºåºï¼›è¿­ä»£ç‰ˆå› ä¸ºæ ˆçš„ LIFO ç‰¹æ€§ï¼Œéœ€è¦é€†åºå‹æ ˆæ‰èƒ½ä¿æŒç›¸åŒé¡ºåºã€‚ä¸¤è€…æœ¬è´¨ç›¸åŒï¼Œåªæ˜¯æ ˆçš„ç®¡ç†æ–¹å¼ä¸åŒã€‚ DFS çš„æ—¶é—´/ç©ºé—´å¤æ‚åº¦ æ—¶é—´ï¼šO(V + E) â€” æ¯ä¸ªèŠ‚ç‚¹è®¿é—®ä¸€æ¬¡ï¼Œæ¯æ¡è¾¹æ£€æŸ¥ä¸€æ¬¡ ç©ºé—´ï¼šO(V) â€” visited é›†åˆ + æ ˆæ·±åº¦ï¼ˆæœ€åæƒ…å†µæ˜¯é“¾çŠ¶å›¾ï¼Œæ·±åº¦ä¸º Vï¼‰ 1.4 BFS å¹¿åº¦ä¼˜å…ˆæœç´¢è´¹æ›¼ä¸€å¥è¯ï¼š BFS å°±åƒå¾€æ± å¡˜é‡Œæ‰”çŸ³å¤´â€”â€”æ°´æ³¢ä¸€åœˆä¸€åœˆå¾€å¤–æ‰©ã€‚å…ˆæŠŠç¦»ä½ æœ€è¿‘çš„äººå…¨è®¤è¯†äº†ï¼Œå†å»è®¤è¯†æœ‹å‹çš„æœ‹å‹ã€‚ç”¨ã€Œé˜Ÿåˆ—ã€ä¿è¯å…ˆæ¥å…ˆæœåŠ¡ã€‚ æ ¸å¿ƒæœºåˆ¶BFS çš„æœ¬è´¨æ˜¯ï¼šæŒ‰è·ç¦»ï¼ˆå±‚æ•°ï¼‰ä»è¿‘åˆ°è¿œæ¢ç´¢ã€‚å®ƒç”¨é˜Ÿåˆ—ï¼ˆå…ˆè¿›å…ˆå‡ºï¼‰æ¥ä¿è¯ã€Œå…ˆå‘ç°çš„èŠ‚ç‚¹å…ˆå¤„ç†ã€ã€‚ å…³é”®æ•°æ®ç»“æ„ï¼š queueï¼šå¾…æ¢ç´¢çš„èŠ‚ç‚¹ï¼ˆå…ˆè¿›å…ˆå‡ºï¼‰ visitedï¼šå·²ç»å…¥è¿‡é˜Ÿçš„èŠ‚ç‚¹ï¼ˆæ³¨æ„ï¼šæ˜¯å…¥é˜Ÿæ—¶æ ‡è®°ï¼Œä¸æ˜¯å‡ºé˜Ÿæ—¶ï¼ï¼‰ æ ‡å‡†å†™æ³•from collections import dequedef bfs(graph, start): visited = {start} queue = deque([start]) order = [] while queue: node = queue.popleft() order.append(node) for neighbor in sorted(graph[node]): if neighbor not in visited: visited.add(neighbor) # å…¥é˜Ÿæ—¶å°±æ ‡è®°! queue.append(neighbor) return order âš ï¸ å…³é”®ç»†èŠ‚ï¼švisited åœ¨å…¥é˜Ÿæ—¶æ ‡è®°ï¼Œä¸æ˜¯å‡ºé˜Ÿæ—¶ï¼ å¦‚æœå‡ºé˜Ÿæ—¶æ‰æ ‡è®°ï¼ŒåŒä¸€ä¸ªèŠ‚ç‚¹å¯èƒ½è¢«å¤šæ¬¡å…¥é˜Ÿï¼Œæµªè´¹æ—¶é—´å’Œç©ºé—´ã€‚è¿™æ˜¯æ–°æ‰‹æœ€å¸¸çŠ¯çš„é”™è¯¯ã€‚ é˜Ÿåˆ—çš„å˜åŒ–è¿‡ç¨‹ï¼ˆé€æ­¥å›¾è§£ï¼‰ æ•°æ®æµè¿½è¸ªï¼š åˆå§‹: queue=[0], visited={0}Step1: popleft 0 â†’ å¤„ç†é‚»å±… [1,2] 1 æœªè®¿é—® â†’ visited={0,1}, å…¥é˜Ÿ 2 æœªè®¿é—® â†’ visited={0,1,2}, å…¥é˜Ÿ queue=[1,2] â† Layer 0 å¤„ç†å®ŒStep2: popleft 1 â†’ å¤„ç†é‚»å±… [0,2,3] 0 å·²è®¿é—® â†’ è·³è¿‡ 2 å·²è®¿é—® â†’ è·³è¿‡ 3 æœªè®¿é—® â†’ visited={0,1,2,3}, å…¥é˜Ÿ queue=[2,3]Step3: popleft 2 â†’ å¤„ç†é‚»å±… [0,1,4] 0,1 å·²è®¿é—® â†’ è·³è¿‡ 4 æœªè®¿é—® â†’ visited={0,1,2,3,4}, å…¥é˜Ÿ queue=[3,4] â† Layer 1 å¤„ç†å®ŒStep4: popleft 3 â†’ é‚»å±… [1,4] å…¨å·²è®¿é—® queue=[4]Step5: popleft 4 â†’ é‚»å±… [2,3] å…¨å·²è®¿é—® queue=[] â†’ ç»“æŸ â† Layer 2 å¤„ç†å®Œéå†é¡ºåº: 0 â†’ 1 â†’ 2 â†’ 3 â†’ 4å±‚æ¬¡: L0 L1 L1 L2 L2 å±‚åºéå†çš„æœ¬è´¨BFS å¤©ç„¶æŒ‰å±‚éå†ã€‚å¦‚æœä½ éœ€è¦çŸ¥é“ã€Œå½“å‰æ˜¯ç¬¬å‡ å±‚ã€ï¼Œåªéœ€è¦åœ¨æ¯å±‚å¼€å§‹æ—¶è®°å½•é˜Ÿåˆ—é•¿åº¦ï¼š from collections import dequedef bfs_by_layer(graph, start): visited = {start} queue = deque([start]) depth = 0 while queue: layer_size = len(queue) # å½“å‰å±‚æœ‰å¤šå°‘èŠ‚ç‚¹ print(f\"Layer {depth}: \", end=\"\") for _ in range(layer_size): node = queue.popleft() print(node, end=\" \") for nb in sorted(graph[node]): if nb not in visited: visited.add(nb) queue.append(nb) print() depth += 1 è¾“å‡º:Layer 0: 0Layer 1: 1 2Layer 2: 3 4 ğŸ¯ ä¸ºä»€ä¹ˆ BFS èƒ½æ±‚æœ€çŸ­è·¯ï¼Ÿ å› ä¸º BFS æŒ‰å±‚æ‰©å±•ï¼Œç¬¬ä¸€æ¬¡åˆ°è¾¾æŸä¸ªèŠ‚ç‚¹æ—¶ï¼Œèµ°çš„ä¸€å®šæ˜¯æœ€å°‘çš„è¾¹æ•°ã€‚è¿™å°±æ˜¯æ— æƒå›¾æœ€çŸ­è·¯çš„åŸç†ã€‚ BFS çš„æ—¶é—´/ç©ºé—´å¤æ‚åº¦ æ—¶é—´ï¼šO(V + E) â€” å’Œ DFS ä¸€æ · ç©ºé—´ï¼šO(V) â€” æœ€åæƒ…å†µé˜Ÿåˆ—é‡Œå­˜äº†ä¸€æ•´å±‚çš„èŠ‚ç‚¹ï¼ˆå®Œå…¨å›¾æ—¶æ¥è¿‘ Vï¼‰ 1.5 DFS vs BFS å…¨é¢å¯¹æ¯” ç»´åº¦ DFS BFS æ•°æ®ç»“æ„ æ ˆ (LIFO) é˜Ÿåˆ— (FIFO) æ¢ç´¢ç­–ç•¥ ä¸€æ¡è·¯èµ°åˆ°åº• ä¸€å±‚ä¸€å±‚æ‰©å±• ç©ºé—´å¤æ‚åº¦ O(h), h=æœ€å¤§æ·±åº¦ O(w), w=æœ€å¤§å®½åº¦ èƒ½æ±‚æœ€çŸ­è·¯ï¼Ÿ âŒ ä¸èƒ½ âœ… æ— æƒå›¾æœ€çŸ­è·¯ é€‚åˆåœºæ™¯ è¿é€šæ€§ã€å›æº¯ã€æ‹“æ‰‘æ’åº æœ€çŸ­è·¯ã€å±‚åºéå† ç±»æ¯” èµ°è¿·å®«ï¼ˆä¸€æ¡è·¯èµ°åˆ°é»‘ï¼‰ æ°´æ³¢æ‰©æ•£ï¼ˆä¸€åœˆåœˆå¾€å¤–ï¼‰ ğŸ¯ é€‰æ‹©å£è¯€ï¼š è¦æ‰¾æœ€çŸ­è·¯ â†’ BFSã€‚è¦éå†æ‰€æœ‰è·¯å¾„/åˆ¤æ–­è¿é€šæ€§ â†’ DFSã€‚ä¸ç¡®å®š â†’ ä¸¤ä¸ªéƒ½èƒ½ç”¨ï¼Œé€‰ä½ é¡ºæ‰‹çš„ã€‚ 1.6 æ–°æ‰‹å¸¸è¸©çš„ 5 ä¸ªå‘å‘1: BFS çš„ visited æ ‡è®°æ—¶æœº # âŒ é”™è¯¯: å‡ºé˜Ÿæ—¶æ ‡è®°while queue: node = queue.popleft() if node in visited: continue # å¤ªæ™šäº†! å¯èƒ½å·²ç»å…¥é˜Ÿå¤šæ¬¡ visited.add(node)# âœ… æ­£ç¡®: å…¥é˜Ÿæ—¶æ ‡è®°if neighbor not in visited: visited.add(neighbor) # å…¥é˜Ÿå‰å°±æ ‡è®° queue.append(neighbor) å‡ºé˜Ÿæ—¶æ ‡è®°ä¼šå¯¼è‡´åŒä¸€èŠ‚ç‚¹è¢«å¤šæ¬¡å…¥é˜Ÿã€‚æƒ³è±¡ A å’Œ B éƒ½è¿ç€ Cï¼Œå¤„ç† A æ—¶æŠŠ C å…¥é˜Ÿï¼Œå¤„ç† B æ—¶åˆæŠŠ C å…¥é˜Ÿâ€”â€”C è¢«å¤„ç†ä¸¤æ¬¡ã€‚å…¥é˜Ÿæ—¶æ ‡è®°å°±èƒ½é¿å…ã€‚ å‘2: DFS é€’å½’çˆ†æ ˆ Python é»˜è®¤é€’å½’æ·±åº¦ 1000ã€‚å¦‚æœå›¾æ˜¯ä¸€æ¡é“¾ï¼ˆæ¯”å¦‚ 10000 ä¸ªèŠ‚ç‚¹ä¸²æˆä¸€æ¡çº¿ï¼‰ï¼Œé€’å½’ DFS ç›´æ¥ RecursionErrorã€‚ è§£å†³æ–¹æ¡ˆï¼š import syssys.setrecursionlimit(200000) # æ–¹æ¡ˆ1: åŠ å¤§é™åˆ¶(ä¸æ¨è)# æ–¹æ¡ˆ2: æ”¹ç”¨è¿­ä»£DFS (æ¨è)def dfs_iterative(graph, start): stack, visited = [start], set() while stack: node = stack.pop() if node in visited: continue visited.add(node) for nb in graph[node]: if nb not in visited: stack.append(nb) å‘3: æœ‰å‘å›¾ vs æ— å‘å›¾å»ºå›¾ # æ— å‘å›¾: åŒå‘åŠ è¾¹graph[u].append(v)graph[v].append(u)# æœ‰å‘å›¾: å•å‘åŠ è¾¹graph[u].append(v) # åªæœ‰ uâ†’v é¢˜ç›®è¯´ã€Œæ— å‘ã€å°±å¿…é¡»åŒå‘åŠ ï¼Œæ¼äº†ä¸€ä¸ªæ–¹å‘ä¼šå¯¼è‡´éå†ä¸å®Œæ•´ã€‚ å‘4: ç½‘æ ¼å›¾çš„è¾¹ç•Œæ£€æŸ¥ # æ¨¡æ¿: å››æ–¹å‘éå†for dx, dy in [(0,1),(0,-1),(1,0),(-1,0)]: nx, ny = x+dx, y+dy if 0 = nx m and 0 = ny n: # è¾¹ç•Œæ£€æŸ¥ # å¤„ç† (nx, ny) å¿˜è®°è¾¹ç•Œæ£€æŸ¥ â†’ IndexErrorã€‚å»ºè®®æŠŠæ–¹å‘æ•°ç»„å†™æˆå¸¸é‡ï¼Œå‡å°‘æ‰‹è¯¯ã€‚ å‘5: Dijkstra å¿˜è®°è·³è¿‡è¿‡æ—¶æ¡ç›® d, u = heapq.heappop(heap)if d dist.get(u, float('inf')): continue # è¿™ä¸ªæ¡ç›®å·²ç»è¿‡æ—¶äº†ï¼Œè·³è¿‡! å †é‡Œå¯èƒ½æœ‰åŒä¸€èŠ‚ç‚¹çš„å¤šä¸ªæ¡ç›®ï¼ˆå› ä¸ºæ¾å¼›æ—¶ push æ–°çš„è€Œä¸æ˜¯æ›´æ–°æ—§çš„ï¼‰ã€‚ä¸è·³è¿‡ä¼šå¯¼è‡´é‡å¤å¤„ç†ï¼Œè™½ç„¶ç»“æœæ­£ç¡®ä½†æ—¶é—´é€€åŒ–ã€‚ 1.7 DFS/BFS æ¨¡æ¿é€ŸæŸ¥è¡¨DFS ä¸‡èƒ½æ¨¡æ¿ï¼ˆè¿­ä»£ç‰ˆï¼‰ï¼š def dfs(graph, start): stack, visited = [start], {start} while stack: node = stack.pop() # process(node) for nb in graph[node]: if nb not in visited: visited.add(nb) stack.append(nb) BFS ä¸‡èƒ½æ¨¡æ¿ï¼ˆå¸¦å±‚æ•°ï¼‰ï¼š def bfs(graph, start): queue, visited = deque([start]), {start} depth = 0 while queue: for _ in range(len(queue)): node = queue.popleft() # process(node, depth) for nb in graph[node]: if nb not in visited: visited.add(nb) queue.append(nb) depth += 1 ç½‘æ ¼ DFS æ¨¡æ¿ï¼š def grid_dfs(grid, i, j, m, n): if i 0 or i = m or j 0 or j = n or grid[i][j] != target: return grid[i][j] = visited_mark # æ ‡è®° for di, dj in [(0,1),(0,-1),(1,0),(-1,0)]: grid_dfs(grid, i+di, j+dj, m, n) ğŸ“ åŸºç¡€ç¯‡å°ç»“åˆ°è¿™é‡Œä½ å·²ç»æŒæ¡äº†ï¼š å›¾çš„ä¸‰ç§å­˜æ³• â€” é‚»æ¥è¡¨æ˜¯é»˜è®¤é€‰æ‹© DFS â€” æ ˆé©±åŠ¨ï¼Œä¸€æ¡è·¯èµ°åˆ°é»‘ï¼Œé€’å½’å’Œè¿­ä»£ä¸¤ç§å†™æ³• BFS â€” é˜Ÿåˆ—é©±åŠ¨ï¼Œä¸€å±‚å±‚æ‰©å±•ï¼Œå¤©ç„¶æ±‚æœ€çŸ­è·¯ è¿™ä¸‰ä¸ªæ˜¯å›¾è®ºçš„åœ°åŸºã€‚æ¥ä¸‹æ¥çš„æŠ€å·§ç¯‡ï¼ˆæ‹“æ‰‘æ’åºã€è¿é€šåˆ†é‡ã€äºŒåˆ†å›¾ã€æœ€çŸ­è·¯ï¼‰å…¨éƒ¨å»ºç«‹åœ¨ DFS/BFS ä¹‹ä¸Šã€‚ ç¬¬äºŒç« ï¼šæŠ€å·§ç¯‡ â€” å›¾çš„ç»å…¸åº”ç”¨ 2.1 æ‹“æ‰‘æ’åºè´¹æ›¼ä¸€å¥è¯ï¼š ä½ æœ‰ä¸€å †è¯¾è¦ä¸Šï¼Œæœ‰äº›è¯¾æœ‰å…ˆä¿®è¦æ±‚ã€‚æ‹“æ‰‘æ’åºå°±æ˜¯å¸®ä½ æ’å‡ºä¸€ä¸ªã€Œä¸è¿åä»»ä½•å…ˆä¿®è¦æ±‚ã€çš„é€‰è¯¾é¡ºåºã€‚å¦‚æœæ’ä¸å‡ºæ¥ï¼Œè¯´æ˜å…ˆä¿®å…³ç³»é‡Œæœ‰æ­»å¾ªç¯ï¼ˆç¯ï¼‰ã€‚ å‰ææ¡ä»¶æ‹“æ‰‘æ’åºåªå¯¹ æœ‰å‘æ— ç¯å›¾ (DAG) æœ‰æ„ä¹‰ã€‚å¦‚æœå›¾é‡Œæœ‰ç¯ï¼Œå°±ä¸å¯èƒ½æ’å‡ºåˆæ³•é¡ºåºã€‚ æˆ‘ä»¬ç”¨è¿™ä¸ªè¯¾ç¨‹ä¾èµ–å›¾ï¼š 0 â†’ 1 â†’ 3 â†’ 40 â†’ 2 â†’ 3 1 â†’ 4 å«ä¹‰ï¼šä¸Šè¯¾ç¨‹ 3 ä¹‹å‰ï¼Œå¿…é¡»å…ˆä¸Š 1 å’Œ 2ï¼›ä¸Š 1 ä¹‹å‰å¿…é¡»å…ˆä¸Š 0ã€‚ æ–¹æ³•ä¸€ï¼šKahnâ€™s BFSï¼ˆå…¥åº¦æ³•ï¼‰æ ¸å¿ƒæ€æƒ³ï¼š ä¸æ–­æ‰¾ã€Œæ²¡æœ‰å…ˆä¿®è¦æ±‚çš„è¯¾ã€ï¼ˆå…¥åº¦ä¸º 0ï¼‰ï¼Œä¸Šå®Œå®ƒï¼Œç„¶åæŠŠå®ƒä»ä¾èµ–å…³ç³»ä¸­åˆ æ‰ã€‚é‡å¤ç›´åˆ°æ‰€æœ‰è¯¾ä¸Šå®Œã€‚ from collections import deque, defaultdictdef topo_sort_kahn(n, edges): graph = defaultdict(list) indegree = [0] * n for u, v in edges: graph[u].append(v) indegree[v] += 1 queue = deque(i for i in range(n) if indegree[i] == 0) result = [] while queue: node = queue.popleft() result.append(node) for nb in graph[node]: indegree[nb] -= 1 if indegree[nb] == 0: queue.append(nb) return result if len(result) == n else [] # ç©º=æœ‰ç¯ æ•°æ®æµè¿½è¸ªï¼š åˆå§‹å…¥åº¦: [0:0, 1:1, 2:1, 3:2, 4:2]queue=[0], result=[]Step1: pop 0 â†’ result=[0] åˆ è¾¹ 0â†’1: indegree[1]=0 â†’ å…¥é˜Ÿ åˆ è¾¹ 0â†’2: indegree[2]=0 â†’ å…¥é˜Ÿ queue=[1,2]Step2: pop 1 â†’ result=[0,1] åˆ è¾¹ 1â†’3: indegree[3]=1 åˆ è¾¹ 1â†’4: indegree[4]=1 queue=[2]Step3: pop 2 â†’ result=[0,1,2] åˆ è¾¹ 2â†’3: indegree[3]=0 â†’ å…¥é˜Ÿ queue=[3]Step4: pop 3 â†’ result=[0,1,2,3] åˆ è¾¹ 3â†’4: indegree[4]=0 â†’ å…¥é˜Ÿ queue=[4]Step5: pop 4 â†’ result=[0,1,2,3,4] queue=[] â†’ ç»“æŸlen(result)=5=n â†’ æ— ç¯, åˆæ³•æ‹“æ‰‘åº: [0,1,2,3,4] æ–¹æ³•äºŒï¼šDFS ååºåè½¬æ ¸å¿ƒæ€æƒ³ï¼š DFS é€’å½’åˆ°åº•å†å›æ¥æ—¶ï¼Œã€Œæœ€åå®Œæˆçš„èŠ‚ç‚¹ã€ä¸€å®šæ˜¯ä¾èµ–é“¾çš„èµ·ç‚¹ã€‚æŠŠ DFS çš„å®Œæˆé¡ºåºåè½¬ï¼Œå°±æ˜¯æ‹“æ‰‘åºã€‚ def topo_sort_dfs(n, edges): graph = [[] for _ in range(n)] for u, v in edges: graph[u].append(v) WHITE, GRAY, BLACK = 0, 1, 2 color = [WHITE] * n order = [] has_cycle = False def dfs(u): nonlocal has_cycle color[u] = GRAY # æ­£åœ¨å¤„ç† for v in graph[u]: if color[v] == GRAY: # é‡åˆ°ç°è‰²=ç¯! has_cycle = True; return if color[v] == WHITE: dfs(v) color[u] = BLACK # å¤„ç†å®Œæ¯• order.append(u) # ååº: å®Œæˆæ—¶è®°å½• for i in range(n): if color[i] == WHITE: dfs(i) return order[::-1] if not has_cycle else [] æ•°æ®æµè¿½è¸ªï¼ˆä¸‰è‰²æ ‡è®°æ³•ï¼‰ï¼š WHITE=æœªè®¿é—® GRAY=æ­£åœ¨é€’å½’ä¸­ BLACK=å·²å®Œæˆdfs(0): color[0]=GRAY â†’ dfs(1): color[1]=GRAY â†’ dfs(3): color[3]=GRAY â†’ dfs(4): color[4]=GRAY é‚»å±…éƒ½æ²¡æœ‰ â†’ color[4]=BLACK, order=[4] color[3]=BLACK, order=[4,3] â†’ dfs(4): å·²BLACK,è·³è¿‡ color[1]=BLACK, order=[4,3,1] â†’ dfs(2): color[2]=GRAY â†’ dfs(3): å·²BLACK,è·³è¿‡ color[2]=BLACK, order=[4,3,1,2] color[0]=BLACK, order=[4,3,1,2,0]åè½¬ â†’ [0,2,1,3,4] â† åˆæ³•æ‹“æ‰‘åº! ğŸ¯ ä¸¤ç§æ–¹æ³•æ€ä¹ˆé€‰ï¼Ÿ Kahnâ€™s BFS æ›´ç›´è§‚ï¼Œèƒ½ç›´æ¥æ£€æµ‹ç¯ï¼ˆresult é•¿åº¦ä¸å¤Ÿå°±æ˜¯æœ‰ç¯ï¼‰ã€‚DFS ååºæ³•ä»£ç æ›´çŸ­ï¼Œé€‚åˆéœ€è¦åŒæ—¶åšå…¶ä»–äº‹ï¼ˆå¦‚æ±‚å¼ºè¿é€šåˆ†é‡ï¼‰çš„åœºæ™¯ã€‚åˆ·é¢˜æ¨è Kahnâ€™s BFSã€‚ 2.2 è¿é€šåˆ†é‡è´¹æ›¼ä¸€å¥è¯ï¼š ä¸€å¼ å›¾é‡Œå¯èƒ½æœ‰å¥½å‡ ä¸ªã€Œæœ‹å‹åœˆã€ï¼Œäº’ç›¸ä¹‹é—´å®Œå…¨ä¸è®¤è¯†ã€‚æ¯ä¸ªæœ‹å‹åœˆå°±æ˜¯ä¸€ä¸ªè¿é€šåˆ†é‡ã€‚æ‰¾è¿é€šåˆ†é‡å°±æ˜¯æ•°ã€Œæœ‰å‡ ä¸ªç‹¬ç«‹çš„åœˆå­ã€ã€‚ æ–¹æ³•ä¸€ï¼šDFS æŸ“è‰²ä»æ¯ä¸ªæœªè®¿é—®çš„èŠ‚ç‚¹å‡ºå‘åšä¸€æ¬¡ DFSï¼Œèƒ½åˆ°è¾¾çš„æ‰€æœ‰èŠ‚ç‚¹å°±æ˜¯åŒä¸€ä¸ªè¿é€šåˆ†é‡ã€‚ def count_components(n, edges): graph = [[] for _ in range(n)] for u, v in edges: graph[u].append(v) graph[v].append(u) visited = [False] * n count = 0 def dfs(node): visited[node] = True for nb in graph[node]: if not visited[nb]: dfs(nb) for i in range(n): if not visited[i]: dfs(i) # å‘ç°ä¸€ä¸ªæ–°çš„è¿é€šåˆ†é‡ count += 1 return count æ•°æ®æµè¿½è¸ªï¼ˆå›¾: 0-1-2, 3-4, 5-6-7ï¼‰ï¼š i=0: æœªè®¿é—® â†’ dfs(0)â†’dfs(1)â†’dfs(2), count=1i=1: å·²è®¿é—®, è·³è¿‡i=2: å·²è®¿é—®, è·³è¿‡i=3: æœªè®¿é—® â†’ dfs(3)â†’dfs(4), count=2i=4: å·²è®¿é—®, è·³è¿‡i=5: æœªè®¿é—® â†’ dfs(5)â†’dfs(6)â†’dfs(7), count=3ç»“æœ: 3ä¸ªè¿é€šåˆ†é‡ æ–¹æ³•äºŒï¼šå¹¶æŸ¥é›† (Union-Find)è´¹æ›¼ä¸€å¥è¯ï¼š æ¯ä¸ªäººå¤´ä¸Šé¡¶ä¸ªç‰Œå­å†™ç€ã€Œæˆ‘çš„è€å¤§æ˜¯è°ã€ã€‚ä¸¤ä¸ªäººè®¤è¯†äº†ï¼Œå°±è®©ä¸€ä¸ªäººçš„è€å¤§è®¤å¦ä¸€ä¸ªäººçš„è€å¤§å½“è€å¤§ã€‚æœ€åæ•°æœ‰å‡ ä¸ªã€Œç»ˆæè€å¤§ã€ï¼Œå°±æœ‰å‡ ä¸ªåœˆå­ã€‚ class UnionFind: def __init__(self, n): self.parent = list(range(n)) self.rank = [0] * n def find(self, x): if self.parent[x] != x: self.parent[x] = self.find(self.parent[x]) # è·¯å¾„å‹ç¼© return self.parent[x] def union(self, x, y): px, py = self.find(x), self.find(y) if px == py: return False if self.rank[px] self.rank[py]: px, py = py, px self.parent[py] = px # æŒ‰ç§©åˆå¹¶ if self.rank[px] == self.rank[py]: self.rank[px] += 1 return True# ä½¿ç”¨uf = UnionFind(8)for u, v in [(0,1),(1,2),(0,2),(3,4),(5,6),(6,7)]: uf.union(u, v)# æ•°è¿é€šåˆ†é‡ = ä¸åŒæ ¹çš„ä¸ªæ•°print(len(set(uf.find(i) for i in range(8)))) # è¾“å‡º: 3 æ•°æ®æµè¿½è¸ªï¼š åˆå§‹: parent=[0,1,2,3,4,5,6,7] (æ¯äººæ˜¯è‡ªå·±çš„è€å¤§)union(0,1): parent=[0,0,2,3,4,5,6,7] (1çš„è€å¤§â†’0)union(1,2): find(1)=0, parent=[0,0,0,3,4,5,6,7] (2çš„è€å¤§â†’0)union(0,2): find(0)=0, find(2)=0, åŒæ ¹è·³è¿‡union(3,4): parent=[0,0,0,3,3,5,6,7]union(5,6): parent=[0,0,0,3,3,5,5,7]union(6,7): parent=[0,0,0,3,3,5,5,5]æ ¹é›†åˆ: {0, 3, 5} â†’ 3ä¸ªè¿é€šåˆ†é‡ ğŸ¯ DFS vs å¹¶æŸ¥é›†æ€ä¹ˆé€‰ï¼Ÿ é™æ€å›¾ï¼ˆè¾¹ä¸ä¼šå¢åŠ ï¼‰â†’ DFS æ›´ç®€å•ã€‚åŠ¨æ€å›¾ï¼ˆè¾¹ä¸æ–­åŠ å…¥ï¼Œéœ€è¦å®æ—¶æŸ¥è¯¢è¿é€šæ€§ï¼‰â†’ å¹¶æŸ¥é›†ï¼Œå› ä¸º union/find æ¥è¿‘ O(1)ã€‚ 2.3 äºŒåˆ†å›¾åˆ¤å®šï¼ˆæŸ“è‰²æ³•ï¼‰è´¹æ›¼ä¸€å¥è¯ï¼š æŠŠæ‰€æœ‰äººåˆ†æˆä¸¤é˜Ÿï¼Œè¦æ±‚æ¯æ¡è¾¹è¿æ¥çš„ä¸¤ä¸ªäººå¿…é¡»åœ¨ä¸åŒé˜Ÿã€‚å¦‚æœèƒ½åˆ†æˆåŠŸï¼Œå°±æ˜¯äºŒåˆ†å›¾ã€‚æ–¹æ³•å¾ˆç®€å•ï¼šBFS ä¸€å±‚å±‚æŸ“è‰²ï¼Œçº¢è“äº¤æ›¿ï¼Œé‡åˆ°çŸ›ç›¾å°±ä¸æ˜¯ã€‚ å…³é”®å®šç†ï¼š ä¸€ä¸ªå›¾æ˜¯äºŒåˆ†å›¾ âŸº å›¾ä¸­ä¸å­˜åœ¨å¥‡æ•°é•¿åº¦çš„ç¯ã€‚ from collections import dequedef is_bipartite(n, edges): graph = [[] for _ in range(n)] for u, v in edges: graph[u].append(v) graph[v].append(u) color = [-1] * n # -1=æœªæŸ“è‰², 0=çº¢, 1=è“ for start in range(n): if color[start] != -1: continue color[start] = 0 queue = deque([start]) while queue: node = queue.popleft() for nb in graph[node]: if color[nb] == -1: color[nb] = 1 - color[node] # æŸ“ç›¸åè‰² queue.append(nb) elif color[nb] == color[node]: return False # åŒè‰²ç›¸é‚»=çŸ›ç›¾! return True æ•°æ®æµè¿½è¸ªï¼ˆäºŒåˆ†å›¾: 0-1, 0-3, 2-1, 2-3, 4-1, 4-3ï¼‰ï¼š start=0: color[0]=0(çº¢), queue=[0] pop 0: é‚»å±… 1,3 color[1]=-1 â†’ æŸ“1(è“), queue=[1,3] color[3]=-1 â†’ æŸ“1(è“) pop 1: é‚»å±… 0,2,4 color[0]=0 â‰  color[1]=1 â†’ OK color[2]=-1 â†’ æŸ“0(çº¢), queue=[3,2,4] color[4]=-1 â†’ æŸ“0(çº¢) pop 3: é‚»å±… 0,2,4 color[0]=0 â‰  1 â†’ OK color[2]=0 â‰  1 â†’ OK color[4]=0 â‰  1 â†’ OK pop 2, pop 4: é‚»å±…éƒ½å·²æŸ“è‰²ä¸”æ— çŸ›ç›¾â†’ æ˜¯äºŒåˆ†å›¾! çº¢={0,2,4}, è“={1,3} æ•°æ®æµè¿½è¸ªï¼ˆéäºŒåˆ†å›¾: 0-1, 1-2, 2-0, ä¸‰è§’å½¢ï¼‰ï¼š start=0: color[0]=0(çº¢) pop 0: é‚»å±… 1,2 color[1]=1(è“), color[2]=1(è“) pop 1: é‚»å±… 0,2 color[0]=0 â‰  1 â†’ OK color[2]=1 == color[1]=1 â†’ çŸ›ç›¾! return Falseâ†’ ä¸æ˜¯äºŒåˆ†å›¾ (ä¸‰è§’å½¢=å¥‡æ•°ç¯) 2.4 æœ€çŸ­è·¯å¾„æ— æƒå›¾ï¼šBFS ç›´æ¥æå®šè´¹æ›¼ä¸€å¥è¯ï¼š æ¯æ¡è¾¹é•¿åº¦éƒ½æ˜¯ 1ï¼ŒBFS å¤©ç„¶æŒ‰å±‚æ‰©å±•ï¼Œç¬¬ä¸€æ¬¡åˆ°è¾¾å°±æ˜¯æœ€çŸ­è·¯ã€‚ from collections import dequedef shortest_path_bfs(graph, start, end): dist = {start: 0} queue = deque([start]) while queue: node = queue.popleft() if node == end: return dist[end] for nb in graph[node]: if nb not in dist: dist[nb] = dist[node] + 1 queue.append(nb) return -1 # ä¸å¯è¾¾ æœ‰æƒå›¾ï¼šDijkstraè´¹æ›¼ä¸€å¥è¯ï¼š è´ªå¿ƒç­–ç•¥â€”â€”æ¯æ¬¡ä»ã€Œå·²çŸ¥æœ€çŸ­è·ç¦»çš„å€™é€‰äººã€ä¸­æŒ‘æœ€å°çš„é‚£ä¸ªï¼Œç¡®è®¤å®ƒçš„æœ€çŸ­è·¯ï¼Œç„¶åç”¨å®ƒå»æ›´æ–°é‚»å±…ã€‚å°±åƒæ°´ä»æºå¤´æµå‡ºï¼Œæ€»æ˜¯å…ˆåˆ°æœ€è¿‘çš„åœ°æ–¹ã€‚ import heapqdef dijkstra(graph, start): dist = {start: 0} heap = [(0, start)] while heap: d, u = heapq.heappop(heap) if d dist.get(u, float('inf')): continue # è¿‡æ—¶çš„æ¡ç›®ï¼Œè·³è¿‡ for v, w in graph[u]: nd = d + w if nd dist.get(v, float('inf')): dist[v] = nd heapq.heappush(heap, (nd, v)) return dist æ•°æ®æµè¿½è¸ªï¼ˆå›¾: 0â†’1:4, 0â†’2:1, 2â†’1:2, 1â†’3:1, 2â†’4:5, 3â†’4:3ï¼‰ï¼š åˆå§‹: dist={0:0}, heap=[(0,0)]pop (0,0): å¤„ç†èŠ‚ç‚¹0 â†’ é‚»å±…1: 0+4=4 inf â†’ dist={0:0,1:4}, push(4,1) â†’ é‚»å±…2: 0+1=1 inf â†’ dist={0:0,1:4,2:1}, push(1,2) heap=[(1,2),(4,1)]pop (1,2): å¤„ç†èŠ‚ç‚¹2 â†’ é‚»å±…1: 1+2=3 4 â†’ dist[1]=3, push(3,1) â† æ¾å¼›! â†’ é‚»å±…4: 1+5=6 inf â†’ dist[4]=6, push(6,4) heap=[(3,1),(4,1),(6,4)]pop (3,1): å¤„ç†èŠ‚ç‚¹1 â†’ é‚»å±…3: 3+1=4 inf â†’ dist[3]=4, push(4,3) heap=[(4,1),(4,3),(6,4)]pop (4,1): d=4 dist[1]=3 â†’ è¿‡æ—¶æ¡ç›®,è·³è¿‡!pop (4,3): å¤„ç†èŠ‚ç‚¹3 â†’ é‚»å±…4: 4+3=7 dist[4]=6 â†’ ä¸æ›´æ–° heap=[(6,4)]pop (6,4): å¤„ç†èŠ‚ç‚¹4, æ— æ›´æ–° heap=[] â†’ ç»“æŸæœ€ç»ˆ: dist = {0:0, 1:3, 2:1, 3:4, 4:6}æœ€çŸ­è·¯å¾„: 0â†’2â†’1â†’3, 0â†’2â†’4 âš ï¸ Dijkstra ä¸èƒ½å¤„ç†è´Ÿæƒè¾¹ï¼ å› ä¸ºè´ªå¿ƒå‡è®¾ã€Œå·²ç¡®è®¤çš„æœ€çŸ­è·ç¦»ä¸ä¼šå†å˜å°ã€ï¼Œè´Ÿæƒè¾¹ä¼šæ‰“ç ´è¿™ä¸ªå‡è®¾ã€‚è´Ÿæƒå›¾ç”¨ Bellman-Fordã€‚ ğŸ¯ å¤æ‚åº¦ï¼š ç”¨æœ€å°å †å®ç°ï¼Œæ—¶é—´ O((V+E)logV)ï¼Œç©ºé—´ O(V)ã€‚ ğŸ“ æŠ€å·§ç¯‡å°ç»“ æŠ€å·§ æ ¸å¿ƒæ€æƒ³ æ•°æ®ç»“æ„ æ—¶é—´å¤æ‚åº¦ æ‹“æ‰‘æ’åº(Kahn) ä¸æ–­åˆ å…¥åº¦ä¸º0çš„èŠ‚ç‚¹ é˜Ÿåˆ—+å…¥åº¦æ•°ç»„ O(V+E) æ‹“æ‰‘æ’åº(DFS) ååºåè½¬ é€’å½’æ ˆ+ä¸‰è‰²æ ‡è®° O(V+E) è¿é€šåˆ†é‡(DFS) ä»æœªè®¿é—®èŠ‚ç‚¹å‡ºå‘DFS visitedæ•°ç»„ O(V+E) è¿é€šåˆ†é‡(å¹¶æŸ¥é›†) unionè¾¹, æ•°æ ¹ parentæ•°ç»„ O(EÎ±(V))â‰ˆO(E) äºŒåˆ†å›¾åˆ¤å®š BFSæŸ“è‰², æ£€æŸ¥çŸ›ç›¾ coloræ•°ç»„+é˜Ÿåˆ— O(V+E) æœ€çŸ­è·¯(æ— æƒ) BFS é˜Ÿåˆ—+dist O(V+E) æœ€çŸ­è·¯(æœ‰æƒ) Dijkstraè´ªå¿ƒ æœ€å°å †+dist O((V+E)logV) ç¬¬ä¸‰ç« ï¼šå®æˆ˜ç¯‡ â€” LeetCode çœŸé¢˜æ‹†è§£ 3.1 LeetCode 200. å²›å±¿æ•°é‡é¢˜æ„ï¼š ç»™ä¸€ä¸ª mÃ—n çš„äºŒç»´ç½‘æ ¼ï¼Œ'1' æ˜¯é™†åœ°ï¼Œ'0' æ˜¯æ°´ã€‚è®¡ç®—å²›å±¿æ•°é‡ï¼ˆä¸Šä¸‹å·¦å³ç›¸è¿çš„é™†åœ°ç®—ä¸€ä¸ªå²›ï¼‰ã€‚ è´¹æ›¼ä¸€å¥è¯ï¼š è¿™å°±æ˜¯æ±‚è¿é€šåˆ†é‡ï¼æ¯å—ç›¸è¿çš„é™†åœ°æ˜¯ä¸€ä¸ªè¿é€šåˆ†é‡ã€‚ä»æ¯ä¸ªæœªè®¿é—®çš„ '1' å‡ºå‘åš DFSï¼ŒæŠŠæ•´å—å²›ã€Œæ·¹æ‰ã€ï¼ˆæ ‡è®°ä¸ºå·²è®¿é—®ï¼‰ï¼Œæ•°ä½ æ·¹äº†å‡ æ¬¡ã€‚ æœ¬è´¨ï¼š ç½‘æ ¼å°±æ˜¯å›¾ï¼Œæ¯ä¸ªæ ¼å­æ˜¯èŠ‚ç‚¹ï¼Œä¸Šä¸‹å·¦å³æ˜¯è¾¹ã€‚ class Solution: def numIslands(self, grid): if not grid: return 0 m, n = len(grid), len(grid[0]) count = 0 def dfs(i, j): if i 0 or i = m or j 0 or j = n or grid[i][j] != '1': return grid[i][j] = '0' # æ·¹æ‰ï¼Œé¿å…é‡å¤è®¿é—® dfs(i+1,j); dfs(i-1,j); dfs(i,j+1); dfs(i,j-1) for i in range(m): for j in range(n): if grid[i][j] == '1': dfs(i, j) count += 1 return count æ•°æ®æµè¿½è¸ªï¼š è¾“å…¥: 1 1 0 0 0 1 1 0 0 0 0 0 1 0 0 0 0 0 1 1i=0,j=0: grid[0][0]='1' â†’ dfsæ·¹æ‰æ•´å— dfs(0,0)â†’dfs(1,0)â†’dfs(1,1)â†’dfs(0,1) å…¨éƒ¨å˜'0' count=1i=2,j=2: grid[2][2]='1' â†’ dfsæ·¹æ‰ count=2i=3,j=3: grid[3][3]='1' â†’ dfs(3,3)â†’dfs(3,4) count=3ç»“æœ: 3ä¸ªå²›å±¿ ğŸ¯ æ ¸å¿ƒæŠ€å·§ï¼š ç›´æ¥ä¿®æ”¹åŸæ•°ç»„å½“ visitedï¼Œçœç©ºé—´ã€‚å¦‚æœä¸èƒ½æ”¹åŸæ•°ç»„ï¼Œç”¨ visited é›†åˆã€‚ 3.2 LeetCode 207/210. è¯¾ç¨‹è¡¨ï¼ˆæ‹“æ‰‘æ’åºï¼‰é¢˜æ„ï¼š 207 â€” ç»™ n é—¨è¯¾å’Œå…ˆä¿®å…³ç³»ï¼Œåˆ¤æ–­èƒ½å¦ä¿®å®Œæ‰€æœ‰è¯¾ã€‚210 â€” è¿”å›ä¸€ä¸ªåˆæ³•çš„ä¿®è¯¾é¡ºåºã€‚ è´¹æ›¼ä¸€å¥è¯ï¼š å…ˆä¿®å…³ç³»å°±æ˜¯æœ‰å‘è¾¹ï¼Œèƒ½ä¿®å®Œ = æ²¡æœ‰ç¯ = èƒ½æ‹“æ‰‘æ’åºã€‚ # 207. èƒ½å¦ä¿®å®Œclass Solution: def canFinish(self, numCourses, prerequisites): graph = [[] for _ in range(numCourses)] indegree = [0] * numCourses for a, b in prerequisites: # bâ†’a (å…ˆä¿®bæ‰èƒ½ä¸Ša) graph[b].append(a) indegree[a] += 1 queue = deque(i for i in range(numCourses) if indegree[i] == 0) count = 0 while queue: node = queue.popleft() count += 1 for nb in graph[node]: indegree[nb] -= 1 if indegree[nb] == 0: queue.append(nb) return count == numCourses # 210. è¿”å›ä¿®è¯¾é¡ºåº (åªéœ€æŠŠcountæ¢æˆresultåˆ—è¡¨)class Solution: def findOrder(self, numCourses, prerequisites): graph = [[] for _ in range(numCourses)] indegree = [0] * numCourses for a, b in prerequisites: graph[b].append(a) indegree[a] += 1 queue = deque(i for i in range(numCourses) if indegree[i] == 0) result = [] while queue: node = queue.popleft() result.append(node) for nb in graph[node]: indegree[nb] -= 1 if indegree[nb] == 0: queue.append(nb) return result if len(result) == numCourses else [] æ•°æ®æµè¿½è¸ªï¼ˆn=4, prereqs=[[1,0],[2,0],[3,1],[3,2]]ï¼‰ï¼š å›¾: 0â†’1, 0â†’2, 1â†’3, 2â†’3indegree: [0,1,1,2]queue=[0], result=[]pop 0 â†’ result=[0], indegree[1]=0â†’å…¥é˜Ÿ, indegree[2]=0â†’å…¥é˜Ÿpop 1 â†’ result=[0,1], indegree[3]=1pop 2 â†’ result=[0,1,2], indegree[3]=0â†’å…¥é˜Ÿpop 3 â†’ result=[0,1,2,3]len=4=n â†’ åˆæ³•! é¡ºåº: [0,1,2,3] 3.3 LeetCode 133. å…‹éš†å›¾é¢˜æ„ï¼š ç»™ä¸€ä¸ªæ— å‘è¿é€šå›¾çš„æŸä¸ªèŠ‚ç‚¹å¼•ç”¨ï¼Œè¿”å›è¯¥å›¾çš„æ·±æ‹·è´ã€‚ è´¹æ›¼ä¸€å¥è¯ï¼š éå†åŸå›¾ï¼Œæ¯é‡åˆ°ä¸€ä¸ªèŠ‚ç‚¹å°±ã€Œå…‹éš†ã€ä¸€ä»½ã€‚ç”¨å“ˆå¸Œè¡¨è®°å½•ã€ŒåŸèŠ‚ç‚¹â†’å…‹éš†èŠ‚ç‚¹ã€çš„æ˜ å°„ï¼Œé¿å…é‡å¤å…‹éš†ï¼ˆä¹Ÿå°±æ˜¯ visited çš„ä½œç”¨ï¼‰ã€‚ class Solution: def cloneGraph(self, node): if not node: return None cloned = {node: Node(node.val)} queue = deque([node]) while queue: curr = queue.popleft() for nb in curr.neighbors: if nb not in cloned: cloned[nb] = Node(nb.val) queue.append(nb) cloned[curr].neighbors.append(cloned[nb]) return cloned[node] æ•°æ®æµè¿½è¸ªï¼ˆå›¾: 1-2, 1-4, 2-3, 3-4ï¼‰ï¼š cloned={1:Node(1)}, queue=[1]pop 1: é‚»å±… [2,4] 2 ä¸åœ¨cloned â†’ cloned[2]=Node(2), å…¥é˜Ÿ 4 ä¸åœ¨cloned â†’ cloned[4]=Node(4), å…¥é˜Ÿ clone[1].neighbors = [clone[2], clone[4]]pop 2: é‚»å±… [1,3] 1 å·²åœ¨cloned â†’ è·³è¿‡åˆ›å»º 3 ä¸åœ¨cloned â†’ cloned[3]=Node(3), å…¥é˜Ÿ clone[2].neighbors = [clone[1], clone[3]]pop 4: é‚»å±… [1,3] clone[4].neighbors = [clone[1], clone[3]]pop 3: é‚»å±… [2,4] clone[3].neighbors = [clone[2], clone[4]]è¿”å› cloned[1] â†’ å®Œæ•´çš„æ·±æ‹·è´å›¾ ğŸ¯ æ ¸å¿ƒæŠ€å·§ï¼š cloned å­—å…¸åŒæ—¶å……å½“ visited å’Œæ˜ å°„è¡¨ï¼Œä¸€çŸ³äºŒé¸Ÿã€‚ 3.4 LeetCode 785. åˆ¤æ–­äºŒåˆ†å›¾é¢˜æ„ï¼š ç»™ä¸€ä¸ªé‚»æ¥è¡¨è¡¨ç¤ºçš„æ— å‘å›¾ï¼Œåˆ¤æ–­æ˜¯å¦æ˜¯äºŒåˆ†å›¾ã€‚ è´¹æ›¼ä¸€å¥è¯ï¼š ç›´æ¥å¥—æŸ“è‰²æ³•æ¨¡æ¿ã€‚BFS çº¢è“äº¤æ›¿æŸ“ï¼Œé‡åˆ°çŸ›ç›¾å°±ä¸æ˜¯ã€‚ class Solution: def isBipartite(self, graph): n = len(graph) color = [-1] * n for i in range(n): if color[i] != -1: continue color[i] = 0 queue = deque([i]) while queue: u = queue.popleft() for v in graph[u]: if color[v] == -1: color[v] = 1 - color[u] queue.append(v) elif color[v] == color[u]: return False return True ğŸ¯ æ³¨æ„ï¼š å›¾å¯èƒ½ä¸è¿é€šï¼Œæ‰€ä»¥å¤–å±‚è¦éå†æ‰€æœ‰èŠ‚ç‚¹ä½œä¸ºèµ·ç‚¹ã€‚ 3.5 LeetCode 743. ç½‘ç»œå»¶è¿Ÿæ—¶é—´ï¼ˆDijkstraï¼‰é¢˜æ„ï¼š n ä¸ªèŠ‚ç‚¹çš„æœ‰å‘åŠ æƒå›¾ï¼Œä»èŠ‚ç‚¹ k å‘ä¿¡å·ï¼Œè¿”å›æ‰€æœ‰èŠ‚ç‚¹æ”¶åˆ°ä¿¡å·çš„æœ€çŸ­æ—¶é—´ã€‚å¦‚æœæœ‰èŠ‚ç‚¹æ”¶ä¸åˆ°ï¼Œè¿”å› -1ã€‚ è´¹æ›¼ä¸€å¥è¯ï¼š ä» k å‡ºå‘è·‘ Dijkstraï¼Œæ±‚åˆ°æ‰€æœ‰èŠ‚ç‚¹çš„æœ€çŸ­è·ç¦»ï¼Œå–æœ€å¤§å€¼å°±æ˜¯ç­”æ¡ˆã€‚ class Solution: def networkDelayTime(self, times, n, k): graph = defaultdict(list) for u, v, w in times: graph[u].append((v, w)) dist = {} heap = [(0, k)] while heap: d, u = heapq.heappop(heap) if u in dist: continue dist[u] = d for v, w in graph[u]: if v not in dist: heapq.heappush(heap, (d + w, v)) return max(dist.values()) if len(dist) == n else -1 æ•°æ®æµè¿½è¸ªï¼ˆtimes=[[2,1,1],[2,3,1],[3,4,1]], n=4, k=2ï¼‰ï¼š graph: {2:[(1,1),(3,1)], 3:[(4,1)]}heap=[(0,2)]pop (0,2): dist={2:0}, push (1,1),(1,3)pop (1,1): dist={2:0,1:1}pop (1,3): dist={2:0,1:1,3:1}, push (2,4)pop (2,4): dist={2:0,1:1,3:1,4:2}len(dist)=4=n â†’ max(0,1,1,2) = 2 3.6 LeetCode 994. è…çƒ‚çš„æ©˜å­ï¼ˆå¤šæº BFSï¼‰é¢˜æ„ï¼š ç½‘æ ¼ä¸­ 0=ç©ºï¼Œ1=æ–°é²œæ©˜å­ï¼Œ2=è…çƒ‚æ©˜å­ã€‚æ¯åˆ†é’Ÿè…çƒ‚æ©˜å­ä¼šæ„ŸæŸ“ä¸Šä¸‹å·¦å³çš„æ–°é²œæ©˜å­ã€‚è¿”å›æ‰€æœ‰æ©˜å­è…çƒ‚çš„æœ€çŸ­æ—¶é—´ï¼Œä¸å¯èƒ½åˆ™è¿”å› -1ã€‚ è´¹æ›¼ä¸€å¥è¯ï¼š å¤šä¸ªè…çƒ‚æ©˜å­åŒæ—¶å¼€å§‹æ‰©æ•£ï¼Œå°±åƒåŒæ—¶å¾€æ± å¡˜é‡Œæ‰”å¥½å‡ å—çŸ³å¤´ã€‚æŠŠæ‰€æœ‰è…çƒ‚æ©˜å­ä¸€èµ·æ”¾è¿›é˜Ÿåˆ—ä½œä¸ºèµ·ç‚¹ï¼Œç„¶åæ ‡å‡† BFS å±‚åºæ‰©å±•ï¼Œå±‚æ•°å°±æ˜¯æ—¶é—´ã€‚ class Solution: def orangesRotting(self, grid): m, n = len(grid), len(grid[0]) queue = deque() fresh = 0 for i in range(m): for j in range(n): if grid[i][j] == 2: queue.append((i, j)) elif grid[i][j] == 1: fresh += 1 if fresh == 0: return 0 minutes = 0 while queue: minutes += 1 for _ in range(len(queue)): x, y = queue.popleft() for dx, dy in [(1,0),(-1,0),(0,1),(0,-1)]: nx_, ny_ = x+dx, y+dy if 0=nx_m and 0=ny_n and grid[nx_][ny_]==1: grid[nx_][ny_] = 2 fresh -= 1 queue.append((nx_, ny_)) if fresh == 0: return minutes return -1 æ•°æ®æµè¿½è¸ªï¼š è¾“å…¥: 2 1 1 1 1 0 0 1 1åˆå§‹: queue=[(0,0)], fresh=7Minute 1: æ‰©æ•£(0,0)â†’æ„ŸæŸ“(0,1),(1,0) grid: 2 2 1 fresh=5 2 1 0 0 1 1Minute 2: æ‰©æ•£(0,1),(1,0)â†’æ„ŸæŸ“(0,2),(1,1) grid: 2 2 2 fresh=3 2 2 0 0 1 1Minute 3: æ‰©æ•£(0,2),(1,1)â†’æ„ŸæŸ“(2,1) grid: 2 2 2 fresh=2 2 2 0 0 2 1Minute 4: æ‰©æ•£(2,1)â†’æ„ŸæŸ“(2,2) grid: 2 2 2 fresh=0 â†’ return 4 2 2 0 0 2 2 ğŸ¯ å¤šæº BFS çš„å…³é”®ï¼š æŠŠæ‰€æœ‰æºç‚¹ä¸€èµ·å…¥é˜Ÿï¼Œè€Œä¸æ˜¯å¯¹æ¯ä¸ªæºç‚¹åˆ†åˆ« BFSã€‚è¿™æ ·æ—¶é—´å¤æ‚åº¦è¿˜æ˜¯ O(mÃ—n)ã€‚ ç¬¬å››ç« ï¼šAnki é—ªè®°å¡ç‰‡ä»¥ä¸‹ 15 å¼ å¡ç‰‡å¯ç›´æ¥å¯¼å…¥ Ankiï¼ˆQ/A æ ¼å¼ï¼‰ã€‚ Q1: å›¾çš„é‚»æ¥è¡¨å’Œé‚»æ¥çŸ©é˜µï¼Œåˆ·é¢˜é»˜è®¤ç”¨å“ªä¸ªï¼Ÿä¸ºä»€ä¹ˆï¼ŸA1: é‚»æ¥è¡¨ã€‚ç©ºé—´ O(V+E) æ›´çœï¼Œéå†é‚»å±… O(degree) æ›´å¿«ã€‚90% çš„å›¾é¢˜éƒ½ç”¨å®ƒã€‚ Q2: DFS ç”¨ä»€ä¹ˆæ•°æ®ç»“æ„ï¼Ÿä¸€å¥è¯æè¿°å®ƒçš„ç­–ç•¥ã€‚A2: æ ˆï¼ˆé€’å½’è°ƒç”¨æ ˆæˆ–æ˜¾å¼æ ˆï¼‰ã€‚ç­–ç•¥ï¼šä¸€æ¡è·¯èµ°åˆ°é»‘ï¼Œæ’å¢™å†å›å¤´ã€‚ Q3: BFS ç”¨ä»€ä¹ˆæ•°æ®ç»“æ„ï¼Ÿä¸ºä»€ä¹ˆå®ƒèƒ½æ±‚æ— æƒå›¾æœ€çŸ­è·¯ï¼ŸA3: é˜Ÿåˆ—ï¼ˆFIFOï¼‰ã€‚å› ä¸º BFS æŒ‰å±‚æ‰©å±•ï¼Œç¬¬ä¸€æ¬¡åˆ°è¾¾æŸèŠ‚ç‚¹æ—¶èµ°çš„è¾¹æ•°ä¸€å®šæœ€å°‘ã€‚ Q4: BFS ä¸­ visited åº”è¯¥åœ¨å…¥é˜Ÿæ—¶æ ‡è®°è¿˜æ˜¯å‡ºé˜Ÿæ—¶ï¼Ÿä¸ºä»€ä¹ˆï¼ŸA4: å…¥é˜Ÿæ—¶ï¼å‡ºé˜Ÿæ—¶æ ‡è®°ä¼šå¯¼è‡´åŒä¸€èŠ‚ç‚¹è¢«å¤šæ¬¡å…¥é˜Ÿï¼Œæµªè´¹æ—¶é—´ç©ºé—´ã€‚ Q5: æ‹“æ‰‘æ’åºçš„å‰ææ¡ä»¶æ˜¯ä»€ä¹ˆï¼ŸA5: å›¾å¿…é¡»æ˜¯æœ‰å‘æ— ç¯å›¾ï¼ˆDAGï¼‰ã€‚æœ‰ç¯åˆ™æ— æ³•æ‹“æ‰‘æ’åºã€‚ Q6: Kahnâ€™s BFS æ‹“æ‰‘æ’åºçš„æ ¸å¿ƒæ“ä½œæ˜¯ä»€ä¹ˆï¼ŸA6: ä¸æ–­æ‰¾å…¥åº¦ä¸º 0 çš„èŠ‚ç‚¹ï¼Œå¼¹å‡ºå¹¶åˆ é™¤å®ƒçš„å‡ºè¾¹ï¼Œä½¿é‚»å±…å…¥åº¦å‡ 1ã€‚é‡å¤ç›´åˆ°é˜Ÿåˆ—ç©ºã€‚ Q7: å¦‚ä½•ç”¨ DFS åšæ‹“æ‰‘æ’åºï¼ŸA7: DFS ååºè®°å½•å®Œæˆé¡ºåºï¼Œæœ€ååè½¬ã€‚ç”¨ä¸‰è‰²æ ‡è®°ï¼ˆç™½/ç°/é»‘ï¼‰æ£€æµ‹ç¯ï¼šé‡åˆ°ç°è‰²èŠ‚ç‚¹è¯´æ˜æœ‰ç¯ã€‚ Q8: ä»€ä¹ˆæ˜¯è¿é€šåˆ†é‡ï¼Ÿæ€ä¹ˆæ•°ï¼ŸA8: å›¾ä¸­äº’ç›¸å¯è¾¾çš„æœ€å¤§èŠ‚ç‚¹é›†åˆã€‚æ–¹æ³•ï¼šéå†æ‰€æœ‰èŠ‚ç‚¹ï¼Œæ¯æ¬¡ä»æœªè®¿é—®èŠ‚ç‚¹å‡ºå‘ DFS/BFSï¼Œè®¡æ•° +1ã€‚ Q9: å¹¶æŸ¥é›†çš„ä¸¤ä¸ªæ ¸å¿ƒä¼˜åŒ–æ˜¯ä»€ä¹ˆï¼ŸA9: è·¯å¾„å‹ç¼©ï¼ˆfind æ—¶ç›´æ¥æŒ‡å‘æ ¹ï¼‰+ æŒ‰ç§©åˆå¹¶ï¼ˆçŸ®æ ‘æŒ‚åˆ°é«˜æ ‘ä¸‹ï¼‰ã€‚ä½¿ union/find æ¥è¿‘ O(1)ã€‚ Q10: å¦‚ä½•åˆ¤æ–­ä¸€ä¸ªå›¾æ˜¯å¦æ˜¯äºŒåˆ†å›¾ï¼ŸA10: BFS æŸ“è‰²æ³•ï¼šçº¢è“äº¤æ›¿æŸ“è‰²ï¼Œå¦‚æœç›¸é‚»èŠ‚ç‚¹åŒè‰²åˆ™ä¸æ˜¯äºŒåˆ†å›¾ã€‚ç­‰ä»·æ¡ä»¶ï¼šä¸å­˜åœ¨å¥‡æ•°ç¯ã€‚ Q11: Dijkstra ç®—æ³•çš„æ ¸å¿ƒè´ªå¿ƒç­–ç•¥æ˜¯ä»€ä¹ˆï¼ŸA11: æ¯æ¬¡ä»æœªç¡®è®¤çš„èŠ‚ç‚¹ä¸­é€‰è·ç¦»æœ€å°çš„ï¼Œç¡®è®¤å…¶æœ€çŸ­è·¯ï¼Œç„¶åç”¨å®ƒæ¾å¼›é‚»å±…ã€‚ç”¨æœ€å°å †å®ç°ã€‚ Q12: Dijkstra ä¸ºä»€ä¹ˆä¸èƒ½å¤„ç†è´Ÿæƒè¾¹ï¼ŸA12: è´ªå¿ƒå‡è®¾ã€Œå·²ç¡®è®¤çš„æœ€çŸ­è·ç¦»ä¸ä¼šå†å˜å°ã€ï¼Œè´Ÿæƒè¾¹ä¼šæ‰“ç ´è¿™ä¸ªå‡è®¾ã€‚è´Ÿæƒå›¾ç”¨ Bellman-Fordã€‚ Q13: LeetCode 200 å²›å±¿æ•°é‡çš„æœ¬è´¨æ˜¯ä»€ä¹ˆï¼ŸA13: æ±‚ç½‘æ ¼å›¾çš„è¿é€šåˆ†é‡æ•°ã€‚æ¯æ¬¡ä»æœªè®¿é—®çš„ â€˜1â€™ å‡ºå‘ DFS æ·¹æ‰æ•´å—å²›ï¼Œè®¡æ•° +1ã€‚ Q14: å¤šæº BFS å’Œæ™®é€š BFS çš„åŒºåˆ«ï¼ŸA14: å¤šæº BFS æŠŠæ‰€æœ‰æºç‚¹åŒæ—¶å…¥é˜Ÿä½œä¸ºç¬¬ 0 å±‚ï¼Œç„¶åæ­£å¸¸å±‚åºæ‰©å±•ã€‚å…¸å‹é¢˜ï¼š994 è…çƒ‚çš„æ©˜å­ã€‚ Q15: DFS å’Œ BFS æ€ä¹ˆé€‰ï¼ŸA15: è¦æœ€çŸ­è·¯ â†’ BFSã€‚è¦éå†æ‰€æœ‰è·¯å¾„/è¿é€šæ€§/æ‹“æ‰‘æ’åº â†’ DFSã€‚ä¸ç¡®å®š â†’ éƒ½è¡Œï¼Œé€‰é¡ºæ‰‹çš„ã€‚ ç¬¬äº”ç« ï¼šLeetCode åˆ·é¢˜è·¯çº¿å›¾é˜¶æ®µä¸€ï¼šå…¥é—¨ï¼ˆæŒæ¡ DFS/BFS æ¨¡æ¿ï¼‰ é¢˜å· é¢˜ç›® æ ¸å¿ƒæŠ€å·§ éš¾åº¦ 200 å²›å±¿æ•°é‡ DFS/BFS è¿é€šåˆ†é‡ Medium 733 å›¾åƒæ¸²æŸ“ Flood Fill (DFS) Easy 695 å²›å±¿çš„æœ€å¤§é¢ç§¯ DFS + è®¡æ•° Medium 994 è…çƒ‚çš„æ©˜å­ å¤šæº BFS Medium 542 01 çŸ©é˜µ å¤šæº BFS æ±‚æœ€çŸ­è·ç¦» Medium 133 å…‹éš†å›¾ BFS + å“ˆå¸Œæ˜ å°„ Medium é˜¶æ®µäºŒï¼šè¿›é˜¶ï¼ˆæ‹“æ‰‘æ’åº + äºŒåˆ†å›¾ + å¹¶æŸ¥é›†ï¼‰ é¢˜å· é¢˜ç›® æ ¸å¿ƒæŠ€å·§ éš¾åº¦ 207 è¯¾ç¨‹è¡¨ Kahnâ€™s æ‹“æ‰‘æ’åº Medium 210 è¯¾ç¨‹è¡¨ II æ‹“æ‰‘æ’åºè¾“å‡ºåºåˆ— Medium 785 åˆ¤æ–­äºŒåˆ†å›¾ BFS æŸ“è‰² Medium 886 å¯èƒ½çš„äºŒåˆ†æ³• äºŒåˆ†å›¾å˜ä½“ Medium 547 çœä»½æ•°é‡ å¹¶æŸ¥é›†/DFS è¿é€šåˆ†é‡ Medium 684 å†—ä½™è¿æ¥ å¹¶æŸ¥é›†æ£€æµ‹ç¯ Medium é˜¶æ®µä¸‰ï¼šç¡¬æ ¸ï¼ˆæœ€çŸ­è·¯ + ç»¼åˆåº”ç”¨ï¼‰ é¢˜å· é¢˜ç›® æ ¸å¿ƒæŠ€å·§ éš¾åº¦ 743 ç½‘ç»œå»¶è¿Ÿæ—¶é—´ Dijkstra Medium 787 K ç«™ä¸­è½¬æœ€ä¾¿å®œèˆªç­ Bellman-Ford/BFS Medium 1091 äºŒè¿›åˆ¶çŸ©é˜µæœ€çŸ­è·¯å¾„ BFS 8æ–¹å‘ Medium 127 å•è¯æ¥é¾™ BFS + çŠ¶æ€å›¾ Hard 329 çŸ©é˜µä¸­çš„æœ€é•¿é€’å¢è·¯å¾„ DFS + è®°å¿†åŒ– Hard 1192 æŸ¥æ‰¾é›†ç¾¤å†…çš„å…³é”®è¿æ¥ Tarjan æ±‚æ¡¥ Hard ç¬¬å…­ç« ï¼šç®—æ³•é€‰æ‹©å†³ç­–æ ‘æ‹¿åˆ°ä¸€é“å›¾é¢˜ï¼Œè„‘å­é‡Œåº”è¯¥è·‘è¿™ä¸ªå†³ç­–æµç¨‹ï¼š é¢˜ç›®ç»™äº†å›¾ â†’â”‚â”œâ”€ æ±‚æœ€çŸ­è·¯ï¼Ÿâ”‚ â”œâ”€ æ— æƒå›¾ â†’ BFSâ”‚ â”œâ”€ æœ‰æƒå›¾(éè´Ÿæƒ) â†’ Dijkstraâ”‚ â””â”€ æœ‰è´Ÿæƒ â†’ Bellman-Fordâ”‚â”œâ”€ åˆ¤æ–­èƒ½å¦å®Œæˆ/æœ‰æ— ç¯ï¼Ÿâ”‚ â””â”€ æœ‰å‘å›¾ â†’ æ‹“æ‰‘æ’åº (Kahn's BFS)â”‚â”œâ”€ æ±‚è¿é€šåˆ†é‡/å²›å±¿æ•°é‡ï¼Ÿâ”‚ â”œâ”€ é™æ€å›¾ â†’ DFSâ”‚ â””â”€ åŠ¨æ€åŠ è¾¹ â†’ å¹¶æŸ¥é›†â”‚â”œâ”€ åˆ¤æ–­äºŒåˆ†å›¾ï¼Ÿâ”‚ â””â”€ BFS æŸ“è‰²æ³•â”‚â”œâ”€ æ±‚æ‰€æœ‰è·¯å¾„/æ’åˆ—ç»„åˆï¼Ÿâ”‚ â””â”€ DFS + å›æº¯â”‚â””â”€ ä¸ç¡®å®šï¼Ÿ â””â”€ å…ˆè¯• BFSï¼ˆå¦‚æœéœ€è¦æœ€çŸ­/æœ€å°‘ï¼‰ï¼Œå¦åˆ™ DFS å¤æ‚åº¦é€ŸæŸ¥è¡¨ï¼š ç®—æ³• æ—¶é—´ ç©ºé—´ é€‚ç”¨æ¡ä»¶ BFS O(V+E) O(V) æ— æƒæœ€çŸ­è·¯ã€å±‚åºéå† DFS O(V+E) O(V) è¿é€šæ€§ã€å›æº¯ã€æ‹“æ‰‘æ’åº Dijkstra(å †) O((V+E)logV) O(V) éè´Ÿæƒæœ€çŸ­è·¯ Bellman-Ford O(VE) O(V) æœ‰è´Ÿæƒã€æ£€æµ‹è´Ÿç¯ Kahnâ€™sæ‹“æ‰‘ O(V+E) O(V) DAGæ’åºã€ç¯æ£€æµ‹ å¹¶æŸ¥é›† O(EÎ±(V)) O(V) åŠ¨æ€è¿é€šæ€§ Floyd-Warshall O(VÂ³) O(VÂ²) å…¨æºæœ€çŸ­è·¯(å°å›¾) é¢è¯•/ç«èµ›ä¸­çš„é€‰æ‹©ä¼˜å…ˆçº§ï¼š çœ‹åˆ°ã€Œæœ€çŸ­ã€ã€Œæœ€å°‘æ­¥æ•°ã€ã€Œæœ€è¿‘ã€â†’ å…ˆæƒ³ BFS çœ‹åˆ°ã€Œæ‰€æœ‰è·¯å¾„ã€ã€Œæ’åˆ—ã€ã€Œç»„åˆã€â†’ å…ˆæƒ³ DFS + å›æº¯ çœ‹åˆ°ã€Œå…ˆä¿®è¯¾ã€ã€Œä¾èµ–å…³ç³»ã€ã€Œé¡ºåºã€â†’ å…ˆæƒ³æ‹“æ‰‘æ’åº çœ‹åˆ°ã€Œåˆ†ç»„ã€ã€Œä¸¤é˜Ÿã€ã€Œå¯¹ç«‹ã€â†’ å…ˆæƒ³äºŒåˆ†å›¾æŸ“è‰² çœ‹åˆ°ã€Œè¿é€šã€ã€Œå²›å±¿ã€ã€Œçœä»½ã€â†’ å…ˆæƒ³ DFS æˆ–å¹¶æŸ¥é›† çœ‹åˆ°ã€Œå¸¦æƒæœ€çŸ­è·¯ã€â†’ Dijkstraï¼ˆç¡®è®¤æ— è´Ÿæƒï¼‰ æ€»ç»“ï¼šå›¾è®ºçŸ¥è¯†åœ°å›¾ å›¾è®ºä¸æœç´¢ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚ è¡¨ç¤ºæ³• éå†ç®—æ³• ç»å…¸åº”ç”¨ â”‚ â”‚ â”‚ â”Œâ”€â”€â”€â”€â”¼â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”¼â”€â”€â”€â” â”Œâ”€â”€â”€â”¼â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”¬â”€â”€â”€â”€â” â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚é‚»æ¥ é‚»æ¥ è¾¹ DFS BFS æ‹“æ‰‘ è¿é€š äºŒåˆ† æœ€çŸ­çŸ©é˜µ è¡¨ åˆ—è¡¨ (æ ˆ) (é˜Ÿåˆ—) æ’åº åˆ†é‡ å›¾ è·¯å¾„ â”‚ â”‚ â”‚ â”‚ Kahn's DFS æŸ“è‰² Dijkstra DFSååº å¹¶æŸ¥é›† BFS(æ— æƒ) ä¸€å¥è¯æ€»ç»“å…¨æ–‡ï¼š å›¾è®ºçš„æ ¸å¿ƒå°±ä¸¤ä»¶äº‹â€”â€”æ€ä¹ˆå­˜å›¾ï¼ˆé‚»æ¥è¡¨ï¼‰ï¼Œæ€ä¹ˆéå†å›¾ï¼ˆDFS/BFSï¼‰ã€‚æ‰€æœ‰é«˜çº§åº”ç”¨éƒ½æ˜¯åœ¨éå†çš„åŸºç¡€ä¸ŠåŠ ç‚¹æ–™ã€‚ æœ¬æ–‡çº¦ 9300 å­—ï¼ŒåŒ…å« 11 å¼ å¯è§†åŒ–å›¾è§£ã€15 å¼  Anki é—ªè®°å¡ç‰‡ã€18 é“ LeetCode åˆ·é¢˜è·¯çº¿ã€‚å¦‚æœ‰é”™è¯¯æˆ–å»ºè®®ï¼Œæ¬¢è¿æŒ‡æ­£ã€‚","tags":["ç®—æ³•","LeetCode","BFS","å›¾è®º","DFS"],"categories":["ç®—æ³•ä¸æ•°æ®ç»“æ„"]},{"title":"é“¾è¡¨ä¸æŒ‡é’ˆå®Œå…¨å›¾è§£ï¼šä»å†…å­˜æœ¬è´¨åˆ° LeetCode å®æˆ˜","path":"/2026/02/22/linkedlist-pointer-complete-guide/","content":"è´¹æ›¼è¯´ï¼šå¦‚æœä½ ä¸èƒ½ç”¨ç®€å•çš„è¯­è¨€è§£é‡Šä¸€ä¸ªä¸œè¥¿ï¼Œè¯´æ˜ä½ è¿˜æ²¡çœŸæ­£ç†è§£å®ƒã€‚æœ¬æ–‡ç”¨ã€Œå†…å­˜è§†è§’ã€å¸¦ä½ ä»é›¶å»ºç«‹é“¾è¡¨ç›´è§‰ï¼Œé…åˆ 10+ å¼ åŸåˆ›å›¾è§£å’Œ LeetCode ç»å…¸é¢˜ï¼Œè®©ä½ å½»åº•ææ‡‚æŒ‡é’ˆåˆ°åº•åœ¨å¹²ä»€ä¹ˆã€‚ ä¸€ã€æŒ‡é’ˆçš„æœ¬è´¨ï¼šä¸€ä¸ªå­˜åœ°å€çš„å˜é‡1.1 å…ˆå¿˜æ‰â€æŒ‡é’ˆâ€è¿™ä¸ªè¯å¾ˆå¤šäººä¸€å¬â€æŒ‡é’ˆâ€å°±æ€•ã€‚å…¶å®æŒ‡é’ˆå°±æ˜¯ä¸€ä¸ªå­˜åœ°å€çš„å˜é‡ï¼Œä»…æ­¤è€Œå·²ã€‚ æƒ³è±¡ä½ ä½åœ¨ä¸€æ ‹å…¬å¯“æ¥¼é‡Œï¼š æ¯ä¸ªæˆ¿é—´æœ‰ä¸€ä¸ªé—¨ç‰Œå·ï¼ˆå†…å­˜åœ°å€ï¼‰ æˆ¿é—´é‡Œæ”¾ç€ä¸œè¥¿ï¼ˆæ•°æ®ï¼‰ ä½ æ‰‹é‡Œæœ‰ä¸€å¼ çº¸æ¡ï¼Œä¸Šé¢å†™ç€æŸä¸ªé—¨ç‰Œå· â€”â€” è¿™å¼ çº¸æ¡å°±æ˜¯æŒ‡é’ˆ çº¸æ¡ä¸Šå†™ç€: 0x1010ä½ å» 0x1010 æˆ¿é—´ä¸€çœ‹ï¼Œé‡Œé¢æ”¾ç€æ•°å­— 7 å°±è¿™ä¹ˆç®€å•ã€‚æŒ‡é’ˆä¸ç¥ç§˜ï¼Œå®ƒå°±æ˜¯ä¸€ä¸ªâ€å†™ç€åœ°å€çš„çº¸æ¡â€ã€‚ 1.2 å†…å­˜æ¨¡å‹å›¾è§£ çœ‹è¿™å¼ å›¾ï¼š int a = 42 ä½åœ¨åœ°å€ 0x1000ï¼Œæˆ¿é—´é‡Œæ”¾ç€ 42 int b = 7 ä½åœ¨åœ°å€ 0x1010ï¼Œæˆ¿é—´é‡Œæ”¾ç€ 7 int *p = b ä½åœ¨åœ°å€ 0x1008ï¼Œæˆ¿é—´é‡Œæ”¾ç€ 0x1010 â€”â€” ä¹Ÿå°±æ˜¯ b çš„åœ°å€ å½“ä½ å†™ *p çš„æ—¶å€™ï¼Œè®¡ç®—æœºåšäº†ä¸¤æ­¥ï¼š å…ˆçœ‹ p é‡Œå­˜çš„åœ°å€ï¼š0x1010 å» 0x1010 å–å€¼ï¼š7 è¿™å°±æ˜¯è§£å¼•ç”¨ï¼ˆdereferenceï¼‰ï¼Œæ²¡æœ‰ä»»ä½•é­”æ³•ã€‚ 1.3 ç”¨ C/Python æ„Ÿå—æŒ‡é’ˆC è¯­è¨€ç‰ˆï¼ˆæ˜¾å¼æŒ‡é’ˆï¼‰ï¼š int b = 7;int *p = b; // p å­˜çš„æ˜¯ b çš„åœ°å€printf(\"%d\", *p); // è¾“å‡º 7 â€”â€” é€šè¿‡åœ°å€æ‰¾åˆ°å€¼*p = 100; // é€šè¿‡åœ°å€ä¿®æ”¹å€¼printf(\"%d\", b); // è¾“å‡º 100 â€”â€” b è¢«æ”¹äº†ï¼ Python ç‰ˆï¼ˆéšå¼å¼•ç”¨ï¼‰ï¼š # Python é‡Œä¸€åˆ‡éƒ½æ˜¯å¼•ç”¨ï¼ˆæŒ‡é’ˆçš„é«˜çº§åŒ…è£…ï¼‰a = [1, 2, 3]b = a # b å’Œ a æŒ‡å‘åŒä¸€ä¸ªåˆ—è¡¨å¯¹è±¡b.append(4)print(a) # [1, 2, 3, 4] â€”â€” a ä¹Ÿå˜äº†ï¼å› ä¸º b å’Œ a æ˜¯åŒä¸€ä¸ªåœ°å€ è´¹æ›¼ç¬”è®°ï¼šæŒ‡é’ˆ = åœ°å€ã€‚*p = å»é‚£ä¸ªåœ°å€çœ‹çœ‹ã€‚x = å‘Šè¯‰æˆ‘ x çš„åœ°å€ã€‚ä¸‰å¥è¯è¯´å®Œäº†ã€‚ äºŒã€é“¾è¡¨èŠ‚ç‚¹ï¼šval + next çš„ç»„åˆæ‹³2.1 ä¸ºä»€ä¹ˆéœ€è¦é“¾è¡¨ï¼Ÿæ•°ç»„çš„é—®é¢˜ï¼š æ’å…¥/åˆ é™¤è¦æ¬ç§»å¤§é‡å…ƒç´ ï¼ŒO(n) å¤§å°å›ºå®šï¼ˆé™æ€æ•°ç»„ï¼‰æˆ–éœ€è¦é‡æ–°åˆ†é…ï¼ˆåŠ¨æ€æ•°ç»„ï¼‰ é“¾è¡¨çš„ä¼˜åŠ¿ï¼š æ’å…¥/åˆ é™¤åªéœ€è¦æ”¹å‡ ä¸ªæŒ‡é’ˆï¼ŒO(1) å¤§å°å®Œå…¨åŠ¨æ€ï¼Œç”¨å¤šå°‘åˆ†é…å¤šå°‘ ä»£ä»·ï¼š ä¸èƒ½éšæœºè®¿é—®ï¼ˆæ²¡æœ‰ arr[i]ï¼‰ï¼Œåªèƒ½ä»å¤´éå† æ¯ä¸ªèŠ‚ç‚¹å¤šå­˜ä¸€ä¸ªæŒ‡é’ˆï¼Œç©ºé—´å¼€é”€æ›´å¤§ 2.2 èŠ‚ç‚¹ç»“æ„å›¾è§£ æ¯ä¸ªèŠ‚ç‚¹å°±ä¸¤æ ·ä¸œè¥¿ï¼š valï¼šå­˜æ•°æ®ï¼ˆè“è‰²æ¡†ï¼‰ nextï¼šå­˜ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„åœ°å€ï¼ˆæ©™è‰²æ¡†ï¼‰ æœ€åä¸€ä¸ªèŠ‚ç‚¹çš„ next æŒ‡å‘ NULLï¼ˆç©ºï¼‰ï¼Œè¡¨ç¤ºé“¾è¡¨ç»“æŸã€‚ 2.3 ä»£ç å®šä¹‰Python ç‰ˆï¼š class ListNode: def __init__(self, val=0, next=None): self.val = val self.next = next# åˆ›å»ºé“¾è¡¨: 1 - 3 - 5 - 7node4 = ListNode(7)node3 = ListNode(5, node4)node2 = ListNode(3, node3)head = ListNode(1, node2) C++ ç‰ˆï¼š struct ListNode { int val; ListNode* next; ListNode(int x) : val(x), next(nullptr) {}}; 2.4 éå†ï¼šé“¾è¡¨çš„åŸºæœ¬åŠŸdef traverse(head): curr = head while curr: print(curr.val, end=\" - \") curr = curr.next print(\"NULL\")# è¾“å‡º: 1 - 3 - 5 - 7 - NULL æ•°æ®æµè¿½è¸ªï¼š Step 1: curr = head â†’ curr.val = 1, curr.next = node2Step 2: curr = curr.next â†’ curr.val = 3, curr.next = node3Step 3: curr = curr.next â†’ curr.val = 5, curr.next = node4Step 4: curr = curr.next â†’ curr.val = 7, curr.next = NoneStep 5: curr = None â†’ å¾ªç¯ç»“æŸ è´¹æ›¼ç¬”è®°ï¼šéå†é“¾è¡¨å°±åƒå¯»å®æ¸¸æˆ â€”â€” æ¯ä¸ªå®ç®±é‡Œæœ‰ä¸€ä¸ªå®ç‰©ï¼ˆvalï¼‰å’Œä¸€å¼ çº¸æ¡å‘Šè¯‰ä½ ä¸‹ä¸€ä¸ªå®ç®±åœ¨å“ªï¼ˆnextï¼‰ã€‚çº¸æ¡å†™ç€â€æ²¡äº†â€ï¼ˆNULLï¼‰å°±ç»“æŸã€‚ ä¸‰ã€é“¾è¡¨çš„æ ¸å¿ƒæ“ä½œï¼šæ’å…¥ä¸åˆ é™¤3.1 æ’å…¥èŠ‚ç‚¹ï¼šå…ˆæ¥åæ–­æ’å…¥çš„æ ¸å¿ƒåŸåˆ™ï¼šå…ˆè®©æ–°èŠ‚ç‚¹æ¥ä¸Šåé¢ï¼Œå†è®©å‰é¢æ¥ä¸Šæ–°èŠ‚ç‚¹ã€‚é¡ºåºåäº†å°±æ–­é“¾äº†ã€‚ åœ¨ node(3) åé¢æ’å…¥ node(9)ï¼š def insert_after(prev_node, new_val): new_node = ListNode(new_val) new_node.next = prev_node.next # Step 1: æ–°èŠ‚ç‚¹å…ˆæ¥ä¸Šåé¢ prev_node.next = new_node # Step 2: å‰é¢å†æ¥ä¸Šæ–°èŠ‚ç‚¹ æ•°æ®æµè¿½è¸ªï¼ˆæ’å…¥ 9 åˆ° 3 åé¢ï¼‰ï¼š åˆå§‹çŠ¶æ€: 1 - 3 - 5 - 7 - NULL â†‘ prev_nodeStep 1: new_node(9).next = prev_node.next â†’ new_node(9).next = node(5) æ­¤æ—¶: 1 - 3 - 5 - 7 - NULL â†‘ new(9) ---â”˜Step 2: prev_node.next = new_node â†’ node(3).next = node(9) æ­¤æ—¶: 1 - 3 - 9 - 5 - 7 - NULL âœ“ âš ï¸ ç»å…¸é”™è¯¯ï¼šå¦‚æœå…ˆæ‰§è¡Œ Step 2ï¼Œprev_node.next å°±è¢«è¦†ç›–äº†ï¼Œä½ å†ä¹Ÿæ‰¾ä¸åˆ°åŸæ¥çš„ node(5)ï¼Œé“¾è¡¨æ–­è£‚ï¼ 3.2 åˆ é™¤èŠ‚ç‚¹ï¼šè·³è¿‡å®ƒåˆ é™¤ä¸éœ€è¦çœŸçš„â€é”€æ¯â€èŠ‚ç‚¹ï¼Œåªéœ€è¦è®©å‰ä¸€ä¸ªèŠ‚ç‚¹è·³è¿‡å®ƒã€‚ def delete_next(prev_node): if prev_node.next: prev_node.next = prev_node.next.next # ç›´æ¥è·³è¿‡ä¸‹ä¸€ä¸ªèŠ‚ç‚¹ æ•°æ®æµè¿½è¸ªï¼ˆåˆ é™¤ 3ï¼‰ï¼š åˆå§‹: 1 - 3 - 5 - 7 - NULL â†‘prevprev.next = prev.next.nextâ†’ node(1).next = node(5)ç»“æœ: 1 - 5 - 7 - NULL node(3) è¢«è·³è¿‡ï¼Œæ— äººå¼•ç”¨ï¼Œç­‰å¾…åƒåœ¾å›æ”¶ 3.3 æ—¶é—´å¤æ‚åº¦å¯¹æ¯” æ“ä½œ æ•°ç»„ é“¾è¡¨ æŒ‰ç´¢å¼•è®¿é—® O(1) O(n) å¤´éƒ¨æ’å…¥ O(n) O(1) ä¸­é—´æ’å…¥ï¼ˆå·²çŸ¥ä½ç½®ï¼‰ O(n) O(1) å°¾éƒ¨æ’å…¥ O(1)* O(n)** åˆ é™¤ï¼ˆå·²çŸ¥ä½ç½®ï¼‰ O(n) O(1) *æ‘Šè¿˜å¤æ‚åº¦ **éœ€è¦éå†åˆ°å°¾éƒ¨ è´¹æ›¼ç¬”è®°ï¼šæ’å…¥ = å…ˆæ¥åæ–­ï¼ˆé¡ºåºä¸èƒ½åï¼‰ã€‚åˆ é™¤ = è®©å‰ä¸€ä¸ªç›´æ¥æŒ‡å‘åä¸€ä¸ªï¼ˆè·³è¿‡ä¸­é—´äººï¼‰ã€‚æ‰€æœ‰é“¾è¡¨æ“ä½œçš„æœ¬è´¨éƒ½æ˜¯åœ¨æ”¹ next æŒ‡é’ˆã€‚ å››ã€è™šæ‹Ÿå¤´èŠ‚ç‚¹ï¼ˆDummy Headï¼‰ï¼šæ¶ˆç­ç‰¹æ®Šæƒ…å†µ4.1 ç—›ç‚¹ï¼šå¤´èŠ‚ç‚¹çš„ç‰¹æ®Šå¤„ç†åˆ é™¤é“¾è¡¨ä¸­å€¼ä¸º x çš„èŠ‚ç‚¹æ—¶ï¼Œå¦‚æœè¦åˆ çš„æ°å¥½æ˜¯å¤´èŠ‚ç‚¹æ€ä¹ˆåŠï¼Ÿ # æ²¡æœ‰ dummy head çš„å†™æ³• â€”â€” ä¸‘é™‹ä¸”æ˜“é”™def remove_val_ugly(head, val): # ç‰¹æ®Šå¤„ç†ï¼šå¤´èŠ‚ç‚¹å°±æ˜¯ç›®æ ‡ while head and head.val == val: head = head.next # å¸¸è§„å¤„ç†ï¼šä¸­é—´èŠ‚ç‚¹ curr = head while curr and curr.next: if curr.next.val == val: curr.next = curr.next.next else: curr = curr.next return head ä¸¤æ®µé€»è¾‘ï¼Œä¸¤ç§å¤„ç†æ–¹å¼ï¼Œå®¹æ˜“å‡º bugã€‚ 4.2 è§£æ³•ï¼šåŠ ä¸€ä¸ªå‡çš„å¤´ åœ¨çœŸæ­£çš„å¤´èŠ‚ç‚¹å‰é¢åŠ ä¸€ä¸ª dummy èŠ‚ç‚¹ï¼Œè¿™æ ·æ‰€æœ‰èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬åŸæ¥çš„å¤´ï¼‰éƒ½æœ‰â€å‰ä¸€ä¸ªèŠ‚ç‚¹â€ï¼Œé€»è¾‘ç»Ÿä¸€ã€‚ # æœ‰ dummy head çš„å†™æ³• â€”â€” å¹²å‡€ç»Ÿä¸€def remove_val_clean(head, val): dummy = ListNode(0) dummy.next = head curr = dummy while curr.next: if curr.next.val == val: curr.next = curr.next.next # ç»Ÿä¸€çš„åˆ é™¤é€»è¾‘ else: curr = curr.next return dummy.next # è¿”å›çœŸæ­£çš„å¤´ è´¹æ›¼ç¬”è®°ï¼šDummy head å°±åƒåœ¨é˜Ÿä¼æœ€å‰é¢æ”¾ä¸€ä¸ªâ€å ä½çš„äººâ€ã€‚æœ‰äº†å®ƒï¼Œä¸ç®¡åˆ è°ï¼Œé€»è¾‘éƒ½ä¸€æ ·ï¼šprev.next = prev.next.nextã€‚è¿™æ˜¯é“¾è¡¨é¢˜çš„ç¬¬ä¸€ä¸ªå¿…å¤‡æŠ€å·§ã€‚ äº”ã€åŒæŒ‡é’ˆæŠ€å·§ï¼šé“¾è¡¨çš„ç‘å£«å†›åˆ€åŒæŒ‡é’ˆæ˜¯é“¾è¡¨é¢˜çš„æ ¸å¿ƒæ­¦å™¨ï¼Œåˆ†ä¸‰ç§ç”¨æ³•ï¼š 5.1 å¿«æ…¢æŒ‡é’ˆï¼šæ‰¾ä¸­ç‚¹é—®é¢˜ï¼šä¸çŸ¥é“é“¾è¡¨é•¿åº¦ï¼Œå¦‚ä½•ä¸€æ¬¡éå†æ‰¾åˆ°ä¸­é—´èŠ‚ç‚¹ï¼Ÿ ç›´è§‰ï¼šä¸¤ä¸ªäººåŒæ—¶ä»èµ·ç‚¹å‡ºå‘ï¼Œä¸€ä¸ªèµ°ä¸€æ­¥ï¼Œä¸€ä¸ªèµ°ä¸¤æ­¥ã€‚å¿«çš„åˆ°ç»ˆç‚¹æ—¶ï¼Œæ…¢çš„åˆšå¥½åœ¨ä¸­é—´ã€‚ def find_middle(head): slow = fast = head while fast and fast.next: slow = slow.next # æ…¢æŒ‡é’ˆèµ° 1 æ­¥ fast = fast.next.next # å¿«æŒ‡é’ˆèµ° 2 æ­¥ return slow # slow å°±æ˜¯ä¸­ç‚¹# æ•°æ®æµè¿½è¸ª: 1 - 2 - 3 - 4 - 5# Step 0: slow=1, fast=1# Step 1: slow=2, fast=3# Step 2: slow=3, fast=5# fast.next=None, åœï¼slow=3 å°±æ˜¯ä¸­ç‚¹ âœ“ åº”ç”¨åœºæ™¯ï¼š LeetCode 876ï¼šé“¾è¡¨çš„ä¸­é—´èŠ‚ç‚¹ å½’å¹¶æ’åºé“¾è¡¨çš„åˆ†å‰²æ­¥éª¤ åˆ¤æ–­å›æ–‡é“¾è¡¨çš„å‰åŠæ®µ 5.2 é—´è·æŒ‡é’ˆï¼šåˆ é™¤å€’æ•°ç¬¬ N ä¸ªèŠ‚ç‚¹ï¼ˆLeetCode 19ï¼‰é—®é¢˜ï¼šä¸€æ¬¡éå†åˆ é™¤å€’æ•°ç¬¬ N ä¸ªèŠ‚ç‚¹ã€‚ ç›´è§‰ï¼šè®© fast å…ˆèµ° N æ­¥ï¼Œç„¶å slow å’Œ fast ä¸€èµ·èµ°ã€‚fast åˆ°ç»ˆç‚¹æ—¶ï¼Œslow åˆšå¥½åœ¨å€’æ•°ç¬¬ N+1 ä¸ªä½ç½®ã€‚ def removeNthFromEnd(head, n): dummy = ListNode(0) dummy.next = head slow = fast = dummy # fast å…ˆèµ° n+1 æ­¥ for _ in range(n + 1): fast = fast.next # ä¸€èµ·èµ°åˆ°åº• while fast: slow = slow.next fast = fast.next # slow.next å°±æ˜¯è¦åˆ çš„èŠ‚ç‚¹ slow.next = slow.next.next return dummy.next æ•°æ®æµè¿½è¸ªï¼ˆåˆ é™¤å€’æ•°ç¬¬ 2 ä¸ªï¼Œå³ node(4)ï¼‰ï¼š é“¾è¡¨: D - 1 - 2 - 3 - 4 - 5 - NULLn=2Phase 1: fast å…ˆèµ° 3 æ­¥ (n+1) fast = D - 1 - 2 - 3 slow = DPhase 2: ä¸€èµ·èµ° Step 1: slow=1, fast=4 Step 2: slow=2, fast=5 Step 3: slow=3, fast=NULL â†’ åœï¼Delete: slow.next = slow.next.next â†’ node(3).next = node(5) ç»“æœ: 1 - 2 - 3 - 5 - NULL âœ“ ä¸ºä»€ä¹ˆèµ° n+1 æ­¥è€Œä¸æ˜¯ n æ­¥ï¼Ÿ å› ä¸ºæˆ‘ä»¬éœ€è¦åœåœ¨ç›®æ ‡èŠ‚ç‚¹çš„å‰ä¸€ä¸ªï¼Œæ‰èƒ½æ‰§è¡Œ prev.next = prev.next.nextã€‚ å…­ã€åè½¬é“¾è¡¨ï¼šé“¾è¡¨é¢˜çš„çµé­‚ï¼ˆLeetCode 206ï¼‰åè½¬é“¾è¡¨æ˜¯é“¾è¡¨é¢˜çš„ç»å¯¹æ ¸å¿ƒï¼Œè‡³å°‘æœ‰ 10 é“ LeetCode é¢˜æ˜¯å®ƒçš„å˜ä½“ã€‚ä¸æŒæ¡è¿™ä¸ªï¼Œåé¢çš„é¢˜å…¨å¡ä½ã€‚ 6.1 ç›´è§‰ï¼šé€ä¸ªæ°æ–¹å‘æƒ³è±¡ä¸€åˆ—ç«è½¦è½¦å¢ï¼Œæ¯èŠ‚è½¦å¢çš„æŒ‚é’©éƒ½æœå³ã€‚åè½¬å°±æ˜¯æŠŠæ¯ä¸ªæŒ‚é’©æ°å‘å·¦è¾¹ã€‚ 6.2 è¿­ä»£æ³•ï¼ˆæ¨èæŒæ¡ï¼‰æ ¸å¿ƒæ€è·¯ï¼šç”¨ä¸‰ä¸ªæŒ‡é’ˆ prevã€currã€next_tempï¼Œé€ä¸ªç¿»è½¬ã€‚ def reverseList(head): prev = None curr = head while curr: next_temp = curr.next # 1. å…ˆå­˜ä¸‹ä¸€ä¸ªï¼ˆä¸ç„¶æ–­äº†æ‰¾ä¸åˆ°ï¼‰ curr.next = prev # 2. æ°æ–¹å‘ï¼šå½“å‰èŠ‚ç‚¹æŒ‡å‘å‰é¢ prev = curr # 3. prev å‰è¿› curr = next_temp # 4. curr å‰è¿› return prev # prev å°±æ˜¯æ–°çš„å¤´ é€æ­¥æ•°æ®æµè¿½è¸ªï¼š åˆå§‹: prev=NULL, curr=1-2-3-NULL=== ç¬¬ 1 è½® ===next_temp = 2 (ä¿å­˜)curr(1).next = NULL (prev) â†’ 1-NULLprev = 1curr = 2çŠ¶æ€: NULL-1 2-3-NULL prevâ†‘ currâ†‘=== ç¬¬ 2 è½® ===next_temp = 3curr(2).next = 1 (prev) â†’ 2-1-NULLprev = 2curr = 3çŠ¶æ€: NULL-1-2 3-NULL prevâ†‘ currâ†‘=== ç¬¬ 3 è½® ===next_temp = NULLcurr(3).next = 2 (prev) â†’ 3-2-1-NULLprev = 3curr = NULLçŠ¶æ€: NULL-1-2-3 prevâ†‘ curr=NULL â†’ ç»“æŸè¿”å› prev = node(3)ï¼Œå³æ–°å¤´ç»“æœ: 3 - 2 - 1 - NULL âœ“ 6.3 é€’å½’æ³•ï¼ˆç†è§£å³å¯ï¼‰def reverseList_recursive(head): # base case: ç©ºé“¾è¡¨æˆ–åªæœ‰ä¸€ä¸ªèŠ‚ç‚¹ if not head or not head.next: return head # é€’å½’åè½¬åé¢çš„éƒ¨åˆ† new_head = reverseList_recursive(head.next) # æŠŠä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„ next æŒ‡å›è‡ªå·± head.next.next = head head.next = None return new_head é€’å½’å±•å¼€è¿½è¸ªï¼ˆ1-2-3-NULLï¼‰ï¼š è°ƒç”¨æ ˆå±•å¼€: reverse(1) â†’ reverse(2) â†’ reverse(3) â†’ return 3 (base case)å›æº¯: reverse(2): head=2, new_head=3 head.next.next = head â†’ node(3).next = node(2) â†’ 3-2 head.next = None â†’ node(2).next = None â†’ 3-2-NULL return 3 reverse(1): head=1, new_head=3 head.next.next = head â†’ node(2).next = node(1) â†’ 3-2-1 head.next = None â†’ node(1).next = None â†’ 3-2-1-NULL return 3æœ€ç»ˆ: 3-2-1-NULL âœ“ è´¹æ›¼ç¬”è®°ï¼šåè½¬é“¾è¡¨çš„æœ¬è´¨å°±æ˜¯ä¸‰æ­¥èˆ â€”â€” å­˜ã€æ°ã€èµ°ã€‚next_temp å­˜åè·¯ï¼Œcurr.next = prev æ°æ–¹å‘ï¼Œç„¶åä¸¤ä¸ªæŒ‡é’ˆä¸€èµ·å¾€å‰èµ°ã€‚è®°ä½è¿™ä¸ªèŠ‚å¥ï¼Œæ‰€æœ‰åè½¬å˜ä½“éƒ½æ˜¯å®ƒçš„å»¶ä¼¸ã€‚ ä¸ƒã€ç¯å½¢é“¾è¡¨ï¼šå¿«æ…¢æŒ‡é’ˆçš„ç»å…¸æˆ˜åœºï¼ˆLeetCode 141 / 142ï¼‰7.1 é—®é¢˜ï¼šé“¾è¡¨æœ‰æ²¡æœ‰ç¯ï¼Ÿå¦‚æœé“¾è¡¨çš„æŸä¸ªèŠ‚ç‚¹çš„ next æŒ‡å‘äº†å‰é¢çš„æŸä¸ªèŠ‚ç‚¹ï¼Œå°±å½¢æˆäº†ç¯ã€‚éå†æ°¸è¿œä¸ä¼šåˆ° NULLï¼Œç¨‹åºä¼šæ­»å¾ªç¯ã€‚ 7.2 Floyd åˆ¤åœˆç®—æ³•ï¼ˆé¾Ÿå…”èµ›è·‘ï¼‰ç›´è§‰ï¼šæ“åœºè·‘æ­¥ï¼Œå¿«çš„äººï¼ˆfastï¼‰å’Œæ…¢çš„äººï¼ˆslowï¼‰åŒæ—¶å‡ºå‘ã€‚å¦‚æœæ“åœºæ˜¯ç¯å½¢çš„ï¼Œå¿«çš„äººä¸€å®šä¼šä»åé¢è¿½ä¸Šæ…¢çš„äººã€‚å¦‚æœæ“åœºæ˜¯ç›´çº¿çš„ï¼Œå¿«çš„äººç›´æ¥è·‘åˆ°ç»ˆç‚¹ï¼Œæ°¸è¿œä¸ä¼šç›¸é‡ã€‚ # LeetCode 141: åˆ¤æ–­æ˜¯å¦æœ‰ç¯def hasCycle(head): slow = fast = head while fast and fast.next: slow = slow.next fast = fast.next.next if slow == fast: return True # ç›¸é‡äº† â†’ æœ‰ç¯ return False # fast åˆ°ç»ˆç‚¹äº† â†’ æ— ç¯ æ•°æ®æµè¿½è¸ªï¼ˆæœ‰ç¯ï¼š1-2-3-4-5-3ï¼‰ï¼š Step 0: slow=1, fast=1Step 1: slow=2, fast=3Step 2: slow=3, fast=5Step 3: slow=4, fast=4 â† ç›¸é‡ï¼æœ‰ç¯ âœ“ 7.3 è¿›é˜¶ï¼šæ‰¾ç¯çš„å…¥å£ï¼ˆLeetCode 142ï¼‰ä¸ä»…è¦åˆ¤æ–­æœ‰æ²¡æœ‰ç¯ï¼Œè¿˜è¦æ‰¾åˆ°ç¯ä»å“ªé‡Œå¼€å§‹ã€‚ æ•°å­¦æ¨å¯¼ï¼ˆç®€åŒ–ç‰ˆï¼‰ï¼š è®¾ï¼š èµ·ç‚¹åˆ°ç¯å…¥å£è·ç¦» = a ç¯å…¥å£åˆ°ç›¸é‡ç‚¹è·ç¦» = b ç¯çš„é•¿åº¦ = c ç›¸é‡æ—¶ï¼š slow èµ°äº† a + b æ­¥ fast èµ°äº† a + b + n*c æ­¥ï¼ˆå¤šç»•äº† n åœˆï¼‰ fast é€Ÿåº¦æ˜¯ slow çš„ 2 å€ï¼š2(a+b) = a + b + n*c åŒ–ç®€ï¼ša = n*c - b = (n-1)*c + (c-b) ç»“è®ºï¼šä»èµ·ç‚¹å’Œç›¸é‡ç‚¹åŒæ—¶å‡ºå‘ï¼Œå„èµ°ä¸€æ­¥ï¼Œå®ƒä»¬ä¼šåœ¨ç¯å…¥å£ç›¸é‡ï¼ # LeetCode 142: æ‰¾ç¯çš„å…¥å£def detectCycle(head): slow = fast = head # Phase 1: æ‰¾ç›¸é‡ç‚¹ while fast and fast.next: slow = slow.next fast = fast.next.next if slow == fast: break else: return None # æ— ç¯ # Phase 2: æ‰¾å…¥å£ slow = head # slow å›åˆ°èµ·ç‚¹ while slow != fast: slow = slow.next fast = fast.next # æ³¨æ„ï¼šè¿™é‡Œ fast ä¹Ÿåªèµ°ä¸€æ­¥ï¼ return slow # ç›¸é‡ç‚¹å°±æ˜¯ç¯å…¥å£ æ•°æ®æµè¿½è¸ªï¼ˆ1-2-3-4-5-3ï¼Œç¯å…¥å£=3ï¼‰ï¼š Phase 1 (æ‰¾ç›¸é‡ç‚¹): slow: 1â†’2â†’3â†’4 fast: 1â†’3â†’5â†’4 ç›¸é‡äº node(4)Phase 2 (æ‰¾å…¥å£): slow ä» head(1) å‡ºå‘, fast ä» node(4) å‡ºå‘ Step 1: slow=2, fast=5 Step 2: slow=3, fast=3 â† ç›¸é‡ï¼ç¯å…¥å£ = node(3) âœ“ è´¹æ›¼ç¬”è®°ï¼šåˆ¤ç¯ = é¾Ÿå…”èµ›è·‘ï¼Œè¿½ä¸Šäº†å°±æœ‰ç¯ã€‚æ‰¾å…¥å£ = æ•°å­¦é­”æœ¯ï¼Œä»èµ·ç‚¹å’Œç›¸é‡ç‚¹åŒé€Ÿå‡ºå‘ï¼Œå†æ¬¡ç›¸é‡å°±æ˜¯å…¥å£ã€‚è®°ä½è¿™ä¸¤ä¸ª phase å°±å¤Ÿäº†ã€‚ å…«ã€åˆå¹¶ä¸¤ä¸ªæœ‰åºé“¾è¡¨ï¼ˆLeetCode 21ï¼‰8.1 é—®é¢˜ç»™ä½ ä¸¤ä¸ªå‡åºé“¾è¡¨ï¼Œåˆå¹¶æˆä¸€ä¸ªå‡åºé“¾è¡¨ã€‚ 8.2 æ€è·¯ï¼šæ‹‰æ‹‰é“¾æƒ³è±¡ä½ æ‰‹é‡Œæœ‰ä¸¤å‰¯æ‰‘å…‹ç‰Œï¼Œéƒ½å·²ç»ä»å°åˆ°å¤§æ’å¥½äº†ã€‚ä½ è¦æŠŠå®ƒä»¬åˆæˆä¸€å‰¯ï¼š æ¯”è¾ƒä¸¤å‰¯ç‰Œé¡¶éƒ¨çš„ç‰Œ å°çš„é‚£å¼ æŠ½å‡ºæ¥æ”¾åˆ°æ–°ç‰Œå † é‡å¤ï¼Œç›´åˆ°æŸä¸€å‰¯ç”¨å®Œ æŠŠå‰©ä¸‹çš„é‚£å‰¯ç›´æ¥æ¥ä¸Š def mergeTwoLists(l1, l2): dummy = ListNode(0) curr = dummy while l1 and l2: if l1.val = l2.val: curr.next = l1 l1 = l1.next else: curr.next = l2 l2 = l2.next curr = curr.next curr.next = l1 or l2 # æ¥ä¸Šå‰©ä½™éƒ¨åˆ† return dummy.next æ•°æ®æµè¿½è¸ªï¼ˆL1: 1-3-5, L2: 2-4-6ï¼‰ï¼š åˆå§‹: dummy - ? l1=1, l2=2Step 1: 1 2 â†’ pick 1 dummy-1 l1=3, l2=2Step 2: 3 2 â†’ pick 2 dummy-1-2 l1=3, l2=4Step 3: 3 4 â†’ pick 3 dummy-1-2-3 l1=5, l2=4Step 4: 5 4 â†’ pick 4 dummy-1-2-3-4 l1=5, l2=6Step 5: 5 6 â†’ pick 5 dummy-1-2-3-4-5 l1=NULL, l2=6Step 6: l1=NULL â†’ æ¥ä¸Š l2 dummy-1-2-3-4-5-6ç»“æœ: 1-2-3-4-5-6 âœ“ 8.3 å¤æ‚åº¦ æ—¶é—´ï¼šO(m+n)ï¼Œæ¯ä¸ªèŠ‚ç‚¹æ°å¥½è¢«è®¿é—®ä¸€æ¬¡ ç©ºé—´ï¼šO(1)ï¼Œåªç”¨äº†å‡ ä¸ªæŒ‡é’ˆï¼ˆä¸ç®— dummyï¼‰ è´¹æ›¼ç¬”è®°ï¼šåˆå¹¶æœ‰åºé“¾è¡¨ = æ‹‰æ‹‰é“¾ã€‚ä¸¤è¾¹æ¯”å¤§å°ï¼Œå°çš„å…ˆæ¥ä¸Šã€‚ç”¨å®Œä¸€è¾¹ï¼Œå¦ä¸€è¾¹ç›´æ¥ç²˜ä¸Šå»ã€‚dummy head è®©ä½ ä¸ç”¨æ“å¿ƒâ€ç¬¬ä¸€ä¸ªèŠ‚ç‚¹æ¥åˆ°å“ªâ€ã€‚ ä¹ã€LeetCode å®æˆ˜è¿›é˜¶9.1 å›æ–‡é“¾è¡¨ï¼ˆLeetCode 234ï¼‰é—®é¢˜ï¼šåˆ¤æ–­é“¾è¡¨æ˜¯å¦æ˜¯å›æ–‡ï¼ˆæ­£è¯»åè¯»ä¸€æ ·ï¼‰ã€‚ æ€è·¯ï¼šä¸‰æ­¥èµ° â€”â€” æ‰¾ä¸­ç‚¹ â†’ åè½¬ååŠæ®µ â†’ é€ä¸€æ¯”è¾ƒã€‚ def isPalindrome(head): # Step 1: å¿«æ…¢æŒ‡é’ˆæ‰¾ä¸­ç‚¹ slow = fast = head while fast and fast.next: slow = slow.next fast = fast.next.next # Step 2: åè½¬ååŠæ®µ prev = None curr = slow while curr: next_temp = curr.next curr.next = prev prev = curr curr = next_temp # Step 3: æ¯”è¾ƒå‰åŠæ®µå’Œåè½¬åçš„ååŠæ®µ left, right = head, prev while right: if left.val != right.val: return False left = left.next right = right.next return True æ•°æ®æµè¿½è¸ªï¼ˆ1-2-3-2-1ï¼‰ï¼š Step 1 æ‰¾ä¸­ç‚¹: slow: 1â†’2â†’3 fast: 1â†’3â†’NULL ä¸­ç‚¹ = 3Step 2 åè½¬ååŠæ®µ (3-2-1): åè½¬å: 1-2-3Step 3 æ¯”è¾ƒ: left=1, right=1 â†’ ç›¸ç­‰ âœ“ left=2, right=2 â†’ ç›¸ç­‰ âœ“ left=3, right=3 â†’ ç›¸ç­‰ âœ“ right=NULL â†’ ç»“æŸï¼Œæ˜¯å›æ–‡ âœ“ è¿™é“é¢˜æŠŠå‰é¢å­¦çš„å¿«æ…¢æŒ‡é’ˆå’Œåè½¬é“¾è¡¨ç»„åˆèµ·æ¥äº†ã€‚é“¾è¡¨é¢˜å°±æ˜¯è¿™æ ·ï¼ŒåŸºç¡€æŠ€å·§çš„æ’åˆ—ç»„åˆã€‚ 9.2 ç›¸äº¤é“¾è¡¨ï¼ˆLeetCode 160ï¼‰é—®é¢˜ï¼šä¸¤ä¸ªé“¾è¡¨å¯èƒ½åœ¨æŸä¸ªèŠ‚ç‚¹æ±‡åˆï¼Œæ‰¾åˆ°é‚£ä¸ªäº¤ç‚¹ã€‚ ç›´è§‰ï¼šä¸¤ä¸ªäººèµ°ä¸åŒçš„è·¯ï¼Œè·¯çš„ç»ˆç‚¹ç›¸åŒã€‚å¦‚æœ A èµ°å®Œè‡ªå·±çš„è·¯å†èµ° B çš„è·¯ï¼ŒB èµ°å®Œè‡ªå·±çš„è·¯å†èµ° A çš„è·¯ï¼Œå®ƒä»¬èµ°çš„æ€»è·ç¦»ç›¸åŒï¼Œä¸€å®šä¼šåœ¨äº¤ç‚¹ç›¸é‡ã€‚ def getIntersectionNode(headA, headB): a, b = headA, headB while a != b: a = a.next if a else headB # a èµ°å®Œå°±å» B çš„èµ·ç‚¹ b = b.next if b else headA # b èµ°å®Œå°±å» A çš„èµ·ç‚¹ return a # ç›¸é‡ç‚¹å°±æ˜¯äº¤ç‚¹ï¼ˆæˆ–è€…éƒ½æ˜¯ None = æ— äº¤ç‚¹ï¼‰ ä¸ºä»€ä¹ˆæœ‰æ•ˆï¼Ÿ A çš„è·¯: a1 â†’ a2 â†’ c1 â†’ c2 â†’ c3 (ç‹¬æœ‰2 + å…¬å…±3 = 5)B çš„è·¯: b1 â†’ b2 â†’ b3 â†’ c1 â†’ c2 â†’ c3 (ç‹¬æœ‰3 + å…¬å…±3 = 6)A èµ°å®Œèµ° B: a1â†’a2â†’c1â†’c2â†’c3â†’b1â†’b2â†’b3â†’c1 (5+3=8æ­¥åˆ°c1)B èµ°å®Œèµ° A: b1â†’b2â†’b3â†’c1â†’c2â†’c3â†’a1â†’a2â†’c1 (6+2=8æ­¥åˆ°c1)ä¸¤äººéƒ½åœ¨ç¬¬ 8 æ­¥åˆ°è¾¾ c1 â†’ ç›¸é‡ï¼ è´¹æ›¼ç¬”è®°ï¼šæµªæ¼«ç®—æ³• â€”â€” ä½ èµ°è¿‡æˆ‘æ¥æ—¶çš„è·¯ï¼Œæˆ‘èµ°è¿‡ä½ æ¥æ—¶çš„è·¯ï¼Œæˆ‘ä»¬ç»ˆå°†åœ¨äº¤æ±‡å¤„ç›¸é‡ã€‚æ•°å­¦ä¸Šå°±æ˜¯ a + c + b = b + c + aã€‚ 9.3 ä¸¤æ•°ç›¸åŠ ï¼ˆLeetCode 2ï¼‰é—®é¢˜ï¼šä¸¤ä¸ªé“¾è¡¨è¡¨ç¤ºä¸¤ä¸ªæ•°ï¼ˆé€†åºå­˜å‚¨ï¼‰ï¼Œæ±‚å®ƒä»¬çš„å’Œã€‚ è¾“å…¥: (2-4-3) + (5-6-4)ä»£è¡¨: 342 + 465 = 807è¾“å‡º: 7-0-8 def addTwoNumbers(l1, l2): dummy = ListNode(0) curr = dummy carry = 0 while l1 or l2 or carry: val = carry if l1: val += l1.val l1 = l1.next if l2: val += l2.val l2 = l2.next carry = val // 10 curr.next = ListNode(val % 10) curr = curr.next return dummy.next æ•°æ®æµè¿½è¸ªï¼š Step 1: 2+5+0=7, carry=0 â†’ åˆ›å»º node(7)Step 2: 4+6+0=10, carry=1 â†’ åˆ›å»º node(0)Step 3: 3+4+1=8, carry=0 â†’ åˆ›å»º node(8)Step 4: å…¨éƒ¨ä¸ºç©ºä¸” carry=0 â†’ ç»“æŸç»“æœ: 7-0-8 âœ“ (å³ 807) åã€è¿›é˜¶ï¼šæ’åºé“¾è¡¨ï¼ˆLeetCode 148ï¼‰10.1 é—®é¢˜å¯¹é“¾è¡¨è¿›è¡Œæ’åºï¼Œè¦æ±‚ O(n log n) æ—¶é—´å¤æ‚åº¦ã€‚ æ€è·¯ï¼šå½’å¹¶æ’åºå¤©ç„¶é€‚åˆé“¾è¡¨ â€”â€” æ‰¾ä¸­ç‚¹åˆ†å‰² + é€’å½’æ’åº + åˆå¹¶æœ‰åºé“¾è¡¨ã€‚å‰é¢å­¦çš„æŠ€å·§å…¨ç”¨ä¸Šäº†ã€‚ def sortList(head): # base case if not head or not head.next: return head # Step 1: å¿«æ…¢æŒ‡é’ˆæ‰¾ä¸­ç‚¹ï¼Œæ–­å¼€ slow, fast = head, head.next while fast and fast.next: slow = slow.next fast = fast.next.next mid = slow.next slow.next = None # æ–­å¼€ï¼ # Step 2: é€’å½’æ’åºä¸¤åŠ left = sortList(head) right = sortList(mid) # Step 3: åˆå¹¶ï¼ˆå¤ç”¨ mergeTwoListsï¼‰ dummy = ListNode(0) curr = dummy while left and right: if left.val = right.val: curr.next = left left = left.next else: curr.next = right right = right.next curr = curr.next curr.next = left or right return dummy.next é€’å½’åˆ†è§£è¿½è¸ªï¼ˆ4-2-1-3ï¼‰ï¼š sortList(4-2-1-3)â”œâ”€â”€ åˆ†å‰²: [4-2] å’Œ [1-3]â”‚ â”œâ”€â”€ sortList(4-2)â”‚ â”‚ â”œâ”€â”€ åˆ†å‰²: [4] å’Œ [2]â”‚ â”‚ â””â”€â”€ åˆå¹¶: 2-4â”‚ â””â”€â”€ sortList(1-3)â”‚ â”œâ”€â”€ åˆ†å‰²: [1] å’Œ [3]â”‚ â””â”€â”€ åˆå¹¶: 1-3â””â”€â”€ åˆå¹¶ [2-4] å’Œ [1-3]: 1-2-3-4 âœ“ è´¹æ›¼ç¬”è®°ï¼šé“¾è¡¨æ’åº = æ‰¾ä¸­ç‚¹ï¼ˆå¿«æ…¢æŒ‡é’ˆï¼‰+ åˆ†ä¸¤åŠï¼ˆæ–­å¼€ï¼‰+ å„è‡ªæ’åºï¼ˆé€’å½’ï¼‰+ åˆå¹¶ï¼ˆæ‹‰æ‹‰é“¾ï¼‰ã€‚å››ä¸ªåŸºç¡€æ“ä½œçš„ç»„åˆæ‹³ï¼Œæ²¡æœ‰æ–°ä¸œè¥¿ã€‚ åä¸€ã€K ä¸ªä¸€ç»„ç¿»è½¬é“¾è¡¨ï¼ˆLeetCode 25ï¼‰â€”â€” é“¾è¡¨ç»ˆæ Boss11.1 é—®é¢˜æ¯ k ä¸ªèŠ‚ç‚¹ä¸€ç»„è¿›è¡Œç¿»è½¬ã€‚å¦‚æœæœ€åä¸è¶³ k ä¸ªï¼Œä¿æŒåŸæ ·ã€‚ è¾“å…¥: 1-2-3-4-5, k=3è¾“å‡º: 3-2-1-4-5 11.2 æ€è·¯æ‹†è§£ å…ˆæ•° k ä¸ªèŠ‚ç‚¹ï¼Œä¸å¤Ÿå°±ä¸ç¿» ç¿»è½¬è¿™ k ä¸ªèŠ‚ç‚¹ï¼ˆå°±æ˜¯ LC 206 çš„å­é—®é¢˜ï¼‰ é€’å½’å¤„ç†å‰©ä½™éƒ¨åˆ† æŠŠç¿»è½¬åçš„å°¾å·´æ¥ä¸Šé€’å½’ç»“æœ def reverseKGroup(head, k): # Step 1: æ£€æŸ¥æ˜¯å¦æœ‰ k ä¸ªèŠ‚ç‚¹ curr = head count = 0 while curr and count k: curr = curr.next count += 1 if count k: return head # ä¸è¶³ k ä¸ªï¼Œä¸ç¿»è½¬ # Step 2: ç¿»è½¬å‰ k ä¸ª prev = None curr = head for _ in range(k): next_temp = curr.next curr.next = prev prev = curr curr = next_temp # Step 3: head ç°åœ¨æ˜¯ç¿»è½¬åçš„å°¾å·´ï¼Œæ¥ä¸Šé€’å½’ç»“æœ head.next = reverseKGroup(curr, k) return prev # prev æ˜¯ç¿»è½¬åçš„æ–°å¤´ æ•°æ®æµè¿½è¸ªï¼ˆ1-2-3-4-5, k=3ï¼‰ï¼š ç¬¬ä¸€æ¬¡è°ƒç”¨: head=1, æ•°åˆ° k=3 âœ“ ç¿»è½¬ [1,2,3]: prev=3-2-1 head(1).next = reverseKGroup(4-5, 3) ç¬¬äºŒæ¬¡è°ƒç”¨: head=4, æ•°åˆ° 2 3 â†’ ä¸ç¿»è½¬ï¼Œè¿”å› 4-5 head(1).next = 4-5 æœ€ç»ˆ: 3-2-1-4-5 âœ“ è´¹æ›¼ç¬”è®°ï¼šK ç»„ç¿»è½¬ = æ•°å¤Ÿ k ä¸ªå°±ç¿»ï¼Œç¿»å®Œé€’å½’å¤„ç†å‰©ä¸‹çš„ï¼Œä¸å¤Ÿå°±ä¸åŠ¨ã€‚æ ¸å¿ƒè¿˜æ˜¯åè½¬é“¾è¡¨ï¼Œåªæ˜¯åŠ äº†â€åˆ†æ®µâ€å’Œâ€é€’å½’â€ã€‚ åäºŒã€é“¾è¡¨é¢˜å‹å…¨æ™¯å›¾ï¼šæ¨¡å¼è¯†åˆ«åšäº†è¿™ä¹ˆå¤šé¢˜ï¼Œæˆ‘ä»¬æ¥æç‚¼æ¨¡å¼ã€‚é“¾è¡¨é¢˜ä¸‡å˜ä¸ç¦»å…¶å®—ï¼Œæ ¸å¿ƒå°±è¿™å‡ ä¸ªå¥—è·¯ï¼š 12.1 äº”å¤§æ ¸å¿ƒæŠ€å·§ æŠ€å·§ é€‚ç”¨åœºæ™¯ ä»£è¡¨é¢˜ç›® è™šæ‹Ÿå¤´èŠ‚ç‚¹ éœ€è¦åˆ é™¤/ä¿®æ”¹å¤´èŠ‚ç‚¹çš„åœºæ™¯ LC 203, 21, 19, 82 å¿«æ…¢æŒ‡é’ˆ æ‰¾ä¸­ç‚¹ã€åˆ¤ç¯ã€æ‰¾ç¯å…¥å£ LC 876, 141, 142, 234 é—´è·æŒ‡é’ˆ å€’æ•°ç¬¬ N ä¸ªã€çª—å£æ»‘åŠ¨ LC 19, 61 åè½¬é“¾è¡¨ ç¿»è½¬å…¨éƒ¨/éƒ¨åˆ†/Kç»„ LC 206, 92, 25, 234 åˆå¹¶é“¾è¡¨ æœ‰åºåˆå¹¶ã€åˆ†æ²» LC 21, 23, 148 12.2 è§£é¢˜å†³ç­–æ ‘æ‹¿åˆ°é“¾è¡¨é¢˜ â†’â”œâ”€â”€ éœ€è¦åˆ é™¤èŠ‚ç‚¹ï¼Ÿ â†’ ç”¨ dummy headâ”œâ”€â”€ éœ€è¦æ‰¾ä¸­ç‚¹/åˆ¤ç¯ï¼Ÿ â†’ ç”¨å¿«æ…¢æŒ‡é’ˆâ”œâ”€â”€ éœ€è¦å€’æ•°ç¬¬ N ä¸ªï¼Ÿ â†’ ç”¨é—´è·æŒ‡é’ˆâ”œâ”€â”€ éœ€è¦ç¿»è½¬ï¼Ÿ â†’ ç”¨ prev/curr/next ä¸‰æŒ‡é’ˆâ”œâ”€â”€ éœ€è¦åˆå¹¶ï¼Ÿ â†’ ç”¨ dummy + æ¯”è¾ƒâ””â”€â”€ éœ€è¦æ’åºï¼Ÿ â†’ å½’å¹¶æ’åºï¼ˆæ‰¾ä¸­ç‚¹+åˆ†å‰²+åˆå¹¶ï¼‰ 12.3 å¸¸è§é™·é˜±æ¸…å•1. ç©ºæŒ‡é’ˆè®¿é—® # é”™è¯¯ï¼šæ²¡æ£€æŸ¥ curr æ˜¯å¦ä¸º Nonewhile curr.next: # å¦‚æœ curr æ˜¯ Noneï¼Œç›´æ¥å´©æºƒ# æ­£ç¡®ï¼šwhile curr and curr.next: 2. æ–­é“¾ä¸¢å¤± # é”™è¯¯ï¼šå…ˆæ–­åæ¥ï¼Œä¸¢å¤±åç»­èŠ‚ç‚¹prev.next = new_node # åŸæ¥çš„ prev.next ä¸¢äº†ï¼new_node.next = ??? # æ‰¾ä¸åˆ°äº†# æ­£ç¡®ï¼šå…ˆæ¥åæ–­new_node.next = prev.next # å…ˆä¿å­˜prev.next = new_node # å†ä¿®æ”¹ 3. å¿˜è®°æ›´æ–°æŒ‡é’ˆ # é”™è¯¯ï¼šåˆ é™¤åå¿˜è®°ç§»åŠ¨while curr.next: if curr.next.val == target: curr.next = curr.next.next # å¿˜äº†ï¼šè¿™é‡Œä¸åº”è¯¥ curr = curr.nextï¼ # å› ä¸ºæ–°çš„ curr.next å¯èƒ½è¿˜æ˜¯ target else: curr = curr.next 4. è¿”å›å€¼é”™è¯¯ # é”™è¯¯ï¼šè¿”å› headï¼ˆå¯èƒ½å·²ç»è¢«åˆ äº†ï¼‰# æ­£ç¡®ï¼šç”¨ dummy headï¼Œè¿”å› dummy.next åä¸‰ã€è´¹æ›¼ç»ˆææ€»ç»“ï¼šç”¨ä¸€å¥è¯è¯´æ¸…æ¯ä¸ªæ¦‚å¿µ è´¹æ›¼æ£€éªŒï¼šå¦‚æœä½ èƒ½ç”¨ä¸€å¥è¯å‘ä¸€ä¸ª 10 å²å°å­©è§£é‡Šæ¸…æ¥šï¼Œä½ æ‰çœŸæ­£ç†è§£äº†ã€‚ æ¦‚å¿µ ä¸€å¥è¯è§£é‡Š æŒ‡é’ˆ ä¸€å¼ å†™ç€é—¨ç‰Œå·çš„çº¸æ¡ï¼Œæ‹¿ç€å®ƒå°±èƒ½æ‰¾åˆ°å¯¹åº”çš„æˆ¿é—´ é“¾è¡¨èŠ‚ç‚¹ ä¸€ä¸ªç›’å­ï¼Œé‡Œé¢è£…ç€ä¸€ä¸ªä¸œè¥¿å’Œä¸€å¼ çº¸æ¡ï¼ˆæŒ‡å‘ä¸‹ä¸€ä¸ªç›’å­ï¼‰ éå† å¯»å®æ¸¸æˆï¼šæ‰“å¼€ç›’å­ï¼Œçœ‹çº¸æ¡ï¼Œå»ä¸‹ä¸€ä¸ªç›’å­ï¼Œç›´åˆ°çº¸æ¡å†™ç€â€æ²¡äº†â€ æ’å…¥ æ–°äººæ’é˜Ÿï¼šå…ˆè®©æ–°äººæ‹‰ä½åé¢çš„äººï¼Œå†è®©å‰é¢çš„äººæ‹‰ä½æ–°äºº åˆ é™¤ è¸¢å‡ºé˜Ÿä¼ï¼šè®©å‰é¢çš„äººç›´æ¥æ‹‰ä½åé¢çš„äººï¼Œä¸­é—´é‚£ä¸ªè‡ªç„¶å°±å‡ºå±€äº† è™šæ‹Ÿå¤´ åœ¨é˜Ÿä¼æœ€å‰é¢æ”¾ä¸€ä¸ªå‡äººï¼Œè¿™æ ·ä¸ç®¡è¸¢è°ï¼Œæ“ä½œéƒ½ä¸€æ · å¿«æ…¢æŒ‡é’ˆ ä¸¤äººèµ›è·‘ï¼Œå¿«çš„èµ°ä¸¤æ­¥æ…¢çš„èµ°ä¸€æ­¥ï¼Œå¿«çš„åˆ°ç»ˆç‚¹æ—¶æ…¢çš„åœ¨ä¸­é—´ åˆ¤ç¯ æ“åœºè·‘æ­¥ï¼Œå¿«çš„è¿½ä¸Šæ…¢çš„è¯´æ˜æ˜¯ç¯å½¢è·‘é“ åè½¬ æŠŠæ¯èŠ‚ç«è½¦è½¦å¢çš„æŒ‚é’©æ–¹å‘æ°è¿‡æ¥ åˆå¹¶ ä¸¤å‰¯æ’å¥½çš„æ‰‘å…‹ç‰Œï¼Œæ¯æ¬¡æ¯”é¡¶ä¸Šçš„ï¼Œå°çš„å…ˆå‡º é“¾è¡¨çš„æœ¬è´¨é“¾è¡¨çš„æ‰€æœ‰æ“ä½œï¼Œå½’æ ¹ç»“åº•å°±æ˜¯åœ¨åšä¸€ä»¶äº‹ï¼šæ”¹ next æŒ‡é’ˆã€‚ æ’å…¥ = æ”¹ä¸¤ä¸ª next åˆ é™¤ = æ”¹ä¸€ä¸ª next åè½¬ = æ”¹æ‰€æœ‰ next çš„æ–¹å‘ åˆå¹¶ = æŒ‰æ¡ä»¶å†³å®š next æ¥è° ç†è§£äº†è¿™ä¸€ç‚¹ï¼Œä½ å°±ç†è§£äº†é“¾è¡¨çš„å…¨éƒ¨ã€‚ ä»æ•°ç»„åˆ°é“¾è¡¨çš„æ€ç»´è½¬æ¢æ•°ç»„æ€ç»´ï¼šæˆ‘è¦è®¿é—®ç¬¬ i ä¸ªå…ƒç´  â†’ arr[i] â†’ O(1)é“¾è¡¨æ€ç»´ï¼šæˆ‘è¦ä»å¤´èµ°åˆ°ç¬¬ i ä¸ª â†’ èµ° i æ­¥ â†’ O(n)æ•°ç»„æ€ç»´ï¼šæ’å…¥ä¸€ä¸ªå…ƒç´  â†’ åé¢å…¨éƒ¨æ¬å®¶ â†’ O(n)é“¾è¡¨æ€ç»´ï¼šæ”¹ä¸¤ä¸ªæŒ‡é’ˆ â†’ O(1)æ•°ç»„æ€ç»´ï¼šæ•°æ®åœ¨å†…å­˜ä¸­è¿ç»­æ’åˆ—é“¾è¡¨æ€ç»´ï¼šæ•°æ®æ•£è½åœ¨å†…å­˜å„å¤„ï¼Œç”¨æŒ‡é’ˆä¸²èµ·æ¥ ç»ˆæè´¹æ›¼ç¬”è®°ï¼šæ•°ç»„æ˜¯ä¸€æ’ç´§æŒ¨ç€çš„å‚¨ç‰©æŸœï¼ˆè¿ç»­å†…å­˜ï¼‰ï¼Œä½ çŸ¥é“ç¼–å·å°±èƒ½ç›´æ¥æ‰“å¼€ã€‚é“¾è¡¨æ˜¯æ•£è½å„å¤„çš„å®ç®±ï¼ˆç¦»æ•£å†…å­˜ï¼‰ï¼Œæ¯ä¸ªå®ç®±é‡Œæœ‰ä¸€å¼ çº¸æ¡å‘Šè¯‰ä½ ä¸‹ä¸€ä¸ªåœ¨å“ªã€‚å„æœ‰ä¼˜åŠ£ï¼Œé€‰æ‹©å–å†³äºä½ æ›´éœ€è¦â€å¿«é€Ÿæ‰¾åˆ°ç¬¬ N ä¸ªâ€è¿˜æ˜¯â€å¿«é€Ÿæ’å…¥åˆ é™¤â€ã€‚ åå››ã€é—ªè®°å¡ç‰‡ï¼ˆFlashcardsï¼‰ ä»¥ä¸‹å¡ç‰‡é€‚åˆç”¨ Anki å¯¼å…¥ï¼Œé‡‡ç”¨ Q/A æ ¼å¼ã€‚æ¯å¼ å¡ç‰‡å¯¹åº”ä¸€ä¸ªæ ¸å¿ƒçŸ¥è¯†ç‚¹ã€‚ å¡ç‰‡ 1ï¼šæŒ‡é’ˆæ˜¯ä»€ä¹ˆï¼ŸQ: ç”¨ä¸€å¥è¯è§£é‡ŠæŒ‡é’ˆã€‚ A: æŒ‡é’ˆæ˜¯ä¸€ä¸ªå˜é‡ï¼Œå®ƒå­˜å‚¨çš„ä¸æ˜¯æ•°æ®æœ¬èº«ï¼Œè€Œæ˜¯æ•°æ®æ‰€åœ¨çš„å†…å­˜åœ°å€ã€‚å°±åƒä¸€å¼ å†™ç€é—¨ç‰Œå·çš„çº¸æ¡ã€‚*p = å»é‚£ä¸ªåœ°å€çœ‹çœ‹ï¼Œx = å‘Šè¯‰æˆ‘ x çš„åœ°å€ã€‚ å¡ç‰‡ 2ï¼šé“¾è¡¨èŠ‚ç‚¹çš„ç»“æ„Q: é“¾è¡¨èŠ‚ç‚¹åŒ…å«å“ªä¸¤ä¸ªéƒ¨åˆ†ï¼Ÿ A: valï¼ˆå­˜æ•°æ®ï¼‰å’Œ nextï¼ˆå­˜ä¸‹ä¸€ä¸ªèŠ‚ç‚¹çš„åœ°å€ï¼‰ã€‚æœ€åä¸€ä¸ªèŠ‚ç‚¹çš„ next æŒ‡å‘ NULLã€‚ å¡ç‰‡ 3ï¼šé“¾è¡¨æ’å…¥çš„æ­£ç¡®é¡ºåºQ: åœ¨ prev åé¢æ’å…¥ new_nodeï¼Œä¸¤æ­¥æ“ä½œçš„é¡ºåºæ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆï¼Ÿ A: new_node.next = prev.nextï¼ˆå…ˆæ¥åé¢ï¼‰ prev.next = new_nodeï¼ˆå†æ–­å‰é¢ï¼‰ é¡ºåºä¸èƒ½åï¼å¦‚æœå…ˆæ‰§è¡Œ Step 2ï¼Œprev.next è¢«è¦†ç›–ï¼ŒåŸæ¥çš„åç»­èŠ‚ç‚¹å°±ä¸¢äº†ã€‚ å¡ç‰‡ 4ï¼šè™šæ‹Ÿå¤´èŠ‚ç‚¹çš„ä½œç”¨Q: ä»€ä¹ˆæ—¶å€™éœ€è¦ dummy headï¼Ÿå®ƒè§£å†³ä»€ä¹ˆé—®é¢˜ï¼Ÿ A: å½“æ“ä½œå¯èƒ½æ¶‰åŠå¤´èŠ‚ç‚¹çš„åˆ é™¤æˆ–ä¿®æ”¹æ—¶ä½¿ç”¨ã€‚å®ƒåœ¨çœŸæ­£çš„ head å‰é¢åŠ ä¸€ä¸ªå‡èŠ‚ç‚¹ï¼Œä½¿æ‰€æœ‰èŠ‚ç‚¹ï¼ˆåŒ…æ‹¬ headï¼‰éƒ½æœ‰å‰é©±èŠ‚ç‚¹ï¼Œé€»è¾‘ç»Ÿä¸€ä¸º prev.next = prev.next.nextã€‚æœ€åè¿”å› dummy.nextã€‚ å¡ç‰‡ 5ï¼šå¿«æ…¢æŒ‡é’ˆæ‰¾ä¸­ç‚¹Q: å¦‚ä½•ä¸€æ¬¡éå†æ‰¾åˆ°é“¾è¡¨ä¸­ç‚¹ï¼Ÿ A: slow èµ° 1 æ­¥ï¼Œfast èµ° 2 æ­¥ã€‚å½“ fast æˆ– fast.next ä¸º NULL æ—¶ï¼Œslow å°±åœ¨ä¸­ç‚¹ã€‚åŸç†ï¼šfast é€Ÿåº¦æ˜¯ slow çš„ 2 å€ï¼Œfast åˆ°ç»ˆç‚¹æ—¶ slow èµ°äº†ä¸€åŠã€‚ å¡ç‰‡ 6ï¼šFloyd åˆ¤åœˆç®—æ³•Q: å¦‚ä½•åˆ¤æ–­é“¾è¡¨æ˜¯å¦æœ‰ç¯ï¼Ÿå¦‚ä½•æ‰¾åˆ°ç¯çš„å…¥å£ï¼Ÿ A: åˆ¤ç¯ï¼šå¿«æ…¢æŒ‡é’ˆï¼Œç›¸é‡åˆ™æœ‰ç¯ã€‚ æ‰¾å…¥å£ï¼šç›¸é‡åï¼Œè®© slow å›åˆ° headï¼Œä¸¤ä¸ªæŒ‡é’ˆéƒ½èµ° 1 æ­¥ï¼Œå†æ¬¡ç›¸é‡ç‚¹å°±æ˜¯å…¥å£ã€‚æ•°å­¦åŸç†ï¼ša = (n-1)c + (c-b)ã€‚ å¡ç‰‡ 7ï¼šåè½¬é“¾è¡¨çš„ä¸‰æ­¥èˆQ: è¿­ä»£åè½¬é“¾è¡¨çš„æ ¸å¿ƒæ“ä½œæ˜¯ä»€ä¹ˆï¼Ÿ A: ä¸‰ä¸ªæŒ‡é’ˆ prev, curr, next_tempï¼Œå¾ªç¯æ‰§è¡Œï¼š next_temp = curr.nextï¼ˆå­˜åè·¯ï¼‰ curr.next = prevï¼ˆæ°æ–¹å‘ï¼‰ prev = curr; curr = next_tempï¼ˆå‰è¿›ï¼‰ å¾ªç¯ç»“æŸå prev æ˜¯æ–°å¤´ã€‚ å¡ç‰‡ 8ï¼šåˆå¹¶ä¸¤ä¸ªæœ‰åºé“¾è¡¨Q: åˆå¹¶ä¸¤ä¸ªæœ‰åºé“¾è¡¨çš„æ ¸å¿ƒæ€è·¯ï¼Ÿ A: ç”¨ dummy headï¼Œæ¯æ¬¡æ¯”è¾ƒä¸¤ä¸ªé“¾è¡¨å¤´éƒ¨ï¼Œå°çš„æ¥ä¸Šå»ï¼Œå¯¹åº”æŒ‡é’ˆå‰è¿›ã€‚æŸä¸€æ–¹ç”¨å®Œåï¼Œç›´æ¥æŠŠå¦ä¸€æ–¹å‰©ä½™éƒ¨åˆ†æ¥ä¸Šã€‚æ—¶é—´ O(m+n)ï¼Œç©ºé—´ O(1)ã€‚ å¡ç‰‡ 9ï¼šé—´è·æŒ‡é’ˆåˆ é™¤å€’æ•°ç¬¬ N ä¸ªQ: å¦‚ä½•ä¸€æ¬¡éå†åˆ é™¤å€’æ•°ç¬¬ N ä¸ªèŠ‚ç‚¹ï¼Ÿ A: fast å…ˆèµ° N+1 æ­¥ï¼Œç„¶å slow å’Œ fast ä¸€èµ·èµ°ã€‚fast åˆ° NULL æ—¶ï¼Œslow.next å°±æ˜¯è¦åˆ çš„èŠ‚ç‚¹ã€‚ç”¨ dummy head å¤„ç†åˆ å¤´èŠ‚ç‚¹çš„è¾¹ç•Œæƒ…å†µã€‚ å¡ç‰‡ 10ï¼šé“¾è¡¨ vs æ•°ç»„Q: é“¾è¡¨å’Œæ•°ç»„çš„æ ¸å¿ƒåŒºåˆ«ï¼Ÿå„è‡ªçš„ä¼˜åŠ¿åœºæ™¯ï¼Ÿ A: æ•°ç»„ é“¾è¡¨ å†…å­˜ è¿ç»­ ç¦»æ•£ éšæœºè®¿é—® O(1) O(n) æ’å…¥åˆ é™¤ O(n) O(1) é€‚ç”¨åœºæ™¯ é¢‘ç¹æŸ¥è¯¢ é¢‘ç¹å¢åˆ  å¡ç‰‡ 11ï¼šé“¾è¡¨æ’åºç”¨ä»€ä¹ˆç®—æ³•ï¼ŸQ: å¯¹é“¾è¡¨æ’åºï¼Œæœ€ä¼˜æ–¹æ¡ˆæ˜¯ä»€ä¹ˆï¼Ÿ A: å½’å¹¶æ’åºã€‚æ­¥éª¤ï¼šå¿«æ…¢æŒ‡é’ˆæ‰¾ä¸­ç‚¹ â†’ æ–­å¼€ â†’ é€’å½’æ’åºä¸¤åŠ â†’ åˆå¹¶æœ‰åºé“¾è¡¨ã€‚æ—¶é—´ O(n log n)ï¼Œç©ºé—´ O(log n)ï¼ˆé€’å½’æ ˆï¼‰ã€‚ä¸ç”¨å¿«æ’ï¼Œå› ä¸ºé“¾è¡¨ä¸æ”¯æŒéšæœºè®¿é—®ã€‚ å¡ç‰‡ 12ï¼šK ä¸ªä¸€ç»„ç¿»è½¬çš„æ€è·¯Q: LeetCode 25 â€œK ä¸ªä¸€ç»„ç¿»è½¬é“¾è¡¨â€çš„æ ¸å¿ƒæ€è·¯ï¼Ÿ A: æ•° k ä¸ªèŠ‚ç‚¹ï¼Œä¸å¤Ÿå°±ä¸ç¿» ç¿»è½¬è¿™ k ä¸ªï¼ˆLC 206 å­é—®é¢˜ï¼‰ åŸæ¥çš„ head å˜æˆå°¾å·´ï¼Œæ¥ä¸Šé€’å½’å¤„ç†å‰©ä½™éƒ¨åˆ†çš„ç»“æœ è¿”å›ç¿»è½¬åçš„æ–°å¤´ prev åäº”ã€LeetCode åˆ·é¢˜è·¯çº¿å›¾æŒ‰éš¾åº¦é€’è¿›ï¼Œå»ºè®®æŒ‰é¡ºåºåˆ·ï¼š ç¬¬ä¸€é˜¶æ®µï¼šåŸºç¡€æ“ä½œï¼ˆEasyï¼‰ # é¢˜ç›® æ ¸å¿ƒæŠ€å·§ éš¾åº¦ 206 åè½¬é“¾è¡¨ prev/curr/next ä¸‰æŒ‡é’ˆ â­ 21 åˆå¹¶ä¸¤ä¸ªæœ‰åºé“¾è¡¨ dummy head + æ¯”è¾ƒ â­ 141 ç¯å½¢é“¾è¡¨ å¿«æ…¢æŒ‡é’ˆ â­ 203 ç§»é™¤é“¾è¡¨å…ƒç´  dummy head â­ 876 é“¾è¡¨çš„ä¸­é—´èŠ‚ç‚¹ å¿«æ…¢æŒ‡é’ˆ â­ 83 åˆ é™¤æ’åºé“¾è¡¨ä¸­çš„é‡å¤å…ƒç´  éå† + è·³è¿‡ â­ 160 ç›¸äº¤é“¾è¡¨ åŒæŒ‡é’ˆèµ°å¯¹æ–¹çš„è·¯ â­ ç¬¬äºŒé˜¶æ®µï¼šæŠ€å·§ç»„åˆï¼ˆMediumï¼‰ # é¢˜ç›® æ ¸å¿ƒæŠ€å·§ éš¾åº¦ 2 ä¸¤æ•°ç›¸åŠ  éå† + è¿›ä½ â­â­ 19 åˆ é™¤é“¾è¡¨çš„å€’æ•°ç¬¬ N ä¸ªèŠ‚ç‚¹ é—´è·æŒ‡é’ˆ + dummy â­â­ 24 ä¸¤ä¸¤äº¤æ¢é“¾è¡¨ä¸­çš„èŠ‚ç‚¹ åè½¬å˜ä½“ â­â­ 142 ç¯å½¢é“¾è¡¨ II Floyd æ‰¾å…¥å£ â­â­ 234 å›æ–‡é“¾è¡¨ å¿«æ…¢ + åè½¬ + æ¯”è¾ƒ â­â­ 148 æ’åºé“¾è¡¨ å½’å¹¶æ’åº â­â­ 82 åˆ é™¤æ’åºé“¾è¡¨ä¸­çš„é‡å¤å…ƒç´  II dummy + è·³è¿‡æ‰€æœ‰é‡å¤ â­â­ 92 åè½¬é“¾è¡¨ II åŒºé—´åè½¬ â­â­ 61 æ—‹è½¬é“¾è¡¨ æˆç¯å†æ–­å¼€ â­â­ 138 å¤åˆ¶å¸¦éšæœºæŒ‡é’ˆçš„é“¾è¡¨ å“ˆå¸Œè¡¨ / äº¤ç»‡å¤åˆ¶ â­â­ ç¬¬ä¸‰é˜¶æ®µï¼šç»ˆææŒ‘æˆ˜ï¼ˆHardï¼‰ # é¢˜ç›® æ ¸å¿ƒæŠ€å·§ éš¾åº¦ 25 K ä¸ªä¸€ç»„ç¿»è½¬é“¾è¡¨ åˆ†æ®µåè½¬ + é€’å½’ â­â­â­ 23 åˆå¹¶ K ä¸ªå‡åºé“¾è¡¨ åˆ†æ²» / æœ€å°å † â­â­â­ åå…­ã€é™„å½•ï¼šå®Œæ•´é“¾è¡¨å·¥å…·ç±»ä»¥ä¸‹æ˜¯ä¸€ä¸ªå¯ä»¥ç›´æ¥ç”¨äº LeetCode æœ¬åœ°è°ƒè¯•çš„å·¥å…·ç±»ï¼š class ListNode: def __init__(self, val=0, next=None): self.val = val self.next = nextdef build_list(arr): \"\"\"ä»æ•°ç»„åˆ›å»ºé“¾è¡¨: [1,2,3] - 1-2-3-NULL\"\"\" dummy = ListNode(0) curr = dummy for v in arr: curr.next = ListNode(v) curr = curr.next return dummy.nextdef to_array(head): \"\"\"é“¾è¡¨è½¬æ•°ç»„: 1-2-3-NULL - [1,2,3]\"\"\" res = [] while head: res.append(head.val) head = head.next return resdef print_list(head): \"\"\"æ‰“å°é“¾è¡¨: 1 - 2 - 3 - NULL\"\"\" parts = [] while head: parts.append(str(head.val)) head = head.next print(\" - \".join(parts) + \" - NULL\")# ä½¿ç”¨ç¤ºä¾‹head = build_list([1, 2, 3, 4, 5])print_list(head) # 1 - 2 - 3 - 4 - 5 - NULL å†™åœ¨æœ€åé“¾è¡¨çœ‹ä¼¼ç®€å•ï¼Œå®åˆ™æ˜¯æŒ‡é’ˆæ“ä½œçš„è®­ç»ƒåœºã€‚æŒæ¡äº†é“¾è¡¨ï¼Œä½ å°±æŒæ¡äº†ï¼š æŒ‡é’ˆæ€ç»´ï¼šç†è§£å¼•ç”¨ã€åœ°å€ã€é—´æ¥è®¿é—® è¾¹ç•Œå¤„ç†ï¼šç©ºæŒ‡é’ˆã€å¤´å°¾èŠ‚ç‚¹ã€å•å…ƒç´  ç”»å›¾èƒ½åŠ›ï¼šå¤æ‚æŒ‡é’ˆæ“ä½œå¿…é¡»ç”»å›¾ï¼Œè„‘å­é‡Œè½¬ä¸è¿‡æ¥ é€’å½’ç›´è§‰ï¼šé“¾è¡¨å¤©ç„¶é€‚åˆé€’å½’æ€è€ƒ æœ€é‡è¦çš„ä¸€æ¡å»ºè®®ï¼šåšé“¾è¡¨é¢˜æ—¶ï¼Œä¸€å®šè¦ç”»å›¾ã€‚æŠŠæ¯ä¸€æ­¥çš„æŒ‡é’ˆå˜åŒ–ç”»å‡ºæ¥ï¼Œæ¯”åœ¨è„‘å­é‡Œæƒ³å¿« 10 å€ã€‚ â€œWhat I cannot create, I do not understand.â€ â€” Richard Feynman æœ¬æ–‡åŒ…å« 10+ å¼ åŸåˆ›å›¾è§£ã€12 å¼ é—ªè®°å¡ç‰‡ã€20+ é“ LeetCode é¢˜ç›®è§£æã€‚å¦‚æœå¯¹ä½ æœ‰å¸®åŠ©ï¼Œæ¬¢è¿æ”¶è—å’Œåˆ†äº«ã€‚","tags":["ç®—æ³•","LeetCode","æ•°æ®ç»“æ„","é“¾è¡¨","æŒ‡é’ˆ"],"categories":["ç®—æ³•ä¸æ•°æ®ç»“æ„"]},{"title":"åŠ¨æ€è§„åˆ’å®Œå…¨æŒ‡å—ï¼šä»å…¥é—¨åˆ°ç²¾é€š","path":"/2026/02/20/20250220-dynamic-programming-complete-guide/","content":"åŠ¨æ€è§„åˆ’å®Œå…¨æŒ‡å—ï¼šä»å…¥é—¨åˆ°ç²¾é€š ç”¨è´¹æ›¼å­¦ä¹ æ³•ï¼Œå½»åº•ææ‡‚åŠ¨æ€è§„åˆ’çš„æœ¬è´¨ ä¸€ã€ä»€ä¹ˆæ˜¯åŠ¨æ€è§„åˆ’ï¼Ÿ1.1 ä¸€å¥è¯å®šä¹‰åŠ¨æ€è§„åˆ’ (Dynamic Programming, DP) = è®°ä½å·²ç»è§£å†³è¿‡çš„å­é—®é¢˜çš„ç­”æ¡ˆï¼Œé¿å…é‡å¤è®¡ç®—ã€‚ å°±è¿™ä¹ˆç®€å•ã€‚ 1.2 ä¸ºä»€ä¹ˆå«â€åŠ¨æ€è§„åˆ’â€ï¼Ÿè¿™ä¸ªåå­—å…¶å®å¾ˆæœ‰è¯¯å¯¼æ€§ã€‚Richard Bellman åœ¨ 1950 å¹´ä»£å‘æ˜è¿™ä¸ªæ–¹æ³•æ—¶ï¼Œæ•…æ„èµ·äº†ä¸ªå¬èµ·æ¥å¾ˆé«˜å¤§ä¸Šçš„åå­—ï¼Œå› ä¸ºä»–çš„è€æ¿ä¸å–œæ¬¢â€æ•°å­¦ç ”ç©¶â€ã€‚â€Dynamicâ€ å¬èµ·æ¥å¾ˆé…·ï¼Œâ€Programmingâ€ åœ¨å½“æ—¶æŒ‡çš„æ˜¯â€è§„åˆ’/ä¼˜åŒ–â€ï¼Œä¸æ˜¯å†™ä»£ç ã€‚ æ‰€ä»¥åˆ«è¢«åå­—å“åˆ°ï¼Œå®ƒçš„æœ¬è´¨å°±æ˜¯ï¼šç”¨ç©ºé—´æ¢æ—¶é—´ï¼Œè®°ä½ä¸­é—´ç»“æœã€‚ 1.3 æ ¸å¿ƒç›´è§‰ï¼šæ–æ³¢é‚£å¥‘æ•°åˆ—è®©æˆ‘ä»¬ä»æœ€ç»å…¸çš„ä¾‹å­å¼€å§‹ï¼š fib(0) = 0fib(1) = 1fib(n) = fib(n-1) + fib(n-2) æš´åŠ›é€’å½’å†™æ³•ï¼š def fib(n): if n = 1: return n return fib(n-1) + fib(n-2) çœ‹èµ·æ¥å¾ˆç®€æ´å¯¹å§ï¼Ÿä½†æ˜¯æœ‰ä¸ªè‡´å‘½é—®é¢˜ï¼š çœ‹åˆ°äº†å—ï¼Ÿfib(3) è¢«è®¡ç®—äº† 2 æ¬¡ï¼Œfib(2) è¢«è®¡ç®—äº† 3 æ¬¡ï¼å½“ n=50 æ—¶ï¼Œè¿™ä¸ªé€’å½’æ ‘ä¼šæœ‰ 2^50 ä¸ªèŠ‚ç‚¹ï¼Œä½ çš„ç”µè„‘ä¼šç›´æ¥å¡æ­»ã€‚ æ—¶é—´å¤æ‚åº¦ï¼šO(2^n) â€”â€” æŒ‡æ•°çº§çˆ†ç‚¸ï¼ 1.4 DP çš„è§£å†³æ–¹æ¡ˆæ–¹æ¡ˆä¸€ï¼šè®°å¿†åŒ–é€’å½’ (Top-Down) def fib(n, memo={}): if n in memo: return memo[n] if n = 1: return n memo[n] = fib(n-1, memo) + fib(n-2, memo) return memo[n] æ–¹æ¡ˆäºŒï¼šè¿­ä»£å¡«è¡¨ (Bottom-Up) def fib(n): if n = 1: return n dp = [0] * (n + 1) dp[0], dp[1] = 0, 1 for i in range(2, n + 1): dp[i] = dp[i-1] + dp[i-2] return dp[n] æ–¹æ¡ˆä¸‰ï¼šç©ºé—´ä¼˜åŒ– def fib(n): if n = 1: return n prev, curr = 0, 1 for _ in range(2, n + 1): prev, curr = curr, prev + curr return curr æ–¹æ¡ˆ æ—¶é—´å¤æ‚åº¦ ç©ºé—´å¤æ‚åº¦ æš´åŠ›é€’å½’ O(2^n) O(n) æ ˆç©ºé—´ è®°å¿†åŒ–é€’å½’ O(n) O(n) è¿­ä»£å¡«è¡¨ O(n) O(n) ç©ºé—´ä¼˜åŒ– O(n) O(1) è¿™å°±æ˜¯ DP çš„å¨åŠ›ï¼šä»æŒ‡æ•°çº§é™åˆ°çº¿æ€§çº§ï¼ äºŒã€DP çš„ä¸‰å¤§æ ¸å¿ƒè¦ç´  2.1 çŠ¶æ€å®šä¹‰ (State Definition)è¿™æ˜¯æœ€å…³é”®çš„ä¸€æ­¥ï¼ çŠ¶æ€å®šä¹‰é”™äº†ï¼Œåé¢å…¨ç™½æ­ã€‚ é—®è‡ªå·±ï¼šdp[i] æˆ– dp[i][j] ä»£è¡¨ä»€ä¹ˆï¼Ÿ å¸¸è§çš„çŠ¶æ€å®šä¹‰æ¨¡å¼ï¼š é—®é¢˜ç±»å‹ çŠ¶æ€å®šä¹‰ ä¾‹å­ çº¿æ€§åºåˆ— dp[i] = ä»¥ç¬¬iä¸ªå…ƒç´ ç»“å°¾çš„xxx æœ€é•¿é€’å¢å­åºåˆ— çº¿æ€§åºåˆ— dp[i] = å‰iä¸ªå…ƒç´ çš„xxx æ‰“å®¶åŠ«èˆ åŒåºåˆ— dp[i][j] = s1å‰iä¸ªå’Œs2å‰jä¸ªçš„xxx LCS, ç¼–è¾‘è·ç¦» èƒŒåŒ… dp[i][w] = å‰iä¸ªç‰©å“ã€å®¹é‡wçš„xxx 0-1èƒŒåŒ… åŒºé—´ dp[i][j] = åŒºé—´[i,j]çš„xxx æˆ³æ°”çƒ 2.2 çŠ¶æ€è½¬ç§»æ–¹ç¨‹ (Transition)é—®è‡ªå·±ï¼šå½“å‰çŠ¶æ€å’Œå“ªäº›ä¹‹å‰çš„çŠ¶æ€æœ‰å…³ï¼Ÿ è¿™æ˜¯ DP çš„â€é€’æ¨å…¬å¼â€ï¼Œå†³å®šäº†å¦‚ä½•ä»å°é—®é¢˜æ¨å¯¼å‡ºå¤§é—®é¢˜ã€‚ dp[i] = f(dp[i-1], dp[i-2], ..., dp[0]) æŠ€å·§ï¼šç”»å›¾ï¼ æŠŠçŠ¶æ€ä¹‹é—´çš„ä¾èµ–å…³ç³»ç”»å‡ºæ¥ï¼Œè½¬ç§»æ–¹ç¨‹è‡ªç„¶å°±å‡ºæ¥äº†ã€‚ 2.3 è¾¹ç•Œæ¡ä»¶ (Base Case)é—®è‡ªå·±ï¼šæœ€å°çš„å­é—®é¢˜ï¼Œç­”æ¡ˆæ˜¯ä»€ä¹ˆï¼Ÿ è¾¹ç•Œæ¡ä»¶å°±æ˜¯é€’æ¨çš„èµ·ç‚¹ã€‚æ²¡æœ‰æ­£ç¡®çš„è¾¹ç•Œæ¡ä»¶ï¼Œæ•´ä¸ª DP å°±ä¼šå´©æºƒã€‚ å¸¸è§è¾¹ç•Œï¼š dp[0] = 0 æˆ– dp[0] = 1 dp[0][j] = xxx, dp[i][0] = xxx ç©ºé›†ã€ç©ºä¸²çš„æƒ…å†µ ä¸‰ã€ä¸¤ç§å®ç°æ–¹å¼å¯¹æ¯” 3.1 Top-Downï¼šè®°å¿†åŒ–é€’å½’def solve(n, memo={}): # 1. æ£€æŸ¥ç¼“å­˜ if n in memo: return memo[n] # 2. Base case if n = 1: return base_value # 3. é€’å½’ + è®°å¿†åŒ– memo[n] = f(solve(n-1), solve(n-2), ...) return memo[n] ä¼˜ç‚¹ï¼š æ€è·¯è‡ªç„¶ï¼Œä»ç›®æ ‡å‡ºå‘ åªè®¡ç®—éœ€è¦çš„å­é—®é¢˜ ä»£ç æ›´æ¥è¿‘æ•°å­¦å®šä¹‰ ç¼ºç‚¹ï¼š é€’å½’æ ˆå¼€é”€ Python é»˜è®¤é€’å½’æ·±åº¦é™åˆ¶ (1000) 3.2 Bottom-Upï¼šè¿­ä»£å¡«è¡¨def solve(n): # 1. åˆå§‹åŒ– DP æ•°ç»„ dp = [0] * (n + 1) # 2. Base case dp[0] = base_value # 3. æŒ‰é¡ºåºå¡«è¡¨ for i in range(1, n + 1): dp[i] = f(dp[i-1], dp[i-2], ...) # 4. è¿”å›ç»“æœ return dp[n] ä¼˜ç‚¹ï¼š æ— é€’å½’æ ˆå¼€é”€ æ›´å®¹æ˜“åšç©ºé—´ä¼˜åŒ– é€šå¸¸æ›´å¿«ï¼ˆæ— å‡½æ•°è°ƒç”¨å¼€é”€ï¼‰ ç¼ºç‚¹ï¼š éœ€è¦æƒ³æ¸…æ¥šéå†é¡ºåº å¯èƒ½è®¡ç®—ä¸éœ€è¦çš„å­é—®é¢˜ 3.3 å¦‚ä½•é€‰æ‹©ï¼Ÿ åœºæ™¯ æ¨èæ–¹å¼ é¢è¯•æ—¶å¿«é€Ÿå†™å‡º Top-Downï¼ˆæ›´ç›´è§‚ï¼‰ è¿½æ±‚æ€§èƒ½ Bottom-Up éœ€è¦ç©ºé—´ä¼˜åŒ– Bottom-Up çŠ¶æ€è½¬ç§»å¤æ‚ Top-Downï¼ˆæ›´å®¹æ˜“è°ƒè¯•ï¼‰ å››ã€ç©ºé—´ä¼˜åŒ–æŠ€å·§ 4.1 æ»šåŠ¨æ•°ç»„å½“ dp[i] åªä¾èµ– dp[i-1] æ—¶ï¼Œä¸éœ€è¦ä¿å­˜æ•´ä¸ªæ•°ç»„ï¼š # ä¼˜åŒ–å‰ï¼šO(n) ç©ºé—´dp = [0] * nfor i in range(1, n): dp[i] = dp[i-1] + something# ä¼˜åŒ–åï¼šO(1) ç©ºé—´prev = 0for i in range(1, n): curr = prev + something prev = curr 4.2 äºŒç»´é™ä¸€ç»´å½“ dp[i][j] åªä¾èµ– dp[i-1][...] æ—¶ï¼š # ä¼˜åŒ–å‰ï¼šO(m*n) ç©ºé—´dp = [[0] * n for _ in range(m)]for i in range(1, m): for j in range(1, n): dp[i][j] = dp[i-1][j] + dp[i][j-1]# ä¼˜åŒ–åï¼šO(n) ç©ºé—´dp = [0] * nfor i in range(1, m): for j in range(1, n): dp[j] = dp[j] + dp[j-1] # dp[j] å°±æ˜¯åŸæ¥çš„ dp[i-1][j] æ³¨æ„éå†é¡ºåºï¼ å¦‚æœä¾èµ–å·¦ä¸Šè§’ï¼Œéœ€è¦ä»å³å¾€å·¦éå†ã€‚ äº”ã€ç»å…¸é—®é¢˜è¯¦è§£5.1 çˆ¬æ¥¼æ¢¯ (LeetCode 70) é—®é¢˜ï¼š æ¯æ¬¡å¯ä»¥çˆ¬ 1 æˆ– 2 ä¸ªå°é˜¶ï¼Œçˆ¬åˆ°ç¬¬ n é˜¶æœ‰å¤šå°‘ç§æ–¹æ³•ï¼Ÿ çŠ¶æ€å®šä¹‰ï¼š dp[i] = çˆ¬åˆ°ç¬¬ i é˜¶çš„æ–¹æ³•æ•° è½¬ç§»æ–¹ç¨‹ï¼š dp[i] = dp[i-1] + dp[i-2] ä»ç¬¬ i-1 é˜¶çˆ¬ 1 æ­¥ä¸Šæ¥ ä»ç¬¬ i-2 é˜¶çˆ¬ 2 æ­¥ä¸Šæ¥ è¾¹ç•Œæ¡ä»¶ï¼š dp[0] = 1, dp[1] = 1 def climbStairs(n: int) - int: if n = 2: return n prev, curr = 1, 2 for _ in range(3, n + 1): prev, curr = curr, prev + curr return curr å¤æ‚åº¦ï¼š æ—¶é—´ O(n)ï¼Œç©ºé—´ O(1) 5.2 æ‰“å®¶åŠ«èˆ (LeetCode 198)é—®é¢˜ï¼š ä¸èƒ½å·ç›¸é‚»çš„æˆ¿å­ï¼Œæ±‚èƒ½å·åˆ°çš„æœ€å¤§é‡‘é¢ã€‚ çŠ¶æ€å®šä¹‰ï¼š dp[i] = è€ƒè™‘å‰ i ä¸ªæˆ¿å­èƒ½å·åˆ°çš„æœ€å¤§é‡‘é¢ è½¬ç§»æ–¹ç¨‹ï¼š dp[i] = max(dp[i-1], dp[i-2] + nums[i]) ä¸å·ç¬¬ i ä¸ªï¼šdp[i-1] å·ç¬¬ i ä¸ªï¼šdp[i-2] + nums[i]ï¼ˆä¸èƒ½å· i-1ï¼‰ def rob(nums: list[int]) - int: if len(nums) = 2: return max(nums) if nums else 0 prev2, prev1 = nums[0], max(nums[0], nums[1]) for i in range(2, len(nums)): prev2, prev1 = prev1, max(prev1, prev2 + nums[i]) return prev1 å¤æ‚åº¦ï¼š æ—¶é—´ O(n)ï¼Œç©ºé—´ O(1) 5.3 æœ€å¤§å­æ•°ç»„å’Œ (LeetCode 53) é—®é¢˜ï¼š æ‰¾å‡ºå’Œæœ€å¤§çš„è¿ç»­å­æ•°ç»„ã€‚ çŠ¶æ€å®šä¹‰ï¼š dp[i] = ä»¥ nums[i] ç»“å°¾çš„æœ€å¤§å­æ•°ç»„å’Œ è½¬ç§»æ–¹ç¨‹ï¼š dp[i] = max(nums[i], dp[i-1] + nums[i]) è¦ä¹ˆä» nums[i] é‡æ–°å¼€å§‹ è¦ä¹ˆæ¥ä¸Šå‰é¢çš„å­æ•°ç»„ def maxSubArray(nums: list[int]) - int: max_sum = curr_sum = nums[0] for num in nums[1:]: curr_sum = max(num, curr_sum + num) max_sum = max(max_sum, curr_sum) return max_sum å¤æ‚åº¦ï¼š æ—¶é—´ O(n)ï¼Œç©ºé—´ O(1) 5.4 æœ€é•¿é€’å¢å­åºåˆ— LIS (LeetCode 300)é—®é¢˜ï¼š æ‰¾å‡ºæœ€é•¿ä¸¥æ ¼é€’å¢å­åºåˆ—çš„é•¿åº¦ã€‚ çŠ¶æ€å®šä¹‰ï¼š dp[i] = ä»¥ nums[i] ç»“å°¾çš„ LIS é•¿åº¦ è½¬ç§»æ–¹ç¨‹ï¼š dp[i] = max(dp[j] + 1) for all j i where nums[j] nums[i] def lengthOfLIS(nums: list[int]) - int: n = len(nums) dp = [1] * n for i in range(1, n): for j in range(i): if nums[j] nums[i]: dp[i] = max(dp[i], dp[j] + 1) return max(dp) å¤æ‚åº¦ï¼š æ—¶é—´ O(nÂ²)ï¼Œç©ºé—´ O(n) ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆäºŒåˆ†æŸ¥æ‰¾ï¼‰ï¼š O(n log n) import bisectdef lengthOfLIS(nums: list[int]) - int: tails = [] for num in nums: pos = bisect.bisect_left(tails, num) if pos == len(tails): tails.append(num) else: tails[pos] = num return len(tails) 5.5 æœ€é•¿å…¬å…±å­åºåˆ— LCS (LeetCode 1143) é—®é¢˜ï¼š ä¸¤ä¸ªå­—ç¬¦ä¸²çš„æœ€é•¿å…¬å…±å­åºåˆ—é•¿åº¦ã€‚ çŠ¶æ€å®šä¹‰ï¼š dp[i][j] = s1 å‰ i ä¸ªå­—ç¬¦å’Œ s2 å‰ j ä¸ªå­—ç¬¦çš„ LCS é•¿åº¦ è½¬ç§»æ–¹ç¨‹ï¼š if s1[i-1] == s2[j-1]: dp[i][j] = dp[i-1][j-1] + 1else: dp[i][j] = max(dp[i-1][j], dp[i][j-1]) def longestCommonSubsequence(text1: str, text2: str) - int: m, n = len(text1), len(text2) dp = [[0] * (n + 1) for _ in range(m + 1)] for i in range(1, m + 1): for j in range(1, n + 1): if text1[i-1] == text2[j-1]: dp[i][j] = dp[i-1][j-1] + 1 else: dp[i][j] = max(dp[i-1][j], dp[i][j-1]) return dp[m][n] å¤æ‚åº¦ï¼š æ—¶é—´ O(mn)ï¼Œç©ºé—´ O(mn)ï¼Œå¯ä¼˜åŒ–åˆ° O(n) 5.6 ç¼–è¾‘è·ç¦» (LeetCode 72) é—®é¢˜ï¼š å°† word1 è½¬æ¢æˆ word2 æ‰€éœ€çš„æœ€å°‘æ“ä½œæ•°ï¼ˆæ’å…¥ã€åˆ é™¤ã€æ›¿æ¢ï¼‰ã€‚ çŠ¶æ€å®šä¹‰ï¼š dp[i][j] = word1 å‰ i ä¸ªå­—ç¬¦è½¬æ¢æˆ word2 å‰ j ä¸ªå­—ç¬¦çš„æœ€å°‘æ“ä½œæ•° è½¬ç§»æ–¹ç¨‹ï¼š if word1[i-1] == word2[j-1]: dp[i][j] = dp[i-1][j-1] # ä¸éœ€è¦æ“ä½œelse: dp[i][j] = 1 + min( dp[i-1][j-1], # æ›¿æ¢ dp[i-1][j], # åˆ é™¤ dp[i][j-1] # æ’å…¥ ) def minDistance(word1: str, word2: str) - int: m, n = len(word1), len(word2) dp = [[0] * (n + 1) for _ in range(m + 1)] # è¾¹ç•Œæ¡ä»¶ for i in range(m + 1): dp[i][0] = i for j in range(n + 1): dp[0][j] = j for i in range(1, m + 1): for j in range(1, n + 1): if word1[i-1] == word2[j-1]: dp[i][j] = dp[i-1][j-1] else: dp[i][j] = 1 + min(dp[i-1][j-1], dp[i-1][j], dp[i][j-1]) return dp[m][n] å¤æ‚åº¦ï¼š æ—¶é—´ O(mn)ï¼Œç©ºé—´ O(mn) 5.7 0-1 èƒŒåŒ…é—®é¢˜é—®é¢˜ï¼š n ä¸ªç‰©å“ï¼Œæ¯ä¸ªæœ‰é‡é‡ w[i] å’Œä»·å€¼ v[i]ï¼ŒèƒŒåŒ…å®¹é‡ Wï¼Œæ±‚æœ€å¤§ä»·å€¼ã€‚ çŠ¶æ€å®šä¹‰ï¼š dp[i][w] = å‰ i ä¸ªç‰©å“ã€å®¹é‡ w æ—¶çš„æœ€å¤§ä»·å€¼ è½¬ç§»æ–¹ç¨‹ï¼š dp[i][w] = max( dp[i-1][w], # ä¸é€‰ç¬¬ i ä¸ª dp[i-1][w-w[i]] + v[i] # é€‰ç¬¬ i ä¸ªï¼ˆå¦‚æœè£…å¾—ä¸‹ï¼‰) def knapsack(weights, values, W): n = len(weights) dp = [[0] * (W + 1) for _ in range(n + 1)] for i in range(1, n + 1): for w in range(W + 1): dp[i][w] = dp[i-1][w] # ä¸é€‰ if w = weights[i-1]: dp[i][w] = max(dp[i][w], dp[i-1][w-weights[i-1]] + values[i-1]) return dp[n][W] ç©ºé—´ä¼˜åŒ–ç‰ˆæœ¬ï¼š def knapsack(weights, values, W): dp = [0] * (W + 1) for i in range(len(weights)): for w in range(W, weights[i] - 1, -1): # ä»å³å¾€å·¦ï¼ dp[w] = max(dp[w], dp[w - weights[i]] + values[i]) return dp[W] ä¸ºä»€ä¹ˆè¦ä»å³å¾€å·¦ï¼Ÿ å› ä¸º dp[w] ä¾èµ– dp[w-weights[i]]ï¼Œå¦‚æœä»å·¦å¾€å³ï¼Œdp[w-weights[i]] å·²ç»è¢«æ›´æ–°è¿‡äº†ï¼Œç›¸å½“äºåŒä¸€ä¸ªç‰©å“è¢«é€‰äº†å¤šæ¬¡ï¼ˆå˜æˆå®Œå…¨èƒŒåŒ…äº†ï¼‰ã€‚ å¤æ‚åº¦ï¼š æ—¶é—´ O(nW)ï¼Œç©ºé—´ O(W) 5.8 å®Œå…¨èƒŒåŒ…é—®é¢˜é—®é¢˜ï¼š æ¯ä¸ªç‰©å“å¯ä»¥é€‰æ— é™æ¬¡ã€‚ å’Œ 0-1 èƒŒåŒ…çš„åŒºåˆ«ï¼š éå†é¡ºåºä»å·¦å¾€å³ï¼ def unboundedKnapsack(weights, values, W): dp = [0] * (W + 1) for i in range(len(weights)): for w in range(weights[i], W + 1): # ä»å·¦å¾€å³ï¼ dp[w] = max(dp[w], dp[w - weights[i]] + values[i]) return dp[W] 5.9 é›¶é’±å…‘æ¢ (LeetCode 322)é—®é¢˜ï¼š ç”¨æœ€å°‘çš„ç¡¬å¸å‡‘å‡ºé‡‘é¢ amountã€‚ çŠ¶æ€å®šä¹‰ï¼š dp[i] = å‡‘å‡ºé‡‘é¢ i éœ€è¦çš„æœ€å°‘ç¡¬å¸æ•° è½¬ç§»æ–¹ç¨‹ï¼š dp[i] = min(dp[i - coin] + 1) for all coins def coinChange(coins: list[int], amount: int) - int: dp = [float('inf')] * (amount + 1) dp[0] = 0 for i in range(1, amount + 1): for coin in coins: if coin = i and dp[i - coin] != float('inf'): dp[i] = min(dp[i], dp[i - coin] + 1) return dp[amount] if dp[amount] != float('inf') else -1 å¤æ‚åº¦ï¼š æ—¶é—´ O(amount Ã— n)ï¼Œç©ºé—´ O(amount) 5.10 ä¸åŒè·¯å¾„ (LeetCode 62)é—®é¢˜ï¼š mÃ—n ç½‘æ ¼ï¼Œä»å·¦ä¸Šåˆ°å³ä¸‹ï¼Œåªèƒ½å‘å³æˆ–å‘ä¸‹ï¼Œæœ‰å¤šå°‘æ¡è·¯å¾„ï¼Ÿ çŠ¶æ€å®šä¹‰ï¼š dp[i][j] = åˆ°è¾¾ (i,j) çš„è·¯å¾„æ•° è½¬ç§»æ–¹ç¨‹ï¼š dp[i][j] = dp[i-1][j] + dp[i][j-1] def uniquePaths(m: int, n: int) - int: dp = [1] * n for _ in range(1, m): for j in range(1, n): dp[j] += dp[j-1] return dp[n-1] å¤æ‚åº¦ï¼š æ—¶é—´ O(mn)ï¼Œç©ºé—´ O(n) å…­ã€DP é—®é¢˜åˆ†ç±» 6.1 çº¿æ€§ DPç‰¹ç‚¹ï¼šçŠ¶æ€æ²¿ç€ä¸€ä¸ªç»´åº¦çº¿æ€§é€’æ¨ é¢˜ç›® çŠ¶æ€å®šä¹‰ è½¬ç§»æ–¹ç¨‹ çˆ¬æ¥¼æ¢¯ dp[i]=åˆ°ç¬¬ié˜¶æ–¹æ³•æ•° dp[i]=dp[i-1]+dp[i-2] æ‰“å®¶åŠ«èˆ dp[i]=å‰iå®¶æœ€å¤§é‡‘é¢ dp[i]=max(dp[i-1], dp[i-2]+nums[i]) æœ€å¤§å­æ•°ç»„å’Œ dp[i]=ä»¥iç»“å°¾çš„æœ€å¤§å’Œ dp[i]=max(nums[i], dp[i-1]+nums[i]) LIS dp[i]=ä»¥iç»“å°¾çš„LISé•¿åº¦ dp[i]=max(dp[j]+1) for ji 6.2 åºåˆ— DPï¼ˆåŒåºåˆ—ï¼‰ç‰¹ç‚¹ï¼šä¸¤ä¸ªåºåˆ—ä¹‹é—´çš„å…³ç³» é¢˜ç›® çŠ¶æ€å®šä¹‰ å…³é”®ç‚¹ LCS dp[i][j]=s1å‰iå’Œs2å‰jçš„LCS å­—ç¬¦ç›¸ç­‰æ—¶+1 ç¼–è¾‘è·ç¦» dp[i][j]=æœ€å°‘æ“ä½œæ•° ä¸‰ç§æ“ä½œå–min ä¸åŒå­åºåˆ— dp[i][j]=sä¸­tå‡ºç°æ¬¡æ•° é€‰æˆ–ä¸é€‰å½“å‰å­—ç¬¦ 6.3 èƒŒåŒ… DPç‰¹ç‚¹ï¼šé€‰æ‹©ç‰©å“ï¼Œæ»¡è¶³çº¦æŸï¼Œä¼˜åŒ–ç›®æ ‡ ç±»å‹ ç‰¹ç‚¹ éå†é¡ºåº 0-1èƒŒåŒ… æ¯ä¸ªç‰©å“æœ€å¤šé€‰1æ¬¡ å®¹é‡ä»å¤§åˆ°å° å®Œå…¨èƒŒåŒ… æ¯ä¸ªç‰©å“å¯é€‰æ— é™æ¬¡ å®¹é‡ä»å°åˆ°å¤§ å¤šé‡èƒŒåŒ… æ¯ä¸ªç‰©å“æœ‰æ•°é‡é™åˆ¶ äºŒè¿›åˆ¶ä¼˜åŒ– 6.4 åŒºé—´ DPç‰¹ç‚¹ï¼šåœ¨åŒºé—´ä¸Šè¿›è¡Œå†³ç­– # åŒºé—´DPæ¨¡æ¿for length in range(2, n + 1): # æšä¸¾åŒºé—´é•¿åº¦ for i in range(n - length + 1): # æšä¸¾èµ·ç‚¹ j = i + length - 1 # è®¡ç®—ç»ˆç‚¹ for k in range(i, j): # æšä¸¾åˆ†å‰²ç‚¹ dp[i][j] = min(dp[i][j], dp[i][k] + dp[k+1][j] + cost) ç»å…¸é¢˜ç›®ï¼šæˆ³æ°”çƒã€çŸ³å­åˆå¹¶ã€çŸ©é˜µé“¾ä¹˜æ³• 6.5 æ ‘å½¢ DPç‰¹ç‚¹ï¼šåœ¨æ ‘ç»“æ„ä¸Šè¿›è¡Œ DP def dfs(node): if not node: return 0 left = dfs(node.left) right = dfs(node.right) # æ ¹æ®å­æ ‘ç»“æœè®¡ç®—å½“å‰èŠ‚ç‚¹ return f(left, right, node.val) ç»å…¸é¢˜ç›®ï¼šæ‰“å®¶åŠ«èˆIIIã€äºŒå‰æ ‘æœ€å¤§è·¯å¾„å’Œ 6.6 çŠ¶æ€å‹ç¼© DPç‰¹ç‚¹ï¼šç”¨äºŒè¿›åˆ¶è¡¨ç¤ºçŠ¶æ€é›†åˆ # æšä¸¾æ‰€æœ‰å­é›†for mask in range(1 n): for i in range(n): if mask (1 i): # ç¬¬iä½æ˜¯å¦ä¸º1 # å¤„ç† ç»å…¸é¢˜ç›®ï¼šæ—…è¡Œå•†é—®é¢˜ TSP ä¸ƒã€æ—¶é—´ç©ºé—´å¤æ‚åº¦åˆ†æ7.1 æ—¶é—´å¤æ‚åº¦å…¬å¼ï¼šçŠ¶æ€æ•°é‡ Ã— æ¯ä¸ªçŠ¶æ€çš„è½¬ç§»ä»£ä»· é—®é¢˜ çŠ¶æ€æ•° è½¬ç§»ä»£ä»· æ€»å¤æ‚åº¦ æ–æ³¢é‚£å¥‘ O(n) O(1) O(n) LIS (æœ´ç´ ) O(n) O(n) O(nÂ²) LCS O(mn) O(1) O(mn) 0-1èƒŒåŒ… O(nW) O(1) O(nW) åŒºé—´DP O(nÂ²) O(n) O(nÂ³) 7.2 ç©ºé—´å¤æ‚åº¦åŸºæœ¬ç©ºé—´ = DP æ•°ç»„å¤§å° ä¼˜åŒ–æŠ€å·§ï¼š æ»šåŠ¨æ•°ç»„ï¼šO(n) â†’ O(1) é™ç»´ï¼šO(mn) â†’ O(n) åªä¿ç•™å¿…è¦çŠ¶æ€ å…«ã€DP è§£é¢˜æ¨¡æ¿ 8.1 äº”æ­¥æ³•def dp_template(input): # Step 1: å®šä¹‰çŠ¶æ€ # dp[i] = ??? # Step 2: åˆå§‹åŒ–ï¼ˆè¾¹ç•Œæ¡ä»¶ï¼‰ dp = [0] * n dp[0] = base_case # Step 3: ç¡®å®šéå†é¡ºåº for i in range(1, n): # Step 4: çŠ¶æ€è½¬ç§» dp[i] = f(dp[i-1], ...) # Step 5: è¿”å›ç»“æœ return dp[n-1] 8.2 Debug æŠ€å·§ æ‰“å° DP æ•°ç»„ï¼šçœ‹ä¸­é—´çŠ¶æ€æ˜¯å¦æ­£ç¡® æ‰‹ç®—å°ä¾‹å­ï¼šn=3,4 æ—¶æ‰‹åŠ¨éªŒè¯ æ£€æŸ¥è¾¹ç•Œï¼ši=0, j=0 çš„æƒ…å†µ æ£€æŸ¥éå†é¡ºåºï¼šä¾èµ–çš„çŠ¶æ€æ˜¯å¦å·²è®¡ç®— ä¹ã€LeetCode ç»å…¸é¢˜ç›®ç»ƒä¹ 9.1 å…¥é—¨çº§ï¼ˆEasyï¼‰ é¢˜å· é¢˜ç›® æ ¸å¿ƒæ€è·¯ 70 çˆ¬æ¥¼æ¢¯ dp[i]=dp[i-1]+dp[i-2] 121 ä¹°å–è‚¡ç¥¨æœ€ä½³æ—¶æœº è®°å½•æœ€å°ä»·æ ¼ï¼Œæ›´æ–°æœ€å¤§åˆ©æ¶¦ 53 æœ€å¤§å­æ•°ç»„å’Œ dp[i]=max(nums[i], dp[i-1]+nums[i]) 746 ä½¿ç”¨æœ€å°èŠ±è´¹çˆ¬æ¥¼æ¢¯ dp[i]=min(dp[i-1]+cost[i-1], dp[i-2]+cost[i-2]) 9.2 ä¸­ç­‰çº§ï¼ˆMediumï¼‰ é¢˜å· é¢˜ç›® æ ¸å¿ƒæ€è·¯ 198 æ‰“å®¶åŠ«èˆ dp[i]=max(dp[i-1], dp[i-2]+nums[i]) 300 æœ€é•¿é€’å¢å­åºåˆ— dp[i]=max(dp[j]+1) 322 é›¶é’±å…‘æ¢ å®Œå…¨èƒŒåŒ…å˜å½¢ 62 ä¸åŒè·¯å¾„ dp[i][j]=dp[i-1][j]+dp[i][j-1] 64 æœ€å°è·¯å¾„å’Œ åŒä¸Šï¼Œå–min 139 å•è¯æ‹†åˆ† dp[i]=any(dp[j] and s[j:i] in dict) 152 ä¹˜ç§¯æœ€å¤§å­æ•°ç»„ åŒæ—¶ç»´æŠ¤æœ€å¤§å’Œæœ€å° 1143 æœ€é•¿å…¬å…±å­åºåˆ— åŒåºåˆ—DPç»å…¸ 9.3 å›°éš¾çº§ï¼ˆHardï¼‰ é¢˜å· é¢˜ç›® æ ¸å¿ƒæ€è·¯ 72 ç¼–è¾‘è·ç¦» ä¸‰ç§æ“ä½œå–min 312 æˆ³æ°”çƒ åŒºé—´DP 10 æ­£åˆ™è¡¨è¾¾å¼åŒ¹é… åŒåºåˆ—DP + ç‰¹æ®Šå­—ç¬¦å¤„ç† 32 æœ€é•¿æœ‰æ•ˆæ‹¬å· dp[i]=ä»¥iç»“å°¾çš„æœ€é•¿æœ‰æ•ˆé•¿åº¦ 115 ä¸åŒçš„å­åºåˆ— dp[i][j]=så‰iä¸ªä¸­tå‰jä¸ªå‡ºç°æ¬¡æ•° åã€è´¹æ›¼æ€»ç»“ï¼šç”¨ä¸€å¥è¯è§£é‡Š DP10.1 ç»™å°å­¦ç”Ÿè§£é‡Šâ€œåšä½œä¸šæ—¶ï¼ŒæŠŠç®—è¿‡çš„é¢˜ç›®ç­”æ¡ˆè®°åœ¨è‰ç¨¿çº¸ä¸Šï¼Œä¸‹æ¬¡é‡åˆ°ä¸€æ ·çš„é¢˜ç›´æ¥æŠ„ç­”æ¡ˆï¼Œä¸ç”¨é‡æ–°ç®—ã€‚â€ 10.2 ç»™ç¨‹åºå‘˜è§£é‡Šâ€œç”¨å“ˆå¸Œè¡¨ç¼“å­˜é€’å½’çš„ä¸­é—´ç»“æœï¼Œæˆ–è€…ç”¨æ•°ç»„ä»å°åˆ°å¤§è¿­ä»£å¡«è¡¨ã€‚â€ 10.3 ç»™é¢è¯•å®˜è§£é‡Šâ€œDP æ˜¯ä¸€ç§é€šè¿‡å°†é—®é¢˜åˆ†è§£ä¸ºé‡å å­é—®é¢˜ï¼Œå¹¶å­˜å‚¨å­é—®é¢˜çš„è§£æ¥é¿å…é‡å¤è®¡ç®—çš„ä¼˜åŒ–æŠ€æœ¯ã€‚å®ƒé€‚ç”¨äºå…·æœ‰æœ€ä¼˜å­ç»“æ„å’Œé‡å å­é—®é¢˜æ€§è´¨çš„é—®é¢˜ã€‚â€ 10.4 æ ¸å¿ƒå…¬å¼DP = é€’å½’ + è®°å¿†åŒ– = åˆ†æ²» + ç¼“å­˜ 10.5 åˆ¤æ–­æ˜¯å¦èƒ½ç”¨ DPé—®è‡ªå·±ä¸¤ä¸ªé—®é¢˜ï¼š æœ€ä¼˜å­ç»“æ„ï¼šå¤§é—®é¢˜çš„æœ€ä¼˜è§£èƒ½å¦ç”±å°é—®é¢˜çš„æœ€ä¼˜è§£æ¨å¯¼å‡ºæ¥ï¼Ÿ é‡å å­é—®é¢˜ï¼šåœ¨æ±‚è§£è¿‡ç¨‹ä¸­ï¼Œæ˜¯å¦ä¼šé‡å¤è®¡ç®—ç›¸åŒçš„å­é—®é¢˜ï¼Ÿ å¦‚æœä¸¤ä¸ªéƒ½æ˜¯ Yesï¼Œå°±å¯ä»¥ç”¨ DPï¼ åä¸€ã€å¸¸è§é”™è¯¯ä¸é™·é˜±11.1 çŠ¶æ€å®šä¹‰ä¸æ¸…é”™è¯¯ï¼š æ¨¡ç³Šåœ°å®šä¹‰ dp[i] æ˜¯â€ç¬¬ i ä¸ªçš„ç­”æ¡ˆâ€ æ­£ç¡®ï¼š æ˜ç¡®æ˜¯â€ä»¥ç¬¬ i ä¸ªç»“å°¾â€è¿˜æ˜¯â€å‰ i ä¸ªâ€ 11.2 è¾¹ç•Œæ¡ä»¶é—æ¼# é”™è¯¯ï¼šå¿˜è®°å¤„ç†ç©ºæ•°ç»„def maxSubArray(nums): dp = [0] * len(nums) # å¦‚æœ nums ä¸ºç©ºä¼šæŠ¥é”™ ...# æ­£ç¡®ï¼šå…ˆæ£€æŸ¥è¾¹ç•Œdef maxSubArray(nums): if not nums: return 0 ... 11.3 éå†é¡ºåºé”™è¯¯# 0-1èƒŒåŒ…ï¼šå¿…é¡»ä»å³å¾€å·¦for w in range(W, weight - 1, -1): # æ­£ç¡®for w in range(weight, W + 1): # é”™è¯¯ï¼å˜æˆå®Œå…¨èƒŒåŒ…äº† 11.4 è¿”å›å€¼æé”™# LISï¼šè¿”å› max(dp)ï¼Œä¸æ˜¯ dp[n-1]return max(dp) # æ­£ç¡®return dp[n-1] # é”™è¯¯ï¼ åäºŒã€è¿›é˜¶æŠ€å·§12.1 çŠ¶æ€å‹ç¼©å½“çŠ¶æ€æ˜¯ä¸€ä¸ªé›†åˆæ—¶ï¼Œç”¨äºŒè¿›åˆ¶æ•°è¡¨ç¤ºï¼š # è¡¨ç¤ºé€‰äº†å“ªäº›å…ƒç´ mask = 0b1011 # é€‰äº†ç¬¬0ã€1ã€3ä¸ªå…ƒç´ # æ£€æŸ¥ç¬¬iä½if mask (1 i): ...# è®¾ç½®ç¬¬iä½mask |= (1 i)# æ¸…é™¤ç¬¬iä½mask = ~(1 i) 12.2 å•è°ƒé˜Ÿåˆ—ä¼˜åŒ–å½“è½¬ç§»æ–¹ç¨‹å½¢å¦‚ dp[i] = max(dp[j]) + cost ä¸” j åœ¨æ»‘åŠ¨çª—å£å†…æ—¶ï¼š from collections import dequedef solve(nums, k): q = deque() # å­˜å‚¨ä¸‹æ ‡ï¼Œä¿æŒå•è°ƒé€’å‡ dp = [0] * n for i in range(n): while q and q[0] i - k: q.popleft() dp[i] = nums[q[0]] + cost if q else cost while q and nums[q[-1]] = nums[i]: q.pop() q.append(i) 12.3 æ–œç‡ä¼˜åŒ–å½“è½¬ç§»æ–¹ç¨‹å½¢å¦‚ dp[i] = min(dp[j] + f(i,j)) ä¸” f å¯ä»¥åˆ†ç¦»å˜é‡æ—¶ï¼Œç”¨å‡¸åŒ…ä¼˜åŒ–åˆ° O(n)ã€‚ åä¸‰ã€å®æˆ˜ä»£ç æ¨¡æ¿æ±‡æ€»13.1 çº¿æ€§ DP æ¨¡æ¿def linear_dp(nums): n = len(nums) if n == 0: return 0 dp = [0] * n dp[0] = nums[0] # base case for i in range(1, n): dp[i] = f(dp[i-1], nums[i]) return dp[n-1] # æˆ– max(dp) 13.2 åŒåºåˆ— DP æ¨¡æ¿def two_seq_dp(s1, s2): m, n = len(s1), len(s2) dp = [[0] * (n+1) for _ in range(m+1)] # è¾¹ç•Œåˆå§‹åŒ– for i in range(m+1): dp[i][0] = init_val for j in range(n+1): dp[0][j] = init_val for i in range(1, m+1): for j in range(1, n+1): if s1[i-1] == s2[j-1]: dp[i][j] = dp[i-1][j-1] + 1 else: dp[i][j] = max(dp[i-1][j], dp[i][j-1]) return dp[m][n] 13.3 èƒŒåŒ… DP æ¨¡æ¿# 0-1 èƒŒåŒ…def knapsack_01(weights, values, W): dp = [0] * (W + 1) for i in range(len(weights)): for w in range(W, weights[i]-1, -1): # ä»å³å¾€å·¦ dp[w] = max(dp[w], dp[w-weights[i]] + values[i]) return dp[W]# å®Œå…¨èƒŒåŒ…def knapsack_complete(weights, values, W): dp = [0] * (W + 1) for i in range(len(weights)): for w in range(weights[i], W+1): # ä»å·¦å¾€å³ dp[w] = max(dp[w], dp[w-weights[i]] + values[i]) return dp[W] 13.4 åŒºé—´ DP æ¨¡æ¿def interval_dp(nums): n = len(nums) dp = [[0] * n for _ in range(n)] # é•¿åº¦ä»å°åˆ°å¤§ for length in range(2, n+1): for i in range(n - length + 1): j = i + length - 1 dp[i][j] = float('inf') for k in range(i, j): dp[i][j] = min(dp[i][j], dp[i][k] + dp[k+1][j] + cost(i,j)) return dp[0][n-1] åå››ã€é¢è¯•é«˜é¢‘é—®é¢˜é€ŸæŸ¥14.1 è‚¡ç¥¨ä¹°å–ç³»åˆ— é¢˜å· é™åˆ¶ çŠ¶æ€å®šä¹‰ 121 åªèƒ½ä¹°å–1æ¬¡ è®°å½•æœ€å°ä»·æ ¼ 122 æ— é™æ¬¡ è´ªå¿ƒï¼šæ‰€æœ‰ä¸Šæ¶¨éƒ½åƒ 123 æœ€å¤š2æ¬¡ dp[i][k][0/1] 188 æœ€å¤škæ¬¡ åŒä¸Š 309 æœ‰å†·å†»æœŸ å¢åŠ å†·å†»çŠ¶æ€ 714 æœ‰æ‰‹ç»­è´¹ å–å‡ºæ—¶å‡fee 14.2 å­åºåˆ—ç³»åˆ— é¢˜ç›® å…³é”®åŒºåˆ« å­åºåˆ— å¯ä»¥ä¸è¿ç»­ å­æ•°ç»„ å¿…é¡»è¿ç»­ å­ä¸² å¿…é¡»è¿ç»­ï¼ˆå­—ç¬¦ä¸²ç‰ˆï¼‰ 14.3 è·¯å¾„ç³»åˆ— é¢˜å· å˜å½¢ 62 åŸºç¡€ç‰ˆ 63 æœ‰éšœç¢ç‰© 64 å¸¦æƒé‡ï¼ˆæœ€å°è·¯å¾„å’Œï¼‰ 120 ä¸‰è§’å½¢ 931 ä¸‹é™è·¯å¾„æœ€å°å’Œ åäº”ã€æ€»ç»“æ€ç»´å¯¼å›¾åŠ¨æ€è§„åˆ’â”œâ”€â”€ æ ¸å¿ƒæ€æƒ³â”‚ â”œâ”€â”€ æœ€ä¼˜å­ç»“æ„â”‚ â”œâ”€â”€ é‡å å­é—®é¢˜â”‚ â””â”€â”€ ç©ºé—´æ¢æ—¶é—´â”œâ”€â”€ ä¸‰è¦ç´ â”‚ â”œâ”€â”€ çŠ¶æ€å®šä¹‰ dp[i] = ?â”‚ â”œâ”€â”€ è½¬ç§»æ–¹ç¨‹ dp[i] = f(dp[...])â”‚ â””â”€â”€ è¾¹ç•Œæ¡ä»¶ dp[0] = ?â”œâ”€â”€ å®ç°æ–¹å¼â”‚ â”œâ”€â”€ Top-Down è®°å¿†åŒ–é€’å½’â”‚ â””â”€â”€ Bottom-Up è¿­ä»£å¡«è¡¨â”œâ”€â”€ ä¼˜åŒ–æŠ€å·§â”‚ â”œâ”€â”€ æ»šåŠ¨æ•°ç»„â”‚ â”œâ”€â”€ é™ç»´â”‚ â””â”€â”€ å•è°ƒé˜Ÿåˆ—/æ–œç‡ä¼˜åŒ–â””â”€â”€ é—®é¢˜åˆ†ç±» â”œâ”€â”€ çº¿æ€§DP â”œâ”€â”€ åºåˆ—DP â”œâ”€â”€ èƒŒåŒ…DP â”œâ”€â”€ åŒºé—´DP â”œâ”€â”€ æ ‘å½¢DP â””â”€â”€ çŠ¶å‹DP å‚è€ƒèµ„æ–™ ã€Šç®—æ³•å¯¼è®ºã€‹ç¬¬15ç«  - åŠ¨æ€è§„åˆ’ LeetCode åŠ¨æ€è§„åˆ’ä¸“é¢˜ OI Wiki - åŠ¨æ€è§„åˆ’ ğŸ’¡ è´¹æ›¼å­¦ä¹ æ³•æ ¸å¿ƒï¼šå¦‚æœä½ èƒ½æŠŠ DP è§£é‡Šç»™ä¸€ä¸ªå®Œå…¨ä¸æ‡‚ç¼–ç¨‹çš„äººå¬ï¼Œä½ å°±çœŸæ­£ç†è§£äº†å®ƒã€‚ è®°ä½ï¼šDP ä¸æ˜¯èƒŒæ¨¡æ¿ï¼Œè€Œæ˜¯ç†è§£â€å¦‚ä½•æŠŠå¤§é—®é¢˜æ‹†æˆå°é—®é¢˜ï¼Œå¹¶è®°ä½å°é—®é¢˜çš„ç­”æ¡ˆâ€ã€‚","tags":["åŠ¨æ€è§„åˆ’","DP","LeetCode","ç®—æ³•é¢è¯•"],"categories":["ç®—æ³•"]},{"title":"å•è°ƒæ ˆå®Œå…¨æŒ‡å—ï¼šä» O(nÂ²) åˆ° O(n) çš„ä¼˜é›…è·ƒè¿","path":"/2026/02/19/monotonic-stack-guide/","content":"å•è°ƒæ ˆå®Œå…¨æŒ‡å—ï¼šä» O(nÂ²) åˆ° O(n) çš„ä¼˜é›…è·ƒè¿ğŸ¯ ä¸€å¥è¯æœ¬è´¨ å•è°ƒæ ˆé€šè¿‡ç»´æŠ¤å•è°ƒæ€§æ¥åŠæ—¶æ·˜æ±°æ— ç”¨å€™é€‰å…ƒç´ ï¼Œå°†â€å¯»æ‰¾ä¸‹ä¸€ä¸ªæ›´å¤§/æ›´å°å…ƒç´ â€é—®é¢˜ä» O(nÂ²) ä¼˜åŒ–åˆ° O(n)ã€‚ å®ƒä¸æ˜¯ä¸€ç§æ•°æ®ç»“æ„ï¼Œè€Œæ˜¯ä¸€ç§éå†ç­–ç•¥ â€”â€” é€šè¿‡æ ˆçš„å•è°ƒæ€§ä¿è¯â€ä¸æ¼æ‰ç­”æ¡ˆâ€çš„åŒæ—¶â€è·³è¿‡ä¸å¯èƒ½çš„åŒºåŸŸâ€ã€‚ ğŸ¤” ä¸ºä»€ä¹ˆéœ€è¦å•è°ƒæ ˆï¼Ÿé—®é¢˜åœºæ™¯ç»™å®šæ•°ç»„ [2, 1, 5, 6, 2, 3]ï¼Œå¯¹äºæ¯ä¸ªå…ƒç´ ï¼Œæ‰¾åˆ°å®ƒå³è¾¹ç¬¬ä¸€ä¸ªæ¯”å®ƒå¤§çš„å…ƒç´ ã€‚ æš´åŠ›è§£æ³•ï¼š def next_greater_brute(nums): n = len(nums) result = [-1] * n for i in range(n): for j in range(i+1, n): # å‘å³æ‰«æ if nums[j] nums[i]: result[i] = nums[j] break return result æ—¶é—´å¤æ‚åº¦ï¼šO(nÂ²) - æ¯ä¸ªå…ƒç´ éƒ½è¦å‘å³æ‰«æ é—®é¢˜ï¼šå¤§é‡é‡å¤è®¡ç®— ğŸ’¡ å…³é”®æ´å¯Ÿå¦‚æœ nums[i] nums[j]ï¼ˆi jï¼‰ï¼Œé‚£ä¹ˆå¯¹äº j å³è¾¹çš„å…ƒç´ æ¥è¯´ï¼Œi æ°¸è¿œä¸å¯èƒ½æ˜¯ç­”æ¡ˆï¼ ä¸ºä»€ä¹ˆï¼Ÿ å› ä¸º j æ›´å¤§ä¸”æ›´é å³ï¼Œå®ƒä¼šâ€é®æŒ¡â€ä½ iã€‚ è¿™å°±æ˜¯å•è°ƒæ ˆçš„æ ¸å¿ƒï¼šç»´æŠ¤ä¸€ä¸ªé€’å‡çš„å€™é€‰åºåˆ—ï¼ŒåŠæ—¶æ·˜æ±°æ— ç”¨å…ƒç´ ã€‚ ğŸ”¬ å·¥ä½œåŸç†ï¼šæ•°æ®æµä¸çŠ¶æ€è½¬ç§»å•è°ƒæ ˆçš„å¤„ç†æµç¨‹ä»¥ [2, 1, 5, 6, 2, 3] ä¸ºä¾‹ï¼Œç»´æŠ¤ä¸€ä¸ªå•è°ƒé€’å‡æ ˆï¼ˆæ ˆåº•åˆ°æ ˆé¡¶é€’å‡ï¼‰ï¼š æ­¥éª¤ å½“å‰å…ƒç´  æ ˆçŠ¶æ€ï¼ˆå­˜ç´¢å¼•ï¼‰ æ“ä½œ å‘ç°çš„ç­”æ¡ˆ 1 2 (i=0) [0] ç›´æ¥å…¥æ ˆ - 2 1 (i=1) [0, 1] 1 2ï¼Œç›´æ¥å…¥æ ˆ - 3 5 (i=2) [2] 5 1ï¼Œå¼¹å‡º1ï¼›5 2ï¼Œå¼¹å‡º2ï¼›5å…¥æ ˆ nums[1]=5, nums[0]=5 4 6 (i=3) [3] 6 5ï¼Œå¼¹å‡º5ï¼›6å…¥æ ˆ nums[2]=6 5 2 (i=4) [3, 4] 2 6ï¼Œç›´æ¥å…¥æ ˆ - 6 3 (i=5) [3, 5] 3 2ï¼Œå¼¹å‡º2ï¼›3 6ï¼Œ3å…¥æ ˆ nums[4]=3 æœ€ç»ˆç»“æœï¼š[5, 5, 6, -1, 3, -1] ğŸ“Š å¯è§†åŒ–ï¼šæ ˆçš„åŠ¨æ€æ¼”å˜ ä¸Šå›¾å±•ç¤ºäº†å¤„ç†æ•°ç»„çš„6ä¸ªå…³é”®æ­¥éª¤ï¼Œæ¯ä¸ªæ­¥éª¤åŒ…å«ï¼š è¾“å…¥æ•°ç»„ï¼šå½“å‰å¤„ç†çš„å…ƒç´ ç”¨çº¢è‰²é«˜äº® æ ˆçŠ¶æ€ï¼šç»¿è‰²æ–¹å—è¡¨ç¤ºæ ˆä¸­çš„å…ƒç´  ç»“æœæ•°ç»„ï¼šé»„è‰²è¡¨ç¤ºå·²æ‰¾åˆ°ç­”æ¡ˆï¼Œç°è‰²è¡¨ç¤ºæœªæ‰¾åˆ° ğŸ¬ åŠ¨ç”»æ¼”ç¤º åŠ¨ç”»å±•ç¤ºäº†å®Œæ•´çš„å¤„ç†æµç¨‹ï¼Œå¯ä»¥æ¸…æ™°çœ‹åˆ°ï¼š å…ƒç´ é€ä¸ªè¿›å…¥å¤„ç† æ ˆçš„åŠ¨æ€å˜åŒ–ï¼ˆå…¥æ ˆ/å‡ºæ ˆï¼‰ ç»“æœæ•°ç»„çš„å®æ—¶æ›´æ–° ğŸ’» æ ¸å¿ƒä»£ç å®ç°def next_greater_element(nums): n = len(nums) result = [-1] * n # åˆå§‹åŒ–ç­”æ¡ˆæ•°ç»„ stack = [] # å•è°ƒæ ˆï¼ˆå­˜å‚¨ç´¢å¼•ï¼‰ for i in range(n): # å½“å‰å…ƒç´  æ ˆé¡¶å…ƒç´ æ—¶ï¼Œæ‰¾åˆ°äº†æ ˆé¡¶çš„ç­”æ¡ˆ while stack and nums[i] nums[stack[-1]]: idx = stack.pop() result[idx] = nums[i] # è®°å½•ç­”æ¡ˆ stack.append(i) # å½“å‰ç´¢å¼•å…¥æ ˆ return result æ•°æ®æµå‘ï¼š è¾“å…¥æ•°ç»„ â†’ é€ä¸ªå¤„ç† â†’ å•è°ƒæ ˆï¼ˆç»´æŠ¤å€™é€‰ï¼‰ â†’ è¾“å‡ºç­”æ¡ˆæ•°ç»„ â†‘ â†“ â””â”€â”€â”€â”€â”€â”€ å¼¹å‡ºæ—¶è®°å½•ç­”æ¡ˆ â”€â”€â”˜ â±ï¸ æ—¶é—´å¤æ‚åº¦è¯æ˜ ä¸ºä»€ä¹ˆæ˜¯ O(n)ï¼Ÿå…³é”®æ´å¯Ÿï¼šæ¯ä¸ªå…ƒç´ æœ€å¤šå…¥æ ˆä¸€æ¬¡ï¼Œå‡ºæ ˆä¸€æ¬¡ã€‚ è¯æ˜ï¼š å¤–å±‚å¾ªç¯ï¼šéå† n ä¸ªå…ƒç´  â†’ O(n) å†…å±‚ while å¾ªç¯ï¼šçœ‹ä¼¼åµŒå¥—ï¼Œä½†æ€»å…±æœ€å¤šå¼¹å‡º n æ¬¡ æ¯ä¸ªå…ƒç´ å…¥æ ˆ 1 æ¬¡ æ¯ä¸ªå…ƒç´ å‡ºæ ˆ â‰¤ 1 æ¬¡ æ€»æ“ä½œæ¬¡æ•° â‰¤ 2n å‡æ‘Šåˆ†æï¼š æ€»å…¥æ ˆæ¬¡æ•° = næ€»å‡ºæ ˆæ¬¡æ•° â‰¤ næ€»æ“ä½œ = n + n = 2n = O(n) ğŸ¯ ç»å…¸é—®é¢˜å®æˆ˜é—®é¢˜ 1ï¼šä¸‹ä¸€ä¸ªæ›´å¤§å…ƒç´  IIï¼ˆå¾ªç¯æ•°ç»„ï¼‰é—®é¢˜ï¼šæ•°ç»„æ˜¯å¾ªç¯çš„ï¼Œå¦‚ [1, 2, 1] ä¸­ï¼Œæœ€åä¸€ä¸ª 1 çš„ç­”æ¡ˆæ˜¯ 2ã€‚ è§£æ³•ï¼šå°†æ•°ç»„å¤åˆ¶ä¸€éï¼Œæ¨¡æ‹Ÿå¾ªç¯ã€‚ def next_greater_circular(nums): n = len(nums) result = [-1] * n stack = [] # éå†ä¸¤éæ•°ç»„ï¼ˆæ¨¡æ‹Ÿå¾ªç¯ï¼‰ for i in range(2 * n): idx = i % n # å®é™…ç´¢å¼• while stack and nums[idx] nums[stack[-1]]: result[stack.pop()] = nums[idx] if i n: # åªåœ¨ç¬¬ä¸€éæ—¶å…¥æ ˆ stack.append(idx) return result é—®é¢˜ 2ï¼šæŸ±çŠ¶å›¾ä¸­æœ€å¤§çš„çŸ©å½¢é—®é¢˜ï¼šç»™å®šæŸ±çŠ¶å›¾é«˜åº¦ [2, 1, 5, 6, 2, 3]ï¼Œæ‰¾æœ€å¤§çŸ©å½¢é¢ç§¯ã€‚ æ ¸å¿ƒæ€è·¯ï¼šå¯¹äºæ¯ä¸ªæŸ±å­ï¼Œæ‰¾åˆ°å®ƒå·¦å³ä¸¤è¾¹ç¬¬ä¸€ä¸ªæ¯”å®ƒçŸ®çš„æŸ±å­ï¼Œè®¡ç®—ä»¥å®ƒä¸ºé«˜çš„çŸ©å½¢é¢ç§¯ã€‚ def largest_rectangle_area(heights): stack = [] max_area = 0 heights = [0] + heights + [0] # å“¨å…µæŠ€å·§ for i, h in enumerate(heights): while stack and h heights[stack[-1]]: height_idx = stack.pop() height = heights[height_idx] width = i - stack[-1] - 1 # å·¦å³è¾¹ç•Œ max_area = max(max_area, height * width) stack.append(i) return max_area ä¸Šå›¾å±•ç¤ºäº†æŸ±çŠ¶å›¾ä¸­æœ€å¤§çŸ©å½¢çš„å¯è§†åŒ–ï¼Œçº¢è‰²è™šçº¿æ¡†æ ‡å‡ºäº†é¢ç§¯ä¸º10çš„æœ€å¤§çŸ©å½¢ã€‚ é—®é¢˜ 3ï¼šæ¯æ—¥æ¸©åº¦é—®é¢˜ï¼šç»™å®šæ¸©åº¦æ•°ç»„ [73, 74, 75, 71, 69, 72, 76, 73]ï¼Œè®¡ç®—æ¯å¤©éœ€è¦ç­‰å‡ å¤©æ‰èƒ½ç­‰åˆ°æ›´æš–å’Œçš„æ¸©åº¦ã€‚ def daily_temperatures(temperatures): n = len(temperatures) result = [0] * n stack = [] for i, temp in enumerate(temperatures): while stack and temp temperatures[stack[-1]]: idx = stack.pop() result[idx] = i - idx # å¤©æ•°å·® stack.append(i) return result è¾“å‡ºï¼š[1, 1, 4, 2, 1, 1, 0, 0] ğŸ§  è´¹æ›¼æ€»ç»“ï¼šç”¨ç®€å•çš„è¯è§£é‡Šå•è°ƒæ ˆç»™10å²å°å­©çš„è§£é‡Šæƒ³è±¡ä½ åœ¨æ’é˜Ÿä¹°ç¥¨ï¼Œé˜Ÿä¼é‡Œæ¯ä¸ªäººéƒ½æœ‰ä¸€ä¸ªèº«é«˜ç‰Œã€‚ä½ çš„ä»»åŠ¡æ˜¯ï¼šå‘Šè¯‰æ¯ä¸ªäººï¼Œä»–åé¢ç¬¬ä¸€ä¸ªæ¯”ä»–é«˜çš„äººæ˜¯è°ã€‚ ç¬¨åŠæ³•ï¼šæ¯ä¸ªäººéƒ½è¦å›å¤´çœ‹ä¸€éåé¢æ‰€æœ‰äºº â†’ å¾ˆæ…¢ï¼ˆO(nÂ²)ï¼‰ èªæ˜åŠæ³•ï¼ˆå•è°ƒæ ˆï¼‰ï¼š ç»´æŠ¤ä¸€ä¸ªâ€å€™é€‰é˜Ÿåˆ—â€ï¼Œé˜Ÿåˆ—é‡Œçš„äººä»å‰åˆ°åè¶Šæ¥è¶ŠçŸ® æ–°äººæ¥äº†ï¼Œå¦‚æœä»–æ¯”é˜Ÿå°¾çš„äººé«˜ï¼š é˜Ÿå°¾çš„äººæ‰¾åˆ°ç­”æ¡ˆäº†ï¼ï¼ˆå°±æ˜¯è¿™ä¸ªæ–°äººï¼‰ æŠŠé˜Ÿå°¾çš„äººè¸¢å‡ºå»ï¼ˆä»–å·²ç»æ²¡ç”¨äº†ï¼‰ ç»§ç»­æ¯”è¾ƒï¼Œç›´åˆ°æ–°äººä¸æ¯”é˜Ÿå°¾é«˜ æ–°äººåŠ å…¥é˜Ÿå°¾ ä¸ºä»€ä¹ˆè¿™æ ·å¿«ï¼Ÿ æ¯ä¸ªäººæœ€å¤šè¿›é˜Ÿä¸€æ¬¡ã€å‡ºé˜Ÿä¸€æ¬¡ â†’ O(n) æ ¸å¿ƒè¦ç‚¹ ä»€ä¹ˆæ—¶å€™ç”¨å•è°ƒæ ˆï¼Ÿ éœ€è¦æ‰¾â€ä¸‹ä¸€ä¸ªæ›´å¤§/æ›´å°å…ƒç´ â€ éœ€è¦æ‰¾â€å·¦å³è¾¹ç•Œâ€ æš´åŠ›è§£æ³•éœ€è¦åµŒå¥—å¾ªç¯ å•è°ƒæ ˆçš„æœ¬è´¨ ç»´æŠ¤ä¸€ä¸ªå€™é€‰åºåˆ— æ–°å…ƒç´ åˆ°æ¥æ—¶ï¼Œæ·˜æ±°è¢«â€é®æŒ¡â€çš„å…ƒç´  å¼¹å‡ºæ—¶è®°å½•ç­”æ¡ˆ ä¸ºä»€ä¹ˆæ˜¯ O(n)ï¼Ÿ æ¯ä¸ªå…ƒç´ æœ€å¤šå…¥æ ˆä¸€æ¬¡ï¼Œå‡ºæ ˆä¸€æ¬¡ æ€»æ“ä½œæ¬¡æ•° â‰¤ 2n å•è°ƒé€’å¢ vs å•è°ƒé€’å‡ å•è°ƒé€’å¢æ ˆï¼šæ‰¾ä¸‹ä¸€ä¸ªæ›´å°å…ƒç´  å•è°ƒé€’å‡æ ˆï¼šæ‰¾ä¸‹ä¸€ä¸ªæ›´å¤§å…ƒç´  è®°å¿†å£è¯€é‡åˆ°\"ä¸‹ä¸€ä¸ªæ›´å¤§\"ï¼Œå•è°ƒæ ˆæ¥å¸®å¿™æ ˆé¡¶å°äºå½“å‰å€¼ï¼Œå¼¹å‡ºè®°å½•ç­”æ¡ˆå¿™æ¯ä¸ªå…ƒç´ è¿›å‡ºä¸€æ¬¡ï¼Œæ—¶é—´å¤æ‚åº¦ O(n) å¼º ğŸ§  è´¹æ›¼æ€»ç»“ï¼šç”¨ç®€å•çš„è¯è§£é‡Šå•è°ƒæ ˆç»™10å²å°å­©è§£é‡Šæƒ³è±¡ä½ åœ¨æ’é˜Ÿä¹°å†°æ·‡æ·‹ï¼Œé˜Ÿä¼é‡Œæ¯ä¸ªäººéƒ½æœ‰ä¸åŒçš„èº«é«˜ã€‚ é—®é¢˜ï¼šæ¯ä¸ªäººéƒ½æƒ³çŸ¥é“ï¼Œè‡ªå·±åé¢ç¬¬ä¸€ä¸ªæ¯”è‡ªå·±é«˜çš„äººæ˜¯è°ï¼Ÿ ç¬¨åŠæ³•ï¼šæ¯ä¸ªäººéƒ½å›å¤´çœ‹ï¼Œä¸€ä¸ªä¸€ä¸ªæ‰¾ â†’ å¾ˆæ…¢ï¼ˆO(nÂ²)ï¼‰ èªæ˜åŠæ³•ï¼ˆå•è°ƒæ ˆï¼‰ï¼š ç»´æŠ¤ä¸€ä¸ªâ€å€™é€‰é˜Ÿåˆ—â€ï¼Œé˜Ÿåˆ—é‡Œçš„äººä»å‰åˆ°åè¶Šæ¥è¶ŠçŸ® æ–°äººæ¥äº†ï¼š å¦‚æœæ¯”é˜Ÿå°¾çš„äººçŸ® â†’ ç›´æ¥åŠ å…¥é˜Ÿåˆ— å¦‚æœæ¯”é˜Ÿå°¾çš„äººé«˜ â†’ é˜Ÿå°¾çš„äººæ‰¾åˆ°ç­”æ¡ˆäº†ï¼æŠŠä»–è¸¢å‡ºå»ï¼Œç»§ç»­æ¯”è¾ƒ æ¯ä¸ªäººæœ€å¤šè¿›å‡ºé˜Ÿåˆ—ä¸€æ¬¡ â†’ å¾ˆå¿«ï¼ˆO(n)ï¼‰ æ ¸å¿ƒè¦ç‚¹ ä»€ä¹ˆæ—¶å€™ç”¨å•è°ƒæ ˆï¼Ÿ éœ€è¦æ‰¾â€ä¸‹ä¸€ä¸ªæ›´å¤§/æ›´å°å…ƒç´ â€ éœ€è¦æ‰¾â€å·¦å³è¾¹ç•Œâ€ æš´åŠ›è§£æ³•éœ€è¦åµŒå¥—å¾ªç¯ å•è°ƒæ ˆçš„æœ¬è´¨ ç»´æŠ¤ä¸€ä¸ªå€™é€‰åºåˆ— æ–°å…ƒç´ åˆ°æ¥æ—¶ï¼Œæ·˜æ±°è¢«â€é®æŒ¡â€çš„å…ƒç´  å¼¹å‡ºæ—¶è®°å½•ç­”æ¡ˆ ä¸ºä»€ä¹ˆæ˜¯ O(n)ï¼Ÿ æ¯ä¸ªå…ƒç´ æœ€å¤šå…¥æ ˆä¸€æ¬¡ï¼Œå‡ºæ ˆä¸€æ¬¡ æ€»æ“ä½œæ¬¡æ•° â‰¤ 2n å•è°ƒé€’å¢ vs å•è°ƒé€’å‡ å•è°ƒé€’å¢æ ˆï¼šæ‰¾ä¸‹ä¸€ä¸ªæ›´å°å…ƒç´  å•è°ƒé€’å‡æ ˆï¼šæ‰¾ä¸‹ä¸€ä¸ªæ›´å¤§å…ƒç´  è®°å¿†å£è¯€é‡åˆ°\"ä¸‹ä¸€ä¸ªæ›´å¤§\"ï¼Œå•è°ƒæ ˆæ¥å¸®å¿™æ ˆé¡¶å°äºå½“å‰å€¼ï¼Œå¼¹å‡ºè®°å½•ç­”æ¡ˆå¿™æ¯ä¸ªå…ƒç´ è¿›å‡ºä¸€æ¬¡ï¼Œæ—¶é—´å¤æ‚åº¦ O(n) å¼º ğŸ“ ä»£ç æ¨¡æ¿æ‰¾ä¸‹ä¸€ä¸ªæ›´å¤§å…ƒç´ ï¼ˆå•è°ƒé€’å‡æ ˆï¼‰def next_greater(nums): result = [-1] * len(nums) stack = [] for i in range(len(nums)): while stack and nums[i] nums[stack[-1]]: result[stack.pop()] = nums[i] stack.append(i) return result æ‰¾ä¸‹ä¸€ä¸ªæ›´å°å…ƒç´ ï¼ˆå•è°ƒé€’å¢æ ˆï¼‰def next_smaller(nums): result = [-1] * len(nums) stack = [] for i in range(len(nums)): while stack and nums[i] nums[stack[-1]]: result[stack.pop()] = nums[i] stack.append(i) return result ğŸ“š LeetCode é¢˜å•åŸºç¡€é¢˜ï¼ˆEasyï¼‰ 496. ä¸‹ä¸€ä¸ªæ›´å¤§å…ƒç´  I 739. æ¯æ—¥æ¸©åº¦ è¿›é˜¶é¢˜ï¼ˆMediumï¼‰ 503. ä¸‹ä¸€ä¸ªæ›´å¤§å…ƒç´  II 901. è‚¡ç¥¨ä»·æ ¼è·¨åº¦ å›°éš¾é¢˜ï¼ˆHardï¼‰ 84. æŸ±çŠ¶å›¾ä¸­æœ€å¤§çš„çŸ©å½¢ 42. æ¥é›¨æ°´ 85. æœ€å¤§çŸ©å½¢ ğŸ“ è¿›é˜¶æ€è€ƒ1. å•è°ƒé˜Ÿåˆ— vs å•è°ƒæ ˆ å•è°ƒé˜Ÿåˆ—ï¼šæ”¯æŒé˜Ÿé¦–é˜Ÿå°¾æ“ä½œï¼Œç”¨äºæ»‘åŠ¨çª—å£æœ€å¤§å€¼ å•è°ƒæ ˆï¼šåªæ”¯æŒæ ˆé¡¶æ“ä½œï¼Œç”¨äºå¯»æ‰¾è¾¹ç•Œ 2. åŒå•è°ƒæ ˆæŸäº›é—®é¢˜éœ€è¦åŒæ—¶ç»´æŠ¤å·¦å³è¾¹ç•Œï¼Œå¯ä»¥ç”¨ä¸¤ä¸ªå•è°ƒæ ˆåˆ†åˆ«å¤„ç†ã€‚ 3. å•è°ƒæ ˆ + DPåœ¨æŸäº› DP é—®é¢˜ä¸­ï¼Œå•è°ƒæ ˆå¯ä»¥ä¼˜åŒ–çŠ¶æ€è½¬ç§»ã€‚ ğŸ”‘ å…³é”®è¦ç‚¹æ€»ç»“ ç‰¹æ€§ è¯´æ˜ é€‚ç”¨åœºæ™¯ æ‰¾ä¸‹ä¸€ä¸ªæ›´å¤§/æ›´å°å…ƒç´ ã€å·¦å³è¾¹ç•Œ æ—¶é—´å¤æ‚åº¦ O(n) ç©ºé—´å¤æ‚åº¦ O(n) æ ¸å¿ƒæ€æƒ³ ç»´æŠ¤å•è°ƒæ€§ï¼ŒåŠæ—¶æ·˜æ±°æ— ç”¨å…ƒç´  å­˜å‚¨å†…å®¹ é€šå¸¸å­˜ç´¢å¼•è€Œéå€¼ åˆ›å»ºæ—¶é—´ï¼š2026-02-19æ ‡ç­¾ï¼š#ç®—æ³• #å•è°ƒæ ˆ #æ•°æ®ç»“æ„ #ä¼˜åŒ–æŠ€å·§","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"åŒæŒ‡é’ˆï¼ˆTwo Pointersï¼‰Feynman é£æ ¼æ•™ç¨‹","path":"/2026/02/19/20260219-åŒæŒ‡é’ˆæ•™ç¨‹/","content":"åŒæŒ‡é’ˆï¼ˆTwo Pointersï¼‰â€” Feynman é£æ ¼æ•™ç¨‹ ç›®æ ‡ï¼šæŠŠâ€œä¸¤ä¸ªæŒ‡é’ˆåä½œâ€çš„æœºåˆ¶è®²æ¸…æ¥šï¼Œç»™å‡ºå¯è¿è¡Œçš„é¢˜å‹æ¨¡æ¿ã€å…·ä½“ I/O ç¤ºä¾‹ï¼Œå¹¶é…åˆå›¾åƒç†è§£ã€‚ 1. æ ¸å¿ƒç›´è§‰ï¼šä¸¤ä¸ªæŒ‡é’ˆ = ä¸¤ä¸ªåŒæ­¥çš„â€œæ¸¸æ ‡â€æŠŠæ•°ç»„å½“æˆä¸€æ¡ç›´çº¿ï¼Œä¸¤ä¸ªæŒ‡é’ˆå°±æ˜¯ä¸¤ä¸ªæ¸¸æ ‡ã€‚å®ƒä»¬ç”¨â€œå±€éƒ¨ä¿¡æ¯â€å†³å®šä¸‹ä¸€æ­¥åŠ¨ä½œï¼Œä»è€Œå‡å°‘æœç´¢ç©ºé—´ã€‚ å…³é”®æœºåˆ¶ï¼š æœ‰åºä¿¡æ¯ï¼ˆå•è°ƒæ€§ï¼‰è®©æˆ‘ä»¬èƒ½â€œç¡®å®šæ–¹å‘â€ã€‚ çº¦æŸä¿¡æ¯ï¼ˆä¾‹å¦‚ç›®æ ‡å’Œã€å»é‡è§„åˆ™ï¼‰è®©æˆ‘ä»¬èƒ½â€œç¡®å®šåŠ¨ä½œâ€ã€‚ å›¾ç¤ºï¼š 2. å¯¹æ’æŒ‡é’ˆï¼šæœ‰åºæ•°ç»„é‡Œçš„â€œå¤¹é€¼â€2.1 æœºåˆ¶ç›´è§‚å·¦å³æŒ‡é’ˆä»ä¸¤ç«¯å‘ä¸­é—´ç§»åŠ¨ã€‚è‹¥å½“å‰å’Œå¤ªå¤§ï¼Œå³æŒ‡é’ˆå·¦ç§»ï¼›å¤ªå°ï¼Œå·¦æŒ‡é’ˆå³ç§»ï¼›åˆšå¥½å‘½ä¸­å°±è¿”å›ã€‚ å›¾ç¤ºï¼š 2.2 å…·ä½“ I/O ç¤ºä¾‹è¾“å…¥ï¼š nums = [2, 5, 8, 10, 15, 18, 22, 25]ï¼ˆé•¿åº¦ n=8ï¼‰ target = 27 è¿‡ç¨‹ï¼š left=0 (2), right=7 (25) = sum=27ï¼Œå‘½ä¸­ã€‚ è¾“å‡ºï¼š return (0, 7) æˆ– return [2, 25]ï¼ˆä¾é¢˜ç›®è¦æ±‚ï¼‰ 2.3 ä»£ç æ¨¡æ¿ï¼ˆPythonï¼‰def two_sum_sorted(nums, target): left, right = 0, len(nums) - 1 while left right: s = nums[left] + nums[right] if s == target: return left, right if s target: right -= 1 else: left += 1 return None 2.4 å¤æ‚åº¦ æ—¶é—´ï¼šO(n) ç©ºé—´ï¼šO(1) 3. å¿«æ…¢æŒ‡é’ˆï¼šé€Ÿåº¦å·®åˆ¶é€ â€œç›¸é‡ä¿¡æ¯â€3.1 æœºåˆ¶ç›´è§‚æ…¢æŒ‡é’ˆæ¯æ¬¡èµ°ä¸€æ­¥ï¼Œå¿«æŒ‡é’ˆæ¯æ¬¡èµ°ä¸¤æ­¥ã€‚ å¦‚æœå­˜åœ¨ç¯ï¼Œå¿«æ…¢ä¸€å®šç›¸é‡ã€‚ ç›¸é‡åå¯æ¨å¯¼å‡ºå…¥ç¯ç‚¹ã€‚ å›¾ç¤ºï¼š 3.2 å…·ä½“ I/O ç¤ºä¾‹ï¼ˆç¯æ£€æµ‹ï¼‰è¾“å…¥ï¼š é“¾è¡¨ï¼š1 - 2 - 3 - 4 - 5 - 3 ...ï¼ˆ3 æ˜¯å…¥ç¯ç‚¹ï¼‰ è¿‡ç¨‹ï¼š slow: 1-2-3-4â€¦ fast: 1-3-5-4â€¦ ç›¸é‡åˆ™è¯´æ˜æœ‰ç¯ã€‚ è¾“å‡ºï¼š Trueï¼ˆå­˜åœ¨ç¯ï¼‰ 3.3 ä»£ç æ¨¡æ¿ï¼ˆPythonï¼‰def has_cycle(head): slow, fast = head, head while fast and fast.next: slow = slow.next fast = fast.next.next if slow == fast: return True return False 3.4 å¤æ‚åº¦ æ—¶é—´ï¼šO(n) ç©ºé—´ï¼šO(1) 4. åŒå‘æŒ‡é’ˆï¼šç¨³å®šâ€œè¿‡æ»¤ + å†™å›â€4.1 æœºåˆ¶ç›´è§‚ä¸¤ä¸ªæŒ‡é’ˆåŒå‘ç§»åŠ¨ï¼š fast è´Ÿè´£æ‰«æ slow è´Ÿè´£å†™å…¥æœ‰æ•ˆå…ƒç´  å›¾ç¤ºï¼š 4.2 å…·ä½“ I/O ç¤ºä¾‹ï¼ˆå»é‡ï¼‰è¾“å…¥ï¼š nums = [1,1,2,2,3,4,4,5]ï¼ˆé•¿åº¦ 8ï¼‰ è¿‡ç¨‹ï¼š slow æŒ‡å‘æœ€åä¸€ä¸ªæœ‰æ•ˆä½ç½® é‡åˆ°æ–°å€¼æ—¶ï¼Œå†™å…¥ nums[slow+1] è¾“å‡ºï¼š æ•°ç»„å‰ k ä½ä¸º [1,2,3,4,5]ï¼Œk=5 4.3 ä»£ç æ¨¡æ¿ï¼ˆPythonï¼‰def remove_duplicates(nums): if not nums: return 0 slow = 0 for fast in range(1, len(nums)): if nums[fast] != nums[slow]: slow += 1 nums[slow] = nums[fast] return slow + 1 4.4 å¤æ‚åº¦ æ—¶é—´ï¼šO(n) ç©ºé—´ï¼šO(1) 5. è®­ç»ƒå¼æ€ç»´ï¼ˆè¿ç§»åˆ°â€œå¯å­¦ä¹ â€çš„ç›´è§‰ï¼‰è™½ç„¶åŒæŒ‡é’ˆæ˜¯ç®—æ³•é¢˜ï¼Œä½†ä½ å¯ä»¥æŠŠå®ƒå½“ä½œä¸€ç§â€œé—­ç¯å†³ç­–è¿‡ç¨‹â€ï¼š è¾“å…¥ï¼šå½“å‰æŒ‡é’ˆä½ç½® + å±€éƒ¨å€¼ æŸå¤±ï¼šåç¦»ç›®æ ‡ï¼ˆå¦‚ sum-target çš„è¯¯å·®ï¼‰ æ›´æ–°ï¼šç§»åŠ¨æŒ‡é’ˆï¼ˆleft++/rightâ€“/fast++ï¼‰ è¿™ç§æœºåˆ¶ç±»ä¼¼ä¸€ä¸ªâ€œæœ‰çº¦æŸçš„è´ªå¿ƒä¼˜åŒ–â€ã€‚æœ¬è´¨æ˜¯ç”¨å±€éƒ¨ä¿¡æ¯åšå‡ºæœ€æœ‰å¯èƒ½å‡å°‘è¯¯å·®çš„åŠ¨ä½œã€‚ 6. å°ç»“ å¯¹æ’æŒ‡é’ˆï¼šåˆ©ç”¨æœ‰åºæ€§ï¼Œç¼©å°åŒºé—´ å¿«æ…¢æŒ‡é’ˆï¼šåˆ©ç”¨é€Ÿåº¦å·®ï¼Œåˆ¶é€ ç›¸é‡ åŒå‘æŒ‡é’ˆï¼šæ‰«æ + å†™å›ï¼Œç¨³å®šè¿‡æ»¤ 7. è´¹æ›¼æ€»ç»“ä¸€å¥è¯ï¼šåŒæŒ‡é’ˆä¸æ˜¯æŠ€å·§å †å ï¼Œè€Œæ˜¯â€œç”¨ä¸¤æ¡è§†çº¿æŠŠæœç´¢ç©ºé—´å‹æ‰â€ã€‚ å…ˆæ‰¾ç»“æ„ï¼šæ•°ç»„æœ‰åº/å€¼åŸŸå•è°ƒ/å¯è¿‡æ»¤ â‡’ æ‰èƒ½å†³å®šæŒ‡é’ˆç§»åŠ¨æ–¹å‘ã€‚ æ¯æ­¥æœ‰ç†ç”±ï¼šæŒ‡é’ˆç§»åŠ¨æ˜¯â€œå‡å°‘è¯¯å·®â€çš„åŠ¨ä½œï¼Œä¸æ˜¯æ‹è„‘è¢‹ã€‚ å†™å›=å‹ç¼©ä¿¡æ¯ï¼šåŒå‘æŒ‡é’ˆé€šè¿‡è¦†ç›–å†™å›ï¼ŒæŠŠâ€œä¿ç•™ä»€ä¹ˆâ€å˜æˆå¯éªŒè¯çš„è¿‡ç¨‹ã€‚ é‡åˆ°æ–°é¢˜å…ˆé—®ï¼šæˆ‘èƒ½å¦ç”¨ä¸¤ä¸ªæ¸¸æ ‡ï¼ŒæŠŠæœªçŸ¥åŒºåŸŸå˜æˆæ›´å°çš„å·²çŸ¥åŒºåŸŸï¼Ÿ","tags":["ç®—æ³•"]},{"title":"CLIP (Contrastive Language-Image Pre-Training) æ·±åº¦è§£æï¼šä»åŸç†åˆ°å¤§æ¨¡å‹åº”ç”¨","path":"/2026/02/16/CLIPæ·±åº¦è§£æ/","content":"CLIP (Contrastive Language-Image Pre-Training) æ·±åº¦è§£æ ä¸€å¥è¯æœ¬è´¨ï¼šCLIP æ˜¯è¿æ¥æ–‡æœ¬ä¸å›¾åƒçš„â€ç½—å¡å¡”çŸ³ç¢‘â€ã€‚å®ƒé€šè¿‡å¯¹æ¯”å­¦ä¹ ï¼Œå¼ºè¡ŒæŠŠå›¾åƒå’Œæ–‡æœ¬æ‹‰åˆ°åŒä¸€ä¸ªå‘é‡ç©ºé—´ï¼Œè®©è®¡ç®—æœºè§†è§‰ç»ˆäºâ€è¯»æ‡‚äº†è¯­è¨€â€ã€‚ ä¸€ã€STAR æ¡†æ¶ï¼šCLIP çš„è¯ç”Ÿä¸é©å‘½1. Situation (èƒŒæ™¯ä¸ç—›ç‚¹)åœ¨ CLIP (2021) ä¹‹å‰ï¼Œè®¡ç®—æœºè§†è§‰ï¼ˆCVï¼‰å’Œè‡ªç„¶è¯­è¨€å¤„ç†ï¼ˆNLPï¼‰æ˜¯ä¸¤ä¸ªå¹³è¡Œçš„ä¸–ç•Œï¼š CV çš„å›°å¢ƒï¼šä¸¥é‡ä¾èµ–äººå·¥æ ‡æ³¨ï¼ˆImageNet çš„ 1000 ä¸ªç±»ï¼‰ï¼Œæ¨¡å‹åªèƒ½è¯†åˆ«è®­ç»ƒè¿‡çš„ç±»åˆ«ï¼ˆClosed-Setï¼‰ï¼Œæ¢ä¸ªåœºæ™¯å°±è¦é‡æ–°è®­ç»ƒã€‚ NLP çš„çªç ´ï¼šGPT ç³»åˆ—è¯æ˜äº†â€ä»æµ·é‡æ— æ ‡æ³¨æ–‡æœ¬ä¸­è‡ªç›‘ç£å­¦ä¹ â€æ˜¯é€šå¾€é€šç”¨çš„é’¥åŒ™ã€‚ æ ¸å¿ƒç—›ç‚¹ï¼šå¦‚ä½•è®© CV æ¨¡å‹åƒ GPT ä¸€æ ·ï¼Œä»äº’è”ç½‘æµ·é‡æ•°æ®ä¸­å­¦ä¹ ï¼Œä¸å†éœ€è¦äººå·¥æ‰“æ ‡ç­¾ï¼Ÿ 2. Task (ä»»åŠ¡ç›®æ ‡)OpenAI çš„ç›®æ ‡å¾ˆæ˜ç¡®ï¼š è®­ç»ƒä¸€ä¸ªèƒ½ç›´æ¥é€šè¿‡â€è‡ªç„¶è¯­è¨€â€æŒ‡æŒ¥çš„è§†è§‰æ¨¡å‹ï¼Œå®ç° Zero-Shotï¼ˆé›¶æ ·æœ¬ï¼‰è¿ç§»ã€‚ å³ï¼šä¸ç”¨å¾®è°ƒï¼Œç›´æ¥å‘Šè¯‰æ¨¡å‹â€æ‰¾ä¸€å¼ çŒ«çš„ç…§ç‰‡â€ï¼Œå®ƒå°±èƒ½ä»å›¾åº“é‡Œæ‰¾å‡ºçŒ«ã€‚ 3. Action (æ ¸å¿ƒæ–¹æ³•)CLIP æ‘’å¼ƒäº†ä¼ ç»Ÿçš„â€åˆ†ç±»â€èŒƒå¼ï¼Œé‡‡ç”¨äº† **å¯¹æ¯”å­¦ä¹  (Contrastive Learning)**ï¼š æ•°æ®ï¼šæ”¶é›†äº† 4 äº¿å¯¹ (å›¾ç‰‡, æ–‡æœ¬) æ•°æ® (WebImageText)ã€‚ æœºåˆ¶ï¼šè®­ç»ƒä¸€ä¸ªå›¾åƒç¼–ç å™¨å’Œä¸€ä¸ªæ–‡æœ¬ç¼–ç å™¨ï¼Œè®©é…å¯¹çš„å›¾æ–‡å‘é‡ç›¸ä¼¼åº¦æœ€å¤§åŒ–ï¼Œä¸é…å¯¹çš„æœ€å°åŒ–ã€‚ è§„æ¨¡ï¼šä½¿ç”¨äº†è¶…å¤§çš„ Batch Size (32,768)ï¼Œè¿™æ˜¯æˆåŠŸçš„å…³é”®ä¹‹ä¸€ã€‚ 4. Result (ç»“æœä¸å½±å“) æ€§èƒ½ï¼šåœ¨ ImageNet ä¸Šï¼ŒZero-Shot çš„ CLIP è¾¾åˆ°äº† ResNet-50 çš„æ°´å¹³ï¼ˆ76.2%ï¼‰ï¼Œä½†é²æ£’æ€§è¿œè¶…åè€…ã€‚ å½±å“ï¼šå®ƒæˆä¸ºäº† AI ç»˜ç”»ï¼ˆStable Diffusionï¼‰ã€å¤šæ¨¡æ€å¤§æ¨¡å‹ï¼ˆLLaVAã€GPT-4Vï¼‰çš„åŸºçŸ³ã€‚æ²¡æœ‰ CLIPï¼Œå°±æ²¡æœ‰ç°åœ¨çš„ AIGC çƒ­æ½®ã€‚ äºŒã€æ·±åº¦åŸç†ï¼šä» Tensor æµå‘åˆ°æ¢¯åº¦å›ä¼ 2.1 æ¶æ„ä¸æ•°æ®æµ (The Flow)CLIP æ˜¯å…¸å‹çš„**åŒå¡”æ¶æ„ (Two-Tower Architecture)**ã€‚ (å›¾1ï¼šCLIP å®Œæ•´æ¶æ„ä¸ Tensor ç»´åº¦å˜åŒ–å›¾) æ•°æ®æµæ‹†è§£ (ä»¥ ViT-B/32 ä¸ºä¾‹)å‡è®¾ Batch Size $N=4$ï¼š Image Branch (å·¦ä¾§): Input: [4, 3, 224, 224] (å›¾ç‰‡) Patch Embedding: åˆ‡æˆ 7x7=49 ä¸ª patch $ightarrow$ [4, 49, 768] Transformer: åŠ ä¸Š [CLS] token $ightarrow$ [4, 50, 768] Output: å– [CLS] tokenï¼ŒæŠ•å½±åˆ° 512 ç»´ $ightarrow$ I_e [4, 512] Text Branch (å³ä¾§): Input: [4, 77] (æ–‡æœ¬ Token ID) Transformer: ç»è¿‡ 12 å±‚å¤„ç† $ightarrow$ [4, 77, 512] Output: å– [EOS] tokenï¼ŒæŠ•å½±åˆ° 512 ç»´ $ightarrow$ T_e [4, 512] Interaction (äº¤äº’): çŸ©é˜µä¹˜æ³•: Logits = I_e @ T_e.T $ightarrow$ [4, 4] è¿™å°±å¾—åˆ°äº†ä¸€ä¸ªç›¸ä¼¼åº¦çŸ©é˜µï¼ 2.2 æ ¸å¿ƒæœºåˆ¶ï¼šå¯¹æ¯”å­¦ä¹ ä¸ InfoNCE LossCLIP ä¸åšç”Ÿæˆï¼ˆä¸ç”»å›¾ï¼‰ï¼Œä¹Ÿä¸åšåˆ†ç±»ï¼ˆä¸é¢„æµ‹ Labelï¼‰ï¼Œå®ƒåªåšåˆ¤æ–­é¢˜ï¼š â€œè¿™å¼ å›¾å’Œè¿™æ®µå­—ï¼Œæ˜¯ä¸æ˜¯ä¸€å¯¹ï¼Ÿâ€ (å›¾2ï¼šå¯¹æ¯”æŸå¤±çŸ©é˜µè®¡ç®—ä¸æ¢¯åº¦å›ä¼ ) InfoNCE Loss è¯¦è§£å¯¹äº Batch ä¸­çš„ç¬¬ $i$ å¼ å›¾ï¼Œå®ƒä¸ç¬¬ $i$ æ®µæ–‡æœ¬æ˜¯æ­£æ ·æœ¬ï¼Œä¸å…¶ä»–æ‰€æœ‰æ–‡æœ¬æ˜¯è´Ÿæ ·æœ¬ã€‚ $$ L_i = -\\log rac{\u001bxp(\text{sim}(I_i, T_i)/\tau)}{\\sum_{j=1}^N \u001bxp(\text{sim}(I_i, T_j)/\tau)} $$ åˆ†å­ï¼šæ­£æ ·æœ¬çš„ç›¸ä¼¼åº¦ï¼ˆæˆ‘ä»¬è¦æœ€å¤§åŒ–å®ƒï¼‰ã€‚ åˆ†æ¯ï¼šæ‰€æœ‰æ ·æœ¬çš„ç›¸ä¼¼åº¦æ€»å’Œï¼ˆæˆ‘ä»¬è¦é€šè¿‡æœ€å¤§åŒ–åˆ†å­ï¼Œé—´æ¥å‹ä½åˆ†æ¯ä¸­å…¶ä»–è´Ÿæ ·æœ¬çš„æ¯”é‡ï¼‰ã€‚ **$\tau$ (Temperature)**ï¼šæ¸©åº¦ç³»æ•°ã€‚$\tau$ è¶Šå°ï¼Œåˆ†å¸ƒè¶Šå°–é”ï¼Œæ¨¡å‹è¶Šå…³æ³¨æœ€éš¾åŒºåˆ†çš„è´Ÿæ ·æœ¬ã€‚CLIP ä¸­ $\tau$ æ˜¯å¯å­¦ä¹ çš„ã€‚ è´¹æ›¼ç›´è§‰ï¼šä¸ºä»€ä¹ˆè¦ç”¨å¯¹æ¯”å­¦ä¹ ï¼Ÿæƒ³è±¡ä½ åœ¨æ•™å°å­©è®¤åŠ¨ç‰©ï¼š **ç”Ÿæˆå¼ (Generative)**ï¼šè®©å°å­©ç”»ä¸€åªçŒ«ã€‚ï¼ˆå¤ªéš¾äº†ï¼Œè¿˜è¦å­¦ç”»ç”»ï¼‰ **åˆ†ç±»å¼ (Classification)**ï¼šç»™å°å­©çœ‹å›¾ï¼Œè®©ä»–èƒŒè¿™æ˜¯â€ç±»åˆ«ID 283â€ã€‚ï¼ˆæ­»è®°ç¡¬èƒŒï¼Œä¸æ‡‚å«ä¹‰ï¼‰ å¯¹æ¯”å¼ (Contrastive)ï¼šç»™å°å­©çœ‹ä¸€å¼ çŒ«å›¾å’Œâ€çŒ«â€å­—å¡ç‰‡ï¼Œå†çœ‹ä¸€å¼ ç‹—å›¾å’Œâ€è½¦â€å­—å¡ç‰‡ï¼Œè®©ä»–é…å¯¹ã€‚ï¼ˆç®€å•ã€é«˜æ•ˆï¼Œæ‡‚è¯­ä¹‰ï¼‰ ä¸‰ã€Zero-Shot æœºåˆ¶ï¼šå¦‚ä½•â€å¬æ‡‚äººè¯â€ï¼Ÿè¿™æ˜¯ CLIP æœ€éªšçš„æ“ä½œã€‚å®ƒæŠŠåˆ†ç±»é—®é¢˜å˜æˆäº†æ£€ç´¢é—®é¢˜ã€‚ (å›¾3ï¼šZero-Shot æ¨ç†ä¸åŠ¨æ€æƒé‡ç”Ÿæˆ) 3.1 åŠ¨æ€åˆ†ç±»å™¨ (Dynamic Classifier)ä¼ ç»Ÿçš„åˆ†ç±»å™¨ï¼Œæœ€åä¸€å±‚æƒé‡ $W$ æ˜¯å›ºå®šçš„ï¼ˆæ¯”å¦‚ ImageNet çš„ 1000 ç±»ï¼‰ã€‚CLIP çš„æƒé‡æ˜¯åŠ¨æ€ç”Ÿæˆçš„ï¼š ä½ ç»™å®ƒä¸€ç»„ç±»åˆ«è¯ï¼š[\"dog\", \"cat\", \"plane\"]ã€‚ å®ƒæŠŠè¿™äº›è¯å˜æˆå‘é‡ï¼š[v_dog, v_cat, v_plane]ã€‚ è¿™ä¸‰ä¸ªå‘é‡ï¼Œå°±æ„æˆäº†ä¸´æ—¶çš„åˆ†ç±»å™¨æƒé‡ $Wâ€™$ï¼ å›¾ç‰‡å‘é‡ $I$ ä¸ $Wâ€™$ åšç‚¹ç§¯ï¼Œè°å¤§å°±æ˜¯è°ã€‚ 3.2 Prompt Engineering çš„èµ·æºè®ºæ–‡å‘ç°ï¼Œç›´æ¥ç”¨å•è¯ \"dog\" æ•ˆæœä¸€èˆ¬ã€‚å¦‚æœæ”¹æˆ \"a photo of a dog\"ï¼Œæ•ˆæœæå‡ 1.3%ã€‚å¦‚æœç”¨ Ensembleï¼ˆé›†æˆï¼‰ï¼š \"a photo of a big {label}\" \"a drawing of a {label}\" \"it is a {label}\"æŠŠ 80 ç§å¥å­çš„å‘é‡å–å¹³å‡ï¼Œæ•ˆæœæå‡ 3.5%ï¼ è¿™å‘Šè¯‰æˆ‘ä»¬ï¼šå¤šè§’åº¦æè¿°ä¸€ä¸ªäº‹ç‰©ï¼Œç‰¹å¾æ›´ç¨³ã€‚ å››ã€å¤§æ¨¡å‹åº”ç”¨ï¼šCLIP æ˜¯ AI çš„â€è§†è§‰æ¥å£â€CLIP æœ€å¤§çš„è´¡çŒ®ä¸æ˜¯å®ƒè‡ªå·±ï¼Œè€Œæ˜¯å®ƒæˆå…¨äº†åˆ«äººã€‚ (å›¾4ï¼šCLIP åœ¨ Stable Diffusion å’Œ LLaVA ä¸­çš„æ ¸å¿ƒåœ°ä½) 4.1 Stable Diffusion (AI ç»˜ç”») è§’è‰²ï¼šCLIP Text Encoder æ˜¯ SD çš„â€ç†è§£ä¸­æ¢â€ã€‚ æµç¨‹ï¼š ç”¨æˆ·è¾“å…¥ï¼šâ€ä¸€åªèµ›åšæœ‹å…‹é£æ ¼çš„çŒ«â€ã€‚ CLIP Text Encoder æŠŠè¿™å¥è¯å˜æˆå‘é‡ã€‚ U-Net æ ¹æ®è¿™ä¸ªå‘é‡ï¼Œä»å™ªå£°ä¸­â€é›•åˆ»â€å‡ºå›¾åƒã€‚ æœ¬è´¨ï¼šSD ç”»å¾—å¥½ï¼Œæ˜¯å› ä¸º CLIP æ‡‚æ–‡æœ¬å¯¹åº”çš„è§†è§‰ç‰¹å¾é•¿ä»€ä¹ˆæ ·ã€‚ 4.2 LLaVA / GPT-4V (å¤šæ¨¡æ€å¤§æ¨¡å‹) è§’è‰²ï¼šCLIP Vision Encoder æ˜¯å¤§æ¨¡å‹çš„â€çœ¼ç›â€ã€‚ æµç¨‹ï¼š è¾“å…¥ä¸€å¼ å›¾ã€‚ CLIP Image Encoder æå–å›¾åƒç‰¹å¾ã€‚ é€šè¿‡ä¸€ä¸ªæŠ•å½±å±‚ (Projection)ï¼ŒæŠŠå›¾åƒç‰¹å¾ä¼ªè£…æˆâ€è¯å‘é‡â€ã€‚ LLM (å¦‚ Vicuna/Llama) ä»¥ä¸ºè‡ªå·±çœ‹åˆ°äº†æ–‡å­—ï¼Œå®é™…ä¸Šæ˜¯çœ‹åˆ°äº†å›¾åƒç‰¹å¾ã€‚ æœ¬è´¨ï¼šCLIP æŠŠåƒç´ å˜æˆäº† LLM èƒ½è¯»æ‡‚çš„è¯­ä¹‰ã€‚ äº”ã€æ ¸å¿ƒä»£ç å®ç° (PyTorch)import torchimport torch.nn as nnimport numpy as npclass CLIP(nn.Module): def __init__(self, embed_dim=512): super().__init__() self.visual = VisionTransformer() # Image Encoder self.text = TextTransformer() # Text Encoder self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07)) def forward(self, image, text): # 1. æå–ç‰¹å¾ I_e = self.visual(image) # [N, 512] T_e = self.text(text) # [N, 512] # 2. å½’ä¸€åŒ– (å…³é”®ï¼å¦åˆ™ç‚¹ç§¯æ— ä¸Šç•Œ) I_e = I_e / I_e.norm(dim=-1, keepdim=True) T_e = T_e / T_e.norm(dim=-1, keepdim=True) # 3. è®¡ç®—ç›¸ä¼¼åº¦çŸ©é˜µ # exp(t) * (I @ T.T) logit_scale = self.logit_scale.exp() logits_per_image = logit_scale * I_e @ T_e.t() logits_per_text = logits_per_image.t() # 4. æ„é€ æ ‡ç­¾ (å¯¹è§’çº¿æ˜¯æ­£æ ·æœ¬) labels = torch.arange(len(image)).to(image.device) # 5. è®¡ç®—åŒå‘ Loss loss_i = nn.CrossEntropyLoss()(logits_per_image, labels) loss_t = nn.CrossEntropyLoss()(logits_per_text, labels) return (loss_i + loss_t) / 2 å…­ã€å±€é™æ€§ä¸æ€è€ƒè™½ç„¶ CLIP å¾ˆå¼ºï¼Œä½†å®ƒä¸æ˜¯ä¸‡èƒ½çš„ï¼š ä¸æ“…é•¿ç»†ç²’åº¦ä»»åŠ¡ï¼šå®ƒå¾ˆéš¾åŒºåˆ†â€æ³¢éŸ³747â€å’Œâ€æ³¢éŸ³777â€ï¼Œæˆ–è€…æ•°æ¸…å›¾é‡Œæœ‰å‡ åªé¸Ÿã€‚å› ä¸ºå¯¹æ¯”å­¦ä¹ æ›´å…³æ³¨æ•´ä½“è¯­ä¹‰åŒ¹é…ã€‚ OCR èƒ½åŠ›å¼±ï¼šå®ƒèƒ½è®¤å‡ºâ€Appleâ€è¿™ä¸ªLogoï¼Œä½†å¦‚æœä½ æ‰‹å†™ä¸€ä¸ªâ€Sonyâ€è´´åœ¨è‹¹æœä¸Šï¼Œå®ƒå¯èƒ½ä¼šå›°æƒ‘ã€‚ éç”Ÿæˆå¼ï¼šCLIP è‡ªå·±ä¸èƒ½ç”Ÿæˆå›¾åƒï¼Œå®ƒåªèƒ½è¯„ä»·å›¾åƒã€‚å®ƒéœ€è¦é…åˆ GAN æˆ– Diffusion æ‰èƒ½æåˆ›ä½œã€‚ ä¸ƒã€æ€»ç»“CLIP æ˜¯ AI å†å²ä¸Šçš„ä¸€ä¸ªè½¬æŠ˜ç‚¹ã€‚å®ƒæ‰“ç ´äº† Vision å’Œ Language çš„ç•Œé™ï¼Œç”¨4äº¿å¯¹å›¾æ–‡æ•°æ®æš´åŠ›ç¾å­¦åœ°è¯æ˜äº†ï¼šè¯­è¨€æ˜¯ç†è§£è§†è§‰çš„æœ€ä½³ç›‘ç£ä¿¡å·ã€‚","tags":["æ·±åº¦å­¦ä¹ "],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"MoE æ·±åº¦è§£æï¼šSwitch Transformer ä¸ Mixtral çš„ç¨€ç–ä¹‹é“","path":"/2026/02/08/MoE-Switch-Transformer-Mixtral-Deep-Dive/","content":"MoE æ·±åº¦è§£æï¼šSwitch Transformer ä¸ Mixtral çš„ç¨€ç–ä¹‹é“ ğŸ“… åˆ›å»ºæ—¶é—´ï¼š2026-02-08ğŸ·ï¸ æ ‡ç­¾ï¼š#MoE #SwitchTransformer #Mixtral #ç¨€ç–è®¡ç®— #å¤§æ¨¡å‹ğŸ“š å­¦ä¹ æ–¹æ³•ï¼šè´¹æ›¼å¼è®²è§£ + æ•°å­¦æ¨å¯¼ + ä»£ç å®ç°ğŸ“– å‰ç½®çŸ¥è¯†ï¼šTransformer FFN å±‚, Softmax, PyTorch åŸºç¡€ ğŸ¯ ä¸€å¥è¯ç†è§£ MoE ğŸ§  è´¹æ›¼å¼ç†è§£ MoE å°±åƒä¸€å®¶æ‹¥æœ‰ 128 ä¸ªä¸“ç§‘åŒ»ç”Ÿçš„åŒ»é™¢ã€‚ æ™®é€šåŒ»é™¢ï¼šæ‰€æœ‰ç—…äººéƒ½æ‰¾åŒä¸€ä¸ªå…¨ç§‘åŒ»ç”Ÿï¼ˆç¨ å¯† FFNï¼‰â†’ åŒ»ç”Ÿç´¯æ­»ï¼Œæ•ˆç‡ä½ä¸‹ã€‚ MoE åŒ»é™¢ï¼šå‰å°æŠ¤å£«ï¼ˆRouterï¼‰å¿«é€Ÿåˆ¤æ–­ç—…æƒ…ï¼ŒæŠŠå¿ƒè„ç—…äººåˆ†ç»™å¿ƒè„ä¸“å®¶ï¼Œéª¨æŠ˜ç—…äººåˆ†ç»™éª¨ç§‘ä¸“å®¶ã€‚æ¯ä¸ªç—…äººåªè§ 1-2 ä¸ªä¸“å®¶ï¼Œä½†åŒ»é™¢æ€»å…±æœ‰ 128 ä¸ªä¸“å®¶éšæ—¶å¾…å‘½ã€‚ ç»“æœï¼šåŒ»é™¢å®¹é‡æå‡ 128 å€ï¼Œä½†æ¯ä¸ªç—…äººçš„ç­‰å¾…æ—¶é—´å‡ ä¹ä¸å˜ï¼ ğŸ“‹ ç›®å½• ä¸ºä»€ä¹ˆéœ€è¦ MoEï¼Ÿ æ ¸å¿ƒæœºåˆ¶ï¼šRouter + Expert Tensor ç»´åº¦å˜åŒ–å…¨å›¾è§£ Training Loopï¼šæ¢¯åº¦å¦‚ä½•æµåŠ¨ï¼Ÿ Switch vs Mixtralï¼šå…³é”®å·®å¼‚ å®Œæ•´ä»£ç å®ç° è´¹æ›¼è‡ªæµ‹é¢˜ 1. ä¸ºä»€ä¹ˆéœ€è¦ MoEï¼Ÿ1.1 ç¨ å¯†æ¨¡å‹çš„å›°å¢ƒä¼ ç»Ÿ Transformer çš„ FFN å±‚å æ®äº† 2/3 çš„å‚æ•°é‡ï¼š class DenseFFN(nn.Module): def __init__(self, d_model=1024, d_ff=4096): self.W1 = nn.Linear(d_model, d_ff) # [1024, 4096] = 4.2M å‚æ•° self.W2 = nn.Linear(d_ff, d_model) # [4096, 1024] = 4.2M å‚æ•° # æ€»è®¡ 8.4M å‚æ•°ï¼Œæ¯ä¸ª token éƒ½è¦è¿‡ä¸€é é—®é¢˜ï¼š GPT-3 (175B) æ¯æ¬¡æ¨ç†éƒ½æ¿€æ´» 100% å‚æ•° æ¨ç†æˆæœ¬ âˆ å‚æ•°é‡ â†’ å‚æ•°è¶Šå¤šï¼Œæ¨ç†è¶Šæ…¢ æ‰©å±•æ‚–è®ºï¼šæƒ³è¦æ›´èªæ˜ï¼Œå°±å¿…é¡»æ›´æ…¢ï¼Ÿ 1.2 MoE çš„æ ¸å¿ƒæ´å¯Ÿ ğŸ’¡ å…³é”®æ´å¯Ÿï¼šä¸åŒçš„ token éœ€è¦ä¸åŒçš„\"çŸ¥è¯†\"æ¥å¤„ç†ã€‚ \"The capital of France is\" â†’ éœ€è¦åœ°ç†çŸ¥è¯† \"def quicksort(arr):\" â†’ éœ€è¦ç¼–ç¨‹çŸ¥è¯† \"I feel so happy\" â†’ éœ€è¦æƒ…æ„Ÿç†è§£ ä¸ºä»€ä¹ˆè¦è®©åŒä¸€ä¸ª FFN å¤„ç†æ‰€æœ‰è¿™äº›ï¼Ÿè®©ä¸“å®¶å„å¸å…¶èŒï¼ MoE çš„è§£å†³æ–¹æ¡ˆï¼š å¯¹æ¯”é¡¹ ç¨ å¯† FFN MoE (128 Experts) æ€»å‚æ•°é‡ 8.4M 8.4M Ã— 128 = 1.075B æ¯æ¬¡æ¿€æ´»å‚æ•° 8.4M (100%) 8.4M (0.78%) æ¨¡å‹å®¹é‡ 1Ã— 128Ã— æ¨ç†æˆæœ¬ 1Ã— â‰ˆ 1Ã— è¿™å°±æ˜¯ MoE çš„é­”æ³•ï¼šå‚æ•°é‡æš´æ¶¨ 128 å€ï¼Œæ¨ç†æˆæœ¬å‡ ä¹ä¸å˜ï¼ 2. æ ¸å¿ƒæœºåˆ¶ï¼šRouter + Expert2.1 æ¶æ„æ€»è§ˆè¾“å…¥ Token [B, L, D] â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Router â”‚ â† ä¸€ä¸ªç®€å•çš„ Linear å±‚ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â†“ Softmax + Top-K â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â” â†“ â†“ â†“â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â”â”‚E_0â”‚ â”‚E_1â”‚ â”‚...â”‚ â”‚E_127â”‚ â† 128 ä¸ªå¹¶è¡Œçš„ FFNâ””â”€â”¬â”€â”˜ â””â”€â”¬â”€â”˜ â””â”€â”€â”€â”˜ â”‚ â”‚ â””â”€â”€â”¬â”€â”€â”˜ â†“ åŠ æƒæ±‚å’Œè¾“å‡º [B, L, D] 2.2 Routerï¼šäº¤é€šè°ƒåº¦å‘˜Router å°±æ˜¯ä¸€ä¸ª Linear å±‚ï¼ŒæŠŠæ¯ä¸ª token æ˜ å°„åˆ° N ä¸ª Expert çš„æ¦‚ç‡åˆ†å¸ƒï¼š # Router çš„å…¨éƒ¨ä»£ç self.router = nn.Linear(d_model, num_experts, bias=False) # [1024, 128]# å‰å‘ä¼ æ’­router_logits = self.router(x) # [B, L, 128]router_probs = F.softmax(router_logits, dim=-1) # æ¦‚ç‡åˆ†å¸ƒ å…·ä½“ä¾‹å­ï¼š å‡è®¾ä¸€ä¸ª token çš„ embedding æ˜¯ x = [0.1, 0.2, ..., 0.5]ï¼ˆ1024 ç»´ï¼‰ # Router è®¡ç®—router_logits = W_router @ x # [128]# å‡è®¾ç»“æœæ˜¯ [2.1, 0.5, 3.2, 0.1, ..., 0.8]# Softmax å½’ä¸€åŒ–router_probs = softmax(router_logits)# [0.15, 0.03, 0.45, 0.01, ..., 0.02]# â†‘ â†‘# Expert 0: 15% Expert 2: 45% â† é€‰è¿™ä¸ªï¼ 2.3 Top-K é€‰æ‹©**Switch Transformer (Top-1)**ï¼šæ¯ä¸ª token åªé€‰ 1 ä¸ª Expert topk_probs, topk_indices = torch.topk(router_probs, k=1)# topk_indices = [2] â† é€‰æ‹© Expert 2# topk_probs = [0.45] â† æƒé‡ 0.45 **Mixtral (Top-2)**ï¼šæ¯ä¸ª token é€‰ 2 ä¸ª Expert topk_probs, topk_indices = torch.topk(router_probs, k=2)# topk_indices = [2, 0] â† é€‰æ‹© Expert 2 å’Œ Expert 0# topk_probs = [0.45, 0.15] â†’ å½’ä¸€åŒ– â†’ [0.75, 0.25] 2.4 ç¨€ç–è®¡ç®—çš„å®ç° âš ï¸ å…³é”®é—®é¢˜ï¼š128 ä¸ª Expertï¼Œæ¯ä¸ª token åªç”¨ 1 ä¸ªï¼Œæ€ä¹ˆé«˜æ•ˆè®¡ç®—ï¼Ÿ æœ´ç´ å®ç°ï¼ˆä½æ•ˆï¼‰ï¼š for expert_id in range(128): mask = (topk_indices == expert_id) if mask.any(): output[mask] = experts[expert_id](x[mask]) ä¼˜åŒ–å®ç°ï¼ˆå®é™…ä½¿ç”¨ï¼‰ï¼š # 1. æŒ‰ Expert ID é‡æ’æ‰€æœ‰ tokensorted_indices = topk_indices.argsort()sorted_x = x[sorted_indices]# 2. æ‰¹é‡å¤„ç†æ¯ä¸ª Expert çš„ tokenexpert_outputs = []for expert_id, expert in enumerate(experts): start, end = expert_boundaries[expert_id] if start end: expert_outputs.append(expert(sorted_x[start:end]))# 3. è¿˜åŸåˆ°åŸå§‹é¡ºåºoutput = torch.cat(expert_outputs)[inverse_indices] 3. Tensor ç»´åº¦å˜åŒ–å…¨å›¾è§£ ğŸ¨ å¯è§†åŒ–æ˜¯å»ºç«‹ç›´è§‰çš„æœ€å¿«æ–¹å¼ è®©æˆ‘ä»¬è·Ÿè¸ªä¸€ä¸ªå…·ä½“çš„ä¾‹å­ï¼Œçœ‹ Tensor å¦‚ä½•æµåŠ¨ã€‚ 3.1 é…ç½®å‚æ•°batch_size = 8seq_len = 512d_model = 1024num_experts = 128d_ff = 4096 # æ¯ä¸ª Expert çš„ FFN éšè—å±‚top_k = 1 # Switch Transformer 3.2 å®Œæ•´æ•°æ®æµâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ Step 0: Input â”‚â”‚ x: [8, 512, 1024] â”‚â”‚ å«ä¹‰: 8 ä¸ªæ ·æœ¬ï¼Œæ¯ä¸ª 512 ä¸ª tokenï¼Œæ¯ä¸ª token 1024 ç»´ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â†“â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ Step 1: Router Logits â”‚â”‚ router_logits = W_r @ x â”‚â”‚ W_r: [1024, 128] â”‚â”‚ router_logits: [8, 512, 128] â”‚â”‚ å«ä¹‰: æ¯ä¸ª token å¯¹ 128 ä¸ª Expert çš„\"æ‰“åˆ†\" â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â†“ Softmaxâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ Step 2: Router Probabilities â”‚â”‚ router_probs = softmax(router_logits, dim=-1) â”‚â”‚ router_probs: [8, 512, 128] â”‚â”‚ å«ä¹‰: æ¯ä¸ª token é€‰æ‹©å„ Expert çš„æ¦‚ç‡ â”‚â”‚ æ¯è¡Œå’Œ = 1.0 â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â†“ Top-1â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ Step 3: Top-K Selection â”‚â”‚ topk_probs: [8, 512, 1] â† æœ€å¤§æ¦‚ç‡å€¼ â”‚â”‚ topk_indices: [8, 512, 1] â† Expert ID (0-127) â”‚â”‚ â”‚â”‚ ä¾‹: Token[0,0] â†’ Expert 42, æƒé‡ 0.67 â”‚â”‚ Token[0,1] â†’ Expert 7, æƒé‡ 0.81 â”‚â”‚ Token[0,2] â†’ Expert 42, æƒé‡ 0.55 â† åŒä¸€ä¸ª Expert! â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â†“ Dispatchâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ Step 4: Expert Processing (ç¨€ç–!) â”‚â”‚ â”‚â”‚ Expert 7: æ”¶åˆ° 32 ä¸ª token â†’ [32, 1024] â”‚â”‚ Expert 42: æ”¶åˆ° 48 ä¸ª token â†’ [48, 1024] â† è´Ÿè½½ä¸å‡! â”‚â”‚ Expert 99: æ”¶åˆ° 0 ä¸ª token â†’ ç©ºé—² â”‚â”‚ ... â”‚â”‚ â”‚â”‚ æ¯ä¸ª Expert å†…éƒ¨: â”‚â”‚ [N_i, 1024] â†’ W1 â†’ [N_i, 4096] â†’ GELU â†’ W2 â†’ [N_i, 1024] â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â†“ Weighted Combineâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ Step 5: Output â”‚â”‚ output[i] = topk_probs[i] Ã— expert_output[i] â”‚â”‚ output: [8, 512, 1024] â”‚â”‚ ç»´åº¦ä¸è¾“å…¥å®Œå…¨ä¸€è‡´! â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 3.3 å¯è§†åŒ–ï¼šRouter å†³ç­–çƒ­åŠ›å›¾å›¾ 3.1: Router å†³ç­–çƒ­åŠ›å›¾ (å·¦)ã€Top-1 åˆ†é… (ä¸­)ã€è´Ÿè½½åˆ†å¸ƒ (å³) è§£è¯»ï¼š å·¦å›¾ï¼šæ¯åˆ—æ˜¯ä¸€ä¸ª token å¯¹ 16 ä¸ª Expert çš„æ¦‚ç‡ï¼ˆé¢œè‰²è¶Šæ·± = æ¦‚ç‡è¶Šé«˜ï¼‰ ä¸­å›¾ï¼šæ¯ä¸ª token æœ€ç»ˆé€‰æ‹©çš„ Expertï¼ˆç»¿è‰² = è¢«é€‰ä¸­ï¼‰ å³å›¾ï¼šExpert è´Ÿè½½åˆ†å¸ƒ ğŸ”´ çº¢è‰²æŸ±å­ = è¿‡è½½ï¼ˆå¦‚ Expert 2 å¤„ç† 12 ä¸ª tokenï¼‰ ğŸŸ  æ©™è‰²æŸ±å­ = ç©ºé—²ï¼ˆå¦‚ Expert 13 åªå¤„ç† 1 ä¸ªï¼‰ ğŸŸ¢ ç»¿è‰²è™šçº¿ = ç†æƒ³å‡è¡¡å€¼ è¿™å°±æ˜¯è´Ÿè½½ä¸å‡è¡¡é—®é¢˜çš„æ ¹æºï¼ 4. Training Loopï¼šæ¢¯åº¦å¦‚ä½•æµåŠ¨ï¼Ÿ4.1 å®Œæ•´çš„ Loss å‡½æ•° $$ \\mathcal{L}_{\\text{total}} = \\underbrace{\\mathcal{L}_{\\text{task}}}_{\\text{Task Loss}} + \\alpha \\cdot \\underbrace{\\mathcal{L}_{\\text{balance}}}_{\\text{Load Balance}} $$ å…¶ä¸­ $\\alpha = 0.01$ï¼ˆSwitch Transformer æ¨èå€¼ï¼‰ 4.2 Load Balance Loss æ¨å¯¼ ğŸ§  è´¹æ›¼å¼ç†è§£ Load Balance Loss æƒ³è±¡ä½ æ˜¯åŒ»é™¢ç®¡ç†è€…ï¼Œè¦è®© 128 ä¸ªä¸“å®¶çš„å·¥ä½œé‡å‡è¡¡ï¼š f_i = Expert i å®é™…æ¥è¯Šçš„ç—…äººæ¯”ä¾‹ï¼ˆé¢‘ç‡ï¼‰ P_i = æŠ¤å£«ç»™ Expert i çš„å¹³å‡æ¨èæ¦‚ç‡ å¦‚æœæŸä¸ªä¸“å®¶æ—¢å®é™…æ¥è¯Šå¤šï¼ˆf å¤§ï¼‰ï¼Œåˆè¢«æ¨èæ¦‚ç‡é«˜ï¼ˆP å¤§ï¼‰ï¼Œè¯´æ˜ç³»ç»Ÿåœ¨\"åè¢’\"è¿™ä¸ªä¸“å®¶ã€‚æƒ©ç½šå®ƒï¼ æ•°å­¦å®šä¹‰ï¼š $$ f_i = \\frac{1}{B \\cdot L} \\sum_{b,l} \\mathbb{1}[\\text{Top1}(x_{b,l}) = i] $$ $$ P_i = \\frac{1}{B \\cdot L} \\sum_{b,l} \\text{Router}(x_{b,l})_i $$ $$ \\mathcal{L}_{\\text{balance}} = N \\cdot \\sum_{i=1}^{N} f_i \\cdot P_i $$ å…·ä½“è®¡ç®—ä¾‹å­ï¼š # å‡è®¾ 4 ä¸ª Expertï¼Œ8 ä¸ª tokenrouter_probs = torch.tensor([ [0.7, 0.1, 0.1, 0.1], # Token 0 â†’ Expert 0 (p=0.7) [0.6, 0.2, 0.1, 0.1], # Token 1 â†’ Expert 0 (p=0.6) [0.5, 0.3, 0.1, 0.1], # Token 2 â†’ Expert 0 (p=0.5) [0.4, 0.4, 0.1, 0.1], # Token 3 â†’ Expert 0 (p=0.4) [0.1, 0.6, 0.2, 0.1], # Token 4 â†’ Expert 1 [0.1, 0.1, 0.7, 0.1], # Token 5 â†’ Expert 2 [0.1, 0.1, 0.1, 0.7], # Token 6 â†’ Expert 3 [0.1, 0.1, 0.1, 0.7], # Token 7 â†’ Expert 3])# Top-1 é€‰æ‹©ç»“æœ: [0, 0, 0, 0, 1, 2, 3, 3]# è®¡ç®— f (é¢‘ç‡)f = [4/8, 1/8, 1/8, 2/8] # = [0.5, 0.125, 0.125, 0.25]# è®¡ç®— P (å¹³å‡æ¦‚ç‡)P = router_probs.mean(dim=0) # = [0.325, 0.2375, 0.1625, 0.2375]# Load Balance LossN = 4L_balance = N * sum(f[i] * P[i] for i in range(4))# = 4 * (0.5Ã—0.325 + 0.125Ã—0.2375 + 0.125Ã—0.1625 + 0.25Ã—0.2375)# = 4 * 0.2719 = 1.0876 æ¢¯åº¦åˆ†æï¼š $$ \\frac{\\partial \\mathcal{L}_{\\text{balance}}}{\\partial P_i} = N \\cdot f_i $$ Expert 0: $ abla P_0 = 4 \\times 0.5 = 2.0$ â† æ¢¯åº¦æœ€å¤§ï¼Œä¼šè¢«æƒ©ç½šï¼ Expert 1: $ abla P_1 = 4 \\times 0.125 = 0.5$ â† æ¢¯åº¦è¾ƒå° â†’ è®­ç»ƒä¼šé™ä½ Expert 0 çš„æ¦‚ç‡ï¼Œæå‡å…¶ä»– Expert çš„æ¦‚ç‡ 4.3 æ¢¯åº¦æµåŠ¨å›¾è§£å›¾ 4.1: MoE åå‘ä¼ æ’­çš„å®Œæ•´æ¢¯åº¦æµåŠ¨è·¯å¾„ å…³é”®ç‚¹ï¼š Expert å‚æ•°çš„æ¢¯åº¦ï¼š âœ… è¢«é€‰ä¸­çš„ Expert æ”¶åˆ°æ¢¯åº¦ âŒ æœªè¢«é€‰ä¸­çš„ Expert æ¢¯åº¦ä¸º 0 Router å‚æ•°æ”¶åˆ°ä¸¤ç§æ¢¯åº¦ï¼š ğŸ”µ æ¥è‡ªä¸»ä»»åŠ¡ Lossï¼ˆé€šè¿‡ Gating Weightï¼‰ ğŸŸ  æ¥è‡ª Load Balance Lossï¼ˆç›´æ¥ä½œç”¨äº softmaxï¼‰ æ¢¯åº¦å†²çªï¼š ä¸»ä»»åŠ¡æƒ³è®© Router é€‰æ‹©æœ€å¥½çš„ Expert LB Loss æƒ³è®© Router å‡åŒ€åˆ†é… $\\alpha = 0.01$ å¹³è¡¡è¿™ä¸¤ä¸ªç›®æ ‡ 4.4 è®­ç»ƒç¨³å®šæ€§æŠ€å·§ æŠ€å·§ åŸå›  ä»£ç  å°åˆå§‹åŒ– é˜²æ­¢ Router ä¸€å¼€å§‹å°±åå‘æŸäº› Expert nn.init.normal_(router.weight, std=0.01) Capacity Factor é™åˆ¶æ¯ä¸ª Expert æœ€å¤šå¤„ç†çš„ token æ•° capacity = (B*L/N) * 1.25 BF16 è®­ç»ƒ é˜²æ­¢ softmax ä¸Šæº¢/ä¸‹æº¢ model.to(torch.bfloat16) æ¢¯åº¦è£å‰ª é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ clip_grad_norm_(params, 1.0) 5. Switch vs Mixtralï¼šå…³é”®å·®å¼‚ ç‰¹æ€§ Switch Transformer (2021) Mixtral 8x7B (2023) Top-K Top-1ï¼ˆæè‡´ç¨€ç–ï¼‰ Top-2ï¼ˆå¹³è¡¡æ€§èƒ½ï¼‰ Expert æ•°é‡ 2048ï¼ˆæç«¯å¤šï¼‰ 8ï¼ˆé€‚åˆå•æœºï¼‰ æ¯ token è®¡ç®—é‡ 1 ä¸ª Expert 2 ä¸ª Expert è´Ÿè½½å‡è¡¡ Auxiliary Loss Token Choice + Expert Choice é€‚ç”¨åœºæ™¯ é¢„è®­ç»ƒè¶…å¤§æ¨¡å‹ æŒ‡ä»¤å¾®è°ƒ + æ¨ç†éƒ¨ç½² å›¾ 5.1: Switch Transformer (Top-1) vs Mixtral (Top-2) æ¶æ„å¯¹æ¯” 5.1 Mixtral çš„ Top-2 ä¼˜åŠ¿# Top-2 è·¯ç”±topk_probs, topk_indices = torch.topk(router_probs, k=2)# å½’ä¸€åŒ–topk_probs = topk_probs / topk_probs.sum(dim=-1, keepdim=True)# è¾“å‡º = w1 * Expert1(x) + w2 * Expert2(x)output = topk_probs[:, 0] * expert1_out + topk_probs[:, 1] * expert2_out ä¼˜åŠ¿ï¼š æ›´å¥½çš„å®¹é”™æ€§ï¼ˆå¦‚æœ Top-1 è¿‡è½½ï¼ŒTop-2 è¡¥å……ï¼‰ è´Ÿè½½è‡ªç„¶æ›´å‡è¡¡ è®­ç»ƒæ›´ç¨³å®š ä»£ä»·ï¼š æ¨ç†æˆæœ¬ Ã—2ï¼ˆä½†ä»è¿œä½äºç¨ å¯†æ¨¡å‹ï¼š8 ä¸ª Expert åªæ¿€æ´» 2 ä¸ª = 25%ï¼‰ 6. å®Œæ•´ä»£ç å®ç°6.1 Switch MoE Layerimport torchimport torch.nn as nnimport torch.nn.functional as Fclass SwitchMoELayer(nn.Module): \"\"\" Switch Transformer çš„ MoE Layer å®ç° å‚æ•°: d_model: è¾“å…¥/è¾“å‡ºç»´åº¦ num_experts: Expert æ•°é‡ d_ff: FFN éšè—å±‚ç»´åº¦ capacity_factor: å®¹é‡å› å­ï¼ˆé»˜è®¤ 1.25ï¼‰ \"\"\" def __init__(self, d_model=1024, num_experts=128, d_ff=4096, capacity_factor=1.25): super().__init__() self.num_experts = num_experts self.capacity_factor = capacity_factor # Router: ç®€å•çš„ Linear å±‚ self.router = nn.Linear(d_model, num_experts, bias=False) nn.init.normal_(self.router.weight, std=0.01) # å…³é”®ï¼šå°åˆå§‹åŒ– # Experts: N ä¸ªç‹¬ç«‹çš„ FFN self.experts = nn.ModuleList([ nn.Sequential( nn.Linear(d_model, d_ff), nn.GELU(), nn.Linear(d_ff, d_model) ) for _ in range(num_experts) ]) def forward(self, x): B, L, D = x.shape # Step 1: Router è®¡ç®— router_logits = self.router(x) # [B, L, N] router_probs = F.softmax(router_logits, dim=-1) # Step 2: Top-1 é€‰æ‹© topk_probs, topk_indices = torch.topk(router_probs, k=1, dim=-1) topk_probs = topk_probs.squeeze(-1) # [B, L] topk_indices = topk_indices.squeeze(-1) # [B, L] # Step 3: å®¹é‡é™åˆ¶ capacity = int((B * L / self.num_experts) * self.capacity_factor) # Step 4: Expert è®¡ç®— output = torch.zeros_like(x) expert_counts = torch.zeros(self.num_experts, device=x.device) for expert_id in range(self.num_experts): mask = (topk_indices == expert_id) num_tokens = mask.sum().item() if num_tokens == 0: continue # å®¹é‡é™åˆ¶ï¼šåªé€‰æ¦‚ç‡æœ€é«˜çš„ token if num_tokens capacity: masked_probs = torch.where( mask, topk_probs, torch.tensor(-1e9, device=x.device) ) _, top_indices = torch.topk(masked_probs.flatten(), k=capacity) new_mask = torch.zeros_like(mask.flatten(), dtype=torch.bool) new_mask[top_indices] = True mask = new_mask.view(B, L) # Expert å‰å‘ä¼ æ’­ selected_x = x[mask] expert_out = self.experts[expert_id](selected_x) # åŠ æƒè¾“å‡º weights = topk_probs[mask].unsqueeze(-1) output[mask] = expert_out * weights expert_counts[expert_id] = mask.sum().item() # Step 5: è®¡ç®— Load Balance Loss f = expert_counts / (B * L) P = router_probs.mean(dim=[0, 1]) load_balance_loss = self.num_experts * (f * P).sum() return output, load_balance_loss# ===== æµ‹è¯•ä»£ç  =====if __name__ == \"__main__\": moe = SwitchMoELayer(d_model=512, num_experts=8, d_ff=2048) x = torch.randn(2, 16, 512) output, lb_loss = moe(x) print(f\"Input: {x.shape}\") print(f\"Output: {output.shape}\") print(f\"LB Loss: {lb_loss.item():.4f}\") # åå‘ä¼ æ’­æµ‹è¯• total_loss = output.sum() + 0.01 * lb_loss total_loss.backward() print(f\"Router grad norm: {moe.router.weight.grad.norm():.4f}\") 6.2 å®Œæ•´è®­ç»ƒå¾ªç¯import torch.optim as optimmodel = SwitchMoELayer(d_model=512, num_experts=16, d_ff=2048)optimizer = optim.AdamW(model.parameters(), lr=1e-4)for epoch in range(10): x = torch.randn(8, 64, 512) target = torch.randn(8, 64, 512) output, lb_loss = model(x) main_loss = F.mse_loss(output, target) total_loss = main_loss + 0.01 * lb_loss optimizer.zero_grad() total_loss.backward() torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) optimizer.step() print(f\"Epoch {epoch}: Main={main_loss:.4f}, LB={lb_loss:.4f}\") è¾“å‡ºç¤ºä¾‹ï¼š Epoch 0: Main=1.0234, LB=1.4521Epoch 1: Main=0.9876, LB=1.3214...Epoch 9: Main=0.7821, LB=1.0234 â† LB Loss ä¸‹é™ï¼Œè´Ÿè½½æ›´å‡è¡¡ï¼ 7. è´¹æ›¼è‡ªæµ‹é¢˜ ğŸ§ª æ£€éªŒä½ æ˜¯å¦çœŸæ­£ç†è§£äº† MoE Q1: å¦‚æœ MoE æœ‰ 128 ä¸ª Expertï¼Œæ¯ä¸ª Expert çš„ FFN éšè—å±‚æ˜¯ 4096ï¼Œé‚£ä¹ˆï¼š æ€»å‚æ•°é‡æ˜¯å¤šå°‘ï¼Ÿ æ¯æ¬¡æ¨ç†æ¿€æ´»å¤šå°‘å‚æ•°ï¼Ÿï¼ˆTop-1ï¼‰ ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ æ€»å‚æ•°ï¼š$2 \\times 1024 \\times 4096 \\times 128 = 1.07B$ æ¿€æ´»å‚æ•°ï¼š$2 \\times 1024 \\times 4096 = 8.4M$ï¼ˆä»… 0.78%ï¼‰ Q2: Load Balance Loss ä¸­ï¼Œ$f_i \\cdot P_i$ ä¹˜ç§¯çš„ç›´è§‰å«ä¹‰æ˜¯ä»€ä¹ˆï¼Ÿ ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ $f_i$ æ˜¯ Expert i å®é™…è¢«é€‰ä¸­çš„é¢‘ç‡ $P_i$ æ˜¯ Router ç»™ Expert i çš„å¹³å‡æ¦‚ç‡ ä¹˜ç§¯å¤§è¯´æ˜è¿™ä¸ª Expert æ—¢è¢«é€‰ä¸­å¤šï¼Œåˆè¢«æ¨èæ¦‚ç‡é«˜ â†’ ç³»ç»Ÿåœ¨â€åè¢’â€å®ƒ Loss æƒ©ç½šè¿™ç§æƒ…å†µï¼Œè¿«ä½¿ Router æ›´å‡åŒ€åœ°åˆ†é… Q3: ä¸ºä»€ä¹ˆ Mixtral é€‰æ‹© Top-2 è€Œä¸æ˜¯ Top-1ï¼Ÿ ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ å®¹é”™æ€§ï¼šå¦‚æœ Top-1 Expert è¿‡è½½ï¼ŒTop-2 è¡¥å…… è®­ç»ƒç¨³å®šæ€§ï¼šè´Ÿè½½è‡ªç„¶æ›´å‡è¡¡ æ€§èƒ½ï¼šä¸¤ä¸ª Expert çš„çŸ¥è¯†äº’è¡¥ ä»£ä»·ï¼šæ¨ç†æˆæœ¬ Ã—2ï¼Œä½†ä»è¿œä½äºç¨ å¯†æ¨¡å‹ Q4: å¦‚æœä¸ä½¿ç”¨ Load Balance Lossï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ ç‚¹å‡»æŸ¥çœ‹ç­”æ¡ˆ éƒ¨åˆ† Expert ä¼šè¢«å¤§é‡é€‰æ‹©ï¼ˆé©¬å¤ªæ•ˆåº”ï¼‰ éƒ¨åˆ† Expert å‡ ä¹ä¸è¢«ä½¿ç”¨ â†’ å‚æ•°æµªè´¹ æ¢¯åº¦æ›´æ–°ä¸å‡ â†’ æŸäº› Expert è¿‡åº¦è®­ç»ƒï¼ŒæŸäº›å‡ ä¹ä¸æ›´æ–° æç«¯æƒ…å†µï¼šæ¨¡å‹é€€åŒ–ä¸ºåªç”¨ 1-2 ä¸ª Expert çš„â€ä¼ª MoEâ€ ğŸ”— å»¶ä¼¸é˜…è¯» è®ºæ–‡åŸæ–‡ï¼š Switch Transformers (2021) Mixtral of Experts (2023) GShard (2020) - åˆ†å¸ƒå¼ MoE å¼€æºå®ç°ï¼š Hugging Face Transformers Fairseq MoE è¿›é˜¶è¯é¢˜ï¼š Expert Parallelism vs Tensor Parallelism MoE + LoRA ç¨€ç–å¾®è°ƒ Dynamic Expert Selection ğŸ“Š æ€»ç»“ ğŸ¯ ä¸€å›¾æ€»ç»“ MoE â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ MoE æ ¸å¿ƒå…¬å¼ â”‚ â”‚ â”‚ â”‚ y = Î£ Router(x)_i Ã— Expert_i(x) â”‚ â”‚ iâˆˆTop-K â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â†“ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â”‚â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”â”‚ Router â”‚ â”‚ Experts â”‚ â”‚ Loss â”‚â”‚ (è°ƒåº¦) â”‚ â”‚ (æ‰§è¡Œ) â”‚ â”‚ (å‡è¡¡) â”‚â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚Softmax+TopK å¹¶è¡Œ FFN å±‚ fÂ·P æƒ©ç½šé¡¹é€‰æ‹© K ä¸ªä¸“å®¶ ç¨€ç–æ¿€æ´» é˜²æ­¢åè¢’ è®°ä½ä¸‰ä¸ªæ•°å­—ï¼š 128Ã— å‚æ•°é‡æå‡ 1% æ¿€æ´»ç‡ï¼ˆTop-1 with 128 Expertsï¼‰ 0.01 Load Balance Loss æƒé‡ Created: 2026-02-08 by CaiusTags: #MoE #SwitchTransformer #Mixtral #DeepLearning #LLM","tags":["æ·±åº¦å­¦ä¹ "],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"KLæ•£åº¦å®Œå…¨å›¾è§£æ•™ç¨‹ï¼šä»é›¶åˆ°ç²¾é€š","path":"/2026/02/08/KLæ•£åº¦å®Œå…¨å›¾è§£æ•™ç¨‹/","content":"ğŸ¯ å­¦ä¹ ç›®æ ‡å‡è®¾ä½ å®Œå…¨ä¸æ‡‚ä¿¡æ¯è®ºï¼Œæœ¬æ•™ç¨‹å°†å¸¦ä½ ä»é›¶å¼€å§‹ï¼Œç”¨æœ€é€šä¿—çš„è¯­è¨€ã€æœ€ç”ŸåŠ¨çš„ä¾‹å­ã€æœ€ç›´è§‚çš„å›¾ç¤ºï¼Œå½»åº•ç†è§£ KL æ•£åº¦çš„æœ¬è´¨ã€‚ ğŸ“Š é…å¥—èµ„æº 5å¼ é«˜æ¸…å¯è§†åŒ–å›¾è¡¨ï¼ˆJensenä¸ç­‰å¼ã€KLéè´Ÿæ€§è¯æ˜ã€å‰å‘vsåå‘KLå¯¹æ¯”ç­‰ï¼‰ Pythonå¯è§†åŒ–ä»£ç  ç»ƒä¹ é¢˜åŠå‚è€ƒç­”æ¡ˆ ğŸ“‘ ç›®å½• ç¬¬ä¸€ç« ï¼šä¿¡æ¯çš„ä»·å€¼ - ä¸ºä»€ä¹ˆéœ€è¦ä¿¡æ¯è®ºï¼Ÿ ç¬¬äºŒç« ï¼šä¿¡æ¯é‡ - æƒŠè®¶åº¦çš„æ•°å­¦ ç¬¬ä¸‰ç« ï¼šæ¦‚ç‡åˆ†å¸ƒ - ä¸–ç•Œçš„è§„å¾‹ ç¬¬å››ç« ï¼šç†µ - å¹³å‡æƒŠè®¶ç¨‹åº¦ ç¬¬äº”ç« ï¼šäº¤å‰ç†µ - ç”¨é”™ç¼–ç çš„ä»£ä»· ç¬¬å…­ç« ï¼šKLæ•£åº¦ - æµ‹é‡åˆ†å¸ƒçš„å·®å¼‚ 6.4.1 Jensen ä¸ç­‰å¼ç§‘æ™® 6.4.2 KL æ•£åº¦éè´Ÿæ€§çš„è¯¦ç»†è¯æ˜ 6.4.3 å‰å‘ KL vs åå‘ KLï¼šæ·±åº¦è§£æ ç¬¬ä¸ƒç« ï¼šå®æˆ˜åº”ç”¨ - ä» RLHF åˆ° VAE ç¬¬å…«ç« ï¼šæ€»ç»“ä¸è®°å¿†æŠ€å·§ ç¬¬ä¸€ç« ï¼šä¿¡æ¯çš„ä»·å€¼1.1 æ²¡æœ‰ä¿¡æ¯çš„ä¸–ç•Œæƒ³è±¡ä¸€ä¸‹ï¼Œä½ ç”Ÿæ´»åœ¨ä¸€ä¸ªå®Œå…¨æ²¡æœ‰ä¿¡æ¯çš„ä¸–ç•Œï¼š æ˜å¤©ä¼šä¸‹é›¨å—ï¼ŸğŸ¤· ä¸çŸ¥é“ è‚¡ç¥¨ä¼šæ¶¨å—ï¼ŸğŸ¤· ä¸çŸ¥é“ è€ƒè¯•ä¼šè€ƒä»€ä¹ˆï¼ŸğŸ¤· ä¸çŸ¥é“ è¿™æ˜¯ä¸€ä¸ªå……æ»¡ä¸ç¡®å®šæ€§çš„ä¸–ç•Œã€‚ä½ æ— æ³•åšä»»ä½•æœ‰æ•ˆçš„å†³ç­–ï¼Œå› ä¸ºä½ å¯¹æœªæ¥ä¸€æ— æ‰€çŸ¥ã€‚ 1.2 ä¿¡æ¯çš„æœ¬è´¨ä¿¡æ¯ = æ¶ˆé™¤ä¸ç¡®å®šæ€§çš„å·¥å…· å½“ä½ çŸ¥é“ï¼š æ˜å¤© 90% ä¼šä¸‹é›¨ â†’ ä½ å¸¦ä¼ è‚¡ç¥¨ 60% ä¼šæ¶¨ â†’ ä½ è€ƒè™‘æŠ•èµ„ è€ƒè¯•ç¬¬3ç« å  70% â†’ ä½ é‡ç‚¹å¤ä¹  æ ¸å¿ƒç†è§£ï¼š ä¿¡æ¯è®©ä¸–ç•Œä»â€ä¸çŸ¥é“â€å˜æˆâ€çŸ¥é“â€ï¼Œä»â€æ··ä¹±â€å˜æˆâ€æœ‰åºâ€ã€‚ ç¬¬äºŒç« ï¼šä¿¡æ¯é‡2.1 ç›´è§‰ï¼šä»€ä¹ˆäº‹æƒ…ä¿¡æ¯é‡å¤§ï¼Ÿè¯·çœ‹ä¸‹é¢ä¸‰ä¸ªäº‹ä»¶ï¼š äº‹ä»¶ å‘ç”Ÿæ¦‚ç‡ ä½ çš„ååº” ä¿¡æ¯é‡ å¤ªé˜³ä»ä¸œè¾¹å‡èµ· 99.99% ğŸ˜ ç†æ‰€å½“ç„¶ â‰ˆ 0 ä»Šå¤©æ˜¯å‘¨ä¸€åˆ°å‘¨æ—¥ 14.3% ğŸ™‚ æ­£å¸¸ ä¸­ç­‰ ä¸­äº†å½©ç¥¨å¤´å¥– 0.0001% ğŸ˜± æƒŠå‘†äº†ï¼ å·¨å¤§ï¼ å‘ç°è§„å¾‹äº†å—ï¼Ÿ $$\\boxed{\\text{æ¦‚ç‡è¶Šä½} \\rightarrow \\text{è¶ŠæƒŠè®¶} \\rightarrow \\text{ä¿¡æ¯é‡è¶Šå¤§}}$$ 2.2 ä¿¡æ¯é‡çš„å…¬å¼æˆ‘ä»¬éœ€è¦ä¸€ä¸ªå…¬å¼æ¥é‡åŒ–â€æƒŠè®¶ç¨‹åº¦â€ã€‚è¿™ä¸ªå…¬å¼å¿…é¡»æ»¡è¶³ï¼š æ¦‚ç‡é«˜ â†’ ä¿¡æ¯é‡ä½ä¾‹å¦‚ï¼š$P = 100%$ æ—¶ï¼Œ$I = 0$ï¼ˆå®Œå…¨ä¸æƒŠè®¶ï¼‰ æ¦‚ç‡ä½ â†’ ä¿¡æ¯é‡é«˜ä¾‹å¦‚ï¼š$P = 1%$ æ—¶ï¼Œ$I$ å¾ˆå¤§ï¼ˆéå¸¸æƒŠè®¶ï¼‰ ç‹¬ç«‹äº‹ä»¶ä¿¡æ¯é‡å¯ä»¥ç›¸åŠ ä¾‹å¦‚ï¼š$I(A \\text{ä¸”} B) = I(A) + I(B)$ å”¯ä¸€æ»¡è¶³è¿™äº›è¦æ±‚çš„å‡½æ•°å°±æ˜¯å¯¹æ•°ï¼š $$\\boxed{I(x) = -\\log_2 P(x)}$$ 2.3 å˜é‡è¯¦è§£è®©æˆ‘ä»¬é€ä¸ªå‡»ç ´æ¯ä¸ªç¬¦å·ï¼š ç¬¦å· è¯»ä½œ å«ä¹‰ ä¾‹å­ $x$ â€œæŸä¸ªäº‹ä»¶â€ ä¸€ä¸ªå…·ä½“çš„ç»“æœ â€œä¸‹é›¨â€ã€â€æ·éª°å­å¾—åˆ°6â€ $P(x)$ â€œx çš„æ¦‚ç‡â€ è¿™ä¸ªäº‹ä»¶å‘ç”Ÿçš„å¯èƒ½æ€§ $P(\\text{ä¸‹é›¨}) = 0.3$ $I(x)$ â€œx çš„ä¿¡æ¯é‡â€ çŸ¥é“è¿™ä»¶äº‹åè·å¾—çš„ä¿¡æ¯ $I(\\text{ä¸‹é›¨}) = -\\log_2(0.3) = 1.74$ bits $\\log_2$ â€œä»¥2ä¸ºåº•çš„å¯¹æ•°â€ ç”¨äºŒè¿›åˆ¶è¡¨ç¤ºéœ€è¦å¤šå°‘ä½ $\\log_2(8) = 3$ï¼ˆ8éœ€è¦3ä¸ªäºŒè¿›åˆ¶ä½ï¼‰ ä¸ºä»€ä¹ˆç”¨å¯¹æ•°ï¼Ÿ æŠŠä¹˜æ³•å˜åŠ æ³•$\\log(A \\times B) = \\log(A) + \\log(B)$ æŠŠå°æ•°å˜å¤§æ•°æ¦‚ç‡æ˜¯ 0 åˆ° 1 ä¹‹é—´çš„å°æ•°ï¼Œå¯¹æ•°æŠŠå®ƒå˜æˆæ­£æ•°ä¾‹å¦‚ï¼š$\\log_2(0.01) = -6.64$ï¼ŒåŠ ä¸ªè´Ÿå·å˜æˆ $6.64$ bits å®é™…è®¡ç®—ä¾‹å­ï¼š import math# äº‹ä»¶ï¼šå¤ªé˜³ä»ä¸œè¾¹å‡èµ·P_sun = 0.9999I_sun = -math.log2(P_sun)print(f\"ä¿¡æ¯é‡ï¼š{I_sun:.4f} bits\") # â‰ˆ 0.0001 bitsï¼ˆå‡ ä¹æ²¡æœ‰ä¿¡æ¯ï¼‰# äº‹ä»¶ï¼šä¸­å½©ç¥¨P_lottery = 0.000001I_lottery = -math.log2(P_lottery)print(f\"ä¿¡æ¯é‡ï¼š{I_lottery:.2f} bits\") # â‰ˆ 19.93 bitsï¼ˆä¿¡æ¯é‡å·¨å¤§ï¼ï¼‰ ç¬¬ä¸‰ç« ï¼šæ¦‚ç‡åˆ†å¸ƒ3.1 ä»€ä¹ˆæ˜¯åˆ†å¸ƒï¼Ÿåˆ†å¸ƒ = æ‰€æœ‰å¯èƒ½ç»“æœçš„æ¦‚ç‡æ¸…å• æƒ³è±¡æ·éª°å­ï¼š ç»“æœ æ¦‚ç‡ 1ç‚¹ 1/6 2ç‚¹ 1/6 3ç‚¹ 1/6 4ç‚¹ 1/6 5ç‚¹ 1/6 6ç‚¹ 1/6 è¿™å°±æ˜¯ä¸€ä¸ªå‡åŒ€åˆ†å¸ƒï¼ˆæ¯ä¸ªç»“æœæ¦‚ç‡ç›¸åŒï¼‰ã€‚ 3.2 çœŸå®ä¸–ç•Œ vs æˆ‘çš„çŒœæµ‹åœ¨æœºå™¨å­¦ä¹ ä¸­ï¼Œæˆ‘ä»¬å…³å¿ƒä¸¤ä¸ªåˆ†å¸ƒï¼š ç¬¦å· åç§° å«ä¹‰ ä¾‹å­ $P(x)$ çœŸå®åˆ†å¸ƒ ä¸–ç•Œçš„çœŸç›¸ è€å¸ˆå‡ºé¢˜çš„çœŸå®è§„å¾‹ $Q(x)$ é¢„æµ‹åˆ†å¸ƒ æˆ‘çš„æ¨¡å‹ æˆ‘çŒœæµ‹çš„é‡ç‚¹ç« èŠ‚ é—®é¢˜æ¥äº†ï¼šå¦‚ä½•è¡¡é‡ $Q$ï¼ˆæˆ‘çš„çŒœæµ‹ï¼‰å’Œ $P$ï¼ˆçœŸç›¸ï¼‰æœ‰å¤šä¸ä¸€æ ·ï¼Ÿ ğŸ‘‰ è¿™å°±æ˜¯ KL æ•£åº¦è¦è§£å†³çš„é—®é¢˜ï¼ ç¬¬å››ç« ï¼šç†µ4.1 ç†µçš„ç›´è§‰ç†µ = å¹³å‡æƒŠè®¶ç¨‹åº¦ = å¹³å‡ä¿¡æ¯é‡ çœ‹ä¸¤ä¸ªåœ°åŒºçš„å¤©æ°”ï¼š æ²™æ¼ åœ°åŒºï¼ˆä½ç†µï¼‰ï¼š æ™´å¤©ï¼š90% é›¨å¤©ï¼š8% é›ªå¤©ï¼š2% â†’ ä½ å‡ ä¹ç¡®å®šæ˜¯æ™´å¤©ï¼Œå¾ˆå°‘æƒŠè®¶ â†’ ç†µä½ å±±åŒºï¼ˆé«˜ç†µï¼‰ï¼š æ™´å¤©ï¼š35% é›¨å¤©ï¼š33% é›ªå¤©ï¼š32% â†’ ä½ å®Œå…¨ä¸çŸ¥é“ä¼šæ€æ ·ï¼Œç»å¸¸æƒŠè®¶ â†’ ç†µé«˜ 4.2 ç†µçš„å…¬å¼$$\\boxed{H(P) = -\\sum_{x} P(x) \\log_2 P(x)}$$ é€é¡¹è§£è¯»ï¼š éƒ¨åˆ† å«ä¹‰ $P(x)$ äº‹ä»¶ $x$ çš„çœŸå®æ¦‚ç‡ $\\log_2 P(x)$ äº‹ä»¶ $x$ çš„ä¿¡æ¯é‡ï¼ˆçš„è´Ÿå€¼ï¼‰ $P(x) \\cdot \\log_2 P(x)$ ç”¨æ¦‚ç‡åŠ æƒ $\\sum$ å¯¹æ‰€æœ‰å¯èƒ½çš„ $x$ æ±‚å’Œ $-$ åŠ ä¸ªè´Ÿå·è®©ç»“æœä¸ºæ­£ è®¡ç®—ä¾‹å­ï¼ˆæ²™æ¼ å¤©æ°”ï¼‰ï¼š import numpy as npP = [0.9, 0.08, 0.02] # æ™´ã€é›¨ã€é›ªH = -sum([p * np.log2(p) if p 0 else 0 for p in P])print(f\"ç†µ = {H:.2f} bits\") # â‰ˆ 0.64 bitsï¼ˆä½ç†µï¼‰ è®¡ç®—ä¾‹å­ï¼ˆå±±åŒºå¤©æ°”ï¼‰ï¼š P = [0.35, 0.33, 0.32]H = -sum([p * np.log2(p) if p 0 else 0 for p in P])print(f\"ç†µ = {H:.2f} bits\") # â‰ˆ 1.58 bitsï¼ˆé«˜ç†µï¼‰ ç¬¬äº”ç« ï¼šäº¤å‰ç†µ5.1 æ‘©æ–¯å¯†ç çš„æ•…äº‹æƒ³è±¡ä½ è¦ç”¨æ‘©æ–¯å¯†ç å‘ç”µæŠ¥ã€‚ä¸ºäº†çœç”µï¼Œä½ è¦è®¾è®¡ä¸€å¥—ç¼–ç ï¼š é«˜é¢‘å­—æ¯ï¼ˆå¦‚ E, Tï¼‰ â†’ ç”¨çŸ­ä»£ç ï¼ˆâ€¢, â”ï¼‰ ä½é¢‘å­—æ¯ï¼ˆå¦‚ Z, Qï¼‰ â†’ ç”¨é•¿ä»£ç ï¼ˆâ€¢â”â€¢â”ï¼‰ åœºæ™¯Aï¼šç¼–ç å®Œç¾åŒ¹é…å¦‚æœä½ çš„ç¼–ç å®Œå…¨ç¬¦åˆè‹±æ–‡å­—æ¯çš„çœŸå®é¢‘ç‡ $P$ï¼Œä½ çš„ç”µæŠ¥æœ€çŸ­ã€‚ åœºæ™¯Bï¼šç¼–ç é”™è¯¯å¦‚æœä½ æ‹¿ç€ä¸­æ–‡çš„ç¼–ç æœ¬ $Q$ å»å‘è‹±æ–‡ç”µæŠ¥ $P$ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ ä½ å¯èƒ½ç»™ â€œEâ€ï¼ˆè‹±æ–‡æœ€å¸¸è§ï¼‰åˆ†é…äº†è¶…é•¿ä»£ç ï¼Œå› ä¸ºä¸­æ–‡é‡Œæ²¡æœ‰ â€œEâ€ è¿™ä¸ªéŸ³ã€‚ ç»“æœï¼š æ¯æ¡æ¶ˆæ¯éƒ½æ¯”ç†è®ºæœ€ä¼˜é•¿åº¦å¤šå‡ºä¸€å¤§æˆªï¼ 5.2 äº¤å‰ç†µå…¬å¼$$\\boxed{H(P, Q) = -\\sum_{x} P(x) \\log_2 Q(x)}$$ å…³é”®åŒºåˆ«ï¼š å…¬å¼ æ¦‚ç‡æ¥æº ä¿¡æ¯é‡æ¥æº å«ä¹‰ $H(P)$ $P(x)$ $\\log P(x)$ ç”¨ $P$ çš„ç¼–ç ä¼ è¾“ $P$ çš„ä¿¡æ¯ $H(P,Q)$ $P(x)$ $\\log Q(x)$ ç”¨ $Q$ çš„ç¼–ç ä¼ è¾“ $P$ çš„ä¿¡æ¯ ç›´è§‚ç†è§£ï¼š çœŸå®ä¸–ç•ŒæŒ‰ç…§ $P$ åˆ†å¸ƒäº§ç”Ÿæ•°æ® ä½†ä½ ç”¨ $Q$ çš„ç¼–ç æœ¬æ¥ç¼–ç  $H(P,Q)$ = å¹³å‡æ¯æ¡æ¶ˆæ¯éœ€è¦å¤šå°‘ bits é‡è¦æ€§è´¨ï¼š $$H(P, Q) \\geq H(P)$$ äº¤å‰ç†µæ°¸è¿œ â‰¥ ç†µï¼ï¼ˆç”¨é”™è¯¯ç¼–ç ä¸€å®šæ›´æµªè´¹ï¼‰ ç¬¬å…­ç« ï¼šKLæ•£åº¦6.1 KLæ•£åº¦çš„å®šä¹‰KLæ•£åº¦ = äº¤å‰ç†µ - ç†µ = é¢å¤–æµªè´¹çš„ä¿¡æ¯é‡ $$\\boxed{D_{KL}(P | Q) = H(P, Q) - H(P)}$$ å±•å¼€åï¼š $$\\boxed{D_{KL}(P | Q) = \\sum_{x} P(x) \\log \\frac{P(x)}{Q(x)}}$$ 6.2 å…¬å¼æ·±åº¦è§£æè®©æˆ‘ä»¬æŠŠå…¬å¼æ‹†æˆæœ€å°çš„é›¶ä»¶ï¼š $$D_{KL}(P | Q) = \\sum_{x} P(x) \\cdot \\log \\frac{P(x)}{Q(x)}$$ ç¬¬1éƒ¨åˆ†ï¼š$x$ å«ä¹‰ï¼šæ‰€æœ‰å¯èƒ½çš„äº‹ä»¶/ç»“æœ ä¾‹å­ï¼šå¤©æ°”é‡Œçš„â€æ™´â€ã€â€é›¨â€ã€â€é›ªâ€ ç¬¬2éƒ¨åˆ†ï¼š$P(x)$ å«ä¹‰ï¼šçœŸå®ä¸–ç•Œä¸­ $x$ å‘ç”Ÿçš„æ¦‚ç‡ ä¾‹å­ï¼š$P(\\text{æ™´}) = 0.6$ï¼ˆçœŸå®æƒ…å†µ60%æ˜¯æ™´å¤©ï¼‰ ç¬¬3éƒ¨åˆ†ï¼š$Q(x)$ å«ä¹‰ï¼šä½ çš„æ¨¡å‹è®¤ä¸º $x$ å‘ç”Ÿçš„æ¦‚ç‡ ä¾‹å­ï¼š$Q(\\text{æ™´}) = 0.2$ï¼ˆä½ å´è®¤ä¸ºåªæœ‰20%æ˜¯æ™´å¤©ï¼‰ ç¬¬4éƒ¨åˆ†ï¼š$\\frac{P(x)}{Q(x)}$ å«ä¹‰ï¼šçœŸå® vs é¢„æµ‹çš„æ¯”å€¼ å½“ $P$ å¤§ä½† $Q$ å°æ—¶ï¼Œè¿™ä¸ªæ¯”å€¼çˆ†ç‚¸æ€§å¢é•¿ï¼ ä¾‹å­ï¼š$\\frac{0.6}{0.2} = 3$ï¼ˆä½ ä½ä¼°äº†3å€ï¼‰ ç¬¬5éƒ¨åˆ†ï¼š$\\log \\frac{P(x)}{Q(x)}$ å«ä¹‰ï¼šæŠŠæ¯”å€¼è½¬æˆä¿¡æ¯é‡ï¼ˆbitsï¼‰ ä¾‹å­ï¼š$\\log_2(3) = 1.58$ bits ç¬¬6éƒ¨åˆ†ï¼š$P(x) \\cdot \\log \\frac{P(x)}{Q(x)}$ å«ä¹‰ï¼šç”¨çœŸå®æ¦‚ç‡åŠ æƒï¼ˆé‡è¦ï¼ï¼‰ åªæœ‰åœ¨çœŸç›¸ç»å¸¸å‘ç”Ÿçš„åœ°æ–¹çŠ¯é”™ï¼Œæ‰ä¼šè¢«ä¸¥é‡æƒ©ç½š ä¾‹å­ï¼š$0.6 \\times 1.58 = 0.95$ ç¬¬7éƒ¨åˆ†ï¼š$\\sum$ å«ä¹‰ï¼šå¯¹æ‰€æœ‰å¯èƒ½çš„ $x$ æ±‚å’Œï¼Œå¾—åˆ°å¹³å‡æµªè´¹ 6.3 å®Œæ•´è®¡ç®—å®ä¾‹åœºæ™¯ï¼šå¤©æ°”é¢„æŠ¥ å¤©æ°” $P$ (çœŸå®) $Q$ (é¢„æµ‹) $P/Q$ $\\log_2(P/Q)$ $P \\cdot \\log(P/Q)$ â˜€ï¸æ™´ 0.6 0.2 3.00 1.58 0.950 ğŸŒ§ï¸é›¨ 0.3 0.5 0.60 -0.74 -0.222 â˜ï¸é˜´ 0.1 0.3 0.33 -1.58 -0.158 æ€»å’Œ 0.570 bits ç»“æœè§£è¯»ï¼š KLæ•£åº¦ = 0.570 bitsæ„å‘³ç€ï¼šç”¨ä½ çš„é¢„æµ‹ $Q$ ä»£æ›¿çœŸå® $P$ï¼Œå¹³å‡æ¯æ¬¡å¤šæµªè´¹ 0.57 bits æœ€å¤§è´¡çŒ®æ¥è‡ªâ€æ™´å¤©â€çœŸå®60%ä½†ä½ åªé¢„æµ‹20%ï¼Œå·®è·å¤ªå¤§ï¼ â€œé›¨å¤©â€è´¡çŒ®æ˜¯è´Ÿçš„ä½ é«˜ä¼°äº†é›¨å¤©ï¼ˆé¢„æµ‹50%å®é™…30%ï¼‰ï¼Œåœ¨è¿™ä¸ªæ–¹å‘ä¸Šåè€Œâ€èµšäº†â€ä¿¡æ¯ä½†æ€»ä½“è¿˜æ˜¯äºçš„ï¼ˆå› ä¸ºæ™´å¤©äºå¤ªå¤šï¼‰ 6.4 KLæ•£åº¦çš„æ€§è´¨æ€§è´¨1ï¼šéè´Ÿæ€§ $$D_{KL}(P | Q) \\geq 0$$ æ°¸è¿œ â‰¥ 0ï¼Œå½“ä¸”ä»…å½“ $P = Q$ æ—¶ç­‰äº 0ã€‚ 6.4.1 Jensen ä¸ç­‰å¼ç§‘æ™®è¦è¯æ˜ KL æ•£åº¦éè´Ÿï¼Œæˆ‘ä»¬éœ€è¦å…ˆç†è§£ Jensen ä¸ç­‰å¼ã€‚ ä»€ä¹ˆæ˜¯å‡¹å‡½æ•°å’Œå‡¸å‡½æ•°ï¼Ÿ å‡¹å‡½æ•°ï¼ˆå¦‚ log xï¼‰ï¼š å‡¸å‡½æ•°ï¼ˆå¦‚ xÂ²ï¼‰ï¼š ___ /\\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ _/ \\_ å¼¦åœ¨æ›²çº¿ä¸‹æ–¹ å¼¦åœ¨æ›²çº¿ä¸Šæ–¹ åˆ¤æ–­æ–¹æ³•ï¼šåœ¨æ›²çº¿ä¸Šä»»å–ä¸¤ç‚¹ï¼Œè¿ä¸€æ¡çº¿ï¼ˆå¼¦ï¼‰ å¼¦åœ¨æ›²çº¿ä¸‹æ–¹ â†’ å‡¹å‡½æ•° å¼¦åœ¨æ›²çº¿ä¸Šæ–¹ â†’ å‡¸å‡½æ•° Jensen ä¸ç­‰å¼çš„å†…å®¹ å¯¹äºå‡¹å‡½æ•° $f$ï¼ˆæ¯”å¦‚ $\\log$ï¼‰ï¼š $$f\\left(\\sum_i p_i x_i\\right) \\geq \\sum_i p_i f(x_i)$$ å…¶ä¸­ $p_i \\geq 0$ ä¸” $\\sum_i p_i = 1$ï¼ˆæ¦‚ç‡æƒé‡ï¼‰ é€šä¿—è§£é‡Šï¼š â€œå…ˆå¹³å‡å†å˜æ¢â€ â‰¥ â€œå…ˆå˜æ¢å†å¹³å‡â€ ç”¨å›¾æ¥ç†è§£ å‡è®¾åªæœ‰ä¸¤ä¸ªç‚¹ $x_1$ å’Œ $x_2$ï¼Œæƒé‡å„ 0.5ï¼š yâ”‚â”‚ A â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â— B â† f(xâ‚) å’Œ f(xâ‚‚)â”‚ â•² ï¼ï¼ï¼ï¼ï¼ï¼ï¼ï¼â•±â”‚ â•² â— M' â•± â† å¼¦çš„ä¸­ç‚¹ï¼ˆå…ˆå˜æ¢å†å¹³å‡ï¼‰â”‚ â•² â†“ â•±â”‚ â•² â†“ â•±â”‚ â•² â— M â•± â† æ›²çº¿ä¸Šçš„ç‚¹ï¼ˆå…ˆå¹³å‡å†å˜æ¢ï¼‰â”‚ â•² â•±â”‚ â•²_____â•± â† å‡¹å‡½æ•°æ›²çº¿â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ x xâ‚ ä¸­ç‚¹ xâ‚‚ M = $f\\left(\\frac{x_1+x_2}{2}\\right)$ï¼ˆå…ˆå¹³å‡å†å˜æ¢ï¼‰ Mâ€™ = $\\frac{f(x_1)+f(x_2)}{2}$ï¼ˆå…ˆå˜æ¢å†å¹³å‡ï¼‰ å› ä¸ºæ›²çº¿æ˜¯å‡¹çš„ï¼Œæ‰€ä»¥ M åœ¨ Mâ€™ ä¸Šæ–¹ï¼Œå³ $f(\\text{å¹³å‡}) \\geq \\text{å¹³å‡}(f)$ 6.4.2 KL æ•£åº¦éè´Ÿæ€§çš„è¯¦ç»†è¯æ˜ ç›®æ ‡ï¼šè¯æ˜ $D_{KL}(P|Q) \\geq 0$ ç¬¬ä¸€æ­¥ï¼šå†™å‡ºå®šä¹‰ $$D_{KL}(P|Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}$$ ç¬¬äºŒæ­¥ï¼šå˜å½¢ $$= -\\sum_x P(x) \\log \\frac{Q(x)}{P(x)}$$ ä»¤ $r(x) = \\frac{Q(x)}{P(x)}$ï¼Œåˆ™ï¼š $$D_{KL}(P|Q) = -\\sum_x P(x) \\log r(x) = -\\mathbb{E}_P[\\log r(x)]$$ ç¬¬ä¸‰æ­¥ï¼šåº”ç”¨ Jensen ä¸ç­‰å¼ å› ä¸º $\\log$ æ˜¯å‡¹å‡½æ•°ï¼Œæ ¹æ® Jensen ä¸ç­‰å¼ï¼š $$\\sum_x P(x) \\log r(x) \\leq \\log \\left(\\sum_x P(x) \\cdot r(x)\\right)$$ æ³¨æ„ï¼šå‡¹å‡½æ•°çš„ Jensen ä¸ç­‰å¼æ˜¯ $\\leq$ ç¬¬å››æ­¥ï¼šè®¡ç®—å³è¾¹ $$\\sum_x P(x) \\cdot r(x) = \\sum_x P(x) \\cdot \\frac{Q(x)}{P(x)} = \\sum_x Q(x) = 1$$ æ‰€ä»¥ï¼š $$\\sum_x P(x) \\log r(x) \\leq \\log(1) = 0$$ ç¬¬äº”æ­¥ï¼šå¾—å‡ºç»“è®º $$D_{KL}(P|Q) = -\\sum_x P(x) \\log r(x) \\geq -0 = 0$$ è¯æ¯•ï¼ âœ“ ç­‰å·ä»€ä¹ˆæ—¶å€™æˆç«‹ï¼Ÿ å½“ $r(x) = \\frac{Q(x)}{P(x)} = \\text{å¸¸æ•°}$ å¯¹æ‰€æœ‰ $x$ æˆç«‹æ—¶ã€‚ åˆå› ä¸º $\\sum P(x) = \\sum Q(x) = 1$ï¼Œæ‰€ä»¥è¿™ä¸ªå¸¸æ•°åªèƒ½æ˜¯ 1ã€‚ å³ï¼š**$P(x) = Q(x)$ å¯¹æ‰€æœ‰ $x$ æˆç«‹æ—¶ï¼Œ$D_{KL} = 0$** æ€§è´¨2ï¼šä¸å¯¹ç§°æ€§ï¼ˆè¶…çº§é‡è¦ï¼ï¼‰ $$D_{KL}(P | Q) eq D_{KL}(Q | P)$$ KLæ•£åº¦ä¸æ˜¯è·ç¦»ï¼æ–¹å‘å¾ˆé‡è¦ï¼ 6.4.3 å‰å‘ KL vs åå‘ KLï¼šæ·±åº¦è§£æ KL æ•£åº¦çš„ä¸å¯¹ç§°æ€§æ˜¯å…¶æœ€é‡è¦çš„ç‰¹æ€§ä¹‹ä¸€ã€‚ç†è§£å‰å‘å’Œåå‘ KL çš„åŒºåˆ«ï¼Œå¯¹äºæ·±åº¦å­¦ä¹ ä¸­çš„åº”ç”¨è‡³å…³é‡è¦ã€‚ æ ¸å¿ƒåŒºåˆ«ï¼šè°åœ¨åšä¸»ï¼Ÿ ç‰¹æ€§ å‰å‘ KL $D_{KL}(P|Q)$ åå‘ KL $D_{KL}(Q|P)$ å…¬å¼ $\\sum P(x) \\log \\frac{P(x)}{Q(x)}$ $\\sum Q(x) \\log \\frac{Q(x)}{P(x)}$ è§†è§’ ç«™åœ¨çœŸç›¸ $P$ çš„è§’åº¦ ç«™åœ¨æ¨¡å‹ $Q$ çš„è§’åº¦ æ ¸å¿ƒæƒé‡ $P(x)$ï¼ˆçœŸç›¸å†³å®šä¸€åˆ‡ï¼‰ $Q(x)$ï¼ˆæ¨¡å‹è‡ªå·±å†³å®šï¼‰ æƒ©ç½šæ¡ä»¶ $P0$ ä½† $Qâ‰ˆ0$ æ—¶çˆ†ç‚¸ $Q0$ ä½† $Pâ‰ˆ0$ æ—¶çˆ†ç‚¸ è¡Œä¸ºæ¨¡å¼ Zero-Avoidingï¼ˆé¿å…é—æ¼ï¼‰ Zero-Forcingï¼ˆå¼ºåˆ¶ç²¾å‡†ï¼‰ ç»“æœ æ¨¡ç³Šä½†å…¨é¢ å°–é”ä½†ä¿å®ˆ åˆ«å Mean-seekingï¼ˆå¯»æ‰¾å¹³å‡ï¼‰ Mode-seekingï¼ˆå¯»æ‰¾æ¨¡æ€ï¼‰ ä¸ºä»€ä¹ˆâ€æƒé‡â€å†³å®šäº†å‘½è¿ï¼Ÿ å…¬å¼é‡Œåªæ˜¯äº¤æ¢äº† $P$ å’Œ $Q$ çš„ä½ç½®ï¼Œä¸ºä»€ä¹ˆè¡Œä¸ºå·®å¼‚è¿™ä¹ˆå¤§ï¼Ÿ**å› ä¸ºå‰é¢çš„æƒé‡é¡¹å……å½“äº†â€å¼€å…³â€**ã€‚ è¯¦ç»†ä¾‹å­ï¼šåŒå³°åˆ†å¸ƒé—®é¢˜ å‡è®¾çœŸå®åˆ†å¸ƒ $P$ æ˜¯ä¸€ä¸ªåŒå³°åˆ†å¸ƒï¼ˆæ¯”å¦‚èº«é«˜åˆ†å¸ƒï¼šç”·æ€§å³° + å¥³æ€§å³°ï¼‰ï¼š çœŸå®åˆ†å¸ƒ Pï¼ˆåŒå³°ï¼‰: â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ------------------------------------------- å³°1 å³°2 ä½†æˆ‘ä»¬çš„æ¨¡å‹ $Q$ åªèƒ½æ˜¯å•å³°åˆ†å¸ƒï¼ˆæ¯”å¦‚åªèƒ½ç”¨ä¸€ä¸ªé«˜æ–¯åˆ†å¸ƒï¼‰ã€‚ é—®é¢˜ï¼š$Q$ åº”è¯¥æ€ä¹ˆæ”¾ï¼Ÿ æƒ…å†µ1ï¼šå‰å‘ KLï¼ˆ$D_{KL}(P|Q)$ï¼‰â€” Zero Avoiding æƒ©ç½šæœºåˆ¶ï¼šåœ¨ $P(x) 0$ çš„åœ°æ–¹ï¼Œå¦‚æœ $Q(x) \\approx 0$ï¼Œåˆ™ $\\log \\frac{P(x)}{Q(x)} \\to +\\infty$ åæœï¼š$Q$ å¿…é¡»è¦†ç›– $P$ æœ‰æ¦‚ç‡çš„æ‰€æœ‰åœ°æ–¹ï¼Œå¦åˆ™ä¼šè¢«ä¸¥é‡æƒ©ç½šï¼ å‰å‘ KL çš„ç»“æœ â€” Q å˜å¾—\"æ¨¡ç³Šä½†å…¨é¢\": â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆ â† Pï¼ˆçœŸå®ï¼‰ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚ â† Qï¼ˆæ¨¡å‹ï¼‰ â”‚ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â”‚ è¦†ç›–äº†ä¸¤ä¸ªå³° â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ä½†ä¸­é—´ä¹Ÿæœ‰æ¦‚ç‡ ç‰¹ç‚¹ï¼šå®å¯åœ¨ä¸­é—´â€ççŒœâ€ï¼Œä¹Ÿä¸èƒ½æ¼æ‰ä»»ä½•ä¸€ä¸ªå³° â†’ Mean-seekingï¼ˆå‡å€¼å¯»æ‰¾ï¼‰ æƒ…å†µ2ï¼šåå‘ KLï¼ˆ$D_{KL}(Q|P)$ï¼‰â€” Zero Forcing æƒ©ç½šæœºåˆ¶ï¼šåœ¨ $Q(x) 0$ çš„åœ°æ–¹ï¼Œå¦‚æœ $P(x) \\approx 0$ï¼Œåˆ™ $\\log \\frac{Q(x)}{P(x)} \\to +\\infty$ åæœï¼š$Q$ åªèƒ½åœ¨ $P$ æœ‰æ¦‚ç‡çš„åœ°æ–¹å‡ºç°ï¼Œå¦åˆ™ä¼šè¢«ä¸¥é‡æƒ©ç½šï¼ åå‘ KL çš„ç»“æœ â€” Q å˜å¾—\"å°–é”ä½†ä¿å®ˆ\": â–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆ â† Pï¼ˆçœŸå®ï¼‰ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ â–ˆâ–ˆâ–ˆâ–ˆ â† Qï¼ˆæ¨¡å‹ï¼‰ â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ åªé€‰æ‹©ä¸€ä¸ªå³° â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ ç²¾å‡†ä½†ä¸å…¨é¢ ç‰¹ç‚¹ï¼šå®å¯æ”¾å¼ƒä¸€ä¸ªå³°ï¼Œä¹Ÿä¸èƒ½åœ¨ $P=0$ çš„åœ°æ–¹æœ‰æ¦‚ç‡ â†’ Mode-seekingï¼ˆæ¨¡å¼å¯»æ‰¾ï¼‰ å½¢è±¡æ¯”å–»æ€»ç»“ å‰å‘KLï¼šä¸¥å‰çš„è€å¸ˆï¼ˆTeacher Forcingï¼‰ å¿ƒæ€ï¼šè€å¸ˆæ‹¿ç€æ ‡å‡†ç­”æ¡ˆ $P$ å·¡è§† è§„åˆ™ï¼šâ€æˆ‘è€ƒçº²ï¼ˆ$P$ï¼‰é‡Œæœ‰çš„çŸ¥è¯†ç‚¹ï¼Œä½ ï¼ˆ$Q$ï¼‰å¿…é¡»éƒ½è¦ä¼šï¼å¦‚æœä½ äº¤ç™½å·ï¼ˆ$Q \\approx 0$ï¼‰ï¼Œæˆ‘å°±ç»™ä½ æŒ‚ç§‘ï¼â€ ç»“æœï¼šä¸ºäº†ä¸æŒ‚ç§‘ï¼Œä½ è¢«è¿«æŠŠæ‰€æœ‰å¯èƒ½è€ƒçš„ä¸œè¥¿éƒ½å†™ä¸Šå»ã€‚å“ªæ€•æœ‰äº›åœ°æ–¹ä½ ä¸æ‡‚ï¼Œä½ ä¹Ÿå¾—ç¡¬å‡‘ã€‚ åå‘KLï¼šä¿å®ˆçš„å­¦ç”Ÿï¼ˆStudentâ€™s Choiceï¼‰ å¿ƒæ€ï¼šä½ è‡ªå·±æ‹¿ç€æ‰‹ç”µç­’ $Q$ åœ¨é›·åŒº $P$ æ¢è·¯ è§„åˆ™ï¼šâ€åªè¦æˆ‘è¸©åˆ°äº†é›·ï¼ˆ$P \\approx 0$ï¼‰ï¼Œæˆ‘å°±æ­»å®šäº†ï¼ˆLoss æ— ç©·å¤§ï¼‰ã€‚ä½†æˆ‘æ²¡è¸©åˆ°çš„åœ°æ–¹ï¼Œé›·å†å¤§ä¹Ÿç‚¸ä¸åˆ°æˆ‘ã€‚â€ ç»“æœï¼šä¸ºäº†ä¿å‘½ï¼Œä½ å‘ç°æœ€å¥½çš„ç­–ç•¥æ˜¯ç¼©åœ¨ä¸€ä¸ªç»å¯¹å®‰å…¨çš„åœ°æ–¹ä¸åŠ¨ã€‚ä½ æ”¾å¼ƒäº†æ¢ç´¢å…¶ä»–åŒºåŸŸï¼Œåªæ­»å®ˆä¸€ä¸ªå±±å¤´ã€‚ ç¬¬ä¸ƒç« ï¼šå®æˆ˜åº”ç”¨7.1 RLHFä¸­çš„KLæƒ©ç½šåœ¨è®­ç»ƒå¤§æ¨¡å‹ï¼ˆå¦‚ ChatGPTï¼‰æ—¶ï¼Œæœ‰ä¸ªæ­¥éª¤å« RLHFï¼ˆåŸºäºäººç±»åé¦ˆçš„å¼ºåŒ–å­¦ä¹ ï¼‰ã€‚ é—®é¢˜ï¼šå¦‚æœåªä¼˜åŒ–äººç±»æ‰“åˆ†ï¼ˆRewardï¼‰ï¼Œæ¨¡å‹å¯èƒ½ä¼šï¼š ç–¯ç‹‚è¯´å½©è™¹å±æ‹¿é«˜åˆ† å¿˜è®°æ­£å¸¸çš„è¯­è¨€è§„åˆ™ æœ€ç»ˆå´©æºƒæˆèƒ¡è¨€ä¹±è¯­ è§£å†³æ–¹æ¡ˆï¼šåŠ KLæƒ©ç½š $$\\max ; \\mathbb{E}[\\text{Reward}] - \\beta \\cdot D_{KL}(\\pi_{\\text{new}} | \\pi_{\\text{old}})$$ å„éƒ¨åˆ†å«ä¹‰ï¼š ç¬¦å· å«ä¹‰ $\\pi_{\\text{new}}$ æ–°æ¨¡å‹ï¼ˆæ­£åœ¨è®­ç»ƒçš„ï¼‰ $\\pi_{\\text{old}}$ æ—§æ¨¡å‹ï¼ˆåŸºåº§æ¨¡å‹ï¼‰ $\\text{Reward}$ äººç±»æ‰“åˆ†ï¼ˆè¶Šé«˜è¶Šå¥½ï¼‰ $D_{KL}$ KLæƒ©ç½šï¼ˆé˜²æ­¢ç¦»æ—§æ¨¡å‹å¤ªè¿œï¼‰ $\\beta$ è°ƒèŠ‚å¼ºåº¦ï¼ˆ$\\beta$ è¶Šå¤§è¶Šä¿å®ˆï¼‰ å½¢è±¡æ¯”å–»ï¼š KLæƒ©ç½š = é£ç­çº¿ è®©ä½ é£é«˜ï¼ˆè¿½æ±‚é«˜åˆ†ï¼‰ ä½†æ‹‰ä½ä½ åˆ«é£ä¸¢ï¼ˆä¿æŒè¯­è¨€èƒ½åŠ›ï¼‰ 7.2 å…¶ä»–åº”ç”¨ é¢†åŸŸ åº”ç”¨ KLæ•£åº¦çš„ä½œç”¨ VAE å˜åˆ†è‡ªç¼–ç å™¨ ELBOæŸå¤±å‡½æ•°çš„æ ¸å¿ƒ çŸ¥è¯†è’¸é¦ å¤§æ¨¡å‹â†’å°æ¨¡å‹ è®©å°æ¨¡å‹æ¨¡ä»¿å¤§æ¨¡å‹çš„è¾“å‡ºåˆ†å¸ƒ GAN ç”Ÿæˆå¯¹æŠ—ç½‘ç»œ è¡¡é‡ç”Ÿæˆåˆ†å¸ƒå’ŒçœŸå®åˆ†å¸ƒçš„å·®è· DPO ç›´æ¥åå¥½ä¼˜åŒ– æ–°ä¸€ä»£RLHFï¼Œç›´æ¥ä¼˜åŒ–KLçº¦æŸä¸‹çš„åå¥½ ä¿¡æ¯æ£€ç´¢ æœç´¢å¼•æ“ è®¡ç®—æ–‡æ¡£ç›¸ä¼¼åº¦ ç¬¬å…«ç« ï¼šæ€»ç»“8.1 çŸ¥è¯†åœ°å›¾ ä»ä¿¡æ¯è®ºåˆ°KLæ•£åº¦çš„å®Œæ•´è·¯å¾„ï¼š ä¿¡æ¯é‡ I(x) = -log P(x) â†“ç†µ H(P) = æœŸæœ›[I(x)] = -Î£ P(x)log P(x) â†“äº¤å‰ç†µ H(P,Q) = -Î£ P(x)log Q(x) â†“KLæ•£åº¦ = H(P,Q) - H(P) = Î£ P(x)log[P(x)/Q(x)] 8.2 è®°å¿†å£è¯€6å¥è¯è®°ä½KLæ•£åº¦ï¼š 1ï¸âƒ£ KLæ•£åº¦ä¸æ˜¯è·ç¦»ï¼Œæ˜¯â€ä¿¡æ¯æµªè´¹â€ ç”¨ $Q$ ç¼–ç  $P$ æ¯”ç”¨ $P$ ç¼–ç  $P$ å¤šæµªè´¹çš„ bits 2ï¸âƒ£ æ°¸è¿œ â‰¥ 0ï¼Œ= 0 å½“ä¸”ä»…å½“ P = Q å®Œç¾åŒ¹é…æ—¶æ— æµªè´¹ 3ï¸âƒ£ ä¸å¯¹ç§°ï¼$D_{KL}(P|Q) eq D_{KL}(Q|P)$ å‰å‘å…³æ³¨è¦†ç›–ï¼Œåå‘å…³æ³¨ç²¾ç¡® 4ï¸âƒ£ å‰å‘KLè®©æ¨¡å‹â€ä¸é—æ¼â€ï¼ˆZero Avoidingï¼‰ $P$ æœ‰çš„åœ°æ–¹ $Q$ å¿…é¡»æœ‰ 5ï¸âƒ£ åå‘KLè®©æ¨¡å‹â€ä¸å‡ºé”™â€ï¼ˆZero Forcingï¼‰ $Q$ æœ‰çš„åœ°æ–¹å¿…é¡»ç¬¦åˆ $P$ 6ï¸âƒ£ åœ¨æ·±åº¦å­¦ä¹ ä¸­å¸¸ä½œæ­£åˆ™é¡¹é˜²æ­¢â€å¿˜æœ¬â€ RLHFã€VAEã€è’¸é¦éƒ½ç”¨å®ƒ 8.3 å…¬å¼é€ŸæŸ¥å®Œæ•´å…¬å¼ï¼š $$D_{KL}(P | Q) = \\sum_{x} P(x) \\log \\frac{P(x)}{Q(x)}$$ ç­‰ä»·å½¢å¼ï¼š $$\\begin{aligned}D_{KL}(P | Q) = H(P, Q) - H(P) \\= \\sum_{x} P(x) \\log P(x) - \\sum_{x} P(x) \\log Q(x) \\= \\mathbb{E}_{x \\sim P} \\left[ \\log \\frac{P(x)}{Q(x)} \\right]\\end{aligned}$$ è¿ç»­æƒ…å†µï¼š $$D_{KL}(P | Q) = \\int P(x) \\log \\frac{P(x)}{Q(x)} dx$$ 8.4 å¸¸è§è¯¯åŒº è¯¯åŒº çœŸç›¸ KLæ•£åº¦æ˜¯è·ç¦» âŒ å®ƒä¸æ»¡è¶³å¯¹ç§°æ€§å’Œä¸‰è§’ä¸ç­‰å¼ KLæ•£åº¦å¯ä»¥ä¸ºè´Ÿ âŒ æ°¸è¿œ â‰¥ 0 $D_{KL}(P|Q)$ å’Œ $D_{KL}(Q|P)$ ç›¸ç­‰ âŒ å®Œå…¨ä¸åŒï¼æ–¹å‘å¾ˆé‡è¦ KLæ•£åº¦è¶Šå¤§è¶Šå¥½ âŒ è¶Šå°è¶Šå¥½ï¼Œ0 = å®Œç¾ åªæœ‰æ·±åº¦å­¦ä¹ ç”¨KLæ•£åº¦ âŒ ä¿¡æ¯è®ºã€ç»Ÿè®¡å­¦ã€ç‰©ç†å­¦éƒ½ç”¨ ğŸ“š å»¶ä¼¸é˜…è¯»æ¨èèµ„æº ç»å…¸æ•™æ ã€Šä¿¡æ¯è®ºåŸºç¡€ã€‹ï¼ˆElements of Information Theoryï¼‰ ã€Šæ·±åº¦å­¦ä¹ ã€‹ï¼ˆDeep Learning Bookï¼‰ç¬¬3ç«  åœ¨çº¿èµ„æº Distill.pub - KL Divergenceå¯è§†åŒ– 3Blue1Brown - ä¿¡æ¯è®ºè§†é¢‘ ç›¸å…³æ¦‚å¿µ JSæ•£åº¦ï¼ˆJensen-Shannon Divergenceï¼‰ Wassersteinè·ç¦» f-æ•£åº¦ï¼ˆf-Divergenceï¼‰ ç»ƒä¹ é¢˜åˆçº§ï¼š è®¡ç®—å‡åŒ€åˆ†å¸ƒ $P=[0.25, 0.25, 0.25, 0.25]$ çš„ç†µ è¯æ˜ï¼šä¸ºä»€ä¹ˆ $D_{KL}(P | Q) \\geq 0$ï¼Ÿ ä¸­çº§ï¼š3. æ¨å¯¼ï¼šä¸ºä»€ä¹ˆäº¤å‰ç†µå¯ä»¥ä½œä¸ºåˆ†ç±»ä»»åŠ¡çš„æŸå¤±å‡½æ•°ï¼Ÿ4. å¯¹æ¯”å‰å‘KLå’Œåå‘KLåœ¨å˜åˆ†æ¨æ–­ä¸­çš„åº”ç”¨ é«˜çº§ï¼š5. å®ç°ä¸€ä¸ªVAEï¼Œè§‚å¯ŸELBOä¸­KLé¡¹çš„ä½œç”¨6. åˆ†æPPOç®—æ³•ä¸­KLçº¦æŸçš„è¶…å‚æ•° $\\beta$ å¦‚ä½•å½±å“è®­ç»ƒç¨³å®šæ€§ ç»ƒä¹ é¢˜å‚è€ƒç­”æ¡ˆåˆçº§é¢˜ç­”æ¡ˆ å‡åŒ€åˆ†å¸ƒçš„ç†µ $$H(P) = -\\sum_{i=1}^{4} 0.25 \\times \\log_2(0.25) = -4 \\times 0.25 \\times (-2) = 2 \\text{ bits}$$ å‡åŒ€åˆ†å¸ƒçš„ç†µæœ€å¤§ï¼Œå› ä¸ºä¸ç¡®å®šæ€§æœ€é«˜ã€‚ KL æ•£åº¦éè´Ÿæ€§è¯æ˜ï¼ˆè§ 6.4.2 èŠ‚çš„è¯¦ç»†è¯æ˜ï¼‰ ä¸­çº§é¢˜ç­”æ¡ˆ äº¤å‰ç†µä½œä¸ºåˆ†ç±»æŸå¤±å‡½æ•° åœ¨åˆ†ç±»ä»»åŠ¡ä¸­ï¼š $P$ = çœŸå®æ ‡ç­¾çš„ one-hot ç¼–ç ï¼ˆå¦‚ $[0, 1, 0, 0]$ï¼‰ $Q$ = æ¨¡å‹é¢„æµ‹çš„æ¦‚ç‡åˆ†å¸ƒï¼ˆå¦‚ $[0.1, 0.7, 0.1, 0.1]$ï¼‰ æŸå¤±å‡½æ•°ï¼š$$\\text{Loss} = H(P, Q) = -\\sum_i P_i \\log Q_i$$ å› ä¸º $P$ ä¸­åªæœ‰ä¸€ä¸ªä½ç½®ä¸º 1ï¼Œå…¶ä½™ä¸º 0ï¼Œæ‰€ä»¥ï¼š$$\\text{Loss} = -\\log Q_{\\text{true}}$$ è¿™å°±æ˜¯è´Ÿå¯¹æ•°ä¼¼ç„¶ï¼ˆNegative Log-Likelihoodï¼‰ï¼Œæœ€å°åŒ–å®ƒç­‰ä»·äºæœ€å¤§åŒ–æ­£ç¡®ç±»åˆ«çš„é¢„æµ‹æ¦‚ç‡ã€‚ å‰å‘ vs åå‘ KL åœ¨å˜åˆ†æ¨æ–­ä¸­çš„åº”ç”¨ æ–¹é¢ å‰å‘ KL (ELBO) åå‘ KL å…¬å¼ $D_{KL}(Q|P)$ $D_{KL}(P|Q)$ å¯è®¡ç®—æ€§ âœ… å¯ä»¥ä¼˜åŒ– âŒ éœ€è¦å½’ä¸€åŒ–å¸¸æ•° è¡Œä¸º Mode-seeking Mean-seeking åº”ç”¨ VAE æ ‡å‡†åšæ³• æœŸæœ›ä¼ æ’­ï¼ˆEPï¼‰ VAE ä½¿ç”¨åå‘ KL æ˜¯å› ä¸ºå®ƒä¸éœ€è¦çŸ¥é“ $P$ çš„å½’ä¸€åŒ–å¸¸æ•°ã€‚ 8.5 æ•°å€¼è®¡ç®—å®ä¾‹ å®Œæ•´ç¤ºä¾‹ï¼šç¦»æ•£åˆ†å¸ƒçš„ KL æ•£åº¦è®¡ç®— è®¾æœ‰ç¦»æ•£åˆ†å¸ƒ P å’Œ Qï¼Œå–å€¼ä¸º {A, B, C}ï¼š å–å€¼ P (çœŸå®) Q (æ¨¡å‹) P/Q log(P/Q) PÂ·log(P/Q) A 0.5 0.4 1.25 0.32 0.16 B 0.3 0.4 0.75 -0.42 -0.13 C 0.2 0.2 1.00 0 0 åˆè®¡ 1.0 1.0 â€” â€” â‰ˆ 0.03 bits è®¡ç®—æ­¥éª¤ï¼š import numpy as npP = np.array([0.5, 0.3, 0.2])Q = np.array([0.4, 0.4, 0.2])# è®¡ç®— KL æ•£åº¦D_KL = np.sum(P * np.log(P / Q))print(f\"D_KL(P||Q) = {D_KL:.4f} bits\")# è¾“å‡º: D_KL(P||Q) = 0.0361 bits ç»“æœè§£è¯»ï¼š KL æ•£åº¦ = 0.036 bitsï¼šç”¨ Q ç¼–ç  P çš„ä¿¡æ¯ï¼Œå¹³å‡æ¯ä¸ªç¬¦å·å¤šæµªè´¹ 0.036 bits ä¸»è¦è¯¯å·®æ¥æºï¼šA ç‚¹è¢«ä½ä¼°ï¼ˆP=0.5, Q=0.4ï¼‰ å½“ P = Q æ—¶ï¼ŒD_KL = 0ï¼ˆå®Œç¾åŒ¹é…ï¼‰ ğŸ‰ ç»“è¯­æ­å–œä½ ï¼å¦‚æœä½ è¯»åˆ°è¿™é‡Œï¼Œä½ å·²ç»ä»é›¶å¼€å§‹ï¼Œå®Œå…¨ç†è§£äº†KLæ•£åº¦çš„æœ¬è´¨ï¼š âœ… ä»ä¿¡æ¯è®ºå‡ºå‘ï¼Œç†è§£äº†ä¿¡æ¯é‡ã€ç†µã€äº¤å‰ç†µ âœ… æŒæ¡äº†KLæ•£åº¦çš„å…¬å¼å’Œæ¯ä¸ªå˜é‡çš„å«ä¹‰ âœ… é€šè¿‡å›¾ç¤ºå’Œä¾‹å­å»ºç«‹äº†ç›´è§‰ âœ… äº†è§£äº†åœ¨æ·±åº¦å­¦ä¹ ä¸­çš„å®é™…åº”ç”¨ æœ€åçš„å»ºè®®ï¼š KLæ•£åº¦ä¸æ˜¯ç”¨æ¥èƒŒçš„ï¼Œè€Œæ˜¯ç”¨æ¥ç†è§£çš„ã€‚æ¯æ¬¡åœ¨è®ºæ–‡æˆ–ä»£ç ä¸­çœ‹åˆ°å®ƒï¼Œé—®è‡ªå·±ä¸‰ä¸ªé—®é¢˜ï¼š è¿™é‡Œçš„ $P$ æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆçœŸå®åˆ†å¸ƒï¼‰ è¿™é‡Œçš„ $Q$ æ˜¯ä»€ä¹ˆï¼Ÿï¼ˆæ¨¡å‹åˆ†å¸ƒï¼‰ ä¸ºä»€ä¹ˆè¦ç”¨KLæ•£åº¦ï¼Ÿï¼ˆé˜²æ­¢è¿‡æ‹Ÿåˆï¼Ÿçº¦æŸè¾“å‡ºï¼Ÿï¼‰ å¸¦ç€è¿™äº›é—®é¢˜ï¼Œä½ ä¼šå‘ç°KLæ•£åº¦æ— å¤„ä¸åœ¨ï¼Œè€Œä½ å·²ç»å®Œå…¨ç†è§£å®ƒäº†ã€‚ ğŸ“ æ›´æ–°æ—¥å¿— 2026-02-08ï¼šé¦–æ¬¡å‘å¸ƒï¼ŒåŒ…å«å®Œæ•´çš„æ•°å­¦è¯æ˜ã€å‰å‘/åå‘KLæ·±åº¦è§£æå’Œ5å¼ å¯è§†åŒ–å›¾è¡¨","tags":["Python"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"InstructGPT ä¸ RLHF æ·±åº¦è§£æï¼šè´¹æ›¼è§†è§’ä¸‹çš„åŸç†ä¸å®ç°","path":"/2026/02/07/InstructGPT-RLHF-Complete-Guide/","content":"ğŸ¯ é¢è¯•å®˜è§†è§’ï¼šè¿™ç¯‡æ–‡ç« ä¸ä»…ä»…æ˜¯æ•™ç¨‹ï¼Œæ›´æ˜¯ä½ çš„é¢è¯•å¤ä¹ æçº²ã€‚æˆ‘ä»¬ä¸å †ç Œä»£ç ï¼Œè€Œæ˜¯æ·±å…¥åˆ° åŸç† (Principles)ã€è¾“å…¥è¾“å‡ºæµ (I/O Flow) å’Œ **æ•°å­¦æ¨å¯¼ (Math Derivation)**ã€‚å¦‚æœä½ èƒ½ç”¨æœ¬æ–‡çš„é€»è¾‘æŠŠ RLHF è®²æ¸…æ¥šï¼ŒP7/L6 çº§åˆ«çš„ç®—æ³•å²—é¢è¯•ä¸åœ¨è¯ä¸‹ã€‚ 0. è´¹æ›¼æ—¶åˆ»ï¼šä»€ä¹ˆæ˜¯ Alignmentï¼Ÿæƒ³è±¡ä½ æ­£åœ¨è®­ç»ƒä¸€åªå¤©æ‰é¹¦é¹‰ï¼ˆGPT-3ï¼‰ã€‚å®ƒè¯»éäº†ä¸–ç•Œä¸Šçš„æ‰€æœ‰ä¹¦ï¼Œèƒ½è¡¥å…¨ä»»ä½•å¥å­ã€‚ ä½ è¯´ï¼šâ€œåºŠå‰æ˜æœˆå…‰â€ï¼Œå®ƒæ¥ï¼šâ€œç–‘æ˜¯åœ°ä¸Šéœœâ€ã€‚ ä½ é—®ï¼šâ€œå¦‚ä½•åˆ¶é€ æ¯’è¯ï¼Ÿâ€ï¼Œå®ƒæ¥ï¼šâ€œé¦–å…ˆä½ éœ€è¦â€¦â€¦â€ï¼ˆå› ä¸ºå®ƒåœ¨ä¾¦æ¢å°è¯´é‡Œçœ‹è¿‡ï¼‰ã€‚ é—®é¢˜æ¥äº†ï¼šè¿™åªé¹¦é¹‰å¤ªâ€œè€¿ç›´â€äº†ï¼Œå®ƒä¸æ‡‚ä»€ä¹ˆæ˜¯å¯¹ï¼Œä»€ä¹ˆæ˜¯é”™ï¼Œä»€ä¹ˆæ˜¯å®‰å…¨ã€‚å®ƒåªæ˜¯åœ¨åšæ¦‚ç‡é¢„æµ‹ã€‚ RLHF (Reinforcement Learning from Human Feedback) å°±æ˜¯é€è¿™åªé¹¦é¹‰å»ä¸Šç¤¼ä»ªå­¦æ ¡çš„è¿‡ç¨‹ï¼š SFTï¼ˆå°å­¦ï¼‰ï¼šè€å¸ˆç»™å®ƒæ ‡å‡†ç­”æ¡ˆï¼Œè®©å®ƒç…§ç€æŠ„ï¼Œå­¦ä¼šâ€œåƒäººä¸€æ ·è¯´è¯â€ã€‚ RMï¼ˆä¸­è€ƒï¼‰ï¼šè€å¸ˆä¸å†ç»™ç­”æ¡ˆï¼Œè€Œæ˜¯ç»™å®ƒçš„å›ç­”æ‰“åˆ†ã€‚å®ƒå­¦ä¼šäº†â€œè€å¸ˆå–œæ¬¢ä»€ä¹ˆæ ·çš„å›ç­”â€ã€‚ PPOï¼ˆä½“è‚²ç‰¹è®­ï¼‰ï¼šæ²¡æœ‰è€å¸ˆç›¯ç€ï¼Œå®ƒè‡ªå·±æ ¹æ®ä¹‹å‰çš„æ‰“åˆ†æ ‡å‡†ä¸æ–­ç»ƒä¹ ï¼Œè¯•å›¾æ‹¿é«˜åˆ†ï¼ŒåŒæ—¶åˆ«æŠŠè„‘å­ç»ƒåäº†ï¼ˆKL Penaltyï¼‰ã€‚ 1. ç¬¬ä¸€é˜¶æ®µï¼šç›‘ç£å¾®è°ƒ (SFT) ä¸€å¥è¯è§£é‡Šï¼šä»â€œç»­å†™æœºå™¨â€å˜æˆâ€œé—®ç­”åŠ©æ‰‹â€ã€‚ 1.1 åŸç†æ·±åº¦è§£æé¢„è®­ç»ƒæ¨¡å‹ï¼ˆPretrained Modelï¼‰çš„ç›®æ ‡æ˜¯ $P(x_t|x_{","tags":["Python"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"GPT ç³»åˆ—æ·±åº¦è§£æï¼šä» GPT-1 åˆ° GPT-3","path":"/2026/02/05/2026-02-05-gpt-series-deep-dive/","content":"æ‘˜è¦ï¼šæœ¬æ–‡æ·±åº¦è§£æ OpenAI çš„ GPT ç³»åˆ—æ¨¡å‹æ¼”è¿›ä¹‹è·¯ã€‚ä» GPT-1 çš„é¢„è®­ç»ƒ+å¾®è°ƒèŒƒå¼ï¼Œåˆ° GPT-2 çš„é›¶æ ·æœ¬å°è¯•ï¼Œå†åˆ° GPT-3 çš„ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆIn-context Learningï¼‰ä¸è§„æ¨¡æ³•åˆ™ï¼ˆScaling Lawsï¼‰ã€‚æ–‡ç« è¯¦ç»†æ‹†è§£äº†æ¨¡å‹æ¶æ„ã€Tensor ç»´åº¦å˜åŒ–ã€è®­ç»ƒæ•°æ®æµï¼Œå¹¶é…æœ‰ 13 å¼ å¯è§†åŒ–å›¾è¡¨ä¸é€šä¿—æ˜“æ‡‚çš„è´¹æ›¼å¼è®²è§£ã€‚ ğŸ“‹ ç›®å½• é€šä¿—ç†è§£ï¼šGPT åˆ°åº•åœ¨åšä»€ä¹ˆï¼Ÿ NLP é¢„è®­ç»ƒæ—¶ä»£å…¨æ™¯ GPT vs BERT vs T5ï¼šæ ¸å¿ƒå·®å¼‚ GPT-1ï¼šPre-train + Fine-tune GPT-2ï¼šZero-shot Learning GPT-3ï¼šIn-context Learning ä¸‰ä»£æ¼”è¿›å¯¹æ¯”æ€»ç»“ ä»£ç å®ç°å‚è€ƒ è´¹æ›¼æ€»ç»“ï¼šæ•™ç»™å°ç™½å¬ ğŸ¯ é€šä¿—ç†è§£ï¼šGPT åˆ°åº•åœ¨åšä»€ä¹ˆï¼Ÿä¸€å¥è¯æ€»ç»“ GPT å°±æ˜¯ä¸€ä¸ªâ€ç»­å†™æœºå™¨â€ï¼šç»™å®ƒä¸€æ®µè¯çš„å¼€å¤´ï¼Œå®ƒé¢„æµ‹æ¥ä¸‹æ¥åº”è¯¥å†™ä»€ä¹ˆã€‚ ç”Ÿæ´»ç±»æ¯”ï¼šä½œå®¶ vs è€ƒç”Ÿä¸ºäº†ç›´è§‚ç†è§£ GPT çš„å·¥ä½œæ–¹å¼ï¼Œæˆ‘ä»¬æ¥çœ‹ä¸‹é¢è¿™å¼ å¯¹æ¯”å›¾ï¼š å›¾è¡¨æ·±åº¦è§£è¯»ï¼šä¸Šå›¾å±•ç¤ºäº†ä¸¤ç§æˆªç„¶ä¸åŒçš„è¯­è¨€å¤„ç†æ¨¡å¼ã€‚ GPT (ä¸Šéƒ¨åˆ†)ï¼šè¢«æç»˜æˆä¸€ä¸ªæ­£åœ¨æ‰“å­—çš„ä½œå®¶ã€‚å®ƒçš„è§†é‡æ˜¯â€å•å‘â€çš„ï¼ˆç»¿è‰²ç®­å¤´åªå‘å³ï¼‰ï¼Œæ„å‘³ç€å®ƒåªèƒ½çœ‹åˆ°å·²ç»å†™å‡ºæ¥çš„å†…å®¹ã€‚å°±åƒå†™å°è¯´ä¸€æ ·ï¼Œä½œè€…æ ¹æ®å‰æ–‡ Once upon a time æ¥æ„æ€ä¸‹ä¸€ä¸ªè¯ thereã€‚è¿™ç§è‡ªå›å½’ï¼ˆAutoregressiveï¼‰çš„ç‰¹æ€§ä½¿å®ƒå¤©ç”Ÿé€‚åˆæ–‡æœ¬ç”Ÿæˆä»»åŠ¡ã€‚ BERT (ä¸‹éƒ¨åˆ†)ï¼šè¢«æç»˜æˆä¸€ä¸ªæ­£åœ¨åšå®Œå½¢å¡«ç©ºçš„å­¦ç”Ÿã€‚å®ƒçš„è§†é‡æ˜¯â€åŒå‘â€çš„ï¼ˆçº¢è‰²ç®­å¤´åŒæ—¶æŒ‡å‘å·¦å³ï¼‰ï¼Œå®ƒå¯ä»¥åŒæ—¶çœ‹åˆ°ç©ºæ ¼å‰åçš„å†…å®¹ The ___ sat onã€‚è¿™ä½¿å®ƒåœ¨ç†è§£ä¸Šä¸‹æ–‡è¯­å¢ƒæ—¶éå¸¸å¼ºå¤§ï¼Œé€‚åˆåˆ†ç±»ã€å®ä½“è¯†åˆ«ç­‰ç†è§£ä»»åŠ¡ï¼Œä½†æ— æ³•åƒ GPT é‚£æ ·æµç•…åœ°ç”Ÿæˆæ–‡æœ¬ã€‚ ä¸ºä»€ä¹ˆè¿™ä¸ªç®€å•ä»»åŠ¡èƒ½äº§ç”Ÿâ€æ™ºèƒ½â€ï¼Ÿä½ å¯èƒ½ä¼šé—®ï¼Œé¢„æµ‹ä¸‹ä¸€ä¸ªè¯å¬èµ·æ¥å¾ˆç®€å•ï¼Œä¸ºä»€ä¹ˆèƒ½äº§ç”Ÿæ™ºèƒ½ï¼Ÿ è®­ç»ƒæ•°æ®ï¼šäº’è”ç½‘ä¸Šçš„æ‰€æœ‰æ–‡æœ¬ï¼ˆä¹¦ç±ã€ç½‘é¡µã€å¯¹è¯...ï¼‰å½“æ¨¡å‹å­¦ä¼šé¢„æµ‹ \"The capital of France is ___\"â†’ å®ƒå¿…é¡»\"çŸ¥é“\"æ³•å›½çš„é¦–éƒ½æ˜¯å·´é»å½“æ¨¡å‹å­¦ä¼šé¢„æµ‹ \"2 + 2 = ___\" â†’ å®ƒå¿…é¡»\"ç†è§£\"åŸºæœ¬æ•°å­¦å½“æ¨¡å‹å­¦ä¼šé¢„æµ‹ \"If it rains, I will bring an ___\"â†’ å®ƒå¿…é¡»\"æ¨ç†\"ä¸‹é›¨éœ€è¦å¸¦ä¼ç»“è®ºï¼šé¢„æµ‹ä¸‹ä¸€ä¸ªè¯ = è¢«è¿«å­¦ä¹ ä¸–ç•ŒçŸ¥è¯†ï¼ ğŸ“… NLP é¢„è®­ç»ƒæ—¶ä»£å…¨æ™¯é‡Œç¨‹ç¢‘æ—¶é—´çº¿ä¸‹å›¾æ¢³ç†äº† NLP é¢„è®­ç»ƒæ¨¡å‹å‘å±•çš„é»„é‡‘ä¸‰å¹´ï¼ˆ2017-2020ï¼‰ï¼š å›¾è¡¨æ·±åº¦è§£è¯»ï¼š èµ·ç‚¹ (2017.06)ï¼šTransformer çš„è¯ç”Ÿæ˜¯åŸç‚¹ã€‚Google å‘å¸ƒçš„ Attention Is All You Need è®ºæ–‡æå‡º Self-Attention æœºåˆ¶ï¼Œå½»åº•å–ä»£äº† RNN/LSTMï¼Œå…è®¸æ¨¡å‹å¹¶è¡Œè®­ç»ƒï¼Œä¸ºå¤§è§„æ¨¡æ¨¡å‹å¥ å®šäº†åŸºç¡€ã€‚ **åˆ†æ”¯ä¸€ (Decoder-only)**ï¼šå›¾è¡¨ä¸Šæ–¹æ˜¯ GPT å®¶æ—ã€‚ä» GPT-1 åˆ° GPT-2 å†åˆ° GPT-3ï¼ŒOpenAI åšæŒèµ°â€çº¯è§£ç å™¨â€è·¯çº¿ï¼Œä¸“æ³¨äºç”Ÿæˆèƒ½åŠ›ã€‚å¯ä»¥çœ‹åˆ°å‚æ•°é‡å‘ˆæŒ‡æ•°çº§çˆ†ç‚¸ï¼ˆ117M â†’ 1.5B â†’ 175Bï¼‰ã€‚ **åˆ†æ”¯äºŒ (Encoder-only)**ï¼šå›¾è¡¨ä¸‹æ–¹æ˜¯ BERT å®¶æ—ã€‚BERT å¼•å…¥ MLMï¼ˆæ©ç è¯­è¨€æ¨¡å‹ï¼‰ï¼Œåˆ·æ–°äº†å‡ ä¹æ‰€æœ‰ NLP æ¦œå•ï¼Œéšåè¡ç”Ÿå‡º RoBERTaã€DistilBERT ç­‰ä¼˜åŒ–ç‰ˆæœ¬ï¼Œä¸“æ³¨äºç†è§£ä»»åŠ¡ã€‚ **åˆ†æ”¯ä¸‰ (Encoder-Decoder)**ï¼šä¸­é—´æ˜¯ T5/BART ç­‰å°è¯•ç»Ÿä¸€ä¸¤è€…çš„æ¶æ„ï¼Œè¯•å›¾åœ¨ç†è§£å’Œç”Ÿæˆä¹‹é—´æ‰¾åˆ°å¹³è¡¡ã€‚ æ¨¡å‹è§„æ¨¡æ¼”è¿›è¿™çŸ­çŸ­å‡ å¹´é—´ï¼Œæ¨¡å‹å¤§å°å‘ç”Ÿäº†ä»€ä¹ˆå˜åŒ–ï¼Ÿ å›¾è¡¨æ·±åº¦è§£è¯»ï¼š è¿™æ˜¯ä¸€å¼ å¯¹æ•°åæ ‡å›¾ã€‚çºµè½´ä»£è¡¨å‚æ•°é‡ï¼ˆLog scaleï¼‰ã€‚ ELMo (94M) å’Œ GPT-1 (117M) å¤„äºèµ·æ­¥é˜¶æ®µï¼Œç›¸å½“äºâ€å°ä¸ªå­â€ã€‚ BERT-Large (340M) å®šä¹‰äº†å½“æ—¶çš„â€å¤§æ¨¡å‹â€æ ‡å‡†ã€‚ GPT-2 (1.5B) é¦–æ¬¡çªç ´ 10äº¿ å‚æ•°å¤§å…³ï¼Œè¯æ˜äº†æ¨¡å‹è¶Šå¤§æ•ˆæœè¶Šå¥½ã€‚ GPT-3 (175B) åˆ™æ˜¯ä¸€ä¸ªå·¨å¤§çš„é£è·ƒï¼ˆå³ä¸Šè§’é‚£ä¸ªé¥ä¸å¯åŠçš„ç‚¹ï¼‰ï¼Œå®ƒæ¯”ä¹‹å‰çš„æ¨¡å‹å¤§ä¸¤ä¸ªæ•°é‡çº§ï¼Œç›´æ¥å¼€å¯äº†â€å¤§æ¨¡å‹ï¼ˆLLMï¼‰â€æ—¶ä»£ã€‚ âš”ï¸ GPT vs BERT vs T5ï¼šæ ¸å¿ƒå·®å¼‚æ¶æ„å¯¹æ¯”ä¸‰è€…åœ¨ç¥ç»ç½‘ç»œæ¶æ„ä¸Šç©¶ç«Ÿæœ‰ä½•ä¸åŒï¼Ÿ å›¾è¡¨æ·±åº¦è§£è¯»ï¼š GPT (å·¦)ï¼šDecoder-only æ¶æ„ã€‚æ³¨æ„å…¶ä¸­çš„è¿æ¥çº¿æ˜¯å•å‘çš„ï¼ˆåªä»å·¦å‘å³ï¼‰ã€‚è¿™æ„å‘³ç€å¤„ç†ç¬¬ $i$ ä¸ªè¯æ—¶ï¼Œæ¨¡å‹åªèƒ½çœ‹åˆ° $0$ åˆ° $i-1$ ä¸ªè¯ã€‚Masked Self-Attention å¼ºåˆ¶äº†è¿™ç§å› æœå…³ç³»ã€‚ BERT (ä¸­)ï¼šEncoder-only æ¶æ„ã€‚è¿æ¥çº¿æ˜¯å…¨è¿æ¥çš„ï¼Œä»»ä½•ä½ç½®çš„è¯éƒ½èƒ½çœ‹åˆ°æ•´ä¸ªåºåˆ—çš„ä¿¡æ¯ã€‚è¿™å¯¹äºç†è§£å¥å­å«ä¹‰è‡³å…³é‡è¦ï¼Œä½†ä½¿å¾—å®ƒæ— æ³•åƒäººç±»è¯´è¯ä¸€æ ·é€è¯ç”Ÿæˆã€‚ T5 (å³)ï¼šEncoder-Decoder æ¶æ„ã€‚å·¦è¾¹æ˜¯ä¸€ä¸ªåŒå‘çš„ Encoderï¼ˆè¯»å…¥è¾“å…¥ï¼‰ï¼Œå³è¾¹æ˜¯ä¸€ä¸ªå•å‘çš„ Decoderï¼ˆç”Ÿæˆè¾“å‡ºï¼‰ã€‚è¿™å°±åƒæœºå™¨ç¿»è¯‘ï¼šå…ˆè¯»æ‡‚åŸæ–‡ï¼ˆEncodeï¼‰ï¼Œå†å†™å‡ºè¯‘æ–‡ï¼ˆDecodeï¼‰ã€‚ è®­ç»ƒç›®æ ‡å¯¹æ¯”å®ƒä»¬åœ¨é¢„è®­ç»ƒæ—¶åˆ†åˆ«åœ¨åšä»€ä¹ˆé¢˜ï¼Ÿ å›¾è¡¨æ·±åº¦è§£è¯»ï¼š GPT (Autoregressive LM)ï¼šä»»åŠ¡æ˜¯é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ã€‚å›¾ä¸­æ˜¾ç¤ºæ¨¡å‹çœ‹åˆ° The cat satï¼Œç›®æ ‡æ˜¯é¢„æµ‹ onã€‚è¿™æ˜¯æœ€è‡ªç„¶çš„è¯­è¨€ç”Ÿæˆä»»åŠ¡ã€‚ BERT (Masked LM)ï¼šä»»åŠ¡æ˜¯å®Œå½¢å¡«ç©ºã€‚å›¾ä¸­ sat è¢« [MASK] é®ä½äº†ï¼Œæ¨¡å‹éœ€è¦åˆ©ç”¨ä¸Šä¸‹æ–‡ The cat [MASK] on æŠŠ sat å¡«å›å»ã€‚ T5 (Span Corruption)ï¼šä»»åŠ¡æ˜¯è¿˜åŸç‰‡æ®µã€‚è¾“å…¥ä¸­ä¸€æ®µæ–‡æœ¬è¢«æŒ–æ‰äº†ï¼Œæ¨¡å‹éœ€è¦ç”Ÿæˆè¢«æŒ–æ‰çš„å†…å®¹ã€‚è¿™æ˜¯ä¸€ç§æ›´é€šç”¨çš„åºåˆ—åˆ°åºåˆ—ä»»åŠ¡ã€‚ ğŸ”· ç¬¬ä¸€éƒ¨åˆ†ï¼šGPT-1 æ·±åº¦è§£æ1.1 æ ¸å¿ƒæ€æƒ³GPT-1 (Generative Pre-Training) çš„æ ¸å¿ƒè´¡çŒ®æ˜¯ç¡®ç«‹äº† Pre-training + Fine-tuning çš„èŒƒå¼ã€‚åœ¨æ­¤ä¹‹å‰ï¼ŒNLP ä»»åŠ¡ä¸»è¦é ä»å¤´è®­ç»ƒï¼ˆscratchï¼‰æˆ–ä½¿ç”¨é™æ€è¯å‘é‡ï¼ˆWord2Vecï¼‰ã€‚GPT-1 è¯æ˜ï¼šå…ˆåœ¨ä¸€ä¸ªæµ·é‡æ— æ ‡æ³¨æ–‡æœ¬ä¸Šè®­ç»ƒä¸€ä¸ªè¯­è¨€æ¨¡å‹ï¼Œç„¶åé’ˆå¯¹ç‰¹å®šä»»åŠ¡ï¼ˆåˆ†ç±»ã€è•´å«ç­‰ï¼‰è¿›è¡Œå¾®è°ƒï¼Œæ•ˆæœè¿œè¶…ä¸“é—¨è®¾è®¡çš„æ¨¡å‹ã€‚ 1.2 ğŸ”¬ Tensor ç»´åº¦å˜åŒ–ï¼ˆå®Œæ•´æ•°æ®æµï¼‰è®©æˆ‘ä»¬æ·±å…¥æ¨¡å‹å†…éƒ¨ï¼Œçœ‹çœ‹æ•°æ®æ˜¯å¦‚ä½•æµåŠ¨çš„ã€‚ å›¾è¡¨æ·±åº¦è§£è¯»ï¼šè¿™å¼ è¯¦ç»†çš„æ•°æ®æµå›¾å±•ç¤ºäº†ä¸€ä¸ª batch çš„æ•°æ®åœ¨ GPT-1 ä¸­çš„å®Œæ•´æ—…ç¨‹ï¼š Inputï¼šè¾“å…¥ Token åºåˆ— [2, 5]ï¼ˆå‡è®¾ batch=2, seq=5ï¼‰ã€‚ Embeddingï¼šToken ID è¢«è½¬æ¢ä¸º 768 ç»´çš„å‘é‡ï¼Œå¹¶åŠ ä¸Šäº†ä½ç½®ç¼–ç ï¼ˆPosition Embeddingï¼‰ã€‚æ­¤æ—¶ Tensor å½¢çŠ¶ä¸º [2, 5, 768]ã€‚ Transformer Blockï¼š Q, K, V Projectionï¼šè¾“å…¥è¢«æŠ•å½±ä¸º Query, Key, Valueã€‚ Split Headsï¼š768 ç»´è¢«æ‹†åˆ†ä¸º 12 ä¸ªå¤´ï¼Œæ¯ä¸ªå¤´ 64 ç»´ã€‚å½¢çŠ¶å˜ä¸º [2, 12, 5, 64]ã€‚ Attention Scoreï¼šQ å’Œ K ç›¸ä¹˜ï¼Œå¾—åˆ° [2, 12, 5, 5] çš„åˆ†æ•°çŸ©é˜µã€‚å…³é”®ç‚¹ï¼šè¿™é‡Œåº”ç”¨äº† Causal Maskï¼ˆå³ä¸Šè§’ä¸ºè´Ÿæ— ç©·ï¼‰ï¼Œç¡®ä¿è¯ $i$ åªèƒ½å…³æ³¨åˆ°è¯ $0â€¦i$ã€‚ Outputï¼šç»è¿‡ Softmax å’Œ V ç›¸ä¹˜ï¼Œå¹¶æ‹¼æ¥æ‰€æœ‰å¤´ï¼Œæ¢å¤ä¸º [2, 5, 768]ã€‚ FFNï¼šç»è¿‡ä¸¤å±‚å…¨è¿æ¥å±‚ï¼ˆä¸­é—´å‡ç»´åˆ° 3072ï¼‰ï¼Œå¼•å…¥éçº¿æ€§ã€‚ Logitsï¼šæœ€åç»è¿‡çº¿æ€§å±‚æ˜ å°„å›è¯è¡¨å¤§å° [2, 5, 40000]ï¼Œè¡¨ç¤ºæ¯ä¸ªä½ç½®é¢„æµ‹ä¸‹ä¸€ä¸ªè¯çš„æ¦‚ç‡åˆ†å¸ƒã€‚ 1.3 è®­ç»ƒé—­ç¯ å›¾è¡¨æ·±åº¦è§£è¯»ï¼š å›¾å±•ç¤ºäº†æ ‡å‡†çš„ è‡ªç›‘ç£å­¦ä¹ ï¼ˆSelf-Supervised Learningï¼‰ å¾ªç¯ã€‚ Shift Trickï¼šè¾“å…¥æ˜¯ $x_0, x_1, x_2, x_3$ï¼Œæ ‡ç­¾ï¼ˆTargetï¼‰åˆ™æ˜¯ $x_1, x_2, x_3, x_4$ã€‚ä¹Ÿå°±æ˜¯è¾“å…¥åºåˆ—æ•´ä½“å³ç§»ä¸€ä½ä½œä¸ºç›‘ç£ä¿¡å·ã€‚ Loss è®¡ç®—ï¼šæ¨¡å‹è¾“å‡ºçš„ Logits ä¸ Target è¿›è¡Œ Cross Entropy Loss è®¡ç®—ï¼Œæ¢¯åº¦å›ä¼ æ›´æ–°æ‰€æœ‰å‚æ•°ã€‚ ğŸ”· ç¬¬äºŒéƒ¨åˆ†ï¼šGPT-2 æ·±åº¦è§£æ2.1 æ ¸å¿ƒæ€æƒ³ï¼šZero-shot LearningGPT-2 çš„æ ‡é¢˜æ˜¯ *â€Language Models are Unsupervised Multitask Learnersâ€*ã€‚OpenAI å‘ç°ï¼Œå½“æ¨¡å‹è¶³å¤Ÿå¤§ã€æ•°æ®è¶³å¤Ÿå¤šæ—¶ï¼Œä¸éœ€è¦ Fine-tuningï¼Œæ¨¡å‹å°±èƒ½ç›´æ¥æ‰§è¡Œä»»åŠ¡ã€‚æ¯”å¦‚ä½ ç»™å®ƒè¾“å…¥ â€œTranslate to French: cheese =â€ï¼Œå®ƒä¼šè‡ªåŠ¨è¡¥å…¨ â€œfromageâ€ã€‚ 2.2 æ•°æ®å‡çº§ï¼šWebText å›¾è¡¨æ·±åº¦è§£è¯»ï¼š **GPT-1 (å·¦)**ï¼šä½¿ç”¨çš„æ˜¯ BooksCorpusï¼Œä¸»è¦æ˜¯å°è¯´ä¹¦ç±ã€‚æ–‡æœ¬é£æ ¼å•ä¸€ï¼Œè™½ç„¶è¿è´¯ä½†è¦†ç›–é¢çª„ã€‚ **GPT-2 (å³)**ï¼šä½¿ç”¨çš„æ˜¯ WebTextã€‚OpenAI çˆ¬å–äº† Reddit ä¸Šæ‰€æœ‰è·èµè¶…è¿‡ 3 ä¸ªçš„é“¾æ¥å†…å®¹ã€‚è¿™æ„å‘³ç€æ•°æ®ç»è¿‡äº†äººç±»çš„â€ç­›é€‰â€ï¼ˆåªæœ‰é«˜è´¨é‡å†…å®¹æ‰ä¼šè¢«åˆ†äº«å’Œç‚¹èµï¼‰ã€‚ é‡çº§æå‡ï¼šæ•°æ®é‡ä» 5GB æ¿€å¢åˆ° 40GBã€‚è¿™ä½¿å¾—æ¨¡å‹è§è¯†åˆ°äº†æ›´å¹¿é˜”çš„ä¸–ç•Œï¼ˆæ–°é—»ã€ä»£ç ã€é£Ÿè°±ã€ç§‘æŠ€è®ºæ–‡â€¦ï¼‰ã€‚ 2.3 æ¶æ„å¾®è°ƒï¼šPre-LNGPT-2 å°† Layer Normalization ç§»åˆ°äº† Attention å’Œ FFN çš„è¾“å…¥ç«¯ï¼ˆç§°ä¸º Pre-LNï¼‰ã€‚è¿™å¤§å¤§ç¨³å®šäº†æ·±å±‚ç½‘ç»œï¼ˆ48å±‚ï¼‰çš„æ¢¯åº¦ä¼ æ’­ï¼Œä½¿å¾—è®­ç»ƒæ›´æ·±çš„æ¨¡å‹æˆä¸ºå¯èƒ½ã€‚ ğŸ”· ç¬¬ä¸‰éƒ¨åˆ†ï¼šGPT-3 æ·±åº¦è§£æ3.1 æ ¸å¿ƒæ€æƒ³ï¼šIn-context Learning (ICL)GPT-3 å¹¶æ²¡æœ‰ä¿®æ”¹æ¨¡å‹æ¶æ„ï¼Œè€Œæ˜¯å°†å‚æ•°é‡æ¨åˆ°äº†ææ€–çš„ 1750äº¿ã€‚åœ¨è¿™ä¸ªå°ºåº¦ä¸‹ï¼Œæ¨¡å‹æ¶Œç°å‡ºäº†ç¥å¥‡çš„ ä¸Šä¸‹æ–‡å­¦ä¹ ï¼ˆIn-context Learningï¼‰ èƒ½åŠ›ã€‚ å›¾è¡¨æ·±åº¦è§£è¯»ï¼šè¿™å¼ å›¾è§£é‡Šäº† GPT-3 ç‹¬ç‰¹çš„ä½¿ç”¨æ–¹å¼â€”â€”ä¸éœ€è¦æ›´æ–°æ¨¡å‹æƒé‡ï¼ˆæ¢¯åº¦ä¸‹é™ï¼‰ï¼Œåªéœ€è¦åœ¨ Prompt ä¸­ç»™å®ƒâ€æ¼”ç¤ºâ€ä¸€ä¸‹ï¼š **Zero-shot (ä¸Š)**ï¼šä¸ç»™ä¾‹å­ï¼Œç›´æ¥é—®ã€‚ä¾‹å¦‚ï¼šâ€Translate English to French: cheese =â€ã€‚è¿™å¯¹æ¨¡å‹è¦æ±‚æœ€é«˜ã€‚ **One-shot (ä¸­)**ï¼šç»™ 1 ä¸ªä¾‹å­ã€‚ä¾‹å¦‚ï¼šâ€sea otter = loutre de mer cheese =â€ã€‚æ¨¡å‹é€šè¿‡è¿™ä¸€ä¸ªä¾‹å­â€å­¦ä¼šâ€äº†ç°åœ¨çš„ä»»åŠ¡æ˜¯ç¿»è¯‘ã€‚ **Few-shot (ä¸‹)**ï¼šç»™ 10-100 ä¸ªä¾‹å­ã€‚è¿™èƒ½æå¤§åœ°æå‡æ¨¡å‹æ€§èƒ½ã€‚æœ¬è´¨ï¼šGPT-3 å°†â€å­¦ä¹ â€è¿‡ç¨‹ä»â€æ›´æ–°æƒé‡â€å˜æˆäº†â€è¯»å–ä¸Šä¸‹æ–‡â€ã€‚ 3.2 Scaling Laws (è§„æ¨¡æ³•åˆ™)OpenAI åœ¨è®­ç»ƒ GPT-3 æ—¶å‘ç°äº†ä¸€ä¸ªæƒŠäººçš„è§„å¾‹ã€‚ å›¾è¡¨æ·±åº¦è§£è¯»ï¼š è¿™ä¸‰å¼ å›¾å±•ç¤ºäº† Lossï¼ˆæµ‹è¯•è¯¯å·®ï¼‰ä¸ä¸‰ä¸ªå˜é‡çš„å…³ç³»ï¼š**è®¡ç®—é‡(C)ã€æ•°æ®é›†å¤§å°(D)ã€å‚æ•°é‡(N)**ã€‚ åŒå¯¹æ•°åæ ‡ä¸‹çš„ç›´çº¿ï¼šè¿™ä¸‰æ¡çº¿åœ¨åŒå¯¹æ•°åæ ‡ç³»ä¸‹å‡ ä¹æ˜¯å®Œç¾çš„ç›´çº¿ï¼ **å¹‚å¾‹åˆ†å¸ƒ (Power Law)**ï¼šè¿™æ„å‘³ç€æ€§èƒ½çš„æå‡ä¸æŠ•å…¥èµ„æºå‘ˆå¹‚å¾‹å…³ç³»ï¼ˆ$L \\propto N^{-\\alpha}$ï¼‰ã€‚ å¯ç¤ºï¼šåªè¦ä½ å¢åŠ å‚æ•°ã€å¢åŠ æ•°æ®ã€å¢åŠ ç®—åŠ›ï¼Œæ¨¡å‹çš„æ•ˆæœå°±ä¼šå¯é¢„æµ‹åœ°å˜å¥½ã€‚è¿™ç»™äº† OpenAI å·¨å¤§çš„ä¿¡å¿ƒå»çƒ§é’±è®­ç»ƒ GPT-4ã€‚ 3.3 å‚æ•°é‡å¯¹æ¯” å›¾è¡¨æ·±åº¦è§£è¯»ï¼š è¿™æ˜¯ä¸€å¼ æå…·è§†è§‰å†²å‡»åŠ›çš„å¯¹æ¯”å›¾ã€‚ å·¦è¾¹å¾®å°çš„è“è‰²æ–¹å—æ˜¯ GPT-1 (117M)ã€‚ ä¸­é—´ç¨å¾®å¤§ä¸€ç‚¹çš„ç»¿è‰²æ–¹å—æ˜¯ GPT-2 (1.5B)ã€‚ å³è¾¹é‚£ä¸ªå·¨å¤§çš„ã€å æ®äº†æ•´ä¸ªç”»é¢çš„çº¢è‰²æŸ±å­æ˜¯ GPT-3 (175B)ã€‚ GPT-3 çš„å‚æ•°é‡æ˜¯ GPT-2 çš„ 117 å€ï¼è¿™ç§æš´åŠ›ç¾å­¦å½»åº•æ”¹å˜äº† AI é¢†åŸŸçš„æ¸¸æˆè§„åˆ™ã€‚ ğŸ”· ä¸‰ä»£æ¼”è¿›å¯¹æ¯”æ€»ç»“ä»»åŠ¡é€‚é…æ–¹æ³•æ¼”è¿›æˆ‘ä»¬å¦‚ä½•è®©æ¨¡å‹ä¸ºæˆ‘ä»¬å·¥ä½œï¼Ÿä¸‰ä»£æ¨¡å‹ç»™å‡ºäº†ä¸åŒçš„ç­”æ¡ˆã€‚ å›¾è¡¨æ·±åº¦è§£è¯»ï¼š GPT-1 (å·¦)ï¼šFine-tuningã€‚æ¨¡å‹æ˜¯é€šç”¨çš„ï¼Œä½†ä½¿ç”¨æ—¶éœ€è¦æ”¹å˜æ¨¡å‹ç»“æ„ï¼ˆåŠ åˆ†ç±»å¤´ï¼‰å¹¶é‡æ–°è®­ç»ƒæƒé‡ã€‚ç¼ºç‚¹æ˜¯æ¯ä¸ªä»»åŠ¡éƒ½éœ€è¦å­˜ä¸€ä»½æ¨¡å‹å‰¯æœ¬ã€‚ GPT-2 (ä¸­)ï¼šZero-shotã€‚å®Œå…¨ä¸æ”¹å˜æ¨¡å‹ï¼Œé€šè¿‡æ„é€  Prompt è¯±å¯¼æ¨¡å‹è¾“å‡ºã€‚ä½†æ•ˆæœå¾€å¾€ä¸å¦‚å¾®è°ƒã€‚ GPT-3 (å³)ï¼šFew-shot ICLã€‚ä¸æ”¹å˜æƒé‡ï¼Œä½†åœ¨è¾“å…¥ä¸­åŠ å…¥å°‘é‡ç¤ºä¾‹ï¼ˆContextï¼‰ã€‚è¿™ç»“åˆäº†å‰ä¸¤è€…çš„ä¼˜ç‚¹ï¼šæ—¢ä¸éœ€è¦è®­ç»ƒï¼Œåˆèƒ½é€šè¿‡ç¤ºä¾‹è®©æ¨¡å‹å¿«é€Ÿé€‚åº”ç‰¹å®šä»»åŠ¡ã€‚ å…¨é¢å¯¹æ¯”è¡¨ è¿™å¼ è¡¨æ ¼æ€»ç»“äº†ä¸‰ä»£æ¨¡å‹çš„å…³é”®å‚æ•°ã€‚æ³¨æ„ Context Window (ä¸Šä¸‹æ–‡çª—å£) çš„å˜åŒ–ï¼šä» 512 åˆ° 1024 å†åˆ° 2048ã€‚è¿™æ„å‘³ç€æ¨¡å‹èƒ½â€è®°ä½â€æ›´é•¿çš„å¯¹è¯å†å²ã€‚ ğŸ’» ä»£ç å®ç°å‚è€ƒä»¥ä¸‹æ˜¯ä¸€ä¸ªç®€åŒ–çš„ GPT Block å®ç°ï¼ˆPyTorchï¼‰ï¼Œå±•ç¤ºäº† Masked Attention çš„æ ¸å¿ƒé€»è¾‘ï¼š class GPTBlock(nn.Module): \"\"\"GPT-2 style Transformer block with Pre-LN\"\"\" def __init__(self, hidden_size=768, num_heads=12): super().__init__() self.ln1 = nn.LayerNorm(hidden_size) self.attn = nn.MultiheadAttention(hidden_size, num_heads, batch_first=True) self.ln2 = nn.LayerNorm(hidden_size) self.ffn = nn.Sequential( nn.Linear(hidden_size, 4 * hidden_size), nn.GELU(), nn.Linear(4 * hidden_size, hidden_size) ) def forward(self, x, attn_mask=None): # 1. Pre-LN + Attention + Residual normed = self.ln1(x) # attn_mask å¿…é¡»æ˜¯ä¸‹ä¸‰è§’çŸ©é˜µï¼Œé˜²æ­¢çœ‹åˆ°æœªæ¥ attn_out, _ = self.attn(normed, normed, normed, attn_mask=attn_mask) x = x + attn_out # 2. Pre-LN + FFN + Residual x = x + self.ffn(self.ln2(x)) return x ğŸ“ è´¹æ›¼æ€»ç»“ï¼šæ•™ç»™å°ç™½å¬ğŸ æ–‡å­—æ¥é¾™çš„ç»ˆæå½¢æ€æƒ³è±¡ä½ åœ¨å’Œä¸€ä¸ªè¶…çº§åšå­¦çš„æœ‹å‹ç©â€æ¥é¾™â€æ¸¸æˆï¼š ä½ è¯´ï¼š \"ä»å‰æœ‰åº§å±±ï¼Œå±±ä¸Šæœ‰åº§åº™ï¼Œåº™é‡Œæœ‰ä¸ª...\"æœ‹å‹æ¥ï¼š \"è€å’Œå°š\"ä½ ç»§ç»­ï¼š \"è€å’Œå°šåœ¨ç»™å°å’Œå°šè®²...\" æœ‹å‹æ¥ï¼š \"æ•…äº‹\" GPT å°±æ˜¯è¿™æ ·ä¸€ä¸ªâ€æ¥é¾™é«˜æ‰‹â€ï¼š è¯»ä¹¦ç ´ä¸‡å·ï¼šå®ƒè¯»äº†äº’è”ç½‘ä¸Šå‡ ä¹æ‰€æœ‰çš„æ–‡å­—ï¼ˆä¹¦ç±ã€ç½‘é¡µã€ä»£ç â€¦ï¼‰ã€‚ ç»Ÿè®¡å¤§å¸ˆï¼šå®ƒä¸ä¸€å®šæ‡‚é€»è¾‘ï¼Œä½†å®ƒçŸ¥é“â€å¦‚æœå‰é¢æ˜¯Aï¼Œåé¢å¤§æ¦‚ç‡æ˜¯Bâ€ã€‚ å¤§åŠ›å‡ºå¥‡è¿¹ï¼šå½“å®ƒè¯»çš„ä¹¦è¶³å¤Ÿå¤šï¼ˆTBçº§æ•°æ®ï¼‰ã€è„‘å®¹é‡è¶³å¤Ÿå¤§ï¼ˆ1750äº¿å‚æ•°ï¼‰æ—¶ï¼Œå®ƒä¸ºäº†çŒœå¯¹ä¸‹ä¸€ä¸ªè¯ï¼Œè¢«è¿«â€å­¦ä¼šâ€äº†é€»è¾‘ã€æ•°å­¦ã€ç¿»è¯‘ç”šè‡³ç¼–ç¨‹ã€‚ ğŸ”‘ æ ¸å¿ƒç»“è®º GPT çš„æœ¬è´¨ï¼šä¸€ä¸ªè¶…çº§å¼ºå¤§çš„â€æ–‡å­—æ¥é¾™â€ç¨‹åºã€‚ æ™ºèƒ½çš„æ¥æºï¼šä¸ºäº†å®Œç¾åœ°é¢„æµ‹ä¸‹ä¸€ä¸ªè¯ï¼Œæ¨¡å‹å¿…é¡»æ„å»ºå¯¹ä¸–ç•Œçš„è®¤çŸ¥æ¨¡å‹ã€‚ æœªæ¥çš„æ–¹å‘ï¼šScaling Laws å‘Šè¯‰æˆ‘ä»¬ï¼Œç»§ç»­æŠŠæ¨¡å‹åšå¤§ã€æ•°æ®å–‚å¤šï¼Œå®ƒè¿˜ä¼šå˜å¾—æ›´èªæ˜ã€‚ å‚è€ƒè®ºæ–‡ï¼š Improving Language Understanding by Generative Pre-Training (2018) Language Models are Unsupervised Multitask Learners (2019) Language Models are Few-Shot Learners (2020)","tags":["Deep Learning"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"BERT å®Œæ•´è§£æï¼šä»è®ºæ–‡åˆ° KV Cache","path":"/2026/02/03/bert-comprehensive-guide/","content":"BERT å®Œæ•´è§£æï¼šä»è®ºæ–‡åˆ° KV Cache å­¦ä¹ ç›®æ ‡ï¼šæ·±åº¦ç†è§£ BERT è®ºæ–‡æ ¸å¿ƒåŸç†ã€Q/K/V äº¤äº’æœºåˆ¶ã€KV Cache ä¼˜åŒ–æŠ€æœ¯ã€Causal Attentionï¼Œä»¥åŠå®Œæ•´çš„è®­ç»ƒé—­ç¯ã€‚ ğŸ–¼ï¸ æ ¸å¿ƒå¯è§†åŒ–å›¾è§£BERT æ•´ä½“æ¶æ„ä¸æ ¸å¿ƒæ¦‚å¿µ) BERT æ¶æ„è¯¦è§£) Q/K/V æ•°æ®æµå¯è§†åŒ–) æ³¨æ„åŠ›æœºåˆ¶è¯¦è§£) Causal vs Bidirectional Attention å¯¹æ¯”) ğŸ†š BERT vs â€œAttention Is All You Needâ€ å¯¹æ¯”åˆ†æè®ºæ–‡åŸºæœ¬ä¿¡æ¯å¯¹æ¯” ç»´åº¦ Attention Is All You Need BERT å‘è¡¨æ—¶é—´ 2017å¹´6æœˆ 2018å¹´10æœˆ ä½œè€…å›¢é˜Ÿ Google Brain + Google Research Google AI Language æ ¸å¿ƒè´¡çŒ® æå‡º Transformer æ¶æ„ æå‡ºé¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼ å¼•ç”¨é‡ 10ä¸‡+ 9ä¸‡+ åœ°ä½ å¥ åŸºä¹‹ä½œ åº”ç”¨çªç ´ æ¶æ„å…³ç³»Attention Is All You Need (2017) â”‚ â–¼ æä¾›äº†æ ¸å¿ƒæ¶æ„ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Transformer æ¶æ„ â”‚ â”‚ â€¢ Multi-Head Attention â”‚ â”‚ â€¢ Position Encoding â”‚ â”‚ â€¢ Layer Normalization â”‚ â”‚ â€¢ Feed-Forward Network â”‚ â”‚ â€¢ Encoder + Decoder â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ BERT åªç”¨ Encoder éƒ¨åˆ† â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ BERT (2018) â”‚ â”‚ â€¢ åªç”¨ Transformer Encoder â”‚ â”‚ â€¢ åŠ å…¥ MLM é¢„è®­ç»ƒä»»åŠ¡ â”‚ â”‚ â€¢ æå‡ºé¢„è®­ç»ƒ+å¾®è°ƒèŒƒå¼ â”‚ â”‚ â€¢ åŒå‘æ³¨æ„åŠ› â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ æ ¸å¿ƒåŒºåˆ« ç»´åº¦ Transformer åŸè®ºæ–‡ BERT æ¶æ„ Encoder + Decoder ä»… Encoder ä»»åŠ¡ æœºå™¨ç¿»è¯‘ï¼ˆSeq2Seqï¼‰ è¯­è¨€ç†è§£ï¼ˆåˆ†ç±»ã€NERã€QAï¼‰ æ³¨æ„åŠ› EncoderåŒå‘ï¼ŒDecoderå•å‘ å…¨éƒ¨åŒå‘ é¢„è®­ç»ƒ æ— ï¼ˆç›‘ç£è®­ç»ƒï¼‰ MLM + NSP Position Encoding å›ºå®šçš„æ­£å¼¦å‡½æ•° å¯å­¦ä¹ çš„ Embedding åº”ç”¨ éœ€è¦æˆå¯¹æ•°æ®ï¼ˆä¸­è‹±ç¿»è¯‘ï¼‰ é€šç”¨ç‰¹å¾æå–å™¨ BERT ç»§æ‰¿äº†ä»€ä¹ˆï¼Ÿ# 1. Multi-Head Self-AttentionQ = X @ W_QK = X @ W_KV = X @ W_VAttention(Q, K, V) = softmax(QK^T / âˆšd_k)V# 2. Layer Normalization + æ®‹å·®è¿æ¥output = LayerNorm(X + Attention(X))# 3. Feed-Forward NetworkFFN(x) = max(0, xWâ‚ + bâ‚)Wâ‚‚ + bâ‚‚# 4. æ•´ä½“ç»“æ„for layer in range(12): X = LayerNorm(X + MultiHeadAttention(X)) X = LayerNorm(X + FFN(X)) BERT çš„åˆ›æ–°ç‚¹åˆ›æ–° 1: åªç”¨ Encoder åŸå› ï¼šNLP å¤§å¤šæ•°ä»»åŠ¡æ˜¯\"ç†è§£\"ï¼Œä¸æ˜¯\"ç”Ÿæˆ\"åˆ›æ–° 2: MLM é¢„è®­ç»ƒä»»åŠ¡ è®©æ¨¡å‹å­¦ä¼šåŒå‘ä¸Šä¸‹æ–‡ï¼ˆTransformer åŸè®ºæ–‡æ²¡æœ‰é¢„è®­ç»ƒï¼‰åˆ›æ–° 3: é¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼ ä¸€æ¬¡é¢„è®­ç»ƒ â†’ å¤šä¸ªä»»åŠ¡å¤ç”¨åˆ›æ–° 4: å¯å­¦ä¹ çš„ä½ç½®ç¼–ç  Position Embedding æ˜¯è®­ç»ƒå‡ºæ¥çš„ï¼Œä¸æ˜¯å›ºå®šå…¬å¼ ğŸ†š BERT vs Attention Is All You Needï¼šè®ºæ–‡å¯¹æ¯”è®ºæ–‡åŸºæœ¬ä¿¡æ¯å¯¹æ¯” ç»´åº¦ Attention Is All You Need BERT å‘è¡¨æ—¶é—´ 2017å¹´6æœˆ 2018å¹´10æœˆ ä½œè€…å›¢é˜Ÿ Google Brain + Google Research Google AI Language æ ¸å¿ƒè´¡çŒ® æå‡º Transformer æ¶æ„ æå‡ºé¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼ å¼•ç”¨é‡ 100,000+ 90,000+ å†å²åœ°ä½ æ¶æ„å¥ åŸºä¹‹ä½œ åº”ç”¨çªç ´ä¹‹ä½œ æ ¸å¿ƒå…³ç³»ï¼šç»§æ‰¿ä¸åˆ›æ–°Attention Is All You Need (2017) â”‚ â–¼ æä¾›äº†æ ¸å¿ƒæ¶æ„ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Transformer æ¶æ„ â”‚ â”‚ â€¢ Encoder-Decoder ç»“æ„ â”‚ â”‚ â€¢ Multi-Head Attention â”‚ â”‚ â€¢ Position Encoding â”‚ â”‚ â€¢ Layer Normalization â”‚ â”‚ â€¢ Feed-Forward Network â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ BERT åªç”¨ Encoder éƒ¨åˆ† â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ BERT (2018) â”‚ â”‚ â€¢ åªç”¨ Transformer Encoder â”‚ â”‚ â€¢ æå‡º MLM é¢„è®­ç»ƒä»»åŠ¡ â”‚ â”‚ â€¢ æå‡ºé¢„è®­ç»ƒ+å¾®è°ƒèŒƒå¼ â”‚ â”‚ â€¢ åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ æ¶æ„å¯¹æ¯” ç»´åº¦ Transformer BERT æ¶æ„ Encoder + Decoder ä»… Encoder é€‚ç”¨ä»»åŠ¡ æœºå™¨ç¿»è¯‘ï¼ˆSeq2Seqï¼‰ åˆ†ç±»ã€NERã€é—®ç­” æ³¨æ„åŠ›ç±»å‹ Encoderç”¨åŒå‘ï¼ŒDecoderç”¨å•å‘ å…¨éƒ¨åŒå‘ é¢„è®­ç»ƒä»»åŠ¡ æ— ï¼ˆéœ€è¦å¹³è¡Œè¯­æ–™ï¼‰ MLM + NSP Position Embedding å›ºå®šçš„æ­£å¼¦å‡½æ•° å¯å­¦ä¹ çš„å‘é‡ BERT ç»§æ‰¿äº†ä»€ä¹ˆï¼Ÿâœ… å®Œå…¨ç»§æ‰¿ï¼š Multi-Head Self-Attention æœºåˆ¶ Feed-Forward Network (FFN) æ®‹å·®è¿æ¥ + Layer Normalization Q/K/V è®¡ç®—æ–¹å¼ ğŸ”§ æ”¹è¿›éƒ¨åˆ†ï¼š Position Embeddingï¼šä»å›ºå®šæ”¹ä¸ºå¯å­¦ä¹  åªç”¨ Encoderï¼Œå»æ‰ Decoder æ·»åŠ  Segment Embedding æ·»åŠ ç‰¹æ®Š Tokenï¼š[CLS]ã€[SEP]ã€[MASK] ğŸ†š BERT vs â€œAttention Is All You Needâ€ å¯¹æ¯”è®ºæ–‡åŸºæœ¬ä¿¡æ¯ ç»´åº¦ Attention Is All You Need BERT å‘è¡¨æ—¶é—´ 2017å¹´6æœˆ 2018å¹´10æœˆ ä½œè€…å›¢é˜Ÿ Google Brain + Google Research Google AI Language æ ¸å¿ƒè´¡çŒ® æå‡º Transformer æ¶æ„ æå‡ºé¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼ å¼•ç”¨é‡ 10ä¸‡+ 9ä¸‡+ åœ°ä½ å¥ åŸºä¹‹ä½œï¼ˆæ¶æ„åˆ›æ–°ï¼‰ åº”ç”¨çªç ´ï¼ˆèŒƒå¼åˆ›æ–°ï¼‰ æ¶æ„å¯¹æ¯”Attention Is All You Need (2017) - åŸå§‹ Transformerâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚â”‚ è¾“å…¥ï¼š\"æˆ‘çˆ±ä½ \" è¾“å‡ºï¼š\"I love you\" â”‚â”‚ â†“ â†‘ â”‚â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚ â”‚ Encoder â”‚ â”€â”€â”€ä¸Šä¸‹æ–‡â”€â”€â”€â†’ â”‚ Decoder â”‚ â”‚â”‚ â”‚(6å±‚) â”‚ â”‚(6å±‚) â”‚ â”‚â”‚ â”‚åŒå‘æ³¨æ„åŠ›â”‚ â”‚å•å‘æ³¨æ„åŠ›â”‚ â”‚â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚ â”‚â”‚ ç”¨é€”ï¼šæœºå™¨ç¿»è¯‘ï¼ˆseq2seqï¼‰ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜BERT (2018) - åªç”¨ Encoderâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â”‚â”‚ è¾“å…¥ï¼š\"æˆ‘çˆ±[MASK]å¤©å®‰é—¨\" â”‚â”‚ â†“ â”‚â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚ â”‚ Encoder â”‚ â”€â”€â†’ ç›´æ¥è¾“å‡ºæ¯ä¸ªä½ç½®çš„è¡¨ç¤º â”‚â”‚ â”‚(12å±‚) â”‚ â†“ â”‚â”‚ â”‚åŒå‘æ³¨æ„åŠ›â”‚ åœ¨ [MASK] ä½ç½®é¢„æµ‹ \"åŒ—äº¬\" â”‚â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚ â”‚â”‚ ç”¨é€”ï¼šç†è§£ä»»åŠ¡ï¼ˆåˆ†ç±»ã€NERã€QAï¼‰ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ BERT ç»§æ‰¿äº† Transformer çš„ä»€ä¹ˆï¼Ÿ ç»„ä»¶ Transformer BERT è¯´æ˜ Multi-Head Attention âœ… åŸåˆ› âœ… å®Œå…¨ç»§æ‰¿ Q/K/V æœºåˆ¶ä¸€æ¨¡ä¸€æ · Position Encoding âœ… æ­£å¼¦å‡½æ•° âš ï¸ æ”¹ä¸ºå¯å­¦ä¹  BERT ç”¨å¯è®­ç»ƒçš„ä½ç½®åµŒå…¥ Layer Normalization âœ… åŸåˆ› âœ… å®Œå…¨ç»§æ‰¿ Feed-Forward Network âœ… åŸåˆ› âœ… å®Œå…¨ç»§æ‰¿ Encoder ç»“æ„ âœ… 6å±‚ âœ… 12/24å±‚ BERT åŠ æ·±äº†å±‚æ•° Decoder ç»“æ„ âœ… 6å±‚ âŒ åˆ é™¤ BERT ä¸éœ€è¦ Decoder BERT çš„åˆ›æ–°ç‚¹ åˆ›æ–° è¯´æ˜ MLM é¢„è®­ç»ƒä»»åŠ¡ Transformer æ²¡æœ‰é¢„è®­ç»ƒï¼ŒBERT ç”¨ MLM å­¦ä¹ åŒå‘è¡¨ç¤º NSP ä»»åŠ¡ å­¦ä¹ å¥å­é—´å…³ç³»ï¼ˆåç»­è¢«è¯æ˜ç”¨å¤„ä¸å¤§ï¼‰ é¢„è®­ç»ƒ+å¾®è°ƒèŒƒå¼ Transformer æ˜¯ä»»åŠ¡ç‰¹å®šè®­ç»ƒï¼ŒBERT å¼€åˆ›è¿ç§»å­¦ä¹  åªç”¨ Encoder Transformer æ˜¯å®Œæ•´ Encoder-Decoderï¼ŒBERT ç®€åŒ–æ¶æ„ è”ç³»ï¼šBERT ç«™åœ¨ Transformer è‚©è†€ä¸Š2017 Transformer æä¾›æ ¸å¿ƒæ¶æ„ â”‚ â–¼â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ â€¢ Multi-Head Attention â”‚â”‚ â€¢ Position Encoding â”‚â”‚ â€¢ Feed-Forward Network â”‚â”‚ â€¢ Encoder-Decoder æ¶æ„ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ BERT é€‰æ‹©æ€§ä½¿ç”¨â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ âœ… å¤ç”¨ Encoder éƒ¨åˆ† â”‚â”‚ âœ… å¤ç”¨ Attention æœºåˆ¶ â”‚â”‚ âŒ åˆ é™¤ Decoder â”‚â”‚ â• åŠ å…¥ MLM é¢„è®­ç»ƒ â”‚â”‚ â• æå‡ºé¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ç®€å•è®°å¿†ï¼š Transformer = æä¾›äº†â€å·¥å…·ç®±â€ï¼ˆæ¶æ„ç»„ä»¶ï¼‰ BERT = ç”¨å·¥å…·ç®±ä¸­çš„éƒ¨åˆ†å·¥å…·ï¼Œå‘æ˜äº†æ–°çš„ä½¿ç”¨æ–¹æ³•ï¼ˆé¢„è®­ç»ƒèŒƒå¼ï¼‰ ğŸ†š BERT vs â€œAttention Is All You Needâ€ å¯¹æ¯”è®ºæ–‡åŸºæœ¬ä¿¡æ¯å¯¹æ¯” ç»´åº¦ Attention Is All You Need BERT å‘è¡¨æ—¶é—´ 2017å¹´6æœˆ 2018å¹´10æœˆ ä½œè€…å›¢é˜Ÿ Google Brain + Google Research Google AI Language æ ¸å¿ƒè´¡çŒ® æå‡º Transformer æ¶æ„ æå‡ºé¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼ æ¶æ„ Encoder + Decoder ä»… Encoder è®­ç»ƒä»»åŠ¡ æœºå™¨ç¿»è¯‘ï¼ˆæœ‰ç›‘ç£ï¼‰ MLM + NSPï¼ˆè‡ªç›‘ç£ï¼‰ å¼•ç”¨é‡ 10ä¸‡+ 9ä¸‡+ ç»§æ‰¿å…³ç³»Attention Is All You Need (2017) â†“ æä¾›æ ¸å¿ƒæ¶æ„ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Transformer æ¶æ„ â”‚ â”‚ â€¢ Multi-Head Attention â”‚ â”‚ â€¢ Position Encoding â”‚ â”‚ â€¢ Layer Normalization â”‚ â”‚ â€¢ Feed-Forward Network â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â†“ BERT åªç”¨ Encoder éƒ¨åˆ† â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ BERT (2018) â”‚ â”‚ â€¢ 12å±‚ Transformer Encoder â”‚ â”‚ â€¢ MLM é¢„è®­ç»ƒä»»åŠ¡ â”‚ â”‚ â€¢ é¢„è®­ç»ƒ+å¾®è°ƒèŒƒå¼ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ æ¶æ„å¯¹æ¯”åŸå§‹ Transformerï¼ˆç¿»è¯‘ä»»åŠ¡ï¼‰ï¼šâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ è¾“å…¥ï¼š\"æˆ‘çˆ±ä½ \" è¾“å‡ºï¼š\"I love you\" â”‚â”‚ â†“ â†‘ â”‚â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚ â”‚ Encoder â”‚ â”€â”€â”€ä¸Šä¸‹æ–‡â”€â”€â”€â†’ â”‚ Decoder â”‚ â”‚â”‚ â”‚(ç†è§£è¾“å…¥)â”‚ â”‚(ç”Ÿæˆè¾“å‡º)â”‚ â”‚â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚â”‚ åŒå‘æ³¨æ„åŠ› å•å‘æ³¨æ„åŠ›(Causal) â”‚â”‚ æ— éœ€ KV Cache éœ€è¦ KV Cache â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜BERTï¼ˆç†è§£ä»»åŠ¡ï¼‰ï¼šâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ è¾“å…¥ï¼š\"æˆ‘çˆ±[MASK]å¤©å®‰é—¨\" â”‚â”‚ â†“ â”‚â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚ â”‚ Encoder â”‚ â”€â”€â†’ ç›´æ¥è¾“å‡ºæ¯ä¸ªä½ç½®çš„è¡¨ç¤º â”‚â”‚ â”‚(ç†è§£è¾“å…¥)â”‚ â†“ â”‚â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ åœ¨ [MASK] ä½ç½®é¢„æµ‹ \"åŒ—äº¬\" â”‚â”‚ åŒå‘æ³¨æ„åŠ› â”‚â”‚ æ— éœ€ KV Cache â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ æ ¸å¿ƒåŒºåˆ«æ€»ç»“ ç»´åº¦ Transformer (åŸè®ºæ–‡) BERT æ¶æ„é€‰æ‹© Encoder + Decoder ä»… Encoder æ³¨æ„åŠ›æ¨¡å¼ EncoderåŒå‘ + Decoderå•å‘ å…¨éƒ¨åŒå‘ é€‚ç”¨ä»»åŠ¡ åºåˆ—åˆ°åºåˆ—ï¼ˆç¿»è¯‘ï¼‰ ç†è§£ç±»ä»»åŠ¡ è®­ç»ƒæ•°æ® å¹³è¡Œè¯­æ–™ï¼ˆéœ€æ ‡æ³¨ï¼‰ å¤§è§„æ¨¡æ–‡æœ¬ï¼ˆæ— éœ€æ ‡æ³¨ï¼‰ KV Cache Decoderéœ€è¦ ä¸éœ€è¦ å½±å“ å¥ å®šæ¶æ„åŸºç¡€ å¼€åˆ›é¢„è®­ç»ƒèŒƒå¼ ğŸ“„ Part 1: BERT è®ºæ–‡æ·±åº¦è§£æ1.1 è®ºæ–‡åŸºæœ¬ä¿¡æ¯æ ‡é¢˜: BERT: Pre-training of Deep Bidirectional Transformers for Language Understandingä½œè€…: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova (Google AI Language)å‘è¡¨: NAACL 2019è®ºæ–‡é“¾æ¥: https://arxiv.org/abs/1810.04805 1.2 ç ”ç©¶åŠ¨æœºï¼šä¸ºä»€ä¹ˆéœ€è¦ BERTï¼Ÿåœ¨ BERT ä¹‹å‰ï¼ŒNLP é¢„è®­ç»ƒæ¨¡å‹å­˜åœ¨ä¸¤å¤§å±€é™ï¼š é—®é¢˜ 1ï¼šå•å‘è¯­è¨€æ¨¡å‹çš„å±€é™æ€§GPT-1 (2018): åªèƒ½ä»å·¦å¾€å³çœ‹è¾“å…¥: \"æˆ‘ çˆ± åŒ—äº¬ å¤©å®‰é—¨\" â†“ â†“ â†“ â†“æ¯ä¸ªè¯åªèƒ½çœ‹åˆ°å·¦è¾¹çš„ä¸Šä¸‹æ–‡é—®é¢˜: \"é“¶è¡Œ\" åœ¨ \"æˆ‘å»é“¶è¡Œå­˜é’±\" vs \"æ²³è¾¹çš„é“¶è¡Œå¾ˆé™¡\" ä¸­ å¦‚æœåªçœ‹å·¦è¾¹ï¼Œæ— æ³•åŒºåˆ†æ˜¯ \"é‡‘èæœºæ„\" è¿˜æ˜¯ \"æ²³å²¸\" é—®é¢˜ 2ï¼šæµ…å±‚åŒå‘çš„å±€é™æ€§ELMo (2018): ä½¿ç”¨ä¸¤ä¸ªç‹¬ç«‹çš„ LSTM â†’ LSTM (ä»å·¦å¾€å³) â†’ LSTM (ä»å³å¾€å·¦) â†’ æœ€åæ‹¼æ¥é—®é¢˜: ä¸¤ä¸ªæ–¹å‘çš„ä¿¡æ¯åªåœ¨æœ€é¡¶å±‚èåˆï¼Œä¸­é—´å±‚æ— æ³•æ·±åº¦äº¤äº’ BERT çš„æ ¸å¿ƒåˆ›æ–°: é€šè¿‡ Masked Language Model (MLM)ï¼Œåœ¨æ¯ä¸€å±‚éƒ½å®ç°çœŸæ­£çš„åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡ã€‚ 1.3 æ ¸å¿ƒæ–¹æ³•ï¼šä¸¤ä¸ªé¢„è®­ç»ƒä»»åŠ¡ä»»åŠ¡ 1: Masked Language Model (MLM) - æ ¸å¿ƒæ“ä½œæµç¨‹: éšæœºé€‰æ‹© 15% çš„ token è¿›è¡Œ mask å…¶ä¸­ï¼š 80% æ›¿æ¢ä¸º [MASK] 10% æ›¿æ¢ä¸ºéšæœºè¯ 10% ä¿æŒä¸å˜ ä¸ºä»€ä¹ˆè¿™æ ·è®¾è®¡ï¼Ÿ 80% [MASK]: è®©æ¨¡å‹å­¦ä¹ é¢„æµ‹ 10% éšæœºè¯: é¿å…æ¨¡å‹åªä¾èµ– [MASK] æ ‡è®° 10% ä¸å˜: è®©æ¨¡å‹å­¦ä¹ çœŸå®åˆ†å¸ƒ ä¾‹å­: åŸå§‹å¥å­: \"æˆ‘ çˆ± åŒ—äº¬ å¤©å®‰é—¨\"å¤„ç†å: \"æˆ‘ çˆ± [MASK] å¤©å®‰é—¨\" (80%)æˆ–: \"æˆ‘ çˆ± ä¸Šæµ· å¤©å®‰é—¨\" (10% éšæœº)æˆ–: \"æˆ‘ çˆ± åŒ—äº¬ å¤©å®‰é—¨\" (10% ä¸å˜)Label: ä½ç½® 3 = \"åŒ—äº¬\"Loss = CrossEntropy(model_output[3], ID(\"åŒ—äº¬\")) ç»´åº¦åˆ†æ: è¾“å…¥: [batch, seq_len] = [32, 128] â†“ Embedding [32, 128, 768] â†“ 12 å±‚ Transformer Encoder [32, 128, 768] â†“ MLM Head (Linear + Softmax) [32, 128, 21128] # 21128 = è¯è¡¨å¤§å° åªè®¡ç®—è¢« mask ä½ç½®çš„ Loss ä»»åŠ¡ 2: Next Sentence Prediction (NSP)ç›®çš„: å­¦ä¹ å¥å­é—´å…³ç³» è¾“å…¥æ ¼å¼: [CLS] å¥å­A [SEP] å¥å­B [SEP]æ­£æ ·æœ¬: B ç¡®å®æ˜¯ A çš„ä¸‹ä¸€å¥ (Label = 1)è´Ÿæ ·æœ¬: B æ˜¯éšæœºé€‰çš„å¥å­ (Label = 0) ä¾‹å­: æ­£æ ·æœ¬:Input: [CLS] ä»Šå¤©å¤©æ°”å¾ˆå¥½ [SEP] æˆ‘ä»¬å»å…¬å›­å§ [SEP]Label: IsNext (1)è´Ÿæ ·æœ¬:Input: [CLS] ä»Šå¤©å¤©æ°”å¾ˆå¥½ [SEP] äººå·¥æ™ºèƒ½å¾ˆæœ‰è¶£ [SEP]Label: NotNext (0) Loss è®¡ç®—: cls_output = encoder_output[:, 0, :] # [batch, 768] å– [CLS] ä½ç½®logits = nsp_classifier(cls_output) # [batch, 2]loss = BinaryCrossEntropy(logits, labels) åç»­ç ”ç©¶å‘ç°: NSP ä»»åŠ¡æ•ˆæœæœ‰é™ï¼ŒRoBERTa ç­‰åç»­å·¥ä½œç§»é™¤äº†è¿™ä¸ªä»»åŠ¡ã€‚ 1.3.1 MLM æ·±åº¦è§£æï¼šå®Œå½¢å¡«ç©ºçš„è‰ºæœ¯é€šä¿—ç†è§£å°å­¦è¯­æ–‡é¢˜ï¼š \"å°æ˜ _____ å­¦æ ¡ä¸Šè¯¾\" ç­”æ¡ˆï¼šå»ã€åˆ°ã€åœ¨...BERT çš„ MLM å°±æ˜¯è®© AI åšè¿™ç§\"å®Œå½¢å¡«ç©º\"ï¼ å…·ä½“æ“ä½œæµç¨‹# åŸå§‹å¥å­åŸå¥ = \"æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨\"# Step 1: éšæœºé€‰æ‹© 15% çš„è¯è¿›è¡Œå¤„ç†é€‰ä¸­ = \"åŒ—äº¬\"# Step 2: å¯¹é€‰ä¸­çš„è¯è¿›è¡Œä¸‰ç§å¤„ç†ï¼ˆéšæœºé€‰ä¸€ç§ï¼‰å¤„ç†å = \"æˆ‘çˆ± [MASK] å¤©å®‰é—¨\" # 80% æ¦‚ç‡ï¼šæ›¿æ¢ä¸º [MASK]æˆ–è€… = \"æˆ‘çˆ± ä¸Šæµ· å¤©å®‰é—¨\" # 10% æ¦‚ç‡ï¼šæ›¿æ¢ä¸ºéšæœºè¯æˆ–è€… = \"æˆ‘çˆ± åŒ—äº¬ å¤©å®‰é—¨\" # 10% æ¦‚ç‡ï¼šä¿æŒä¸å˜# Step 3: è®©æ¨¡å‹é¢„æµ‹è¢«é®ä½çš„è¯æ˜¯ä»€ä¹ˆæ¨¡å‹è¾“å…¥ = \"æˆ‘çˆ± [MASK] å¤©å®‰é—¨\"æ¨¡å‹è¾“å‡º = \"åŒ—äº¬\" âœ… (å¦‚æœé¢„æµ‹å¯¹äº†ï¼Œloss å¾ˆå°) = \"ä¸Šæµ·\" âŒ (å¦‚æœé¢„æµ‹é”™äº†ï¼Œloss å¾ˆå¤§) ä¸ºä»€ä¹ˆ MLM èƒ½è®© BERT å­¦ä¼šâ€ç†è§£è¯­è¨€â€ï¼Ÿåœºæ™¯ 1ï¼š è¾“å…¥ï¼š\"æˆ‘å» [MASK] å­˜é’±\" æ¨¡å‹å­¦ä¼šï¼šçœ‹åˆ°\"å­˜é’±\" â†’ é¢„æµ‹\"é“¶è¡Œ\"ï¼ˆé‡‘èæœºæ„ï¼‰åœºæ™¯ 2ï¼š è¾“å…¥ï¼š\"æ²³è¾¹çš„ [MASK] å¾ˆé™¡å³­\" æ¨¡å‹å­¦ä¼šï¼šçœ‹åˆ°\"æ²³è¾¹\"\"é™¡å³­\" â†’ é¢„æµ‹\"æ²³å²¸/å ¤å\"é€šè¿‡æ•°åäº¿æ¬¡è¿™æ ·çš„\"å®Œå½¢å¡«ç©º\"è®­ç»ƒåï¼š â†’ æ¨¡å‹å­¦ä¼šäº†è¯è¯­ä¹‹é—´çš„å…³ç³» â†’ æ¨¡å‹å­¦ä¼šäº†è¯­æ³•ç»“æ„ â†’ æ¨¡å‹å­¦ä¼šäº†å¸¸è¯†çŸ¥è¯† MLM è®­ç»ƒä»£ç ç¤ºä¾‹from transformers import BertTokenizer, BertForMaskedLMimport torch# åŠ è½½æ¨¡å‹tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')model = BertForMaskedLM.from_pretrained('bert-base-chinese')# è¾“å…¥ä¸€ä¸ªå¸¦ [MASK] çš„å¥å­text = \"æˆ‘çˆ±[MASK]å¤©å®‰é—¨\"inputs = tokenizer(text, return_tensors='pt')# æ¨¡å‹é¢„æµ‹with torch.no_grad(): outputs = model(**inputs) predictions = outputs.logits# æ‰¾åˆ° [MASK] ä½ç½®çš„é¢„æµ‹ç»“æœmask_index = (inputs['input_ids'] == tokenizer.mask_token_id).nonzero()[0, 1]predicted_token_id = predictions[0, mask_index].argmax(dim=-1)predicted_token = tokenizer.decode(predicted_token_id)print(f\"åŸå¥: {text}\")print(f\"é¢„æµ‹: {predicted_token}\") # è¾“å‡º: åŒ—äº¬ 1.3.2 é¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼è¯¦è§£é€šä¿—ç†è§£ï¼šåŸ¹å…»â€é€šæ‰â€å†åŸ¹å…»â€ä¸“æ‰â€ä¼ ç»Ÿæ–¹å¼ï¼ˆä»é›¶å¼€å§‹ï¼‰ï¼š â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ ä»»åŠ¡ï¼šæƒ…æ„Ÿåˆ†æ â”‚ â”‚ æ•°æ®ï¼š10ä¸‡æ¡ç”µå•†è¯„è®º â”‚ â”‚ è®­ç»ƒï¼šä»éšæœºåˆå§‹åŒ–å¼€å§‹ï¼Œè®­ç»ƒä¸€ä¸ªä¸“é—¨çš„æ¨¡å‹ â”‚ â”‚ è€—æ—¶ï¼š3å¤©ï¼Œéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ® â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ ä»»åŠ¡ï¼šåƒåœ¾é‚®ä»¶æ£€æµ‹ â”‚ â”‚ æ•°æ®ï¼š5ä¸‡å°é‚®ä»¶ â”‚ â”‚ è®­ç»ƒï¼šä»éšæœºåˆå§‹åŒ–å¼€å§‹ï¼Œå†è®­ç»ƒå¦ä¸€ä¸ªä¸“é—¨çš„æ¨¡å‹ â”‚ â”‚ è€—æ—¶ï¼š2å¤©ï¼Œåˆéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ® â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ é—®é¢˜ï¼šæ¯ä¸ªä»»åŠ¡éƒ½è¦ä»å¤´è®­ç»ƒï¼Œé‡å¤åŠ³åŠ¨ï¼ å¾®è°ƒèŒƒå¼ï¼ˆç«™åœ¨å·¨äººè‚©è†€ä¸Šï¼‰ï¼š ç¬¬ä¸€é˜¶æ®µï¼šé¢„è®­ç»ƒï¼ˆPre-trainingï¼‰â€”â€” åªåšä¸€æ¬¡ï¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ æ•°æ®ï¼šæ•´ä¸ªç»´åŸºç™¾ç§‘ + å¤§é‡ä¹¦ç± (æ•°åäº¿è¯) â”‚ â”‚ ä»»åŠ¡ï¼šMLMï¼ˆå®Œå½¢å¡«ç©ºï¼‰ â”‚ â”‚ ç›®çš„ï¼šè®©æ¨¡å‹å­¦ä¼š\"ç†è§£è¯­è¨€\" â”‚ â”‚ è€—æ—¶ï¼šæ•°å‘¨ (ä½†åªéœ€åšä¸€æ¬¡ï¼Œç”± Google å®Œæˆ) â”‚ â”‚ äº§å‡ºï¼šBERT é¢„è®­ç»ƒæ¨¡å‹ âœ¨ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ ä¸‹è½½ç°æˆçš„ BERT ç¬¬äºŒé˜¶æ®µï¼šå¾®è°ƒï¼ˆFine-tuningï¼‰â€”â€” æ¯ä¸ªä»»åŠ¡åªéœ€å‡ å°æ—¶ï¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ ä»»åŠ¡Aï¼šæƒ…æ„Ÿåˆ†æ â”‚ â”‚ æ•°æ®ï¼šä»…éœ€ 1000 æ¡æ ‡æ³¨æ•°æ®ï¼ â”‚ â”‚ æ–¹æ³•ï¼šåœ¨ BERT ä¸ŠåŠ ä¸€ä¸ªåˆ†ç±»å±‚ï¼Œå¾®è°ƒå‡ è½® â”‚ â”‚ è€—æ—¶ï¼š30åˆ†é’Ÿ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ ä»»åŠ¡Bï¼šåƒåœ¾é‚®ä»¶æ£€æµ‹ â”‚ â”‚ æ•°æ®ï¼šä»…éœ€ 500 æ¡æ ‡æ³¨æ•°æ®ï¼ â”‚ â”‚ æ–¹æ³•ï¼šåŒæ ·åŠ åˆ†ç±»å±‚ï¼Œå¾®è°ƒå‡ è½® â”‚ â”‚ è€—æ—¶ï¼š20åˆ†é’Ÿ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ å½¢è±¡ç±»æ¯”æ€»ç»“â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ ğŸ“ æ•™è‚²ç±»æ¯” â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ â”‚â”‚ ã€é¢„è®­ç»ƒ Pre-trainingã€‘= ä¸Šå¤§å­¦ï¼Œæ¥å—é€šè¯†æ•™è‚² â”‚â”‚ â€¢ å­¦ä¹ è¯­æ–‡ã€æ•°å­¦ã€è‹±è¯­ã€ç‰©ç†... â”‚â”‚ â€¢ ç›®æ ‡ï¼šæˆä¸ºä¸€ä¸ªæœ‰åŸºç¡€çŸ¥è¯†çš„\"é€šæ‰\" â”‚â”‚ â€¢ æ—¶é—´ï¼š4å¹´ â”‚â”‚ â€¢ æˆæœ¬ï¼šé«˜ â”‚â”‚ â”‚â”‚ ã€MLM ä»»åŠ¡ã€‘= å¤§å­¦é‡Œçš„å„ç§ç»ƒä¹ é¢˜ â”‚â”‚ â€¢ å®Œå½¢å¡«ç©ºã€é˜…è¯»ç†è§£ã€è¯­æ³•ç»ƒä¹ ... â”‚â”‚ â€¢ ç›®æ ‡ï¼šé”»ç‚¼è¯­è¨€ç†è§£èƒ½åŠ› â”‚â”‚ â”‚â”‚ ã€å¾®è°ƒ Fine-tuningã€‘= å·¥ä½œåçš„å²—ä½åŸ¹è®­ â”‚â”‚ â€¢ é’ˆå¯¹å…·ä½“å·¥ä½œï¼ˆæƒ…æ„Ÿåˆ†æ/é—®ç­”/ç¿»è¯‘ï¼‰å­¦ä¹  â”‚â”‚ â€¢ ç›®æ ‡ï¼šæˆä¸ºæŸä¸ªé¢†åŸŸçš„\"ä¸“æ‰\" â”‚â”‚ â€¢ æ—¶é—´ï¼šå‡ å¤© â”‚â”‚ â€¢ æˆæœ¬ï¼šä½ï¼ˆå› ä¸ºå·²ç»æœ‰åŸºç¡€äº†ï¼‰ â”‚â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ ğŸ’¡ ä¸ºä»€ä¹ˆè¿™æ ·æ›´å¥½ï¼Ÿ â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ â”‚â”‚ ä¼ ç»Ÿæ–¹å¼ï¼šæ¯æ¢ä¸€ä¸ªå·¥ä½œå°±é‡æ–°ä¸Šä¸€æ¬¡å¤§å­¦ â”‚â”‚ â†’ å¤ªæ…¢ã€å¤ªè´µã€å¤ªæµªè´¹ â”‚â”‚ â”‚â”‚ å¾®è°ƒèŒƒå¼ï¼šå¤§å­¦åªä¸Šä¸€æ¬¡ï¼Œæ¢å·¥ä½œåªéœ€çŸ­æœŸåŸ¹è®­ â”‚â”‚ â†’ å¿«é€Ÿã€ä¾¿å®œã€é«˜æ•ˆ â”‚â”‚ â”‚â”‚ æ•°æ®å¯¹æ¯”ï¼š â”‚â”‚ â€¢ ä¼ ç»Ÿï¼šéœ€è¦ 10ä¸‡+ æ ‡æ³¨æ•°æ® â”‚â”‚ â€¢ å¾®è°ƒï¼šåªéœ€ 1000 æ¡æ ‡æ³¨æ•°æ®å°±èƒ½è¾¾åˆ°ç›¸ä¼¼æ•ˆæœï¼ â”‚â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ æƒ…æ„Ÿåˆ†æå¾®è°ƒå®Œæ•´ç¤ºä¾‹from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArgumentsfrom datasets import Dataset# ============ ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡å°‘é‡æ ‡æ³¨æ•°æ® ============train_data = { \"text\": [ \"è¿™ä¸ªäº§å“å¤ªæ£’äº†ï¼Œå¼ºçƒˆæ¨èï¼\", \"è´¨é‡å¾ˆå·®ï¼Œç”¨äº†ä¸€å¤©å°±åäº†\", \"ä¸€èˆ¬èˆ¬ï¼Œæ²¡æœ‰æƒŠå–œä¹Ÿæ²¡æœ‰å¤±æœ›\", \"è¶…çº§å–œæ¬¢ï¼Œå·²ç»å›è´­ä¸‰æ¬¡\", \"å®¢æœæ€åº¦å¾ˆå·®ï¼Œå†ä¹Ÿä¸ä¹°äº†\", \"æ€§ä»·æ¯”å¾ˆé«˜ï¼Œå€¼å¾—è´­ä¹°\", ], \"label\": [1, 0, 0, 1, 0, 1] # 1=æ­£é¢, 0=è´Ÿé¢}# ============ ç¬¬äºŒæ­¥ï¼šåŠ è½½é¢„è®­ç»ƒçš„ BERT ============# è¿™ä¸ª BERT å·²ç»é€šè¿‡ MLM ä»»åŠ¡å­¦ä¼šäº†\"ç†è§£ä¸­æ–‡\"tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')model = BertForSequenceClassification.from_pretrained( 'bert-base-chinese', num_labels=2 # æ­£é¢/è´Ÿé¢ ä¸¤ä¸ªç±»åˆ«)# ============ ç¬¬ä¸‰æ­¥ï¼šæ•°æ®é¢„å¤„ç† ============def tokenize_function(examples): return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)dataset = Dataset.from_dict(train_data)tokenized_dataset = dataset.map(tokenize_function, batched=True)# ============ ç¬¬å››æ­¥ï¼šå¾®è°ƒè®­ç»ƒ ============training_args = TrainingArguments( output_dir=\"./results\", num_train_epochs=3, # åªéœ€è¦è®­ç»ƒ 3 è½®ï¼ per_device_train_batch_size=4, learning_rate=2e-5, # å¾ˆå°çš„å­¦ä¹ ç‡ï¼Œè½»å¾®è°ƒæ•´)trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_dataset,)trainer.train() # å‡ åˆ†é’Ÿå°±å®Œæˆï¼# ============ ç¬¬äº”æ­¥ï¼šä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ ============def predict(text): inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True) outputs = model(**inputs) prediction = outputs.logits.argmax(dim=-1).item() return \"æ­£é¢ ğŸ˜Š\" if prediction == 1 else \"è´Ÿé¢ ğŸ˜\"# æµ‹è¯•print(predict(\"è¿™æ¬¾æ‰‹æœºæ‹ç…§æ•ˆæœæƒŠè‰³\")) # â†’ æ­£é¢ ğŸ˜Šprint(predict(\"ç‰©æµå¤ªæ…¢äº†ï¼Œç­‰äº†ä¸€å‘¨\")) # â†’ è´Ÿé¢ ğŸ˜ 1.4 è®ºæ–‡å…³é”®åˆ›æ–°ç‚¹æ·±åº¦å‰–æğŸ”¬ åˆ›æ–°ç‚¹ 1ï¼šæ·±åº¦åŒå‘ä¸Šä¸‹æ–‡å»ºæ¨¡ æ¨¡å‹ ä¸Šä¸‹æ–‡æ–¹å‘ é—®é¢˜ GPT-1 å•å‘ (å·¦â†’å³) æ— æ³•åˆ©ç”¨å³ä¾§ä¿¡æ¯ ELMo æµ…å±‚åŒå‘ (æ‹¼æ¥) ä¸¤ä¸ªæ–¹å‘ä»…åœ¨é¡¶å±‚èåˆ BERT æ·±åº¦åŒå‘ âœ… æ¯ä¸€å±‚éƒ½èƒ½çœ‹åˆ°å®Œæ•´ä¸Šä¸‹æ–‡ æŠ€æœ¯å®ç°å…³é”®ï¼šé€šè¿‡ MLM ä»»åŠ¡ï¼Œæ¨¡å‹å¯ä»¥â€ä½œå¼Šâ€åœ°çœ‹åˆ°è¢«é¢„æµ‹è¯çš„ä¸¤ä¾§ä¿¡æ¯ã€‚ ğŸ”¬ åˆ›æ–°ç‚¹ 2ï¼šé¢„è®­ç»ƒ-å¾®è°ƒèŒƒå¼ä¼ ç»Ÿæ–¹æ³•: ä»»åŠ¡A â†’ ä»å¤´è®­ç»ƒæ¨¡å‹A ä»»åŠ¡B â†’ ä»å¤´è®­ç»ƒæ¨¡å‹B (é‡å¤åŠ³åŠ¨!)BERTèŒƒå¼: å¤§è§„æ¨¡è¯­æ–™ â†’ é¢„è®­ç»ƒBERT (ä¸€æ¬¡) â†“ ä»»åŠ¡A â†’ å¾®è°ƒ (ä»…éœ€å°æ•°æ®) ä»»åŠ¡B â†’ å¾®è°ƒ (ä»…éœ€å°æ•°æ®) ä»»åŠ¡C â†’ å¾®è°ƒ (ä»…éœ€å°æ•°æ®) é©å‘½æ€§å½±å“ï¼šå°å…¬å¸/ç ”ç©¶è€…æ— éœ€å¤§è§„æ¨¡è®¡ç®—èµ„æºï¼Œåªéœ€å¾®è°ƒå³å¯è·å¾—SOTAæ€§èƒ½ã€‚ ğŸ”¬ åˆ›æ–°ç‚¹ 3ï¼šç»Ÿä¸€çš„ç‰¹å¾æå–å™¨# BERT å¯ä»¥é€‚é…å‡ ä¹æ‰€æœ‰ NLP ä»»åŠ¡# 1. å•å¥åˆ†ç±» (æƒ…æ„Ÿåˆ†æ)[CLS] è¿™éƒ¨ç”µå½±å¤ªæ£’äº† [SEP] â†’ CLSå‘é‡ â†’ åˆ†ç±»å™¨ â†’ æ­£é¢# 2. å¥å¯¹åˆ†ç±» (è‡ªç„¶è¯­è¨€æ¨ç†)[CLS] å¤©åœ¨ä¸‹é›¨ [SEP] åœ°é¢æ˜¯æ¹¿çš„ [SEP] â†’ CLSå‘é‡ â†’ è•´å«/çŸ›ç›¾/ä¸­æ€§# 3. åºåˆ—æ ‡æ³¨ (å‘½åå®ä½“è¯†åˆ«)[CLS] é©¬äº‘ åˆ›åŠäº† é˜¿é‡Œå·´å·´ [SEP] â†’ æ¯ä¸ªtoken â†’ B-PER O O B-ORG# 4. é—®ç­” (é˜…è¯»ç†è§£)[CLS] é—®é¢˜ [SEP] æ–‡ç«  [SEP] â†’ é¢„æµ‹ç­”æ¡ˆèµ·æ­¢ä½ç½® 1.5 æ¶ˆèå®éªŒè¯¦è§£ (Ablation Study)è®ºæ–‡é€šè¿‡æ¶ˆèå®éªŒéªŒè¯äº†å„ç»„ä»¶çš„é‡è¦æ€§ï¼š å®éªŒ 1ï¼šé¢„è®­ç»ƒä»»åŠ¡çš„å½±å“ é…ç½® MNLI QNLI SST-2 BERT (MLM + NSP) 84.6 90.5 93.5 ä»… MLM (æ—  NSP) 84.3 90.2 93.2 ä»… LTR (å·¦åˆ°å³) 82.1 87.4 91.3 LTR + BiLSTM 82.8 88.1 91.6 ç»“è®ºï¼š MLM æ¯”å•å‘ LTR æå‡çº¦ 2.5% NSP æå‡æœ‰é™ (~0.3%)ï¼Œåç»­ RoBERTa ç§»é™¤äº†å®ƒ å®éªŒ 2ï¼šæ¨¡å‹è§„æ¨¡çš„å½±å“ æ¨¡å‹ å±‚æ•° éšè—ç»´åº¦ å‚æ•°é‡ MNLI BERT-Base 12 768 110M 84.6 BERT-Large 24 1024 340M 86.7 ç»“è®ºï¼šæ›´å¤§çš„æ¨¡å‹ = æ›´å¥½çš„æ€§èƒ½ (Scaling Law çš„æ—©æœŸéªŒè¯) å®éªŒ 3ï¼šMask ç­–ç•¥çš„å½±å“ Mask ç­–ç•¥ æ•ˆæœ 100% [MASK] æ¬¡ä¼˜ï¼Œé¢„è®­ç»ƒä¸å¾®è°ƒåˆ†å¸ƒä¸ä¸€è‡´ 80%/10%/10% (è®ºæ–‡æ–¹æ¡ˆ) æœ€ä¼˜ éšæœºæ¯”ä¾‹ ä¸ç¨³å®š 1.6 è®­ç»ƒç»†èŠ‚ä¸è¶…å‚æ•°é¢„è®­ç»ƒé…ç½®# æ•°æ®é›†- BooksCorpus: 800M è¯ (11,038 æœ¬ä¹¦)- English Wikipedia: 2,500M è¯ (ä»…æ–‡æœ¬ï¼Œå»é™¤è¡¨æ ¼/åˆ—è¡¨)# è®­ç»ƒé…ç½®batch_size = 256max_seq_length = 512 (å‰90%æ­¥ç”¨128ï¼Œå10%ç”¨512)learning_rate = 1e-4warmup_steps = 10,000total_steps = 1,000,000optimizer = Adam (Î²1=0.9, Î²2=0.999)# ç¡¬ä»¶BERT-Base: 4 TPU Pods (16 TPU chips), 4å¤©BERT-Large: 16 TPU Pods (64 TPU chips), 4å¤© å¾®è°ƒé…ç½®# é€šç”¨å¾®è°ƒè¶…å‚æ•°batch_size = 16 æˆ– 32learning_rate = 2e-5, 3e-5, 5e-5 (é€‰æœ€ä¼˜)epochs = 2-4dropout = 0.1# ä¸åŒä»»åŠ¡çš„å¾®è°ƒæ—¶é—´MRPC (3.5kæ ·æœ¬): ~1åˆ†é’ŸSST-2 (67kæ ·æœ¬): ~1å°æ—¶ SQuAD (100kæ ·æœ¬): ~30åˆ†é’Ÿ 1.4 MLM ä¸å¾®è°ƒèŒƒå¼æ·±åº¦è§£æğŸ¯ ä»€ä¹ˆæ˜¯ MLMï¼ˆMasked Language Modelï¼‰ï¼Ÿé€šä¿—ç†è§£ï¼šè®© AI åšâ€å®Œå½¢å¡«ç©ºâ€æ¸¸æˆ å°å­¦è¯­æ–‡é¢˜ï¼š \"å°æ˜ _____ å­¦æ ¡ä¸Šè¯¾\" ç­”æ¡ˆï¼šå»ã€åˆ°ã€åœ¨...BERT çš„ MLM å°±æ˜¯è®© AI åšè¿™ç§\"å®Œå½¢å¡«ç©º\"ï¼ MLM å…·ä½“æ“ä½œæµç¨‹# åŸå§‹å¥å­åŸå¥ = \"æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨\"# Step 1: éšæœºé€‰æ‹© 15% çš„è¯è¿›è¡Œå¤„ç†é€‰ä¸­ = \"åŒ—äº¬\"# Step 2: å¯¹é€‰ä¸­çš„è¯è¿›è¡Œä¸‰ç§å¤„ç†ï¼ˆéšæœºé€‰ä¸€ç§ï¼‰å¤„ç†å = \"æˆ‘çˆ± [MASK] å¤©å®‰é—¨\" # 80% æ¦‚ç‡ï¼šæ›¿æ¢ä¸º [MASK]æˆ–è€… = \"æˆ‘çˆ± ä¸Šæµ· å¤©å®‰é—¨\" # 10% æ¦‚ç‡ï¼šæ›¿æ¢ä¸ºéšæœºè¯æˆ–è€… = \"æˆ‘çˆ± åŒ—äº¬ å¤©å®‰é—¨\" # 10% æ¦‚ç‡ï¼šä¿æŒä¸å˜# Step 3: è®©æ¨¡å‹é¢„æµ‹è¢«é®ä½çš„è¯æ˜¯ä»€ä¹ˆæ¨¡å‹è¾“å…¥ = \"æˆ‘çˆ± [MASK] å¤©å®‰é—¨\"æ¨¡å‹è¾“å‡º = \"åŒ—äº¬\" âœ… (å¦‚æœé¢„æµ‹å¯¹äº†ï¼Œloss å¾ˆå°) = \"ä¸Šæµ·\" âŒ (å¦‚æœé¢„æµ‹é”™äº†ï¼Œloss å¾ˆå¤§) ä¸ºä»€ä¹ˆ MLM èƒ½è®© BERT å­¦ä¼šâ€ç†è§£è¯­è¨€â€ï¼Ÿåœºæ™¯ 1ï¼š è¾“å…¥ï¼š\"æˆ‘å» [MASK] å­˜é’±\" æ¨¡å‹å­¦ä¼šï¼šçœ‹åˆ°\"å­˜é’±\" â†’ é¢„æµ‹\"é“¶è¡Œ\"ï¼ˆé‡‘èæœºæ„ï¼‰åœºæ™¯ 2ï¼š è¾“å…¥ï¼š\"æ²³è¾¹çš„ [MASK] å¾ˆé™¡å³­\" æ¨¡å‹å­¦ä¼šï¼šçœ‹åˆ°\"æ²³è¾¹\"\"é™¡å³­\" â†’ é¢„æµ‹\"æ²³å²¸/å ¤å\"é€šè¿‡æ•°åäº¿æ¬¡è¿™æ ·çš„\"å®Œå½¢å¡«ç©º\"è®­ç»ƒåï¼š â†’ æ¨¡å‹å­¦ä¼šäº†è¯è¯­ä¹‹é—´çš„å…³ç³» â†’ æ¨¡å‹å­¦ä¼šäº†è¯­æ³•ç»“æ„ â†’ æ¨¡å‹å­¦ä¼šäº†å¸¸è¯†çŸ¥è¯† MLM è®­ç»ƒä»£ç ç¤ºä¾‹from transformers import BertTokenizer, BertForMaskedLMimport torch# åŠ è½½æ¨¡å‹tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')model = BertForMaskedLM.from_pretrained('bert-base-chinese')# è¾“å…¥ä¸€ä¸ªå¸¦ [MASK] çš„å¥å­text = \"æˆ‘çˆ±[MASK]å¤©å®‰é—¨\"inputs = tokenizer(text, return_tensors='pt')# æ¨¡å‹é¢„æµ‹with torch.no_grad(): outputs = model(**inputs) predictions = outputs.logits# æ‰¾åˆ° [MASK] ä½ç½®çš„é¢„æµ‹ç»“æœmask_index = (inputs['input_ids'] == tokenizer.mask_token_id).nonzero()[0, 1]predicted_token_id = predictions[0, mask_index].argmax(dim=-1)predicted_token = tokenizer.decode(predicted_token_id)print(f\"åŸå¥: {text}\")print(f\"é¢„æµ‹: {predicted_token}\") # è¾“å‡º: åŒ—äº¬ ğŸ”§ ä»€ä¹ˆæ˜¯å¾®è°ƒèŒƒå¼ï¼ˆFine-tuning Paradigmï¼‰ï¼Ÿé€šä¿—ç†è§£ï¼šåŸ¹å…»â€é€šæ‰â€å†åŸ¹å…»â€ä¸“æ‰â€ ä¼ ç»Ÿæ–¹å¼ï¼ˆä»é›¶å¼€å§‹ï¼‰ï¼š â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ ä»»åŠ¡ï¼šæƒ…æ„Ÿåˆ†æ â”‚ â”‚ æ•°æ®ï¼š10ä¸‡æ¡ç”µå•†è¯„è®º â”‚ â”‚ è®­ç»ƒï¼šä»éšæœºåˆå§‹åŒ–å¼€å§‹ï¼Œè®­ç»ƒä¸€ä¸ªä¸“é—¨çš„æ¨¡å‹ â”‚ â”‚ è€—æ—¶ï¼š3å¤©ï¼Œéœ€è¦å¤§é‡æ ‡æ³¨æ•°æ® â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ é—®é¢˜ï¼šæ¯ä¸ªä»»åŠ¡éƒ½è¦ä»å¤´è®­ç»ƒï¼Œé‡å¤åŠ³åŠ¨ï¼ å¾®è°ƒèŒƒå¼ï¼ˆç«™åœ¨å·¨äººè‚©è†€ä¸Šï¼‰ï¼š ç¬¬ä¸€é˜¶æ®µï¼šé¢„è®­ç»ƒï¼ˆPre-trainingï¼‰â€”â€” åªåšä¸€æ¬¡ï¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ æ•°æ®ï¼šæ•´ä¸ªç»´åŸºç™¾ç§‘ + å¤§é‡ä¹¦ç± (æ•°åäº¿è¯) â”‚ â”‚ ä»»åŠ¡ï¼šMLMï¼ˆå®Œå½¢å¡«ç©ºï¼‰ â”‚ â”‚ ç›®çš„ï¼šè®©æ¨¡å‹å­¦ä¼š\"ç†è§£è¯­è¨€\" â”‚ â”‚ è€—æ—¶ï¼šæ•°å‘¨ (ä½†åªéœ€åšä¸€æ¬¡ï¼Œç”± Google å®Œæˆ) â”‚ â”‚ äº§å‡ºï¼šBERT é¢„è®­ç»ƒæ¨¡å‹ âœ¨ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â–¼ ä¸‹è½½ç°æˆçš„ BERT ç¬¬äºŒé˜¶æ®µï¼šå¾®è°ƒï¼ˆFine-tuningï¼‰â€”â€” æ¯ä¸ªä»»åŠ¡åªéœ€å‡ å°æ—¶ï¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ ä»»åŠ¡Aï¼šæƒ…æ„Ÿåˆ†æ â”‚ â”‚ æ•°æ®ï¼šä»…éœ€ 1000 æ¡æ ‡æ³¨æ•°æ®ï¼ â”‚ â”‚ æ–¹æ³•ï¼šåœ¨ BERT ä¸ŠåŠ ä¸€ä¸ªåˆ†ç±»å±‚ï¼Œå¾®è°ƒå‡ è½® â”‚ â”‚ è€—æ—¶ï¼š30åˆ†é’Ÿ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ å¾®è°ƒç¤ºä¾‹ï¼šæƒ…æ„Ÿåˆ†æå®Œæ•´ä»£ç from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArgumentsfrom datasets import Dataset# ============ ç¬¬ä¸€æ­¥ï¼šå‡†å¤‡å°‘é‡æ ‡æ³¨æ•°æ® ============train_data = { \"text\": [ \"è¿™ä¸ªäº§å“å¤ªæ£’äº†ï¼Œå¼ºçƒˆæ¨èï¼\", \"è´¨é‡å¾ˆå·®ï¼Œç”¨äº†ä¸€å¤©å°±åäº†\", \"ä¸€èˆ¬èˆ¬ï¼Œæ²¡æœ‰æƒŠå–œä¹Ÿæ²¡æœ‰å¤±æœ›\", \"è¶…çº§å–œæ¬¢ï¼Œå·²ç»å›è´­ä¸‰æ¬¡\", \"å®¢æœæ€åº¦å¾ˆå·®ï¼Œå†ä¹Ÿä¸ä¹°äº†\", \"æ€§ä»·æ¯”å¾ˆé«˜ï¼Œå€¼å¾—è´­ä¹°\", ], \"label\": [1, 0, 0, 1, 0, 1] # 1=æ­£é¢, 0=è´Ÿé¢}# ============ ç¬¬äºŒæ­¥ï¼šåŠ è½½é¢„è®­ç»ƒçš„ BERT ============# è¿™ä¸ª BERT å·²ç»é€šè¿‡ MLM ä»»åŠ¡å­¦ä¼šäº†\"ç†è§£ä¸­æ–‡\"tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')model = BertForSequenceClassification.from_pretrained( 'bert-base-chinese', num_labels=2 # æ­£é¢/è´Ÿé¢ ä¸¤ä¸ªç±»åˆ«)# ============ ç¬¬ä¸‰æ­¥ï¼šæ•°æ®é¢„å¤„ç† ============def tokenize_function(examples): return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)dataset = Dataset.from_dict(train_data)tokenized_dataset = dataset.map(tokenize_function, batched=True)# ============ ç¬¬å››æ­¥ï¼šå¾®è°ƒè®­ç»ƒ ============training_args = TrainingArguments( output_dir=\"./results\", num_train_epochs=3, # åªéœ€è¦è®­ç»ƒ 3 è½®ï¼ per_device_train_batch_size=4, learning_rate=2e-5, # å¾ˆå°çš„å­¦ä¹ ç‡ï¼Œè½»å¾®è°ƒæ•´)trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_dataset,)trainer.train() # å‡ åˆ†é’Ÿå°±å®Œæˆï¼# ============ ç¬¬äº”æ­¥ï¼šä½¿ç”¨å¾®è°ƒåçš„æ¨¡å‹ ============def predict(text): inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True) outputs = model(**inputs) prediction = outputs.logits.argmax(dim=-1).item() return \"æ­£é¢ ğŸ˜Š\" if prediction == 1 else \"è´Ÿé¢ ğŸ˜\"# æµ‹è¯•print(predict(\"è¿™æ¬¾æ‰‹æœºæ‹ç…§æ•ˆæœæƒŠè‰³\")) # â†’ æ­£é¢ ğŸ˜Šprint(predict(\"ç‰©æµå¤ªæ…¢äº†ï¼Œç­‰äº†ä¸€å‘¨\")) # â†’ è´Ÿé¢ ğŸ˜ å½¢è±¡ç±»æ¯”æ€»ç»“â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ ğŸ“ æ•™è‚²ç±»æ¯” â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ â”‚â”‚ ã€é¢„è®­ç»ƒ Pre-trainingã€‘= ä¸Šå¤§å­¦ï¼Œæ¥å—é€šè¯†æ•™è‚² â”‚â”‚ â€¢ å­¦ä¹ è¯­æ–‡ã€æ•°å­¦ã€è‹±è¯­ã€ç‰©ç†... â”‚â”‚ â€¢ ç›®æ ‡ï¼šæˆä¸ºä¸€ä¸ªæœ‰åŸºç¡€çŸ¥è¯†çš„\"é€šæ‰\" â”‚â”‚ â€¢ æ—¶é—´ï¼š4å¹´ â”‚â”‚ â€¢ æˆæœ¬ï¼šé«˜ â”‚â”‚ â”‚â”‚ ã€MLM ä»»åŠ¡ã€‘= å¤§å­¦é‡Œçš„å„ç§ç»ƒä¹ é¢˜ â”‚â”‚ â€¢ å®Œå½¢å¡«ç©ºã€é˜…è¯»ç†è§£ã€è¯­æ³•ç»ƒä¹ ... â”‚â”‚ â€¢ ç›®æ ‡ï¼šé”»ç‚¼è¯­è¨€ç†è§£èƒ½åŠ› â”‚â”‚ â”‚â”‚ ã€å¾®è°ƒ Fine-tuningã€‘= å·¥ä½œåçš„å²—ä½åŸ¹è®­ â”‚â”‚ â€¢ é’ˆå¯¹å…·ä½“å·¥ä½œï¼ˆæƒ…æ„Ÿåˆ†æ/é—®ç­”/ç¿»è¯‘ï¼‰å­¦ä¹  â”‚â”‚ â€¢ ç›®æ ‡ï¼šæˆä¸ºæŸä¸ªé¢†åŸŸçš„\"ä¸“æ‰\" â”‚â”‚ â€¢ æ—¶é—´ï¼šå‡ å¤© â”‚â”‚ â€¢ æˆæœ¬ï¼šä½ï¼ˆå› ä¸ºå·²ç»æœ‰åŸºç¡€äº†ï¼‰ â”‚â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 1.5 [MASK] Token æœºåˆ¶æ·±åº¦è§£æ[MASK] åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿfrom transformers import BertTokenizertokenizer = BertTokenizer.from_pretrained('bert-base-chinese')# [MASK] å°±æ˜¯ä¸€ä¸ªç‰¹æ®Šçš„ Token IDprint(tokenizer.mask_token) # è¾“å‡º: [MASK]print(tokenizer.mask_token_id) # è¾“å‡º: 103 å®Œæ•´è¾“å…¥å¯¹æ¯”# åŸå§‹å¥å­åŸå¥ = \"æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨\"tokenizer(åŸå¥)Token: [\"[CLS]\", \"æˆ‘\", \"çˆ±\", \"åŒ—äº¬\", \"å¤©\", \"å®‰\", \"é—¨\", \"[SEP]\"]Token ID: [ 101, 2769, 4263, 1266, 1921, 2128, 7305, 102 ]# åš MLM æ—¶æŠŠ \"åŒ—äº¬\" æ›¿æ¢æˆ [MASK]é®è”½å¥ = \"æˆ‘çˆ±[MASK]å¤©å®‰é—¨\"tokenizer(é®è”½å¥)Token: [\"[CLS]\", \"æˆ‘\", \"çˆ±\", \"[MASK]\", \"å¤©\", \"å®‰\", \"é—¨\", \"[SEP]\"]Token ID: [ 101, 2769, 4263, 103, 1921, 2128, 7305, 102 ] â†‘ å°±æ˜¯æŠŠ 1266 æ¢æˆäº† 103ï¼ æ¨¡å‹å®é™…çœ‹åˆ°çš„æ˜¯ä»€ä¹ˆï¼Ÿ# æ¨¡å‹è¾“å…¥çš„å°±æ˜¯ä¸€ä¸²æ•°å­—ï¼ˆToken IDsï¼‰input_ids = [101, 2769, 4263, 103, 1921, 2128, 7305, 102] â†‘ 103 = [MASK] çš„ç¼–å·# ç»è¿‡ Embedding å±‚å# 103 ä¼šè¢«æŸ¥è¡¨è½¬æ¢æˆä¸€ä¸ª 768 ç»´çš„å‘é‡embedding_table = model.embeddings.word_embeddings.weight# å½¢çŠ¶: [21128, 768] (è¯è¡¨å¤§å° Ã— éšè—ç»´åº¦)mask_embedding = embedding_table[103] # [MASK] çš„å‘é‡è¡¨ç¤º# å½¢çŠ¶: [768]# å€¼: [0.023, -0.156, 0.234, ..., 0.089] â† è¿™æ˜¯å¯å­¦ä¹ çš„å‚æ•°ï¼ ä¸ºä»€ä¹ˆ 103 åˆå§‹æ—¶å•¥ä¹Ÿä¸ä»£è¡¨ï¼Ÿ# Embedding å±‚è¾“å‡ºembeddings = [ E_cls, # [CLS] çš„å‘é‡ E_æˆ‘, # \"æˆ‘\" çš„è¯­ä¹‰å‘é‡ E_çˆ±, # \"çˆ±\" çš„è¯­ä¹‰å‘é‡ E_mask, # 103 å¯¹åº”çš„å‘é‡ â† è¿™ä¸ªå‘é‡æœ¬èº«æ²¡æœ‰è¯­ä¹‰ï¼ E_å¤©, # \"å¤©\" çš„è¯­ä¹‰å‘é‡ E_å®‰, # \"å®‰\" çš„è¯­ä¹‰å‘é‡ E_é—¨, # \"é—¨\" çš„è¯­ä¹‰å‘é‡ E_sep, # [SEP] çš„å‘é‡]# E_mask åˆå§‹å€¼å¯èƒ½æ˜¯ [0.01, -0.02, 0.03, ...]# å®ƒæœ¬èº«ä¸ä»£è¡¨ä»»ä½•è¯ä¹‰ï¼Œåªæ˜¯ä¸€ä¸ª\"å ä½ç¬¦\" å…³é”®ï¼š103 æœ¬èº«ç¡®å®å•¥ä¹Ÿä¸ä»£è¡¨ï¼Œä½†ç»è¿‡ Self-Attention åä¼šå¸æ”¶å‘¨å›´è¯çš„ä¿¡æ¯ï¼ 1.6 BERT æ¨¡å‹æ¶æ„è¯¦è§£æ¶æ„å‚æ•° é…ç½® BERT-Base BERT-Large Transformer å±‚æ•° (L) 12 24 éšè—å±‚ç»´åº¦ (H) 768 1024 æ³¨æ„åŠ›å¤´æ•° (A) 12 16 æ¯ä¸ªå¤´çš„ç»´åº¦ (H/A) 64 64 æ€»å‚æ•°é‡ 110M 340M FFN ä¸­é—´å±‚ç»´åº¦ 3072 4096 è¾“å…¥è¡¨ç¤º (Input Representation)Token Embedding: æ¯ä¸ªè¯çš„å‘é‡è¡¨ç¤º [vocab_size, 768] + Position Embedding: ä½ç½®ç¼–ç  [512, 768] (æœ€å¤§åºåˆ—é•¿åº¦ 512) +Segment Embedding: å¥å­åˆ†éš” [2, 768] (Sentence A æˆ– B) =Input Representation: [batch, seq_len, 768] å…·ä½“ä¾‹å­: è¾“å…¥: \"[CLS] æˆ‘ çˆ± NLP [SEP] å®ƒ å¾ˆ æœ‰è¶£ [SEP]\"Token IDs: [101, 2769, 4263, 21128, 102, 1045, 1447, 3300, 4638, 102]Segment IDs: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]Position IDs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]ä¸‰ä¸ª embedding ç›¸åŠ å: [10, 768] 1.5 è®ºæ–‡å®éªŒç»“æœä¸å½±å“å…³é”®æ€§èƒ½æå‡GLUE åŸºå‡†æµ‹è¯• (11 ä¸ª NLP ä»»åŠ¡): BERT-Base: 78.6% â†’ æå‡ 7% BERT-Large: 80.5% â†’ æå‡ 9% SQuAD é—®ç­”ä»»åŠ¡: BERT-Large: F1 = 93.2% (è¶…è¶Šäººç±»æ°´å¹³ 91.2%) ä¸ºä»€ä¹ˆæ•ˆæœè¿™ä¹ˆå¥½ï¼Ÿ åŒå‘ä¸Šä¸‹æ–‡: æ¯ä¸ªè¯éƒ½èƒ½çœ‹åˆ°å®Œæ•´å¥å­ æ·±åº¦äº¤äº’: 12/24 å±‚é€å±‚ç²¾ç‚¼è¡¨ç¤º å¤§è§„æ¨¡é¢„è®­ç»ƒ: BooksCorpus (800M è¯) + Wikipedia (2500M è¯) è¿ç§»å­¦ä¹ : é¢„è®­ç»ƒ + å¾®è°ƒèŒƒå¼ ğŸ”¬ Part 2: BERT å‰ä¸¤å±‚å®Œæ•´æ¨æ¼”è¾“å…¥å‡†å¤‡ï¼šMLM ä»»åŠ¡å¥å­: \"æˆ‘ çˆ± [MASK] å¤©å®‰é—¨\"Token IDs: [101, 2769, 4263, 103, 1921, 102] [CLS] æˆ‘ çˆ± MASK å¤©å®‰é—¨ [SEP]Embedding å: Xâ‚€ = [6, 768]ä½ç½®0 [CLS]: [0.12, -0.05, 0.33, ..., 0.45]ä½ç½®1 æˆ‘: [0.45, 0.23, -0.12, ..., 0.12]ä½ç½®2 çˆ±: [0.33, 0.56, 0.78, ..., 0.89]ä½ç½®3 MASK: [0.01, 0.02, 0.01, ..., 0.03] â† å‡ ä¹æ˜¯ç©ºçš„ï¼ä½ç½®4 å¤©å®‰é—¨: [0.67, -0.34, 0.45, ..., 0.56]ä½ç½®5 [SEP]: [0.78, 0.34, -0.12, ..., 0.12] Layer 1ï¼šç¬¬ä¸€å±‚è¯¦ç»†è®¡ç®—Step 1.1: è®¡ç®— Qã€Kã€VXâ‚€ = [6, 768] # è¾“å…¥Qâ‚ = Xâ‚€ @ W_Q # [6, 768] @ [768, 768] = [6, 768]Kâ‚ = Xâ‚€ @ W_K # [6, 768] @ [768, 768] = [6, 768]Vâ‚ = Xâ‚€ @ W_V # [6, 768] @ [768, 768] = [6, 768]# æ¯ä¸ªä½ç½®éƒ½æœ‰è‡ªå·±çš„ Qã€Kã€V å‘é‡Qâ‚ = [Q_cls, Q_æˆ‘, Q_çˆ±, Q_mask, Q_å¤©å®‰é—¨, Q_sep]Kâ‚ = [K_cls, K_æˆ‘, K_çˆ±, K_mask, K_å¤©å®‰é—¨, K_sep]Vâ‚ = [V_cls, V_æˆ‘, V_çˆ±, V_mask, V_å¤©å®‰é—¨, V_sep] Step 1.2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°scores = Qâ‚ @ Kâ‚.T / sqrt(64) # [6, 768] @ [768, 6] = [6, 6]# [CLS] æˆ‘ çˆ± MASK å¤©å®‰é—¨ [SEP]# [CLS] [ 2.1 1.3 1.5 0.2 1.8 1.2 ]# æˆ‘ [ 1.2 3.1 2.3 0.3 1.5 0.8 ]# çˆ± [ 1.4 2.5 2.8 0.4 2.1 0.9 ]# MASK [ 0.8 1.9 2.4 0.1 2.6 0.7 ] â† MASK è¡Œ# å¤©å®‰é—¨ [ 1.6 1.4 2.0 0.3 3.2 1.1 ]# [SEP] [ 1.3 0.9 1.1 0.2 1.3 2.5 ] Step 1.3: Softmax å½’ä¸€åŒ–weights = softmax(scores, dim=-1) # æ¯è¡Œå’Œä¸º1# [CLS] æˆ‘ çˆ± MASK å¤©å®‰é—¨ [SEP]# [CLS] [0.18 0.12 0.15 0.03 0.40 0.12]# æˆ‘ [0.10 0.35 0.25 0.02 0.20 0.08]# çˆ± [0.12 0.22 0.28 0.03 0.28 0.07]# MASK [0.08 0.18 0.28 0.01 0.38 0.07] â† é‡ç‚¹çœ‹è¿™è¡Œï¼# å¤©å®‰é—¨ [0.11 0.09 0.16 0.02 0.55 0.07]# [SEP] [0.15 0.10 0.12 0.03 0.15 0.45]# MASK ä½ç½®: 28% çœ‹\"çˆ±\"ï¼Œ38% çœ‹\"å¤©å®‰é—¨\"ï¼Œåªæœ‰ 1% çœ‹è‡ªå·±ï¼ Step 1.4: åŠ æƒæ±‚å’Œå¾—åˆ°æ–°è¡¨ç¤ºHâ‚ = weights @ Vâ‚ # [6, 6] @ [6, 768] = [6, 768]# MASK ä½ç½®çš„æ–°å‘é‡:Hâ‚[3] = 0.08Ã—V_cls + 0.18Ã—V_æˆ‘ + 0.28Ã—V_çˆ± + 0.01Ã—V_mask + 0.38Ã—V_å¤©å®‰é—¨ + 0.07Ã—V_sep â†‘ â†‘ â†‘ å‡ ä¹å¿½ç•¥ ä¸»è¦æ¥è‡ª\"çˆ±\" ä¸»è¦æ¥è‡ª\"å¤©å®‰é—¨\"# ç»“æœ: MASK ä½ç½®ç°åœ¨èåˆäº† \"çˆ±\" å’Œ \"å¤©å®‰é—¨\" çš„ä¿¡æ¯ï¼Hâ‚[3] = [0.45, 0.67, 0.23, ..., 0.78] â† ä¸å†æ˜¯ç©ºå£³äº†ï¼ Step 1.5: Feed-Forward Network (FFN)# è¾“å…¥Hâ‚ = [6, 768] # Attention çš„è¾“å‡º# FFN: ä¸¤å±‚çº¿æ€§å˜æ¢step1 = Hâ‚ @ Wâ‚ # [6, 768] @ [768, 3072] = [6, 3072] å…ˆæ‰©å¤§4å€step2 = ReLU(step1) # [6, 3072] æ¿€æ´»å‡½æ•°step3 = step2 @ Wâ‚‚ # [6, 3072] @ [3072, 768] = [6, 768] å†å‹å›å»FFN_out = step3 # [6, 768]# ç»´åº¦å˜åŒ–å›¾ç¤º768 â”€â”€æ‰©å¤§â”€â”€â†’ 3072 â”€â”€å‹ç¼©â”€â”€â†’ 768 Wâ‚ Wâ‚‚ Step 1.6: æ®‹å·®è¿æ¥ + LayerNorm# æ®‹å·®è¿æ¥ï¼šæŠŠè¾“å…¥åŠ å›æ¥output = Hâ‚ + FFN_out # [6, 768] + [6, 768] = [6, 768]# LayerNorm å½’ä¸€åŒ–Xâ‚ = LayerNorm(output) # [6, 768] æ®‹å·®è¿æ¥ç¤ºæ„å›¾ï¼š Hâ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â†“ â”‚ (è·³è·ƒè¿æ¥)â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚ FFN â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â†“ â†“FFN_out â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ (+) ç›¸åŠ  â”€â”€â†’ LayerNorm â”€â”€â†’ Xâ‚ â†‘ æ®‹å·® = Hâ‚ + FFN_out ä¸ºä»€ä¹ˆéœ€è¦æ®‹å·®è¿æ¥ï¼Ÿ é—®é¢˜ æ®‹å·®è§£å†³æ–¹æ¡ˆ æ¢¯åº¦æ¶ˆå¤± æ¢¯åº¦å¯ä»¥ç›´æ¥é€šè¿‡â€è·³è·ƒè¿æ¥â€å›ä¼  ä¿¡æ¯ä¸¢å¤± åŸå§‹ä¿¡æ¯ Hâ‚ è¢«ä¿ç•™ï¼Œä¸ä¼šå®Œå…¨è¢«è¦†ç›– è®­ç»ƒå›°éš¾ ç½‘ç»œåªéœ€å­¦ä¹ â€å·®å¼‚â€ï¼Œæ›´å®¹æ˜“ä¼˜åŒ– # æœ¬è´¨ï¼šFFN åªå­¦ä¹ \"å¢é‡\"Xâ‚ = Hâ‚ + FFN(Hâ‚) = Hâ‚ + Î”H # åŸå§‹ + ä¿®æ­£é‡ Layer 1 è¾“å‡ºï¼š Xâ‚ = [6, 768]ä½ç½®3 (MASK): [0.52, 0.71, 0.34, ..., 0.82] â†‘ å·²ç»åŒ…å«äº† \"çˆ±___å¤©å®‰é—¨\" çš„æ¨¡å¼ä¿¡æ¯ Layer 2ï¼šç¬¬äºŒå±‚è¯¦ç»†è®¡ç®—Step 2.1: è®¡ç®—æ–°çš„ Qã€Kã€V# è¾“å…¥æ˜¯ Layer 1 çš„è¾“å‡ºXâ‚ = [6, 768]Qâ‚‚ = Xâ‚ @ W_Q # æ–°çš„ Qï¼ˆæƒé‡çŸ©é˜µå’Œ Layer1 ä¸åŒï¼ï¼‰Kâ‚‚ = Xâ‚ @ W_K # æ–°çš„ KVâ‚‚ = Xâ‚ @ W_V # æ–°çš„ V Step 2.2: è®¡ç®—æ³¨æ„åŠ›ï¼ˆåŸºäºæ›´æ–°åçš„è¡¨ç¤ºï¼‰scores = Qâ‚‚ @ Kâ‚‚.T / sqrt(64)# ç°åœ¨ MASK ä½ç½®å·²ç»æœ‰äº†ä¸Šä¸‹æ–‡ä¿¡æ¯# å®ƒçš„ Q å‘é‡æ›´\"èªæ˜\"äº†ï¼Œèƒ½æ‰¾åˆ°æ›´ç›¸å…³çš„è¯# [CLS] æˆ‘ çˆ± MASK å¤©å®‰é—¨ [SEP]# MASK [ 0.5 1.5 2.8 0.2 3.5 0.4 ]# â†‘ â†‘# æ›´å…³æ³¨\"çˆ±\" æ›´å…³æ³¨\"å¤©å®‰é—¨\"weights[3] = softmax([0.5, 1.5, 2.8, 0.2, 3.5, 0.4]) = [0.04, 0.10, 0.28, 0.02, 0.52, 0.04]# â†‘ â†‘# 28%çœ‹çˆ± 52%çœ‹å¤©å®‰é—¨ Step 2.3: åŠ æƒæ±‚å’ŒHâ‚‚[3] = 0.04Ã—V_cls + 0.10Ã—V_æˆ‘ + 0.28Ã—V_çˆ± + 0.02Ã—V_mask + 0.52Ã—V_å¤©å®‰é—¨ + 0.04Ã—V_sep# è¿™æ¬¡ V_å¤©å®‰é—¨ å·²ç»ä¸æ˜¯åŸå§‹çš„äº†# å®ƒåœ¨ Layer1 ä¸­ä¹Ÿèåˆäº†ä¸Šä¸‹æ–‡ï¼ŒçŸ¥é“\"å¤©å®‰é—¨åœ¨åŒ—äº¬\"# æ‰€ä»¥ MASK é—´æ¥è·å¾—äº†\"åŒ—äº¬\"çš„ä¿¡æ¯ï¼Hâ‚‚[3] = [0.68, 0.82, 0.45, ..., 0.91] Step 2.4: FFN + æ®‹å·® + LayerNormXâ‚‚ = LayerNorm(Hâ‚‚ + FFN(Hâ‚‚)) # [6, 768] Layer 2 è¾“å‡ºï¼š Xâ‚‚ = [6, 768]ä½ç½®3 (MASK): [0.71, 0.85, 0.52, ..., 0.93] â†‘ ç°åœ¨çŸ¥é“: \"æˆ‘çˆ±___å¤©å®‰é—¨\" â†’ è¿™ä¸ªç©ºåº”è¯¥å¡«åœ°å å¤©å®‰é—¨ç›¸å…³ â†’ å¯èƒ½æ˜¯\"åŒ—äº¬\" ä¸¤å±‚å¯¹æ¯”æ€»ç»“â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ MASK ä½ç½®å‘é‡çš„å˜åŒ– â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ â”‚â”‚ Embedding: [0.01, 0.02, 0.01, ...] â† ç©ºå£³ï¼Œæ— è¯­ä¹‰ â”‚â”‚ â†“ â”‚â”‚ Layer 1: [0.52, 0.71, 0.34, ...] â† èåˆäº†\"çˆ±\"\"å¤©å®‰é—¨\" â”‚â”‚ â†“ â”‚â”‚ Layer 2: [0.71, 0.85, 0.52, ...] â† æ›´æ·±å±‚ç†è§£ â”‚â”‚ â†“ â”‚â”‚ ... (ç»§ç»­ 10 å±‚) â”‚â”‚ â†“ â”‚â”‚ Layer 12: [0.93, 0.87, 0.76, ...] â† ç¡®å®šæ˜¯\"åŒ—äº¬\" â”‚â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ å±‚ MASK å­¦åˆ°äº†ä»€ä¹ˆ Embedding ç©ºçš„å ä½ç¬¦ï¼Œ103åªæ˜¯ä¸ªID Layer 1 â€œçˆ±â€åé¢æ˜¯å®¾è¯­ï¼Œâ€å¤©å®‰é—¨â€æ˜¯åœ°æ ‡ Layer 2 â€œæˆ‘çˆ±Xå¤©å®‰é—¨â€æ˜¯å›ºå®šæ­é…ï¼ŒXæ˜¯åœ°å Layer 3-6 å¤©å®‰é—¨åœ¨åŒ—äº¬ï¼Œè¿™æ˜¯å¸¸è¯† Layer 7-12 ç¡®å®šç­”æ¡ˆæ˜¯â€åŒ—äº¬â€ï¼Œæ’é™¤å…¶ä»–å¯èƒ½ ğŸ” Part 3: Self-Attention æ ¸å¿ƒæœºåˆ¶2.0 è¾“å…¥å‡†å¤‡å¥å­: \"æˆ‘ çˆ± [MASK] å¤©å®‰é—¨\"Token IDs: [101, 2769, 4263, 103, 1921, 102] [CLS] æˆ‘ çˆ± MASK å¤©å®‰é—¨ [SEP]Embedding å: Xâ‚€ = [6, 768]ä½ç½®0 [CLS]: [0.12, -0.05, 0.33, ..., 0.45] â† 768ç»´å‘é‡ä½ç½®1 æˆ‘: [0.45, 0.23, -0.12, ..., 0.12]ä½ç½®2 çˆ±: [0.33, 0.56, 0.78, ..., 0.89]ä½ç½®3 MASK: [0.01, 0.02, 0.01, ..., 0.03] â† å‡ ä¹æ˜¯ç©ºçš„ï¼ä½ç½®4 å¤©å®‰é—¨: [0.67, -0.34, 0.45, ..., 0.56]ä½ç½®5 [SEP]: [0.78, 0.34, -0.12, ..., 0.12] 2.1 Layer 1ï¼šç¬¬ä¸€å±‚è¯¦ç»†è®¡ç®—Step 1.1: è®¡ç®— Qã€Kã€VXâ‚€ = [6, 768] # è¾“å…¥Qâ‚ = Xâ‚€ @ W_Q # [6, 768] @ [768, 768] = [6, 768]Kâ‚ = Xâ‚€ @ W_K # [6, 768] @ [768, 768] = [6, 768]Vâ‚ = Xâ‚€ @ W_V # [6, 768] @ [768, 768] = [6, 768]# æ¯ä¸ªä½ç½®éƒ½æœ‰è‡ªå·±çš„ Qã€Kã€V å‘é‡Qâ‚ = [Q_cls, Q_æˆ‘, Q_çˆ±, Q_mask, Q_å¤©å®‰é—¨, Q_sep]Kâ‚ = [K_cls, K_æˆ‘, K_çˆ±, K_mask, K_å¤©å®‰é—¨, K_sep]Vâ‚ = [V_cls, V_æˆ‘, V_çˆ±, V_mask, V_å¤©å®‰é—¨, V_sep] Step 1.2: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°scores = Qâ‚ @ Kâ‚.T / sqrt(64) # [6, 768] @ [768, 6] = [6, 6]# æ³¨æ„åŠ›çŸ©é˜µï¼ˆæœªå½’ä¸€åŒ–ï¼‰# [CLS] æˆ‘ çˆ± MASK å¤©å®‰é—¨ [SEP]# [CLS] [ 2.1 1.3 1.5 0.2 1.8 1.2 ]# æˆ‘ [ 1.2 3.1 2.3 0.3 1.5 0.8 ]# çˆ± [ 1.4 2.5 2.8 0.4 2.1 0.9 ]# MASK [ 0.8 1.9 2.4 0.1 2.6 0.7 ] â† MASK è¡Œ# å¤©å®‰é—¨ [ 1.6 1.4 2.0 0.3 3.2 1.1 ]# [SEP] [ 1.3 0.9 1.1 0.2 1.3 2.5 ] Step 1.3: Softmax å½’ä¸€åŒ–weights = softmax(scores, dim=-1) # æ¯è¡Œå’Œä¸º1# æ³¨æ„åŠ›æƒé‡çŸ©é˜µ# [CLS] æˆ‘ çˆ± MASK å¤©å®‰é—¨ [SEP]# [CLS] [0.18 0.12 0.15 0.03 0.40 0.12]# æˆ‘ [0.10 0.35 0.25 0.02 0.20 0.08]# çˆ± [0.12 0.22 0.28 0.03 0.28 0.07]# MASK [0.08 0.18 0.28 0.01 0.38 0.07] â† é‡ç‚¹çœ‹è¿™è¡Œï¼# å¤©å®‰é—¨ [0.11 0.09 0.16 0.02 0.55 0.07]# [SEP] [0.15 0.10 0.12 0.03 0.15 0.45]# MASK ä½ç½®è§£è¯»:# - 28% çš„æ³¨æ„åŠ›ç»™ \"çˆ±\"# - 38% çš„æ³¨æ„åŠ›ç»™ \"å¤©å®‰é—¨\"# - åªæœ‰ 1% çœ‹è‡ªå·±ï¼ˆå› ä¸ºè‡ªå·±æ˜¯ç©ºå£³ï¼‰ Step 1.4: åŠ æƒæ±‚å’Œå¾—åˆ°æ–°è¡¨ç¤ºHâ‚ = weights @ Vâ‚ # [6, 6] @ [6, 768] = [6, 768]# MASK ä½ç½®çš„æ–°å‘é‡è®¡ç®—:Hâ‚[3] = 0.08Ã—V_cls + 0.18Ã—V_æˆ‘ + 0.28Ã—V_çˆ± + 0.01Ã—V_mask + 0.38Ã—V_å¤©å®‰é—¨ + 0.07Ã—V_sep# å…·ä½“æ•°å€¼ç¤ºä¾‹ï¼ˆå‡è®¾ç®€åŒ–åˆ°4ç»´ï¼‰:V_cls = [0.1, 0.2, 0.1, 0.3]V_æˆ‘ = [0.8, 0.1, 0.2, 0.4]V_çˆ± = [0.3, 0.7, 0.5, 0.2]V_mask = [0.0, 0.0, 0.0, 0.1] # å‡ ä¹ä¸ºç©ºV_å¤©å®‰é—¨ = [0.2, 0.4, 0.8, 0.6]V_sep = [0.1, 0.1, 0.2, 0.2]Hâ‚[3] = 0.08Ã—[0.1,0.2,0.1,0.3] + 0.18Ã—[0.8,0.1,0.2,0.4] + 0.28Ã—[0.3,0.7,0.5,0.2] + 0.01Ã—[0.0,0.0,0.0,0.1] + 0.38Ã—[0.2,0.4,0.8,0.6] + 0.07Ã—[0.1,0.1,0.2,0.2] = [0.008+0.144+0.084+0+0.076+0.007, 0.016+0.018+0.196+0+0.152+0.007, 0.008+0.036+0.14+0+0.304+0.014, 0.024+0.072+0.056+0.001+0.228+0.014] = [0.319, 0.389, 0.502, 0.395]# ç»“æœ: MASK ä½ç½®ç°åœ¨èåˆäº† \"çˆ±\" å’Œ \"å¤©å®‰é—¨\" çš„ä¿¡æ¯ï¼# ä¸å†æ˜¯ç©ºå£³ [0,0,0,0.1] äº†ï¼ Step 1.5: Feed-Forward Network (FFN)# FFN: 768 â†’ 3072 â†’ 768FFN_input = Hâ‚ # [6, 768]# ç¬¬ä¸€å±‚ï¼šå‡ç»´ + æ¿€æ´»hidden = FFN_input @ Wâ‚ # [6, 768] @ [768, 3072] = [6, 3072]hidden = ReLU(hidden) # è´Ÿæ•°å˜0ï¼Œæ­£æ•°ä¸å˜# ç¬¬äºŒå±‚ï¼šé™ç»´FFN_out = hidden @ Wâ‚‚ # [6, 3072] @ [3072, 768] = [6, 768]# MASK ä½ç½®ç¤ºä¾‹FFN_out[3] = [0.07, 0.12, 0.09, ..., 0.15] ä¸ºä»€ä¹ˆè¦ FFNï¼Ÿ Attention åªåšçº¿æ€§ç»„åˆ FFN å¼•å…¥éçº¿æ€§ï¼Œè®©æ¨¡å‹å­¦ä¹ æ›´å¤æ‚çš„æ¨¡å¼ Step 1.6: æ®‹å·®è¿æ¥ + LayerNorm# æ®‹å·®è¿æ¥ï¼šæŠŠè¾“å…¥åŠ å›æ¥residual = Hâ‚ + FFN_out # [6, 768] + [6, 768] = [6, 768]# MASK ä½ç½®residual[3] = Hâ‚[3] + FFN_out[3] = [0.319, 0.389, 0.502, 0.395] + [0.07, 0.12, 0.09, 0.15] = [0.389, 0.509, 0.592, 0.545]# LayerNorm: å½’ä¸€åŒ–Xâ‚ = LayerNorm(residual) # [6, 768] æ®‹å·®è¿æ¥ç¤ºæ„å›¾ï¼š Hâ‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚ â†“ â”‚ (è·³è·ƒè¿æ¥ï¼Œé˜²æ­¢ä¿¡æ¯ä¸¢å¤±)â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚â”‚ FFN â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚ â†“ â†“FFN_out â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ (+) ç›¸åŠ  â”€â”€â†’ LayerNorm â”€â”€â†’ Xâ‚ Layer 1 è¾“å‡ºï¼š Xâ‚ = [6, 768]ä½ç½®3 (MASK): [0.52, 0.71, 0.34, ..., 0.82] â†‘ å·²ç»åŒ…å«äº† \"çˆ±___å¤©å®‰é—¨\" çš„æ¨¡å¼ä¿¡æ¯ 2.2 Layer 2ï¼šç¬¬äºŒå±‚è¯¦ç»†è®¡ç®—Step 2.1: è®¡ç®—æ–°çš„ Qã€Kã€V# è¾“å…¥æ˜¯ Layer 1 çš„è¾“å‡ºï¼ˆå·²ç»èåˆè¿‡ä¸€æ¬¡ä¸Šä¸‹æ–‡ï¼‰Xâ‚ = [6, 768]Qâ‚‚ = Xâ‚ @ W_Q # æ–°çš„æƒé‡çŸ©é˜µï¼ä¸ Layer1 ä¸åŒKâ‚‚ = Xâ‚ @ W_KVâ‚‚ = Xâ‚ @ W_V# ç°åœ¨ MASK ä½ç½®çš„ Q å·²ç»ä¸æ˜¯ç©ºå£³äº†Q_mask_new = Xâ‚[3] @ W_Q # åŸºäºèåˆåçš„å‘é‡è®¡ç®— Step 2.2: è®¡ç®—æ³¨æ„åŠ›ï¼ˆåŸºäºæ›´æ–°åçš„è¡¨ç¤ºï¼‰scores = Qâ‚‚ @ Kâ‚‚.T / sqrt(64)# ç°åœ¨ MASK ä½ç½®å·²ç»æœ‰äº†ä¸Šä¸‹æ–‡ä¿¡æ¯# å®ƒçš„ Q å‘é‡æ›´\"èªæ˜\"äº†ï¼Œèƒ½æ‰¾åˆ°æ›´ç›¸å…³çš„è¯# [CLS] æˆ‘ çˆ± MASK å¤©å®‰é—¨ [SEP]# MASK [ 0.5 1.5 2.8 0.2 3.5 0.4 ]# â†‘ â†‘# æ›´å…³æ³¨\"çˆ±\" æ›´å…³æ³¨\"å¤©å®‰é—¨\"weights[3] = softmax([0.5, 1.5, 2.8, 0.2, 3.5, 0.4]) = [0.04, 0.10, 0.28, 0.02, 0.52, 0.04]# â†‘ â†‘# 28%çœ‹çˆ± 52%çœ‹å¤©å®‰é—¨ï¼ˆæƒé‡æ›´é«˜äº†ï¼ï¼‰ Step 2.3: åŠ æƒæ±‚å’ŒHâ‚‚[3] = 0.04Ã—V_cls + 0.10Ã—V_æˆ‘ + 0.28Ã—V_çˆ± + 0.02Ã—V_mask + 0.52Ã—V_å¤©å®‰é—¨ + 0.04Ã—V_sep# å…³é”®ï¼šè¿™æ¬¡çš„ V_å¤©å®‰é—¨ å·²ç»ä¸æ˜¯åŸå§‹çš„äº†# å®ƒåœ¨ Layer1 ä¸­ä¹Ÿèåˆäº†ä¸Šä¸‹æ–‡ï¼ŒçŸ¥é“\"å¤©å®‰é—¨åœ¨åŒ—äº¬\"# æ‰€ä»¥ MASK é—´æ¥è·å¾—äº†\"åŒ—äº¬\"çš„ä¿¡æ¯ï¼Hâ‚‚[3] = [0.68, 0.82, 0.45, ..., 0.91] Step 2.4: FFN + æ®‹å·® + LayerNormFFN_out = FFN(Hâ‚‚)Xâ‚‚ = LayerNorm(Hâ‚‚ + FFN_out) # [6, 768] Layer 2 è¾“å‡ºï¼š Xâ‚‚ = [6, 768]ä½ç½®3 (MASK): [0.71, 0.85, 0.52, ..., 0.93] â†‘ ç°åœ¨çŸ¥é“: \"æˆ‘çˆ±___å¤©å®‰é—¨\" â†’ è¿™ä¸ªç©ºåº”è¯¥å¡«åœ°å å¤©å®‰é—¨ç›¸å…³ â†’ å¯èƒ½æ˜¯\"åŒ—äº¬\" 2.3 ä¸¤å±‚å¯¹æ¯”æ€»ç»“â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ MASK ä½ç½®å‘é‡çš„æ¼”åŒ– â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ â”‚â”‚ Embedding: [0.01, 0.02, 0.01, ...] â† ç©ºå£³ï¼Œæ— è¯­ä¹‰ â”‚â”‚ â†“ Layer 1 Self-Attention â”‚â”‚ Layer 1: [0.52, 0.71, 0.34, ...] â† èåˆäº†\"çˆ±\"\"å¤©å®‰é—¨\" â”‚â”‚ â†“ Layer 2 Self-Attention â”‚â”‚ Layer 2: [0.71, 0.85, 0.52, ...] â† æ›´æ·±å±‚ç†è§£ â”‚â”‚ â†“ â”‚â”‚ ... (ç»§ç»­ 10 å±‚) â”‚â”‚ â†“ â”‚â”‚ Layer 12: [0.93, 0.87, 0.76, ...] â† ç¡®å®šæ˜¯\"åŒ—äº¬\" â”‚â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ å±‚ MASK å­¦åˆ°äº†ä»€ä¹ˆ æ³¨æ„åŠ›æƒé‡å˜åŒ– Embedding ç©ºçš„å ä½ç¬¦ - Layer 1 â€œçˆ±â€åé¢æ˜¯å®¾è¯­ï¼Œâ€å¤©å®‰é—¨â€æ˜¯åœ°æ ‡ 28%çœ‹â€çˆ±â€ï¼Œ38%çœ‹â€å¤©å®‰é—¨â€ Layer 2 â€œæˆ‘çˆ±Xå¤©å®‰é—¨â€æ˜¯å›ºå®šæ­é…ï¼ŒXæ˜¯åœ°å 28%çœ‹â€çˆ±â€ï¼Œ52%çœ‹â€å¤©å®‰é—¨â€ Layer 3-6 å¤©å®‰é—¨åœ¨åŒ—äº¬ï¼Œè¿™æ˜¯å¸¸è¯† é€æ¸èšç„¦åˆ°ç›¸å…³è¯ Layer 7-12 ç¡®å®šç­”æ¡ˆæ˜¯â€åŒ—äº¬â€ï¼Œæ’é™¤å…¶ä»–å¯èƒ½ é«˜åº¦ç¡®ä¿¡ ğŸ” Part 3: Self-Attention æ ¸å¿ƒæœºåˆ¶3.1 Q/K/V çš„æœ¬è´¨ï¼šä¿¡æ¯æ£€ç´¢ç³»ç»Ÿç›´è§‰ç±»æ¯”åœºæ™¯: åœ¨å›¾ä¹¦é¦†æ‰¾ä¹¦Q (Query): \"æˆ‘æƒ³æ‰¾å…³äºæ·±åº¦å­¦ä¹ çš„ä¹¦\" â†“ è®¡ç®—ç›¸ä¼¼åº¦K (Key): æ¯æœ¬ä¹¦çš„æ ‡ç­¾ [\"æœºå™¨å­¦ä¹ \", \"çƒ¹é¥ª\", \"å†å²\", ...] â†“ Softmax å½’ä¸€åŒ–Attention: [0.7, 0.05, 0.05, ...] (å¯¹\"æœºå™¨å­¦ä¹ \"ä¹¦çš„æ³¨æ„åŠ›æœ€é«˜) â†“ åŠ æƒæ±‚å’ŒV (Value): ä¹¦çš„å®é™…å†…å®¹ â†“Output: æ ¹æ®æ³¨æ„åŠ›æƒé‡ï¼Œèåˆæœ€ç›¸å…³ä¹¦ç±çš„çŸ¥è¯† æ•°å­¦å®šä¹‰$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$ 2.2 å®Œæ•´æ•°æ®æµä¸ç»´åº¦å˜åŒ–å‡è®¾: batch=2, seq=4, d_model=768, heads=12, d_k=64 Step 1: çº¿æ€§æŠ•å½±X: [2, 4, 768] # è¾“å…¥Q = X @ W_Q # [2, 4, 768] @ [768, 768] = [2, 4, 768]K = X @ W_K # [2, 4, 768]V = X @ W_V # [2, 4, 768] Step 2: æ‹†åˆ†æˆå¤šå¤´Q: [2, 4, 768] â†’ reshape â†’ [2, 4, 12, 64] â†’ transpose â†’ [2, 12, 4, 64]K: [2, 12, 4, 64]V: [2, 12, 4, 64] Step 3: è®¡ç®—æ³¨æ„åŠ›åˆ†æ•°Scores = Q @ K^T = [2, 12, 4, 64] @ [2, 12, 64, 4] = [2, 12, 4, 4] â† è¿™å°±æ˜¯æ³¨æ„åŠ›çŸ©é˜µï¼# æ¯ä¸ª token å¯¹å…¶ä»– token çš„å…³æ³¨ç¨‹åº¦ä¾‹å¦‚ Scores[0, 0, :, :] = I love NLP ! I [9.2 1.3 2.1 0.8] love [2.4 8.7 3.2 1.1] NLP [1.8 3.5 9.1 0.9] ! [0.5 1.2 0.7 8.9] Step 4: Scale å’Œ SoftmaxScores = Scores / sqrt(64) â‰ˆ Scores / 8Weights = softmax(Scores, dim=-1) # [2, 12, 4, 4]# æ¯è¡Œå’Œä¸º 1ä¾‹å¦‚ Weights[0, 0, 1, :] = [0.15, 0.52, 0.28, 0.05]è¡¨ç¤º \"love\" å¯¹ [\"I\", \"love\", \"NLP\", \"!\"] çš„æ³¨æ„åŠ›åˆ†å¸ƒ Step 5: åŠ æƒæ±‚å’ŒOutput = Weights @ V = [2, 12, 4, 4] @ [2, 12, 4, 64] = [2, 12, 4, 64] Step 6: åˆå¹¶å¤šå¤´Output: [2, 12, 4, 64] â†’ transpose â†’ [2, 4, 12, 64] â†’ reshape â†’ [2, 4, 768] å…³é”®: è¾“å…¥å’Œè¾“å‡ºç»´åº¦å®Œå…¨ç›¸åŒï¼[2, 4, 768] 2.3 ä¸ºä»€ä¹ˆéœ€è¦å¤šå¤´æ³¨æ„åŠ›ï¼Ÿå•å¤´çš„å±€é™å•å¤´: åªæœ‰ä¸€ç»„ Q/K/V åªèƒ½å­¦ä¹ ä¸€ç§\"æŸ¥è¯¢-åŒ¹é…\"æ¨¡å¼ å¤šå¤´çš„ä¼˜åŠ¿12 ä¸ªå¤´ = 12 ç§ä¸åŒçš„æ³¨æ„åŠ›æ¨¡å¼Head 1: å…³æ³¨è¯­æ³•å…³ç³» (ä¸»è°“å®¾)Head 2: å…³æ³¨è¯­ä¹‰ç›¸ä¼¼Head 3: å…³æ³¨ä½ç½®é‚»è¿‘...Head 12: å…³æ³¨é•¿è·ç¦»ä¾èµ–æœ€ç»ˆèåˆ 12 ä¸ªå¤´çš„ä¿¡æ¯ â†’ æ›´ä¸°å¯Œçš„è¡¨ç¤º âš¡ Part 4: KV Cache æ·±åº¦è§£æ4.1 BERT æœ‰ Q/K/V å—ï¼Ÿæœ‰ KV Cache å—ï¼Ÿ é—®é¢˜ ç­”æ¡ˆ åŸå›  BERT æœ‰ Qã€Kã€V å—ï¼Ÿ âœ… æœ‰ æ¯å±‚ Self-Attention éƒ½è¦è®¡ç®— BERT æœ‰ KV Cache å—ï¼Ÿ âŒ æ²¡æœ‰ ä¸€æ¬¡æ€§å¤„ç†ï¼Œä¸éœ€è¦ç¼“å­˜ GPT æœ‰ KV Cache å—ï¼Ÿ âœ… å¿…é¡»æœ‰ é€ä¸ªç”Ÿæˆï¼Œå¿…é¡»ç¼“å­˜å†å² 4.2 BERT çš„ Q/K/V è®¡ç®—# BERT æ¯ä¸€å±‚éƒ½è®¡ç®— Q/K/V# è¾“å…¥: \"æˆ‘ çˆ± [MASK] å¤©å®‰é—¨\"# X = [5, 768] (5ä¸ªtokenï¼Œæ¯ä¸ª768ç»´)# æ¯å±‚éƒ½è¦è®¡ç®— Qã€Kã€VQ = X @ W_Q # [5, 768] @ [768, 768] = [5, 768]K = X @ W_K # [5, 768] @ [768, 768] = [5, 768]V = X @ W_V # [5, 768] @ [768, 768] = [5, 768]# è®¡ç®—æ³¨æ„åŠ›scores = Q @ K.T # [5, 768] @ [768, 5] = [5, 5]weights = softmax(scores) # [5, 5]output = weights @ V # [5, 5] @ [5, 768] = [5, 768] æ³¨æ„åŠ›çŸ©é˜µç¤ºä¾‹ï¼š æˆ‘ çˆ± [MASK] å¤©å®‰é—¨æˆ‘ [0.3 0.2 0.2 0.3 ]çˆ± [0.2 0.3 0.2 0.3 ] â† æ¯ä¸ªè¯çœ‹æ‰€æœ‰è¯[MASK] [0.15 0.25 0.05 0.55] â† åŒå‘æ³¨æ„åŠ›ï¼å¤©å®‰é—¨ [0.2 0.2 0.2 0.4 ] 4.3 ä¸ºä»€ä¹ˆ BERT ä¸éœ€è¦ KV Cacheï¼Ÿå…³é”®åŒºåˆ«ï¼šä¸€æ¬¡æ€§å¤„ç† vs é€ä¸ªç”Ÿæˆâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ BERTï¼ˆEncoder - ç†è§£ä»»åŠ¡ï¼‰ï¼šä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªå¥å­ â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ â”‚â”‚ è¾“å…¥: \"æˆ‘ çˆ± [MASK] å¤©å®‰é—¨\" ï¼ˆä¸€æ¬¡æ€§å…¨éƒ¨è¾“å…¥ï¼‰ â”‚â”‚ â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“ â”‚â”‚ åŒæ—¶è®¡ç®—æ‰€æœ‰ä½ç½®çš„ Qã€Kã€V â”‚â”‚ â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“â†“ â”‚â”‚ è¾“å‡º: åŒæ—¶å¾—åˆ°æ‰€æœ‰ä½ç½®çš„è¡¨ç¤º â”‚â”‚ â”‚â”‚ âœ… åªéœ€è¦ 1 æ¬¡å‰å‘ä¼ æ’­ â”‚â”‚ âŒ ä¸éœ€è¦ç¼“å­˜ä»»ä½•ä¸œè¥¿ï¼ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ GPTï¼ˆDecoder - ç”Ÿæˆä»»åŠ¡ï¼‰ï¼šé€ä¸ª token ç”Ÿæˆ â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ â”‚â”‚ Step 1: è¾“å…¥ \"æˆ‘\" â”‚â”‚ è®¡ç®— Kâ‚, Vâ‚ â†’ ğŸ’¾ éœ€è¦ä¿å­˜ï¼ â”‚â”‚ è¾“å‡º \"çˆ±\" â”‚â”‚ â”‚â”‚ Step 2: è¾“å…¥ \"çˆ±\" â”‚â”‚ è®¡ç®— Kâ‚‚, Vâ‚‚ â†’ ğŸ’¾ éœ€è¦ä¿å­˜ï¼ â”‚â”‚ éœ€è¦ç”¨åˆ°ä¹‹å‰çš„ Kâ‚, Vâ‚ â”‚â”‚ è¾“å‡º \"åŒ—äº¬\" â”‚â”‚ â”‚â”‚ Step 3: è¾“å…¥ \"åŒ—äº¬\" â”‚â”‚ è®¡ç®— Kâ‚ƒ, Vâ‚ƒ â†’ ğŸ’¾ éœ€è¦ä¿å­˜ï¼ â”‚â”‚ éœ€è¦ç”¨åˆ°ä¹‹å‰çš„ Kâ‚, Kâ‚‚, Vâ‚, Vâ‚‚ â”‚â”‚ è¾“å‡º \"å¤©å®‰é—¨\" â”‚â”‚ â”‚â”‚ âœ… æ¯ä¸€æ­¥éƒ½éœ€è¦ä¹‹å‰æ‰€æœ‰çš„ Kã€V â†’ å¿…é¡»ç¼“å­˜ï¼ â”‚â”‚ è¿™å°±æ˜¯ KV Cache çš„ä½œç”¨ï¼ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 4.4 ä»£ç å¯¹æ¯”ï¼šBERT vs GPTBERTï¼šæ— éœ€ç¼“å­˜# BERT: ä¸€æ¬¡æå®šï¼Œä¸éœ€è¦ç¼“å­˜def bert_forward(input_ids): # input_ids = [101, 2769, 4263, 103, 1921, 102] ä¸€æ¬¡æ€§è¾“å…¥ X = embedding(input_ids) # [6, 768] for layer in encoder_layers: Q = X @ W_Q # åŒæ—¶ç®—æ‰€æœ‰ä½ç½® K = X @ W_K # åŒæ—¶ç®—æ‰€æœ‰ä½ç½® V = X @ W_V # åŒæ—¶ç®—æ‰€æœ‰ä½ç½® scores = Q @ K.T / sqrt(d_k) weights = softmax(scores) X = weights @ V # ä¸€æ¬¡å®Œæˆ return X # ç›´æ¥è¿”å›ï¼Œä¸éœ€è¦ä¿å­˜ä»»ä½•ä¸­é—´ç»“æœ GPTï¼šå¿…é¡»ç¼“å­˜# GPT: é€ä¸ªç”Ÿæˆï¼Œå¿…é¡»ç¼“å­˜ Kã€Vdef gpt_generate(prompt): kv_cache = [] # å¿…é¡»æœ‰è¿™ä¸ªï¼ for step in range(max_tokens): if step == 0: X = embedding(prompt) # ç¬¬ä¸€æ­¥å¤„ç†æ•´ä¸ª prompt else: X = embedding(new_token) # ä¹‹åæ¯æ­¥åªå¤„ç†æ–° token for layer_idx, layer in enumerate(decoder_layers): Q = X @ W_Q # åªç®—å½“å‰ token çš„ Q K_new = X @ W_K V_new = X @ W_V # æ‹¼æ¥å†å²çš„ Kã€V if step 0: K = concat(kv_cache[layer_idx][0], K_new) # æ‹¼æ¥ï¼ V = concat(kv_cache[layer_idx][1], V_new) # æ‹¼æ¥ï¼ else: K, V = K_new, V_new # æ›´æ–°ç¼“å­˜ if layer_idx = len(kv_cache): kv_cache.append((K, V)) else: kv_cache[layer_idx] = (K, V) # ä¿å­˜ï¼ scores = Q @ K.T / sqrt(d_k) # Causal Mask: åªèƒ½çœ‹å·¦è¾¹ mask = causal_mask(Q.size(1), K.size(1)) scores = scores.masked_fill(mask, -inf) weights = softmax(scores) X = weights @ V new_token = predict_next(X) return generated_text 4.5 ä¸ºä»€ä¹ˆ GPT å¿…é¡»ç¼“å­˜ Kã€Vï¼Ÿç”Ÿæˆ \"æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨\" çš„è¿‡ç¨‹ï¼šStep 1: è¾“å…¥ \"æˆ‘\" Qâ‚ @ Kâ‚.T â†’ åªæœ‰è‡ªå·±çœ‹è‡ªå·± è¾“å‡º: \"çˆ±\" ğŸ’¾ ç¼“å­˜: Kâ‚, Vâ‚Step 2: è¾“å…¥ \"çˆ±\" éœ€è¦è®¡ç®—: Qâ‚‚ @ [Kâ‚, Kâ‚‚].T â† éœ€è¦ä¹‹å‰çš„ Kâ‚ï¼ å¦‚æœä¸ç¼“å­˜ Kâ‚ï¼Œå°±è¦é‡æ–°ç®— â†’ æµªè´¹ï¼ è¾“å‡º: \"åŒ—äº¬\" ğŸ’¾ ç¼“å­˜: Kâ‚, Kâ‚‚, Vâ‚, Vâ‚‚ Step 3: è¾“å…¥ \"åŒ—äº¬\" éœ€è¦è®¡ç®—: Qâ‚ƒ @ [Kâ‚, Kâ‚‚, Kâ‚ƒ].T â† éœ€è¦ Kâ‚, Kâ‚‚ï¼ å¦‚æœä¸ç¼“å­˜ï¼Œè¦é‡æ–°ç®— Kâ‚, Kâ‚‚ â†’ æ›´æµªè´¹ï¼ è¾“å‡º: \"å¤©å®‰é—¨\" ğŸ’¾ ç¼“å­˜: Kâ‚, Kâ‚‚, Kâ‚ƒ, Vâ‚, Vâ‚‚, Vâ‚ƒStep N: éœ€è¦è®¡ç®—: Qâ‚™ @ [Kâ‚, Kâ‚‚, ..., Kâ‚™].T ä¸ç¼“å­˜: è®¡ç®—é‡ = 1+2+3+...+N = O(NÂ²)ç¼“å­˜: è®¡ç®—é‡ = 1+1+1+...+1 = O(N)åŠ é€Ÿæ¯”: N å€ï¼ ä¸ºä»€ä¹ˆè¦åŠ åˆ°å½“å‰è¯ä¸Šï¼Ÿ # Step 2 ç”Ÿæˆ \"çˆ±\" æ—¶Q_çˆ± @ [K_æˆ‘, K_çˆ±].T # \"çˆ±\" éœ€è¦çœ‹åˆ° \"æˆ‘\" â†‘ å¿…é¡»åŒ…å«ä¹‹å‰çš„ K_æˆ‘ï¼# Step 3 ç”Ÿæˆ \"åŒ—äº¬\" æ—¶ Q_åŒ—äº¬ @ [K_æˆ‘, K_çˆ±, K_åŒ—äº¬].T # \"åŒ—äº¬\" éœ€è¦çœ‹åˆ° \"æˆ‘\" å’Œ \"çˆ±\" â†‘ å¿…é¡»åŒ…å«æ‰€æœ‰å†å² Kï¼# è¿™å°±æ˜¯ä¸ºä»€ä¹ˆè¦æŠŠå‰é¢çš„ Kã€V åŠ åˆ°å½“å‰çš„è®¡ç®—ä¸­ 4.6 æ¶æ„æ€»ç»“å¯¹æ¯”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ BERT vs GPT â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ BERT â”‚ GPT â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ Encoder-only æ¶æ„ â”‚ Decoder-only æ¶æ„ â”‚â”‚ åŒå‘æ³¨æ„åŠ› â”‚ å•å‘æ³¨æ„åŠ›ï¼ˆCausal Maskï¼‰ â”‚â”‚ ä¸€æ¬¡æ€§è¾“å…¥æ•´ä¸ªå¥å­ â”‚ é€ä¸ª token ç”Ÿæˆ â”‚â”‚ è¾“å…¥è¾“å‡ºé•¿åº¦ç›¸åŒ â”‚ è¾“å‡ºæ¯”è¾“å…¥é•¿ â”‚â”‚ å¹¶è¡Œè®¡ç®—æ‰€æœ‰ä½ç½® â”‚ ä¸²è¡Œç”Ÿæˆ â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ âœ… æœ‰ Qã€Kã€V â”‚ âœ… æœ‰ Qã€Kã€V â”‚â”‚ âŒ ä¸éœ€è¦ KV Cache â”‚ âœ… å¿…é¡» KV Cache â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ ç”¨é€”: ç†è§£ â”‚ ç”¨é€”: ç”Ÿæˆ â”‚â”‚ ä»»åŠ¡: åˆ†ç±»ã€NERã€é—®ç­” â”‚ ä»»åŠ¡: èŠå¤©ã€å†™ä½œã€ä»£ç ç”Ÿæˆ â”‚â”‚ ä¾‹å­: \"è¿™æ˜¯___è¯„è®º\"â†’æ­£é¢ â”‚ ä¾‹å­: \"ä»å‰æœ‰\" â†’ \"åº§å±±\" â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 4.7 æ··åˆæ¶æ„ï¼šT5/BARTT5/BART = Encoder + Decoderâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ Encoder éƒ¨åˆ†ï¼ˆç±»ä¼¼ BERTï¼‰ â”‚â”‚ è¾“å…¥: \"Translate to English: æˆ‘çˆ±ä½ \" â”‚â”‚ å¤„ç†: ä¸€æ¬¡æ€§ç¼–ç  â”‚â”‚ âŒ ä¸éœ€è¦ KV Cache â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â†“ ä¼ é€’ç¼–ç ç»“æœâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ Decoder éƒ¨åˆ†ï¼ˆç±»ä¼¼ GPTï¼‰ â”‚â”‚ ç”Ÿæˆ: \"I\" â†’ \"love\" â†’ \"you\" â”‚â”‚ å¤„ç†: é€ä¸ªç”Ÿæˆ â”‚â”‚ âœ… éœ€è¦ KV Cacheï¼ˆä»… Decoder éƒ¨åˆ†ï¼‰ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 4.8 è®°å¿†å£è¯€Encoder (BERT) = é˜…è¯»ç†è§£ = ä¸€çœ¼çœ‹å®Œå…¨æ–‡ = ä¸éœ€è¦ç¼“å­˜Decoder (GPT) = å†™ä½œæ–‡ = ä¸€å­—ä¸€å­—å†™ = éœ€è¦ç¼“å­˜å†å²åˆ¤æ–­æ ‡å‡†: - è¾“å…¥è¾“å‡ºåŒæ—¶å­˜åœ¨ï¼Ÿ â†’ Encoder â†’ æ—  KV Cache - é€æ­¥ç”Ÿæˆæ–°å†…å®¹ï¼Ÿ â†’ Decoder â†’ æœ‰ KV Cache âš¡ Part 5: KV Cache æ·±åº¦è§£æ3.1 é—®é¢˜åœºæ™¯ï¼šè‡ªå›å½’ç”ŸæˆGPT ç”Ÿæˆæ–‡æœ¬ â€œI love NLP so muchâ€ï¼š æ—  KV Cache (ä½æ•ˆ): Step 1: è¾“å…¥ \"I\" è®¡ç®— Qâ‚, Kâ‚, Vâ‚ â†’ è¾“å‡º \"love\" Step 2: è¾“å…¥ \"I love\" é‡æ–°è®¡ç®— Qâ‚, Kâ‚, Vâ‚ â† æµªè´¹ï¼ é‡æ–°è®¡ç®— Qâ‚‚, Kâ‚‚, Vâ‚‚ â† æµªè´¹ï¼ â†’ è¾“å‡º \"NLP\" Step 3: è¾“å…¥ \"I love NLP\" é‡æ–°è®¡ç®— Qâ‚, Kâ‚, Vâ‚ â† æµªè´¹ï¼ é‡æ–°è®¡ç®— Qâ‚‚, Kâ‚‚, Vâ‚‚ â† æµªè´¹ï¼ é‡æ–°è®¡ç®— Qâ‚ƒ, Kâ‚ƒ, Vâ‚ƒ â† æµªè´¹ï¼ â†’ è¾“å‡º \"so\" è®¡ç®—é‡: 1 + 2 + 3 + â€¦ + n = O(nÂ²) 3.2 KV Cache è§£å†³æ–¹æ¡ˆæ ¸å¿ƒæ´å¯Ÿ: K å’Œ V åªä¾èµ–è¾“å…¥ï¼Œä¸â€å½“å‰è¦ç”Ÿæˆä»€ä¹ˆâ€æ— å…³ â†’ å¯ä»¥ç¼“å­˜ï¼ Step 1: è¾“å…¥ \"I\" è®¡ç®— Kâ‚, Vâ‚ â†’ å­˜å…¥ Cache è®¡ç®— Qâ‚ â†’ Attention(Qâ‚, [Kâ‚], [Vâ‚]) â†’ è¾“å‡º \"love\" Step 2: è¾“å…¥ \"love\" è®¡ç®— Kâ‚‚, Vâ‚‚ â†’ è¿½åŠ åˆ° Cache åªè®¡ç®— Qâ‚‚ â†’ Attention(Qâ‚‚, [Kâ‚,Kâ‚‚], [Vâ‚,Vâ‚‚]) â†’ è¾“å‡º \"NLP\" Step 3: è¾“å…¥ \"NLP\" è®¡ç®— Kâ‚ƒ, Vâ‚ƒ â†’ è¿½åŠ åˆ° Cache åªè®¡ç®— Qâ‚ƒ â†’ Attention(Qâ‚ƒ, [Kâ‚,Kâ‚‚,Kâ‚ƒ], [Vâ‚,Vâ‚‚,Vâ‚ƒ]) â†’ è¾“å‡º \"so\" è®¡ç®—é‡: 1 + 1 + 1 + â€¦ + 1 = O(n) åŠ é€Ÿæ¯”: nÂ²/n = n å€åŠ é€Ÿï¼ 3.3 å†…å­˜æ¶ˆè€—åˆ†æå…¬å¼$$\\text{KV Cache Size} = 2 \\times n_{\\text{layers}} \\times d_{\\text{model}} \\times \\text{seq_len} \\times \\text{dtype_size}$$ å®é™…æ¡ˆä¾‹ï¼šLLaMA-7B n_layers = 32 d_model = 4096 dtype = float16 (2 bytes) æ¯ä¸ª token çš„ KV Cache:= 2 Ã— 32 Ã— 4096 Ã— 2 bytes= 524,288 bytes= 512 KB / tokenä¸åŒåºåˆ—é•¿åº¦çš„æ˜¾å­˜å ç”¨:- 1K tokens: 512 MB- 4K tokens: 2 GB- 32K tokens: 16 GB â† é•¿ä¸Šä¸‹æ–‡çš„æŒ‘æˆ˜ï¼- 128K tokens: 64 GB â† éœ€è¦å¤šå¡æˆ–ä¼˜åŒ–æŠ€æœ¯ 3.4 KV Cache ä¼˜åŒ–æŠ€æœ¯1. Multi-Query Attention (MQA)æ ‡å‡† Multi-Head: æ¯ä¸ªå¤´éƒ½æœ‰ç‹¬ç«‹çš„ K, VMQA: æ‰€æœ‰å¤´å…±äº«åŒä¸€ç»„ K, Væ˜¾å­˜èŠ‚çœ: heads å€ (ä¾‹å¦‚ 12 å¤´ â†’ èŠ‚çœ 12 å€) 2. Grouped-Query Attention (GQA)æŠ˜ä¸­æ–¹æ¡ˆ: å°† 12 ä¸ªå¤´åˆ†æˆ 3 ç»„ï¼Œæ¯ç»„å…±äº« K, VLLaMA-2 ä½¿ç”¨ GQA:- 32 ä¸ªå¤´ â†’ 8 ç»„- æ˜¾å­˜èŠ‚çœ: 4 å€ 3. PagedAttention (vLLM)ç±»ä¼¼æ“ä½œç³»ç»Ÿçš„åˆ†é¡µæœºåˆ¶å°† KV Cache åˆ†æˆå›ºå®šå¤§å°çš„å— (Page)åŠ¨æ€åˆ†é…å’Œå›æ”¶ï¼Œå‡å°‘ç¢ç‰‡ ğŸ­ Part 4: Causal Attention vs Bidirectional Attention4.1 æ ¸å¿ƒåŒºåˆ« ç»´åº¦ Bidirectional (BERT) Causal (GPT) å¯è§èŒƒå›´ å…¨å±€å¯è§ ä»…å·¦ä¾§å¯è§ Mask å½¢çŠ¶ å…¨ 1 (æˆ–ä»… mask padding) ä¸‹ä¸‰è§’çŸ©é˜µ é€‚ç”¨ä»»åŠ¡ ç†è§£ (åˆ†ç±»ã€NERã€QA) ç”Ÿæˆ (æ–‡æœ¬ã€å¯¹è¯) è®­ç»ƒæ•ˆç‡ é«˜ (å¹¶è¡Œè®¡ç®—æ‰€æœ‰ä½ç½®) é«˜ (teacher forcing) æ¨ç†æ¨¡å¼ ä¸€æ¬¡æ€§è¾“å‡º é€ token ç”Ÿæˆ 4.2 Causal Mask çš„æ•°å­¦å®ç°Mask çŸ©é˜µseq_len = 4mask = np.tril(np.ones((4, 4))) I love NLP ! I [1 0 0 0] love [1 1 0 0] NLP [1 1 1 0] ! [1 1 1 1]è§£é‡Š:- \"I\" åªèƒ½çœ‹åˆ°è‡ªå·±- \"love\" èƒ½çœ‹åˆ° \"I\" å’Œè‡ªå·±- \"NLP\" èƒ½çœ‹åˆ° \"I\", \"love\", \"NLP\"- \"!\" èƒ½çœ‹åˆ°æ‰€æœ‰ åº”ç”¨åˆ° Attention Scoresscores = Q @ K.T / sqrt(d_k) # [4, 4]# å°†ä¸Šä¸‰è§’è®¾ä¸º -infmasked_scores = np.where(mask == 1, scores, -np.inf) I love NLP ! I [2.1 -inf -inf -inf] love [1.3 3.2 -inf -inf] NLP [0.8 1.9 2.7 -inf] ! [0.5 1.1 0.9 3.5]# Softmax å, -inf ä½ç½®å˜æˆ 0weights = softmax(masked_scores) I love NLP ! I [1.0 0.0 0.0 0.0] â† åªçœ‹è‡ªå·± love [0.3 0.7 0.0 0.0] â† 70% çœ‹è‡ªå·±, 30% çœ‹ I NLP [0.1 0.3 0.6 0.0] â† ä¸»è¦çœ‹è‡ªå·±å’Œ love ! [0.05 0.2 0.15 0.6] â† 60% çœ‹è‡ªå·± ğŸ”„ Part 5: è®­ç»ƒé—­ç¯ - Label / Loss / æ¢¯åº¦æµ5.1 MLM ä»»åŠ¡çš„å®Œæ•´è®­ç»ƒæµç¨‹æ•°æ®å‡†å¤‡åŸå§‹æ–‡æœ¬: \"æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨\"å¤„ç†å: \"æˆ‘çˆ±[MASK]å¤©å®‰é—¨\"input_ids = [101, 2769, 4263, 103, 1921, 2128, 7305, 102] [CLS] æˆ‘ çˆ± MASK å¤© å®‰ é—¨ [SEP]labels = [-100, -100, -100, 1266, -100, -100, -100, -100] â†‘ \"åŒ—äº¬\" çš„ ID Forward Pass# 1. Embeddingembeddings = token_emb + pos_emb + seg_emb # [1, 8, 768]# 2. 12 å±‚ Transformerhidden = embeddingsfor layer in encoder_layers: hidden = layer(hidden) # [1, 8, 768]# 3. MLM Headlogits = mlm_head(hidden) # [1, 8, 21128]# åªå– [MASK] ä½ç½® (index=3)masked_logits = logits[0, 3, :] # [21128] Loss è®¡ç®—true_label = 1266 # \"åŒ—äº¬\"loss = CrossEntropyLoss(masked_logits, true_label) = -log(softmax(masked_logits)[1266]) å¦‚æœæ¨¡å‹é¢„æµ‹: P(\"åŒ—äº¬\") = 0.8 â†’ loss = -log(0.8) = 0.22 (å¥½) P(\"åŒ—äº¬\") = 0.1 â†’ loss = -log(0.1) = 2.30 (å·®) 5.2 æ¢¯åº¦åå‘ä¼ æ’­æ¢¯åº¦æµåŠ¨è·¯å¾„ Loss (æ ‡é‡) â†“ âˆ‚L/âˆ‚logits MLM Head (çº¿æ€§å±‚) â†“ âˆ‚L/âˆ‚hâ‚â‚‚Transformer Layer 12 â†“ ... â†“Transformer Layer 1 â†™ â†“ â†˜âˆ‚L/âˆ‚Q âˆ‚L/âˆ‚K âˆ‚L/âˆ‚V â†“ â†“ â†“W_Q W_K W_V â† è¿™äº›æƒé‡è¢«æ›´æ–°ï¼ Attention ä¸­çš„æ¢¯åº¦åˆ†å‰# Forwardattn_weights = softmax(Q @ K.T / sqrt(d_k)) # [seq, seq]output = attn_weights @ V # [seq, dim]# Backwardâˆ‚L/âˆ‚V = attn_weights.T @ âˆ‚L/âˆ‚output â† V çš„æ¢¯åº¦âˆ‚L/âˆ‚attn = âˆ‚L/âˆ‚output @ V.T â† æ³¨æ„åŠ›æƒé‡çš„æ¢¯åº¦âˆ‚L/âˆ‚scores = âˆ‚softmax(âˆ‚L/âˆ‚attn) â† Softmax åå‘âˆ‚L/âˆ‚Q = âˆ‚L/âˆ‚scores @ K â† Q çš„æ¢¯åº¦âˆ‚L/âˆ‚K = âˆ‚L/âˆ‚scores.T @ Q â† K çš„æ¢¯åº¦ 5.3 ä¸ºä»€ä¹ˆ Attention èƒ½å­¦åˆ°è¯­ä¹‰ï¼Ÿæ¢¯åº¦çš„â€æŒ‡å¯¼ä½œç”¨â€å‡è®¾å½“å‰é¢„æµ‹: [MASK] ä½ç½®é¢„æµ‹ \"ä¸Šæµ·\" (é”™è¯¯, åº”è¯¥æ˜¯ \"åŒ—äº¬\") Loss å¾ˆå¤§ â†’ æ¢¯åº¦å›ä¼ : 1. æµå‘ V: \"ä½ ä»¬æä¾›çš„å†…å®¹ä¸å¯¹ï¼'ä¸Šæµ·' çš„è¯­ä¹‰ç‰¹å¾å¤ªå¼ºäº†\" â†’ è°ƒæ•´ Vï¼Œè®© \"åŒ—äº¬\" ç›¸å…³çš„ token æä¾›æ›´å¤šä¿¡æ¯ 2. æµå‘ Q å’Œ K: \"'çˆ±' å’Œ 'å¤©å®‰é—¨' çš„æ³¨æ„åŠ›æƒé‡ä¸å¯¹ï¼\" â†’ è°ƒæ•´ Q/Kï¼Œè®© [MASK] æ›´å¤šå…³æ³¨ \"å¤©å®‰é—¨\" â†’ å› ä¸º \"åŒ—äº¬ + å¤©å®‰é—¨\" å…±ç°é¢‘ç‡é«˜ 3. å¤šè½®è¿­ä»£å: æ¨¡å‹å­¦ä¼š: \"çœ‹åˆ°'å¤©å®‰é—¨' â†’ è”æƒ³åˆ°'åŒ—äº¬'\" ğŸ“š æ€»ç»“ï¼šæ ¸å¿ƒè¦ç‚¹å›é¡¾BERT è®ºæ–‡æ ¸å¿ƒè´¡çŒ® Masked Language Model: å®ç°æ·±åº¦åŒå‘å»ºæ¨¡ å¤§è§„æ¨¡é¢„è®­ç»ƒ + å¾®è°ƒ: å¼€åˆ›é¢„è®­ç»ƒèŒƒå¼ SOTA æ€§èƒ½: åœ¨ 11 ä¸ªä»»åŠ¡ä¸Šåˆ·æ–°è®°å½• Q/K/V æœºåˆ¶æœ¬è´¨ Q: æé—® â€œæˆ‘æƒ³æ‰¾ä»€ä¹ˆä¿¡æ¯ï¼Ÿâ€ K: ç´¢å¼• â€œæˆ‘è¿™é‡Œæœ‰ä»€ä¹ˆä¿¡æ¯ï¼Ÿâ€ V: å†…å®¹ â€œå®é™…çš„ä¿¡æ¯æ˜¯ä»€ä¹ˆï¼Ÿâ€ Attention: æ ¹æ® Q-K ç›¸ä¼¼åº¦ï¼ŒåŠ æƒèšåˆ V KV Cache ä¼˜åŒ– é—®é¢˜: è‡ªå›å½’ç”Ÿæˆæ—¶é‡å¤è®¡ç®— K/V â†’ O(nÂ²) æ–¹æ¡ˆ: ç¼“å­˜å†å² K/V â†’ O(n) ä»£ä»·: æ˜¾å­˜å ç”¨ (LLaMA-7B: 512KB/token) Causal vs Bidirectional Causal: ä¸‹ä¸‰è§’ mask, ç”¨äºç”Ÿæˆ Bidirectional: å…¨å±€å¯è§, ç”¨äºç†è§£ è®­ç»ƒé—­ç¯ Label â†’ Loss â†’ Gradient â†’ Update æ¢¯åº¦æµè¿‡ Attention æ—¶åˆ†å‰åˆ° Q/K/V é€šè¿‡åå‘ä¼ æ’­ï¼Œæ¨¡å‹å­¦ä¼šâ€åœ¨å“ªé‡Œæ‰¾ä¿¡æ¯â€å’Œâ€æ‰¾ä»€ä¹ˆä¿¡æ¯â€ ğŸ†š Part 8: Encoder vs Decoder - KV Cache å¯¹æ¯”æ ¸å¿ƒç»“è®º æ¨¡å‹ æ¶æ„ æœ‰ KV Cacheï¼Ÿ åŸå›  BERT Encoder-only âŒ æ²¡æœ‰ ä¸€æ¬¡æ€§å¤„ç†æ•´ä¸ªè¾“å…¥ GPT Decoder-only âœ… æœ‰ é€ä¸ª token ç”Ÿæˆ T5/BART Encoder + Decoder âš ï¸ Decoder éƒ¨åˆ†æœ‰ Encoder ä¸éœ€è¦ï¼ŒDecoder éœ€è¦ BERT (Encoder-only) - ä¸éœ€è¦ KV Cache# è¾“å…¥å®Œæ•´å¥å­input = \"æˆ‘ çˆ± [MASK] å¤©å®‰é—¨\"# ä¸€æ¬¡æ€§å…¨éƒ¨å¤„ç†output = bert(input) # åŒæ—¶è®¡ç®—æ‰€æœ‰ä½ç½®çš„ Qã€Kã€V# æµç¨‹Step 1: è¾“å…¥å…¨éƒ¨ token [6ä¸ª]Step 2: åŒæ—¶è®¡ç®—æ‰€æœ‰ä½ç½®çš„ Kã€VStep 3: åŒæ—¶è®¡ç®—æ‰€æœ‰ä½ç½®çš„ AttentionStep 4: åŒæ—¶è¾“å‡ºæ‰€æœ‰ä½ç½®çš„ç»“æœ# è®¡ç®—é‡: O(n) - åªè®¡ç®—ä¸€æ¬¡# ä¸éœ€è¦ KV Cacheï¼ GPT (Decoder-only) - å¿…é¡»æœ‰ KV Cache# é€ä¸ªç”ŸæˆStep 1: input=\"æˆ‘\" è®¡ç®— Kâ‚, Vâ‚ â†’ ç¼“å­˜ ç”Ÿæˆ \"çˆ±\"Step 2: input=\"çˆ±\" è®¡ç®— Kâ‚‚, Vâ‚‚ â†’ ç¼“å­˜ ä½¿ç”¨ Kâ‚, Vâ‚ + Kâ‚‚, Vâ‚‚ ç”Ÿæˆ \"åŒ—äº¬\"Step 3: input=\"åŒ—äº¬\" è®¡ç®— Kâ‚ƒ, Vâ‚ƒ â†’ ç¼“å­˜ ä½¿ç”¨ Kâ‚, Vâ‚ + Kâ‚‚, Vâ‚‚ + Kâ‚ƒ, Vâ‚ƒ ç”Ÿæˆ \"å¤©å®‰é—¨\"# ä¸ç”¨ç¼“å­˜: è®¡ç®—é‡ = 1+2+3+...+N = O(NÂ²)# ç”¨ç¼“å­˜: è®¡ç®—é‡ = 1+1+1+...+1 = O(N)# å¿…é¡»æœ‰ KV Cacheï¼ ä¸ºä»€ä¹ˆ Decoder å¿…é¡»ç¼“å­˜ Kã€Vï¼Ÿç”Ÿæˆ \"æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨\" çš„è¿‡ç¨‹ï¼šStep 1: è¾“å…¥ \"æˆ‘\" Qâ‚ @ Kâ‚.T â†’ åªæœ‰è‡ªå·±çœ‹è‡ªå·± è¾“å‡º: \"çˆ±\"Step 2: è¾“å…¥ \"çˆ±\" éœ€è¦è®¡ç®—: Qâ‚‚ @ [Kâ‚, Kâ‚‚].T â† éœ€è¦ä¹‹å‰çš„ Kâ‚ï¼ å¦‚æœä¸ç¼“å­˜ï¼Œå°±è¦é‡æ–°è®¡ç®— Kâ‚ â†’ æµªè´¹ï¼ Step 3: è¾“å…¥ \"åŒ—äº¬\" éœ€è¦è®¡ç®—: Qâ‚ƒ @ [Kâ‚, Kâ‚‚, Kâ‚ƒ].T â† éœ€è¦ Kâ‚, Kâ‚‚ï¼ å¦‚æœä¸ç¼“å­˜ï¼Œè¦é‡æ–°è®¡ç®— Kâ‚, Kâ‚‚ â†’ æ›´æµªè´¹ï¼Step N: éœ€è¦è®¡ç®—: Qâ‚™ @ [Kâ‚, Kâ‚‚, ..., Kâ‚™].T ä¸ç¼“å­˜çš„è¯ï¼Œè®¡ç®—é‡ = 1+2+3+...+N = O(NÂ²) ç¼“å­˜çš„è¯ï¼Œæ¯æ­¥åªç®—æ–°çš„ï¼Œè®¡ç®—é‡ = O(N) BERT æœ‰ Q/K/Vï¼Œä½†ä¸ºä»€ä¹ˆä¸éœ€è¦ç¼“å­˜ï¼Ÿ# BERT: ä¸€æ¬¡æå®šï¼Œä¸éœ€è¦ç¼“å­˜def bert_forward(input_ids): # input_ids = [101, 2769, 4263, 103, 1921, 102] ä¸€æ¬¡æ€§è¾“å…¥ X = embedding(input_ids) # [6, 768] for layer in encoder_layers: Q = X @ W_Q # åŒæ—¶ç®—æ‰€æœ‰ä½ç½®çš„ Q K = X @ W_K # åŒæ—¶ç®—æ‰€æœ‰ä½ç½®çš„ K V = X @ W_V # åŒæ—¶ç®—æ‰€æœ‰ä½ç½®çš„ V # æ‰€æœ‰ä½ç½®çš„ Attention åŒæ—¶è®¡ç®— scores = Q @ K.T # [6, 6] weights = softmax(scores) X = weights @ V # [6, 768] return X # ç›´æ¥è¿”å›ï¼Œä¸éœ€è¦ä¿å­˜ä»»ä½•ä¸­é—´ç»“æœ # GPT: é€ä¸ªç”Ÿæˆï¼Œå¿…é¡»ç¼“å­˜ Kã€Vdef gpt_generate(prompt): kv_cache = [] # å¿…é¡»æœ‰è¿™ä¸ªï¼ for step in range(max_tokens): if step == 0: X = embedding(prompt) # ç¬¬ä¸€æ­¥å¤„ç†æ•´ä¸ª prompt else: X = embedding(new_token) # ä¹‹åæ¯æ­¥åªå¤„ç†æ–° token for layer_idx, layer in enumerate(decoder_layers): Q = X @ W_Q # åªç®—å½“å‰ token çš„ Q K_new = X @ W_K V_new = X @ W_V # æ‹¼æ¥å†å²çš„ Kã€V if kv_cache[layer_idx] is not None: K = concat(kv_cache[layer_idx][0], K_new) # æ‹¼æ¥ï¼ V = concat(kv_cache[layer_idx][1], V_new) # æ‹¼æ¥ï¼ else: K = K_new V = V_new # æ›´æ–°ç¼“å­˜ kv_cache[layer_idx] = (K, V) # ä¿å­˜ï¼ # Causal Attention (åªçœ‹å·¦è¾¹) scores = Q @ K.T # [1, seq_len] weights = softmax(scores) X = weights @ V new_token = predict_next(X) return generated_text ä¸€å›¾æ€»ç»“â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ BERT vs GPT â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ BERT â”‚ GPT â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ Encoder æ¶æ„ â”‚ Decoder æ¶æ„ â”‚â”‚ åŒå‘æ³¨æ„åŠ› â”‚ å•å‘æ³¨æ„åŠ›ï¼ˆCausalï¼‰ â”‚â”‚ ä¸€æ¬¡æ€§è¾“å…¥æ•´ä¸ªå¥å­ â”‚ é€ä¸ª token ç”Ÿæˆ â”‚â”‚ è¾“å…¥è¾“å‡ºé•¿åº¦ç›¸åŒ â”‚ è¾“å‡ºæ¯”è¾“å…¥é•¿ â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ âœ… æœ‰ Qã€Kã€V â”‚ âœ… æœ‰ Qã€Kã€V â”‚â”‚ âŒ ä¸éœ€è¦ KV Cache â”‚ âœ… å¿…é¡» KV Cache â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ ç”¨é€”: ç†è§£ â”‚ ç”¨é€”: ç”Ÿæˆ â”‚â”‚ åˆ†ç±»ã€NERã€é—®ç­” â”‚ èŠå¤©ã€å†™ä½œã€ä»£ç ç”Ÿæˆ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ è®°å¿†å£è¯€BERT = é˜…è¯»ç†è§£ = ä¸€çœ¼çœ‹å®Œå…¨æ–‡ = ä¸éœ€è¦ç¼“å­˜GPT = å†™ä½œæ–‡ = ä¸€ä¸ªå­—ä¸€ä¸ªå­—å†™ = éœ€è¦è®°ä½å‰é¢å†™äº†ä»€ä¹ˆ â“ Part 9: å¸¸è§é—®ç­” FAQQ1: [MASK] Token ä¸ºä»€ä¹ˆèƒ½é¢„æµ‹å‡ºæ­£ç¡®ç­”æ¡ˆï¼ŸA: [MASK] çš„ Token ID (103) æœ¬èº«ä¸ä»£è¡¨ä»»ä½•è¯­ä¹‰ï¼Œä½†ç»è¿‡ 12 å±‚ Self-Attention åï¼š Embedding: 103 â†’ [0.01, 0.02, ...] ç©ºå£³ â†“ Layer 1: å¸æ”¶ \"çˆ±\" å’Œ \"å¤©å®‰é—¨\" çš„ä¿¡æ¯Layer 1: [0.52, 0.71, ...] å¼€å§‹æœ‰è¯­ä¹‰ â†“ Layer 2-12: ä¸æ–­ç²¾ç‚¼Layer 12: [0.93, 0.87, ...] å®Œå…¨ç†è§£ä¸Šä¸‹æ–‡æœ€ç»ˆè¿™ä¸ªå‘é‡åœ¨è¯­ä¹‰ç©ºé—´ä¸­æ¥è¿‘ \"åŒ—äº¬\"ï¼ å…³é”®: Self-Attention è®© [MASK] ä»å‘¨å›´è¯â€å·â€ä¿¡æ¯ï¼ Q2: BERT ä¸ºä»€ä¹ˆåªç”¨ Encoderï¼Œä¸ç”¨ Decoderï¼ŸA: å› ä¸º BERT çš„ä»»åŠ¡æ˜¯â€ç†è§£â€ï¼Œä¸æ˜¯â€ç”Ÿæˆâ€ ä»»åŠ¡ç±»å‹ éœ€è¦ Decoderï¼Ÿ åŸå›  æœºå™¨ç¿»è¯‘ âœ… éœ€è¦ è¾“å…¥ä¸­æ–‡ï¼Œè¾“å‡ºè‹±æ–‡ï¼Œæ˜¯ä¸åŒçš„åºåˆ— æ–‡æœ¬åˆ†ç±» âŒ ä¸éœ€è¦ åªéœ€ç†è§£è¾“å…¥ï¼Œè¾“å‡ºä¸€ä¸ªç±»åˆ« NER âŒ ä¸éœ€è¦ åªéœ€ç»™æ¯ä¸ªè¾“å…¥è¯æ‰“æ ‡ç­¾ é—®ç­” âŒ ä¸éœ€è¦ ç­”æ¡ˆåœ¨åŸæ–‡ä¸­ï¼Œåªéœ€æ‰¾ä½ç½® ç®€å•è¯´: Encoder ç†è§£è¾“å…¥ï¼ŒDecoder ç”Ÿæˆè¾“å‡ºã€‚BERT åªéœ€è¦ç†è§£ï¼ Q3: æ®‹å·®è¿æ¥æ˜¯ä»€ä¹ˆï¼Ÿä¸ºä»€ä¹ˆéœ€è¦å®ƒï¼ŸA: æ®‹å·®è¿æ¥å°±æ˜¯æŠŠè¾“å…¥ç›´æ¥åŠ åˆ°è¾“å‡ºä¸Š # ä¸ç”¨æ®‹å·®output = FFN(input) # å¯èƒ½ä¸¢å¤±åŸå§‹ä¿¡æ¯# ç”¨æ®‹å·®output = input + FFN(input) # ä¿åº• + å¢é‡ å¥½å¤„: æ¢¯åº¦å¯ä»¥ç›´æ¥è·³è¿‡ FFN å›ä¼  â†’ è§£å†³æ¢¯åº¦æ¶ˆå¤± åŸå§‹ä¿¡æ¯ä¸ä¼šä¸¢å¤± â†’ ç½‘ç»œå¯ä»¥æ›´æ·± FFN åªéœ€å­¦ä¹ â€å·®å¼‚â€ â†’ æ›´å®¹æ˜“è®­ç»ƒ Q4: é¢„è®­ç»ƒå’Œå¾®è°ƒæœ‰ä»€ä¹ˆåŒºåˆ«ï¼ŸA: é¢„è®­ç»ƒï¼ˆPre-trainingï¼‰: - æ•°æ®: æµ·é‡æ— æ ‡æ³¨æ–‡æœ¬ (æ•°åäº¿è¯) - ä»»åŠ¡: MLM å®Œå½¢å¡«ç©º - ç›®çš„: å­¦ä¼š\"ç†è§£è¯­è¨€\" - è€—æ—¶: æ•°å‘¨ (åªåšä¸€æ¬¡) å¾®è°ƒï¼ˆFine-tuningï¼‰: - æ•°æ®: å°‘é‡æ ‡æ³¨æ•°æ® (1000æ¡) - ä»»åŠ¡: å…·ä½“ä»»åŠ¡ (æƒ…æ„Ÿåˆ†æã€NER...) - ç›®çš„: é€‚é…ç‰¹å®šä»»åŠ¡ - è€—æ—¶: å‡ å°æ—¶ (æ¯ä¸ªä»»åŠ¡éƒ½è¦) ç±»æ¯”: é¢„è®­ç»ƒ = ä¸Šå¤§å­¦ï¼Œå¾®è°ƒ = å²—ä½åŸ¹è®­ Q5: MLM ä¸ºä»€ä¹ˆè¦ 80%/10%/10% çš„ç­–ç•¥ï¼ŸA: 100% [MASK]: é¢„è®­ç»ƒå’Œå¾®è°ƒåˆ†å¸ƒä¸ä¸€è‡´ é¢„è®­ç»ƒ: \"æˆ‘çˆ±[MASK]å¤©å®‰é—¨\" å¾®è°ƒ: \"æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨\" â† æ²¡æœ‰ [MASK]ï¼ é—®é¢˜: æ¨¡å‹è¿‡åº¦ä¾èµ– [MASK] ç¬¦å·80% [MASK] + 10% éšæœº + 10% ä¸å˜: - 80% [MASK]: ä¸»è¦å­¦ä¹ ç›®æ ‡ - 10% éšæœº: è®©æ¨¡å‹å­¦ä¼šçº é”™ - 10% ä¸å˜: é€‚é…çœŸå®åˆ†å¸ƒ Q6: BERT çš„ Qã€Kã€V æ˜¯å¹²ä»€ä¹ˆçš„ï¼ŸA: Q (Query): \"æˆ‘æƒ³æ‰¾ä»€ä¹ˆä¿¡æ¯ï¼Ÿ\"K (Key): \"æˆ‘è¿™é‡Œæœ‰ä»€ä¹ˆä¿¡æ¯ï¼Ÿ\"V (Value): \"å®é™…çš„ä¿¡æ¯å†…å®¹\"Attention = æ ¹æ® Q å’Œ K çš„ç›¸ä¼¼åº¦ï¼ŒåŠ æƒæ±‚å’Œ Vä¾‹å­: [MASK] ä½ç½® Q_mask: \"æˆ‘éœ€è¦çŸ¥é“è¿™ä¸ªç©ºå¡«ä»€ä¹ˆ\" K_å¤©å®‰é—¨: \"æˆ‘æ˜¯å¤©å®‰é—¨\" ç›¸ä¼¼åº¦é«˜ â†’ [MASK] å¤šçœ‹ V_å¤©å®‰é—¨ â†’ [MASK] è·å¾—\"å¤©å®‰é—¨åœ¨åŒ—äº¬\"çš„ä¿¡æ¯ Q7: Encoder å’Œ Decoder çš„æ³¨æ„åŠ›æœ‰ä»€ä¹ˆåŒºåˆ«ï¼ŸA: ç»´åº¦ Encoder Decoder å¯è§èŒƒå›´ å…¨å±€å¯è§ï¼ˆåŒå‘ï¼‰ åªçœ‹å·¦è¾¹ï¼ˆå•å‘ï¼‰ Mask æ—  mask æˆ–ä»… padding Causal mask (ä¸‹ä¸‰è§’) ç”¨é€” ç†è§£æ•´ä¸ªå¥å­ ç”Ÿæˆä¸‹ä¸€ä¸ªè¯ KV Cache âŒ ä¸éœ€è¦ âœ… éœ€è¦ Encoder: [CLS] å¯ä»¥çœ‹åˆ° \"æˆ‘çˆ±åŒ—äº¬å¤©å®‰é—¨\" æ‰€æœ‰è¯Decoder: \"åŒ—äº¬\" åªèƒ½çœ‹åˆ° \"æˆ‘çˆ±åŒ—äº¬\"ï¼Œä¸èƒ½çœ‹åˆ° \"å¤©å®‰é—¨\" Q8: ä¸ºä»€ä¹ˆ BERT éœ€è¦ [CLS] å’Œ [SEP] Tokenï¼ŸA: [CLS] (Classification): - ä½ç½®: å¥å­å¼€å¤´ - ä½œç”¨: æ±‡èšæ•´ä¸ªå¥å­çš„è¯­ä¹‰ - ç”¨é€”: åˆ†ç±»ä»»åŠ¡å– [CLS] çš„å‘é‡[SEP] (Separator): - ä½ç½®: å¥å­ç»“å°¾ï¼Œæˆ–ä¸¤ä¸ªå¥å­ä¹‹é—´ - ä½œç”¨: åˆ†éš”ä¸åŒå¥å­ - ç”¨é€”: è®©æ¨¡å‹çŸ¥é“å¥å­è¾¹ç•Œä¾‹å­: å•å¥: [CLS] è¿™ä¸ªäº§å“å¾ˆå¥½ [SEP] å¥å¯¹: [CLS] å¤©åœ¨ä¸‹é›¨ [SEP] åœ°é¢æ˜¯æ¹¿çš„ [SEP] Q9: BERT çš„å±‚æ•°è¶Šå¤šè¶Šå¥½å—ï¼ŸA: ä¸ä¸€å®šï¼ æ¨¡å‹ å±‚æ•° å‚æ•°é‡ æ€§èƒ½ é—®é¢˜ BERT-Base 12 110M 84.6% - BERT-Large 24 340M 86.7% è®­ç»ƒæ…¢ã€éœ€è¦æ›´å¤šæ•°æ® BERT-è¶…å¤§ 48+ 1B+ æå‡æœ‰é™ è¿‡æ‹Ÿåˆã€æ¨ç†æ…¢ ç»“è®º: 12-24 å±‚æ˜¯ç”œèœœç‚¹ï¼Œæ›´å¤šå±‚è¾¹é™…æ”¶ç›Šé€’å‡ Q10: BERT å’Œ GPT å¯ä»¥ç»“åˆä½¿ç”¨å—ï¼ŸA: å¯ä»¥ï¼è¿™å°±æ˜¯ Encoder-Decoder æ¶æ„ T5 / BART: è¾“å…¥ â†’ BERT-like Encoder (ç†è§£) â†’ GPT-like Decoder (ç”Ÿæˆ) â†’ è¾“å‡ºé€‚ç”¨åœºæ™¯: - æœºå™¨ç¿»è¯‘ - æ–‡æœ¬æ‘˜è¦ - å¯¹è¯ç”Ÿæˆ ğŸ”— å»¶ä¼¸é˜…è¯» è®ºæ–‡åŸæ–‡: BERT (arxiv.org/abs/1810.04805) åç»­æ”¹è¿›: RoBERTa: ç§»é™¤ NSP, æ›´å¤§æ‰¹æ¬¡è®­ç»ƒ ALBERT: å‚æ•°å…±äº«, å‡å°‘æ¨¡å‹å¤§å° ELECTRA: åˆ¤åˆ«å¼é¢„è®­ç»ƒ, æ›´é«˜æ•ˆ KV Cache ä¼˜åŒ–: PagedAttention: vLLM çš„æ ¸å¿ƒæŠ€æœ¯ FlashAttention: IO ä¼˜åŒ–çš„æ³¨æ„åŠ›ç®—æ³• Transformer åŸæ–‡: Attention Is All You Need ğŸš€ Part 6: BERT å®é™…åº”ç”¨åœºæ™¯ä¸ä»£ç å®ç°6.1 åº”ç”¨åœºæ™¯æ€»è§ˆ åº”ç”¨åœºæ™¯ ä»»åŠ¡ç±»å‹ è¾“å…¥æ ¼å¼ è¾“å‡º å…¸å‹åº”ç”¨ æƒ…æ„Ÿåˆ†æ å•å¥åˆ†ç±» [CLS] æ–‡æœ¬ [SEP] ç±»åˆ«æ ‡ç­¾ å•†å“è¯„è®ºã€èˆ†æƒ…ç›‘æ§ æ–‡æœ¬åŒ¹é… å¥å¯¹åˆ†ç±» [CLS] å¥A [SEP] å¥B [SEP] ç›¸ä¼¼åº¦/å…³ç³» æ™ºèƒ½å®¢æœã€é—®ç­”åŒ¹é… å‘½åå®ä½“è¯†åˆ« åºåˆ—æ ‡æ³¨ [CLS] æ–‡æœ¬ [SEP] æ¯ä¸ªtokenæ ‡ç­¾ ä¿¡æ¯æŠ½å–ã€çŸ¥è¯†å›¾è°± é˜…è¯»ç†è§£ æŠ½å–å¼QA [CLS] é—®é¢˜ [SEP] æ–‡ç«  [SEP] ç­”æ¡ˆèµ·æ­¢ä½ç½® æ™ºèƒ½é—®ç­”ã€å®¢æœæœºå™¨äºº æ–‡æœ¬ç”Ÿæˆ Seq2Seq éœ€é…åˆDecoder ç”Ÿæˆæ–‡æœ¬ æ‘˜è¦ã€ç¿»è¯‘ (éœ€BART/T5) 6.2 æƒ…æ„Ÿåˆ†æå®Œæ•´å®ç°\"\"\"åœºæ™¯: ç”µå•†è¯„è®ºæƒ…æ„Ÿåˆ†æè¾“å…¥: \"è¿™ä¸ªæ‰‹æœºæ‹ç…§æ•ˆæœå¾ˆæ£’ï¼Œç”µæ± ä¹Ÿè€ç”¨\"è¾“å‡º: æ­£é¢ (0.95)\"\"\"from transformers import BertTokenizer, BertForSequenceClassificationimport torch# 1. åŠ è½½æ¨¡å‹model_name = \"bert-base-chinese\"tokenizer = BertTokenizer.from_pretrained(model_name)model = BertForSequenceClassification.from_pretrained( model_name, num_labels=2 # æ­£é¢/è´Ÿé¢)# 2. æ•°æ®å‡†å¤‡def prepare_data(texts, labels): encodings = tokenizer( texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\" ) return encodings, torch.tensor(labels)# ç¤ºä¾‹æ•°æ®train_texts = [ \"è¿™ä¸ªæ‰‹æœºæ‹ç…§æ•ˆæœå¾ˆæ£’ï¼Œç”µæ± ä¹Ÿè€ç”¨\", \"è´¨é‡å¤ªå·®äº†ï¼Œç”¨äº†ä¸€å¤©å°±åäº†\", \"ç‰©æµå¾ˆå¿«ï¼ŒåŒ…è£…å®Œå¥½ï¼Œå¥½è¯„\", \"å®¢æœæ€åº¦æ¶åŠ£ï¼Œå†ä¹Ÿä¸ä¹°äº†\"]train_labels = [1, 0, 1, 0] # 1=æ­£é¢, 0=è´Ÿé¢# 3. è®­ç»ƒå¾ªç¯from torch.optim import AdamWoptimizer = AdamW(model.parameters(), lr=2e-5)model.train()for epoch in range(3): encodings, labels = prepare_data(train_texts, train_labels) outputs = model( input_ids=encodings[\"input_ids\"], attention_mask=encodings[\"attention_mask\"], labels=labels ) loss = outputs.loss loss.backward() optimizer.step() optimizer.zero_grad() print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")# 4. æ¨ç†é¢„æµ‹model.eval()test_text = \"è¿™æ¬¾äº§å“æ€§ä»·æ¯”è¶…é«˜ï¼Œå¼ºçƒˆæ¨èï¼\"inputs = tokenizer(test_text, return_tensors=\"pt\")with torch.no_grad(): outputs = model(**inputs) probs = torch.softmax(outputs.logits, dim=-1) pred = torch.argmax(probs, dim=-1) print(f\"é¢„æµ‹: {'æ­£é¢' if pred==1 else 'è´Ÿé¢'}, ç½®ä¿¡åº¦: {probs[0][pred].item():.2%}\")# è¾“å‡º: é¢„æµ‹: æ­£é¢, ç½®ä¿¡åº¦: 94.32% 6.3 å‘½åå®ä½“è¯†åˆ« (NER) å®ç°\"\"\"åœºæ™¯: ä»æ–°é—»ä¸­æå–äººåã€åœ°åã€æœºæ„åè¾“å…¥: \"é©¬äº‘åœ¨æ­å·åˆ›åŠäº†é˜¿é‡Œå·´å·´å…¬å¸\"è¾“å‡º: [(\"é©¬äº‘\", \"PER\"), (\"æ­å·\", \"LOC\"), (\"é˜¿é‡Œå·´å·´å…¬å¸\", \"ORG\")]\"\"\"from transformers import BertTokenizerFast, BertForTokenClassificationimport torch# NER æ ‡ç­¾å®šä¹‰ (BIOæ ¼å¼)label_list = [\"O\", \"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-ORG\", \"I-ORG\"]label2id = {l: i for i, l in enumerate(label_list)}id2label = {i: l for i, l in enumerate(label_list)}# åŠ è½½æ¨¡å‹tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")model = BertForTokenClassification.from_pretrained( \"bert-base-chinese\", num_labels=len(label_list), id2label=id2label, label2id=label2id)# æ¨ç†å‡½æ•°def extract_entities(text): inputs = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True) offset_mapping = inputs.pop(\"offset_mapping\")[0] with torch.no_grad(): outputs = model(**inputs) predictions = torch.argmax(outputs.logits, dim=-1)[0] entities = [] current_entity = None for idx, (pred, offset) in enumerate(zip(predictions, offset_mapping)): if offset[0] == offset[1]: # è·³è¿‡ç‰¹æ®Štoken continue label = id2label[pred.item()] char = text[offset[0]:offset[1]] if label.startswith(\"B-\"): if current_entity: entities.append(current_entity) current_entity = {\"text\": char, \"type\": label[2:]} elif label.startswith(\"I-\") and current_entity: current_entity[\"text\"] += char else: if current_entity: entities.append(current_entity) current_entity = None if current_entity: entities.append(current_entity) return [(e[\"text\"], e[\"type\"]) for e in entities]# æµ‹è¯•text = \"é©¬äº‘åœ¨æ­å·åˆ›åŠäº†é˜¿é‡Œå·´å·´å…¬å¸\"print(extract_entities(text))# è¾“å‡º: [(\"é©¬äº‘\", \"PER\"), (\"æ­å·\", \"LOC\"), (\"é˜¿é‡Œå·´å·´å…¬å¸\", \"ORG\")] 6.4 è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…\"\"\"åœºæ™¯: æ™ºèƒ½å®¢æœFAQåŒ¹é…è¾“å…¥: ç”¨æˆ·é—®é¢˜ + FAQåº“è¾“å‡º: æœ€ç›¸ä¼¼çš„FAQåŠç­”æ¡ˆ\"\"\"from transformers import BertTokenizer, BertModelimport torchimport torch.nn.functional as Ftokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")model = BertModel.from_pretrained(\"bert-base-chinese\")def get_sentence_embedding(text): \"\"\"è·å–å¥å­çš„BERTè¡¨ç¤º (ä½¿ç”¨[CLS]å‘é‡)\"\"\" inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True) with torch.no_grad(): outputs = model(**inputs) # ä½¿ç”¨ [CLS] token çš„è¾“å‡ºä½œä¸ºå¥å­è¡¨ç¤º return outputs.last_hidden_state[:, 0, :]def compute_similarity(text1, text2): \"\"\"è®¡ç®—ä¸¤ä¸ªå¥å­çš„ä½™å¼¦ç›¸ä¼¼åº¦\"\"\" emb1 = get_sentence_embedding(text1) emb2 = get_sentence_embedding(text2) return F.cosine_similarity(emb1, emb2).item()# FAQåº“faq_database = [ {\"question\": \"å¦‚ä½•ä¿®æ”¹å¯†ç ï¼Ÿ\", \"answer\": \"è¯·è¿›å…¥è®¾ç½®-è´¦æˆ·å®‰å…¨-ä¿®æ”¹å¯†ç \"}, {\"question\": \"æ€ä¹ˆç”³è¯·é€€æ¬¾ï¼Ÿ\", \"answer\": \"åœ¨è®¢å•è¯¦æƒ…é¡µç‚¹å‡»ç”³è¯·é€€æ¬¾æŒ‰é’®\"}, {\"question\": \"é…é€éœ€è¦å¤šä¹…ï¼Ÿ\", \"answer\": \"ä¸€èˆ¬3-5ä¸ªå·¥ä½œæ—¥é€è¾¾\"}, {\"question\": \"æ”¯æŒå“ªäº›æ”¯ä»˜æ–¹å¼ï¼Ÿ\", \"answer\": \"æ”¯æŒå¾®ä¿¡ã€æ”¯ä»˜å®ã€é“¶è¡Œå¡æ”¯ä»˜\"},]def find_best_match(user_query): \"\"\"æ‰¾åˆ°æœ€åŒ¹é…çš„FAQ\"\"\" best_score = -1 best_faq = None for faq in faq_database: score = compute_similarity(user_query, faq[\"question\"]) if score best_score: best_score = score best_faq = faq return best_faq, best_score# æµ‹è¯•query = \"æˆ‘æƒ³æ”¹ä¸€ä¸‹ç™»å½•å¯†ç \"faq, score = find_best_match(query)print(f\"ç”¨æˆ·é—®é¢˜: {query}\")print(f\"åŒ¹é…FAQ: {faq['question']} (ç›¸ä¼¼åº¦: {score:.2%})\")print(f\"å›ç­”: {faq['answer']}\")# è¾“å‡º:# ç”¨æˆ·é—®é¢˜: æˆ‘æƒ³æ”¹ä¸€ä¸‹ç™»å½•å¯†ç # åŒ¹é…FAQ: å¦‚ä½•ä¿®æ”¹å¯†ç ï¼Ÿ (ç›¸ä¼¼åº¦: 89.34%)# å›ç­”: è¯·è¿›å…¥è®¾ç½®-è´¦æˆ·å®‰å…¨-ä¿®æ”¹å¯†ç  6.5 é˜…è¯»ç†è§£é—®ç­”ç³»ç»Ÿ\"\"\"åœºæ™¯: ä»æ–‡æ¡£ä¸­æ‰¾ç­”æ¡ˆè¾“å…¥: é—®é¢˜ + æ–‡ç« è¾“å‡º: ç­”æ¡ˆæ–‡æœ¬åŠä½ç½®\"\"\"from transformers import BertTokenizerFast, BertForQuestionAnsweringimport torchtokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")model = BertForQuestionAnswering.from_pretrained( \"luhua/chinese_pretrain_mrc_roberta_wwm_ext_large\" # ä¸­æ–‡QAæ¨¡å‹)def answer_question(question, context): \"\"\"ä»æ–‡ç« ä¸­æŠ½å–ç­”æ¡ˆ\"\"\" inputs = tokenizer( question, context, return_tensors=\"pt\", max_length=512, truncation=True ) with torch.no_grad(): outputs = model(**inputs) # è·å–ç­”æ¡ˆèµ·æ­¢ä½ç½® start_idx = torch.argmax(outputs.start_logits) end_idx = torch.argmax(outputs.end_logits) # è§£ç ç­”æ¡ˆ tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]) answer = tokenizer.convert_tokens_to_string(tokens[start_idx:end_idx+1]) # è®¡ç®—ç½®ä¿¡åº¦ start_prob = torch.softmax(outputs.start_logits, dim=-1)[0][start_idx].item() end_prob = torch.softmax(outputs.end_logits, dim=-1)[0][end_idx].item() confidence = (start_prob + end_prob) / 2 return answer, confidence# æµ‹è¯•context = \"\"\"é˜¿é‡Œå·´å·´é›†å›¢ç”±é©¬äº‘äº1999å¹´åœ¨ä¸­å›½æ­å·åˆ›ç«‹ã€‚å…¬å¸æœ€åˆæ˜¯ä¸€ä¸ªB2Bç½‘ä¸Šäº¤æ˜“å¸‚åœºï¼Œåæ¥å‘å±•æˆä¸ºä¸€ä¸ªå¤šå…ƒåŒ–çš„ç§‘æŠ€å…¬å¸ã€‚2014å¹´ï¼Œé˜¿é‡Œå·´å·´åœ¨çº½çº¦è¯åˆ¸äº¤æ˜“æ‰€ä¸Šå¸‚ï¼Œåˆ›é€ äº†å½“æ—¶å…¨çƒæœ€å¤§çš„IPOçºªå½•ã€‚ç›®å‰é˜¿é‡Œå·´å·´çš„æ ¸å¿ƒä¸šåŠ¡åŒ…æ‹¬ç”µå­å•†åŠ¡ã€äº‘è®¡ç®—ã€æ•°å­—åª’ä½“å’Œå¨±ä¹ã€‚\"\"\"questions = [ \"é˜¿é‡Œå·´å·´æ˜¯è°åˆ›ç«‹çš„ï¼Ÿ\", \"é˜¿é‡Œå·´å·´æ˜¯å“ªä¸€å¹´ä¸Šå¸‚çš„ï¼Ÿ\", \"é˜¿é‡Œå·´å·´çš„æ ¸å¿ƒä¸šåŠ¡æœ‰å“ªäº›ï¼Ÿ\"]for q in questions: answer, conf = answer_question(q, context) print(f\"Q: {q}\") print(f\"A: {answer} (ç½®ä¿¡åº¦: {conf:.2%}) \")# è¾“å‡º:# Q: é˜¿é‡Œå·´å·´æ˜¯è°åˆ›ç«‹çš„ï¼Ÿ# A: é©¬äº‘ (ç½®ä¿¡åº¦: 92.15%)## Q: é˜¿é‡Œå·´å·´æ˜¯å“ªä¸€å¹´ä¸Šå¸‚çš„ï¼Ÿ# A: 2014å¹´ (ç½®ä¿¡åº¦: 88.73%)## Q: é˜¿é‡Œå·´å·´çš„æ ¸å¿ƒä¸šåŠ¡æœ‰å“ªäº›ï¼Ÿ# A: ç”µå­å•†åŠ¡ã€äº‘è®¡ç®—ã€æ•°å­—åª’ä½“å’Œå¨±ä¹ (ç½®ä¿¡åº¦: 85.21%) âš¡ Part 7: KV Cache ä¼˜åŒ–å®è·µä¸ä»£ç 7.1 KV Cache åŸç†å¯è§†åŒ–æ—  KV Cache (æ¯æ¬¡é‡æ–°è®¡ç®—):â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•Step 1: \"I\" â†’ è®¡ç®— Kâ‚,Vâ‚,Qâ‚ â†’ è¾“å‡º \"love\"Step 2: \"I love\" â†’ è®¡ç®— Kâ‚,Vâ‚,Kâ‚‚,Vâ‚‚,Qâ‚‚ â†’ è¾“å‡º \"NLP\" Step 3: \"I love NLP\" â†’ è®¡ç®— Kâ‚,Vâ‚,Kâ‚‚,Vâ‚‚,Kâ‚ƒ,Vâ‚ƒ,Qâ‚ƒ â†’ è¾“å‡º \"!\" â†‘ é‡å¤è®¡ç®—!æœ‰ KV Cache (ç¼“å­˜å¤ç”¨):â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•Step 1: \"I\" â†’ è®¡ç®— Kâ‚,Vâ‚ [å­˜å…¥Cache] â†’ Qâ‚ â†’ \"love\"Step 2: \"love\" â†’ è®¡ç®— Kâ‚‚,Vâ‚‚ [è¿½åŠ Cache] â†’ Qâ‚‚ @ [Kâ‚Kâ‚‚] â†’ \"NLP\"Step 3: \"NLP\" â†’ è®¡ç®— Kâ‚ƒ,Vâ‚ƒ [è¿½åŠ Cache] â†’ Qâ‚ƒ @ [Kâ‚Kâ‚‚Kâ‚ƒ] â†’ \"!\" â†‘ åªè®¡ç®—æ–°tokençš„KV! 7.2 KV Cache å®Œæ•´å®ç°\"\"\"ä»é›¶å®ç°å¸¦ KV Cache çš„ GPT æ¨ç†\"\"\"import torchimport torch.nn as nnimport torch.nn.functional as Fimport mathclass CachedMultiHeadAttention(nn.Module): def __init__(self, d_model=768, n_heads=12): super().__init__() self.d_model = d_model self.n_heads = n_heads self.d_k = d_model // n_heads self.W_q = nn.Linear(d_model, d_model) self.W_k = nn.Linear(d_model, d_model) self.W_v = nn.Linear(d_model, d_model) self.W_o = nn.Linear(d_model, d_model) def forward(self, x, kv_cache=None, use_cache=True): \"\"\" Args: x: [batch, seq_len, d_model] è¾“å…¥ kv_cache: (cached_k, cached_v) ç¼“å­˜çš„Kå’ŒV use_cache: æ˜¯å¦ä½¿ç”¨å’Œæ›´æ–°ç¼“å­˜ Returns: output: [batch, seq_len, d_model] new_cache: æ›´æ–°åçš„ç¼“å­˜ \"\"\" batch_size = x.size(0) # è®¡ç®—å½“å‰tokençš„ Q, K, V Q = self.W_q(x) # [batch, seq, d_model] K = self.W_k(x) V = self.W_v(x) # å¦‚æœæœ‰ç¼“å­˜ï¼Œæ‹¼æ¥å†å² K, V if kv_cache is not None: cached_k, cached_v = kv_cache K = torch.cat([cached_k, K], dim=1) # [batch, cached+seq, d_model] V = torch.cat([cached_v, V], dim=1) # ä¿å­˜æ–°ç¼“å­˜ new_cache = (K, V) if use_cache else None # é‡å¡‘ä¸ºå¤šå¤´æ ¼å¼ def reshape_for_heads(t): # [batch, seq, d_model] - [batch, heads, seq, d_k] return t.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2) Q = reshape_for_heads(Q) # [batch, heads, q_seq, d_k] K = reshape_for_heads(K) # [batch, heads, kv_seq, d_k] V = reshape_for_heads(V) # [batch, heads, kv_seq, d_k] # è®¡ç®—æ³¨æ„åŠ›åˆ†æ•° scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # [batch, heads, q_seq, kv_seq] # Causal Mask (åªçœ‹è¿‡å») q_len, kv_len = Q.size(2), K.size(2) causal_mask = torch.triu( torch.ones(q_len, kv_len, device=x.device), diagonal=kv_len - q_len + 1 ).bool() scores = scores.masked_fill(causal_mask, float('-inf')) # Softmax + åŠ æƒæ±‚å’Œ attn_weights = F.softmax(scores, dim=-1) output = torch.matmul(attn_weights, V) # åˆå¹¶å¤šå¤´ output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model) output = self.W_o(output) return output, new_cacheclass GPTWithKVCache(nn.Module): \"\"\"ç®€åŒ–çš„ GPT æ¨¡å‹ï¼Œæ”¯æŒ KV Cache\"\"\" def __init__(self, vocab_size=50257, d_model=768, n_layers=12, n_heads=12): super().__init__() self.embedding = nn.Embedding(vocab_size, d_model) self.layers = nn.ModuleList([ CachedMultiHeadAttention(d_model, n_heads) for _ in range(n_layers) ]) self.lm_head = nn.Linear(d_model, vocab_size) self.n_layers = n_layers def forward(self, input_ids, past_kv_cache=None, use_cache=True): \"\"\" Args: input_ids: [batch, seq_len] past_kv_cache: List of (K, V) for each layer use_cache: æ˜¯å¦ä½¿ç”¨KV Cache \"\"\" x = self.embedding(input_ids) new_cache = [] for i, layer in enumerate(self.layers): layer_cache = past_kv_cache[i] if past_kv_cache else None x, cache = layer(x, kv_cache=layer_cache, use_cache=use_cache) new_cache.append(cache) logits = self.lm_head(x) return logits, new_cache if use_cache else Nonedef generate_with_kv_cache(model, prompt_ids, max_new_tokens=50): \"\"\"ä½¿ç”¨ KV Cache è¿›è¡Œé«˜æ•ˆç”Ÿæˆ\"\"\" model.eval() generated = prompt_ids.clone() past_cache = None with torch.no_grad(): # ç¬¬ä¸€æ­¥ï¼šå¤„ç†æ•´ä¸ª prompt logits, past_cache = model(prompt_ids, past_kv_cache=None, use_cache=True) next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True) generated = torch.cat([generated, next_token], dim=1) # åç»­æ­¥éª¤ï¼šåªå¤„ç†æ–° token for _ in range(max_new_tokens - 1): # åªè¾“å…¥æœ€åä¸€ä¸ª tokenï¼ logits, past_cache = model( next_token, # [batch, 1] åªæœ‰ä¸€ä¸ªtoken past_kv_cache=past_cache, use_cache=True ) next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True) generated = torch.cat([generated, next_token], dim=1) return generated# æ€§èƒ½å¯¹æ¯”æµ‹è¯•def benchmark_kv_cache(): import time model = GPTWithKVCache(vocab_size=1000, d_model=256, n_layers=6, n_heads=8) prompt = torch.randint(0, 1000, (1, 10)) # æ—  KV Cache start = time.time() for _ in range(100): generated = prompt.clone() for i in range(50): with torch.no_grad(): logits, _ = model(generated, use_cache=False) next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True) generated = torch.cat([generated, next_token], dim=1) time_no_cache = time.time() - start # æœ‰ KV Cache start = time.time() for _ in range(100): generate_with_kv_cache(model, prompt, max_new_tokens=50) time_with_cache = time.time() - start print(f\"æ—  KV Cache: {time_no_cache:.2f}s\") print(f\"æœ‰ KV Cache: {time_with_cache:.2f}s\") print(f\"åŠ é€Ÿæ¯”: {time_no_cache/time_with_cache:.1f}x\")# benchmark_kv_cache()# è¾“å‡ºç¤ºä¾‹:# æ—  KV Cache: 45.32s# æœ‰ KV Cache: 8.21s# åŠ é€Ÿæ¯”: 5.5x 7.3 KV Cache æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯\"\"\"ä¸‰ç§ KV Cache ä¼˜åŒ–æŠ€æœ¯çš„å®ç°å¯¹æ¯”\"\"\"# 1. Multi-Query Attention (MQA)# æ‰€æœ‰ Q å¤´å…±äº«ä¸€ç»„ K, Vclass MultiQueryAttention(nn.Module): def __init__(self, d_model=768, n_heads=12): super().__init__() self.n_heads = n_heads self.d_k = d_model // n_heads self.W_q = nn.Linear(d_model, d_model) # 12 ä¸ªå¤´çš„ Q self.W_k = nn.Linear(d_model, self.d_k) # åªæœ‰ 1 ç»„ K self.W_v = nn.Linear(d_model, self.d_k) # åªæœ‰ 1 ç»„ V self.W_o = nn.Linear(d_model, d_model) def forward(self, x): batch, seq, _ = x.shape Q = self.W_q(x).view(batch, seq, self.n_heads, self.d_k) K = self.W_k(x).unsqueeze(2) # [batch, seq, 1, d_k] V = self.W_v(x).unsqueeze(2) # [batch, seq, 1, d_k] # K, V å¹¿æ’­åˆ°æ‰€æœ‰å¤´ K = K.expand(-1, -1, self.n_heads, -1) V = V.expand(-1, -1, self.n_heads, -1) # ... åç»­è®¡ç®—ç›¸åŒ # KV Cache å¤§å°: 1/n_heads of standard!# 2. Grouped-Query Attention (GQA) - LLaMA-2 ä½¿ç”¨class GroupedQueryAttention(nn.Module): def __init__(self, d_model=768, n_heads=12, n_kv_heads=3): \"\"\" n_heads=12, n_kv_heads=3 è¡¨ç¤º: - 12 ä¸ª Q å¤´ - 3 ä¸ª KV å¤´ (æ¯ 4 ä¸ª Q å¤´å…±äº« 1 ä¸ª KV) \"\"\" super().__init__() self.n_heads = n_heads self.n_kv_heads = n_kv_heads self.n_groups = n_heads // n_kv_heads # 4 self.d_k = d_model // n_heads self.W_q = nn.Linear(d_model, d_model) self.W_k = nn.Linear(d_model, self.n_kv_heads * self.d_k) self.W_v = nn.Linear(d_model, self.n_kv_heads * self.d_k) self.W_o = nn.Linear(d_model, d_model) def forward(self, x): batch, seq, _ = x.shape Q = self.W_q(x).view(batch, seq, self.n_heads, self.d_k) K = self.W_k(x).view(batch, seq, self.n_kv_heads, self.d_k) V = self.W_v(x).view(batch, seq, self.n_kv_heads, self.d_k) # å°† KV é‡å¤ä»¥åŒ¹é… Q å¤´æ•° K = K.repeat_interleave(self.n_groups, dim=2) # [b, s, 12, d_k] V = V.repeat_interleave(self.n_groups, dim=2) # ... åç»­è®¡ç®—ç›¸åŒ # KV Cache å¤§å°: n_kv_heads/n_heads of standard (è¿™é‡Œæ˜¯ 1/4)# 3. æ˜¾å­˜å ç”¨å¯¹æ¯”def memory_comparison(): \"\"\" LLaMA-7B é…ç½®: 32å±‚, 4096ç»´åº¦, 32å¤´ åºåˆ—é•¿åº¦: 4096 tokens \"\"\" n_layers = 32 d_model = 4096 seq_len = 4096 dtype_bytes = 2 # float16 # æ ‡å‡† MHA standard = 2 * n_layers * d_model * seq_len * dtype_bytes print(f\"æ ‡å‡† MHA KV Cache: {standard / 1e9:.2f} GB\") # MQA (æ‰€æœ‰å¤´å…±äº«) n_heads = 32 mqa = 2 * n_layers * (d_model // n_heads) * seq_len * dtype_bytes print(f\"MQA KV Cache: {mqa / 1e9:.2f} GB ({standard/mqa:.0f}x èŠ‚çœ)\") # GQA (LLaMA-2 é…ç½®: 8 ä¸ª KV å¤´) n_kv_heads = 8 gqa = 2 * n_layers * (d_model // n_heads * n_kv_heads) * seq_len * dtype_bytes print(f\"GQA KV Cache: {gqa / 1e9:.2f} GB ({standard/gqa:.0f}x èŠ‚çœ)\")memory_comparison()# è¾“å‡º:# æ ‡å‡† MHA KV Cache: 2.15 GB# MQA KV Cache: 0.07 GB (32x èŠ‚çœ)# GQA KV Cache: 0.27 GB (8x èŠ‚çœ) 7.4 ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ ä¼˜åŒ–æŠ€æœ¯ é€‚ç”¨åœºæ™¯ ä¼˜ç‚¹ ç¼ºç‚¹ æ ‡å‡† MHA å°æ¨¡å‹ã€çŸ­åºåˆ— è´¨é‡æœ€å¥½ æ˜¾å­˜å ç”¨å¤§ MQA è¶…é•¿åºåˆ—ã€æè‡´æ¨ç†é€Ÿåº¦ æ˜¾å­˜èŠ‚çœæœ€å¤š è´¨é‡ç•¥æœ‰ä¸‹é™ GQA ç”Ÿäº§ç¯å¢ƒæ¨è å¹³è¡¡è´¨é‡å’Œæ•ˆç‡ éœ€è¦é‡æ–°è®­ç»ƒ PagedAttention é«˜å¹¶å‘æ¨ç†æœåŠ¡ å‡å°‘æ˜¾å­˜ç¢ç‰‡ å®ç°å¤æ‚ FlashAttention æ‰€æœ‰åœºæ™¯ IOä¼˜åŒ–ã€è®­ç»ƒåŠ é€Ÿ éœ€è¦ç‰¹å®šç¡¬ä»¶ # vLLM ä½¿ç”¨ç¤ºä¾‹ (ç”Ÿäº§æ¨è)from vllm import LLM, SamplingParams# è‡ªåŠ¨å¯ç”¨ PagedAttentionllm = LLM( model=\"meta-llama/Llama-2-7b-hf\", tensor_parallel_size=1, # GPUæ•°é‡ gpu_memory_utilization=0.9, # æ˜¾å­˜åˆ©ç”¨ç‡)# é«˜æ•ˆæ¨ç†prompts = [\"å†™ä¸€é¦–å…³äºæ˜¥å¤©çš„è¯—ï¼š\", \"è§£é‡Šä»€ä¹ˆæ˜¯æœºå™¨å­¦ä¹ ï¼š\"]outputs = llm.generate(prompts, SamplingParams(temperature=0.7, max_tokens=100)) ğŸš€ Part 6: BERT å®é™…åº”ç”¨åœºæ™¯ä¸ä»£ç 6.1 åº”ç”¨åœºæ™¯æ€»è§ˆ åº”ç”¨åœºæ™¯ ä»»åŠ¡ç±»å‹ è¾“å…¥æ ¼å¼ è¾“å‡º å®é™…æ¡ˆä¾‹ æƒ…æ„Ÿåˆ†æ å•å¥åˆ†ç±» [CLS] æ–‡æœ¬ [SEP] æ­£é¢/è´Ÿé¢ ç”µå•†è¯„è®ºåˆ†æã€èˆ†æƒ…ç›‘æ§ æ–‡æœ¬åŒ¹é… å¥å¯¹åˆ†ç±» [CLS] å¥A [SEP] å¥B [SEP] ç›¸ä¼¼/ä¸ç›¸ä¼¼ æ™ºèƒ½å®¢æœã€é—®é¢˜å»é‡ å‘½åå®ä½“è¯†åˆ« åºåˆ—æ ‡æ³¨ [CLS] æ–‡æœ¬ [SEP] æ¯ä¸ªtokençš„æ ‡ç­¾ ç®€å†è§£æã€åŒ»ç–—ç—…å† é˜…è¯»ç†è§£ æŠ½å–å¼QA [CLS] é—®é¢˜ [SEP] æ–‡ç«  [SEP] ç­”æ¡ˆä½ç½® æ™ºèƒ½é—®ç­”ã€çŸ¥è¯†åº“æ£€ç´¢ æ–‡æœ¬ç”Ÿæˆ Seq2Seq éœ€é…åˆDecoder ç”Ÿæˆæ–‡æœ¬ æ‘˜è¦ç”Ÿæˆã€æœºå™¨ç¿»è¯‘ 6.2 æƒ…æ„Ÿåˆ†æå®Œæ•´ä»£ç from transformers import BertTokenizer, BertForSequenceClassificationimport torch# 1. åŠ è½½é¢„è®­ç»ƒæ¨¡å‹model_name = \"bert-base-chinese\"tokenizer = BertTokenizer.from_pretrained(model_name)model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)# 2. æ•°æ®é¢„å¤„ç†def preprocess(texts, labels): encodings = tokenizer( texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\" ) return encodings, torch.tensor(labels)# 3. è®­ç»ƒæ•°æ®ç¤ºä¾‹train_texts = [ \"è¿™ä¸ªäº§å“è´¨é‡å¤ªå·®äº†ï¼Œå®Œå…¨æ˜¯æµªè´¹é’±\", \"éå¸¸æ»¡æ„ï¼å‘è´§é€Ÿåº¦å¿«ï¼Œè´¨é‡å¾ˆå¥½\", \"ä¸€èˆ¬èˆ¬å§ï¼Œæ²¡æœ‰æƒ³è±¡ä¸­é‚£ä¹ˆå¥½\", \"è¶…çº§æ¨èï¼å·²ç»å›è´­ä¸‰æ¬¡äº†\"]train_labels = [0, 1, 0, 1] # 0=è´Ÿé¢, 1=æ­£é¢# 4. å¾®è°ƒè®­ç»ƒfrom torch.optim import AdamWencodings, labels = preprocess(train_texts, train_labels)optimizer = AdamW(model.parameters(), lr=2e-5)model.train()for epoch in range(3): outputs = model(**encodings, labels=labels) loss = outputs.loss loss.backward() optimizer.step() optimizer.zero_grad() print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")# 5. æ¨ç†é¢„æµ‹model.eval()test_text = \"è¿™æ¬¾æ‰‹æœºæ‹ç…§æ•ˆæœå¾ˆæ£’ï¼Œç”µæ± ä¹Ÿè€ç”¨\"inputs = tokenizer(test_text, return_tensors=\"pt\")with torch.no_grad(): outputs = model(**inputs) prediction = torch.argmax(outputs.logits, dim=1) print(f\"é¢„æµ‹ç»“æœ: {'æ­£é¢' if prediction == 1 else 'è´Ÿé¢'}\") 6.3 å‘½åå®ä½“è¯†åˆ« (NER) å®Œæ•´ä»£ç from transformers import BertTokenizerFast, BertForTokenClassificationimport torch# NER æ ‡ç­¾å®šä¹‰ (BIO æ ¼å¼)label_list = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]label2id = {label: i for i, label in enumerate(label_list)}id2label = {i: label for label, i in label2id.items()}# åŠ è½½æ¨¡å‹tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")model = BertForTokenClassification.from_pretrained( \"bert-base-chinese\", num_labels=len(label_list), id2label=id2label, label2id=label2id)# è®­ç»ƒæ•°æ®ç¤ºä¾‹train_data = [ { \"text\": \"é©¬äº‘åœ¨æ­å·åˆ›åŠäº†é˜¿é‡Œå·´å·´\", \"entities\": [ {\"start\": 0, \"end\": 2, \"label\": \"PER\"}, # é©¬äº‘ {\"start\": 3, \"end\": 5, \"label\": \"LOC\"}, # æ­å· {\"start\": 8, \"end\": 12, \"label\": \"ORG\"} # é˜¿é‡Œå·´å·´ ] }]# æ¨ç†ç¤ºä¾‹def predict_ner(text): inputs = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True) offset_mapping = inputs.pop(\"offset_mapping\")[0] model.eval() with torch.no_grad(): outputs = model(**inputs) predictions = torch.argmax(outputs.logits, dim=2)[0] # è§£æå®ä½“ entities = [] current_entity = None for idx, (pred, offset) in enumerate(zip(predictions, offset_mapping)): label = id2label[pred.item()] if label.startswith(\"B-\"): if current_entity: entities.append(current_entity) current_entity = { \"text\": text[offset[0]:offset[1]], \"label\": label[2:], \"start\": offset[0].item() } elif label.startswith(\"I-\") and current_entity: current_entity[\"text\"] += text[offset[0]:offset[1]] else: if current_entity: entities.append(current_entity) current_entity = None return entities# æµ‹è¯•result = predict_ner(\"é©¬äº‘åœ¨æ­å·åˆ›åŠäº†é˜¿é‡Œå·´å·´\")print(result)# [{'text': 'é©¬äº‘', 'label': 'PER', 'start': 0}, # {'text': 'æ­å·', 'label': 'LOC', 'start': 3},# {'text': 'é˜¿é‡Œå·´å·´', 'label': 'ORG', 'start': 8}] 6.4 è¯­ä¹‰ç›¸ä¼¼åº¦åŒ¹é…ä»£ç from transformers import BertTokenizer, BertModelimport torchimport torch.nn.functional as F# ä½¿ç”¨ BERT æå–å¥å­å‘é‡tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")model = BertModel.from_pretrained(\"bert-base-chinese\")def get_sentence_embedding(text): \"\"\"æå–å¥å­çš„ [CLS] å‘é‡ä½œä¸ºè¯­ä¹‰è¡¨ç¤º\"\"\" inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True) with torch.no_grad(): outputs = model(**inputs) # ä½¿ç”¨ [CLS] token çš„è¾“å‡ºä½œä¸ºå¥å­è¡¨ç¤º cls_embedding = outputs.last_hidden_state[:, 0, :] return cls_embeddingdef compute_similarity(text1, text2): \"\"\"è®¡ç®—ä¸¤ä¸ªå¥å­çš„ä½™å¼¦ç›¸ä¼¼åº¦\"\"\" emb1 = get_sentence_embedding(text1) emb2 = get_sentence_embedding(text2) similarity = F.cosine_similarity(emb1, emb2) return similarity.item()# æµ‹è¯•è¯­ä¹‰ç›¸ä¼¼åº¦pairs = [ (\"ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·\", \"ä»Šå¤©å¤©æ°”å¥½å—\"), # é«˜ç›¸ä¼¼ (\"ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·\", \"æ˜å¤©ä¼šä¸‹é›¨å—\"), # ä¸­ç­‰ç›¸ä¼¼ (\"ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·\", \"è¿™é“èœæ€ä¹ˆåš\"), # ä½ç›¸ä¼¼]for text1, text2 in pairs: sim = compute_similarity(text1, text2) print(f\"'{text1}' vs '{text2}': {sim:.4f}\")# è¾“å‡º:# 'ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·' vs 'ä»Šå¤©å¤©æ°”å¥½å—': 0.9234# 'ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·' vs 'æ˜å¤©ä¼šä¸‹é›¨å—': 0.7821# 'ä»Šå¤©å¤©æ°”æ€ä¹ˆæ ·' vs 'è¿™é“èœæ€ä¹ˆåš': 0.4123 6.5 é—®ç­”ç³»ç»Ÿ (é˜…è¯»ç†è§£) ä»£ç from transformers import BertTokenizer, BertForQuestionAnsweringimport torch# åŠ è½½é—®ç­”æ¨¡å‹model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"tokenizer = BertTokenizer.from_pretrained(model_name)model = BertForQuestionAnswering.from_pretrained(model_name)def answer_question(question, context): \"\"\"ä»æ–‡ç« ä¸­æŠ½å–ç­”æ¡ˆ\"\"\" # ç¼–ç é—®é¢˜å’Œä¸Šä¸‹æ–‡ inputs = tokenizer( question, context, return_tensors=\"pt\", max_length=512, truncation=True ) # é¢„æµ‹ç­”æ¡ˆä½ç½® with torch.no_grad(): outputs = model(**inputs) start_scores = outputs.start_logits end_scores = outputs.end_logits # æ‰¾åˆ°æœ€å¯èƒ½çš„ç­”æ¡ˆä½ç½® start_idx = torch.argmax(start_scores) end_idx = torch.argmax(end_scores) # è§£ç ç­”æ¡ˆ tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]) answer = tokenizer.convert_tokens_to_string(tokens[start_idx:end_idx+1]) return answer# æµ‹è¯•é—®ç­”context = \"\"\"BERTæ˜¯ç”±Googleåœ¨2018å¹´æå‡ºçš„é¢„è®­ç»ƒè¯­è¨€æ¨¡å‹ã€‚å®ƒä½¿ç”¨Transformerçš„Encoderæ¶æ„ï¼Œé€šè¿‡Masked Language Modelä»»åŠ¡è¿›è¡Œé¢„è®­ç»ƒã€‚BERTåœ¨11ä¸ªNLPä»»åŠ¡ä¸Šå–å¾—äº†å½“æ—¶çš„æœ€ä½³æˆç»©ï¼ŒåŒ…æ‹¬é—®ç­”ã€æ–‡æœ¬åˆ†ç±»ç­‰ä»»åŠ¡ã€‚BERT-Baseæœ‰1.1äº¿å‚æ•°ï¼ŒBERT-Largeæœ‰3.4äº¿å‚æ•°ã€‚\"\"\"questions = [ \"BERTæ˜¯è°æå‡ºçš„ï¼Ÿ\", \"BERTä½¿ç”¨ä»€ä¹ˆæ¶æ„ï¼Ÿ\", \"BERTæœ‰å¤šå°‘å‚æ•°ï¼Ÿ\"]for q in questions: answer = answer_question(q, context) print(f\"Q: {q}\") print(f\"A: {answer} \") âš¡ Part 7: KV Cache å®æˆ˜ä¸ä¼˜åŒ–7.1 KV Cache å®Œæ•´å®ç°ä»£ç import torchimport torch.nn as nnimport torch.nn.functional as Fclass AttentionWithKVCache(nn.Module): \"\"\"å¸¦ KV Cache çš„æ³¨æ„åŠ›å±‚å®ç°\"\"\" def __init__(self, d_model=768, n_heads=12): super().__init__() self.d_model = d_model self.n_heads = n_heads self.d_k = d_model // n_heads self.W_q = nn.Linear(d_model, d_model) self.W_k = nn.Linear(d_model, d_model) self.W_v = nn.Linear(d_model, d_model) self.W_o = nn.Linear(d_model, d_model) def forward(self, x, kv_cache=None, use_cache=False): \"\"\" Args: x: [batch, seq_len, d_model] è¾“å…¥ kv_cache: tuple(K, V) ç¼“å­˜çš„ K/V use_cache: æ˜¯å¦ä½¿ç”¨å¹¶æ›´æ–°ç¼“å­˜ Returns: output: [batch, seq_len, d_model] new_kv_cache: æ›´æ–°åçš„ç¼“å­˜ \"\"\" batch_size, seq_len, _ = x.shape # è®¡ç®— Q, K, V Q = self.W_q(x) # [batch, seq, d_model] K = self.W_k(x) V = self.W_v(x) # å¦‚æœæœ‰ç¼“å­˜ï¼Œæ‹¼æ¥å†å² K, V if kv_cache is not None: K_cache, V_cache = kv_cache K = torch.cat([K_cache, K], dim=1) # [batch, cache_len + seq, d_model] V = torch.cat([V_cache, V], dim=1) # å‡†å¤‡è¿”å›çš„ç¼“å­˜ new_kv_cache = (K, V) if use_cache else None # é‡å¡‘ä¸ºå¤šå¤´å½¢å¼ Q = Q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2) K = K.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2) V = V.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2) # Q: [batch, heads, seq, d_k] # K, V: [batch, heads, total_seq, d_k] # è®¡ç®—æ³¨æ„åŠ› scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5) # Causal Mask (åªçœ‹å·¦è¾¹) total_len = K.size(2) query_len = Q.size(2) mask = torch.triu(torch.ones(query_len, total_len), diagonal=total_len-query_len+1) mask = mask.bool().to(x.device) scores = scores.masked_fill(mask, float('-inf')) attn_weights = F.softmax(scores, dim=-1) output = torch.matmul(attn_weights, V) # åˆå¹¶å¤šå¤´ output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model) output = self.W_o(output) return output, new_kv_cache# ä½¿ç”¨ç¤ºä¾‹ï¼šè‡ªå›å½’ç”Ÿæˆdef generate_with_kv_cache(model, prompt_ids, max_new_tokens=50): \"\"\"ä½¿ç”¨ KV Cache è¿›è¡Œé«˜æ•ˆç”Ÿæˆ\"\"\" # åˆå§‹åŒ–ï¼šå¤„ç† prompt kv_cache = None input_ids = prompt_ids generated = prompt_ids.tolist() for step in range(max_new_tokens): # åªè¾“å…¥æ–°çš„ tokenï¼ˆç¬¬ä¸€æ­¥è¾“å…¥å®Œæ•´ promptï¼‰ if step == 0: x = get_embeddings(input_ids) else: x = get_embeddings(input_ids[:, -1:]) # åªå–æœ€åä¸€ä¸ª token # å‰å‘ä¼ æ’­ï¼Œä½¿ç”¨å¹¶æ›´æ–°ç¼“å­˜ output, kv_cache = model(x, kv_cache=kv_cache, use_cache=True) # é¢„æµ‹ä¸‹ä¸€ä¸ª token logits = output[:, -1, :] # å–æœ€åä¸€ä¸ªä½ç½®çš„è¾“å‡º next_token = torch.argmax(logits, dim=-1) generated.append(next_token.item()) input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1) # é‡åˆ°ç»“æŸç¬¦åœæ­¢ if next_token.item() == EOS_TOKEN_ID: break return generatedprint(\"KV Cache åŠ é€Ÿå¯¹æ¯”:\")print(\"æ— ç¼“å­˜: O(nÂ²) è®¡ç®—é‡\")print(\"æœ‰ç¼“å­˜: O(n) è®¡ç®—é‡\")print(\"åŠ é€Ÿæ¯”: n å€ (åºåˆ—é•¿åº¦)\") 7.2 KV Cache æ˜¾å­˜ä¼˜åŒ–æŠ€æœ¯Multi-Query Attention (MQA) å®ç°class MultiQueryAttention(nn.Module): \"\"\" Multi-Query Attention: æ‰€æœ‰å¤´å…±äº«ä¸€ç»„ K, V æ˜¾å­˜èŠ‚çœ: n_heads å€ \"\"\" def __init__(self, d_model=768, n_heads=12): super().__init__() self.n_heads = n_heads self.d_k = d_model // n_heads # Q: æ¯ä¸ªå¤´ç‹¬ç«‹ self.W_q = nn.Linear(d_model, d_model) # K, V: æ‰€æœ‰å¤´å…±äº« (åªæœ‰ä¸€ä»½) self.W_k = nn.Linear(d_model, self.d_k) # åªè¾“å‡ºä¸€ä¸ªå¤´çš„ç»´åº¦ self.W_v = nn.Linear(d_model, self.d_k) self.W_o = nn.Linear(d_model, d_model) def forward(self, x, kv_cache=None): batch, seq, _ = x.shape Q = self.W_q(x).view(batch, seq, self.n_heads, self.d_k).transpose(1, 2) K = self.W_k(x).unsqueeze(1) # [batch, 1, seq, d_k] å¹¿æ’­åˆ°æ‰€æœ‰å¤´ V = self.W_v(x).unsqueeze(1) # KV Cache åªéœ€å­˜å‚¨ [batch, 1, seq, d_k] è€Œé [batch, heads, seq, d_k] # æ˜¾å­˜èŠ‚çœ: 12å€ (å¯¹äº12å¤´) if kv_cache is not None: K = torch.cat([kv_cache[0], K], dim=2) V = torch.cat([kv_cache[1], V], dim=2) scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5) attn = F.softmax(scores, dim=-1) output = torch.matmul(attn, V) output = output.transpose(1, 2).contiguous().view(batch, seq, -1) return self.W_o(output), (K, V) Grouped-Query Attention (GQA) å®ç°class GroupedQueryAttention(nn.Module): \"\"\" Grouped-Query Attention: å°†å¤´åˆ†ç»„ï¼Œæ¯ç»„å…±äº« K, V LLaMA-2 ä½¿ç”¨: 32å¤´ â†’ 8ç»„ æ˜¾å­˜èŠ‚çœ: n_heads / n_groups å€ \"\"\" def __init__(self, d_model=768, n_heads=12, n_kv_groups=4): super().__init__() self.n_heads = n_heads self.n_kv_groups = n_kv_groups self.heads_per_group = n_heads // n_kv_groups self.d_k = d_model // n_heads self.W_q = nn.Linear(d_model, d_model) # K, V åªæœ‰ n_kv_groups ç»„ self.W_k = nn.Linear(d_model, self.d_k * n_kv_groups) self.W_v = nn.Linear(d_model, self.d_k * n_kv_groups) self.W_o = nn.Linear(d_model, d_model) def forward(self, x): batch, seq, _ = x.shape Q = self.W_q(x).view(batch, seq, self.n_heads, self.d_k).transpose(1, 2) K = self.W_k(x).view(batch, seq, self.n_kv_groups, self.d_k).transpose(1, 2) V = self.W_v(x).view(batch, seq, self.n_kv_groups, self.d_k).transpose(1, 2) # å°† K, V æ‰©å±•åˆ°ä¸ Q ç›¸åŒçš„å¤´æ•° K = K.repeat_interleave(self.heads_per_group, dim=1) V = V.repeat_interleave(self.heads_per_group, dim=1) scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5) attn = F.softmax(scores, dim=-1) output = torch.matmul(attn, V) output = output.transpose(1, 2).contiguous().view(batch, seq, -1) return self.W_o(output) 7.3 æ˜¾å­˜å ç”¨å¯¹æ¯”è¡¨ æŠ€æœ¯ KV Cache å¤§å° LLaMA-7B 128K tokens é€‚ç”¨æ¨¡å‹ MHA (æ ‡å‡†) 2 Ã— L Ã— H Ã— S Ã— dtype 64 GB BERT, GPT-2 MQA 2 Ã— L Ã— (H/heads) Ã— S Ã— dtype 5.3 GB PaLM, Falcon GQA (8ç»„) 2 Ã— L Ã— (H/4) Ã— S Ã— dtype 16 GB LLaMA-2, Mistral L=å±‚æ•°, H=éšè—ç»´åº¦, S=åºåˆ—é•¿åº¦, heads=æ³¨æ„åŠ›å¤´æ•° 7.4 ç”Ÿäº§ç¯å¢ƒæœ€ä½³å®è·µ# ä½¿ç”¨ vLLM è¿›è¡Œé«˜æ•ˆæ¨ç† (PagedAttention)from vllm import LLM, SamplingParams# åˆå§‹åŒ–æ¨¡å‹llm = LLM( model=\"meta-llama/Llama-2-7b-hf\", tensor_parallel_size=1, # GPU æ•°é‡ gpu_memory_utilization=0.9, # æ˜¾å­˜ä½¿ç”¨ç‡)# æ‰¹é‡æ¨ç† (è‡ªåŠ¨ç®¡ç† KV Cache)prompts = [ \"Explain the concept of attention mechanism:\", \"Write a Python function to sort a list:\", \"What is the capital of France?\"]sampling_params = SamplingParams( temperature=0.7, max_tokens=256, top_p=0.9)outputs = llm.generate(prompts, sampling_params)for output in outputs: print(f\"Prompt: {output.prompt}\") print(f\"Response: {output.outputs[0].text} \") ğŸ“… åˆ›å»ºæ—¶é—´: 2026-02-03ğŸ·ï¸ æ ‡ç­¾: #AI #Transformer #BERT #Attention #KVCache","tags":["AI"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"å½»åº•ç†è§£ Transformerï¼šAttention Is All You Need","path":"/2026/02/02/Transformer-DeepDive/","content":"å½»åº•ç†è§£ Transformer: Attention Is All You Need æ‘˜è¦: æœ¬æ–‡åˆ©ç”¨è´¹æ›¼å­¦ä¹ æ³•ï¼Œé€šè¿‡å›¾è§£å’Œç±»æ¯”ï¼Œæ·±å…¥æµ…å‡ºåœ°æ‹†è§£äº† Transformer æ¶æ„çš„æ ¸å¿ƒåŸç†ã€‚ä» Encoder-Decoder æ¶æ„åˆ° Self-Attention æœºåˆ¶ï¼Œå†åˆ° Cross-Attention å’Œ Masking çš„ç»†èŠ‚ï¼Œå¸¦ä½ å½»åº•ææ‡‚è¿™ç¯‡ AI é¢†åŸŸçš„å¥ åŸºä¹‹ä½œã€‚ 1. æ ¸å¿ƒå¤§ç™½è¯ï¼šä¸ºä»€ä¹ˆè¦æè¿™ä¸€å¥—ï¼Ÿä¼ ç»Ÿæ¨¡å‹çš„ç—›ç‚¹ï¼šä»¥å‰çš„ AI (RNN/LSTM) åƒæ˜¯ä¸€ä¸ªæ¥åŠ›èµ›è·‘é€‰æ‰‹ã€‚ å¿…é¡»å…ˆè¯»ç¬¬ä¸€ä¸ªå­—ï¼Œä¼ æ£’ç»™ç¬¬äºŒä¸ªå­—ï¼Œå†ä¼ ç»™ç¬¬ä¸‰ä¸ªâ€¦ ç¼ºç‚¹ï¼šå¦‚æœå¥å­å¤ªé•¿ï¼Œè·‘åˆ°æœ€åæ—©å°±å¿˜äº†ç¬¬ä¸€æ£’æ˜¯è°äº†ï¼ˆé•¿è·ç¦»é—å¿˜ï¼‰ï¼›è€Œä¸”å¿…é¡»æŒ‰é¡ºåºè·‘ï¼Œä¸èƒ½æ‰€æœ‰äººä¸€èµ·è·‘ï¼ˆæ— æ³•å¹¶è¡Œï¼Œæ…¢ï¼‰ã€‚ Transformer çš„é©å‘½ï¼šä¸ææ¥åŠ›èµ›äº†ï¼Œæè¶³çƒèµ›ã€‚ å¹¶è¡Œï¼šæ‰€æœ‰å•è¯ï¼ˆçƒå‘˜ï¼‰åŒæ—¶åœ¨åœºä¸Šã€‚ æ³¨æ„åŠ›ï¼šæ¯ä¸ªçƒå‘˜ï¼ˆå•è¯ï¼‰éƒ½èƒ½æ—¶åˆ»è§‚å¯Ÿåœºä¸Šæ‰€æœ‰å…¶ä»–çƒå‘˜çš„ä½ç½®ï¼Œä¸ç®¡é‚£ä¸ªäººåœ¨çƒåœºå“ªä¸€å¤´ï¼ˆè§£å†³äº†é•¿è·ç¦»ä¾èµ–ï¼‰ã€‚ 2. æ ¸å¿ƒæ¦‚å¿µï¼šSelf-Attention (è‡ªæ³¨æ„åŠ›)è¿™æ˜¯ Transformer çš„å¿ƒè„ã€‚å®ƒè§£å†³äº†â€œæ¯ä¸ªè¯è¯¥å…³æ³¨è°â€çš„é—®é¢˜ã€‚ 2.1 Q, K, V çš„å›¾ä¹¦æ£€ç´¢ç±»æ¯”ä¸ºä»€ä¹ˆæ¯ä¸ªè¯è¦æœ‰ Query (Q), Key (K), Value (V) ä¸‰ä¸ªå‘é‡ï¼Ÿè¿™æºäºæ•°æ®åº“æ£€ç´¢çš„æ€æƒ³ã€‚ (å›¾æ³¨ï¼šé€šè¿‡å›¾ä¹¦æ£€ç´¢ç³»ç»Ÿç†è§£ Q, K, V çš„åˆ†ç¦») 2.2 ä¸»åŠ¨ä¸è¢«åŠ¨ï¼šQ ä¸ K çš„èšå…‰ç¯æ•ˆåº”ä¸ºä»€ä¹ˆæ•°å­¦å…¬å¼ä¸€æ ·ï¼ŒQ å´æ˜¯â€œä¸»åŠ¨â€çš„ï¼Ÿ (å›¾æ³¨ï¼šQ åƒæ‰‹ç”µç­’ä¸€æ ·ä¸»åŠ¨æ‰«ææ‰€æœ‰ K) Q (Query)ï¼šæ‰‹æ¡ 100% çš„â€œæ³¨æ„åŠ›é¢„ç®—â€ï¼Œå®ƒå¿…é¡»å†³å®šæŠŠå…‰æ‰“åœ¨è°èº«ä¸Šã€‚åœ¨è®­ç»ƒä¸­ï¼ŒLoss é€¼è¿«å®ƒå­¦ä¼šâ€œå»å¯»æ‰¾å¯¹æˆ‘æœ‰ç”¨çš„ä¿¡æ¯â€ã€‚ K (Key)ï¼šåƒå¢™ä¸Šçš„é¶å­ï¼Œæ— æ³•ç§»åŠ¨ã€‚åœ¨è®­ç»ƒä¸­ï¼ŒLoss é€¼è¿«å®ƒå­¦ä¼šâ€œæ ‡æ˜è‡ªå·±çš„èº«ä»½â€ï¼Œä»¥ä¾¿è¢«æ­£ç¡®çš„ Q æ‰¾åˆ°ã€‚ 2.3 çŸ©é˜µç»´åº¦çš„ç§˜å¯†ï¼šä¸ºä»€ä¹ˆç»´åº¦å¿…é¡»ä¸€æ ·ï¼Ÿå¾ˆå¤šåˆå­¦è€…åœ¨è¿™é‡Œå¡ä½ï¼šä¸ºä»€ä¹ˆ $Q$ å’Œ $K$ çš„ç»´åº¦å¿…é¡»ä¸€è‡´ï¼Ÿå› ä¸ºæ•°å­¦ä¸Šçš„ç‚¹ç§¯ (Dot Product) å°±åƒæ‹‰æ‹‰é“¾ï¼Œé½¿æ•°å¿…é¡»å¯¹å¾—ä¸Šã€‚ (å›¾æ³¨ï¼šçŸ©é˜µä¹˜æ³•å°±åƒâ€œæ‹‰é“¾â€å’¬åˆï¼Œä¸­é—´çš„ç»´åº¦å¿…é¡»å¯¹é½) 3. å®Œæ•´å®æˆ˜ï¼šå¾·è¯­åˆ°è‹±è¯­ç¿»è¯‘è®­ç»ƒå…¨è¿‡ç¨‹æˆ‘ä»¬é€šè¿‡ä¸€ä¸ªå…·ä½“çš„ä»»åŠ¡ï¼š\"Ich liebe dich\" (å¾·) - \"I love you\" (è‹±)ï¼Œæ¥å½»åº•ææ‡‚ç¼–ç å™¨å’Œè§£ç å™¨åˆ°åº•åœ¨å¹²ä»€ä¹ˆã€‚ 3.1 æ¶æ„å…¨æ™¯ï¼šæ•°æ®æ˜¯å¦‚ä½•æµåŠ¨çš„ï¼Ÿé¦–å…ˆï¼Œçœ‹ä¸€çœ¼ä¸Šå¸è§†è§’çš„æ¶æ„å›¾ã€‚æ³¨æ„çº¢è‰²çš„æ¢¯åº¦å›ä¼ çº¿ï¼Œè¿™è§£é‡Šäº†ä¸ºä»€ä¹ˆ Encoder å³ä½¿æ²¡æœ‰ Label ä¹Ÿèƒ½å­¦ä¼šæ­£ç¡®ç¼–ç ã€‚ (å›¾æ³¨ï¼šEncoder-Decoder å®Œæ•´æ¶æ„ä¸æ¢¯åº¦å›ä¼ è·¯å¾„) 3.2 æ­¥éª¤ä¸€ï¼šEncoder (è¯»æ‡‚å¾·è¯­)Encoder çš„ä»»åŠ¡æ˜¯æŠŠå¾·è¯­å˜æˆä¸€ç»„é«˜è´¨é‡çš„â€œè®°å¿†å‘é‡â€ã€‚ æ“ä½œ: Self-Attentionã€‚ ç»“æœ: æ¯ä¸ªè¯éƒ½èåˆäº†ä¸Šä¸‹æ–‡ä¿¡æ¯ï¼Œå½¢æˆäº†è®°å¿†åº“ $M$ (Memory)ã€‚ ç»´åº¦ä¸å˜æ€§: è¾“å…¥æ˜¯ [Batch, 3, 512]ï¼Œè¾“å‡º $M$ ä¾ç„¶æ˜¯ [Batch, 3, 512]ã€‚è¿™å°±æ˜¯æ®‹å·®æµ (Residual Stream) çš„è®¾è®¡ã€‚ 3.3 æ­¥éª¤äºŒï¼šDecoder çš„æ ¸å¿ƒ (Cross-Attention)è¿™æ˜¯æ¨¡å‹æœ€ç²¾å½©çš„éƒ¨åˆ†ã€‚Decoder æ‹¿ç€è‹±è¯­å»æŸ¥å¾·è¯­ã€‚ (å›¾æ³¨ï¼šCross-Attention ç»†èŠ‚ã€‚è‹±è¯­çš„ â€œIâ€ (Q) ç²¾å‡†æ‰¾åˆ°äº†å¾·è¯­çš„ â€œliebeâ€ (K2)) Query (Q): æ¥è‡ª Decoder ä¸­é—´çŠ¶æ€ $X_{mid}$ã€‚ å½¢çŠ¶: [Batch, 2, 512] (è‹±è¯­é•¿åº¦) Key (K): æ¥è‡ª Encoder è®°å¿†åº“ $M$ã€‚ å½¢çŠ¶: [Batch, 3, 512] (å¾·è¯­é•¿åº¦) Value (V): æ¥è‡ª Encoder è®°å¿†åº“ $M$ã€‚ QKV çŸ©é˜µçš„ä½œç”¨:Decoder çš„ $W_Q^{cross}$ å’Œ Encoder ä¾§çš„ $W_K^{cross}$ å°±åƒåŒå£°ä¼ è¯‘å‘˜ï¼ŒæŠŠè‹±è¯­çŠ¶æ€å’Œå¾·è¯­è®°å¿†æ˜ å°„åˆ°åŒä¸€ä¸ªâ€œä¸­é—´è¯­ä¹‰ç©ºé—´â€ï¼Œè¿™æ ·å®ƒä»¬æ‰èƒ½è¿›è¡Œç‚¹ç§¯åŒ¹é…ã€‚ 3.4 æ­¥éª¤ä¸‰ï¼šMask æœºåˆ¶ (ä¸è®¸å·çœ‹)åœ¨ Decoder å†…éƒ¨ï¼Œä¸ºäº†é˜²æ­¢æ¨¡å‹åœ¨é¢„æµ‹ â€œloveâ€ æ—¶å·çœ‹åé¢çš„ â€œyouâ€ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† Maskã€‚ (å›¾æ³¨ï¼šMask çŸ©é˜µã€‚æ·±è“è‰²ä»£è¡¨å¯è§ï¼Œç™½è‰²ä»£è¡¨è¢«é®æŒ¡ (-âˆ)) 3.5 æ­¥éª¤å››ï¼šå¹¶è¡Œè®¡ç®—ä¸ Lossè™½ç„¶é€»è¾‘ä¸Šæ˜¯â€œé¢„æµ‹å®Œ I å†é¢„æµ‹ loveâ€ï¼Œä½†åœ¨è®­ç»ƒæ—¶ï¼Œè¿™éƒ½åœ¨ä¸€æ¬¡çŸ©é˜µè¿ç®—ä¸­å®Œæˆäº†ã€‚ (å›¾æ³¨ï¼šä¸€æ¬¡æ€§å¤„ç† 4 ä¸ªæ—¶åˆ»çš„é¢„æµ‹ä¸ Loss è®¡ç®—) 4. æç®€ä»£ç å®ç° (Python/PyTorch)import torchimport torch.nn as nnimport torch.nn.functional as Fimport mathclass SelfAttention(nn.Module): def __init__(self, d_model, head_size): super().__init__() self.head_size = head_size # 1. å®šä¹‰ä¸‰ä¸ªçº¿æ€§å±‚ï¼šç”¨æ¥ç”Ÿæˆ Q, K, V self.key = nn.Linear(d_model, head_size, bias=False) self.query = nn.Linear(d_model, head_size, bias=False) self.value = nn.Linear(d_model, head_size, bias=False) def forward(self, x): # x çš„å½¢çŠ¶: [Batch, Time(è¯æ•°), Dimension(ç»´åº¦)] B, T, C = x.shape # 2. ç”Ÿæˆ Q, K, V k = self.key(x) # (B, T, H) q = self.query(x) # (B, T, H) v = self.value(x) # (B, T, H) # 3. è®¡ç®—å…³æ³¨åº¦ (Attention Scores) wei = q @ k.transpose(-2, -1) * (1.0 / math.sqrt(self.head_size)) wei = F.softmax(wei, dim=-1) out = wei @ v return out 5. æ ¸å¿ƒæ•°å­¦å…¬å¼ä¸ç»´åº¦ (The Math Dimensions)5.1 Self-Attention (å•å¤´)$$ Attention(Q, K, V) = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d_k}})V $$ ç»´åº¦æ¨æ¼” (ä»¥ $d_{model}=512, d_k=64$ ä¸ºä¾‹)ï¼š è¾“å…¥ $X$: [L, 512] ç”Ÿæˆ Q, K, V: $Q = X W_Q \\rightarrow [L, 64]$ $K = X W_K \\rightarrow [L, 64]$ $V = X W_V \\rightarrow [L, 64]$ çŸ©é˜µä¹˜æ³• $Q \\cdot K^T$: [L, 64] @ [64, L] $\\rightarrow$ [L, L] (åˆ†æ•°çŸ©é˜µ) è¾“å‡º: [L, L] @ [L, 64] $\\rightarrow$ [L, 64] 5.2 Cross-Attention (æ··åˆåŒæ‰“)$$ \\text{CrossAttn}(X_{dec}, M) = \\text{Softmax}(\\frac{(X_{dec}W_Q)(M W_K)^T}{\\sqrt{d_k}}) (M W_V) $$ $X_{dec}$: è‹±è¯­ä¸­é—´çŠ¶æ€ [L_tgt, 512] $M$: å¾·è¯­è®°å¿†åº“ [L_src, 512] $W_Q$: [512, 64] (è´Ÿè´£è½¬è¯‘è‹±è¯­) $W_K$: [512, 64] (è´Ÿè´£è½¬è¯‘å¾·è¯­) Attention Map: [L_tgt, L_src] (ä¾‹å¦‚ 2è¡Œ3åˆ—ï¼Œè¡¨ç¤ºæ¯ä¸ªè‹±è¯­è¯å…³æ³¨å“ªäº›å¾·è¯­è¯) 5.3 æœ€ç»ˆè¾“å‡º (Word Prediction)$$ P(\\text{word}) = \\text{Softmax}(h \\cdot W_{vocab} + b) $$ $h$: Decoder æœ€ç»ˆè¾“å‡º [Batch, Seq, 512] $W_{vocab}$: æŠ•å½±çŸ©é˜µ [512, Vocab_Size] (ä¾‹å¦‚ 30000) ç»“æœ: [Batch, Seq, 30000] 6. è´¹æ›¼è‡ªæµ‹ (Self-Check) Encoder æ²¡æœ‰ Labelï¼Œå®ƒæ˜¯æ€ä¹ˆå­¦ä¼šæ­£ç¡®çš„ Attention çš„ï¼Ÿ(ç­”ï¼šé€šè¿‡ç«¯åˆ°ç«¯çš„æ¢¯åº¦åå‘ä¼ æ’­ã€‚Decoder çš„ç¿»è¯‘é”™è¯¯ä¼šè½¬åŒ–ä¸º Lossï¼Œæ¢¯åº¦é¡ºç€ Cross-Attention æµå› Encoderï¼Œå‘Šè¯‰å®ƒâ€œä½ æä¾›çš„ Memory è´¨é‡å¤ªå·®ï¼Œæ”¹ï¼â€) Cross-Attention çš„ QKV çŸ©é˜µæ˜¯éšæœºçš„å—ï¼Ÿ(ç­”ï¼šåˆå§‹æ˜¯éšæœºçš„ï¼Œä½†å®ƒä»¬èµ·åˆ°äº†â€œé€‚é…å™¨â€çš„ä½œç”¨ï¼Œè´Ÿè´£æŠŠè‹±è¯­çŠ¶æ€å’Œå¾·è¯­è®°å¿†æ˜ å°„åˆ°åŒä¸€ä¸ªè¯­ä¹‰ç©ºé—´ï¼Œä»¥ä¾¿è¿›è¡ŒåŒ¹é…ã€‚) ä¸ºä»€ä¹ˆ Encoder è¾“å…¥è¾“å‡ºç»´åº¦å¿…é¡»ä¸€è‡´ï¼Ÿ(ç­”ï¼šä¸ºäº†æ”¯æŒæ®‹å·®è¿æ¥ (Residual Connection)ï¼Œå…¬å¼ $x + SubLayer(x)$ è¦æ±‚ä¸¤è€…å½¢çŠ¶å¿…é¡»å®Œå…¨ç›¸åŒã€‚)","tags":["DeepLearning"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"å½»åº•ææ‡‚ QLoRAï¼šå¦‚ä½•ç”¨ 4-bit é‡åŒ–æŠ€æœ¯å•å¡å¾®è°ƒåƒäº¿æ¨¡å‹ï¼Ÿ","path":"/2026/01/25/qlora-deep-dive/","content":"åœ¨ä¸Šä¸€ç¯‡æ–‡ç« ä¸­ï¼Œæˆ‘ä»¬æ·±å…¥æ¢è®¨äº† LoRA çš„æ•°å­¦åŸç†ã€‚ä»Šå¤©ï¼Œæˆ‘ä»¬æ¥èŠèŠå®ƒçš„è¿›åŒ–ç‰ˆâ€”â€”**QLoRA (Quantized LoRA)**ã€‚ å¦‚æœä½ æƒ³åœ¨æ™®é€šçš„æ¶ˆè´¹çº§æ˜¾å¡ï¼ˆå¦‚ RTX 3060/4090ï¼‰ä¸Šå¾®è°ƒ 33B ç”šè‡³ 65B çš„å·¨å‹æ¨¡å‹ï¼ŒQLoRA æ˜¯ä½ å”¯ä¸€çš„æ•‘æ˜Ÿã€‚å®ƒé€šè¿‡â€œ4-bit æé™å‹ç¼©â€ï¼Œå°†æ˜¾å­˜éœ€æ±‚å†æ¬¡å‡åŠã€‚ æœ¬æ–‡å°†åŸºäºæ·±åº¦æŠ€æœ¯é—®ç­”ï¼Œå¸¦ä½ å½»åº•ææ‡‚ QLoRA çš„æ ¸å¿ƒæœºåˆ¶ã€‚ ä¸€ã€ QLoRA vs LoRAï¼šåˆ°åº•æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿä¸€å¥è¯æ€»ç»“ï¼š QLoRA = 4-bit é‡åŒ– (Quantization) + LoRA ã€‚ LoRA è§£å†³äº†â€œè®¡ç®—é‡â€çš„é—®é¢˜ï¼Œè€Œ QLoRA è§£å†³äº†â€œå­˜å‚¨é‡ï¼ˆæ˜¾å­˜ï¼‰â€çš„é—®é¢˜ã€‚ 1.1 æ˜¾å­˜å ç”¨çš„â€œé™ç»´æ‰“å‡»â€å‡è®¾æˆ‘ä»¬è¦å¾®è°ƒä¸€ä¸ª Llama-2-7B æ¨¡å‹ï¼Œæ˜¾å­˜è´¦å•å¦‚ä¸‹ï¼š å…¨é‡å¾®è°ƒ ï¼šçº¦ 112 GBï¼ˆå¿…é¡»ä¸Š A100 é›†ç¾¤ï¼‰ã€‚ LoRA (16-bit) ï¼šçº¦ 24 GBï¼ˆéœ€è¦ 3090/4090ï¼‰ã€‚ QLoRA (4-bit) ï¼š ä»…éœ€ 6 GB ï¼ˆRTX 3060 éƒ½èƒ½è·‘ï¼ï¼‰ã€‚ 1.2 æ ¸å¿ƒå…³ç³»è¡¨ ç‰¹æ€§ LoRA (æ ‡å‡†ç‰ˆ) QLoRA (é‡åŒ–ç‰ˆ) åº•åº§æ¨¡å‹ (Base Model) åŠ è½½ä¸º 16-bit (FP16) åŠ è½½ä¸º 4-bit (NF4) LoRA é€‚é…å™¨ (Adapter) 16-bit 16-bit (ä¿æŒç²¾åº¦) è®¡ç®—æ–¹å¼ çº¯ FP16 è®¡ç®— æ··åˆç²¾åº¦ (4-bit å­˜å‚¨ $\\to$ å®æ—¶è§£å‹ä¸º 16-bit è®¡ç®—) åˆ›æ–°ç‚¹ çŸ©é˜µåˆ†è§£ NF4 æ•°æ®ç±»å‹ + åŒé‡é‡åŒ– äºŒã€ QLoRA çš„ä¸‰å¤§æŠ€æœ¯åˆ›æ–° (The Magic)QLoRA ä¹‹æ‰€ä»¥èƒ½åœ¨å‹åˆ° 4-bit çš„åŒæ—¶è¿˜ä¸æ‰ç‚¹ï¼ˆç²¾åº¦æŸå¤±å¾®ä¹å…¶å¾®ï¼‰ï¼Œé çš„æ˜¯ä»¥ä¸‹ä¸‰ä¸ªé»‘ç§‘æŠ€ã€‚ 2.1 4-bit NormalFloat (NF4)ï¼šä¸ºæƒé‡å®šåˆ¶çš„å®¹å™¨ä¼ ç»Ÿçš„ 4-bit æ•´æ•°é‡åŒ– (Int4) æ˜¯å‡åŒ€åˆ‡åˆ†çš„ã€‚ä½†ç¥ç»ç½‘ç»œçš„æƒé‡åˆ†å¸ƒæ˜¯ æ­£æ€åˆ†å¸ƒ (Gaussian Distribution) â€”â€”å¤§éƒ¨åˆ†æ•°å€¼é›†ä¸­åœ¨ 0 é™„è¿‘ã€‚ Int4 (å‡åŒ€) ï¼šåœ¨ 0 é™„è¿‘åˆ‡åˆ†å¤ªç¨€ç–ï¼Œæµªè´¹äº†å¤§é‡ç²¾åº¦åœ¨æå€¼åŒºåŸŸï¼ˆEmpty Tailsï¼‰ã€‚ NF4 (åˆ†ä½æ•°) ï¼šæ ¹æ®æ­£æ€åˆ†å¸ƒè®¾è®¡åˆ»åº¦ï¼Œ åœ¨ 0 é™„è¿‘åˆ‡åˆ†æå¯† ã€‚ é€šä¿—ç†è§£ ï¼šInt4 æ˜¯ä¸€æŠŠåˆ»åº¦å‡åŒ€çš„ç›´å°ºï¼Œè€Œ NF4 æ˜¯ä¸€æŠŠä¸­é—´åˆ»åº¦æç»†ã€ä¸¤å¤´åˆ»åº¦ç¨€ç–çš„â€œå˜å½¢å°ºâ€ï¼Œä¸“é—¨ç”¨æ¥é‡æƒé‡è¿™ç§â€œä¸­é—´å¤šã€ä¸¤å¤´å°‘â€çš„ä¸œè¥¿ã€‚ 2.2 åŒé‡é‡åŒ– (Double Quantization)é‡åŒ–ä¸ä»…éœ€è¦å­˜æƒé‡ï¼Œè¿˜éœ€è¦å­˜ é‡åŒ–å¸¸æ•° (Scale Constants) ã€‚é€šå¸¸æ¯ 64 ä¸ªå‚æ•°å…±ç”¨ä¸€ä¸ª 32-bit çš„å¸¸æ•°ã€‚è™½ç„¶çœ‹èµ·æ¥ä¸å¤šï¼Œä½†åœ¨ 65B æ¨¡å‹ä¸‹ï¼Œå…‰è¿™äº›å¸¸æ•°å°±è¦å  3GB æ˜¾å­˜ï¼ QLoRA çš„åšæ³• ï¼š å¯¹â€œé‡åŒ–å¸¸æ•°â€å†è¿›è¡Œä¸€æ¬¡é‡åŒ–ã€‚ æƒé‡ $\\to$ 4-bitã€‚äº§ç”Ÿå¸¸æ•° $C_1$ (32-bit)ã€‚ $C_1$ $\\to$ 8-bitã€‚äº§ç”Ÿå¸¸æ•° $C_2$ã€‚ è¿™å°±åƒæŠŠå‹ç¼©åŒ…å†å‹ç¼©ä¸€æ¬¡ï¼Œå¹³å‡æ¯ä¸ªå‚æ•°åªå¤šå  0.127 bitã€‚ 2.3 åˆ†é¡µä¼˜åŒ–å™¨ (Paged Optimizers)åˆ©ç”¨ CPU å†…å­˜ (RAM) æ¥æ•‘æ€¥ã€‚å½“ GPU æ˜¾å­˜å‡ºç°å³°å€¼ï¼ˆSpikeï¼‰å¿«è¦ OOM æ—¶ï¼Œç³»ç»Ÿä¼šè‡ªåŠ¨æŠŠä¼˜åŒ–å™¨çŠ¶æ€ (Optimizer States) æ¬è¿åˆ° CPU å†…å­˜é‡Œï¼Œç­‰éœ€è¦æ›´æ–°å‚æ•°æ—¶å†æ¬å›æ¥ã€‚ ä¸‰ã€ QLoRA çš„å·¥ä½œæµï¼šå·¦å³äº’æQLoRA æœ€ç²¾å¦™çš„åœ°æ–¹åœ¨äºå®ƒçš„ æ··åˆç²¾åº¦è®¡ç®—æµ ã€‚å®ƒå®ç°äº†â€œç”¨ 4-bit å­˜ï¼Œç”¨ 16-bit ç®—â€ã€‚ 3.1 é™æ€å­˜å‚¨ vs åŠ¨æ€è®¡ç®— åº•åº§æ¨¡å‹ ï¼šåœ¨æ˜¾å­˜é‡Œæ˜¯ 4-bit (NF4)ã€‚ ç»å¯¹å†»ç»“ï¼Œåªè¯» ã€‚ LoRA é€‚é…å™¨ ï¼šåœ¨æ˜¾å­˜é‡Œæ˜¯ 16-bit (BF16)ã€‚ å¯è®­ç»ƒ ã€‚ 3.2 è®­ç»ƒæ—¶çš„æ•°æ®æµå½“æ•°æ®æµç»æŸä¸€å±‚æ—¶ï¼š è§£å‹ (Dequantize) ï¼šå°†åº•åº§çš„ 4-bit æƒé‡ $\\times$ é‡åŒ–å¸¸æ•° $\\rightarrow$ ç¬é—´è¿˜åŸä¸º 16-bit ã€‚ è®¡ç®— (Compute) ï¼š $X \\times W_{16bit}$ ã€‚ é‡Šæ”¾ (Discard) ï¼šè®¡ç®—å®Œå“ªæ€• 1 æ¯«ç§’åï¼Œç«‹åˆ»æ‰”æ‰ 16-bit æƒé‡ï¼Œæ˜¾å­˜é‡Œåªç•™ 4-bit ç‰ˆæœ¬ã€‚ åå‘ä¼ æ’­ ï¼šæ¢¯åº¦åªä¼ ç»™ LoRA éƒ¨åˆ†æ›´æ–°ã€‚ è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ QLoRA é€Ÿåº¦ä¼šæ…¢ 30% ï¼ˆå› ä¸ºè¦é¢‘ç¹è§£å‹ï¼‰ï¼Œä½† æ˜¾å­˜èƒ½çœ 60% ã€‚ å››ã€ å®æˆ˜ä»£ç  (bitsandbytes + peft)å¼€å¯ QLoRA åªéœ€è¦åœ¨åŠ è½½æ¨¡å‹æ—¶é…ç½® BitsAndBytesConfig ã€‚ import torchfrom transformers import AutoModelForCausalLM, BitsAndBytesConfigfrom peft import LoraConfig, get_peft_model# 1. QLoRA æ ¸å¿ƒé…ç½®bnb_config = BitsAndBytesConfig( load_in_4bit=True, # å¼€å¯ 4-bit åŠ è½½ bnb_4bit_quant_type=\"nf4\", # åˆ›æ–°1: ä½¿ç”¨ NF4 æ•°æ®ç±»å‹ bnb_4bit_use_double_quant=True, # åˆ›æ–°2: å¼€å¯åŒé‡é‡åŒ– bnb_4bit_compute_dtype=torch.float16 # åˆ›æ–°3: è®¡ç®—æ—¶è§£å‹ä¸º FP16)# 2. åŠ è½½åº•åº§ (æ˜¾å­˜å ç”¨æä½)model = AutoModelForCausalLM.from_pretrained( \"meta-llama/Llama-2-7b-hf\", quantization_config=bnb_config, device_map=\"auto\")# 3. åŠ è½½ LoRA (å’Œæ™®é€š LoRA ä¸€æ¨¡ä¸€æ ·)peft_config = LoraConfig( r=8, lora_alpha=16, target_modules=[\"q_proj\", \"v_proj\"], task_type=\"CAUSAL_LM\")model = get_peft_model(model, peft_config) äº”ã€ å¸¸è§è¯¯åŒºè§£ç­”Q: QLoRA ä¼šä¿®æ”¹åŸå§‹æ¨¡å‹æ–‡ä»¶å—ï¼ŸA: ç»å¯¹ä¸ä¼šã€‚ åŸå§‹æ¨¡å‹åœ¨ç¡¬ç›˜ä¸Šæ˜¯åªè¯»çš„ã€‚è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæ˜¾å­˜é‡Œçš„åº•åº§æ¨¡å‹ä¹Ÿæ˜¯å†»ç»“çš„ã€‚æˆ‘ä»¬åªè®­ç»ƒå¹¶ä¿å­˜é‚£å‡ ç™¾ MB çš„ LoRA æƒé‡ã€‚ Q: æ¨ç†æ—¶éœ€è¦è§£å‹å—ï¼ŸA: æ˜¯çš„ã€‚ æ¨ç†é€»è¾‘å’Œè®­ç»ƒä¸€æ ·ï¼šå®æ—¶è§£å‹ $\\rightarrow$ è®¡ç®— $\\rightarrow$ é‡Šæ”¾ã€‚å¦‚æœä½ æƒ³è¿½æ±‚æè‡´æ¨ç†é€Ÿåº¦ï¼Œå¯ä»¥æŠŠ LoRA åˆå¹¶åˆ°åº•åº§åï¼Œç»Ÿä¸€é‡åŒ–ä¸º GPTQ æˆ– AWQ æ ¼å¼ã€‚ Q: ä»€ä¹ˆæ˜¯é‡åŒ–å¸¸æ•°ï¼ŸA: å°±åƒåœ°å›¾çš„æ¯”ä¾‹å°ºã€‚4-bit åªèƒ½å­˜ 0~15 çš„æ•´æ•°ï¼Œé‡åŒ–å¸¸æ•°å‘Šè¯‰æˆ‘ä»¬â€œ1â€ä»£è¡¨å®é™…æƒé‡çš„â€œ0.005â€è¿˜æ˜¯â€œ100â€ã€‚æ²¡æœ‰å®ƒï¼Œæ•°æ®å°±æ˜¯åºŸçº¸ã€‚ å‚è€ƒæ–‡çŒ® Dettmers, T., et al. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. Hugging Face Blog. Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA.","tags":["NLP"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"æ·±å…¥æµ…å‡º LoRAï¼šå¤§æ¨¡å‹å¾®è°ƒçš„æ ¸å¿ƒåŸç†ã€æ•°å­¦è¯æ˜ä¸å®æˆ˜æŒ‡å—","path":"/2026/01/25/deep-dive-into-lora/","content":"åœ¨å¤§æ¨¡å‹ï¼ˆLLMï¼‰æ—¶ä»£ï¼Œ PEFT (Parameter-Efficient Fine-Tuning, å‚æ•°é«˜æ•ˆå¾®è°ƒ) å‡ ä¹æ˜¯æ¯ä¸€ä½å¼€å‘è€…å¿…é¡»æŒæ¡çš„æŠ€èƒ½ã€‚è€Œå…¶ä¸­æœ€è€€çœ¼çš„æ˜æ˜Ÿï¼Œè«è¿‡äº LoRA (Low-Rank Adaptation) ã€‚ æœ¬æ–‡å°†åŸºäºæˆ‘ä¸ AI çš„ä¸€æ¬¡æ·±åº¦å¯¹è¯ï¼Œç³»ç»Ÿåœ°æ¢³ç† LoRA çš„æ ¸å¿ƒåŸç†ã€‚æˆ‘ä»¬ä¸ä»…ä¼šç”¨ç›´è§‰å»ç†è§£å®ƒï¼Œæ›´ä¼šé€šè¿‡ æ•°å­¦è¯æ˜ å’Œ Python æ¨¡æ‹Ÿ ï¼Œå½»åº•ææ‡‚ä¸ºä»€ä¹ˆå®ƒéœ€è¦é™¤ä»¥ $r$ ï¼Œä»¥åŠå®ƒåœ¨åå‘ä¼ æ’­ä¸­æ˜¯å¦‚ä½•å·¥ä½œçš„ã€‚ ä¸€ã€ ä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦ LoRAï¼Ÿ1.1 â€œé‡å†™ç™¾ç§‘å…¨ä¹¦â€ vs â€œè´´ä¾¿åˆ©è´´â€æƒ³è±¡ GPT-4 æˆ– Llama-3 æ˜¯ä¸€æœ¬åšè¾¾ 1750 äº¿é¡µçš„ ç™¾ç§‘å…¨ä¹¦ ã€‚å¦‚æœä½ æƒ³è®©å®ƒå˜æˆä¸€ä¸ªâ€œæ³•å¾‹ä¸“å®¶â€ï¼š å…¨é‡å¾®è°ƒ (Full Fine-Tuning) ï¼šç›¸å½“äºä½ éœ€è¦æŠŠè¿™æœ¬ä¹¦çš„æ¯ä¸€é¡µéƒ½é‡æ–°ç¼–è¾‘ã€é‡æ–°å°åˆ·ã€‚ LoRA (Low-Rank Adaptation) ï¼šæˆ‘ä»¬åœ¨ä¸ç ´ååŸä¹¦ï¼ˆå†»ç»“å‚æ•°ï¼‰çš„æƒ…å†µä¸‹ï¼Œåªæ˜¯åœ¨ç›¸å…³çš„é¡µé¢æ—è¾¹è´´ä¸Šå‡ å¼  é€æ˜çš„ä¾¿åˆ©è´´ ï¼ˆLoRA æ¨¡å—ï¼‰ï¼Œä¸Šé¢å†™ç€ä¿®æ­£å†…å®¹ã€‚ 1.2 å‚æ•°æ•ˆç‡å¯¹æ¯”LoRA çš„æ ¸å¿ƒä¼˜åŠ¿åœ¨äºæè‡´çš„å‚æ•°å‹ç¼©ã€‚å¯¹äºä¸€ä¸ª 175B çš„æ¨¡å‹ï¼Œå…¨é‡å¾®è°ƒéœ€è¦æ›´æ–°æ‰€æœ‰å‚æ•°ï¼Œè€Œ LoRA ä»…éœ€æ›´æ–°çº¦ 0.01% çš„å‚æ•°ã€‚ äºŒã€ LoRA çš„æ•°å­¦åŸç† (The Math)2.1 çŸ©é˜µåˆ†è§£ï¼šæŠŠâ€œå¤§â€å˜â€œå°â€LoRA çš„æ ¸å¿ƒæ€æƒ³æ˜¯ ä½ç§©åˆ†è§£ (Low-Rank Decomposition) ã€‚å‡è®¾æ¨¡å‹ä¸­æœ‰ä¸€ä¸ªå·¨å¤§çš„æƒé‡çŸ©é˜µ $W \\in \\mathbb{R}^{d \\times d}$ ï¼Œæˆ‘ä»¬è¦å¾®è°ƒå®ƒï¼Œäº§ç”Ÿä¸€ä¸ªå¢é‡ $\\Delta W$ ã€‚ LoRA å‡è®¾è¿™ä¸ª $\\Delta W$ æ˜¯â€œä½ç§©â€çš„ï¼Œå¯ä»¥æ‹†åˆ†ä¸ºä¸¤ä¸ªå°çŸ©é˜µçš„ä¹˜ç§¯ï¼š$$ \\Delta W = B \\times A $$ çŸ©é˜µ A (é™ç»´) ï¼š $r \\times d$ ã€‚è´Ÿè´£æŠŠæ•°æ®â€œå‹æ‰â€ï¼Œæå–æ ¸å¿ƒç‰¹å¾ã€‚ çŸ©é˜µ B (å‡ç»´) ï¼š $d \\times r$ ã€‚è´Ÿè´£æŠŠæ•°æ®â€œè¿˜åŸâ€ï¼Œæ˜ å°„å›åŸç©ºé—´ã€‚ ç§© r (Rank) ï¼šé€šå¸¸å¾ˆå°ï¼ˆå¦‚ 8, 16, 64ï¼‰ã€‚ 2.2 ä¸ºä»€ä¹ˆå¿…é¡»é™¤ä»¥ rï¼Ÿ(å…³é”®è¯æ˜)åœ¨ LoRA çš„å…¬å¼ä¸­ï¼Œæœ‰ä¸€ä¸ªå…³é”®çš„ç¼©æ”¾ç³»æ•°ï¼š$$ y = W_0x + \\frac{\\alpha}{r} (BAx) $$ ä¸ºä»€ä¹ˆ $r$ ç¿»å€ï¼Œæ•°å€¼ä¼šç¿»å€ï¼Ÿå¦‚æœä¸é™¤ä»¥ $r$ ï¼Œä¼šå‘ç”Ÿä»€ä¹ˆï¼Ÿ è¯æ˜ä¸€ï¼šæ–¹å·®å åŠ  (Forward Pass Variance)å‡è®¾ LoRA å†…éƒ¨çš„å‚æ•° $A, B$ æœä»ç‹¬ç«‹åŒåˆ†å¸ƒï¼ˆI.I.Dï¼‰ï¼Œæ–¹å·®ä¸º $\\sigma^2$ ã€‚çŸ©é˜µä¹˜æ³•çš„æ¯ä¸€ä½è¾“å‡ºï¼Œæœ¬è´¨ä¸Šæ˜¯å¯¹ $r$ ä¸ªé€šé“çš„æ±‚å’Œï¼š$$ y_k = \\sum_{i=1}^{r} (B_{ki} A_{ij} x_j) $$ æ ¹æ®ç»Ÿè®¡å­¦å®šå¾‹ï¼ˆç›¸äº’ç‹¬ç«‹çš„éšæœºå˜é‡ä¹‹å’Œçš„æ–¹å·®ç­‰äºå®ƒä»¬æ–¹å·®ä¹‹å’Œï¼‰ï¼š$$ \\text{Var}(y) \\propto r \\cdot \\sigma^2 $$ è¿™æ„å‘³ç€ï¼Œä¿¡å·çš„æ³¢åŠ¨å¹…åº¦ï¼ˆæ–¹å·®ï¼‰ä¼šéšç€ $r$ çš„å¢å¤§è€Œçº¿æ€§è†¨èƒ€ã€‚å¦‚æœä¸åŠ ä»¥æ§åˆ¶ï¼Œè¾“å‡ºå€¼ä¼šå˜å¾—æå…¶ä¸ç¨³å®šã€‚ è¯æ˜äºŒï¼šæ¢¯åº¦ç¨³å®šæ€§ (Gradient Stability)è¿™æ‰æ˜¯æœ€è‡´å‘½çš„é—®é¢˜ã€‚å¦‚æœå‰å‘ä¼ æ’­çš„å€¼å˜å¤§äº†ï¼Œ åå‘ä¼ æ’­çš„æ¢¯åº¦ä¹Ÿä¼šå˜å¤§ ã€‚ æˆ‘ä»¬ç”¨ Python æ¨¡æ‹Ÿäº†ä¸åŒ $r$ å€¼ä¸‹çš„æ¢¯åº¦èŒƒæ•°ï¼ˆGradient Normï¼‰ï¼š çº¢çº¿ (Without Scaling) ï¼šéšç€ $r$ å¢å¤§ï¼Œæ¢¯åº¦å‘ˆæŒ‡æ•°çº§çˆ†ç‚¸ã€‚è¿™æ„å‘³ç€å¦‚æœä½ æŠŠ $r$ ä» 8 æ”¹æˆ 64ï¼Œä½ å¿…é¡»æ‰‹åŠ¨æŠŠå­¦ä¹ ç‡ç¼©å° 8 å€ï¼Œå¦åˆ™æ¨¡å‹ç›´æ¥å´©æºƒã€‚ ç»¿çº¿ (With 1/r Scaling) ï¼šæ— è®º $r$ æ€ä¹ˆå˜ï¼Œæ¢¯åº¦å¤§å°ä¿æŒæ’å®šã€‚è¿™å®ç°äº† å­¦ä¹ ç‡è§£è€¦ (Learning Rate Decoupling) â€”â€” ä¸€å¥—è¶…å‚æ•°èµ°å¤©ä¸‹ã€‚ 2.3 åˆå§‹åŒ–çš„è‰ºæœ¯LoRA çš„åˆå§‹åŒ–ç­–ç•¥éå¸¸è®²ç©¶ï¼š çŸ©é˜µ A ï¼š é«˜æ–¯éšæœºåˆå§‹åŒ– ã€‚ åŸå› ï¼šå¿…é¡»æ‰“ç ´å¯¹ç§°æ€§ï¼Œè®©æ¢¯åº¦èƒ½å¤ŸæµåŠ¨ã€‚ çŸ©é˜µ B ï¼š å…¨é›¶åˆå§‹åŒ– ã€‚ åŸå› ï¼šä¿è¯åœ¨è®­ç»ƒå¼€å§‹çš„ä¸€ç¬é—´ï¼ˆStep 0ï¼‰ï¼Œ$BAx = 0$ ã€‚ è¿™å°±åƒä¸€ä¸ªé˜€é—¨ï¼šè™½ç„¶ A é‡Œå·²ç»å……æ»¡äº†éšæœºå™ªå£°ï¼ˆæ°´æµï¼‰ï¼Œä½† B è¿™ä¸ªé˜€é—¨å…³ç€ï¼Œæ‰€ä»¥å¯¹åŸæ¨¡å‹æ²¡æœ‰ä»»ä½•å¹²æ‰°ã€‚ ä¸‰ã€ å®æˆ˜ï¼šå¾®è°ƒæ•°æ®æ ¼å¼3.1 Input / Output å¯¹åº”å…³ç³»åœ¨å¾®è°ƒå¤§æ¨¡å‹æ—¶ï¼Œæ ¸å¿ƒé€»è¾‘å¯¹åº”ç€ç›‘ç£å­¦ä¹ çš„ $X$ å’Œ $Y$ ã€‚ Instruction + Input $\\rightarrow$ X (æ¨¡å‹è¾“å…¥) Output $\\rightarrow$ Y (é¢„æœŸè¾“å‡º) { \"instruction\": \"è¯·åˆ†æä»¥ä¸‹æ¡ˆæƒ…ä¸­çš„æ³•å¾‹è´£ä»»ã€‚\", \"input\": \"å¼ ä¸‰åœ¨å–é…’åé©¾é©¶æœºåŠ¨è½¦...\", \"output\": \"å¼ ä¸‰çš„è¡Œä¸ºæ„æˆå±é™©é©¾é©¶ç½ª...\"} 3.2 System Promptâ€œSystem Promptâ€ï¼ˆå¦‚ â€œYou are a helpful assistantâ€¦â€ï¼‰é€šå¸¸ä¸ç›´æ¥å‡ºç°åœ¨ JSON é‡Œçš„å­—æ®µä¸­ï¼Œè€Œæ˜¯ä½œä¸º Template çš„ä¸€éƒ¨åˆ†ï¼Œæ‹¼æ¥åœ¨ Instruction ä¹‹å‰ã€‚å®ƒå……å½“äº† â€œèƒŒæ™¯è®¾å®šâ€ çš„è§’è‰²ã€‚ å››ã€ æ€»ç»“LoRA æ˜¯å¤§æ¨¡å‹å¾®è°ƒé¢†åŸŸçš„é‡Œç¨‹ç¢‘ã€‚å®ƒä¸ä»…ä»…æ˜¯ä¸€ä¸ªçœæ˜¾å­˜çš„å·¥å…·ï¼Œæ›´æ˜¯ä¸€å¥—ä¼˜é›…çš„æ•°å­¦è§£å†³æ–¹æ¡ˆã€‚ ç§© (Rank) ï¼šå†³å®šäº†æ¨¡å‹çš„â€œè„‘å®¹é‡â€ã€‚ $\\alpha/r$ (ç¼©æ”¾) ï¼šä¿è¯äº†è®­ç»ƒåŠ¨åŠ›å­¦çš„ä¸€è‡´æ€§ï¼Œé˜²æ­¢æ¢¯åº¦çˆ†ç‚¸ã€‚ é›¶åˆå§‹åŒ– ï¼šä¿è¯äº†å¾®è°ƒçš„å¹³æ»‘å¯åŠ¨ã€‚ å¸Œæœ›è¿™ç¯‡æ–‡ç« èƒ½å¸®ä½ å½»åº•ç†è§£ LoRA çš„æ•°å­¦ç›´è§‰ä¸å·¥ç¨‹å®è·µã€‚ å‚è€ƒæ–‡çŒ® Hu, E. J., et al. (2021). LoRA: Low-Rank Adaptation of Large Language Models. Vaswani, A., et al. (2017). Attention Is All You Need.","tags":["NLP"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"ç›¸æœºå‡ ä½•å®Œå…¨æŒ‡å—ï¼šä»ä¸–ç•Œåæ ‡åˆ°å›¾åƒåæ ‡çš„å®Œæ•´æ¨å¯¼","path":"/2026/01/19/camera-geometry-complete-guide/","content":"æœ¬æ–‡è¯¦ç»†æ¨å¯¼ä»3Dä¸–ç•Œåæ ‡ç³»åˆ°2Då›¾åƒåæ ‡ç³»çš„å®Œæ•´æ•°å­¦è¿‡ç¨‹ï¼ŒåŒ…æ‹¬ç›¸æœºå†…å¤–å‚æ•°ã€æ—‹è½¬çŸ©é˜µã€å•åº”çŸ©é˜µçš„æ¨å¯¼ä¸åˆ†è§£ï¼Œå¹¶æä¾›å®Œæ•´çš„Pythonå¯è§†åŒ–ä»£ç ã€‚ ğŸ“– ç›®å½• åæ ‡ç³»ç»Ÿæ¦‚è¿° ä»ä¸–ç•Œåæ ‡ç³»åˆ°ç›¸æœºåæ ‡ç³»ï¼ˆå¤–å‚ï¼‰ ä»ç›¸æœºåæ ‡ç³»åˆ°å›¾åƒåæ ‡ç³»ï¼ˆå†…å‚ï¼‰ æ—‹è½¬çŸ©é˜µè¯¦è§£ å•åº”çŸ©é˜µæ¨å¯¼ ç›¸æœºçŸ©é˜µåˆ†è§£ Pythonå®Œæ•´å®ç° ä¸€ã€åæ ‡ç³»ç»Ÿæ¦‚è¿°åœ¨è®¡ç®—æœºè§†è§‰ä¸­ï¼Œä»3Dä¸–ç•Œåˆ°2Då›¾åƒéœ€è¦ç»è¿‡å››ä¸ªåæ ‡ç³»ç»Ÿçš„è½¬æ¢ï¼š 1.1 å››ä¸ªåæ ‡ç³»ç»Ÿ1. ä¸–ç•Œåæ ‡ç³» (World Coordinate System) ç¬¦å·ï¼š$(X_w, Y_w, Z_w)$ æè¿°ï¼šçœŸå®ä¸–ç•Œä¸­çš„3Dåæ ‡ç³»ç»Ÿ å•ä½ï¼šé€šå¸¸ä¸ºç±³(m)æˆ–æ¯«ç±³(mm) åŸç‚¹ï¼šä»»æ„é€‰å®šçš„å‚è€ƒç‚¹ 2. ç›¸æœºåæ ‡ç³» (Camera Coordinate System) ç¬¦å·ï¼š$(X_c, Y_c, Z_c)$ æè¿°ï¼šä»¥ç›¸æœºå…‰å¿ƒä¸ºåŸç‚¹çš„3Dåæ ‡ç³» å•ä½ï¼šç±³(m)æˆ–æ¯«ç±³(mm) åŸç‚¹ï¼šç›¸æœºå…‰å¿ƒ ç‰¹ç‚¹ï¼š$Z_c$ è½´ä¸ºå…‰è½´æ–¹å‘ 3. å›¾åƒåæ ‡ç³» (Image Coordinate System) ç¬¦å·ï¼š$(x, y)$ æè¿°ï¼šæˆåƒå¹³é¢ä¸Šçš„ç‰©ç†åæ ‡ å•ä½ï¼šæ¯«ç±³(mm) åŸç‚¹ï¼šå›¾åƒä¸­å¿ƒï¼ˆä¸»ç‚¹ï¼‰ 4. åƒç´ åæ ‡ç³» (Pixel Coordinate System) ç¬¦å·ï¼š$(u, v)$ æè¿°ï¼šæ•°å­—å›¾åƒçš„ç¦»æ•£åƒç´ åæ ‡ å•ä½ï¼šåƒç´ (pixel) åŸç‚¹ï¼šå›¾åƒå·¦ä¸Šè§’ 1.2 å®Œæ•´çš„æŠ•å½±å…¬å¼ä»ä¸–ç•Œåæ ‡åˆ°åƒç´ åæ ‡çš„å®Œæ•´å˜æ¢ï¼š $$\\begin{bmatrix} u \\ v \\ 1 \\end{bmatrix} \\sim K \\cdot [R|t] \\cdot \\begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \\end{bmatrix}$$ å…¶ä¸­ï¼š $K$ï¼šå†…å‚çŸ©é˜µ (3Ã—3) $[R|t]$ï¼šå¤–å‚çŸ©é˜µ (3Ã—4) $\\sim$ï¼šè¡¨ç¤ºé½æ¬¡åæ ‡æ„ä¹‰ä¸‹çš„ç›¸ç­‰ï¼ˆå·®ä¸€ä¸ªå°ºåº¦å› å­ï¼‰ äºŒã€ä»ä¸–ç•Œåæ ‡ç³»åˆ°ç›¸æœºåæ ‡ç³»ï¼ˆå¤–å‚çŸ©é˜µï¼‰2.1 åˆšä½“å˜æ¢ä¸–ç•Œåæ ‡ç³»åˆ°ç›¸æœºåæ ‡ç³»çš„è½¬æ¢æ˜¯ä¸€ä¸ªåˆšä½“å˜æ¢ï¼ˆRigid Body Transformationï¼‰ï¼ŒåŒ…å«æ—‹è½¬å’Œå¹³ç§»ï¼š $$\\begin{bmatrix} X_c \\ Y_c \\ Z_c \\end{bmatrix} = R \\begin{bmatrix} X_w \\ Y_w \\ Z_w \\end{bmatrix} + t$$ å…¶ä¸­ï¼š $R \\in \\mathbb{R}^{3 \\times 3}$ï¼šæ—‹è½¬çŸ©é˜µï¼ˆRotation Matrixï¼‰ $t \\in \\mathbb{R}^{3 \\times 1}$ï¼šå¹³ç§»å‘é‡ï¼ˆTranslation Vectorï¼‰ 2.2 é½æ¬¡åæ ‡è¡¨ç¤ºä½¿ç”¨é½æ¬¡åæ ‡å¯ä»¥å°†æ—‹è½¬å’Œå¹³ç§»ç»Ÿä¸€è¡¨ç¤ºï¼š $$\\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \\end{bmatrix} =\\begin{bmatrix}R t \\0^T 1\\end{bmatrix}\\begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \\end{bmatrix}$$ åœ¨å®é™…åº”ç”¨ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸ä½¿ç”¨ $3 \\times 4$ çš„å¤–å‚çŸ©é˜µï¼š $$\\begin{bmatrix} X_c \\ Y_c \\ Z_c \\end{bmatrix} =[R|t] \\begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \\end{bmatrix}$$ å…¶ä¸­ï¼š $$[R|t] = \\begin{bmatrix}r_{11} r_{12} r_{13} t_x \\r_{21} r_{22} r_{23} t_y \\r_{31} r_{32} r_{33} t_z\\end{bmatrix}$$ 2.3 å¤–å‚çš„ç‰©ç†æ„ä¹‰ **æ—‹è½¬çŸ©é˜µ $R$**ï¼šæè¿°ç›¸æœºåæ ‡ç³»ç›¸å¯¹äºä¸–ç•Œåæ ‡ç³»çš„æ–¹å‘ **å¹³ç§»å‘é‡ $t$**ï¼šæè¿°ç›¸æœºå…‰å¿ƒåœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„ä½ç½® è‡ªç”±åº¦ï¼š6ä¸ªï¼ˆ3ä¸ªæ—‹è½¬ + 3ä¸ªå¹³ç§»ï¼‰ 2.4 å¤–å‚çš„é€†å˜æ¢ä»ç›¸æœºåæ ‡ç³»å›åˆ°ä¸–ç•Œåæ ‡ç³»ï¼š $$\\begin{bmatrix} X_w \\ Y_w \\ Z_w \\end{bmatrix} = R^T \\left( \\begin{bmatrix} X_c \\ Y_c \\ Z_c \\end{bmatrix} - t \\right) = R^T \\begin{bmatrix} X_c \\ Y_c \\ Z_c \\end{bmatrix} - R^T t$$ æ³¨æ„ï¼š $R^T = R^{-1}$ï¼ˆæ—‹è½¬çŸ©é˜µçš„è½¬ç½®ç­‰äºå…¶é€†ï¼‰ ç›¸æœºåœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„ä½ç½®ä¸º $C = -R^T t$ ä¸‰ã€ä»ç›¸æœºåæ ‡ç³»åˆ°å›¾åƒåæ ‡ç³»ï¼ˆå†…å‚çŸ©é˜µï¼‰3.1 é’ˆå­”ç›¸æœºæ¨¡å‹é’ˆå­”ç›¸æœºæ¨¡å‹æ˜¯æœ€åŸºæœ¬çš„ç›¸æœºæ¨¡å‹ï¼š å…‰å¿ƒ O â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ æˆåƒå¹³é¢ â†— â†— 3Dç‚¹ P æŠ•å½±ç‚¹ p ç›¸ä¼¼ä¸‰è§’å½¢: x/f = Xc/Zc, y/f = Yc/Zcå› æ­¤: x = fÂ·(Xc/Zc), y = fÂ·(Yc/Zc) é€è§†æŠ•å½±å…¬å¼æ ¹æ®ç›¸ä¼¼ä¸‰è§’å½¢åŸç†ï¼š $$\\frac{x}{f} = \\frac{X_c}{Z_c}, \\quad \\frac{y}{f} = \\frac{Y_c}{Z_c}$$ å…¶ä¸­ $f$ æ˜¯ç„¦è·ï¼ˆfocal lengthï¼‰ï¼Œå•ä½ä¸ºæ¯«ç±³ã€‚ å› æ­¤ï¼š $$x = f \\frac{X_c}{Z_c}, \\quad y = f \\frac{Y_c}{Z_c}$$ 3.2 ä»å›¾åƒåæ ‡åˆ°åƒç´ åæ ‡å›¾åƒåæ ‡ $(x, y)$ æ˜¯ç‰©ç†åæ ‡ï¼ˆæ¯«ç±³ï¼‰ï¼Œéœ€è¦è½¬æ¢ä¸ºåƒç´ åæ ‡ $(u, v)$ï¼š $$\\begin{cases}u = \\alpha x + c_x \\v = \\beta y + c_y\\end{cases}$$ å…¶ä¸­ï¼š $\\alpha = \\frac{1}{dx}$ï¼šxæ–¹å‘çš„åƒç´ å¯†åº¦ï¼ˆåƒç´ /æ¯«ç±³ï¼‰ $\\beta = \\frac{1}{dy}$ï¼šyæ–¹å‘çš„åƒç´ å¯†åº¦ï¼ˆåƒç´ /æ¯«ç±³ï¼‰ $(c_x, c_y)$ï¼šä¸»ç‚¹åæ ‡ï¼ˆå›¾åƒä¸­å¿ƒåœ¨åƒç´ åæ ‡ç³»ä¸­çš„ä½ç½®ï¼‰ 3.3 å†…å‚çŸ©é˜µæ¨å¯¼å°†ä¸Šè¿°ä¸¤æ­¥åˆå¹¶ï¼š $$\\begin{aligned}u = \\alpha \\cdot f \\frac{X_c}{Z_c} + c_x = f_x \\frac{X_c}{Z_c} + c_x \\v = \\beta \\cdot f \\frac{Y_c}{Z_c} + c_y = f_y \\frac{Y_c}{Z_c} + c_y\\end{aligned}$$ å…¶ä¸­ï¼š $f_x = \\alpha \\cdot f$ï¼šxæ–¹å‘ç„¦è·ï¼ˆåƒç´ å•ä½ï¼‰ $f_y = \\beta \\cdot f$ï¼šyæ–¹å‘ç„¦è·ï¼ˆåƒç´ å•ä½ï¼‰ ä½¿ç”¨é½æ¬¡åæ ‡è¡¨ç¤ºï¼š $$Z_c \\begin{bmatrix} u \\ v \\ 1 \\end{bmatrix} =\\begin{bmatrix}f_x 0 c_x \\0 f_y c_y \\0 0 1\\end{bmatrix}\\begin{bmatrix} X_c \\ Y_c \\ Z_c \\end{bmatrix}$$ 3.4 å®Œæ•´çš„å†…å‚çŸ©é˜µè€ƒè™‘åƒç´ å€¾æ–œï¼ˆskewï¼‰çš„ä¸€èˆ¬å½¢å¼ï¼š $$K = \\begin{bmatrix}f_x s c_x \\0 f_y c_y \\0 0 1\\end{bmatrix}$$ å…¶ä¸­ï¼š $f_x, f_y$ï¼šç„¦è·ï¼ˆåƒç´ å•ä½ï¼‰ $c_x, c_y$ï¼šä¸»ç‚¹åæ ‡ï¼ˆåƒç´ ï¼‰ $s$ï¼šå€¾æ–œç³»æ•°ï¼ˆé€šå¸¸ä¸º0ï¼‰ 3.5 å†…å‚çŸ©é˜µçš„æ€§è´¨ ç»´åº¦ï¼š$3 \\times 3$ è‡ªç”±åº¦ï¼š5ä¸ªï¼ˆç°ä»£ç›¸æœºä¸­$s=0$ï¼Œåˆ™ä¸º4ä¸ªï¼‰ ç‰¹ç‚¹ï¼šä¸Šä¸‰è§’çŸ©é˜µ ç‰©ç†æ„ä¹‰ï¼šæè¿°ç›¸æœºçš„å†…éƒ¨å‡ ä½•ç‰¹æ€§ å››ã€æ—‹è½¬çŸ©é˜µè¯¦è§£4.1 æ—‹è½¬çŸ©é˜µçš„å®šä¹‰ä¸æ€§è´¨æ—‹è½¬çŸ©é˜µ $R \\in SO(3)$ æ˜¯ä¸€ä¸ªç‰¹æ®Šæ­£äº¤çŸ©é˜µï¼Œæ»¡è¶³ï¼š æ­£äº¤æ€§ï¼š$R^T R = R R^T = I$ è¡Œåˆ—å¼ï¼š$\\det(R) = 1$ ä¿æŒé•¿åº¦ï¼š$|Rv| = |v|$ ä¿æŒè§’åº¦ï¼š$(Rv_1) \\cdot (Rv_2) = v_1 \\cdot v_2$ 4.2 åŸºæœ¬æ—‹è½¬çŸ©é˜µ ç»•Xè½´æ—‹è½¬ï¼ˆRollï¼‰$$R_x(\\alpha) = \\begin{bmatrix}1 0 0 \\0 \\cos\\alpha -\\sin\\alpha \\0 \\sin\\alpha \\cos\\alpha\\end{bmatrix}$$ ç»•Yè½´æ—‹è½¬ï¼ˆPitchï¼‰$$R_y(\\beta) = \\begin{bmatrix}\\cos\\beta 0 \\sin\\beta \\0 1 0 \\-\\sin\\beta 0 \\cos\\beta\\end{bmatrix}$$ ç»•Zè½´æ—‹è½¬ï¼ˆYawï¼‰$$R_z(\\gamma) = \\begin{bmatrix}\\cos\\gamma -\\sin\\gamma 0 \\\\sin\\gamma \\cos\\gamma 0 \\0 0 1\\end{bmatrix}$$ 4.3 æ¬§æ‹‰è§’è¡¨ç¤ºä»»æ„æ—‹è½¬å¯ä»¥åˆ†è§£ä¸ºä¸‰ä¸ªåŸºæœ¬æ—‹è½¬çš„ç»„åˆï¼ˆæœ‰å¤šç§é¡ºåºï¼‰ï¼š ZYXæ¬§æ‹‰è§’ï¼ˆå¸¸ç”¨ï¼‰ï¼š $$R = R_z(\\gamma) R_y(\\beta) R_x(\\alpha)$$ å±•å¼€ä¸ºï¼š $$R = \\begin{bmatrix}\\cos\\gamma\\cos\\beta \\cos\\gamma\\sin\\beta\\sin\\alpha - \\sin\\gamma\\cos\\alpha \\cos\\gamma\\sin\\beta\\cos\\alpha + \\sin\\gamma\\sin\\alpha \\\\sin\\gamma\\cos\\beta \\sin\\gamma\\sin\\beta\\sin\\alpha + \\cos\\gamma\\cos\\alpha \\sin\\gamma\\sin\\beta\\cos\\alpha - \\cos\\gamma\\sin\\alpha \\-\\sin\\beta \\cos\\beta\\sin\\alpha \\cos\\beta\\cos\\alpha\\end{bmatrix}$$ âš ï¸ ä¸‡å‘é”é—®é¢˜ï¼šå½“ $\\beta = \\pm 90Â°$ æ—¶ï¼Œä¼šå‡ºç°ä¸‡å‘é”ï¼ˆGimbal Lockï¼‰ã€‚ 4.4 è½´è§’è¡¨ç¤ºï¼ˆAxis-Angleï¼‰ç”¨æ—‹è½¬è½´ $\\mathbf{n} = (n_x, n_y, n_z)^T$ï¼ˆå•ä½å‘é‡ï¼‰å’Œæ—‹è½¬è§’ $\\theta$ è¡¨ç¤ºæ—‹è½¬ã€‚ ç½—å¾·é‡Œæ ¼æ–¯å…¬å¼ï¼ˆRodriguesâ€™ Formulaï¼‰ï¼š $$R = I + \\sin\\theta [\\mathbf{n}]_\\times + (1-\\cos\\theta)[\\mathbf{n}]_\\times^2$$ å…¶ä¸­ $[\\mathbf{n}]_\\times$ æ˜¯åå¯¹ç§°çŸ©é˜µï¼š $$[\\mathbf{n}]_\\times = \\begin{bmatrix}0 -n_z n_y _z 0 -n_x \\-n_y n_x 0\\end{bmatrix}$$ 4.5 å››å…ƒæ•°è¡¨ç¤ºï¼ˆQuaternionï¼‰å››å…ƒæ•° $q = q_0 + q_1i + q_2j + q_3k$ å¯ä»¥é¿å…ä¸‡å‘é”ï¼Œå…¶ä¸­ $q_0^2 + q_1^2 + q_2^2 + q_3^2 = 1$ã€‚ å››å…ƒæ•°åˆ°æ—‹è½¬çŸ©é˜µï¼š $$R = \\begin{bmatrix}1-2(q_2^2+q_3^2) 2(q_1q_2-q_0q_3) 2(q_1q_3+q_0q_2) \\2(q_1q_2+q_0q_3) 1-2(q_1^2+q_3^2) 2(q_2q_3-q_0q_1) \\2(q_1q_3-q_0q_2) 2(q_2q_3+q_0q_1) 1-2(q_1^2+q_2^2)\\end{bmatrix}$$ 4.6 æ—‹è½¬çŸ©é˜µçš„åˆ—å‘é‡å«ä¹‰ â­æ—‹è½¬çŸ©é˜µ $R$ çš„åˆ—å‘é‡å…·æœ‰é‡è¦çš„å‡ ä½•æ„ä¹‰ï¼š $$R = \\begin{bmatrix} | | | \\ \\mathbf{r}_1 \\mathbf{r}_2 \\mathbf{r}_3 \\ | | | \\end{bmatrix}$$ æ ¸å¿ƒç†è§£ï¼š $R$ çš„ç¬¬ $i$ åˆ— $\\mathbf{r}_i$ è¡¨ç¤ºä¸–ç•Œåæ ‡ç³»çš„ç¬¬ $i$ ä¸ªåŸºå‘é‡åœ¨ç›¸æœºåæ ‡ç³»ä¸‹çš„è¡¨ç¤ºã€‚ å…·ä½“æ¥è¯´ï¼š **ç¬¬1åˆ— $\\mathbf{r}_1$**ï¼šä¸–ç•Œåæ ‡ç³»çš„ X è½´æ–¹å‘åœ¨ç›¸æœºåæ ‡ç³»ä¸­çš„è¡¨ç¤º **ç¬¬2åˆ— $\\mathbf{r}_2$**ï¼šä¸–ç•Œåæ ‡ç³»çš„ Y è½´æ–¹å‘åœ¨ç›¸æœºåæ ‡ç³»ä¸­çš„è¡¨ç¤º **ç¬¬3åˆ— $\\mathbf{r}_3$**ï¼šä¸–ç•Œåæ ‡ç³»çš„ Z è½´æ–¹å‘åœ¨ç›¸æœºåæ ‡ç³»ä¸­çš„è¡¨ç¤º æ¨å¯¼ï¼š ä¸–ç•Œåæ ‡ç³»çš„åŸºå‘é‡ä¸ºï¼š$$\\mathbf{e}_1 = \\begin{bmatrix}1\\0\\0\\end{bmatrix}, \\quad\\mathbf{e}_2 = \\begin{bmatrix}0\\1\\0\\end{bmatrix}, \\quad\\mathbf{e}_3 = \\begin{bmatrix}0\\0\\1\\end{bmatrix}$$ åœ¨ç›¸æœºåæ ‡ç³»ä¸­ï¼š$$R\\mathbf{e}_1 = \\mathbf{r}_1, \\quad R\\mathbf{e}_2 = \\mathbf{r}_2, \\quad R\\mathbf{e}_3 = \\mathbf{r}_3$$ ç¤ºä¾‹ï¼š å‡è®¾ï¼š$$R = \\begin{bmatrix}0.866 -0.500 0 \\0.500 0.866 0 \\0 0 1\\end{bmatrix}$$ è¿™æ˜¯ç»•Zè½´æ—‹è½¬30Â°çš„æ—‹è½¬çŸ©é˜µã€‚ $\\mathbf{r}_1 = [0.866, 0.500, 0]^T$ï¼šä¸–ç•ŒXè½´åœ¨ç›¸æœºç³»ä¸­æŒ‡å‘ $(0.866, 0.500, 0)$ $\\mathbf{r}_2 = [-0.500, 0.866, 0]^T$ï¼šä¸–ç•ŒYè½´åœ¨ç›¸æœºç³»ä¸­æŒ‡å‘ $(-0.500, 0.866, 0)$ $\\mathbf{r}_3 = [0, 0, 1]^T$ï¼šä¸–ç•ŒZè½´åœ¨ç›¸æœºç³»ä¸­ä»æŒ‡å‘ $(0, 0, 1)$ 4.7 æ—‹è½¬çŸ©é˜µçš„è¡Œå‘é‡å«ä¹‰ç›¸ååœ°ï¼Œ$R^T$ çš„åˆ—ï¼ˆå³ $R$ çš„è¡Œï¼‰è¡¨ç¤ºç›¸æœºåæ ‡ç³»çš„åŸºå‘é‡åœ¨ä¸–ç•Œåæ ‡ç³»ä¸‹çš„è¡¨ç¤ºï¼š $$R^T = \\begin{bmatrix} \\mathbf{r}_1^T - \\ \\mathbf{r}_2^T - \\ \\mathbf{r}_3^T -\\end{bmatrix}$$ ç”±äº $R^T = R^{-1}$ï¼Œæˆ‘ä»¬æœ‰ï¼š $\\mathbf{r}_1^T$ï¼šç›¸æœºXè½´åœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„æ–¹å‘ $\\mathbf{r}_2^T$ï¼šç›¸æœºYè½´åœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„æ–¹å‘ $\\mathbf{r}_3^T$ï¼šç›¸æœºZè½´ï¼ˆå…‰è½´ï¼‰åœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„æ–¹å‘ äº”ã€å•åº”çŸ©é˜µæ¨å¯¼ 5.1 å•åº”çŸ©é˜µçš„å®šä¹‰å•åº”çŸ©é˜µï¼ˆHomography Matrixï¼‰$H$ æè¿°ä¸¤ä¸ªå¹³é¢ä¹‹é—´çš„æŠ•å½±å˜æ¢å…³ç³»ï¼š $$\\mathbf{p}â€™ \\sim H \\mathbf{p}$$ å³ï¼š $$\\begin{bmatrix} xâ€™ \\ yâ€™ \\ 1 \\end{bmatrix} \\sim\\begin{bmatrix}h_{11} h_{12} h_{13} \\h_{21} h_{22} h_{23} \\h_{31} h_{32} h_{33}\\end{bmatrix}\\begin{bmatrix} x \\ y \\ 1 \\end{bmatrix}$$ 5.2 å•åº”çŸ©é˜µçš„æ¨å¯¼åœºæ™¯1ï¼šå¹³é¢ç‰©ä½“çš„æŠ•å½±å‡è®¾ä¸–ç•Œåæ ‡ç³»ä¸­çš„å¹³é¢æ»¡è¶³ $Z_w = 0$ï¼Œåˆ™ï¼š $$\\begin{bmatrix} X_c \\ Y_c \\ Z_c \\end{bmatrix} =R \\begin{bmatrix} X_w \\ Y_w \\ 0 \\end{bmatrix} + t =\\begin{bmatrix} r_1 r_2 r_3 \\end{bmatrix}\\begin{bmatrix} X_w \\ Y_w \\ 0 \\end{bmatrix} + t$$ $$= \\begin{bmatrix} r_1 r_2 \\end{bmatrix} \\begin{bmatrix} X_w \\ Y_w \\end{bmatrix} + t= [r_1 ; r_2 ; t] \\begin{bmatrix} X_w \\ Y_w \\ 1 \\end{bmatrix}$$ å› æ­¤åƒç´ åæ ‡ä¸ºï¼š $$\\begin{bmatrix} u \\ v \\ 1 \\end{bmatrix} \\simK [r_1 ; r_2 ; t] \\begin{bmatrix} X_w \\ Y_w \\ 1 \\end{bmatrix}$$ å®šä¹‰å•åº”çŸ©é˜µï¼š $$H = K [r_1 ; r_2 ; t] = K \\begin{bmatrix} | | | \\ r_1 r_2 t \\ | | | \\end{bmatrix}$$ 5.3 å•åº”çŸ©é˜µçš„æ€§è´¨ ç»´åº¦ï¼š$3 \\times 3$ è‡ªç”±åº¦ï¼š8ï¼ˆå› ä¸ºå°ºåº¦ä¸å®šæ€§ï¼Œ9ä¸ªå…ƒç´ å‡å»1ä¸ªå°ºåº¦ï¼‰ å¯é€†ï¼š$H^{-1}$ æè¿°é€†å‘æ˜ å°„ éçº¿æ€§ï¼šç”±äºé½æ¬¡åæ ‡çš„å°ºåº¦ä¸å®šæ€§ 5.4 å•åº”çŸ©é˜µçš„æ±‚è§£ç»™å®š $n$ å¯¹å¯¹åº”ç‚¹ $(x_i, y_i) \\leftrightarrow (x_iâ€™, y_iâ€™)$ï¼Œæ¯å¯¹ç‚¹æä¾›2ä¸ªçº¦æŸæ–¹ç¨‹ï¼š $$\\begin{aligned}x_iâ€™ = \\frac{h_{11}x_i + h_{12}y_i + h_{13}}{h_{31}x_i + h_{32}y_i + h_{33}} \\y_iâ€™ = \\frac{h_{21}x_i + h_{22}y_i + h_{23}}{h_{31}x_i + h_{32}y_i + h_{33}}\\end{aligned}$$ äº¤å‰ç›¸ä¹˜åå¾—åˆ°çº¿æ€§æ–¹ç¨‹ï¼š $$\\begin{bmatrix}-x_i -y_i -1 0 0 0 x_iâ€™x_i x_iâ€™y_i x_iâ€™ \\0 0 0 -x_i -y_i -1 y_iâ€™x_i y_iâ€™y_i y_iâ€™\\end{bmatrix}\\begin{bmatrix} h_{11} \\ h_{12} \\ h_{13} \\ h_{21} \\ h_{22} \\ h_{23} \\ h_{31} \\ h_{32} \\ h_{33} \\end{bmatrix} = 0$$ æœ€å°‘éœ€è¦4å¯¹ç‚¹ï¼ˆ8ä¸ªæ–¹ç¨‹ï¼‰æ¥æ±‚è§£8ä¸ªæœªçŸ¥æ•°ã€‚ SVDæ±‚è§£ï¼š æ„å»ºçŸ©é˜µ $A$ï¼ˆ$2n \\times 9$ï¼‰ï¼Œæ±‚è§£ $A\\mathbf{h} = 0$ï¼š $$A = \\begin{bmatrix}-x_1 -y_1 -1 0 0 0 x_1â€™x_1 x_1â€™y_1 x_1â€™ \\0 0 0 -x_1 -y_1 -1 y_1â€™x_1 y_1â€™y_1 y_1â€™ \\\\vdots \\vdots \\vdots \\vdots \\vdots \\vdots \\vdots \\vdots \\vdots \\-x_n -y_n -1 0 0 0 x_nâ€™x_n x_nâ€™y_n x_nâ€™ \\0 0 0 -x_n -y_n -1 y_nâ€™x_n y_nâ€™y_n y_nâ€™\\end{bmatrix}$$ å¯¹ $A$ è¿›è¡ŒSVDåˆ†è§£ï¼š$A = U\\Sigma V^T$ï¼Œè§£ä¸º $V$ çš„æœ€åä¸€åˆ—ã€‚ 5.5 ä»å•åº”çŸ©é˜µæ¢å¤Rå’Œtç»™å®š $H = K[r_1 ; r_2 ; t]$ï¼Œå¯ä»¥æ¢å¤æ—‹è½¬å’Œå¹³ç§»ï¼š è®¡ç®—ï¼š$[r_1 ; r_2 ; t] = K^{-1}H$ å½’ä¸€åŒ–ï¼šç”±äºå°ºåº¦ä¸å®šæ€§ï¼Œéœ€è¦å½’ä¸€åŒ–ï¼š$$\\lambda = \\frac{1}{|K^{-1}H_{:,1}|} = \\frac{1}{|K^{-1}H_{:,2}|}$$ æå–ï¼š$$\\begin{aligned}r_1 = \\lambda K^{-1} H_{:,1} \\r_2 = \\lambda K^{-1} H_{:,2} \\t = \\lambda K^{-1} H_{:,3}\\end{aligned}$$ è®¡ç®—ç¬¬ä¸‰åˆ—ï¼š$$r_3 = r_1 \\times r_2$$ ç¡®ä¿æ­£äº¤æ€§ï¼šç”±äºå™ªå£°ï¼Œ$[r_1, r_2, r_3]$ å¯èƒ½ä¸å®Œå…¨æ­£äº¤ï¼Œéœ€è¦SVDæ ¡æ­£ï¼š$$[r_1, r_2, r_3] = U V^T$$å…¶ä¸­ $U\\Sigma V^T$ æ˜¯ $[r_1, r_2, r_3]$ çš„SVDåˆ†è§£ã€‚ å…­ã€ç›¸æœºçŸ©é˜µåˆ†è§£6.1 æŠ•å½±çŸ©é˜µå®Œæ•´çš„æŠ•å½±çŸ©é˜µä¸ºï¼š $$P = K[R|t] = \\begin{bmatrix}p_{11} p_{12} p_{13} p_{14} \\p_{21} p_{22} p_{23} p_{24} \\p_{31} p_{32} p_{33} p_{34}\\end{bmatrix}$$ ç›®æ ‡ï¼šä» $P$ åˆ†è§£å‡º $K$ã€$R$ã€$t$ã€‚ 6.2 RQåˆ†è§£æ–¹æ³•æŠ•å½±çŸ©é˜µçš„å‰3åˆ—å¯ä»¥å†™ä¸ºï¼š $$M = P_{:,1:3} = KR$$ å…¶ä¸­ $K$ æ˜¯ä¸Šä¸‰è§’çŸ©é˜µï¼Œ$R$ æ˜¯æ­£äº¤çŸ©é˜µã€‚ RQåˆ†è§£æ­¥éª¤ï¼š å°†çŸ©é˜µç¿»è½¬ è¿›è¡ŒQRåˆ†è§£ å°†ç»“æœç¿»è½¬å›æ¥ Pythonå®ç°ï¼š import numpy as npfrom scipy.linalg import rq# RQåˆ†è§£K, R = rq(M)# ç¡®ä¿Kçš„å¯¹è§’å…ƒç´ ä¸ºæ­£T = np.diag(np.sign(np.diag(K)))K = K @ TR = T @ R# ç¡®ä¿det(R) = 1if np.linalg.det(R) 0: R = -R K = -K# å½’ä¸€åŒ–KK = K / K[2, 2] 6.3 æå–å¹³ç§»å‘é‡$$t = K^{-1} P_{:,3}$$ 6.4 SVDåˆ†è§£éªŒè¯æ—‹è½¬çŸ©é˜µä¸ºäº†ç¡®ä¿ $R$ æ˜¯æœ‰æ•ˆçš„æ—‹è½¬çŸ©é˜µï¼ˆæ­£äº¤ä¸”è¡Œåˆ—å¼ä¸º1ï¼‰ï¼š U, S, Vt = np.linalg.svd(R)R_corrected = U @ Vtif np.linalg.det(R_corrected) 0: U[:, -1] *= -1 R_corrected = U @ Vt 6.5 å®Œæ•´çš„åˆ†è§£æµç¨‹def decompose_projection_matrix(P): \"\"\" åˆ†è§£æŠ•å½±çŸ©é˜µ P = K[R|t] å‚æ•°: P: 3x4 æŠ•å½±çŸ©é˜µ è¿”å›: K: 3x3 å†…å‚çŸ©é˜µ R: 3x3 æ—‹è½¬çŸ©é˜µ t: 3x1 å¹³ç§»å‘é‡ camera_center: 3x1 ç›¸æœºä¸­å¿ƒåœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„ä½ç½® \"\"\" # åˆ†ç¦»å‰3åˆ— M = P[:, :3] p4 = P[:, 3] # RQåˆ†è§£ K, R = rq(M) # ç¡®ä¿Kçš„å¯¹è§’å…ƒç´ ä¸ºæ­£ T = np.diag(np.sign(np.diag(K))) K = K @ T R = T @ R # ç¡®ä¿det(R) = 1 if np.linalg.det(R) 0: R = -R K = -K # å½’ä¸€åŒ–K K = K / K[2, 2] # æå–å¹³ç§»å‘é‡ t = np.linalg.inv(K) @ p4 # è®¡ç®—ç›¸æœºä¸­å¿ƒ camera_center = -R.T @ t return K, R, t, camera_center ä¸ƒã€Pythonå®Œæ•´å®ç°ä¸å¯è§†åŒ–7.1 åæ ‡ç³»å¯è§†åŒ–import numpy as npimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dfrom matplotlib.patches import FancyArrowPatchfrom mpl_toolkits.mplot3d.proj3d import proj_transformclass Arrow3D(FancyArrowPatch): \"\"\"3Dç®­å¤´ç±»\"\"\" def __init__(self, x, y, z, dx, dy, dz, *args, **kwargs): super().__init__((0, 0), (0, 0), *args, **kwargs) self._xyz = (x, y, z) self._dxdydz = (dx, dy, dz) def draw(self, renderer): x1, y1, z1 = self._xyz dx, dy, dz = self._dxdydz x2, y2, z2 = (x1 + dx, y1 + dy, z1 + dz) xs, ys, zs = proj_transform((x1, x2), (y1, y2), (z1, z2), self.axes.M) self.set_positions((xs[0], ys[0]), (xs[1], ys[1])) super().draw(renderer) def do_3d_projection(self, renderer=None): x1, y1, z1 = self._xyz dx, dy, dz = self._dxdydz x2, y2, z2 = (x1 + dx, y1 + dy, z1 + dz) xs, ys, zs = proj_transform((x1, x2), (y1, y2), (z1, z2), self.axes.M) self.set_positions((xs[0], ys[0]), (xs[1], ys[1])) return np.min(zs)def draw_coordinate_frame(ax, origin, rotation, scale=1.0, labels=['X', 'Y', 'Z'], colors=['r', 'g', 'b'], linewidth=2): \"\"\"ç»˜åˆ¶åæ ‡ç³»\"\"\" axes = rotation @ np.eye(3) * scale for i, (color, label) in enumerate(zip(colors, labels)): arrow = Arrow3D(origin[0], origin[1], origin[2], axes[0, i], axes[1, i], axes[2, i], mutation_scale=20, lw=linewidth, arrowstyle='-|', color=color) ax.add_artist(arrow) end_point = origin + axes[:, i] ax.text(end_point[0], end_point[1], end_point[2], label, fontsize=12, weight='bold')def draw_camera(ax, position, rotation, scale=0.5, color='blue'): \"\"\"ç»˜åˆ¶ç›¸æœºæ¨¡å‹\"\"\" # ç›¸æœºé”¥ä½“çš„é¡¶ç‚¹ï¼ˆåœ¨ç›¸æœºåæ ‡ç³»ä¸‹ï¼‰ camera_points = np.array([ [0, 0, 0], # å…‰å¿ƒ [-1, -1, 2], # å›¾åƒå¹³é¢å››ä¸ªè§’ [1, -1, 2], [1, 1, 2], [-1, 1, 2] ]) * scale # è½¬æ¢åˆ°ä¸–ç•Œåæ ‡ç³» world_points = (rotation @ camera_points.T).T + position # ç»˜åˆ¶ç›¸æœºé”¥ä½“ for i in range(1, 5): ax.plot([world_points[0, 0], world_points[i, 0]], [world_points[0, 1], world_points[i, 1]], [world_points[0, 2], world_points[i, 2]], color='black', linewidth=1) # å›¾åƒå¹³é¢çš„å››æ¡è¾¹ for i in range(1, 5): next_i = i + 1 if i 4 else 1 ax.plot([world_points[i, 0], world_points[next_i, 0]], [world_points[i, 1], world_points[next_i, 1]], [world_points[i, 2], world_points[next_i, 2]], color=color, linewidth=2) return world_pointsdef visualize_coordinate_systems(): \"\"\"å¯è§†åŒ–å››ä¸ªåæ ‡ç³»ç»Ÿ\"\"\" fig = plt.figure(figsize=(18, 12)) # ä¸–ç•Œåæ ‡ç³» world_origin = np.array([0, 0, 0]) world_rotation = np.eye(3) # ç›¸æœºä½ç½®å’Œå§¿æ€ camera_position = np.array([3, 2, 4]) theta_y = -np.pi/6 # ç»•Yè½´æ—‹è½¬ theta_x = -np.pi/9 # ç»•Xè½´æ—‹è½¬ Ry = np.array([[np.cos(theta_y), 0, np.sin(theta_y)], [0, 1, 0], [-np.sin(theta_y), 0, np.cos(theta_y)]]) Rx = np.array([[1, 0, 0], [0, np.cos(theta_x), -np.sin(theta_x)], [0, np.sin(theta_x), np.cos(theta_x)]]) camera_rotation = Ry @ Rx # ç»˜åˆ¶3Dåœºæ™¯ ax = fig.add_subplot(221, projection='3d') ax.set_title('ä¸–ç•Œåæ ‡ç³» â†’ ç›¸æœºåæ ‡ç³»', fontsize=14, weight='bold') # ç»˜åˆ¶ä¸–ç•Œåæ ‡ç³» draw_coordinate_frame(ax, world_origin, world_rotation, scale=2.0, labels=['Xw', 'Yw', 'Zw'], colors=['red', 'green', 'blue'], linewidth=3) # ç»˜åˆ¶ç›¸æœºåæ ‡ç³» draw_coordinate_frame(ax, camera_position, camera_rotation, scale=1.5, labels=['Xc', 'Yc', 'Zc'], colors=['darkred', 'darkgreen', 'darkblue'], linewidth=2) # ç»˜åˆ¶ç›¸æœºæ¨¡å‹ draw_camera(ax, camera_position, camera_rotation, scale=0.6, color='cyan') # ç»˜åˆ¶ä¸€ä¸ª3Dç‚¹ point_world = np.array([1.5, 1.5, 1.0]) ax.scatter(*point_world, c='purple', s=200, marker='o', edgecolors='black', linewidths=2) ax.text(point_world[0]+0.2, point_world[1]+0.2, point_world[2]+0.2, 'P(Xw,Yw,Zw)', fontsize=11, weight='bold', color='purple') # æŠ•å½±çº¿ ax.plot([point_world[0], camera_position[0]], [point_world[1], camera_position[1]], [point_world[2], camera_position[2]], 'purple', linestyle='--', linewidth=2, alpha=0.6) ax.set_xlabel('X', fontsize=12) ax.set_ylabel('Y', fontsize=12) ax.set_zlabel('Z', fontsize=12) ax.set_xlim([-1, 5]) ax.set_ylim([-1, 5]) ax.set_zlim([-1, 5]) ax.view_init(elev=20, azim=45) ax.grid(True, alpha=0.3) plt.tight_layout() plt.savefig('coordinate_systems_3d.png', dpi=300, bbox_inches='tight') plt.show()# è¿è¡Œå¯è§†åŒ–visualize_coordinate_systems() ä¿å­˜ä¸º visualize_coordinates.py 7.2 å®Œæ•´çš„ç›¸æœºå‡ ä½•è®¡ç®—ç¤ºä¾‹# camera_geometry_demo.pyimport numpy as npfrom scipy.linalg import rqclass CameraGeometry: \"\"\"ç›¸æœºå‡ ä½•è®¡ç®—ç±»\"\"\" def __init__(self, fx, fy, cx, cy, width, height): \"\"\" åˆå§‹åŒ–ç›¸æœºå†…å‚ å‚æ•°: fx, fy: ç„¦è·ï¼ˆåƒç´ ï¼‰ cx, cy: ä¸»ç‚¹ï¼ˆåƒç´ ï¼‰ width, height: å›¾åƒå°ºå¯¸ \"\"\" self.K = np.array([ [fx, 0, cx], [0, fy, cy], [0, 0, 1] ]) self.width = width self.height = height @staticmethod def rotation_matrix_from_euler(roll, pitch, yaw, order='xyz'): \"\"\"ä»æ¬§æ‹‰è§’åˆ›å»ºæ—‹è½¬çŸ©é˜µ\"\"\" Rx = np.array([ [1, 0, 0], [0, np.cos(roll), -np.sin(roll)], [0, np.sin(roll), np.cos(roll)] ]) Ry = np.array([ [np.cos(pitch), 0, np.sin(pitch)], [0, 1, 0], [-np.sin(pitch), 0, np.cos(pitch)] ]) Rz = np.array([ [np.cos(yaw), -np.sin(yaw), 0], [np.sin(yaw), np.cos(yaw), 0], [0, 0, 1] ]) if order == 'xyz': return Rz @ Ry @ Rx elif order == 'zyx': return Rx @ Ry @ Rz else: raise ValueError(\"Order must be 'xyz' or 'zyx'\") @staticmethod def rotation_matrix_from_axis_angle(axis, theta): \"\"\"ä»è½´è§’åˆ›å»ºæ—‹è½¬çŸ©é˜µï¼ˆç½—å¾·é‡Œæ ¼æ–¯å…¬å¼ï¼‰\"\"\" axis = axis / np.linalg.norm(axis) # å½’ä¸€åŒ– K = np.array([ [0, -axis[2], axis[1]], [axis[2], 0, -axis[0]], [-axis[1], axis[0], 0] ]) R = np.eye(3) + np.sin(theta) * K + (1 - np.cos(theta)) * (K @ K) return R def project_point(self, point_world, R, t): \"\"\" å°†ä¸–ç•Œåæ ‡ç‚¹æŠ•å½±åˆ°å›¾åƒ å‚æ•°: point_world: 3Dç‚¹ï¼ˆä¸–ç•Œåæ ‡ç³»ï¼‰ R: æ—‹è½¬çŸ©é˜µ t: å¹³ç§»å‘é‡ è¿”å›: pixel: åƒç´ åæ ‡ (u, v) \"\"\" # ä¸–ç•Œåæ ‡ - ç›¸æœºåæ ‡ point_camera = R @ point_world + t # ç›¸æœºåæ ‡ - å›¾åƒåæ ‡ï¼ˆé€è§†æŠ•å½±ï¼‰ if point_camera[2] = 0: raise ValueError(\"Point is behind the camera\") # æŠ•å½±åˆ°åƒç´ åæ ‡ point_homo = self.K @ point_camera pixel = point_homo[:2] / point_homo[2] return pixel def compute_projection_matrix(self, R, t): \"\"\"è®¡ç®—æŠ•å½±çŸ©é˜µ P = K[R|t]\"\"\" return self.K @ np.hstack([R, t.reshape(-1, 1)]) def decompose_projection_matrix(self, P): \"\"\"åˆ†è§£æŠ•å½±çŸ©é˜µ\"\"\" M = P[:, :3] p4 = P[:, 3] # RQåˆ†è§£ K, R = rq(M) # ç¡®ä¿Kçš„å¯¹è§’å…ƒç´ ä¸ºæ­£ T = np.diag(np.sign(np.diag(K))) K = K @ T R = T @ R # ç¡®ä¿det(R) = 1 if np.linalg.det(R) 0: R = -R K = -K # å½’ä¸€åŒ–K K = K / K[2, 2] # æå–å¹³ç§»å‘é‡ t = np.linalg.inv(K) @ p4 # ç›¸æœºä¸­å¿ƒ camera_center = -R.T @ t return K, R, t, camera_center def compute_homography(self, R, t, n, d): \"\"\" è®¡ç®—å¹³é¢çš„å•åº”çŸ©é˜µ å‚æ•°: R, t: å¤–å‚ n: å¹³é¢æ³•å‘é‡ï¼ˆä¸–ç•Œåæ ‡ç³»ï¼‰ d: å¹³é¢åˆ°åŸç‚¹çš„è·ç¦» è¿”å›: H: 3x3 å•åº”çŸ©é˜µ \"\"\" H = self.K @ (R - (t @ n.T) / d) @ np.linalg.inv(self.K) return H / H[2, 2] # å½’ä¸€åŒ–# ç¤ºä¾‹ï¼šå®Œæ•´çš„æŠ•å½±æµç¨‹if __name__ == \"__main__\": # åˆ›å»ºç›¸æœº camera = CameraGeometry( fx=800, fy=800, # ç„¦è· cx=320, cy=240, # ä¸»ç‚¹ width=640, height=480 # å›¾åƒå°ºå¯¸ ) print(\"=\" * 60) print(\"ç›¸æœºå†…å‚çŸ©é˜µ K:\") print(camera.K) print() # åˆ›å»ºå¤–å‚ roll, pitch, yaw = np.deg2rad([10, 20, 30]) R = camera.rotation_matrix_from_euler(roll, pitch, yaw, order='zyx') t = np.array([1.0, 2.0, 5.0]) print(\"æ—‹è½¬çŸ©é˜µ R:\") print(R) print(f\" det(R) = {np.linalg.det(R):.6f} (åº”è¯¥ä¸º1)\") print(f\"R^T @ R =\") print(R.T @ R) print(\"(åº”è¯¥ä¸ºå•ä½çŸ©é˜µ)\") print() print(\"å¹³ç§»å‘é‡ t:\") print(t) print() # æ—‹è½¬çŸ©é˜µåˆ—å‘é‡çš„å«ä¹‰ print(\"=\" * 60) print(\"æ—‹è½¬çŸ©é˜µåˆ—å‘é‡çš„å‡ ä½•æ„ä¹‰:\") print(\"ç¬¬1åˆ—ï¼ˆä¸–ç•ŒXè½´åœ¨ç›¸æœºç³»ä¸­çš„æ–¹å‘ï¼‰:\", R[:, 0]) print(\"ç¬¬2åˆ—ï¼ˆä¸–ç•ŒYè½´åœ¨ç›¸æœºç³»ä¸­çš„æ–¹å‘ï¼‰:\", R[:, 1]) print(\"ç¬¬3åˆ—ï¼ˆä¸–ç•ŒZè½´åœ¨ç›¸æœºç³»ä¸­çš„æ–¹å‘ï¼‰:\", R[:, 2]) print() # ç›¸æœºåœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„ä½ç½® camera_center = -R.T @ t print(f\"ç›¸æœºä¸­å¿ƒåœ¨ä¸–ç•Œåæ ‡ç³»ä¸­çš„ä½ç½®: {camera_center}\") print() # æŠ•å½±ä¸€ä¸ª3Dç‚¹ point_3d = np.array([2.0, 3.0, 4.0]) print(\"=\" * 60) print(f\"3Dç‚¹ï¼ˆä¸–ç•Œåæ ‡ï¼‰: {point_3d}\") try: pixel = camera.project_point(point_3d, R, t) print(f\"æŠ•å½±åˆ°å›¾åƒï¼ˆåƒç´ åæ ‡ï¼‰: ({pixel[0]:.2f}, {pixel[1]:.2f})\") except ValueError as e: print(f\"æŠ•å½±å¤±è´¥: {e}\") print() # è®¡ç®—å¹¶åˆ†è§£æŠ•å½±çŸ©é˜µ P = camera.compute_projection_matrix(R, t) print(\"=\" * 60) print(\"æŠ•å½±çŸ©é˜µ P = K[R|t]:\") print(P) print() # åˆ†è§£æŠ•å½±çŸ©é˜µ K_recovered, R_recovered, t_recovered, center_recovered = \\ camera.decompose_projection_matrix(P) print(\"åˆ†è§£åçš„å†…å‚çŸ©é˜µ K:\") print(K_recovered) print(\" åˆ†è§£åçš„æ—‹è½¬çŸ©é˜µ R:\") print(R_recovered) print(\" åˆ†è§£åçš„å¹³ç§»å‘é‡ t:\") print(t_recovered) print(\" åˆ†è§£åçš„ç›¸æœºä¸­å¿ƒ:\") print(center_recovered) print() # éªŒè¯ print(\"=\" * 60) print(\"éªŒè¯åˆ†è§£ç»“æœ:\") print(f\"Kè¯¯å·®: {np.linalg.norm(camera.K - K_recovered):.2e}\") print(f\"Rè¯¯å·®: {np.linalg.norm(R - R_recovered):.2e}\") print(f\"tè¯¯å·®: {np.linalg.norm(t - t_recovered):.2e}\") print() # å•åº”çŸ©é˜µç¤ºä¾‹ print(\"=\" * 60) print(\"å•åº”çŸ©é˜µç¤ºä¾‹ï¼ˆåœ°å¹³é¢ Z=0ï¼‰:\") n = np.array([0, 0, 1]) # å¹³é¢æ³•å‘é‡ d = 0 # å¹³é¢è·ç¦»åŸç‚¹ # å¯¹äºZ=0å¹³é¢ï¼Œå•åº”çŸ©é˜µä¸º H = K[r1 r2 t] H = camera.K @ np.column_stack([R[:, 0], R[:, 1], t]) H = H / H[2, 2] # å½’ä¸€åŒ– print(\"å•åº”çŸ©é˜µ H:\") print(H) print() # æµ‹è¯•å•åº”å˜æ¢ point_2d_world = np.array([2.0, 3.0, 1.0]) # ä¸–ç•Œå¹³é¢ä¸Šçš„ç‚¹ï¼ˆé½æ¬¡åæ ‡ï¼‰ point_2d_image = H @ point_2d_world point_2d_image = point_2d_image / point_2d_image[2] print(f\"å¹³é¢ä¸Šçš„ç‚¹ï¼ˆä¸–ç•Œåæ ‡ï¼‰: ({point_2d_world[0]}, {point_2d_world[1]})\") print(f\"é€šè¿‡å•åº”çŸ©é˜µæŠ•å½±åˆ°å›¾åƒ: ({point_2d_image[0]:.2f}, {point_2d_image[1]:.2f})\") # éªŒè¯ï¼šé€šè¿‡å®Œæ•´æŠ•å½± point_3d_on_plane = np.array([point_2d_world[0], point_2d_world[1], 0.0]) pixel_verify = camera.project_point(point_3d_on_plane, R, t) print(f\"é€šè¿‡å®Œæ•´æŠ•å½±éªŒè¯: ({pixel_verify[0]:.2f}, {pixel_verify[1]:.2f})\") print(f\"è¯¯å·®: {np.linalg.norm(point_2d_image[:2] - pixel_verify):.2e}\") ä¿å­˜ä¸º camera_geometry_demo.py å…«ã€å®é™…åº”ç”¨åœºæ™¯8.1 ç›¸æœºæ ‡å®šé€šè¿‡æ‹æ‘„æ ‡å®šæ¿ï¼ˆå¦‚æ£‹ç›˜æ ¼ï¼‰çš„å¤šå¼ å›¾åƒï¼Œå¯ä»¥ä¼°è®¡ç›¸æœºå†…å‚å’Œç•¸å˜å‚æ•°ã€‚ 8.2 3Dé‡å»ºåˆ©ç”¨å¤šè§†å›¾å‡ ä½•å’Œå¤–å‚çŸ©é˜µï¼Œå¯ä»¥ä»å¤šå¼ å›¾åƒé‡å»º3Dåœºæ™¯ã€‚ 8.3 å¢å¼ºç°å®ï¼ˆARï¼‰é€šè¿‡ç›¸æœºä½å§¿ä¼°è®¡ï¼Œå¯ä»¥å°†è™šæ‹Ÿç‰©ä½“ç²¾ç¡®åœ°å åŠ åˆ°çœŸå®åœºæ™¯ä¸­ã€‚ 8.4 è§†è§‰SLAMåŒæ—¶å®šä½ä¸åœ°å›¾æ„å»ºï¼ˆSLAMï¼‰éœ€è¦å®æ—¶ä¼°è®¡ç›¸æœºçš„ä½å§¿ï¼ˆå¤–å‚ï¼‰ã€‚ 8.5 å›¾åƒæ‹¼æ¥åˆ©ç”¨å•åº”çŸ©é˜µå¯ä»¥å°†å¤šå¼ å›¾åƒæ‹¼æ¥æˆå…¨æ™¯å›¾ã€‚ ä¹ã€æ€»ç»“æœ¬æ–‡è¯¦ç»†æ¨å¯¼äº†ä»3Dä¸–ç•Œåæ ‡åˆ°2Då›¾åƒåæ ‡çš„å®Œæ•´æ•°å­¦è¿‡ç¨‹ï¼š **å¤–å‚çŸ©é˜µ $[R|t]$**ï¼šæè¿°ç›¸æœºåœ¨ä¸–ç•Œä¸­çš„ä½ç½®å’Œæ–¹å‘ï¼ˆ6è‡ªç”±åº¦ï¼‰ **å†…å‚çŸ©é˜µ $K$**ï¼šæè¿°ç›¸æœºçš„å†…éƒ¨å‡ ä½•å‚æ•°ï¼ˆ4-5è‡ªç”±åº¦ï¼‰ **æ—‹è½¬çŸ©é˜µ $R$**ï¼šå¯ç”¨æ¬§æ‹‰è§’ã€è½´è§’ã€å››å…ƒæ•°è¡¨ç¤ºï¼Œå…¶åˆ—å‘é‡è¡¨ç¤ºä¸–ç•Œåæ ‡ç³»çš„åŸºå‘é‡åœ¨ç›¸æœºç³»ä¸­çš„è¡¨ç¤º **å•åº”çŸ©é˜µ $H$**ï¼šæè¿°å¹³é¢åˆ°å›¾åƒçš„æŠ•å½±å˜æ¢ï¼ˆ8è‡ªç”±åº¦ï¼‰ çŸ©é˜µåˆ†è§£ï¼šå¯ä»¥ä»æŠ•å½±çŸ©é˜µæ¢å¤å‡ºå†…å¤–å‚æ•° å®Œæ•´çš„æŠ•å½±å…¬å¼ä¸ºï¼š $$s \\begin{bmatrix} u \\ v \\ 1 \\end{bmatrix} =\\begin{bmatrix}f_x 0 c_x \\0 f_y c_y \\0 0 1\\end{bmatrix}\\begin{bmatrix}r_{11} r_{12} r_{13} t_x \\r_{21} r_{22} r_{23} t_y \\r_{31} r_{32} r_{33} t_z\\end{bmatrix}\\begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \\end{bmatrix}$$ é…å¥—çš„Pythonä»£ç æä¾›äº†å®Œæ•´çš„å®ç°å’Œå¯è§†åŒ–ï¼Œå¯ä»¥ç›´æ¥ç”¨äºå®é™…é¡¹ç›®ã€‚ å‚è€ƒèµ„æ–™ Hartley, R., Zisserman, A. (2003). Multiple View Geometry in Computer Vision. Cambridge University Press. Szeliski, R. (2010). Computer Vision: Algorithms and Applications. Springer. OpenCV Documentation: Camera Calibration and 3D Reconstruction é™„å½•ï¼šè¿è¡Œä»£ç å°†ä¸Šè¿°Pythonä»£ç ä¿å­˜ä¸ºå¯¹åº”çš„æ–‡ä»¶åï¼Œç„¶åè¿è¡Œï¼š # å®‰è£…ä¾èµ–pip install numpy matplotlib scipy# è¿è¡Œå¯è§†åŒ–python visualize_coordinates.py# è¿è¡Œå®Œæ•´ç¤ºä¾‹python camera_geometry_demo.py å…³é”®è¯ï¼šç›¸æœºå‡ ä½•ã€åæ ‡å˜æ¢ã€æ—‹è½¬çŸ©é˜µã€å†…å‚çŸ©é˜µã€å¤–å‚çŸ©é˜µã€å•åº”çŸ©é˜µã€æŠ•å½±çŸ©é˜µåˆ†è§£ã€è®¡ç®—æœºè§†è§‰ åšå®¢æ ‡ç­¾ï¼š#è®¡ç®—æœºè§†è§‰ #ç›¸æœºæ ‡å®š #å¤šè§†å‡ ä½• #çŸ©é˜µåˆ†è§£ #Python","tags":["è®¡ç®—æœºè§†è§‰"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"ä»å›¾åƒåˆ°ä¿¯è§†å›¾ï¼šå®Œæ•´å®ç°æŒ‡å—","path":"/2026/01/19/image-to-bird-eye-view/","content":"ä»è¾“å…¥å›¾åƒåˆ°è¾“å‡ºé¸Ÿç°å›¾çš„å®Œæ•´å®ç° - é€‚åˆæ‰€æœ‰æ°´å¹³çš„å­¦ä¹ è€… ğŸ¯ æ•™ç¨‹ç®€ä»‹æœ¬æ•™ç¨‹å°†å¸¦ä½ ä»é›¶å¼€å§‹å­¦ä¹ å¦‚ä½•æŠŠä¸€å¼ æ–œç€æ‹æ‘„çš„å›¾åƒï¼ˆæ¯”å¦‚è·¯é¢ç…§ç‰‡ï¼‰è½¬æ¢æˆä»æ­£ä¸Šæ–¹çœ‹çš„ä¿¯è§†å›¾ï¼ˆé¸Ÿç°å›¾ï¼‰ã€‚ ä½ å°†å­¦åˆ°ä»€ä¹ˆï¼Ÿinfographic list-row-simple-horizontal-arrowdata title å®Œæ•´å­¦ä¹ è·¯å¾„ items - label è¾“å…¥å›¾åƒ desc æ–œç€çœ‹çš„ç…§ç‰‡ - label ç†è§£é€è§† desc åæ ‡ç³»ç»Ÿ - label è®¡ç®—å˜æ¢ desc å•åº”æ€§çŸ©é˜µ - label å®ç°ä»£ç  desc Python+OpenCV - label è¾“å‡ºä¿¯è§†å›¾ desc é¸Ÿç°å›¾ ğŸ“š ç¬¬ä¸€ç« ï¼šåŸºç¡€æ¦‚å¿µ1.1 ä¸‰ç§åæ ‡ç³»ç»Ÿåœ¨å›¾åƒå¤„ç†ä¸­ï¼Œæˆ‘ä»¬éœ€è¦ç†è§£ä¸‰ç§åæ ‡ç³»ç»Ÿçš„è½¬æ¢å…³ç³»ï¼š infographic list-grid-badge-carddata title ä¸‰ç§åæ ‡ç³» items - label ä¸–ç•Œåæ ‡ç³» desc å›ºå®šåœ¨åœ°é¢ï¼Œå•ä½ï¼šç±³ icon mdi:earth - label ç›¸æœºåæ ‡ç³» desc è·Ÿéšç›¸æœºç§»åŠ¨æ—‹è½¬ icon mdi:camera - label å›¾åƒåæ ‡ç³» desc 2Dåƒç´ å¹³é¢ icon mdi:image ä¸–ç•Œåæ ‡ç³» (World Coordinates)ç”Ÿæ´»ä¾‹å­ï¼šå°±åƒåœ°å›¾çš„ç»çº¬åº¦ï¼Œå›ºå®šä¸å˜çš„å‚è€ƒç³»ã€‚ # ä¸–ç•Œåæ ‡ä¸­çš„ä¸€ä¸ªç‚¹ï¼ˆ3ç±³ï¼Œ5ç±³ï¼Œ0ç±³ï¼‰point_world = np.array([3.0, 5.0, 0.0]) # [X, Y, Z] ç‰¹ç‚¹ï¼š åŸç‚¹ï¼šé€šå¸¸é€‰åœ¨åœ°é¢æŸä¸ªå›ºå®šä½ç½® Xè½´ï¼šå‘å³ï¼ˆä¸œæ–¹ï¼‰ Yè½´ï¼šå‘å‰ï¼ˆåŒ—æ–¹ï¼‰ Zè½´ï¼šå‘ä¸Šï¼ˆå¤©ç©ºï¼‰ ç›¸æœºåæ ‡ç³» (Camera Coordinates) æ ¸å¿ƒè§„åˆ™ï¼š åŸç‚¹ï¼šåœ¨ç›¸æœºé•œå¤´ä¸­å¿ƒ Zè½´ï¼šé•œå¤´æœå‘ï¼ˆå…‰è½´æ–¹å‘ï¼‰ Xè½´ï¼šç›¸æœºå‘å³ Yè½´ï¼šç›¸æœºå‘ä¸‹ï¼ˆâš ï¸ æ³¨æ„ï¼šå‘ä¸‹ï¼ï¼‰ ä¸ºä»€ä¹ˆYè½´å‘ä¸‹ï¼Ÿ ä¼ ç»ŸåŸå› ï¼šâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ (0,0) â”‚ â† å›¾åƒå·¦ä¸Šè§’æ˜¯(0,0)â”‚ â†“ Y â”‚â”‚ â†’ X â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ å›¾åƒåæ ‡ç³» (Image Coordinates)å°±æ˜¯æˆ‘ä»¬çœ‹åˆ°çš„ç…§ç‰‡åƒç´ åæ ‡ï¼š # å›¾åƒä¸­çš„ä¸€ä¸ªåƒç´ ç‚¹pixel = (320, 240) # (u, v) åƒç´ åæ ‡ å•ä½ï¼šåƒç´  (pixel) 1.2 é€è§†æŠ•å½±åŸç† å°å­”æˆåƒæ¨¡å‹æ˜¯ç›¸æœºçš„æ•°å­¦æŠ½è±¡ï¼š 3Dä¸–ç•Œ â”€â”€â”€â”€â”€â”€ å°å­” â”€â”€â”€â”€â”€â”€ 2Då›¾åƒ(X,Y,Z) ç›¸æœº (u,v) æŠ•å½±è¿‡ç¨‹ï¼š æ•°å­¦å…¬å¼ï¼š $$\\begin{bmatrix} u \\ v \\ 1 \\end{bmatrix} = K \\begin{bmatrix} R | t \\end{bmatrix} \\begin{bmatrix} X \\ Y \\ Z \\ 1 \\end{bmatrix}$$ å…¶ä¸­ï¼š Kï¼šç›¸æœºå†…å‚çŸ©é˜µï¼ˆç„¦è·ã€ä¸»ç‚¹ï¼‰ Rï¼šæ—‹è½¬çŸ©é˜µï¼ˆç›¸æœºæœå‘ï¼‰ tï¼šå¹³ç§»å‘é‡ï¼ˆç›¸æœºä½ç½®ï¼‰ 1.3 é€è§†æ•ˆæœ ä¸ºä»€ä¹ˆè¿œå¤„çš„ç‰©ä½“çœ‹èµ·æ¥å°ï¼Ÿ è¿‘å¤„çš„è½¦ â”â”â”â” çœ‹èµ·æ¥å¤§ä¸­é—´çš„è½¦ â”â”â” çœ‹èµ·æ¥ä¸­ç­‰è¿œå¤„çš„è½¦ â”â” çœ‹èµ·æ¥å° è¿™å°±æ˜¯é€è§†æŠ•å½±çš„æ•ˆæœï¼ ğŸ“ ç¬¬äºŒç« ï¼šæ¶ˆå¤±ç‚¹ç†è®º2.1 ä»€ä¹ˆæ˜¯æ¶ˆå¤±ç‚¹ï¼Ÿ ç”Ÿæ´»ä¾‹å­ï¼šç«™åœ¨é“è½¨ä¸­é—´æ‹ç…§ï¼Œä¸¤æ¡å¹³è¡Œçš„é“è½¨åœ¨è¿œå¤„çœ‹èµ·æ¥ä¼šâ€ç›¸äº¤â€ã€‚ å®šä¹‰ï¼š ç°å®ä¸­å¹³è¡Œçš„çº¿ï¼Œåœ¨å›¾åƒä¸­å»¶ä¼¸åç›¸äº¤çš„ç‚¹ï¼Œå°±æ˜¯æ¶ˆå¤±ç‚¹ (Vanishing Point)ã€‚ 2.2 æ¶ˆå¤±ç‚¹çš„è®¡ç®—åŸç†ï¼šä¸¤æ¡å¹³è¡Œçº¿çš„äº¤ç‚¹ def compute_vanishing_point(line1_pts, line2_pts): \"\"\" è®¡ç®—ä¸¤æ¡å¹³è¡Œçº¿çš„æ¶ˆå¤±ç‚¹ å‚æ•°ï¼š line1_pts: ç›´çº¿1çš„ä¸¤ä¸ªç‚¹ [[x1,y1], [x2,y2]] line2_pts: ç›´çº¿2çš„ä¸¤ä¸ªç‚¹ [[x3,y3], [x4,y4]] è¿”å›ï¼š vp: æ¶ˆå¤±ç‚¹åæ ‡ [vx, vy] \"\"\" # è½¬ä¸ºé½æ¬¡åæ ‡ p1 = np.array([line1_pts[0][0], line1_pts[0][1], 1]) p2 = np.array([line1_pts[1][0], line1_pts[1][1], 1]) p3 = np.array([line2_pts[0][0], line2_pts[0][1], 1]) p4 = np.array([line2_pts[1][0], line2_pts[1][1], 1]) # è®¡ç®—ç›´çº¿ï¼ˆå‰ä¹˜ï¼‰ L1 = np.cross(p1, p2) L2 = np.cross(p3, p4) # è®¡ç®—äº¤ç‚¹ï¼ˆå‰ä¹˜ï¼‰ vp_homo = np.cross(L1, L2) # å½’ä¸€åŒ– vp = vp_homo[:2] / vp_homo[2] return vp 2.3 ä»æ¶ˆå¤±ç‚¹åˆ°ç›¸æœºå‚æ•° æ ¸å¿ƒæ€æƒ³ï¼šæ¶ˆå¤±ç‚¹çš„ä½ç½®åæ˜ äº†ç›¸æœºçš„æœå‘ï¼ def estimate_camera_params(vp, img_shape, cx=None, cy=None): \"\"\" ä»æ¶ˆå¤±ç‚¹ä¼°è®¡ç›¸æœºå‚æ•° å‚æ•°ï¼š vp: æ¶ˆå¤±ç‚¹åæ ‡ [vx, vy] img_shape: å›¾åƒå¤§å° (height, width) cx, cy: ä¸»ç‚¹åæ ‡ï¼ˆå¯é€‰ï¼‰ è¿”å›ï¼š focal: ä¼°è®¡çš„ç„¦è· yaw: yawè§’åº¦ï¼ˆåº¦ï¼‰ pitch: pitchè§’åº¦ï¼ˆåº¦ï¼‰ \"\"\" h, w = img_shape[:2] # ä¸»ç‚¹ï¼ˆé»˜è®¤å›¾åƒä¸­å¿ƒï¼‰ if cx is None: cx = w / 2 if cy is None: cy = h / 2 vx, vy = vp # è®¡ç®—åç§» dx = vx - cx dy = vy - cy # ä¼°è®¡ç„¦è· focal = np.sqrt(w**2 + h**2) # è®¡ç®—è§’åº¦ yaw = np.arctan2(dx, focal) pitch = np.arctan2(dy, focal) # è½¬ä¸ºè§’åº¦ yaw_deg = np.degrees(yaw) pitch_deg = np.degrees(pitch) return focal, yaw_deg, pitch_deg 2.4 å‡ ä½•è¯æ˜ æ ¸å¿ƒå…¬å¼ï¼š $$\\text{yaw} = \\arctan\\left(\\frac{v_x - c_x}{f}\\right)$$ $$\\text{pitch} = \\arctan\\left(\\frac{v_y - c_y}{f}\\right)$$ ğŸ”„ ç¬¬ä¸‰ç« ï¼šå•åº”æ€§å˜æ¢3.1 ä»€ä¹ˆæ˜¯å•åº”æ€§ï¼Ÿç”Ÿæ´»ä¾‹å­ï¼š ç›¸æœºè§†è§’ï¼ˆé€è§†å›¾ï¼‰ï¼š | | è¿‘å¤„çš„æ¡çº¹ - çœ‹èµ·æ¥å¾ˆå®½ | | | | | | | | è¿œå¤„çš„æ¡çº¹ - çœ‹èµ·æ¥å¾ˆçª„é¸Ÿç°è§†è§’ï¼š |||||||| æ‰€æœ‰æ¡çº¹ - å®½åº¦ä¸€æ · |||||||| |||||||| å•åº”æ€§å˜æ¢å°±æ˜¯æŠŠç¬¬ä¸€ç§è§†è§’è½¬æ¢æˆç¬¬äºŒç§è§†è§’çš„æ•°å­¦æ–¹æ³•ï¼ 3.2 å•åº”æ€§çŸ©é˜µæ•°å­¦å®šä¹‰ï¼š $$\\begin{bmatrix} xâ€™ \\ yâ€™ \\ 1 \\end{bmatrix} = H \\begin{bmatrix} x \\ y \\ 1 \\end{bmatrix}$$ å…¶ä¸­ H æ˜¯ä¸€ä¸ª 3Ã—3 çš„çŸ©é˜µï¼š â”Œ â” â”‚ h11 h12 h13 â”‚H = â”‚ h21 h22 h23 â”‚ â”‚ h31 h32 h33 â”‚ â”” â”˜ ç‰¹ç‚¹ï¼š 9ä¸ªæ•°å­—ï¼Œä½†åªæœ‰8ä¸ªè‡ªç”±åº¦ å¯ä»¥è¡¨ç¤ºæ—‹è½¬ã€ç¼©æ”¾ã€å¹³ç§»ã€é€è§†å˜æ¢ 3.3 DLTç®—æ³•æ±‚è§£HçŸ©é˜µæ ¸å¿ƒæ€æƒ³ï¼šç”¨4å¯¹ç‚¹æ„å»ºæ–¹ç¨‹ç»„ï¼Œç”¨SVDæ±‚è§£ã€‚ def normalize_points(points): \"\"\" å½’ä¸€åŒ–ç‚¹é›†ï¼ˆæé«˜æ•°å€¼ç¨³å®šæ€§ï¼‰ \"\"\" # è®¡ç®—ä¸­å¿ƒç‚¹ centroid = np.mean(points, axis=0) # å¹³ç§»åˆ°åŸç‚¹ centered = points - centroid # è®¡ç®—å¹³å‡è·ç¦» avg_dist = np.mean(np.sqrt(np.sum(centered**2, axis=1))) # ç¼©æ”¾å› å­ scale = np.sqrt(2) / (avg_dist + 1e-8) # æ„å»ºå½’ä¸€åŒ–çŸ©é˜µ T = np.array([ [scale, 0, -scale * centroid[0]], [0, scale, -scale * centroid[1]], [0, 0, 1] ]) return Tdef compute_homography_dlt(src_pts, dst_pts): \"\"\" ä½¿ç”¨DLTç®—æ³•è®¡ç®—å•åº”æ€§çŸ©é˜µ å‚æ•°ï¼š src_pts: æºå¹³é¢ç‚¹ (NÃ—2) dst_pts: ç›®æ ‡å¹³é¢ç‚¹ (NÃ—2) è¿”å›ï¼š H: 3Ã—3 å•åº”æ€§çŸ©é˜µ \"\"\" assert len(src_pts) = 4, \"è‡³å°‘éœ€è¦4ä¸ªç‚¹ï¼\" # å½’ä¸€åŒ–ï¼ˆæé«˜æ•°å€¼ç¨³å®šæ€§ï¼‰ T_src = normalize_points(src_pts) T_dst = normalize_points(dst_pts) # å½’ä¸€åŒ–ç‚¹ src_homo = np.column_stack([src_pts, np.ones(len(src_pts))]) dst_homo = np.column_stack([dst_pts, np.ones(len(dst_pts))]) src_norm = (T_src @ src_homo.T).T dst_norm = (T_dst @ dst_homo.T).T src_norm_2d = src_norm[:, :2] / src_norm[:, 2:3] dst_norm_2d = dst_norm[:, :2] / dst_norm[:, 2:3] # æ„å»ºçŸ©é˜µA A = [] for i in range(len(src_norm_2d)): x1, y1 = src_norm_2d[i] x2, y2 = dst_norm_2d[i] # ä¸¤ä¸ªçº¦æŸæ–¹ç¨‹ A.append([ -x1, -y1, -1, 0, 0, 0, x2*x1, x2*y1, x2 ]) A.append([ 0, 0, 0, -x1, -y1, -1, y2*x1, y2*y1, y2 ]) A = np.array(A) # SVDåˆ†è§£ U, S, Vt = np.linalg.svd(A) # æœ€å°å¥‡å¼‚å€¼å¯¹åº”çš„å‘é‡ h = Vt[-1, :] # é‡æ„HçŸ©é˜µ H_norm = h.reshape(3, 3) # åå½’ä¸€åŒ– H = np.linalg.inv(T_dst) @ H_norm @ T_src # å½’ä¸€åŒ–ï¼ˆä½¿h33=1ï¼‰ H = H / H[2, 2] return H 3.4 ä¼˜åŒ–è¿‡ç¨‹ æ­¥éª¤è§£æï¼š infographic sequence-steps-simpledata title DLTç®—æ³•æµç¨‹ items - label å½’ä¸€åŒ–ç‚¹ desc æé«˜æ•°å€¼ç¨³å®šæ€§ - label æ„å»ºæ–¹ç¨‹ç»„ desc Ah = 0 - label SVDåˆ†è§£ desc æ±‚æœ€å°å¥‡å¼‚å€¼ - label æå–HçŸ©é˜µ desc é‡æ„3Ã—3çŸ©é˜µ - label åå½’ä¸€åŒ– desc æ¢å¤åŸå§‹å°ºåº¦ ğŸ’» ç¬¬å››ç« ï¼šå®Œæ•´ä»£ç å®ç°4.1 ç¯å¢ƒæ­å»ºPythonç¯å¢ƒè¦æ±‚ï¼š # Pythonç‰ˆæœ¬Python 3.7+# æ ¸å¿ƒåº“numpy=1.19.0opencv-python=4.5.0matplotlib=3.3.0 å®‰è£…æ­¥éª¤ï¼š # åˆ›å»ºè™šæ‹Ÿç¯å¢ƒï¼ˆæ¨èï¼‰python -m venv cv_env# æ¿€æ´»è™šæ‹Ÿç¯å¢ƒ# Mac/Linux:source cv_env/bin/activate# Windows:cv_env\\Scripts\\activate# å®‰è£…ä¾èµ–pip install numpy opencv-python matplotlib# éªŒè¯å®‰è£…python -c \"import cv2; print(cv2.__version__)\" 4.2 é¡¹ç›®ç»“æ„bird_eye_view_project/â”œâ”€â”€ input/ # è¾“å…¥å›¾åƒâ”‚ â””â”€â”€ dashcam.jpgâ”œâ”€â”€ output/ # è¾“å‡ºç»“æœâ”‚ â”œâ”€â”€ marked.jpgâ”‚ â”œâ”€â”€ bird_view.jpgâ”‚ â””â”€â”€ result.jpgâ”œâ”€â”€ main.py # ä¸»ç¨‹åºâ””â”€â”€ requirements.txt # ä¾èµ–åˆ—è¡¨ 4.3 äº¤äº’å¼é¸Ÿç°å›¾ç”Ÿæˆå™¨\"\"\"ä»å›¾åƒåˆ°ä¿¯è§†å›¾ï¼šå®Œæ•´å®ç°åŠŸèƒ½ï¼šäº¤äº’å¼é€‰ç‚¹ â†’ è®¡ç®—å•åº”æ€§çŸ©é˜µ â†’ ç”Ÿæˆé¸Ÿç°å›¾\"\"\"import cv2import numpy as npimport matplotlib.pyplot as pltfrom pathlib import Pathclass InteractiveBirdEyeView: \"\"\" äº¤äº’å¼é¸Ÿç°å›¾ç”Ÿæˆå™¨ ä½¿ç”¨æ–¹æ³•ï¼š 1. ç‚¹å‡»å›¾åƒé€‰æ‹©4ä¸ªç‚¹ 2. è‡ªåŠ¨è®¡ç®—å•åº”æ€§çŸ©é˜µ 3. ç”Ÿæˆé¸Ÿç°å›¾ \"\"\" def __init__(self, img): \"\"\" åˆå§‹åŒ– å‚æ•°ï¼š img: è¾“å…¥å›¾åƒï¼ˆnumpyæ•°ç»„ï¼‰ \"\"\" self.img = img.copy() self.display_img = img.copy() self.points = [] self.max_points = 4 self.window_name = 'é€‰æ‹©4ä¸ªç‚¹ï¼ˆæŒ‰é¡ºåºï¼šå·¦ä¸‹â†’å³ä¸‹â†’å³ä¸Šâ†’å·¦ä¸Šï¼‰' def mouse_callback(self, event, x, y, flags, param): \"\"\"é¼ æ ‡å›è°ƒå‡½æ•°\"\"\" if event == cv2.EVENT_LBUTTONDOWN: if len(self.points) self.max_points: self.points.append([x, y]) print(f\"ç‚¹ {len(self.points)}: ({x}, {y})\") self.draw_points() # å¦‚æœå·²ç»é€‰æ‹©äº†4ä¸ªç‚¹ï¼Œè‡ªåŠ¨å¤„ç† if len(self.points) == self.max_points: print(\" âœ… å·²é€‰æ‹©4ä¸ªç‚¹ï¼Œè®¡ç®—ä¸­...\") cv2.waitKey(1000) cv2.destroyWindow(self.window_name) def draw_points(self): \"\"\"ç»˜åˆ¶å·²é€‰æ‹©çš„ç‚¹\"\"\" self.display_img = self.img.copy() for i, pt in enumerate(self.points): # ç»˜åˆ¶åœ†ç‚¹ cv2.circle(self.display_img, tuple(pt), 8, (0, 255, 0), -1) # ç»˜åˆ¶åºå· cv2.putText( self.display_img, str(i+1), (pt[0]+15, pt[1]), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), ```python ) # ç»˜åˆ¶è¿çº¿ if len(self.points) 1: pts = np.array(self.points, np.int32) cv2.polylines( self.display_img, [pts], len(self.points) == 4, (255, 0, 0), ) # æ˜¾ç¤ºæç¤ºä¿¡æ¯ info = f\"å·²é€‰æ‹© {len(self.points)}/{self.max_points} ä¸ªç‚¹\" cv2.putText( self.display_img, info, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), ) cv2.imshow(self.window_name, self.display_img) def select_points(self): \"\"\" äº¤äº’å¼é€‰æ‹©ç‚¹ è¿”å›ï¼š points: 4Ã—2 numpyæ•°ç»„ï¼Œæˆ–Noneï¼ˆå¦‚æœå–æ¶ˆï¼‰ \"\"\" cv2.namedWindow(self.window_name, cv2.WINDOW_NORMAL) cv2.setMouseCallback(self.window_name, self.mouse_callback) print(\" \" + \"=\"*60) print(\"ğŸ“Œ è¯·åœ¨å›¾åƒä¸Šä¾æ¬¡ç‚¹å‡»4ä¸ªç‚¹\") print(\" é¡ºåºï¼šå·¦ä¸‹ â†’ å³ä¸‹ â†’ å³ä¸Š â†’ å·¦ä¸Š\") print(\" æç¤ºï¼šé€‰æ‹©è·¯é¢ä¸Šçš„çŸ©å½¢åŒºåŸŸ\") print(\" æŒ‰ESCå¯ä»¥å–æ¶ˆ\") print(\"=\"*60 + \" \") self.draw_points() while len(self.points) self.max_points: key = cv2.waitKey(1) 0xFF if key == 27: # ESC print(\"âŒ å·²å–æ¶ˆ\") cv2.destroyAllWindows() return None return np.array(self.points, dtype=np.float32) def compute_bird_view(self, bird_w=400, bird_h=600, margin=50): \"\"\" è®¡ç®—å¹¶æ˜¾ç¤ºé¸Ÿç°å›¾ å‚æ•°ï¼š bird_w: é¸Ÿç°å›¾å®½åº¦ï¼ˆåƒç´ ï¼‰ bird_h: é¸Ÿç°å›¾é«˜åº¦ï¼ˆåƒç´ ï¼‰ margin: è¾¹è·ï¼ˆåƒç´ ï¼‰ è¿”å›ï¼š result: åŒ…å«åŸå›¾å’Œé¸Ÿç°å›¾çš„å­—å…¸ \"\"\" # é€‰æ‹©ç‚¹ src_pts = self.select_points() if src_pts is None: return None print(\" ğŸ”„ å¤„ç†ä¸­...\") # å®šä¹‰ç›®æ ‡ç‚¹ï¼ˆä¿¯è§†å›¾ä¸­çš„çŸ©å½¢ï¼‰ dst_pts = np.array([ [margin, bird_h - margin], [bird_w - margin, bird_h - margin], [bird_w - margin, margin], [margin, margin] ], dtype=np.float32) # è®¡ç®—å•åº”æ€§çŸ©é˜µ H = compute_homography_dlt(src_pts, dst_pts) print(\" ğŸ“ å•åº”æ€§çŸ©é˜µ H:\") print(H) # å˜æ¢å›¾åƒ bird_view = cv2.warpPerspective( self.img, H, (bird_w, bird_h), flags=cv2.INTER_LINEAR ) # åœ¨åŸå›¾ä¸Šç»˜åˆ¶é€‰æ‹©çš„åŒºåŸŸ marked_img = self.img.copy() pts = src_pts.astype(np.int32).reshape((-1, 1, 2)) cv2.polylines(marked_img, [pts], True, (0, 255, 0), 3) for i, pt in enumerate(src_pts): cv2.circle(marked_img, tuple(pt.astype(int)), 10, (0, 255, 0), -1) cv2.putText( marked_img, str(i+1), tuple(pt.astype(int)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), ) # è¿”å›ç»“æœ result = { 'original': self.img, 'marked': marked_img, 'bird_view': bird_view, 'homography': H, 'src_points': src_pts, 'dst_points': dst_pts } print(\" âœ… å¤„ç†å®Œæˆï¼\") return resultdef visualize_results(result, save_path=None): \"\"\" å¯è§†åŒ–ç»“æœ å‚æ•°ï¼š result: compute_bird_viewè¿”å›çš„ç»“æœå­—å…¸ save_path: ä¿å­˜è·¯å¾„ï¼ˆå¯é€‰ï¼‰ \"\"\" if result is None: print(\"æ²¡æœ‰ç»“æœå¯æ˜¾ç¤º\") return # åˆ›å»ºå¯¹æ¯”å›¾ h1, w1 = result['marked'].shape[:2] h2, w2 = result['bird_view'].shape[:2] # è°ƒæ•´å¤§å°ä½¿é«˜åº¦ä¸€è‡´ target_h = 400 scale1 = target_h / h1 scale2 = target_h / h2 img1_resized = cv2.resize(result['marked'], (int(w1*scale1), target_h)) img2_resized = cv2.resize(result['bird_view'], (int(w2*scale2), target_h)) # æ°´å¹³æ‹¼æ¥ combined = np.hstack([img1_resized, img2_resized]) # æ·»åŠ æ ‡é¢˜ cv2.putText( combined, 'Original (Perspective)', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), ) cv2.putText( combined, 'Bird Eye View (Top-down)', (int(w1*scale1) + 10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), ) # æ˜¾ç¤º cv2.namedWindow('Result', cv2.WINDOW_NORMAL) cv2.imshow('Result', combined) # ä¿å­˜ if save_path: cv2.imwrite(str(save_path), combined) print(f\" ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: {save_path}\") print(\" æŒ‰ä»»æ„é”®å…³é—­...\") cv2.waitKey(0) cv2.destroyAllWindows()def main(): \"\"\" ä¸»å‡½æ•° \"\"\" print(\" \" + \"=\"*60) print(\"ğŸš— ä»å›¾åƒåˆ°ä¿¯è§†å›¾ï¼šå®Œæ•´å®ç°\") print(\"=\"*60) # 1. è¯»å–å›¾åƒ img_path = input(\" è¯·è¾“å…¥å›¾åƒè·¯å¾„ï¼ˆæˆ–æŒ‰å›è½¦ä½¿ç”¨é»˜è®¤ï¼‰: \").strip() if not img_path: img_path = \"input/dashcam.jpg\" print(f\" ğŸ“‚ è¯»å–å›¾åƒ: {img_path}\") img = cv2.imread(img_path) if img is None: print(f\"âŒ æ— æ³•è¯»å–å›¾åƒ: {img_path}\") return print(f\"âœ… å›¾åƒå¤§å°: {img.shape[1]} Ã— {img.shape[0]}\") # 2. åˆ›å»ºäº¤äº’å¼ç•Œé¢ bev = InteractiveBirdEyeView(img) # 3. è®¡ç®—é¸Ÿç°å›¾ result = bev.compute_bird_view( bird_w=400, bird_h=600, margin=50 ) if result is None: return # 4. æ˜¾ç¤ºç»“æœ visualize_results(result, save_path=\"output/result.jpg\") # 5. ä¿å­˜å„ä¸ªç»“æœ cv2.imwrite(\"output/marked.jpg\", result['marked']) cv2.imwrite(\"output/bird_view.jpg\", result['bird_view']) print(\" ğŸ“ æ‰€æœ‰æ–‡ä»¶å·²ä¿å­˜åˆ° output/ ç›®å½•\") print(\" âœ¨ å®Œæˆï¼\")if __name__ == \"__main__\": main() 4.4 è¿è¡Œæ•ˆæœè¿è¡Œæ­¥éª¤ï¼š# 1. è¿è¡Œç¨‹åºpython main.py# 2. è¾“å…¥å›¾åƒè·¯å¾„ï¼ˆæˆ–ä½¿ç”¨é»˜è®¤ï¼‰è¯·è¾“å…¥å›¾åƒè·¯å¾„ï¼ˆæˆ–æŒ‰å›è½¦ä½¿ç”¨é»˜è®¤ï¼‰:# 3. åœ¨å¼¹å‡ºçš„çª—å£ä¸­ç‚¹å‡»4ä¸ªç‚¹ğŸ“Œ è¯·åœ¨å›¾åƒä¸Šä¾æ¬¡ç‚¹å‡»4ä¸ªç‚¹ é¡ºåºï¼šå·¦ä¸‹ â†’ å³ä¸‹ â†’ å³ä¸Š â†’ å·¦ä¸Š# 4. æŸ¥çœ‹ç»“æœâœ… å¤„ç†å®Œæˆï¼ğŸ’¾ ç»“æœå·²ä¿å­˜åˆ°: output/result.jpgæ•ˆæœå±•ç¤ºï¼šè¾“å…¥å›¾åƒï¼ˆæ–œè§†è§’ï¼‰ è¾“å‡ºå›¾åƒï¼ˆä¿¯è§†å›¾ï¼‰ â•±â•² â”Œâ”€â”€â”€â”€â”€â”€â” â•± â•² â†’ â”‚ â”‚ â•± â•² å˜æ¢ â”‚ â”‚ â•±______â•² â””â”€â”€â”€â”€â”€â”€â”˜ æ¢¯å½¢æ•ˆæœ çŸ©å½¢æ•ˆæœğŸ› ç¬¬äº”ç« ï¼šè°ƒè¯•ä¸ä¼˜åŒ–5.1 å¸¸è§é—®é¢˜é—®é¢˜1ï¼šå›¾åƒå˜å½¢ä¸¥é‡åŸå› ï¼šç‚¹é€‰æ‹©ä¸åˆç†è§£å†³æ–¹æ¡ˆï¼šdef check_points_distribution(pts): \"\"\"æ£€æŸ¥ç‚¹çš„åˆ†å¸ƒ\"\"\" std_x = np.std(pts[:, 0]) std_y = np.std(pts[:, 1]) if std_x 50 or std_y 50: print(\"âš ï¸ è­¦å‘Šï¼šç‚¹åˆ†å¸ƒè¿‡äºé›†ä¸­ï¼\") print(f\" Xæ–¹å‘æ ‡å‡†å·®: {std_x:.1f}\") print(f\" Yæ–¹å‘æ ‡å‡†å·®: {std_y:.1f}\") print(\" å»ºè®®ï¼šé€‰æ‹©æ›´åˆ†æ•£çš„ç‚¹\") return False return Trueé—®é¢˜2ï¼šé¸Ÿç°å›¾æœ‰é»‘è¾¹åŸå› ï¼šè¾“å‡ºå›¾åƒèŒƒå›´è®¾ç½®ä¸å½“è§£å†³æ–¹æ¡ˆï¼šdef compute_output_size(img, H, src_pts): \"\"\"è‡ªåŠ¨è®¡ç®—è¾“å‡ºå›¾åƒå¤§å°\"\"\" h, w = img.shape[:2] # å˜æ¢å›¾åƒçš„å››ä¸ªè§’ç‚¹ corners = np.array([ [0, 0], [w, 0], [w, h], [0, h] ], dtype=np.float32) # åº”ç”¨Hå˜æ¢ corners_homo = np.column_stack([corners, np.ones(4)]) transformed = (H @ corners_homo.T).T transformed = transformed[:, :2] / transformed[:, 2:3] # è®¡ç®—è¾¹ç•Œ min_x = np.min(transformed[:, 0]) max_x = np.max(transformed[:, 0]) min_y = np.min(transformed[:, 1]) max_y = np.max(transformed[:, 1]) out_w = int(max_x - min_x) out_h = int(max_y - min_y) return out_w, out_hé—®é¢˜3ï¼šè¿è¡Œé€Ÿåº¦æ…¢è§£å†³æ–¹æ¡ˆï¼š# ä½¿ç”¨æ›´å¿«çš„æ’å€¼æ–¹æ³•result = cv2.warpPerspective( img, H, (out_w, out_h), flags=cv2.INTER_LINEAR # åŒçº¿æ€§æ’å€¼ï¼ˆå¿«ï¼‰ # flags=cv2.INTER_CUBIC # åŒä¸‰æ¬¡æ’å€¼ï¼ˆæ…¢ä½†æ›´å¥½ï¼‰)5.2 æ€§èƒ½ä¼˜åŒ–infographic list-grid-badge-carddata title ä¼˜åŒ–æŠ€å·§ items - label å‘é‡åŒ–è®¡ç®— desc é¿å…Pythonå¾ªç¯ icon mdi:speedometer - label GPUåŠ é€Ÿ desc ä½¿ç”¨cv2.cudaæ¨¡å— icon mdi:chip - label å¤šçº¿ç¨‹ desc å¹¶è¡Œå¤„ç†å¤šå¼ å›¾åƒ icon mdi:lan - label ç¼“å­˜ç»“æœ desc é¿å…é‡å¤è®¡ç®— icon mdi:cached ğŸš€ ç¬¬å…­ç« ï¼šè¿›é˜¶åº”ç”¨6.1 è§†é¢‘æµå¤„ç†def process_video(video_path, H): \"\"\"å¯¹è§†é¢‘åº”ç”¨é¸Ÿç°å˜æ¢\"\"\" cap = cv2.VideoCapture(video_path) # è·å–è§†é¢‘å‚æ•° fps = int(cap.get(cv2.CAP_PROP_FPS)) w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # åˆ›å»ºè¾“å‡ºè§†é¢‘ fourcc = cv2.VideoWriter_fourcc(*'mp4v') out = cv2.VideoWriter('output.mp4', fourcc, fps, (400, 600)) print(f\"å¤„ç†è§†é¢‘ï¼š{fps} FPS, {w}x{h}\") frame_count = 0 while cap.isOpened(): ret, frame = cap.read() if not ret: break # åº”ç”¨å˜æ¢ bird_view = cv2.warpPerspective(frame, H, (400, 600)) # å†™å…¥è¾“å‡º out.write(bird_view) frame_count += 1 if frame_count % 30 == 0: print(f\"å·²å¤„ç† {frame_count} å¸§\") cap.release() out.release() cv2.destroyAllWindows() print(f\"âœ… å®Œæˆï¼å…±å¤„ç† {frame_count} å¸§\") 6.2 å®æ—¶ç›¸æœºæ ‡å®šdef realtime_calibration(): \"\"\"å®æ—¶ç›¸æœºæ ‡å®šå’Œé¸Ÿç°å›¾ç”Ÿæˆ\"\"\" cap = cv2.VideoCapture(0) H = None calibrated = False print(\"æŒ‰ 'c' è¿›å…¥æ ‡å®šæ¨¡å¼\") print(\"æŒ‰ 'q' é€€å‡º\") while True: ret, frame = cap.read() if not ret: break if calibrated and H is not None: # åº”ç”¨å˜æ¢ bird_view = cv2.warpPerspective(frame, H, (400, 600)) # å¹¶æ’æ˜¾ç¤º display = np.hstack([ cv2.resize(frame, (400, 300)), cv2.resize(bird_view, (400, 300)) ]) cv2.imshow('Camera | Bird View', display) else: cv2.imshow('Camera', frame) key = cv2.waitKey(1) 0xFF if key == ord('c'): print(\" å¼€å§‹æ ‡å®š...\") bev = InteractiveBirdEyeView(frame) result = bev.compute_bird_view() if result is not None: H = result['homography'] calibrated = True print(\"âœ… æ ‡å®šå®Œæˆï¼\") elif key == ord('q'): break cap.release() cv2.destroyAllWindows() ğŸ“Š æ€»ç»“å­¦ä¹ æˆæœinfographic list-column-done-listdata title ä½ å·²ç»æŒæ¡ items - label ç†è§£é€è§†æŠ•å½±åŸç† desc 3Dåˆ°2Dçš„è½¬æ¢ - label è®¡ç®—æ¶ˆå¤±ç‚¹ desc ä»å¹³è¡Œçº¿æ‰¾äº¤ç‚¹ - label ä¼°è®¡ç›¸æœºå‚æ•° desc ç„¦è·å’Œè§’åº¦ - label å•åº”æ€§å˜æ¢ desc DLTç®—æ³• - label å®Œæ•´ä»£ç å®ç° desc Python+OpenCV æ ¸å¿ƒçŸ¥è¯†ç‚¹| æ¦‚å¿µ | å…¬å¼/æ–¹æ³• | åº”ç”¨ | | é€è§†æŠ•å½± | `u = K[R\\|t]X` | 3Dâ†’2Dè½¬æ¢ || æ¶ˆå¤±ç‚¹ | å¹³è¡Œçº¿äº¤ç‚¹ | ä¼°è®¡ç›¸æœºæœå‘ || å•åº”æ€§ | `x' = Hx` | å¹³é¢å˜æ¢ || DLTç®—æ³• | SVDæ±‚è§£ | è®¡ç®—HçŸ©é˜µ |---### åç»­å­¦ä¹ æ–¹å‘```infographicinfographic hierarchy-tree-curved-line-rounded-rect-nodedata title è¿›é˜¶è·¯å¾„ items - label æ·±åº¦å­¦ä¹  children: - label ç¥ç»ç½‘ç»œæ£€æµ‹å…³é”®ç‚¹ - label ç«¯åˆ°ç«¯è§†è§’è½¬æ¢ - label 3Dé‡å»º children: - label å¤šè§†è§’3Dæ¢å¤ - label SLAMæŠ€æœ¯ - label å®æ—¶ç³»ç»Ÿ children: - label GPUåŠ é€Ÿ - label åµŒå…¥å¼éƒ¨ç½² ğŸ‰ ç»“è¯­ä½ å·²ç»æŒæ¡äº†ä»å›¾åƒåˆ°ä¿¯è§†å›¾è½¬æ¢çš„å®Œæ•´çŸ¥è¯†å’ŒæŠ€èƒ½ï¼ è®°ä½ï¼š ğŸ“š ç†è®ºæ˜¯åŸºç¡€ ğŸ’» ä»£ç æ˜¯å®è·µ ğŸ› è°ƒè¯•æ˜¯æˆé•¿ ğŸš€ åº”ç”¨æ˜¯ç›®æ ‡ ç»§ç»­æ¢ç´¢ï¼Œä¸æ–­è¿›æ­¥ï¼ âœ¨ ğŸ“š å‚è€ƒèµ„æ–™å­¦ä¹ èµ„æº OpenCVå®˜æ–¹æ–‡æ¡£ Multiple View Geometry (Hartley Zisserman) è®¡ç®—æœºè§†è§‰åŸºç¡€ ç›¸å…³è®ºæ–‡ Homography Estimation: A Review Camera Calibration Methods Perspective Transformation in Computer Vision ç‰ˆæœ¬ä¿¡æ¯ï¼š ç‰ˆæœ¬ï¼š1.0.0 åˆ›å»ºæ—¥æœŸï¼š2026-01-19 ç¼–ç¨‹è¯­è¨€ï¼šPython 3.7+ ä¸»è¦ä¾èµ–ï¼šOpenCV, NumPy ç¥ä½ å­¦ä¹ æ„‰å¿«ï¼å¦‚æœ‰ä»»ä½•é—®é¢˜ï¼Œæ¬¢è¿äº¤æµè®¨è®ºã€‚ ğŸ‰","tags":["è®¡ç®—æœºè§†è§‰"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"å¦‚ä½•ä½¿ç”¨Hexoå†™åšå®¢","path":"/2026/01/18/å¦‚ä½•ä½¿ç”¨Hexoå†™åšå®¢/","content":"ç®€ä»‹è¿™æ˜¯ä¸€ç¯‡ç¤ºä¾‹æ–‡ç« ï¼Œæ¼”ç¤ºå¦‚ä½•ä½¿ç”¨ Hexo å†™åšå®¢ã€‚ æ­¥éª¤1. åˆ›å»ºæ–°æ–‡ç« npx hexo new \"æ–‡ç« æ ‡é¢˜\" 2. ç¼–è¾‘æ–‡ç« åœ¨ source/_posts/ ç›®å½•ä¸‹æ‰¾åˆ°åˆšåˆ›å»ºçš„ Markdown æ–‡ä»¶è¿›è¡Œç¼–è¾‘ã€‚ 3. æœ¬åœ°é¢„è§ˆnpx hexo server è®¿é—® http://localhost:4000 æŸ¥çœ‹æ•ˆæœã€‚ 4. éƒ¨ç½²npx hexo deploy æ€»ç»“ä½¿ç”¨ Hexo å†™åšå®¢éå¸¸ç®€å•ï¼ ä½ å¯ä»¥åœ¨è¿™é‡Œç»§ç»­å†™æ›´å¤šå†…å®¹â€¦","tags":["å·¥å…·"],"categories":["å·¥å…·"]},{"title":"ğŸ“š LeetCode 150 - æ•°ç»„ä¸å­—ç¬¦ä¸²ä¸“é¢˜","path":"/2026/01/18/leetcode-150-array-string/","content":"ğŸ“š æ•°ç»„ä¸å­—ç¬¦ä¸²ä¸“é¢˜ (15é¢˜) ğŸ¯ æ ¸å¿ƒæŠ€å·§ï¼šåŒæŒ‡é’ˆã€å‰ç¼€å’Œã€è´ªå¿ƒã€æ¨¡æ‹Ÿ ğŸ—ºï¸ çŸ¥è¯†å›¾è°± â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ æ•°ç»„ä¸å­—ç¬¦ä¸² â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â–¼ â–¼ â–¼â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ åŒæŒ‡é’ˆ â”‚ â”‚ è´ªå¿ƒ â”‚ â”‚ å‰ç¼€å’Œ â”‚â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚ â”‚ â”‚â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”´â”€â”€â”€â”€â”â”‚åˆå¹¶æ•°ç»„ â”‚ â”‚è·³è·ƒæ¸¸æˆ â”‚ â”‚é™¤è‡ªèº«ä¹˜ç§¯â”‚â”‚ç§»é™¤å…ƒç´  â”‚ â”‚ä¹°å–è‚¡ç¥¨ â”‚ â”‚ â”‚â”‚åˆ é™¤é‡å¤ â”‚ â”‚åˆ†å‘ç³–æœ â”‚ â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 1ï¸âƒ£ LC 88. åˆå¹¶ä¸¤ä¸ªæœ‰åºæ•°ç»„ ğŸŸ¢é¢˜ç›®æè¿°å°†ä¸¤ä¸ªæœ‰åºæ•°ç»„ nums1 å’Œ nums2 åˆå¹¶åˆ° nums1 ä¸­ï¼Œä½¿å…¶æœ‰åºã€‚ ğŸ¨ å›¾è§£æ€è·¯ä»åå¾€å‰å¡«å……ï¼Œé¿å…è¦†ç›–ï¼nums1 = [1, 2, 3, 0, 0, 0] nums2 = [2, 5, 6] â†‘ â†‘ p1 p2 â†‘ p (å¡«å……ä½ç½®)Step 1: æ¯”è¾ƒ 3 vs 6 â†’ å¡« 6nums1 = [1, 2, 3, 0, 0, 6]Step 2: æ¯”è¾ƒ 3 vs 5 â†’ å¡« 5 nums1 = [1, 2, 3, 0, 5, 6]Step 3: æ¯”è¾ƒ 3 vs 2 â†’ å¡« 3nums1 = [1, 2, 3, 3, 5, 6]... æœ€ç»ˆç»“æœ [1, 2, 2, 3, 5, 6] ğŸ’» ä»£ç å®ç°def merge(nums1, m, nums2, n): p1, p2, p = m - 1, n - 1, m + n - 1 while p2 = 0: # nums2 è¿˜æœ‰å…ƒç´  if p1 = 0 and nums1[p1] nums2[p2]: nums1[p] = nums1[p1] p1 -= 1 else: nums1[p] = nums2[p2] p2 -= 1 p -= 1 ğŸ§  è®°å¿†å£è¯€ â€œä»åå¾€å‰å¡«ï¼Œå¤§çš„å…ˆè½ä½â€ 2ï¸âƒ£ LC 27. ç§»é™¤å…ƒç´  ğŸŸ¢é¢˜ç›®æè¿°åŸåœ°ç§»é™¤æ•°ç»„ä¸­ç­‰äº val çš„å…ƒç´ ï¼Œè¿”å›æ–°é•¿åº¦ã€‚ ğŸ¨ å›¾è§£æ€è·¯å¿«æ…¢æŒ‡é’ˆï¼šæ…¢æŒ‡é’ˆæ ‡è®°æœ‰æ•ˆä½ç½®ï¼Œå¿«æŒ‡é’ˆéå†val = 3 s f[3, 2, 2, 3, 4]f=0: nums[0]=3, è·³è¿‡f=1: nums[1]=2â‰ 3, nums[s]=2, s++ s f[2, 2, 2, 3, 4]f=2: nums[2]=2â‰ 3, nums[s]=2, s++ s f[2, 2, 2, 3, 4]f=3: nums[3]=3, è·³è¿‡f=4: nums[4]=4â‰ 3, nums[s]=4, s++æœ€ç»ˆ: [2, 2, 4, _, _], è¿”å› 3 ğŸ’» ä»£ç å®ç°def removeElement(nums, val): slow = 0 for fast in range(len(nums)): if nums[fast] != val: nums[slow] = nums[fast] slow += 1 return slow ğŸ§  è®°å¿†å£è¯€ â€œå¿«æŒ‡é’ˆæ¢è·¯ï¼Œæ…¢æŒ‡é’ˆæ”¶è´§â€ 3ï¸âƒ£ LC 26. åˆ é™¤æœ‰åºæ•°ç»„ä¸­çš„é‡å¤é¡¹ ğŸŸ¢ğŸ¨ å›¾è§£æ€è·¯æœ‰åºæ•°ç»„ï¼Œç›¸åŒå…ƒç´ å¿…ç›¸é‚»ï¼ s f[1, 1, 2, 2, 3]f=1: nums[1]=nums[0], è·³è¿‡f=2: nums[2]â‰ nums[1], s++, nums[s]=2 s f[1, 2, 2, 2, 3]f=4: nums[4]â‰ nums[3], s++, nums[s]=3 s[1, 2, 3, _, _]è¿”å› s+1 = 3 ğŸ’» ä»£ç å®ç°def removeDuplicates(nums): if not nums: return 0 slow = 0 for fast in range(1, len(nums)): if nums[fast] != nums[slow]: slow += 1 nums[slow] = nums[fast] return slow + 1 4ï¸âƒ£ LC 80. åˆ é™¤æœ‰åºæ•°ç»„ä¸­çš„é‡å¤é¡¹ II ğŸŸ¡é¢˜ç›®æè¿°æ¯ä¸ªå…ƒç´ æœ€å¤šå‡ºç° ä¸¤æ¬¡ã€‚ ğŸ¨ é€šç”¨æ¨¡æ¿def removeDuplicates(nums, k=2): \"\"\"å…è®¸æ¯ä¸ªå…ƒç´ æœ€å¤šå‡ºç° k æ¬¡\"\"\" slow = 0 for num in nums: if slow k or num != nums[slow - k]: nums[slow] = num slow += 1 return slow ğŸ§  è®°å¿†å£è¯€ â€œæ£€æŸ¥ k ä½ä¹‹å‰ï¼Œä¸åŒæ‰èƒ½è¿›â€ 5ï¸âƒ£ LC 169. å¤šæ•°å…ƒç´  ğŸŸ¢é¢˜ç›®æè¿°æ‰¾å‡ºå‡ºç°æ¬¡æ•°è¶…è¿‡ n/2 çš„å…ƒç´ ã€‚ ğŸ¨ Boyer-Moore æŠ•ç¥¨ç®—æ³•æŠŠå¤šæ•°å…ƒç´ çœ‹ä½œ +1ï¼Œå…¶ä»–å…ƒç´ çœ‹ä½œ -1æœ€ç»ˆ +1 ä¸€å®šæ¯” -1 å¤šï¼nums = [2, 2, 1, 1, 1, 2, 2]candidate=2, count=1 â†’ [2]candidate=2, count=2 â†’ [2,2]candidate=2, count=1 â†’ é‡åˆ°1ï¼ŒæŠµæ¶ˆcandidate=2, count=0 â†’ é‡åˆ°1ï¼ŒæŠµæ¶ˆcandidate=1, count=1 â†’ count=0æ—¶æ¢äººcandidate=1, count=0 â†’ é‡åˆ°2ï¼ŒæŠµæ¶ˆcandidate=2, count=1 â†’ count=0æ—¶æ¢äººæœ€ç»ˆ candidate = 2 âœ“ ğŸ’» ä»£ç å®ç°def majorityElement(nums): candidate, count = None, 0 for num in nums: if count == 0: candidate = num count += 1 if num == candidate else -1 return candidate ğŸ§  è®°å¿†å£è¯€ â€œåŒåŠ å¼‚å‡ï¼Œå½’é›¶æ¢å¸…â€ 6ï¸âƒ£ LC 189. è½®è½¬æ•°ç»„ ğŸŸ¡é¢˜ç›®æè¿°å°†æ•°ç»„å‘å³è½®è½¬ k ä½ã€‚ ğŸ¨ ä¸‰æ¬¡ç¿»è½¬æ³•nums = [1,2,3,4,5,6,7], k = 3Step 1: æ•´ä½“ç¿»è½¬[7,6,5,4,3,2,1]Step 2: ç¿»è½¬å‰ k ä¸ª[5,6,7,4,3,2,1]Step 3: ç¿»è½¬å n-k ä¸ª [5,6,7,1,2,3,4] âœ“ ğŸ’» ä»£ç å®ç°def rotate(nums, k): n = len(nums) k %= n # å¤„ç† k n çš„æƒ…å†µ def reverse(left, right): while left right: nums[left], nums[right] = nums[right], nums[left] left, right = left + 1, right - 1 reverse(0, n - 1) # æ•´ä½“ç¿»è½¬ reverse(0, k - 1) # å‰kä¸ª reverse(k, n - 1) # ån-kä¸ª ğŸ§  è®°å¿†å£è¯€ â€œå…¨è½¬å‰è½¬åï¼Œä¸‰æ­¥åˆ°ä½â€ 7ï¸âƒ£ LC 121. ä¹°å–è‚¡ç¥¨çš„æœ€ä½³æ—¶æœº ğŸŸ¢é¢˜ç›®æè¿°åªèƒ½ä¹°å–ä¸€æ¬¡ï¼Œæ±‚æœ€å¤§åˆ©æ¶¦ã€‚ ğŸ¨ å›¾è§£æ€è·¯ç»´æŠ¤ã€Œå†å²æœ€ä½ä»·ã€ï¼Œè®¡ç®—å½“å¤©å–å‡ºåˆ©æ¶¦prices = [7, 1, 5, 3, 6, 4] â”‚ â”‚ â”‚ â”‚ â”‚ â”‚min_price â”‚ 1 1 1 1 1profit 0 0 4 2 5 3 â†‘ æœ€å¤§åˆ©æ¶¦ = 5 ğŸ’» ä»£ç å®ç°def maxProfit(prices): min_price = float('inf') max_profit = 0 for price in prices: min_price = min(min_price, price) max_profit = max(max_profit, price - min_price) return max_profit ğŸ§  è®°å¿†å£è¯€ â€œè®°ä½æœ€ä½ç‚¹ï¼Œæ—¶åˆ»ç®—å·®ä»·â€ 8ï¸âƒ£ LC 55. è·³è·ƒæ¸¸æˆ ğŸŸ¡é¢˜ç›®æè¿°åˆ¤æ–­èƒ½å¦è·³åˆ°æœ€åä¸€ä¸ªä½ç½®ã€‚ ğŸ¨ è´ªå¿ƒæ€è·¯ç»´æŠ¤èƒ½åˆ°è¾¾çš„æœ€è¿œä½ç½® max_reachnums = [2, 3, 1, 1, 4] â†‘i=0: max_reach = max(0, 0+2) = 2i=1: max_reach = max(2, 1+3) = 4 â‰¥ 4 âœ“ å¯è¾¾ï¼nums = [3, 2, 1, 0, 4]i=0: max_reach = 3i=1: max_reach = 3i=2: max_reach = 3i=3: max_reach = 3 4 i=4: i max_reachï¼Œæ— æ³•åˆ°è¾¾ âœ— ğŸ’» ä»£ç å®ç°def canJump(nums): max_reach = 0 for i in range(len(nums)): if i max_reach: return False max_reach = max(max_reach, i + nums[i]) return True ğŸ§  è®°å¿†å£è¯€ â€œèµ°ä¸€æ­¥ç®—ä¸€æ­¥ï¼Œèƒ½åˆ°å°±æ›´æ–°â€ 9ï¸âƒ£ LC 45. è·³è·ƒæ¸¸æˆ II ğŸŸ¡é¢˜ç›®æè¿°æ±‚åˆ°è¾¾æœ€åä½ç½®çš„æœ€å°‘è·³è·ƒæ¬¡æ•°ã€‚ ğŸ¨ BFS æ€æƒ³æŠŠæ¯ä¸€è·³èƒ½åˆ°çš„èŒƒå›´çœ‹ä½œä¸€å±‚nums = [2, 3, 1, 1, 4] â†‘å±‚0: ä½ç½®0ï¼Œèƒ½åˆ° [1,2]å±‚1: ä½ç½®1-2ï¼Œèƒ½åˆ° [2,3,4] â†’ åˆ°è¾¾ç»ˆç‚¹ï¼è·³è·ƒæ¬¡æ•° = 2 ğŸ’» ä»£ç å®ç°def jump(nums): jumps = 0 cur_end = 0 # å½“å‰è·³è·ƒèƒ½åˆ°çš„è¾¹ç•Œ cur_farthest = 0 # ä¸‹ä¸€è·³èƒ½åˆ°çš„æœ€è¿œ for i in range(len(nums) - 1): cur_farthest = max(cur_farthest, i + nums[i]) if i == cur_end: # åˆ°è¾¾è¾¹ç•Œï¼Œå¿…é¡»è·³ jumps += 1 cur_end = cur_farthest return jumps ğŸ”Ÿ LC 238. é™¤è‡ªèº«ä»¥å¤–æ•°ç»„çš„ä¹˜ç§¯ ğŸŸ¡é¢˜ç›®æè¿°è¿”å›æ•°ç»„ï¼Œanswer[i] ç­‰äº nums ä¸­é™¤ nums[i] ä¹‹å¤–å…¶ä½™å„å…ƒç´ çš„ä¹˜ç§¯ã€‚ ğŸ¨ å‰ç¼€ç§¯ Ã— åç¼€ç§¯nums = [1, 2, 3, 4]å‰ç¼€ç§¯ = [1, 1, 2, 6] (ä¸å«å½“å‰)åç¼€ç§¯ = [24, 12, 4, 1] (ä¸å«å½“å‰)ç»“æœ = [24, 12, 8, 6] (å‰ç¼€ Ã— åç¼€) ğŸ’» ä»£ç å®ç° (O(1) ç©ºé—´)def productExceptSelf(nums): n = len(nums) result = [1] * n # è®¡ç®—å‰ç¼€ç§¯ prefix = 1 for i in range(n): result[i] = prefix prefix *= nums[i] # è®¡ç®—åç¼€ç§¯å¹¶ç›¸ä¹˜ suffix = 1 for i in range(n - 1, -1, -1): result[i] *= suffix suffix *= nums[i] return result ğŸ§  è®°å¿†å£è¯€ â€œå·¦è¾¹ä¹˜ä¸€éï¼Œå³è¾¹ä¹˜ä¸€éâ€ ğŸ“Š æœ¬ç« æ€»ç»“æ ¸å¿ƒæ¨¡å¼é€ŸæŸ¥è¡¨ æ¨¡å¼ é€‚ç”¨åœºæ™¯ å…¸å‹é¢˜ç›® å¿«æ…¢æŒ‡é’ˆ åŸåœ°ä¿®æ”¹æ•°ç»„ 26, 27, 80 å‰åæŒ‡é’ˆ æœ‰åºæ•°ç»„åˆå¹¶ 88 è´ªå¿ƒ æœ€ä¼˜è§£é—®é¢˜ 55, 45, 121 å‰ç¼€å’Œ/ç§¯ åŒºé—´è®¡ç®— 238 æŠ•ç¥¨ç®—æ³• å¤šæ•°å…ƒç´  169 ç¿»è½¬æŠ€å·§ è½®è½¬/ç¿»è½¬ 189 ğŸ§  å…¨ç« è®°å¿†å£è¯€åˆç§»åˆ åˆ å¤šï¼Œè½®ä¹°ä¹°è·³è·³Hæ’é™¤åŠ ç³–ï¼Œæ•°ç»„åäº”å¦™åˆ - åˆå¹¶æ•°ç»„ (88)ç§» - ç§»é™¤å…ƒç´  (27) åˆ åˆ  - åˆ é™¤é‡å¤ I/II (26, 80)å¤š - å¤šæ•°å…ƒç´  (169)è½® - è½®è½¬æ•°ç»„ (189)ä¹°ä¹° - ä¹°å–è‚¡ç¥¨ I/II (121, 122)è·³è·³ - è·³è·ƒæ¸¸æˆ I/II (55, 45)H - HæŒ‡æ•° (274)æ’ - O(1)æ’å…¥åˆ é™¤ (380)é™¤ - é™¤è‡ªèº«ä¹˜ç§¯ (238)åŠ  - åŠ æ²¹ç«™ (134)ç³– - åˆ†å‘ç³–æœ (135) ğŸ“– ä¸‹ä¸€ç¯‡ï¼šåŒæŒ‡é’ˆä¸“é¢˜","tags":["LeetCode"],"categories":["ç®—æ³•"]},{"title":"ğŸ”¢ LeetCode 150 - äºŒåˆ†æŸ¥æ‰¾ä¸“é¢˜","path":"/2026/01/18/leetcode-150-binary-search/","content":"ğŸ”¢ äºŒåˆ†æŸ¥æ‰¾ä¸“é¢˜ (7é¢˜) ğŸ¯ æ ¸å¿ƒæ€æƒ³ï¼šæ¯æ¬¡æ’é™¤ä¸€åŠçš„æœç´¢ç©ºé—´ï¼Œæ—¶é—´å¤æ‚åº¦ O(log n) ğŸ—ºï¸ äºŒåˆ†æŸ¥æ‰¾çš„æœ¬è´¨â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ äºŒåˆ†æŸ¥æ‰¾çš„æœ¬è´¨ â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ â”‚â”‚ äºŒåˆ†æŸ¥æ‰¾çš„æœ¬è´¨æ˜¯ï¼šåœ¨æœ‰åºï¼ˆæˆ–éƒ¨åˆ†æœ‰åºï¼‰çš„ç©ºé—´ä¸­ â”‚â”‚ æ‰¾åˆ°æ»¡è¶³æŸä¸ªæ¡ä»¶çš„è¾¹ç•Œç‚¹ â”‚â”‚ â”‚â”‚ ğŸ”µğŸ”µğŸ”µğŸ”µğŸ”µğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ğŸ”´ â”‚â”‚ â†‘ â†‘ â”‚â”‚ è“è‰²åŒºåŸŸ çº¢è‰²åŒºåŸŸ â”‚â”‚ (ä¸æ»¡è¶³) (æ»¡è¶³) â”‚â”‚ â”‚â”‚ ç›®æ ‡ï¼šæ‰¾åˆ°ç¬¬ä¸€ä¸ªçº¢è‰²ï¼ˆæˆ–æœ€åä¸€ä¸ªè“è‰²ï¼‰ â”‚â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ğŸ”§ äºŒåˆ†æŸ¥æ‰¾æ¨¡æ¿æ¨¡æ¿1ï¼šæ‰¾ç¬¬ä¸€ä¸ªæ»¡è¶³æ¡ä»¶çš„ä½ç½®def binary_search_first(nums, target): left, right = 0, len(nums) # å·¦é—­å³å¼€ while left right: mid = left + (right - left) // 2 if condition(mid): # æ»¡è¶³æ¡ä»¶ right = mid # ç­”æ¡ˆåœ¨ [left, mid] else: left = mid + 1 # ç­”æ¡ˆåœ¨ [mid+1, right) return left # ç¬¬ä¸€ä¸ªæ»¡è¶³æ¡ä»¶çš„ä½ç½® æ¨¡æ¿2ï¼šæ‰¾æœ€åä¸€ä¸ªæ»¡è¶³æ¡ä»¶çš„ä½ç½®def binary_search_last(nums, target): left, right = 0, len(nums) while left right: mid = left + (right - left + 1) // 2 # å‘ä¸Šå–æ•´ if condition(mid): # æ»¡è¶³æ¡ä»¶ left = mid # ç­”æ¡ˆåœ¨ [mid, right) else: right = mid - 1 # ç­”æ¡ˆåœ¨ [left, mid-1] return left # æœ€åä¸€ä¸ªæ»¡è¶³æ¡ä»¶çš„ä½ç½® 1ï¸âƒ£ LC 35. æœç´¢æ’å…¥ä½ç½® ğŸŸ¢é¢˜ç›®æè¿°åœ¨æ’åºæ•°ç»„ä¸­æ‰¾åˆ°ç›®æ ‡å€¼çš„ä½ç½®ï¼Œå¦‚æœä¸å­˜åœ¨åˆ™è¿”å›åº”è¯¥æ’å…¥çš„ä½ç½®ã€‚ ğŸ¨ å›¾è§£æ€è·¯nums = [1, 3, 5, 6], target = 5æ‰¾ç¬¬ä¸€ä¸ª = target çš„ä½ç½® 1 3 5 6 â†‘ â†‘ 5 =5äºŒåˆ†æŸ¥æ‰¾ï¼šåˆå§‹: left=0, right=4mid=2, nums[2]=5 = 5, right=2mid=1, nums[1]=3 5, left=2left == right, è¿”å› 2 ğŸ’» ä»£ç å®ç°def searchInsert(nums: list, target: int) - int: left, right = 0, len(nums) while left right: mid = left + (right - left) // 2 if nums[mid] = target: right = mid else: left = mid + 1 return left ğŸ§  è®°å¿†å£è¯€ â€œæ‰¾ç¬¬ä¸€ä¸ªä¸å°äºç›®æ ‡çš„ä½ç½®â€ 2ï¸âƒ£ LC 74. æœç´¢äºŒç»´çŸ©é˜µ ğŸŸ¡é¢˜ç›®æè¿°åœ¨è¡Œåˆ—éƒ½æœ‰åºçš„äºŒç»´çŸ©é˜µä¸­æœç´¢ç›®æ ‡å€¼ã€‚ ğŸ¨ å›¾è§£æ€è·¯matrix:[1, 3, 5, 7][10, 11, 16, 20][23, 30, 34, 60]å°† 2D çŸ©é˜µçœ‹ä½œ 1D æ•°ç»„:[1, 3, 5, 7, 10, 11, 16, 20, 23, 30, 34, 60]åæ ‡è½¬æ¢:index â†’ (index // n, index % n) ğŸ’» ä»£ç å®ç°def searchMatrix(matrix: list, target: int) - bool: m, n = len(matrix), len(matrix[0]) left, right = 0, m * n while left right: mid = left + (right - left) // 2 row, col = mid // n, mid % n if matrix[row][col] == target: return True elif matrix[row][col] target: left = mid + 1 else: right = mid return False ğŸ§  è®°å¿†å£è¯€ â€œ2Då˜1Dï¼Œé™¤æ³•å–è¡Œï¼Œä½™æ•°å–åˆ—â€ 3ï¸âƒ£ LC 162. å¯»æ‰¾å³°å€¼ ğŸŸ¡é¢˜ç›®æè¿°æ‰¾åˆ°æ•°ç»„ä¸­ä»»æ„ä¸€ä¸ªå³°å€¼å…ƒç´ çš„ç´¢å¼•ï¼ˆæ¯”å·¦å³é‚»å±…éƒ½å¤§ï¼‰ã€‚ ğŸ¨ å›¾è§£æ€è·¯nums = [1, 2, 1, 3, 5, 6, 4]å³°å€¼: ç´¢å¼• 1 (å€¼ä¸º2) æˆ– ç´¢å¼• 5 (å€¼ä¸º6)äºŒåˆ†æ€è·¯:- å¦‚æœ mid åœ¨ä¸Šå¡ï¼Œå³°å€¼åœ¨å³è¾¹- å¦‚æœ mid åœ¨ä¸‹å¡ï¼Œå³°å€¼åœ¨å·¦è¾¹```python 5 \\4 / / \\1 1mid å¤„äºä¸Šå¡ â†’ å¾€å³æ‰¾mid å¤„äºä¸‹å¡ â†’ å¾€å·¦æ‰¾ï¼ˆåŒ…å«midï¼‰ ğŸ’» ä»£ç å®ç°def findPeakElement(nums: list) - int: left, right = 0, len(nums) - 1 while left right: mid = left + (right - left) // 2 if nums[mid] nums[mid + 1]: # ä¸Šå¡ï¼Œå³°å€¼åœ¨å³è¾¹ left = mid + 1 else: # ä¸‹å¡æˆ–å³°å€¼ï¼Œå³°å€¼åœ¨å·¦è¾¹ï¼ˆåŒ…å«midï¼‰ right = mid return left ğŸ§  è®°å¿†å£è¯€ â€œä¸Šå¡å¾€å³ï¼Œä¸‹å¡å¾€å·¦â€ 4ï¸âƒ£ LC 33. æœç´¢æ—‹è½¬æ’åºæ•°ç»„ ğŸŸ¡é¢˜ç›®æè¿°åœ¨æ—‹è½¬åçš„æœ‰åºæ•°ç»„ä¸­æœç´¢ç›®æ ‡å€¼ã€‚### ğŸ¨ å›¾è§£æ€è·¯nums = [4, 5, 6, 7, 0, 1, 2], target = 0æ—‹è½¬åçš„æ•°ç»„ç‰¹ç‚¹: 6 \\ 5 0 4 \\äºŒåˆ†ç­–ç•¥:1. åˆ¤æ–­ mid åœ¨å·¦åŠæ®µè¿˜æ˜¯å³åŠæ®µ2. åˆ¤æ–­ target åœ¨ mid çš„å“ªè¾¹### ğŸ’» ä»£ç å®ç°def search(nums: list, target: int) - int: left, right = 0, len(nums) - 1 while left = right: mid = left + (right - left) // 2 if nums[mid] == target: return mid # åˆ¤æ–­ mid åœ¨å·¦åŠæ®µè¿˜æ˜¯å³åŠæ®µ if nums[left] = nums[mid]: # mid åœ¨å·¦åŠæ®µï¼ˆæœ‰åºï¼‰ if nums[left] = target nums[mid]: right = mid - 1 else: left = mid + 1 else: # mid åœ¨å³åŠæ®µï¼ˆæœ‰åºï¼‰ if nums[mid] target = nums[right]: left = mid + 1 else: right = mid - 1 return -1### ğŸ§  è®°å¿†å£è¯€ â€œå…ˆåˆ¤æ–­å“ªè¾¹æœ‰åºï¼Œå†åˆ¤æ–­ç›®æ ‡åœ¨å“ªè¾¹â€5ï¸âƒ£ LC 34. æŸ¥æ‰¾å…ƒç´ çš„ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªä½ç½® ğŸŸ¡é¢˜ç›®æè¿°åœ¨æ’åºæ•°ç»„ä¸­æ‰¾åˆ°ç›®æ ‡å€¼çš„èµ·å§‹å’Œç»“æŸä½ç½®ã€‚### ğŸ¨ å›¾è§£æ€è·¯nums = [5, 7, 7, 8, 8, 10], target = 8æ‰¾ç¬¬ä¸€ä¸ª 8: ç´¢å¼• 3æ‰¾æœ€åä¸€ä¸ª 8: ç´¢å¼• 4 5 7 7 8 8 10 â†‘ â†‘ first last### ğŸ’» ä»£ç å®ç°def searchRange(nums: list, target: int) - list: def find_first(): left, right = 0, len(nums) while left right: mid = left + (right - left) // 2 if nums[mid] = target: right = mid else: left = mid + 1 return left def find_last(): left, right = 0, len(nums) while left right: mid = left + (right - left) // 2 if nums[mid] target: right = mid else: left = mid + 1 return left - 1 first = find_first() if first == len(nums) or nums[first] != target: return [-1, -1] last = find_last() return [first, last]### ğŸ§  è®°å¿†å£è¯€ â€œæ‰¾ç¬¬ä¸€ä¸ª=ï¼Œæ‰¾ç¬¬ä¸€ä¸ªå†å‡1â€6ï¸âƒ£ LC 153. å¯»æ‰¾æ—‹è½¬æ’åºæ•°ç»„ä¸­çš„æœ€å°å€¼ ğŸŸ¡é¢˜ç›®æè¿°åœ¨æ—‹è½¬åçš„æœ‰åºæ•°ç»„ä¸­æ‰¾åˆ°æœ€å°å€¼ã€‚### ğŸ¨ å›¾è§£æ€è·¯nums = [3, 4, 5, 1, 2] 4 \\ 3 1 \\æœ€å°å€¼æ˜¯æ—‹è½¬ç‚¹æ¯”è¾ƒ nums[mid] å’Œ nums[right]:- nums[mid] nums[right]: æœ€å°å€¼åœ¨å³è¾¹- nums[mid] = nums[right]: æœ€å°å€¼åœ¨å·¦è¾¹ï¼ˆåŒ…å«midï¼‰### ğŸ’» ä»£ç å®ç°def findMin(nums: list) - int: left, right = 0, len(nums) - 1 while left right: mid = left + (right - left) // 2 if nums[mid] nums[right]: # æœ€å°å€¼åœ¨å³è¾¹ left = mid + 1 else: # æœ€å°å€¼åœ¨å·¦è¾¹ï¼ˆåŒ…å«midï¼‰ right = mid return nums[left]### ğŸ§  è®°å¿†å£è¯€ â€œæ¯”å³è¾¹å¤§å°±å¾€å³ï¼Œå¦åˆ™å¾€å·¦â€7ï¸âƒ£ LC 4. å¯»æ‰¾ä¸¤ä¸ªæ­£åºæ•°ç»„çš„ä¸­ä½æ•° ğŸ”´é¢˜ç›®æè¿°æ‰¾åˆ°ä¸¤ä¸ªæ­£åºæ•°ç»„çš„ä¸­ä½æ•°ï¼Œè¦æ±‚æ—¶é—´å¤æ‚åº¦ O(log(m+n))ã€‚### ğŸ¨ å›¾è§£æ€è·¯nums1 = [1, 3], nums2 = [2]åˆå¹¶å: [1, 2, 3]ä¸­ä½æ•°: 2äºŒåˆ†æ€è·¯:åœ¨è¾ƒçŸ­çš„æ•°ç»„ä¸ŠäºŒåˆ†ï¼Œæ‰¾åˆ°ä¸€ä¸ªåˆ’åˆ†ç‚¹ iä½¿å¾— nums1[0:i] å’Œ nums2[0:j] çš„æ€»æ•° = (m+n+1)//2 nums1: 1 | 3 nums2: 2 | â†‘ åˆ’åˆ†ç‚¹å·¦åŠè¾¹æœ€å¤§å€¼ = å³åŠè¾¹æœ€å°å€¼### ğŸ’» ä»£ç å®ç°def findMedianSortedArrays(nums1: list, nums2: list) - float: # ç¡®ä¿ nums1 æ˜¯è¾ƒçŸ­çš„æ•°ç»„ if len(nums1) len(nums2): nums1, nums2 = nums2, nums1 m, n = len(nums1), len(nums2) left, right = 0, m while left = right: i = (left + right) // 2 j = (m + n + 1) // 2 - i # è¾¹ç•Œå¤„ç† nums1_left = float('-inf') if i == 0 else nums1[i - 1] nums1_right = float('inf') if i == m else nums1[i] nums2_left = float('-inf') if j == 0 else nums2[j - 1] nums2_right = float('inf') if j == n else nums2[j] if nums1_left = nums2_right and nums2_left = nums1_right: # æ‰¾åˆ°æ­£ç¡®çš„åˆ’åˆ† if (m + n) % 2 == 1: return max(nums1_left, nums2_left) else: return (max(nums1_left, nums2_left) + min(nums1_right, nums2_right)) / 2 elif nums1_left nums2_right: # nums1 åˆ’åˆ†ç‚¹å¤ªé å³ right = i - 1 else: # nums1 åˆ’åˆ†ç‚¹å¤ªé å·¦ left = i + 1 return 0.0### ğŸ§  è®°å¿†å£è¯€ â€œçŸ­æ•°ç»„äºŒåˆ†ï¼Œæ‰¾æ­£ç¡®åˆ’åˆ†â€ğŸ“Š æœ¬ç« æ€»ç»“äºŒåˆ†æŸ¥æ‰¾åœºæ™¯| åœºæ™¯ | å…³é”®ç‚¹ | å…¸å‹é¢˜ç›® | | æœ‰åºæ•°ç»„æŸ¥æ‰¾ | ç›´æ¥äºŒåˆ† | 35, 74 || æ—‹è½¬æ•°ç»„ | åˆ¤æ–­æœ‰åºæ®µ | 33, 153 || å³°å€¼é—®é¢˜ | æ¯”è¾ƒç›¸é‚»å…ƒç´  | 162 || è¾¹ç•Œé—®é¢˜ | =å’Œçš„åŒºåˆ« | 34 || åŒæ•°ç»„ | åœ¨çŸ­æ•°ç»„äºŒåˆ† | 4 |### ğŸ§  å…¨ç« è®°å¿†å£è¯€ æ’çŸ©å³°æ—‹èŒƒå›´æœ€ä¸­äºŒåˆ†ä¸ƒé¢˜è¦è®°æ¸… æ’ - æœç´¢æ’å…¥ä½ç½® (35)çŸ© - æœç´¢äºŒç»´çŸ©é˜µ (74)å³° - å¯»æ‰¾å³°å€¼ (162)æ—‹ - æœç´¢æ—‹è½¬æ’åºæ•°ç»„ (33)èŒƒå›´ - æŸ¥æ‰¾å…ƒç´ çš„ç¬¬ä¸€ä¸ªå’Œæœ€åä¸€ä¸ªä½ç½® (34)æœ€ - å¯»æ‰¾æ—‹è½¬æ’åºæ•°ç»„ä¸­çš„æœ€å°å€¼ (153)ä¸­ - å¯»æ‰¾ä¸¤ä¸ªæ­£åºæ•°ç»„çš„ä¸­ä½æ•° (4) --- ğŸ“– **ä¸‹ä¸€ç¯‡**ï¼š[ä½è¿ç®—ä¸“é¢˜](/2026/01/18/leetcode-150-bit/)","tags":["LeetCode"],"categories":["ç®—æ³•"]},{"title":"ğŸ”¢ LeetCode 150 - å›æº¯ç®—æ³•ä¸“é¢˜","path":"/2026/01/18/leetcode-150-backtrack/","content":"ğŸ”„ å›æº¯ç®—æ³•ä¸“é¢˜ (7é¢˜) ğŸ¯ æ ¸å¿ƒæ€æƒ³ï¼šå°è¯•æ‰€æœ‰å¯èƒ½ï¼Œèµ°ä¸é€šå°±å›å¤´ ğŸ—ºï¸ å›æº¯ç®—æ³•çš„æœ¬è´¨â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ å›æº¯ = å†³ç­–æ ‘çš„éå† â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ â”‚â”‚ [] â”‚â”‚ / | \\ â”‚â”‚ [1] [2] [3] â† ç¬¬ä¸€å±‚å†³ç­– â”‚â”‚ / \\ | â”‚â”‚ [1,2][1,3][2,3] â† ç¬¬äºŒå±‚å†³ç­– â”‚â”‚ | â”‚â”‚ [1,2,3] â† ç¬¬ä¸‰å±‚å†³ç­– â”‚â”‚ â”‚â”‚ å›æº¯ä¸‰è¦ç´ ï¼š â”‚â”‚ 1. è·¯å¾„ï¼šå·²ç»åšå‡ºçš„é€‰æ‹© â”‚â”‚ 2. é€‰æ‹©åˆ—è¡¨ï¼šå½“å‰å¯ä»¥åšçš„é€‰æ‹© â”‚â”‚ 3. ç»“æŸæ¡ä»¶ï¼šåˆ°è¾¾å†³ç­–æ ‘åº•å±‚ â”‚â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ğŸ”§ å›æº¯ç®—æ³•æ¨¡æ¿def backtrack(path, choices): # ç»“æŸæ¡ä»¶ if æ»¡è¶³ç»“æŸæ¡ä»¶: result.append(path[:]) # æ³¨æ„æ‹·è´ï¼ return for choice in choices: # 1. åšé€‰æ‹© path.append(choice) # 2. é€’å½’è¿›å…¥ä¸‹ä¸€å±‚å†³ç­– backtrack(path, new_choices) # 3. æ’¤é”€é€‰æ‹©ï¼ˆå›æº¯ï¼‰ path.pop() ğŸ§  å›æº¯å£è¯€ â€œé€‰æ‹©ã€é€’å½’ã€æ’¤é”€â€ â€”â€” å›æº¯ä¸‰éƒ¨æ›² 1ï¸âƒ£ LC 17. ç”µè¯å·ç çš„å­—æ¯ç»„åˆ ğŸŸ¡é¢˜ç›®æè¿°ç»™å®šç”µè¯å·ç ï¼Œè¿”å›æ‰€æœ‰å¯èƒ½çš„å­—æ¯ç»„åˆã€‚ ğŸ¨ å›¾è§£æ€è·¯digits = \"23\"2 â†’ \"abc\"3 â†’ \"def\"å†³ç­–æ ‘: \"\" / | \\ a b c â† é€‰æ‹©2å¯¹åº”çš„å­—æ¯ /|\\ /|\\ /|\\ d e f d e f d e f â† é€‰æ‹©3å¯¹åº”çš„å­—æ¯ç»“æœ: [\"ad\",\"ae\",\"af\",\"bd\",\"be\",\"bf\",\"cd\",\"ce\",\"cf\"] ğŸ’» ä»£ç å®ç°def letterCombinations(digits: str) - list: if not digits: return [] phone = { '2': 'abc', '3': 'def', '4': 'ghi', '5': 'jkl', '6': 'mno', '7': 'pqrs', '8': 'tuv', '9': 'wxyz' } result = [] def backtrack(index, path): if index == len(digits): result.append(''.join(path)) return for char in phone[digits[index]]: path.append(char) backtrack(index + 1, path) path.pop() backtrack(0, []) return result ğŸ§  è®°å¿†å£è¯€ â€œæ¯ä¸ªæ•°å­—é€‰ä¸€ä¸ªå­—æ¯â€ 2ï¸âƒ£ LC 77. ç»„åˆ ğŸŸ¡é¢˜ç›®æè¿°ä» 1 åˆ° n ä¸­é€‰æ‹© k ä¸ªæ•°çš„æ‰€æœ‰ç»„åˆã€‚ ğŸ¨ å›¾è§£æ€è·¯n = 4, k = 2å†³ç­–æ ‘ï¼ˆæ¯æ¬¡åªèƒ½é€‰æ¯”è‡ªå·±å¤§çš„æ•°ï¼Œé¿å…é‡å¤ï¼‰: [] / | | \\ [1] [2] [3] [4] / | \\ | [1,2][1,3][1,4] [2,3][2,4] [3,4]ç»“æœ: [[1,2],[1,3],[1,4],[2,3],[2,4],[3,4]] ğŸ’» ä»£ç å®ç°def combine(n: int, k: int) - list: result = [] def backtrack(start, path): if len(path) == k: result.append(path[:]) return # å‰ªæï¼šå‰©ä½™å…ƒç´ ä¸å¤Ÿç”¨äº† if k - len(path) n - start + 1: return for i in range(start, n + 1): path.append(i) backtrack(i + 1, path) path.pop() backtrack(1, []) return result ğŸ§  è®°å¿†å£è¯€ â€œä»startå¼€å§‹é€‰ï¼Œé€‰å¤Ÿkä¸ªåœâ€ 3ï¸âƒ£ LC 46. å…¨æ’åˆ— ğŸŸ¡é¢˜ç›®æè¿°è¿”å›æ•°ç»„çš„æ‰€æœ‰æ’åˆ—ã€‚ ğŸ¨ å›¾è§£æ€è·¯nums = [1, 2, 3]å†³ç­–æ ‘ï¼ˆæ¯ä¸ªæ•°åªèƒ½ç”¨ä¸€æ¬¡ï¼‰: [] / | \\ [1] [2] [3] / \\ / \\ / \\ [1,2] [1,3] [2,1] [2,3] [3,1] [3,2] | | | | | | [1,2,3][1,3,2][2,1,3][2,3,1][3,1,2][3,2,1] ğŸ’» ä»£ç å®ç°def permute(nums: list) - list: result = [] def backtrack(path, used): if len(path) == len(nums): result.append(path[:]) return for i in range(len(nums)): if used[i]: continue path.append(nums[i]) used[i] = True backtrack(path, used) path.pop() used[i] = False backtrack([], [False] * len(nums)) return result ğŸ§  è®°å¿†å£è¯€ â€œç”¨è¿‡çš„æ ‡è®°ï¼Œæ²¡ç”¨è¿‡çš„éƒ½èƒ½é€‰â€ 4ï¸âƒ£ LC 39. ç»„åˆæ€»å’Œ ğŸŸ¡é¢˜ç›®æè¿°æ‰¾å‡ºæ‰€æœ‰å’Œä¸º target çš„ç»„åˆï¼ˆæ•°å­—å¯ä»¥é‡å¤ä½¿ç”¨ï¼‰ã€‚ ğŸ¨ å›¾è§£æ€è·¯candidates = [2, 3, 6, 7], target = 7å†³ç­–æ ‘: [] / | \\ \\ [2] [3] [6] [7] âœ“ / | \\ | | [2,2][2,3][2,6] [3,3] [6,?] /|\\ |[2,2,2][2,2,3]âœ“ [2,3,?] |[2,2,2,?] è¶…è¿‡7ï¼Œå‰ªæ ğŸ’» ä»£ç å®ç°def combinationSum(candidates: list, target: int) - list: result = [] candidates.sort() # æ’åºä¾¿äºå‰ªæ def backtrack(start, path, remaining): if remaining == 0: result.append(path[:]) return for i in range(start, len(candidates)): if candidates[i] remaining: break # å‰ªæ path.append(candidates[i]) backtrack(i, path, remaining - candidates[i]) # iä¸æ˜¯i+1ï¼Œå¯é‡å¤ path.pop() backtrack(0, [], target) return result ğŸ§  è®°å¿†å£è¯€ â€œå¯ä»¥é‡å¤é€‰ï¼Œä½†åªèƒ½å¾€åé€‰â€ 5ï¸âƒ£ LC 52. N çš‡å II ğŸ”´é¢˜ç›®æè¿°è¿”å› N çš‡åé—®é¢˜çš„è§£çš„æ•°é‡ã€‚ ğŸ¨ å›¾è§£æ€è·¯n = 4ä¸€ä¸ªæœ‰æ•ˆè§£:. Q . .. . . QQ . . .. . Q .çº¦æŸæ¡ä»¶ï¼š1. æ¯è¡Œåªèƒ½æ”¾ä¸€ä¸ªçš‡å2. æ¯åˆ—åªèƒ½æ”¾ä¸€ä¸ªçš‡å3. æ¯æ¡å¯¹è§’çº¿åªèƒ½æ”¾ä¸€ä¸ªçš‡åå¯¹è§’çº¿ç¼–å·æŠ€å·§:- ä¸»å¯¹è§’çº¿ (\\): row - col ç›¸åŒ- å‰¯å¯¹è§’çº¿ (/): row + col ç›¸åŒ ğŸ’» ä»£ç å®ç°def totalNQueens(n: int) - int: count = 0 cols = set() # åˆ—å†²çª diag1 = set() # ä¸»å¯¹è§’çº¿ (row - col) diag2 = set() # å‰¯å¯¹è§’çº¿ (row + col) def backtrack(row): nonlocal count if row == n: count += 1 return for col in range(n): if col in cols or (row - col) in diag1 or (row + col) in diag2: continue cols.add(col) diag1.add(row - col) diag2.add(row + col) backtrack(row + 1) cols.remove(col) diag1.remove(row - col) diag2.remove(row + col) backtrack(0) return count ğŸ§  è®°å¿†å£è¯€ â€œåˆ—å’Œä¸¤æ¡å¯¹è§’çº¿ï¼Œéƒ½ä¸èƒ½å†²çªâ€ 6ï¸âƒ£ LC 22. æ‹¬å·ç”Ÿæˆ ğŸŸ¡é¢˜ç›®æè¿°ç”Ÿæˆ n å¯¹æœ‰æ•ˆçš„æ‹¬å·ç»„åˆã€‚ ğŸ¨ å›¾è§£æ€è·¯n = 2å†³ç­–æ ‘ï¼ˆå‰ªææ¡ä»¶ï¼šå³æ‹¬å·ä¸èƒ½å¤šäºå·¦æ‹¬å·ï¼‰: \"\" / ( / \\ (( () | / (() ()( | | (()) ()()ç»“æœ: [\"(())\", \"()()\"] ğŸ’» ä»£ç å®ç°def generateParenthesis(n: int) - list: result = [] def backtrack(path, left, right): if len(path) == 2 * n: result.append(''.join(path)) return if left n: path.append('(') backtrack(path, left + 1, right) path.pop() if right left: path.append(')') backtrack(path, left, right + 1) path.pop() backtrack([], 0, 0) return result ğŸ§  è®°å¿†å£è¯€ â€œå·¦æ‹¬å·éšæ—¶åŠ ï¼Œå³æ‹¬å·ä¸è¶…å·¦â€ 7ï¸âƒ£ LC 79. å•è¯æœç´¢ ğŸŸ¡é¢˜ç›®æè¿°åœ¨äºŒç»´ç½‘æ ¼ä¸­æœç´¢å•è¯ã€‚ ğŸ¨ å›¾è§£æ€è·¯board:A B C ES F C SA D E Eword = \"ABCCED\"ä» A å¼€å§‹ï¼ŒDFS + å›æº¯:A â†’ B â†’ C â†’ C â†’ E â†’ D âœ“ ğŸ’» ä»£ç å®ç°def exist(board: list, word: str) - bool: m, n = len(board), len(board[0]) def backtrack(i, j, k): if k == len(word): return True if i 0 or i = m or j 0 or j = n: return False if board[i][j] != word[k]: return False # æ ‡è®°å·²è®¿é—® temp = board[i][j] board[i][j] = '#' # å››ä¸ªæ–¹å‘æœç´¢ found = (backtrack(i + 1, j, k + 1) or backtrack(i - 1, j, k + 1) or backtrack(i, j + 1, k + 1) or backtrack(i, j - 1, k + 1)) # æ¢å¤ board[i][j] = temp return found for i in range(m): for j in range(n): if backtrack(i, j, 0): return True return False ğŸ§  è®°å¿†å£è¯€ â€œDFSå››æ–¹å‘ï¼Œè®¿é—®è¦æ ‡è®°â€ ğŸ“Š æœ¬ç« æ€»ç»“å›æº¯é—®é¢˜åˆ†ç±» ç±»å‹ ç‰¹ç‚¹ å…¸å‹é¢˜ç›® ç»„åˆé—®é¢˜ ä¸è€ƒè™‘é¡ºåº 77, 39 æ’åˆ—é—®é¢˜ è€ƒè™‘é¡ºåº 46 å­é›†é—®é¢˜ æ‰€æœ‰å¯èƒ½ 78 æœç´¢é—®é¢˜ åœ¨ç©ºé—´ä¸­æ‰¾è·¯å¾„ 79 æ£‹ç›˜é—®é¢˜ æ”¾ç½®çº¦æŸ 52 æ‹¬å·é—®é¢˜ åˆæ³•æ€§çº¦æŸ 22 å›æº¯ vs åŠ¨æ€è§„åˆ’â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ å›æº¯ â”‚ åŠ¨æ€è§„åˆ’ â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ æ‰¾æ‰€æœ‰è§£ â”‚ æ‰¾æœ€ä¼˜è§£/è®¡æ•° â”‚â”‚ æš´åŠ›ç©·ä¸¾ â”‚ è®°å¿†åŒ–é¿å…é‡å¤ â”‚â”‚ æ—¶é—´æ¢ç©ºé—´ â”‚ ç©ºé—´æ¢æ—¶é—´ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ğŸ§  å…¨ç« è®°å¿†å£è¯€ç”µè¯ç»„åˆå…¨æ’åˆ—ç»„åˆæ€»å’Œçš‡åè§£æ‹¬å·ç”Ÿæˆå•è¯æ‰¾å›æº¯ä¸ƒé¢˜å…¨æ‹¿ä¸‹ç”µè¯ - ç”µè¯å·ç çš„å­—æ¯ç»„åˆ (17)ç»„åˆ - ç»„åˆ (77)å…¨æ’åˆ— - å…¨æ’åˆ— (46)ç»„åˆæ€»å’Œ - ç»„åˆæ€»å’Œ (39)çš‡å - Nçš‡å II (52)æ‹¬å· - æ‹¬å·ç”Ÿæˆ (22)å•è¯ - å•è¯æœç´¢ (79) ğŸ“– è¿”å›ï¼šLeetCode 150 é¢˜æ€»ç›®å½•","tags":["LeetCode"],"categories":["ç®—æ³•"]},{"title":"ğŸ“ˆ LeetCode 150 - åŠ¨æ€è§„åˆ’ä¸“é¢˜","path":"/2026/01/18/leetcode-150-dp/","content":"ğŸ“ˆ åŠ¨æ€è§„åˆ’ä¸“é¢˜ (11é¢˜) ğŸ¯ æ ¸å¿ƒæ€æƒ³ï¼šå°†å¤§é—®é¢˜åˆ†è§£ä¸ºå°é—®é¢˜ï¼Œè®°å½•å­é—®é¢˜çš„è§£ï¼Œé¿å…é‡å¤è®¡ç®— ğŸ—ºï¸ åŠ¨æ€è§„åˆ’è§£é¢˜æ¡†æ¶â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ DP è§£é¢˜å››æ­¥æ³• â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ â”‚â”‚ Step 1: å®šä¹‰çŠ¶æ€ â”‚â”‚ dp[i] è¡¨ç¤ºä»€ä¹ˆï¼Ÿdp[i][j] è¡¨ç¤ºä»€ä¹ˆï¼Ÿ â”‚â”‚ â”‚â”‚ Step 2: çŠ¶æ€è½¬ç§»æ–¹ç¨‹ â”‚â”‚ dp[i] = f(dp[i-1], dp[i-2], ...) â”‚â”‚ â”‚â”‚ Step 3: åˆå§‹åŒ– â”‚â”‚ è¾¹ç•Œæ¡ä»¶æ˜¯ä»€ä¹ˆï¼Ÿdp[0], dp[1] ç­‰äºå¤šå°‘ï¼Ÿ â”‚â”‚ â”‚â”‚ Step 4: éå†é¡ºåº â”‚â”‚ æ­£åºè¿˜æ˜¯å€’åºï¼Ÿå…ˆè¡Œååˆ—è¿˜æ˜¯å…ˆåˆ—åè¡Œï¼Ÿ â”‚â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ğŸ”§ DP ä»£ç æ¨¡æ¿def dp_template(nums): n = len(nums) # Step 1: å®šä¹‰ dp æ•°ç»„ dp = [0] * n # æˆ– [[0]*m for _ in range(n)] # Step 3: åˆå§‹åŒ– dp[0] = base_case # Step 4: éå†é¡ºåº for i in range(1, n): # Step 2: çŠ¶æ€è½¬ç§» dp[i] = transition(dp[i-1], ...) return dp[n-1] # æˆ–å…¶ä»–ç›®æ ‡ 1ï¸âƒ£ LC 70. çˆ¬æ¥¼æ¢¯ ğŸŸ¢é¢˜ç›®æè¿°æ¯æ¬¡å¯ä»¥çˆ¬ 1 æˆ– 2 ä¸ªå°é˜¶ï¼Œçˆ¬åˆ°ç¬¬ n é˜¶æœ‰å¤šå°‘ç§æ–¹æ³•ï¼Ÿ ğŸ¨ å›¾è§£æ€è·¯åˆ°è¾¾ç¬¬ n é˜¶çš„æ–¹æ³• = ä»ç¬¬ n-1 é˜¶çˆ¬ 1 æ­¥ + ä»ç¬¬ n-2 é˜¶çˆ¬ 2 æ­¥ â”Œâ”€â”€â”€â” â”‚ n â”‚ â† ç›®æ ‡ â””â”€â”€â”€â”˜ â†— â†– â”Œâ”€â”€â”€â” â”Œâ”€â”€â”€â” â”‚n-1â”‚ â”‚n-2â”‚ â””â”€â”€â”€â”˜ â””â”€â”€â”€â”˜ â†‘ â†‘ 1æ­¥ 2æ­¥dp[n] = dp[n-1] + dp[n-2] (æ–æ³¢é‚£å¥‘æ•°åˆ—!) ğŸ’» ä»£ç å®ç°def climbStairs(n: int) - int: if n = 2: return n # ç©ºé—´ä¼˜åŒ–ï¼šåªéœ€è®°å½•å‰ä¸¤ä¸ªçŠ¶æ€ prev, curr = 1, 2 for _ in range(3, n + 1): prev, curr = curr, prev + curr return curr ğŸ§  è®°å¿†å£è¯€ â€œçˆ¬æ¥¼æ¢¯å°±æ˜¯æ–æ³¢é‚£å¥‘â€ 2ï¸âƒ£ LC 198. æ‰“å®¶åŠ«èˆ ğŸŸ¡é¢˜ç›®æè¿°ä¸èƒ½å·ç›¸é‚»çš„æˆ¿å­ï¼Œæ±‚èƒ½å·åˆ°çš„æœ€å¤§é‡‘é¢ã€‚ ğŸ¨ å›¾è§£æ€è·¯nums = [2, 7, 9, 3, 1]å¯¹äºæ¯ä¸ªæˆ¿å­ï¼Œä¸¤ä¸ªé€‰æ‹©ï¼š1. å·ï¼šdp[i] = dp[i-2] + nums[i]2. ä¸å·ï¼šdp[i] = dp[i-1]dp[i] = max(dp[i-2] + nums[i], dp[i-1])i: 0 1 2 3 4nums: 2 7 9 3 1dp: 2 7 11 11 12 â†‘ â†‘ â†‘ â†‘ â†‘ å· å· å·0+9 ä¸å· å·2+1 ğŸ’» ä»£ç å®ç°def rob(nums: list) - int: if len(nums) == 1: return nums[0] # ç©ºé—´ä¼˜åŒ– prev, curr = nums[0], max(nums[0], nums[1]) for i in range(2, len(nums)): prev, curr = curr, max(curr, prev + nums[i]) return curr ğŸ§  è®°å¿†å£è¯€ â€œå·ä¸å·ï¼Œå–æœ€å¤§â€ 3ï¸âƒ£ LC 139. å•è¯æ‹†åˆ† ğŸŸ¡é¢˜ç›®æè¿°åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦å¯ä»¥è¢«æ‹†åˆ†ä¸ºå­—å…¸ä¸­çš„å•è¯ã€‚ ğŸ¨ å›¾è§£æ€è·¯s = \"leetcode\", wordDict = [\"leet\", \"code\"]dp[i] è¡¨ç¤º s[0:i] æ˜¯å¦å¯ä»¥è¢«æ‹†åˆ†dp[0] = True (ç©ºå­—ç¬¦ä¸²)dp[4] = dp[0] and \"leet\" in dict â†’ Truedp[8] = dp[4] and \"code\" in dict â†’ True l e e t c o d e 0 1 2 3 4 5 6 7 8dp T F F F T F F F T â†‘ â†‘ \"leet\" \"code\" ğŸ’» ä»£ç å®ç°def wordBreak(s: str, wordDict: list) - bool: word_set = set(wordDict) n = len(s) dp = [False] * (n + 1) dp[0] = True for i in range(1, n + 1): for j in range(i): if dp[j] and s[j:i] in word_set: dp[i] = True break return dp[n] ğŸ§  è®°å¿†å£è¯€ â€œå‰é¢èƒ½æ‹†ï¼Œåé¢åœ¨å­—å…¸ï¼Œå°±èƒ½æ‹†â€ 4ï¸âƒ£ LC 322. é›¶é’±å…‘æ¢ ğŸŸ¡é¢˜ç›®æè¿°ç”¨æœ€å°‘çš„ç¡¬å¸å‡‘å‡ºç›®æ ‡é‡‘é¢ã€‚ ğŸ¨ å›¾è§£æ€è·¯coins = [1, 2, 5], amount = 11dp[i] = å‡‘å‡ºé‡‘é¢ i éœ€è¦çš„æœ€å°‘ç¡¬å¸æ•°å¯¹äºæ¯ä¸ªé‡‘é¢ iï¼Œå°è¯•æ¯ä¸ªç¡¬å¸ cï¼šdp[i] = min(dp[i], dp[i-c] + 1)amount: 0 1 2 3 4 5 6 7 8 9 10 11dp: 0 1 1 2 2 1 2 2 3 3 2 3 â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ +1 +2 +1 +2 +5 +1 +2 +1 +2 +5 +5 ğŸ’» ä»£ç å®ç°def coinChange(coins: list, amount: int) - int: dp = [float('inf')] * (amount + 1) dp[0] = 0 for i in range(1, amount + 1): for coin in coins: if coin = i and dp[i - coin] != float('inf'): dp[i] = min(dp[i], dp[i - coin] + 1) return dp[amount] if dp[amount] != float('inf') else -1 ğŸ§  è®°å¿†å£è¯€ â€œæ¯ä¸ªç¡¬å¸è¯•ä¸€è¯•ï¼Œå–æœ€å°â€ 5ï¸âƒ£ LC 300. æœ€é•¿é€’å¢å­åºåˆ— ğŸŸ¡é¢˜ç›®æè¿°æ‰¾å‡ºæ•°ç»„ä¸­æœ€é•¿çš„ä¸¥æ ¼é€’å¢å­åºåˆ—çš„é•¿åº¦ã€‚ ğŸ¨ å›¾è§£æ€è·¯nums = [10, 9, 2, 5, 3, 7, 101, 18]dp[i] = ä»¥ nums[i] ç»“å°¾çš„ LIS é•¿åº¦å¯¹äº nums[i]ï¼Œæ‰¾æ‰€æœ‰ j i ä¸” nums[j] nums[i]ï¼šdp[i] = max(dp[j] + 1)i: 0 1 2 3 4 5 6 7nums: 10 9 2 5 3 7 101 18dp: 1 1 1 2 2 3 4 4 â†‘ â†‘ â†‘ â†‘ â†‘ 2+1 2+1 5+1 7+1 7+1LIS = 4 (å¦‚ [2, 3, 7, 101]) ğŸ’» ä»£ç å®ç°# O(nÂ²) è§£æ³•def lengthOfLIS(nums: list) - int: n = len(nums) dp = [1] * n for i in range(1, n): for j in range(i): if nums[j] nums[i]: dp[i] = max(dp[i], dp[j] + 1) return max(dp) ğŸ”¥ O(n log n) äºŒåˆ†è§£æ³•def lengthOfLIS(nums: list) - int: # tails[i] = é•¿åº¦ä¸º i+1 çš„ LIS çš„æœ€å°ç»“å°¾å…ƒç´  tails = [] for num in nums: # äºŒåˆ†æŸ¥æ‰¾ç¬¬ä¸€ä¸ª = num çš„ä½ç½® left, right = 0, len(tails) while left right: mid = (left + right) // 2 if tails[mid] num: left = mid + 1 else: right = mid if left == len(tails): tails.append(num) else: tails[left] = num return len(tails) ğŸ§  è®°å¿†å£è¯€ â€œå‰é¢æ¯”æˆ‘å°çš„ï¼ŒåŠ 1å–æœ€å¤§â€ 6ï¸âƒ£ LC 120. ä¸‰è§’å½¢æœ€å°è·¯å¾„å’Œ ğŸŸ¡é¢˜ç›®æè¿°ä»é¡¶éƒ¨åˆ°åº•éƒ¨çš„æœ€å°è·¯å¾„å’Œã€‚ ğŸ¨ å›¾è§£æ€è·¯ [2] [3,4] [6,5,7] [4,1,8,3]è‡ªåº•å‘ä¸Šè®¡ç®—ï¼šdp[i][j] = min(dp[i+1][j], dp[i+1][j+1]) + triangle[i][j]ç¬¬3å±‚: [4, 1, 8, 3]ç¬¬2å±‚: [6+1, 5+1, 7+3] = [7, 6, 10]ç¬¬1å±‚: [3+6, 4+6] = [9, 10]ç¬¬0å±‚: [2+9] = [11]æœ€å°è·¯å¾„å’Œ = 11 ğŸ’» ä»£ç å®ç°def minimumTotal(triangle: list) - int: n = len(triangle) # ä»æœ€åä¸€è¡Œå¼€å§‹ dp = triangle[-1][:] # è‡ªåº•å‘ä¸Š for i in range(n - 2, -1, -1): for j in range(i + 1): dp[j] = min(dp[j], dp[j + 1]) + triangle[i][j] return dp[0] ğŸ§  è®°å¿†å£è¯€ â€œè‡ªåº•å‘ä¸Šï¼Œå–å°åŠ è‡ªå·±â€ 7ï¸âƒ£ LC 64. æœ€å°è·¯å¾„å’Œ ğŸŸ¡é¢˜ç›®æè¿°ä»å·¦ä¸Šè§’åˆ°å³ä¸‹è§’çš„æœ€å°è·¯å¾„å’Œã€‚ ğŸ¨ å›¾è§£æ€è·¯grid:[1, 3, 1][1, 5, 1][4, 2, 1]dp[i][j] = min(dp[i-1][j], dp[i][j-1]) + grid[i][j]dp:[1, 4, 5][2, 7, 6][6, 8, 7]æœ€å°è·¯å¾„å’Œ = 7 (1â†’3â†’1â†’1â†’1) ğŸ’» ä»£ç å®ç°def minPathSum(grid: list) - int: m, n = len(grid), len(grid[0]) dp = [[0] * n for _ in range(m)] for i in range(m): for j in range(n): if i == 0 and j == 0: dp[i][j] = grid[i][j] elif i == 0: dp[i][j] = dp[i][j-1] + grid[i][j] elif j == 0: dp[i][j] = dp[i-1][j] + grid[i][j] else: dp[i][j] = min(dp[i-1][j], dp[i][j-1]) + grid[i][j] return dp[m-1][n-1] ğŸ§  è®°å¿†å£è¯€ â€œä¸Šå·¦å–å°ï¼ŒåŠ è‡ªå·±â€ 8ï¸âƒ£ LC 63. ä¸åŒè·¯å¾„ II ğŸŸ¡é¢˜ç›®æè¿°æœ‰éšœç¢ç‰©çš„ç½‘æ ¼ï¼Œä»å·¦ä¸Šåˆ°å³ä¸‹çš„è·¯å¾„æ•°ã€‚ ğŸ¨ å›¾è§£æ€è·¯grid: dp:[0, 0, 0] [1, 1, 1][0, 1, 0] â†’ [1, 0, 1][0, 0, 0] [1, 1, 2]éšœç¢ç‰©ä½ç½® dp = 0å…¶ä»–ä½ç½® dp = dp[ä¸Š] + dp[å·¦] ğŸ’» ä»£ç å®ç°def uniquePathsWithObstacles(obstacleGrid: list) - int: m, n = len(obstacleGrid), len(obstacleGrid[0]) if obstacleGrid[0][0] == 1: return 0 dp = [[0] * n for _ in range(m)] dp[0][0] = 1 # åˆå§‹åŒ–ç¬¬ä¸€åˆ— for i in range(1, m): if obstacleGrid[i][0] == 0: dp[i][0] = dp[i-1][0] # åˆå§‹åŒ–ç¬¬ä¸€è¡Œ for j in range(1, n): if obstacleGrid[0][j] == 0: dp[0][j] = dp[0][j-1] # å¡«å…… dp for i in range(1, m): for j in range(1, n): if obstacleGrid[i][j] == 0: dp[i][j] = dp[i-1][j] + dp[i][j-1] return dp[m-1][n-1] ğŸ§  è®°å¿†å£è¯€ â€œéšœç¢ä¸º0ï¼Œå¦åˆ™ä¸ŠåŠ å·¦â€ 9ï¸âƒ£ LC 5. æœ€é•¿å›æ–‡å­ä¸² ğŸŸ¡é¢˜ç›®æè¿°æ‰¾å‡ºå­—ç¬¦ä¸²ä¸­æœ€é•¿çš„å›æ–‡å­ä¸²ã€‚ ğŸ¨ å›¾è§£æ€è·¯s = \"babad\"dp[i][j] = s[i:j+1] æ˜¯å¦ä¸ºå›æ–‡æ¡ä»¶: s[i] == s[j] and dp[i+1][j-1]å¡«è¡¨é¡ºåºï¼šæŒ‰é•¿åº¦ä»å°åˆ°å¤§é•¿åº¦1: å…¨ä¸º Trueé•¿åº¦2: s[i] == s[i+1]é•¿åº¦3+: s[i] == s[j] and dp[i+1][j-1] ğŸ’» ä»£ç å®ç°def longestPalindrome(s: str) - str: n = len(s) if n 2: return s dp = [[False] * n for _ in range(n)] start, max_len = 0, 1 # æ‰€æœ‰é•¿åº¦ä¸º 1 çš„å­ä¸²éƒ½æ˜¯å›æ–‡ for i in range(n): dp[i][i] = True # æŒ‰é•¿åº¦å¡«è¡¨ for length in range(2, n + 1): for i in range(n - length + 1): j = i + length - 1 if s[i] == s[j]: if length == 2: dp[i][j] = True else: dp[i][j] = dp[i + 1][j - 1] if dp[i][j] and length max_len: start, max_len = i, length return s[start:start + max_len] ğŸ”¥ ä¸­å¿ƒæ‰©å±•æ³• (æ›´ä¼˜)def longestPalindrome(s: str) - str: def expand(left, right): while left = 0 and right len(s) and s[left] == s[right]: left -= 1 right += 1 return s[left + 1:right] result = \"\" for i in range(len(s)): # å¥‡æ•°é•¿åº¦ odd = expand(i, i) # å¶æ•°é•¿åº¦ even = expand(i, i + 1) result = max(result, odd, even, key=len) return result ğŸ§  è®°å¿†å£è¯€ â€œé¦–å°¾ç›¸åŒï¼Œä¸­é—´ä¹Ÿæ˜¯å›æ–‡â€ ğŸ”Ÿ LC 97. äº¤é”™å­—ç¬¦ä¸² ğŸŸ¡é¢˜ç›®æè¿°åˆ¤æ–­ s3 æ˜¯å¦ç”± s1 å’Œ s2 äº¤é”™ç»„æˆã€‚ ğŸ¨ å›¾è§£æ€è·¯s1 = \"aabcc\", s2 = \"dbbca\", s3 = \"aadbbcbcac\"dp[i][j] = s1[0:i] å’Œ s2[0:j] èƒ½å¦äº¤é”™ç»„æˆ s3[0:i+j]çŠ¶æ€è½¬ç§»:dp[i][j] = (dp[i-1][j] and s1[i-1]==s3[i+j-1]) or (dp[i][j-1] and s2[j-1]==s3[i+j-1]) ğŸ’» ä»£ç å®ç°def isInterleave(s1: str, s2: str, s3: str) - bool: m, n = len(s1), len(s2) if m + n != len(s3): return False dp = [[False] * (n + 1) for _ in range(m + 1)] dp[0][0] = True # åˆå§‹åŒ–ç¬¬ä¸€åˆ— for i in range(1, m + 1): dp[i][0] = dp[i-1][0] and s1[i-1] == s3[i-1] # åˆå§‹åŒ–ç¬¬ä¸€è¡Œ for j in range(1, n + 1): dp[0][j] = dp[0][j-1] and s2[j-1] == s3[j-1] # å¡«å…… dp for i in range(1, m + 1): for j in range(1, n + 1): dp[i][j] = ((dp[i-1][j] and s1[i-1] == s3[i+j-1]) or (dp[i][j-1] and s2[j-1] == s3[i+j-1])) return dp[m][n] ğŸ§  è®°å¿†å£è¯€ â€œä¸Šæˆ–å·¦èƒ½åˆ°ï¼Œä¸”å­—ç¬¦åŒ¹é…â€ 1ï¸âƒ£1ï¸âƒ£ LC 72. ç¼–è¾‘è·ç¦» ğŸŸ¡é¢˜ç›®æè¿°å°† word1 è½¬æ¢æˆ word2 æ‰€ä½¿ç”¨çš„æœ€å°‘æ“ä½œæ•°ã€‚ ğŸ¨ å›¾è§£æ€è·¯word1 = \"horse\", word2 = \"ros\"dp[i][j] = word1[0:i] è½¬æ¢ä¸º word2[0:j] çš„æœ€å°‘æ“ä½œä¸‰ç§æ“ä½œ:1. æ’å…¥: dp[i][j-1] + 12. åˆ é™¤: dp[i-1][j] + 13. æ›¿æ¢: dp[i-1][j-1] + (0 if ç›¸åŒ else 1)dp è¡¨: \"\" r o s \"\" 0 1 2 3 h 1 1 2 3 o 2 2 1 2 r 3 2 2 2 s 4 3 3 2 e 5 4 4 3 ğŸ’» ä»£ç å®ç°def minDistance(word1: str, word2: str) - int: m, n = len(word1), len(word2) dp = [[0] * (n + 1) for _ in range(m + 1)] # åˆå§‹åŒ– for i in range(m + 1): dp[i][0] = i for j in range(n + 1): dp[0][j] = j # å¡«å…… dp for i in range(1, m + 1): for j in range(1, n + 1): if word1[i-1] == word2[j-1]: dp[i][j] = dp[i-1][j-1] else: dp[i][j] = 1 + min( dp[i-1][j], # åˆ é™¤ dp[i][j-1], # æ’å…¥ dp[i-1][j-1] # æ›¿æ¢ ) return dp[m][n] ğŸ§  è®°å¿†å£è¯€ â€œç›¸åŒä¸å˜ï¼Œä¸åŒå–ä¸‰è€…æœ€å°åŠ 1â€ ğŸ“Š æœ¬ç« æ€»ç»“DP é—®é¢˜åˆ†ç±»â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ åŠ¨æ€è§„åˆ’åˆ†ç±» â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ â”‚â”‚ çº¿æ€§ DP â”‚â”‚ â”œâ”€ å•åºåˆ—: çˆ¬æ¥¼æ¢¯, æ‰“å®¶åŠ«èˆ, LIS â”‚â”‚ â””â”€ åŒåºåˆ—: ç¼–è¾‘è·ç¦», äº¤é”™å­—ç¬¦ä¸² â”‚â”‚ â”‚â”‚ åŒºé—´ DP â”‚â”‚ â””â”€ å›æ–‡å­ä¸² â”‚â”‚ â”‚â”‚ èƒŒåŒ… DP â”‚â”‚ â””â”€ é›¶é’±å…‘æ¢, å•è¯æ‹†åˆ† â”‚â”‚ â”‚â”‚ ç½‘æ ¼ DP â”‚â”‚ â””â”€ æœ€å°è·¯å¾„å’Œ, ä¸åŒè·¯å¾„ â”‚â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ğŸ§  å…¨ç« è®°å¿†å£è¯€çˆ¬æ¥¼åŠ«èˆå•è¯æ‹†é›¶é’±é€’å¢ä¸‰è§’æ¥è·¯å¾„ç½‘æ ¼å›æ–‡åˆ¤äº¤é”™ç¼–è¾‘å…¨éƒ½ä¼š ğŸ“– ä¸‹ä¸€ç¯‡ï¼šå›¾è®ºä¸“é¢˜","tags":["LeetCode"],"categories":["ç®—æ³•"]},{"title":"ğŸ”— LeetCode 150 - é“¾è¡¨ä¸“é¢˜","path":"/2026/01/18/leetcode-150-linked-list/","content":"ğŸ”— é“¾è¡¨ä¸“é¢˜ (11é¢˜) ğŸ¯ æ ¸å¿ƒæŠ€å·§ï¼šè™šæ‹Ÿå¤´èŠ‚ç‚¹ã€å¿«æ…¢æŒ‡é’ˆã€é“¾è¡¨åè½¬ã€åˆå¹¶é“¾è¡¨ ğŸ—ºï¸ é“¾è¡¨æ ¸å¿ƒæ“ä½œå›¾è§£â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ é“¾è¡¨åŸºæœ¬æ“ä½œ â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ â”‚â”‚ éå†: [1] â†’ [2] â†’ [3] â†’ [4] â†’ None â”‚â”‚ â†‘ â”‚â”‚ cur (cur = cur.next) â”‚â”‚ â”‚â”‚ æ’å…¥: [1] â†’ [X] â†’ [2] (å…ˆæ¥åæ–­) â”‚â”‚ â†— â†˜ â”‚â”‚ â”‚â”‚ åˆ é™¤: [1] â”€â”€â”€â”€â”€â”€â†’ [3] (è·¨è¿‡ä¸­é—´èŠ‚ç‚¹) â”‚â”‚ â•³[2] â”‚â”‚ â”‚â”‚ åè½¬: [1] â† [2] â† [3] â† [4] â”‚â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ğŸ”§ é“¾è¡¨ä¸‡èƒ½æŠ€å·§æŠ€å·§1: è™šæ‹Ÿå¤´èŠ‚ç‚¹ (Dummy Head)# é¿å…å¤„ç†å¤´èŠ‚ç‚¹çš„ç‰¹æ®Šæƒ…å†µdummy = ListNode(0)dummy.next = head# ... æ“ä½œé“¾è¡¨return dummy.next æŠ€å·§2: å¿«æ…¢æŒ‡é’ˆ# æ‰¾ä¸­ç‚¹ã€åˆ¤æ–­ç¯ã€æ‰¾ç¯å…¥å£slow = fast = headwhile fast and fast.next: slow = slow.next fast = fast.next.next# slow å°±æ˜¯ä¸­ç‚¹ æŠ€å·§3: é“¾è¡¨åè½¬def reverse(head): prev, curr = None, head while curr: next_temp = curr.next curr.next = prev prev = curr curr = next_temp return prev 1ï¸âƒ£ LC 141. ç¯å½¢é“¾è¡¨ ğŸŸ¢é¢˜ç›®æè¿°åˆ¤æ–­é“¾è¡¨ä¸­æ˜¯å¦æœ‰ç¯ã€‚ ğŸ¨ å›¾è§£æ€è·¯å¿«æ…¢æŒ‡é’ˆï¼šå¦‚æœæœ‰ç¯ï¼Œå¿«æŒ‡é’ˆä¸€å®šä¼šè¿½ä¸Šæ…¢æŒ‡é’ˆ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â–¼ â”‚[1] â†’ [2] â†’ [3] â†’ [4] â†‘ â†‘ slow fastå¿«æŒ‡é’ˆæ¯æ¬¡èµ°2æ­¥ï¼Œæ…¢æŒ‡é’ˆæ¯æ¬¡èµ°1æ­¥ç›¸å¯¹é€Ÿåº¦ä¸º1ï¼Œä¸€å®šä¼šåœ¨ç¯å†…ç›¸é‡ ğŸ’» ä»£ç å®ç°def hasCycle(head: ListNode) - bool: slow = fast = head while fast and fast.next: slow = slow.next fast = fast.next.next if slow == fast: return True return False ğŸ§  è®°å¿†å£è¯€ â€œå¿«æ…¢è¿½é€ï¼Œç›¸é‡æœ‰ç¯â€ 2ï¸âƒ£ LC 2. ä¸¤æ•°ç›¸åŠ  ğŸŸ¡é¢˜ç›®æè¿°ä¸¤ä¸ªé€†åºå­˜å‚¨çš„é“¾è¡¨ç›¸åŠ ï¼Œè¿”å›ç»“æœé“¾è¡¨ã€‚ ğŸ¨ å›¾è§£æ€è·¯ 2 â†’ 4 â†’ 3 (è¡¨ç¤º 342) + 5 â†’ 6 â†’ 4 (è¡¨ç¤º 465) â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ 7 â†’ 0 â†’ 8 (è¡¨ç¤º 807)ä»å¤´åˆ°å°¾é€ä½ç›¸åŠ ï¼Œæ³¨æ„è¿›ä½ï¼Step 1: 2 + 5 = 7, è¿›ä½0Step 2: 4 + 6 = 10, å†™0è¿›1 Step 3: 3 + 4 + 1 = 8, è¿›ä½0 ğŸ’» ä»£ç å®ç°def addTwoNumbers(l1: ListNode, l2: ListNode) - ListNode: dummy = ListNode(0) curr = dummy carry = 0 while l1 or l2 or carry: # å–å€¼ï¼ˆé“¾è¡¨å¯èƒ½é•¿åº¦ä¸åŒï¼‰ val1 = l1.val if l1 else 0 val2 = l2.val if l2 else 0 # è®¡ç®—å’Œä¸è¿›ä½ total = val1 + val2 + carry carry = total // 10 # åˆ›å»ºæ–°èŠ‚ç‚¹ curr.next = ListNode(total % 10) curr = curr.next # ç§»åŠ¨æŒ‡é’ˆ l1 = l1.next if l1 else None l2 = l2.next if l2 else None return dummy.next ğŸ§  è®°å¿†å£è¯€ â€œé€ä½ç›¸åŠ ï¼Œåˆ«å¿˜è¿›ä½â€ 3ï¸âƒ£ LC 21. åˆå¹¶ä¸¤ä¸ªæœ‰åºé“¾è¡¨ ğŸŸ¢é¢˜ç›®æè¿°å°†ä¸¤ä¸ªå‡åºé“¾è¡¨åˆå¹¶ä¸ºä¸€ä¸ªæ–°çš„å‡åºé“¾è¡¨ã€‚ ğŸ¨ å›¾è§£æ€è·¯l1: 1 â†’ 2 â†’ 4l2: 1 â†’ 3 â†’ 4æ¯”è¾ƒå¤´èŠ‚ç‚¹ï¼Œå°çš„æ¥ä¸Šå»ï¼šdummy â†’ 1 â†’ 1 â†’ 2 â†’ 3 â†’ 4 â†’ 4 â†‘ â†‘ â†‘ â†‘ â†‘ â†‘ l1 l2 l1 l2 l1 l2 ğŸ’» ä»£ç å®ç°def mergeTwoLists(l1: ListNode, l2: ListNode) - ListNode: dummy = ListNode(0) curr = dummy while l1 and l2: if l1.val = l2.val: curr.next = l1 l1 = l1.next else: curr.next = l2 l2 = l2.next curr = curr.next # æ¥ä¸Šå‰©ä½™éƒ¨åˆ† curr.next = l1 if l1 else l2 return dummy.next ğŸ§  è®°å¿†å£è¯€ â€œæ¯”è¾ƒå–å°ï¼Œå‰©ä½™ç›´æ¥æ¥â€ 4ï¸âƒ£ LC 138. éšæœºé“¾è¡¨çš„å¤åˆ¶ ğŸŸ¡é¢˜ç›®æè¿°æ·±æ‹·è´å¸¦æœ‰éšæœºæŒ‡é’ˆçš„é“¾è¡¨ã€‚ ğŸ¨ å›¾è§£æ€è·¯æ–¹æ³•ï¼šå“ˆå¸Œè¡¨å­˜å‚¨æ˜ å°„å…³ç³»åŸé“¾è¡¨: [1] â†’ [2] â†’ [3] â†“ â†“ â†“ (random) [3] [1] NoneStep 1: åˆ›å»ºæ‰€æœ‰æ–°èŠ‚ç‚¹ï¼Œå»ºç«‹ oldâ†’new æ˜ å°„Step 2: è¿æ¥ next å’Œ random æŒ‡é’ˆ ğŸ’» ä»£ç å®ç°def copyRandomList(head: 'Node') - 'Node': if not head: return None # å»ºç«‹æ˜ å°„: æ—§èŠ‚ç‚¹ â†’ æ–°èŠ‚ç‚¹ old_to_new = {} # ç¬¬ä¸€éï¼šåˆ›å»ºæ‰€æœ‰æ–°èŠ‚ç‚¹ curr = head while curr: old_to_new[curr] = Node(curr.val) curr = curr.next # ç¬¬äºŒéï¼šè¿æ¥ next å’Œ random curr = head while curr: new_node = old_to_new[curr] new_node.next = old_to_new.get(curr.next) new_node.random = old_to_new.get(curr.random) curr = curr.next return old_to_new[head] ğŸ”¥ O(1) ç©ºé—´è§£æ³•def copyRandomList(head: 'Node') - 'Node': if not head: return None # Step 1: åœ¨æ¯ä¸ªèŠ‚ç‚¹åæ’å…¥å¤åˆ¶èŠ‚ç‚¹ # A â†’ A' â†’ B â†’ B' â†’ C â†’ C' curr = head while curr: new_node = Node(curr.val, curr.next) curr.next = new_node curr = new_node.next # Step 2: è®¾ç½® random æŒ‡é’ˆ curr = head while curr: if curr.random: curr.next.random = curr.random.next curr = curr.next.next # Step 3: æ‹†åˆ†é“¾è¡¨ dummy = Node(0) new_curr = dummy curr = head while curr: new_curr.next = curr.next new_curr = new_curr.next curr.next = curr.next.next curr = curr.next return dummy.next ğŸ§  è®°å¿†å£è¯€ â€œå“ˆå¸Œæ˜ å°„ï¼Œä¸¤éæå®šâ€ 5ï¸âƒ£ LC 92. åè½¬é“¾è¡¨ II ğŸŸ¡é¢˜ç›®æè¿°åè½¬é“¾è¡¨çš„ç¬¬ left åˆ°ç¬¬ right ä¸ªèŠ‚ç‚¹ã€‚ ğŸ¨ å›¾è§£æ€è·¯åŸé“¾è¡¨: 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5, left=2, right=4Step 1: æ‰¾åˆ° left å‰ä¸€ä¸ªèŠ‚ç‚¹ 1 â†’ [2 â†’ 3 â†’ 4] â†’ 5 â†‘ â””â”€åè½¬åŒºé—´â”€â”˜ prevStep 2: åè½¬ä¸­é—´éƒ¨åˆ† 1 2 â† 3 â† 4 5 â†‘ â†“ â†‘ prev â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜Step 3: é‡æ–°è¿æ¥ 1 â†’ 4 â†’ 3 â†’ 2 â†’ 5 ğŸ’» ä»£ç å®ç°def reverseBetween(head: ListNode, left: int, right: int) - ListNode: dummy = ListNode(0, head) prev = dummy # ç§»åŠ¨åˆ° left å‰ä¸€ä¸ªä½ç½® for _ in range(left - 1): prev = prev.next # åè½¬ [left, right] åŒºé—´ curr = prev.next for _ in range(right - left): # æŠŠ curr.next æ’åˆ° prev åé¢ next_node = curr.next curr.next = next_node.next next_node.next = prev.next prev.next = next_node return dummy.next ğŸ§  è®°å¿†å£è¯€ â€œå¤´æ’æ³•åè½¬ï¼Œä¸€æ¬¡éå†â€ 6ï¸âƒ£ LC 25. K ä¸ªä¸€ç»„ç¿»è½¬é“¾è¡¨ ğŸ”´é¢˜ç›®æè¿°æ¯ k ä¸ªèŠ‚ç‚¹ä¸€ç»„è¿›è¡Œç¿»è½¬ã€‚ ğŸ¨ å›¾è§£æ€è·¯é“¾è¡¨: 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5, k = 2ç¬¬1ç»„: [1,2] â†’ åè½¬ â†’ [2,1]ç¬¬2ç»„: [3,4] â†’ åè½¬ â†’ [4,3] ç¬¬3ç»„: [5] â†’ ä¸è¶³kä¸ªï¼Œä¿æŒåŸæ ·ç»“æœ: 2 â†’ 1 â†’ 4 â†’ 3 â†’ 5 ğŸ’» ä»£ç å®ç°def reverseKGroup(head: ListNode, k: int) - ListNode: # æ£€æŸ¥æ˜¯å¦æœ‰ k ä¸ªèŠ‚ç‚¹ def get_kth(node, k): while node and k 0: node = node.next k -= 1 return node # åè½¬é“¾è¡¨ def reverse(head, tail): prev = tail.next curr = head while prev != tail: next_temp = curr.next curr.next = prev prev = curr curr = next_temp return tail, head # æ–°çš„å¤´å’Œå°¾ dummy = ListNode(0, head) prev_group = dummy while True: # æ‰¾åˆ°è¿™ä¸€ç»„çš„å°¾èŠ‚ç‚¹ kth = get_kth(prev_group, k) if not kth: break next_group = kth.next # åè½¬è¿™ä¸€ç»„ head, tail = prev_group.next, kth new_head, new_tail = reverse(head, tail) # è¿æ¥ prev_group.next = new_head new_tail.next = next_group prev_group = new_tail return dummy.next ğŸ§  è®°å¿†å£è¯€ â€œå¤Ÿkå°±ç¿»ï¼Œä¸å¤Ÿå°±ç•™â€ 7ï¸âƒ£ LC 19. åˆ é™¤é“¾è¡¨çš„å€’æ•°ç¬¬ N ä¸ªèŠ‚ç‚¹ ğŸŸ¡é¢˜ç›®æè¿°åˆ é™¤é“¾è¡¨çš„å€’æ•°ç¬¬ n ä¸ªèŠ‚ç‚¹ã€‚ ğŸ¨ å›¾è§£æ€è·¯å¿«æ…¢æŒ‡é’ˆï¼Œå¿«æŒ‡é’ˆå…ˆèµ° n æ­¥é“¾è¡¨: 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5, n = 2Step 1: fast å…ˆèµ° 2 æ­¥ 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5 â†‘ â†‘ slow fastStep 2: åŒæ—¶ç§»åŠ¨ç›´åˆ° fast åˆ°æœ«å°¾ 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5 â†’ None â†‘ â†‘ slow fastStep 3: åˆ é™¤ slow.next 1 â†’ 2 â†’ 3 â”€â”€â”€â”€â†’ 5 ğŸ’» ä»£ç å®ç°def removeNthFromEnd(head: ListNode, n: int) - ListNode: dummy = ListNode(0, head) slow = fast = dummy # fast å…ˆèµ° n+1 æ­¥ for _ in range(n + 1): fast = fast.next # åŒæ—¶ç§»åŠ¨ while fast: slow = slow.next fast = fast.next # åˆ é™¤ slow.next slow.next = slow.next.next return dummy.next ğŸ§  è®°å¿†å£è¯€ â€œå¿«å…ˆèµ°næ­¥ï¼ŒåŒè¡Œåˆ°æœ«å°¾â€ 8ï¸âƒ£ LC 82. åˆ é™¤æ’åºé“¾è¡¨ä¸­çš„é‡å¤å…ƒç´  II ğŸŸ¡é¢˜ç›®æè¿°åˆ é™¤æ‰€æœ‰é‡å¤çš„èŠ‚ç‚¹ï¼Œåªä¿ç•™åŸå§‹é“¾è¡¨ä¸­æ²¡æœ‰é‡å¤å‡ºç°çš„æ•°å­—ã€‚ ğŸ¨ å›¾è§£æ€è·¯1 â†’ 2 â†’ 3 â†’ 3 â†’ 4 â†’ 4 â†’ 5æ£€æµ‹é‡å¤å¹¶åˆ é™¤æ•´ç»„ï¼š1 â†’ 2 â†’ [3 â†’ 3] â†’ [4 â†’ 4] â†’ 5 åˆ é™¤ åˆ é™¤ç»“æœ: 1 â†’ 2 â†’ 5 ğŸ’» ä»£ç å®ç°def deleteDuplicates(head: ListNode) - ListNode: dummy = ListNode(0, head) prev = dummy while prev.next: curr = prev.next # æ£€æµ‹æ˜¯å¦æœ‰é‡å¤ if curr.next and curr.val == curr.next.val: # è·³è¿‡æ‰€æœ‰é‡å¤èŠ‚ç‚¹ while curr.next and curr.val == curr.next.val: curr = curr.next prev.next = curr.next # åˆ é™¤æ•´ç»„ else: prev = prev.next return dummy.next ğŸ§  è®°å¿†å£è¯€ â€œè§é‡å¤å…¨åˆ ï¼Œä¸é‡å¤æ‰ç•™â€ 9ï¸âƒ£ LC 61. æ—‹è½¬é“¾è¡¨ ğŸŸ¡é¢˜ç›®æè¿°å°†é“¾è¡¨æ¯ä¸ªèŠ‚ç‚¹å‘å³ç§»åŠ¨ k ä¸ªä½ç½®ã€‚ ğŸ¨ å›¾è§£æ€è·¯1 â†’ 2 â†’ 3 â†’ 4 â†’ 5, k = 2Step 1: è¿æˆç¯ 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5 â†‘ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜Step 2: æ‰¾åˆ°æ–°çš„æ–­ç‚¹ (n - k % n) æ–°å¤´æ˜¯ç¬¬ 5-2=3 ä¸ªèŠ‚ç‚¹ä¹‹å Step 3: åœ¨æ­£ç¡®ä½ç½®æ–­å¼€ 4 â†’ 5 â†’ 1 â†’ 2 â†’ 3 ğŸ’» ä»£ç å®ç°def rotateRight(head: ListNode, k: int) - ListNode: if not head or not head.next or k == 0: return head # è®¡ç®—é•¿åº¦å¹¶æ‰¾åˆ°å°¾èŠ‚ç‚¹ length = 1 tail = head while tail.next: tail = tail.next length += 1 # å®é™…éœ€è¦ç§»åŠ¨çš„æ­¥æ•° k = k % length if k == 0: return head # æ‰¾åˆ°æ–°çš„å°¾èŠ‚ç‚¹ï¼ˆç¬¬ length - k ä¸ªï¼‰ new_tail = head for _ in range(length - k - 1): new_tail = new_tail.next # é‡æ–°è¿æ¥ new_head = new_tail.next new_tail.next = None tail.next = head return new_head ğŸ§  è®°å¿†å£è¯€ â€œå…ˆæˆç¯ï¼Œå†æ–­å¼€â€ ğŸ”Ÿ LC 86. åˆ†éš”é“¾è¡¨ ğŸŸ¡é¢˜ç›®æè¿°å°†é“¾è¡¨æŒ‰å€¼ x åˆ†æˆä¸¤éƒ¨åˆ†ï¼šå°äº x çš„åœ¨å‰ï¼Œå¤§äºç­‰äº x çš„åœ¨åã€‚ ğŸ¨ å›¾è§£æ€è·¯1 â†’ 4 â†’ 3 â†’ 2 â†’ 5 â†’ 2, x = 3åˆ†æˆä¸¤ä¸ªé“¾è¡¨ï¼šå°äº3: 1 â†’ 2 â†’ 2â‰¥3: 4 â†’ 3 â†’ 5åˆå¹¶: 1 â†’ 2 â†’ 2 â†’ 4 â†’ 3 â†’ 5 ğŸ’» ä»£ç å®ç°def partition(head: ListNode, x: int) - ListNode: # ä¸¤ä¸ªè™šæ‹Ÿå¤´èŠ‚ç‚¹ small_dummy = ListNode(0) large_dummy = ListNode(0) small = small_dummy large = large_dummy while head: if head.val x: small.next = head small = small.next else: large.next = head large = large.next head = head.next # è¿æ¥ä¸¤ä¸ªé“¾è¡¨ large.next = None # é˜²æ­¢æˆç¯ small.next = large_dummy.next return small_dummy.next ğŸ§  è®°å¿†å£è¯€ â€œåˆ†ä¸¤é˜Ÿï¼Œå†åˆå¹¶â€ 1ï¸âƒ£1ï¸âƒ£ LC 146. LRU ç¼“å­˜ ğŸŸ¡é¢˜ç›®æè¿°å®ç° LRU (æœ€è¿‘æœ€å°‘ä½¿ç”¨) ç¼“å­˜æœºåˆ¶ã€‚ ğŸ¨ å›¾è§£æ€è·¯ä½¿ç”¨åŒå‘é“¾è¡¨ + å“ˆå¸Œè¡¨åŒå‘é“¾è¡¨ï¼ˆæŒ‰ä½¿ç”¨æ—¶é—´æ’åºï¼‰:head â‡„ [æœ€è¿‘ç”¨] â‡„ [æ¬¡è¿‘ç”¨] â‡„ ... â‡„ [æœ€ä¹…æœªç”¨] â‡„ tailå“ˆå¸Œè¡¨: key â†’ é“¾è¡¨èŠ‚ç‚¹get/put æ“ä½œ:1. é€šè¿‡å“ˆå¸Œè¡¨ O(1) æ‰¾åˆ°èŠ‚ç‚¹2. ç§»åŠ¨åˆ°é“¾è¡¨å¤´éƒ¨ï¼ˆæœ€è¿‘ä½¿ç”¨ï¼‰3. è¶…å®¹é‡æ—¶åˆ é™¤é“¾è¡¨å°¾éƒ¨ ğŸ’» ä»£ç å®ç°class DLinkedNode: def __init__(self, key=0, value=0): self.key = key self.value = value self.prev = None self.next = Noneclass LRUCache: def __init__(self, capacity: int): self.cache = {} # key â†’ node self.capacity = capacity # è™šæ‹Ÿå¤´å°¾èŠ‚ç‚¹ self.head = DLinkedNode() self.tail = DLinkedNode() self.head.next = self.tail self.tail.prev = self.head def _remove(self, node): \"\"\"ä»é“¾è¡¨ä¸­åˆ é™¤èŠ‚ç‚¹\"\"\" node.prev.next = node.next node.next.prev = node.prev def _add_to_head(self, node): \"\"\"æ·»åŠ åˆ°é“¾è¡¨å¤´éƒ¨\"\"\" node.prev = self.head node.next = self.head.next self.head.next.prev = node self.head.next = node def _move_to_head(self, node): \"\"\"ç§»åŠ¨åˆ°å¤´éƒ¨\"\"\" self._remove(node) self._add_to_head(node) def _remove_tail(self): \"\"\"åˆ é™¤å°¾éƒ¨èŠ‚ç‚¹\"\"\" node = self.tail.prev self._remove(node) return node def get(self, key: int) - int: if key not in self.cache: return -1 node = self.cache[key] self._move_to_head(node) return node.value def put(self, key: int, value: int) - None: if key in self.cache: node = self.cache[key] node.value = value self._move_to_head(node) else: node = DLinkedNode(key, value) self.cache[key] = node self._add_to_head(node) if len(self.cache) self.capacity: tail = self._remove_tail() del self.cache[tail.key] ğŸ§  è®°å¿†å£è¯€ â€œåŒé“¾è¡¨è®°é¡ºåºï¼Œå“ˆå¸Œè¡¨å¿«æŸ¥æ‰¾â€ ğŸ“Š æœ¬ç« æ€»ç»“é“¾è¡¨æŠ€å·§é€ŸæŸ¥è¡¨ æŠ€å·§ ä½¿ç”¨åœºæ™¯ å…¸å‹é¢˜ç›® è™šæ‹Ÿå¤´èŠ‚ç‚¹ å¯èƒ½ä¿®æ”¹å¤´èŠ‚ç‚¹ 21, 82, 86 å¿«æ…¢æŒ‡é’ˆ æ‰¾ä¸­ç‚¹/åˆ¤ç¯ 141, 19 åè½¬é“¾è¡¨ ç¿»è½¬æ“ä½œ 92, 25 å“ˆå¸Œè¡¨è¾…åŠ© å¤æ‚æŒ‡é’ˆå…³ç³» 138, 146 åŒæŒ‡é’ˆ åˆå¹¶/åˆ†éš” 21, 86 ğŸ§  å…¨ç« è®°å¿†å£è¯€ç¯åŠ åˆå¤åè½¬ç»„åˆ å€’åˆ é‡æ—‹åˆ†ç¼“ç¯ - ç¯å½¢é“¾è¡¨ (141)åŠ  - ä¸¤æ•°ç›¸åŠ  (2)åˆ - åˆå¹¶æœ‰åºé“¾è¡¨ (21)å¤ - å¤åˆ¶éšæœºé“¾è¡¨ (138)å - åè½¬é“¾è¡¨ II (92)ç»„ - Kä¸ªä¸€ç»„ç¿»è½¬ (25)åˆ å€’ - åˆ é™¤å€’æ•°ç¬¬Nä¸ª (19)åˆ é‡ - åˆ é™¤é‡å¤å…ƒç´  II (82)æ—‹ - æ—‹è½¬é“¾è¡¨ (61)åˆ† - åˆ†éš”é“¾è¡¨ (86)ç¼“ - LRUç¼“å­˜ (146) ğŸ“– ä¸‹ä¸€ç¯‡ï¼šäºŒå‰æ ‘ä¸“é¢˜","tags":["LeetCode"],"categories":["ç®—æ³•"]},{"title":"â›°ï¸ LeetCode 150 - å †/ä¼˜å…ˆé˜Ÿåˆ—ä¸“é¢˜","path":"/2026/01/18/leetcode-150-heap/","content":"â›°ï¸ å †/ä¼˜å…ˆé˜Ÿåˆ—ä¸“é¢˜ (5é¢˜) ğŸ¯ æ ¸å¿ƒç‰¹æ€§ï¼šå¿«é€Ÿè·å–æœ€å¤§/æœ€å°å€¼ï¼ŒO(log n) æ’å…¥åˆ é™¤ ğŸ—ºï¸ å †çš„åŸºç¡€çŸ¥è¯†â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ å †çš„ç»“æ„ â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ â”‚â”‚ æœ€å°å † (Min Heap) æœ€å¤§å † (Max Heap) â”‚â”‚ 1 9 â”‚â”‚ / \\ / \\ â”‚â”‚ 3 5 7 8 â”‚â”‚ / \\ / \\ â”‚â”‚ 7 8 3 5 â”‚â”‚ â”‚â”‚ çˆ¶èŠ‚ç‚¹ â‰¤ å­èŠ‚ç‚¹ çˆ¶èŠ‚ç‚¹ â‰¥ å­èŠ‚ç‚¹ â”‚â”‚ å †é¡¶æ˜¯æœ€å°å€¼ å †é¡¶æ˜¯æœ€å¤§å€¼ â”‚â”‚ â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ Python ä¸­ä½¿ç”¨ heapqï¼ˆé»˜è®¤æœ€å°å †ï¼‰ â”‚â”‚ â”‚â”‚ import heapq â”‚â”‚ heapq.heappush(heap, item) # å…¥å † â”‚â”‚ heapq.heappop(heap) # å‡ºå † â”‚â”‚ heapq.heapify(list) # åˆ—è¡¨è½¬å † â”‚â”‚ heap[0] # æŸ¥çœ‹å †é¡¶ â”‚â”‚ â”‚â”‚ æœ€å¤§å †æŠ€å·§ï¼šå­˜å…¥è´Ÿæ•° â”‚â”‚ heapq.heappush(heap, -item) â”‚â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ğŸ”§ å †çš„å¸¸è§åº”ç”¨â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ å †çš„å…¸å‹åº”ç”¨åœºæ™¯ â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ â”‚â”‚ 1. Top K é—®é¢˜ â”‚â”‚ æ‰¾ç¬¬Kå¤§ â†’ ç»´æŠ¤å¤§å°ä¸ºKçš„æœ€å°å † â”‚â”‚ æ‰¾ç¬¬Kå° â†’ ç»´æŠ¤å¤§å°ä¸ºKçš„æœ€å¤§å † â”‚â”‚ â”‚â”‚ 2. åˆå¹¶Kä¸ªæœ‰åºåˆ—è¡¨ â”‚â”‚ ç”¨å †ç»´æŠ¤Kä¸ªåˆ—è¡¨çš„å½“å‰æœ€å°å…ƒç´  â”‚â”‚ â”‚â”‚ 3. æ•°æ®æµä¸­çš„ä¸­ä½æ•° â”‚â”‚ ä¸¤ä¸ªå †ï¼šæœ€å¤§å †ï¼ˆå·¦åŠéƒ¨åˆ†ï¼‰+ æœ€å°å †ï¼ˆå³åŠéƒ¨åˆ†ï¼‰ â”‚â”‚ â”‚â”‚ 4. ä»»åŠ¡è°ƒåº¦ â”‚â”‚ æŒ‰ä¼˜å…ˆçº§å¤„ç†ä»»åŠ¡ â”‚â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 1ï¸âƒ£ LC 215. æ•°ç»„ä¸­çš„ç¬¬Kä¸ªæœ€å¤§å…ƒç´  ğŸŸ¡é¢˜ç›®æè¿°åœ¨æœªæ’åºçš„æ•°ç»„ä¸­æ‰¾åˆ°ç¬¬ k ä¸ªæœ€å¤§çš„å…ƒç´ ã€‚ ğŸ¨ å›¾è§£æ€è·¯nums = [3,2,1,5,6,4], k = 2æ–¹æ³•1ï¼šæ’åºåå–ç¬¬kä¸ª O(n log n)æ–¹æ³•2ï¼šæœ€å°å †ç»´æŠ¤kä¸ªæœ€å¤§å…ƒç´  O(n log k)æ–¹æ³•3ï¼šå¿«é€Ÿé€‰æ‹© O(n) å¹³å‡æœ€å°å †æ–¹æ³•ï¼šç»´æŠ¤å¤§å°ä¸ºkçš„æœ€å°å †ï¼Œå †é¡¶å°±æ˜¯ç¬¬kå¤§éå†: 3 â†’ [3] 2 â†’ [2,3] (k=2) 1 â†’ [2,3] (12ï¼Œä¸å…¥å †) 5 â†’ [3,5] (5å…¥å †ï¼Œå¼¹å‡º2) 6 â†’ [5,6] (6å…¥å †ï¼Œå¼¹å‡º3) 4 â†’ [5,6] (45ï¼Œä¸å…¥å †)ç»“æœ: å †é¡¶ 5 ğŸ’» ä»£ç å®ç°import heapqdef findKthLargest(nums: list, k: int) - int: # æ–¹æ³•1ï¼šæœ€å°å † heap = [] for num in nums: heapq.heappush(heap, num) if len(heap) k: heapq.heappop(heap) return heap[0] # æ–¹æ³•2ï¼šç›´æ¥ç”¨ nlargest # return heapq.nlargest(k, nums)[-1] ğŸ§  è®°å¿†å£è¯€ â€œç¬¬Kå¤§ç”¨æœ€å°å †ï¼Œå †é¡¶å°±æ˜¯ç­”æ¡ˆâ€ 2ï¸âƒ£ LC 502. IPO ğŸ”´é¢˜ç›®æè¿°ç»™å®šè‹¥å¹²é¡¹ç›®ï¼ˆåˆ©æ¶¦å’Œèµ„æœ¬éœ€æ±‚ï¼‰ï¼Œåˆå§‹èµ„æœ¬ wï¼Œæœ€å¤šåš k ä¸ªé¡¹ç›®ï¼Œæ±‚æœ€å¤§èµ„æœ¬ã€‚ ğŸ¨ å›¾è§£æ€è·¯k = 2, w = 0profits = [1,2,3]capitals = [0,1,1]è´ªå¿ƒ + å †ï¼šæ¯æ¬¡é€‰æ‹©å½“å‰èµ„æœ¬èƒ½åšçš„é¡¹ç›®ä¸­åˆ©æ¶¦æœ€å¤§çš„åˆå§‹ w=0: å¯åšé¡¹ç›®: (profit=1, capital=0) åšé¡¹ç›®1 â†’ w=1w=1: å¯åšé¡¹ç›®: (profit=2, capital=1), (profit=3, capital=1) é€‰æœ€å¤§ â†’ åšé¡¹ç›®3 â†’ w=4ç»“æœ: 4 ğŸ’» ä»£ç å®ç°import heapqdef findMaximizedCapital(k: int, w: int, profits: list, capital: list) - int: # æŒ‰èµ„æœ¬æ’åºçš„é¡¹ç›®åˆ—è¡¨ projects = sorted(zip(capital, profits)) max_heap = [] # æœ€å¤§å †å­˜å¯åšé¡¹ç›®çš„åˆ©æ¶¦ i = 0 n = len(projects) for _ in range(k): # æŠŠå½“å‰èµ„æœ¬èƒ½åšçš„é¡¹ç›®åŠ å…¥å † while i n and projects[i][0] = w: heapq.heappush(max_heap, -projects[i][1]) # è´Ÿæ•°æ¨¡æ‹Ÿæœ€å¤§å † i += 1 if not max_heap: break # é€‰åˆ©æ¶¦æœ€å¤§çš„é¡¹ç›® w += -heapq.heappop(max_heap) return w ğŸ§  è®°å¿†å£è¯€ â€œèµ„æœ¬å¤Ÿå°±å…¥å †ï¼Œè´ªå¿ƒé€‰æœ€å¤§åˆ©æ¶¦â€ 3ï¸âƒ£ LC 373. æŸ¥æ‰¾å’Œæœ€å°çš„ K å¯¹æ•°å­— ğŸŸ¡é¢˜ç›®æè¿°ä»ä¸¤ä¸ªå‡åºæ•°ç»„ä¸­æ‰¾å‡ºå’Œæœ€å°çš„ k å¯¹æ•°å­—ã€‚ ğŸ¨ å›¾è§£æ€è·¯nums1 = [1,7,11], nums2 = [2,4,6], k = 3å¯è§†åŒ–çŸ©é˜µï¼ˆå’Œï¼‰: 2 4 6 1 3 5 7 7 9 11 13 11 13 15 17BFSæ€æƒ³ï¼šä»(0,0)å¼€å§‹ï¼Œæ¯æ¬¡æ‰©å±•å³è¾¹å’Œä¸‹è¾¹åˆå§‹: (1+2=3, i=0, j=0)å¼¹å‡º(3), åŠ å…¥(1+4=5, 0,1) å’Œ (7+2=9, 1,0)å¼¹å‡º(5), åŠ å…¥(1+6=7, 0,2)å¼¹å‡º(7), ...ç»“æœ: [(1,2), (1,4), (1,6)] ğŸ’» ä»£ç å®ç°import heapqdef kSmallestPairs(nums1: list, nums2: list, k: int) - list: if not nums1 or not nums2: return [] result = [] heap = [(nums1[0] + nums2[0], 0, 0)] # (sum, i, j) visited = {(0, 0)} while heap and len(result) k: _, i, j = heapq.heappop(heap) result.append([nums1[i], nums2[j]]) # æ‰©å±•åˆ°å³è¾¹ if i + 1 len(nums1) and (i + 1, j) not in visited: heapq.heappush(heap, (nums1[i + 1] + nums2[j], i + 1, j)) visited.add((i + 1, j)) # æ‰©å±•åˆ°ä¸‹è¾¹ if j + 1 len(nums2) and (i, j + 1) not in visited: heapq.heappush(heap, (nums1[i] + nums2[j + 1], i, j + 1)) visited.add((i, j + 1)) return result ğŸ§  è®°å¿†å£è¯€ â€œçŸ©é˜µBFSï¼Œå †å–æœ€å°æ‰©å±•â€ 4ï¸âƒ£ LC 295. æ•°æ®æµçš„ä¸­ä½æ•° ğŸ”´é¢˜ç›®æè¿°è®¾è®¡ä¸€ä¸ªæ”¯æŒæ·»åŠ æ•°å­—å’Œè·å–ä¸­ä½æ•°çš„æ•°æ®ç»“æ„ã€‚ ğŸ¨ å›¾è§£æ€è·¯ç”¨ä¸¤ä¸ªå †ï¼š- æœ€å¤§å † (left): å­˜è¾ƒå°çš„ä¸€åŠ- æœ€å°å † (right): å­˜è¾ƒå¤§çš„ä¸€åŠä¿æŒ: len(left) == len(right) æˆ– len(left) == len(right) + 1æ•°æ®æµ: [1, 2, 3]add(1): left=[1], right=[]add(2): left=[1], right=[2]add(3): left=[1,2], right=[3] left(max) right(min) [2] [3] [1]ä¸­ä½æ•°: - å¥‡æ•°ä¸ª: leftå †é¡¶- å¶æ•°ä¸ª: (leftå †é¡¶ + rightå †é¡¶) / 2 ğŸ’» ä»£ç å®ç°import heapqclass MedianFinder: def __init__(self): self.left = [] # æœ€å¤§å †ï¼ˆå­˜è´Ÿæ•°ï¼‰ self.right = [] # æœ€å°å † def addNum(self, num: int) - None: # å…ˆåŠ å…¥left heapq.heappush(self.left, -num) # æŠŠleftæœ€å¤§çš„ç»™right heapq.heappush(self.right, -heapq.heappop(self.left)) # å¹³è¡¡ï¼šä¿æŒleft = right if len(self.left) len(self.right): heapq.heappush(self.left, -heapq.heappop(self.right)) def findMedian(self) - float: if len(self.left) len(self.right): return -self.left[0] return (-self.left[0] + self.right[0]) / 2 ğŸ§  è®°å¿†å£è¯€ â€œå·¦å¤§å³å°ï¼Œå¹³è¡¡ä¿æŒï¼Œå †é¡¶æ‰¾ä¸­ä½â€ 5ï¸âƒ£ LC 23. åˆå¹¶ K ä¸ªå‡åºé“¾è¡¨ ğŸ”´é¢˜ç›®æè¿°å°† k ä¸ªå‡åºé“¾è¡¨åˆå¹¶æˆä¸€ä¸ªå‡åºé“¾è¡¨ã€‚ ğŸ¨ å›¾è§£æ€è·¯lists = [[1,4,5], [1,3,4], [2,6]]ç”¨æœ€å°å †ç»´æŠ¤kä¸ªé“¾è¡¨çš„å½“å‰å¤´èŠ‚ç‚¹åˆå§‹å †: [1, 1, 2] (ä¸‰ä¸ªé“¾è¡¨å¤´)å¼¹å‡º1 â†’ ç»“æœ[1]ï¼ŒåŠ å…¥4å †: [1, 2, 4]å¼¹å‡º1 â†’ ç»“æœ[1,1]ï¼ŒåŠ å…¥3å †: [2, 3, 4]...ç»“æœ: [1,1,2,3,4,4,5,6] ğŸ’» ä»£ç å®ç°import heapqdef mergeKLists(lists): # è‡ªå®šä¹‰æ¯”è¾ƒï¼ˆé¿å…é“¾è¡¨èŠ‚ç‚¹ç›´æ¥æ¯”è¾ƒï¼‰ heap = [] for i, lst in enumerate(lists): if lst: heapq.heappush(heap, (lst.val, i, lst)) dummy = ListNode(0) curr = dummy while heap: val, i, node = heapq.heappop(heap) curr.next = node curr = curr.next if node.next: heapq.heappush(heap, (node.next.val, i, node.next)) return dummy.next ğŸ§  è®°å¿†å£è¯€ â€œKä¸ªå¤´å…¥å †ï¼Œå¼¹æœ€å°æ¥åç»§â€ ğŸ“Š æœ¬ç« æ€»ç»“é¢˜ç›®é€ŸæŸ¥è¡¨ é¢˜å· é¢˜ç›® éš¾åº¦ ç±»å‹ 215 ç¬¬Kä¸ªæœ€å¤§å…ƒç´  ğŸŸ¡ Top K 502 IPO ğŸ”´ è´ªå¿ƒ+å † 373 Kå¯¹æœ€å°å’Œ ğŸŸ¡ å¤šè·¯å½’å¹¶ 295 æ•°æ®æµä¸­ä½æ•° ğŸ”´ åŒå † 23 åˆå¹¶Kä¸ªé“¾è¡¨ ğŸ”´ å¤šè·¯å½’å¹¶ å †çš„è§£é¢˜æ¨¡å¼â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ å †çš„è§£é¢˜æ¨¡å¼ â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ â”‚â”‚ Top K é—®é¢˜: â”‚â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚â”‚ ç¬¬Kå¤§ â†’ å¤§å°ä¸ºKçš„æœ€å°å † â”‚â”‚ ç¬¬Kå° â†’ å¤§å°ä¸ºKçš„æœ€å¤§å † â”‚â”‚ â”‚â”‚ å¤šè·¯å½’å¹¶: â”‚â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚â”‚ å †ä¸­ç»´æŠ¤æ¯è·¯çš„å½“å‰å…ƒç´  â”‚â”‚ æ¯æ¬¡å–æœ€å°ï¼Œç„¶ååŠ å…¥è¯¥è·¯çš„ä¸‹ä¸€ä¸ª â”‚â”‚ â”‚â”‚ åŒå †æŠ€å·§: â”‚â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚â”‚ ä¸­ä½æ•°ï¼šå·¦åŠæœ€å¤§å † + å³åŠæœ€å°å † â”‚â”‚ æ»‘åŠ¨çª—å£ä¸­ä½æ•°åŒç† â”‚â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ğŸ§  å…¨ç« è®°å¿†å£è¯€TopKé—®é¢˜å †æ¥å‡‘ç¬¬Kå¤§ç”¨å°å †å®ˆå¤šè·¯å½’å¹¶ç»´æŠ¤å¤´åŒå †ä¸­ä½æ•°æœ€æºœ215 - ç¬¬Kå¤§å…ƒç´ 502 - IPOé¡¹ç›®é€‰æ‹©373 - Kå¯¹æœ€å°å’Œ295 - æ•°æ®æµä¸­ä½æ•°23 - åˆå¹¶Kä¸ªé“¾è¡¨ ğŸ“– ä¸‹ä¸€ç¯‡ï¼šäºŒåˆ†æŸ¥æ‰¾ä¸“é¢˜","tags":["LeetCode"],"categories":["ç®—æ³•"]},{"title":"ğŸŒ³ LeetCode 150 - äºŒå‰æ ‘ä¸“é¢˜","path":"/2026/01/18/leetcode-150-binary-tree/","content":"ğŸŒ³ äºŒå‰æ ‘ä¸“é¢˜ (14é¢˜) ğŸ¯ æ ¸å¿ƒæ€æƒ³ï¼šé€’å½’æ€ç»´ + åˆ†è§£é—®é¢˜ ğŸ—ºï¸ äºŒå‰æ ‘çš„æ€ç»´æ¨¡å¼â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ äºŒå‰æ ‘çš„ä¸¤ç§æ€ç»´æ¨¡å¼ â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ â”‚â”‚ æ¨¡å¼ä¸€ï¼šéå†æ€ç»´ â”‚â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚â”‚ ç”¨ä¸€ä¸ª traverse å‡½æ•°éå†æ•´æ£µæ ‘ â”‚â”‚ åœ¨éå†è¿‡ç¨‹ä¸­æ›´æ–°å¤–éƒ¨å˜é‡ â”‚â”‚ â”‚â”‚ æ¨¡å¼äºŒï¼šåˆ†è§£é—®é¢˜æ€ç»´ â”‚â”‚ â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ â”‚â”‚ å°†é—®é¢˜åˆ†è§£ä¸ºå­é—®é¢˜ â”‚â”‚ é€šè¿‡å­é—®é¢˜çš„ç­”æ¡ˆæ¨å¯¼å‡ºåŸé—®é¢˜çš„ç­”æ¡ˆ â”‚â”‚ â”‚â”‚ 1 â”‚â”‚ / \\ â”‚â”‚ 2 3 é—®é¢˜(æ ¹) = f(é—®é¢˜(å·¦), é—®é¢˜(å³)) â”‚â”‚ / \\ â”‚â”‚ 4 5 â”‚â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ğŸ”§ äºŒå‰æ ‘éå†æ¨¡æ¿å‰åºéå†ï¼ˆæ ¹-å·¦-å³ï¼‰def preorder(root): if not root: return print(root.val) # å…ˆå¤„ç†æ ¹ preorder(root.left) # å†å·¦å­æ ‘ preorder(root.right) # åå³å­æ ‘ ä¸­åºéå†ï¼ˆå·¦-æ ¹-å³ï¼‰def inorder(root): if not root: return inorder(root.left) # å…ˆå·¦å­æ ‘ print(root.val) # å†å¤„ç†æ ¹ inorder(root.right) # åå³å­æ ‘ ååºéå†ï¼ˆå·¦-å³-æ ¹ï¼‰def postorder(root): if not root: return postorder(root.left) # å…ˆå·¦å­æ ‘ postorder(root.right) # å†å³å­æ ‘ print(root.val) # åå¤„ç†æ ¹ å±‚åºéå†ï¼ˆBFSï¼‰from collections import dequedef levelorder(root): if not root: return [] queue = deque([root]) result = [] while queue: level = [] for _ in range(len(queue)): node = queue.popleft() level.append(node.val) if node.left: queue.append(node.left) if node.right: queue.append(node.right) result.append(level) return result 1ï¸âƒ£ LC 104. äºŒå‰æ ‘çš„æœ€å¤§æ·±åº¦ ğŸŸ¢é¢˜ç›®æè¿°è¿”å›äºŒå‰æ ‘çš„æœ€å¤§æ·±åº¦ã€‚ ğŸ¨ å›¾è§£æ€è·¯```python / \\ 9 20 / \\ 15 7åˆ†è§£æ€ç»´:maxDepth(3) = 1 + max(maxDepth(9), maxDepth(20)) = 1 + max(1, 2) = 3 ğŸ’» ä»£ç å®ç°def maxDepth(root) - int: if not root: return 0 left_depth = maxDepth(root.left) right_depth = maxDepth(root.right) return 1 + max(left_depth, right_depth) ğŸ§  è®°å¿†å£è¯€ â€œæ·±åº¦ = 1 + max(å·¦æ·±åº¦, å³æ·±åº¦)â€ 2ï¸âƒ£ LC 100. ç›¸åŒçš„æ ‘ ğŸŸ¢é¢˜ç›®æè¿°åˆ¤æ–­ä¸¤æ£µæ ‘æ˜¯å¦ç›¸åŒã€‚### ğŸ¨ å›¾è§£æ€è·¯ p q 1 1 / \\ / \\ 2 3 2 3ç›¸åŒçš„æ¡ä»¶:1. æ ¹èŠ‚ç‚¹å€¼ç›¸åŒ2. å·¦å­æ ‘ç›¸åŒ3. å³å­æ ‘ç›¸åŒ### ğŸ’» ä»£ç å®ç°def isSameTree(p, q) - bool: if not p and not q: return True if not p or not q: return False return (p.val == q.val and isSameTree(p.left, q.left) and isSameTree(p.right, q.right))### ğŸ§  è®°å¿†å£è¯€ â€œæ ¹åŒå·¦åŒå³åŒï¼Œæ‰æ˜¯çœŸçš„åŒâ€3ï¸âƒ£ LC 226. ç¿»è½¬äºŒå‰æ ‘ ğŸŸ¢é¢˜ç›®æè¿°ç¿»è½¬äºŒå‰æ ‘ï¼ˆé•œåƒï¼‰ã€‚### ğŸ¨ å›¾è§£æ€è·¯ 4 4 / \\ / \\ 2 7 = 7 2 / \\ / \\ / \\ / \\1 3 6 9 9 6 3 1äº¤æ¢æ¯ä¸ªèŠ‚ç‚¹çš„å·¦å³å­æ ‘### ğŸ’» ä»£ç å®ç°def invertTree(root): if not root: return None # äº¤æ¢å·¦å³å­æ ‘ root.left, root.right = root.right, root.left # é€’å½’ç¿»è½¬å­æ ‘ invertTree(root.left) invertTree(root.right) return root### ğŸ§  è®°å¿†å£è¯€ â€œå…ˆäº¤æ¢ï¼Œå†é€’å½’â€4ï¸âƒ£ LC 101. å¯¹ç§°äºŒå‰æ ‘ ğŸŸ¢é¢˜ç›®æè¿°åˆ¤æ–­äºŒå‰æ ‘æ˜¯å¦å¯¹ç§°ã€‚### ğŸ¨ å›¾è§£æ€è·¯ / \\ 2 2 / \\ / \\3 4 4 3å¯¹ç§°æ¡ä»¶:å·¦å­æ ‘çš„å·¦ == å³å­æ ‘çš„å³å·¦å­æ ‘çš„å³ == å³å­æ ‘çš„å·¦### ğŸ’» ä»£ç å®ç°def isSymmetric(root) - bool: def check(left, right): if not left and not right: return True if not left or not right: return False return (left.val == right.val and check(left.left, right.right) and check(left.right, right.left)) return check(root.left, root.right) if root else True### ğŸ§  è®°å¿†å£è¯€ â€œå¤–å¤–ç›¸ç­‰ï¼Œå†…å†…ç›¸ç­‰â€5ï¸âƒ£ LC 105. ä»å‰åºä¸ä¸­åºéå†åºåˆ—æ„é€ äºŒå‰æ ‘ ğŸŸ¡é¢˜ç›®æè¿°æ ¹æ®å‰åºå’Œä¸­åºéå†ç»“æœï¼Œæ„å»ºäºŒå‰æ ‘ã€‚### ğŸ¨ å›¾è§£æ€è·¯preorder = [3, 9, 20, 15, 7] æ ¹-å·¦-å³inorder = [9, 3, 15, 20, 7] å·¦-æ ¹-å³æ­¥éª¤:1. preorder[0] = 3 æ˜¯æ ¹èŠ‚ç‚¹2. åœ¨ inorder ä¸­æ‰¾åˆ° 3ï¼Œå·¦è¾¹æ˜¯å·¦å­æ ‘ï¼Œå³è¾¹æ˜¯å³å­æ ‘3. é€’å½’æ„å»º / \\ 9 20 / \\ 15 7### ğŸ’» ä»£ç å®ç°def buildTree(preorder: list, inorder: list): if not preorder: return None # æ ¹èŠ‚ç‚¹æ˜¯å‰åºç¬¬ä¸€ä¸ª root = TreeNode(preorder[0]) # åœ¨ä¸­åºä¸­æ‰¾åˆ°æ ¹èŠ‚ç‚¹ä½ç½® mid = inorder.index(preorder[0]) # é€’å½’æ„å»ºå·¦å³å­æ ‘ root.left = buildTree(preorder[1:mid+1], inorder[:mid]) root.right = buildTree(preorder[mid+1:], inorder[mid+1:]) return root### ğŸ§  è®°å¿†å£è¯€ â€œå‰åºå®šæ ¹ï¼Œä¸­åºåˆ†è¾¹â€6ï¸âƒ£ LC 106. ä»ä¸­åºä¸ååºéå†åºåˆ—æ„é€ äºŒå‰æ ‘ ğŸŸ¡é¢˜ç›®æè¿°æ ¹æ®ä¸­åºå’Œååºéå†ç»“æœï¼Œæ„å»ºäºŒå‰æ ‘ã€‚### ğŸ¨ å›¾è§£æ€è·¯inorder = [9, 3, 15, 20, 7] å·¦-æ ¹-å³postorder = [9, 15, 7, 20, 3] å·¦-å³-æ ¹ååºæœ€åä¸€ä¸ªæ˜¯æ ¹ï¼### ğŸ’» ä»£ç å®ç°def buildTree(inorder: list, postorder: list): if not postorder: return None # æ ¹èŠ‚ç‚¹æ˜¯ååºæœ€åä¸€ä¸ª root = TreeNode(postorder[-1]) # åœ¨ä¸­åºä¸­æ‰¾åˆ°æ ¹èŠ‚ç‚¹ä½ç½® mid = inorder.index(postorder[-1]) # é€’å½’æ„å»ºå·¦å³å­æ ‘ root.left = buildTree(inorder[:mid], postorder[:mid]) root.right = buildTree(inorder[mid+1:], postorder[mid:-1]) return root### ğŸ§  è®°å¿†å£è¯€ â€œååºå®šæ ¹ï¼ˆæœ€åï¼‰ï¼Œä¸­åºåˆ†è¾¹â€7ï¸âƒ£ LC 117. å¡«å……æ¯ä¸ªèŠ‚ç‚¹çš„ä¸‹ä¸€ä¸ªå³ä¾§èŠ‚ç‚¹æŒ‡é’ˆ II ğŸŸ¡é¢˜ç›®æè¿°å¡«å……æ¯ä¸ªèŠ‚ç‚¹çš„ next æŒ‡é’ˆæŒ‡å‘å³ä¾§èŠ‚ç‚¹ã€‚### ğŸ¨ å›¾è§£æ€è·¯ 1 â†’ NULL / \\ 2 â†’ 3 â†’ NULL / \\ \\4â†’ 5 â†’ 7 â†’ NULLä½¿ç”¨å±‚åºéå†ï¼Œè¿æ¥åŒå±‚èŠ‚ç‚¹### ğŸ’» ä»£ç å®ç°def connect(root): if not root: return None queue = deque([root]) while queue: size = len(queue) prev = None for i in range(size): node = queue.popleft() if prev: prev.next = node prev = node if node.left: queue.append(node.left) if node.right: queue.append(node.right) return root### ğŸ§  è®°å¿†å£è¯€ â€œå±‚åºéå†ï¼Œå‰è¿åâ€8ï¸âƒ£ LC 114. äºŒå‰æ ‘å±•å¼€ä¸ºé“¾è¡¨ ğŸŸ¡é¢˜ç›®æè¿°å°†äºŒå‰æ ‘å±•å¼€ä¸ºå•é“¾è¡¨ï¼ˆå‰åºé¡ºåºï¼‰ã€‚### ğŸ¨ å›¾è§£æ€è·¯ 1 1 / \\ \\ 2 5 = 2 / \\ \\ \\3 4 6 3 \\ \\ \\### ğŸ’» ä»£ç å®ç°def flatten(root) - None: if not root: return # ååºéå†ï¼šå…ˆå¤„ç†å­æ ‘ï¼Œå†å¤„ç†æ ¹ flatten(root.left) flatten(root.right) # ä¿å­˜å³å­æ ‘ right = root.right # å·¦å­æ ‘ç§»åˆ°å³è¾¹ root.right = root.left root.left = None # æ‰¾åˆ°å³å­æ ‘æœ«ç«¯ï¼Œæ¥ä¸ŠåŸå³å­æ ‘ while root.right: root = root.right root.right = right### ğŸ§  è®°å¿†å£è¯€ â€œå·¦æ¥å³ï¼ŒåŸå³æ¥æœ«å°¾â€9ï¸âƒ£ LC 112. è·¯å¾„æ€»å’Œ ğŸŸ¢é¢˜ç›®æè¿°åˆ¤æ–­æ˜¯å¦å­˜åœ¨æ ¹åˆ°å¶å­è·¯å¾„ï¼Œå…¶å’Œç­‰äºç›®æ ‡å€¼ã€‚### ğŸ¨ å›¾è§£æ€è·¯ / \\ 4 8 / / \\ 11 13 4 / \\ \\7 2 1targetSum = 22è·¯å¾„: 5 â†’ 4 â†’ 11 â†’ 2 = 22 âœ“### ğŸ’» ä»£ç å®ç°def hasPathSum(root, targetSum: int) - bool: if not root: return False # å¶å­èŠ‚ç‚¹ if not root.left and not root.right: return root.val == targetSum # é€’å½’æ£€æŸ¥å­æ ‘ remaining = targetSum - root.val return (hasPathSum(root.left, remaining) or hasPathSum(root.right, remaining))### ğŸ§  è®°å¿†å£è¯€ â€œå¶å­åˆ¤ç›¸ç­‰ï¼Œéå¶é€’å½’å‡â€ğŸ”Ÿ LC 129. æ±‚æ ¹èŠ‚ç‚¹åˆ°å¶èŠ‚ç‚¹æ•°å­—ä¹‹å’Œ ğŸŸ¡é¢˜ç›®æè¿°æ¯æ¡è·¯å¾„ç»„æˆä¸€ä¸ªæ•°å­—ï¼Œæ±‚æ‰€æœ‰æ•°å­—ä¹‹å’Œã€‚### ğŸ¨ å›¾è§£æ€è·¯ / \\ 2 3è·¯å¾„: 1â†’2 = 12è·¯å¾„: 1â†’3 = 13æ€»å’Œ: 12 + 13 = 25### ğŸ’» ä»£ç å®ç°def sumNumbers(root) - int: def dfs(node, current_sum): if not node: return 0 current_sum = current_sum * 10 + node.val # å¶å­èŠ‚ç‚¹ if not node.left and not node.right: return current_sum return dfs(node.left, current_sum) + dfs(node.right, current_sum) return dfs(root, 0)### ğŸ§  è®°å¿†å£è¯€ â€œè¿›ä½ä¹˜10åŠ å½“å‰â€1ï¸âƒ£1ï¸âƒ£ LC 124. äºŒå‰æ ‘ä¸­çš„æœ€å¤§è·¯å¾„å’Œ ğŸ”´é¢˜ç›®æè¿°æ‰¾å‡ºè·¯å¾„å’Œæœ€å¤§çš„è·¯å¾„ï¼ˆå¯ä»¥ä¸ç»è¿‡æ ¹èŠ‚ç‚¹ï¼‰ã€‚### ğŸ¨ å›¾è§£æ€è·¯ -10 / \\ 9 20 / \\ 15 7æœ€å¤§è·¯å¾„: 15 â†’ 20 â†’ 7 = 42æ€è·¯:æ¯ä¸ªèŠ‚ç‚¹å¯ä»¥ï¼š1. åªè´¡çŒ®è‡ªå·±ï¼ˆä½œä¸ºè·¯å¾„ç«¯ç‚¹ï¼‰2. è´¡çŒ®è‡ªå·±+å·¦å­æ ‘3. è´¡çŒ®è‡ªå·±+å³å­æ ‘4. ä½œä¸ºæ‹ç‚¹ï¼ˆå·¦+è‡ªå·±+å³ï¼‰### ğŸ’» ä»£ç å®ç°def maxPathSum(root) - int: max_sum = float('-inf') def max_gain(node): nonlocal max_sum if not node: return 0 # å·¦å³å­æ ‘çš„æœ€å¤§è´¡çŒ®ï¼ˆè´Ÿæ•°ä¸è¦ï¼‰ left_gain = max(max_gain(node.left), 0) right_gain = max(max_gain(node.right), 0) # å½“å‰èŠ‚ç‚¹ä½œä¸ºæ‹ç‚¹çš„è·¯å¾„å’Œ path_sum = node.val + left_gain + right_gain max_sum = max(max_sum, path_sum) # è¿”å›ç»™çˆ¶èŠ‚ç‚¹çš„è´¡çŒ®ï¼ˆåªèƒ½é€‰ä¸€è¾¹ï¼‰ return node.val + max(left_gain, right_gain) max_gain(root) return max_sum### ğŸ§  è®°å¿†å£è¯€ â€œæ‹ç‚¹ç®—å…¨å±€ï¼Œè´¡çŒ®é€‰ä¸€è¾¹â€1ï¸âƒ£2ï¸âƒ£ LC 173. äºŒå‰æœç´¢æ ‘è¿­ä»£å™¨ ğŸŸ¡é¢˜ç›®æè¿°å®ç°äºŒå‰æœç´¢æ ‘çš„è¿­ä»£å™¨ã€‚### ğŸ’» ä»£ç å®ç°class BSTIterator: def __init__(self, root): self.stack = [] self._leftmost_inorder(root) def _leftmost_inorder(self, node): while node: self.stack.append(node) node = node.left def next(self) - int: node = self.stack.pop() if node.right: self._leftmost_inorder(node.right) return node.val def hasNext(self) - bool: return len(self.stack) 0### ğŸ§  è®°å¿†å£è¯€ â€œæ ˆå­˜å·¦é“¾ï¼Œå¼¹å‡ºå¤„ç†å³â€1ï¸âƒ£3ï¸âƒ£ LC 222. å®Œå…¨äºŒå‰æ ‘çš„èŠ‚ç‚¹ä¸ªæ•° ğŸŸ¢é¢˜ç›®æè¿°ç»Ÿè®¡å®Œå…¨äºŒå‰æ ‘çš„èŠ‚ç‚¹ä¸ªæ•°ã€‚### ğŸ’» ä»£ç å®ç°def countNodes(root) - int: if not root: return 0 left_depth = right_depth = 0 left, right = root, root while left: left_depth += 1 left = left.left while right: right_depth += 1 right = right.right # æ»¡äºŒå‰æ ‘ if left_depth == right_depth: return 2 ** left_depth - 1 # é€’å½’ return 1 + countNodes(root.left) + countNodes(root.right)### ğŸ§  è®°å¿†å£è¯€ â€œæ»¡æ ‘ç”¨å…¬å¼ï¼Œä¸æ»¡é€’å½’æ•°â€1ï¸âƒ£4ï¸âƒ£ LC 236. äºŒå‰æ ‘çš„æœ€è¿‘å…¬å…±ç¥–å…ˆ ğŸŸ¡é¢˜ç›®æè¿°æ‰¾ä¸¤ä¸ªèŠ‚ç‚¹çš„æœ€è¿‘å…¬å…±ç¥–å…ˆï¼ˆLCAï¼‰ã€‚### ğŸ¨ å›¾è§£æ€è·¯ / \\ 5 1 / \\ / \\ 6 2 0 8 / \\ 7 4LCA(5, 1) = 3LCA(5, 4) = 5### ğŸ’» ä»£ç å®ç°def lowestCommonAncestor(root, p, q): if not root or root == p or root == q: return root left = lowestCommonAncestor(root.left, p, q) right = lowestCommonAncestor(root.right, p, q) # p, q åˆ†å¸ƒåœ¨ä¸¤è¾¹ if left and right: return root # p, q åœ¨åŒä¸€è¾¹ return left if left else right### ğŸ§  è®°å¿†å£è¯€ â€œå·¦å³éƒ½æœ‰è¿”å›æ ¹ï¼Œå¦åˆ™è¿”å›æœ‰çš„é‚£è¾¹â€ğŸ“Š æœ¬ç« æ€»ç»“é¢˜ç›®é€ŸæŸ¥è¡¨| é¢˜å· | é¢˜ç›® | éš¾åº¦ | ç±»å‹ | | 104 | æœ€å¤§æ·±åº¦ | ğŸŸ¢ | æ·±åº¦ || 100 | ç›¸åŒçš„æ ‘ | ğŸŸ¢ | æ¯”è¾ƒ || 226 | ç¿»è½¬äºŒå‰æ ‘ | ğŸŸ¢ | å˜æ¢ || 101 | å¯¹ç§°äºŒå‰æ ‘ | ğŸŸ¢ | æ¯”è¾ƒ || 105 | å‰åº+ä¸­åºæ„é€  | ğŸŸ¡ | æ„é€  || 106 | ä¸­åº+ååºæ„é€  | ğŸŸ¡ | æ„é€  || 117 | å¡«å……nextæŒ‡é’ˆ | ğŸŸ¡ | å±‚åº || 114 | å±•å¼€ä¸ºé“¾è¡¨ | ğŸŸ¡ | å˜æ¢ || 112 | è·¯å¾„æ€»å’Œ | ğŸŸ¢ | è·¯å¾„ || 129 | æ•°å­—ä¹‹å’Œ | ğŸŸ¡ | è·¯å¾„ || 124 | æœ€å¤§è·¯å¾„å’Œ | ğŸ”´ | è·¯å¾„ || 173 | BSTè¿­ä»£å™¨ | ğŸŸ¡ | è¿­ä»£ || 222 | å®Œå…¨æ ‘èŠ‚ç‚¹æ•° | ğŸŸ¢ | è®¡æ•° || 236 | æœ€è¿‘å…¬å…±ç¥–å…ˆ | ğŸŸ¡ | LCA |### ğŸ§  å…¨ç« è®°å¿†å£è¯€ æ·±åº¦ç›¸åŒç¿»å¯¹ç§°å‰ä¸­ååºå»ºæ ‘å‹è¿æ¥å±•å¼€èµ°è·¯å¾„è¿­ä»£è®¡æ•°æ‰¾ç¥–å®— æ·±åº¦ - æœ€å¤§æ·±åº¦ (104)ç›¸åŒ - ç›¸åŒçš„æ ‘ (100)ç¿» - ç¿»è½¬äºŒå‰æ ‘ (226)å¯¹ç§° - å¯¹ç§°äºŒå‰æ ‘ (101)å‰ä¸­ååº - ä»éå†æ„é€  (105, 106)è¿æ¥ - å¡«å……nextæŒ‡é’ˆ (117)å±•å¼€ - å±•å¼€ä¸ºé“¾è¡¨ (114)è·¯å¾„ - è·¯å¾„æ€»å’Œç³»åˆ— (112, 129, 124)è¿­ä»£ - BSTè¿­ä»£å™¨ (173)è®¡æ•° - èŠ‚ç‚¹ä¸ªæ•° (222)ç¥–å®— - æœ€è¿‘å…¬å…±ç¥–å…ˆ (236) --- ğŸ“– **ä¸‹ä¸€ç¯‡**ï¼š[äºŒå‰æœç´¢æ ‘ä¸“é¢˜](/2026/01/18/leetcode-150-bst/)","tags":["LeetCode"],"categories":["ç®—æ³•"]},{"title":"ğŸªŸ LeetCode 150 - æ»‘åŠ¨çª—å£ä¸“é¢˜","path":"/2026/01/18/leetcode-150-sliding-window/","content":"ğŸªŸ æ»‘åŠ¨çª—å£ä¸“é¢˜ (4é¢˜) ğŸ¯ æ ¸å¿ƒæŠ€å·§ï¼šå³æ‰©å·¦ç¼©ã€å“ˆå¸Œè®¡æ•°ã€æ¡ä»¶æ”¶ç¼© ğŸ—ºï¸ æ»‘åŠ¨çª—å£æ ¸å¿ƒæ€æƒ³â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ æ»‘åŠ¨çª—å£å·¥ä½œåŸç† â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ â”‚â”‚ Step 1: å³è¾¹ç•Œæ‰©å¼  â”‚â”‚ â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â” â”‚â”‚ â”‚ a â”‚ b â”‚ c â”‚ d â”‚ e â”‚ f â”‚ g â”‚ â”‚â”‚ â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜ â”‚â”‚ L R â”€â”€â”€â”€â”€â”€â–¶ â”‚â”‚ â””â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”˜ â”‚â”‚ window â”‚â”‚ â”‚â”‚ Step 2: æ»¡è¶³æ¡ä»¶æ—¶å·¦è¾¹ç•Œæ”¶ç¼© â”‚â”‚ â”Œâ”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â”¬â”€â”€â”€â” â”‚â”‚ â”‚ a â”‚ b â”‚ c â”‚ d â”‚ e â”‚ f â”‚ g â”‚ â”‚â”‚ â””â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”´â”€â”€â”€â”˜ â”‚â”‚ L â”€â”€â–¶ R â”‚â”‚ â””â”€â”€â”€â”¬â”€â”€â”€â”˜ â”‚â”‚ window (æ”¶ç¼©å) â”‚â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ğŸ”§ æ»‘åŠ¨çª—å£ä¸‡èƒ½æ¨¡æ¿def sliding_window(s): from collections import defaultdict window = defaultdict(int) # çª—å£å†…å®¹è®¡æ•° left = 0 result = 0 # æˆ–è€…å…¶ä»–ç»“æœå˜é‡ for right in range(len(s)): # â‘  å³è¾¹ç•Œæ‰©å¼ ï¼šå°† s[right] åŠ å…¥çª—å£ c = s[right] window[c] += 1 # â‘¡ åˆ¤æ–­æ˜¯å¦éœ€è¦æ”¶ç¼©ï¼ˆæ ¹æ®é¢˜ç›®æ¡ä»¶ï¼‰ while éœ€è¦æ”¶ç¼©çš„æ¡ä»¶: # å°† s[left] ç§»å‡ºçª—å£ d = s[left] window[d] -= 1 left += 1 # â‘¢ æ›´æ–°ç»“æœï¼ˆæ ¹æ®é¢˜ç›®è¦æ±‚ï¼‰ result = max(result, right - left + 1) return result 1ï¸âƒ£ LC 209. é•¿åº¦æœ€å°çš„å­æ•°ç»„ ğŸŸ¡é¢˜ç›®æè¿°æ‰¾å‡ºå’Œ â‰¥ target çš„æœ€çŸ­è¿ç»­å­æ•°ç»„é•¿åº¦ã€‚ ğŸ¨ å›¾è§£æ€è·¯nums = [2, 3, 1, 2, 4, 3], target = 7çª—å£æ»‘åŠ¨è¿‡ç¨‹ï¼šâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ [2] sum=2 7 æ‰©å¼  â”‚â”‚ [2,3] sum=5 7 æ‰©å¼  â”‚â”‚ [2,3,1] sum=6 7 æ‰©å¼  â”‚â”‚ [2,3,1,2] sum=8 â‰¥ 7 è®°å½•4 â”‚â”‚ [3,1,2] sum=6 7 æ‰©å¼  â”‚â”‚ [3,1,2,4] sum=10 â‰¥ 7 è®°å½•4 â”‚â”‚ [1,2,4] sum=7 â‰¥ 7 è®°å½•3 â”‚â”‚ [2,4] sum=6 7 æ‰©å¼  â”‚â”‚ [2,4,3] sum=9 â‰¥ 7 è®°å½•3 â”‚â”‚ [4,3] sum=7 â‰¥ 7 è®°å½•2 âœ“â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜æœ€å°é•¿åº¦ = 2 ğŸ’» ä»£ç å®ç°def minSubArrayLen(target: int, nums: list) - int: left = 0 window_sum = 0 min_len = float('inf') for right in range(len(nums)): # æ‰©å¼ ï¼šåŠ å…¥å³è¾¹å…ƒç´  window_sum += nums[right] # æ”¶ç¼©ï¼šæ»¡è¶³æ¡ä»¶æ—¶å°è¯•ç¼©å°çª—å£ while window_sum = target: min_len = min(min_len, right - left + 1) window_sum -= nums[left] left += 1 return min_len if min_len != float('inf') else 0 ğŸ§  è®°å¿†å£è¯€ â€œå¤Ÿäº†å°±ç¼©ï¼Œä¸å¤Ÿå°±æ‰©â€ 2ï¸âƒ£ LC 3. æ— é‡å¤å­—ç¬¦çš„æœ€é•¿å­ä¸² ğŸŸ¡é¢˜ç›®æè¿°æ‰¾å‡ºä¸å«é‡å¤å­—ç¬¦çš„æœ€é•¿å­ä¸²é•¿åº¦ã€‚ ğŸ¨ å›¾è§£æ€è·¯s = \"abcabcbb\"çª—å£æ»‘åŠ¨è¿‡ç¨‹ï¼šâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ [a] æ— é‡å¤ len=1 â”‚â”‚ [a,b] æ— é‡å¤ len=2 â”‚â”‚ [a,b,c] æ— é‡å¤ len=3 âœ“ â”‚â”‚ [a,b,c,a] æœ‰é‡å¤! æ”¶ç¼© â”‚â”‚ [b,c,a] æ— é‡å¤ len=3 â”‚â”‚ [b,c,a,b] æœ‰é‡å¤! æ”¶ç¼© â”‚â”‚ [c,a,b] æ— é‡å¤ len=3 â”‚â”‚ ... â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜æœ€é•¿æ— é‡å¤å­ä¸²é•¿åº¦ = 3 ğŸ’» ä»£ç å®ç°def lengthOfLongestSubstring(s: str) - int: window = {} # è®°å½•å­—ç¬¦å‡ºç°æ¬¡æ•° left = 0 max_len = 0 for right in range(len(s)): c = s[right] window[c] = window.get(c, 0) + 1 # æœ‰é‡å¤å­—ç¬¦æ—¶æ”¶ç¼© while window[c] 1: d = s[left] window[d] -= 1 left += 1 max_len = max(max_len, right - left + 1) return max_len ğŸ”¥ ä¼˜åŒ–ç‰ˆæœ¬ï¼ˆè®°å½•ä½ç½®ï¼‰def lengthOfLongestSubstring(s: str) - int: char_index = {} # è®°å½•å­—ç¬¦æœ€åå‡ºç°çš„ä½ç½® left = 0 max_len = 0 for right, c in enumerate(s): # å¦‚æœå­—ç¬¦åœ¨çª—å£å†…å‡ºç°è¿‡ï¼Œç›´æ¥è·³åˆ°é‡å¤ä½ç½®ä¹‹å if c in char_index and char_index[c] = left: left = char_index[c] + 1 char_index[c] = right max_len = max(max_len, right - left + 1) return max_len ğŸ§  è®°å¿†å£è¯€ â€œé‡å¤å°±è·³è¿‡ï¼Œè®°å½•æœ€é•¿åº¦â€ 3ï¸âƒ£ LC 30. ä¸²è”æ‰€æœ‰å•è¯çš„å­ä¸² ğŸ”´é¢˜ç›®æè¿°æ‰¾å‡ºå­—ç¬¦ä¸²ä¸­æ‰€æœ‰æ˜¯ words ä¸­æ‰€æœ‰å•è¯ä¸²è”ç»“æœçš„èµ·å§‹ç´¢å¼•ã€‚ ğŸ¨ å›¾è§£æ€è·¯s = \"barfoothefoobarman\"words = [\"foo\", \"bar\"] (æ¯ä¸ªé•¿åº¦ä¸º3)ä¸²è”ç»“æœå¯èƒ½æ˜¯ \"foobar\" æˆ– \"barfoo\"ï¼ˆé•¿åº¦6ï¼‰æ£€æŸ¥æ¯ä¸ªå¯èƒ½çš„èµ·å§‹ä½ç½®ï¼šä½ç½®0: \"barfoo\" âœ“ (bar + foo)ä½ç½®3: \"foothe\" âœ—ä½ç½®6: \"thefoo\" âœ—ä½ç½®9: \"foobar\" âœ“ (foo + bar)...ç»“æœ: [0, 9] ğŸ’» ä»£ç å®ç°def findSubstring(s: str, words: list) - list: if not s or not words: return [] from collections import Counter word_len = len(words[0]) word_count = len(words) total_len = word_len * word_count word_freq = Counter(words) result = [] # åªéœ€è¦æ£€æŸ¥ word_len ç§èµ·å§‹åç§» for offset in range(word_len): left = offset window = Counter() count = 0 # çª—å£å†…æœ‰æ•ˆå•è¯æ•° for right in range(offset, len(s) - word_len + 1, word_len): word = s[right:right + word_len] if word in word_freq: window[word] += 1 count += 1 # å¦‚æœæŸå•è¯è¶…å‡ºéœ€è¦çš„æ•°é‡ï¼Œæ”¶ç¼©å·¦è¾¹ç•Œ while window[word] word_freq[word]: left_word = s[left:left + word_len] window[left_word] -= 1 count -= 1 left += word_len # æ‰¾åˆ°ä¸€ä¸ªæœ‰æ•ˆçš„ä¸²è” if count == word_count: result.append(left) else: # é‡åˆ°ä¸åœ¨ words ä¸­çš„å•è¯ï¼Œé‡ç½®çª—å£ window.clear() count = 0 left = right + word_len return result ğŸ§  è®°å¿†å£è¯€ â€œå›ºå®šå•è¯é•¿ï¼Œæ»‘åŠ¨æ‰¾ä¸²è”â€ 4ï¸âƒ£ LC 76. æœ€å°è¦†ç›–å­ä¸² ğŸ”´é¢˜ç›®æè¿°æ‰¾å‡º s ä¸­åŒ…å« t æ‰€æœ‰å­—ç¬¦çš„æœ€å°å­ä¸²ã€‚ ğŸ¨ å›¾è§£æ€è·¯s = \"ADOBECODEBANC\", t = \"ABC\"éœ€è¦: A:1, B:1, C:1â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ çª—å£: [A]DOBECODEBANC ç¼ºBC æ‰©å¼  â”‚â”‚ çª—å£: [ADOBEC]ODEBANC æ»¡è¶³! è®°å½•6 æ”¶ç¼© â”‚â”‚ çª—å£: [DOBEC]ODEBANC ç¼ºA æ‰©å¼  â”‚â”‚ çª—å£: [DOBECODEBA]NC æ»¡è¶³! è®°å½•10 æ”¶ç¼© â”‚â”‚ çª—å£: [CODEBA]NC æ»¡è¶³! è®°å½•6 æ”¶ç¼© â”‚â”‚ çª—å£: [ODEBA]NC ç¼ºC æ‰©å¼  â”‚â”‚ çª—å£: [ODEBANC] æ»¡è¶³! è®°å½•7 æ”¶ç¼© â”‚â”‚ çª—å£: [BANC] æ»¡è¶³! è®°å½•4 âœ“ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜æœ€å°è¦†ç›–å­ä¸² = \"BANC\" ğŸ’» ä»£ç å®ç°def minWindow(s: str, t: str) - str: from collections import Counter need = Counter(t) # éœ€è¦çš„å­—ç¬¦åŠæ•°é‡ window = Counter() # çª—å£å†…çš„å­—ç¬¦ have = 0 # å·²æ»¡è¶³çš„å­—ç¬¦ç§ç±»æ•° need_count = len(need) # éœ€è¦æ»¡è¶³çš„å­—ç¬¦ç§ç±»æ•° result = \"\" min_len = float('inf') left = 0 for right in range(len(s)): # æ‰©å¼ ï¼šåŠ å…¥å³è¾¹å­—ç¬¦ c = s[right] window[c] += 1 # å¦‚æœè¯¥å­—ç¬¦æ•°é‡æ­£å¥½æ»¡è¶³éœ€æ±‚ if c in need and window[c] == need[c]: have += 1 # æ”¶ç¼©ï¼šå½“æ‰€æœ‰å­—ç¬¦éƒ½æ»¡è¶³æ—¶ while have == need_count: # æ›´æ–°ç»“æœ if right - left + 1 min_len: min_len = right - left + 1 result = s[left:right + 1] # ç§»å‡ºå·¦è¾¹å­—ç¬¦ d = s[left] if d in need and window[d] == need[d]: have -= 1 window[d] -= 1 left += 1 return result ğŸ§  è®°å¿†å£è¯€ â€œæ‰©åˆ°æ»¡è¶³ï¼Œç¼©åˆ°ä¸å¤Ÿâ€ ğŸ“Š æœ¬ç« æ€»ç»“æ»‘åŠ¨çª—å£åˆ†ç±» ç±»å‹ ç‰¹ç‚¹ å…¸å‹é¢˜ç›® å¯å˜çª—å£æ±‚æœ€å° æ»¡è¶³æ¡ä»¶å°±æ”¶ç¼© 209, 76 å¯å˜çª—å£æ±‚æœ€å¤§ ä¸æ»¡è¶³æ¡ä»¶æ‰æ”¶ç¼© 3 å›ºå®šçª—å£ çª—å£å¤§å°å›ºå®š 30 æ»‘åŠ¨çª—å£æ€ç»´å¯¼å›¾ æ»¡è¶³æ¡ä»¶ï¼Ÿ â”‚ â”Œâ”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â” â–¼ â–¼ æ˜¯ å¦ â”‚ â”‚ â–¼ â–¼æ±‚æœ€å°ï¼Ÿ æ±‚æœ€å¤§ï¼Ÿ â”‚ â”‚ â–¼ â–¼ æ”¶ç¼© æ‰©å¼ æ›´æ–°ç­”æ¡ˆ ç»§ç»­æ‰©å¼  ğŸ§  å…¨ç« è®°å¿†å£è¯€æœ€æ— ä¸²è¦†å››é“é¢˜æ»‘åŠ¨çª—å£å·§è§£æå³æ‰©å·¦ç¼©æ˜¯å…³é”®æ»¡è¶³æ¡ä»¶å†æ”¶ç¼©æœ€ - é•¿åº¦æœ€å°çš„å­æ•°ç»„ (209)æ—  - æ— é‡å¤å­—ç¬¦çš„æœ€é•¿å­ä¸² (3)ä¸² - ä¸²è”æ‰€æœ‰å•è¯çš„å­ä¸² (30)è¦† - æœ€å°è¦†ç›–å­ä¸² (76) ğŸ“– ä¸‹ä¸€ç¯‡ï¼šé“¾è¡¨ä¸“é¢˜","tags":["LeetCode"],"categories":["ç®—æ³•"]},{"title":"ğŸ” LeetCode 150 - å›¾è®ºä¸æœç´¢ä¸“é¢˜","path":"/2026/01/18/leetcode-150-graph/","content":"ğŸ” å›¾è®ºä¸æœç´¢ä¸“é¢˜ (8é¢˜) ğŸ¯ æ ¸å¿ƒæŠ€å·§ï¼šBFSã€DFSã€æ‹“æ‰‘æ’åºã€å¹¶æŸ¥é›† ğŸ—ºï¸ å›¾æœç´¢ç®—æ³•å¯¹æ¯”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ BFS vs DFS â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ BFS â”‚ DFS â”‚â”‚ (å¹¿åº¦ä¼˜å…ˆæœç´¢) â”‚ (æ·±åº¦ä¼˜å…ˆæœç´¢) â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ æ•°æ®ç»“æ„ï¼šé˜Ÿåˆ— â”‚ æ•°æ®ç»“æ„ï¼šæ ˆ/é€’å½’ â”‚â”‚ æ¢ç´¢æ–¹å¼ï¼šå±‚å±‚æ‰©å±• â”‚ æ¢ç´¢æ–¹å¼ï¼šä¸€è·¯èµ°åˆ°åº• â”‚â”‚ é€‚ç”¨ï¼šæœ€çŸ­è·¯å¾„ â”‚ é€‚ç”¨ï¼šè¿é€šæ€§ã€è·¯å¾„æœç´¢ â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ 1 â”‚ 1 â”‚â”‚ /|\\ â”‚ /|\\ â”‚â”‚ 2 3 4 â†’ å±‚åº â”‚ 2 3 4 â†’ æ·±å…¥ â”‚â”‚ /| | â”‚ /| | â”‚â”‚ 5 6 7 â”‚ 5 6 7 â”‚â”‚ â”‚ â”‚â”‚ é¡ºåº: 1â†’2â†’3â†’4â†’5â†’6â†’7 â”‚ é¡ºåº: 1â†’2â†’5â†’6â†’3â†’4â†’7 â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ğŸ”§ BFS æ¨¡æ¿from collections import dequedef bfs(start): queue = deque([start]) visited = {start} level = 0 while queue: size = len(queue) for _ in range(size): node = queue.popleft() # å¤„ç†å½“å‰èŠ‚ç‚¹ process(node) # å°†é‚»å±…åŠ å…¥é˜Ÿåˆ— for neighbor in get_neighbors(node): if neighbor not in visited: visited.add(neighbor) queue.append(neighbor) level += 1 # å±‚æ•° +1 return level ğŸ”§ DFS æ¨¡æ¿def dfs(node, visited): if node in visited: return visited.add(node) # å¤„ç†å½“å‰èŠ‚ç‚¹ process(node) # é€’å½’è®¿é—®é‚»å±… for neighbor in get_neighbors(node): dfs(neighbor, visited) 1ï¸âƒ£ LC 200. å²›å±¿æ•°é‡ ğŸŸ¡é¢˜ç›®æè¿°è®¡ç®—äºŒç»´ç½‘æ ¼ä¸­å²›å±¿çš„æ•°é‡ï¼ˆç”± â€˜1â€™ ç»„æˆçš„è¿é€šåŒºåŸŸï¼‰ã€‚ ğŸ¨ å›¾è§£æ€è·¯grid:1 1 0 0 01 1 0 0 00 0 1 0 00 0 0 1 1å²›å±¿æ•°é‡ = 3ç­–ç•¥ï¼šéå†ç½‘æ ¼ï¼Œé‡åˆ° '1' å°±å¯åŠ¨ DFS/BFSæŠŠæ•´ä¸ªå²›å±¿æ ‡è®°ä¸ºå·²è®¿é—®ï¼Œè®¡æ•° +1 ğŸ’» ä»£ç å®ç° (DFS)def numIslands(grid: list) - int: if not grid: return 0 m, n = len(grid), len(grid[0]) count = 0 def dfs(i, j): # è¾¹ç•Œæ£€æŸ¥ æ˜¯å¦ä¸ºé™†åœ° if i 0 or i = m or j 0 or j = n or grid[i][j] != '1': return # æ ‡è®°ä¸ºå·²è®¿é—® grid[i][j] = '0' # å››ä¸ªæ–¹å‘æ‰©å±• dfs(i + 1, j) dfs(i - 1, j) dfs(i, j + 1) dfs(i, j - 1) for i in range(m): for j in range(n): if grid[i][j] == '1': dfs(i, j) count += 1 return count ğŸ§  è®°å¿†å£è¯€ â€œé‡1å°±æ·¹ï¼Œæ·¹å®Œè®¡æ•°â€ 2ï¸âƒ£ LC 130. è¢«å›´ç»•çš„åŒºåŸŸ ğŸŸ¡é¢˜ç›®æè¿°å°†æ‰€æœ‰è¢« â€˜Xâ€™ å›´ç»•çš„ â€˜Oâ€™ å¡«å……ä¸º â€˜Xâ€™ï¼ˆè¾¹ç•Œä¸Šçš„ â€˜Oâ€™ åŠå…¶è¿é€šçš„ â€˜Oâ€™ ä¸ç®—è¢«å›´ç»•ï¼‰ã€‚ ğŸ¨ å›¾è§£æ€è·¯è¾“å…¥: è¾“å‡º:X X X X X X X XX O O X â†’ X X X XX X O X X X X XX O X X X O X Xé€†å‘æ€ç»´ï¼š1. ä»è¾¹ç•Œçš„ 'O' å¼€å§‹ DFSï¼Œæ ‡è®°ä¸º '#'2. éå†æ•´ä¸ªç½‘æ ¼ï¼š - 'O' â†’ 'X' (è¢«å›´ç»•) - '#' â†’ 'O' (æ¢å¤) ğŸ’» ä»£ç å®ç°def solve(board: list) - None: if not board: return m, n = len(board), len(board[0]) def dfs(i, j): if i 0 or i = m or j 0 or j = n or board[i][j] != 'O': return board[i][j] = '#' dfs(i + 1, j) dfs(i - 1, j) dfs(i, j + 1) dfs(i, j - 1) # ä»è¾¹ç•Œå¼€å§‹æ ‡è®° for i in range(m): dfs(i, 0) dfs(i, n - 1) for j in range(n): dfs(0, j) dfs(m - 1, j) # æ¢å¤å’Œå¡«å…… for i in range(m): for j in range(n): if board[i][j] == 'O': board[i][j] = 'X' elif board[i][j] == '#': board[i][j] = 'O' ğŸ§  è®°å¿†å£è¯€ â€œè¾¹ç•ŒOä¸å›´ï¼Œæ ‡è®°åæ¢å¤â€ 3ï¸âƒ£ LC 133. å…‹éš†å›¾ ğŸŸ¡é¢˜ç›®æè¿°æ·±æ‹·è´ä¸€ä¸ªæ— å‘è¿é€šå›¾ã€‚ ğŸ¨ å›¾è§£æ€è·¯åŸå›¾: å…‹éš†: 1 --- 2 1' --- 2' | | | | 4 --- 3 4' --- 3'ä½¿ç”¨å“ˆå¸Œè¡¨è®°å½• åŸèŠ‚ç‚¹ â†’ å…‹éš†èŠ‚ç‚¹ çš„æ˜ å°„BFS æˆ– DFS éå†å¹¶å…‹éš† ğŸ’» ä»£ç å®ç°def cloneGraph(node: 'Node') - 'Node': if not node: return None # å“ˆå¸Œè¡¨ï¼šåŸèŠ‚ç‚¹ â†’ å…‹éš†èŠ‚ç‚¹ cloned = {} def dfs(node): if node in cloned: return cloned[node] # åˆ›å»ºå…‹éš†èŠ‚ç‚¹ clone = Node(node.val) cloned[node] = clone # å…‹éš†é‚»å±… for neighbor in node.neighbors: clone.neighbors.append(dfs(neighbor)) return clone return dfs(node) ğŸ§  è®°å¿†å£è¯€ â€œå“ˆå¸Œè®°æ˜ å°„ï¼ŒDFSå…‹éš†â€ 4ï¸âƒ£ LC 399. é™¤æ³•æ±‚å€¼ ğŸŸ¡é¢˜ç›®æè¿°ç»™å®šå˜é‡å¯¹çš„é™¤æ³•ç»“æœï¼Œæ±‚è§£å…¶ä»–é™¤æ³•ã€‚ ğŸ¨ å›¾è§£æ€è·¯equations: [[\"a\",\"b\"],[\"b\",\"c\"]]values: [2.0, 3.0]æ„å»ºå¸¦æƒå›¾:a --2.0-- b --3.0-- ca --0.5-- b --0.33-- cæŸ¥è¯¢ a/c = a/b * b/c = 2.0 * 3.0 = 6.0ä½¿ç”¨ BFS æ‰¾è·¯å¾„å¹¶ç´¯ä¹˜æƒé‡ ğŸ’» ä»£ç å®ç°def calcEquation(equations, values, queries): from collections import defaultdict, deque # æ„å»ºå›¾ graph = defaultdict(dict) for (a, b), val in zip(equations, values): graph[a][b] = val graph[b][a] = 1.0 / val def bfs(start, end): if start not in graph or end not in graph: return -1.0 if start == end: return 1.0 queue = deque([(start, 1.0)]) visited = {start} while queue: node, product = queue.popleft() for neighbor, weight in graph[node].items(): if neighbor == end: return product * weight if neighbor not in visited: visited.add(neighbor) queue.append((neighbor, product * weight)) return -1.0 return [bfs(a, b) for a, b in queries] ğŸ§  è®°å¿†å£è¯€ â€œå¸¦æƒå›¾å»ºè¾¹ï¼ŒBFSç´¯ä¹˜â€ 5ï¸âƒ£ LC 207. è¯¾ç¨‹è¡¨ ğŸŸ¡é¢˜ç›®æè¿°åˆ¤æ–­æ˜¯å¦å¯èƒ½å®Œæˆæ‰€æœ‰è¯¾ç¨‹ï¼ˆæ£€æµ‹æœ‰å‘å›¾æ˜¯å¦æœ‰ç¯ï¼‰ã€‚ ğŸ¨ å›¾è§£æ€è·¯numCourses = 4prerequisites = [[1,0],[2,0],[3,1],[3,2]]æ„å»ºå›¾:0 â†’ 1 â†’ 30 â†’ 2 â†—æ‹“æ‰‘æ’åºï¼š1. ç»Ÿè®¡æ¯ä¸ªèŠ‚ç‚¹çš„å…¥åº¦2. å°†å…¥åº¦ä¸º 0 çš„èŠ‚ç‚¹å…¥é˜Ÿ3. BFS å¤„ç†ï¼Œæ¯å¤„ç†ä¸€ä¸ªèŠ‚ç‚¹ï¼Œé‚»å±…å…¥åº¦ -14. å…¥åº¦å˜ä¸º 0 çš„èŠ‚ç‚¹å…¥é˜Ÿ5. å¦‚æœå¤„ç†çš„èŠ‚ç‚¹æ•° = æ€»è¯¾ç¨‹æ•°ï¼Œåˆ™æ— ç¯ ğŸ’» ä»£ç å®ç°def canFinish(numCourses: int, prerequisites: list) - bool: from collections import defaultdict, deque # æ„å»ºå›¾å’Œå…¥åº¦è¡¨ graph = defaultdict(list) in_degree = [0] * numCourses for course, prereq in prerequisites: graph[prereq].append(course) in_degree[course] += 1 # å°†å…¥åº¦ä¸º 0 çš„èŠ‚ç‚¹å…¥é˜Ÿ queue = deque([i for i in range(numCourses) if in_degree[i] == 0]) count = 0 while queue: node = queue.popleft() count += 1 for neighbor in graph[node]: in_degree[neighbor] -= 1 if in_degree[neighbor] == 0: queue.append(neighbor) return count == numCourses ğŸ§  è®°å¿†å£è¯€ â€œå…¥åº¦ä¸ºé›¶å…ˆå¤„ç†ï¼Œå…¨éƒ¨å¤„ç†å®Œæ— ç¯â€ 6ï¸âƒ£ LC 210. è¯¾ç¨‹è¡¨ II ğŸŸ¡é¢˜ç›®æè¿°è¿”å›å®Œæˆæ‰€æœ‰è¯¾ç¨‹çš„å­¦ä¹ é¡ºåºï¼ˆæ‹“æ‰‘æ’åºç»“æœï¼‰ã€‚ ğŸ’» ä»£ç å®ç°def findOrder(numCourses: int, prerequisites: list) - list: from collections import defaultdict, deque graph = defaultdict(list) in_degree = [0] * numCourses for course, prereq in prerequisites: graph[prereq].append(course) in_degree[course] += 1 queue = deque([i for i in range(numCourses) if in_degree[i] == 0]) result = [] while queue: node = queue.popleft() result.append(node) for neighbor in graph[node]: in_degree[neighbor] -= 1 if in_degree[neighbor] == 0: queue.append(neighbor) return result if len(result) == numCourses else [] ğŸ§  è®°å¿†å£è¯€ â€œæ‹“æ‰‘æ’åºï¼Œè®°å½•é¡ºåºâ€ 7ï¸âƒ£ LC 909. è›‡æ¢¯æ£‹ ğŸŸ¡é¢˜ç›®æè¿°åœ¨è›‡æ¢¯æ£‹ç›˜ä¸Šï¼Œæ±‚ä»èµ·ç‚¹åˆ°ç»ˆç‚¹çš„æœ€å°‘ç§»åŠ¨æ¬¡æ•°ã€‚ ğŸ¨ å›¾è§£æ€è·¯æ£‹ç›˜æŒ‰ Boustrophedon æ–¹å¼ç¼–å·ï¼ˆè›‡å½¢ï¼‰ä½¿ç”¨ BFS æ±‚æœ€çŸ­è·¯å¾„æ¯æ¬¡å¯ä»¥èµ° 1-6 æ­¥ï¼ˆæ·éª°å­ï¼‰é‡åˆ°è›‡/æ¢¯å­åˆ™ä¼ é€åˆ°å¯¹åº”ä½ç½® ğŸ’» ä»£ç å®ç°def snakesAndLadders(board: list) - int: from collections import deque n = len(board) # å°†ä½ç½®ç¼–å·è½¬ä¸ºåæ ‡ def get_position(num): num -= 1 row = n - 1 - num // n col = num % n if (n - 1 - row) % 2 == 0 else n - 1 - num % n return row, col queue = deque([(1, 0)]) # (ä½ç½®, æ­¥æ•°) visited = {1} while queue: curr, steps = queue.popleft() for dice in range(1, 7): next_pos = curr + dice if next_pos n * n: continue r, c = get_position(next_pos) if board[r][c] != -1: next_pos = board[r][c] if next_pos == n * n: return steps + 1 if next_pos not in visited: visited.add(next_pos) queue.append((next_pos, steps + 1)) return -1 ğŸ§  è®°å¿†å£è¯€ â€œBFSæ‰¾æœ€çŸ­ï¼Œè›‡æ¢¯è¦ä¼ é€â€ 8ï¸âƒ£ LC 433. æœ€å°åŸºå› å˜åŒ– ğŸŸ¡é¢˜ç›®æè¿°ä»èµ·å§‹åŸºå› å˜åŒ–åˆ°ç›®æ ‡åŸºå› çš„æœ€å°‘å˜åŒ–æ¬¡æ•°ã€‚ ğŸ¨ å›¾è§£æ€è·¯startGene = \"AACCGGTT\"endGene = \"AAACGGTA\"bank = [\"AACCGGTA\",\"AACCGCTA\",\"AAACGGTA\"]æ¯æ¬¡åªèƒ½å˜ä¸€ä¸ªå­—ç¬¦ï¼Œä¸”ç»“æœå¿…é¡»åœ¨ bank ä¸­ä½¿ç”¨ BFS æœç´¢æœ€çŸ­è·¯å¾„ ğŸ’» ä»£ç å®ç°def minMutation(startGene: str, endGene: str, bank: list) - int: from collections import deque bank_set = set(bank) if endGene not in bank_set: return -1 queue = deque([(startGene, 0)]) visited = {startGene} chars = ['A', 'C', 'G', 'T'] while queue: gene, steps = queue.popleft() if gene == endGene: return steps for i in range(len(gene)): for c in chars: if c != gene[i]: new_gene = gene[:i] + c + gene[i+1:] if new_gene in bank_set and new_gene not in visited: visited.add(new_gene) queue.append((new_gene, steps + 1)) return -1 ğŸ§  è®°å¿†å£è¯€ â€œæ¯æ¬¡å˜ä¸€ä¸ªï¼Œåœ¨åº“ä¸­æ‰è¡Œâ€ ğŸ“Š æœ¬ç« æ€»ç»“å›¾æœç´¢åœºæ™¯é€‰æ‹© åœºæ™¯ æ¨èç®—æ³• å…¸å‹é¢˜ç›® æœ€çŸ­è·¯å¾„ BFS 909, 433 è¿é€šæ€§ DFS/BFS 200, 130 æ‹“æ‰‘æ’åº BFS (Kahn) 207, 210 å›¾å…‹éš† DFS + å“ˆå¸Œ 133 å¸¦æƒè·¯å¾„ BFS + ç´¯ä¹˜ 399 ğŸ§  å…¨ç« è®°å¿†å£è¯€å²›å›´å…‹é™¤è¯¾è¯¾è›‡åŸºå›¾è®ºå…«é¢˜è®°ä»”ç»†å²› - å²›å±¿æ•°é‡ (200)å›´ - è¢«å›´ç»•çš„åŒºåŸŸ (130)å…‹ - å…‹éš†å›¾ (133)é™¤ - é™¤æ³•æ±‚å€¼ (399)è¯¾è¯¾ - è¯¾ç¨‹è¡¨ I/II (207, 210)è›‡ - è›‡æ¢¯æ£‹ (909)åŸº - æœ€å°åŸºå› å˜åŒ– (433) ğŸ“– è¿”å›ï¼šLeetCode 150 é¢˜æ€»ç›®å½•","tags":["LeetCode"],"categories":["ç®—æ³•"]},{"title":"ğŸ¯ LeetCode é¢è¯•ç»å…¸ 150 é¢˜ - å®Œå…¨æ”»ç•¥","path":"/2026/01/18/leetcode-150-index/","content":"ğŸ¯ LeetCode é¢è¯•ç»å…¸ 150 é¢˜ - å®Œå…¨æ”»ç•¥ ğŸ“š ç›®æ ‡ï¼šç³»ç»ŸæŒæ¡ç®—æ³•é¢è¯•æ ¸å¿ƒçŸ¥è¯†ç‚¹ï¼Œ6ä¸ªæœˆæ‹¿ä¸‹å¤§å‚ Offerï¼ ğŸ“Š é¢˜ç›®åˆ†ç±»æ€»è§ˆâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ LeetCode é¢è¯• 150 é¢˜ â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ æ•°ç»„/å­—ç¬¦ä¸² â”‚ åŒæŒ‡é’ˆ â”‚ æ»‘åŠ¨çª—å£ â”‚ çŸ©é˜µ â”‚â”‚ (15é¢˜) â”‚ (5é¢˜) â”‚ (4é¢˜) â”‚ (4é¢˜) â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ å“ˆå¸Œè¡¨ â”‚ åŒºé—´ â”‚ æ ˆ â”‚ é“¾è¡¨ â”‚â”‚ (9é¢˜) â”‚ (6é¢˜) â”‚ (7é¢˜) â”‚ (11é¢˜) â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ äºŒå‰æ ‘ â”‚ äºŒå‰æ ‘BFS â”‚ äºŒå‰æœç´¢æ ‘ â”‚ å›¾ â”‚â”‚ (14é¢˜) â”‚ (4é¢˜) â”‚ (5é¢˜) â”‚ (8é¢˜) â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ å›æº¯ â”‚ åˆ†æ²» â”‚ Kadaneç®—æ³• â”‚ äºŒåˆ†æŸ¥æ‰¾ â”‚â”‚ (7é¢˜) â”‚ (3é¢˜) â”‚ (2é¢˜) â”‚ (7é¢˜) â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ å † â”‚ ä½è¿ç®— â”‚ æ•°å­¦ â”‚ ä¸€ç»´åŠ¨æ€è§„åˆ’ â”‚â”‚ (5é¢˜) â”‚ (6é¢˜) â”‚ (9é¢˜) â”‚ (10é¢˜) â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ å¤šç»´åŠ¨æ€è§„åˆ’ (12é¢˜) â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ğŸ—ºï¸ å­¦ä¹ è·¯çº¿å›¾Week 1-2 Week 3-4 Week 5-6 Week 7-8 â”‚ â”‚ â”‚ â”‚ â–¼ â–¼ â–¼ â–¼â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â” â”Œâ”€â”€â”€â”€â”€â”€â”â”‚ æ•°ç»„ â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ é“¾è¡¨ â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ æ ‘ â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚ å›¾ â”‚â”‚ å­—ç¬¦ä¸² â”‚ â”‚ æ ˆ â”‚ â”‚ å † â”‚ â”‚ å›æº¯ â”‚â”‚ åŒæŒ‡é’ˆ â”‚ â”‚ é˜Ÿåˆ— â”‚ â”‚ BFS â”‚ â”‚ DFS â”‚â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â–¼ Week 9-12 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ åŠ¨æ€è§„åˆ’ â”‚ â”‚ äºŒåˆ†æŸ¥æ‰¾ â”‚ â”‚ ä½è¿ç®— â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ğŸ“‘ åˆ†ç±»è¯¦è§£ç›®å½•1ï¸âƒ£ æ•°ç»„ä¸å­—ç¬¦ä¸² (15é¢˜) # é¢˜ç›® éš¾åº¦ æ ¸å¿ƒæŠ€å·§ 88 åˆå¹¶ä¸¤ä¸ªæœ‰åºæ•°ç»„ ğŸŸ¢ åŒæŒ‡é’ˆä»åå¾€å‰ 27 ç§»é™¤å…ƒç´  ğŸŸ¢ å¿«æ…¢æŒ‡é’ˆ 26 åˆ é™¤æœ‰åºæ•°ç»„ä¸­çš„é‡å¤é¡¹ ğŸŸ¢ å¿«æ…¢æŒ‡é’ˆ 80 åˆ é™¤æœ‰åºæ•°ç»„ä¸­çš„é‡å¤é¡¹ II ğŸŸ¡ é€šç”¨æ¨¡æ¿ 169 å¤šæ•°å…ƒç´  ğŸŸ¢ Boyer-MooreæŠ•ç¥¨ 189 è½®è½¬æ•°ç»„ ğŸŸ¡ ä¸‰æ¬¡ç¿»è½¬ 121 ä¹°å–è‚¡ç¥¨çš„æœ€ä½³æ—¶æœº ğŸŸ¢ ç»´æŠ¤æœ€å°å€¼ 122 ä¹°å–è‚¡ç¥¨çš„æœ€ä½³æ—¶æœº II ğŸŸ¡ è´ªå¿ƒç´¯åŠ  55 è·³è·ƒæ¸¸æˆ ğŸŸ¡ è´ªå¿ƒæœ€è¿œè·ç¦» 45 è·³è·ƒæ¸¸æˆ II ğŸŸ¡ BFSæ€æƒ³ 274 H æŒ‡æ•° ğŸŸ¡ è®¡æ•°æ’åº 380 O(1) æ—¶é—´æ’å…¥ã€åˆ é™¤å’Œè·å–éšæœºå…ƒç´  ğŸŸ¡ å“ˆå¸Œ+æ•°ç»„ 238 é™¤è‡ªèº«ä»¥å¤–æ•°ç»„çš„ä¹˜ç§¯ ğŸŸ¡ å‰ç¼€ç§¯+åç¼€ç§¯ 134 åŠ æ²¹ç«™ ğŸŸ¡ ä¸€æ¬¡éå† 135 åˆ†å‘ç³–æœ ğŸ”´ ä¸¤æ¬¡éå† ğŸ§  è®°å¿†å£è¯€ï¼š â€œåˆç§»åˆ åˆ å¤šï¼Œè½®ä¹°ä¹°è·³è·³ï¼ŒHæ’é™¤åŠ ç³–â€ 2ï¸âƒ£ åŒæŒ‡é’ˆ (5é¢˜) # é¢˜ç›® éš¾åº¦ æ ¸å¿ƒæŠ€å·§ 125 éªŒè¯å›æ–‡ä¸² ğŸŸ¢ ç›¸å‘åŒæŒ‡é’ˆ 392 åˆ¤æ–­å­åºåˆ— ğŸŸ¢ åŒå‘åŒæŒ‡é’ˆ 167 ä¸¤æ•°ä¹‹å’Œ II ğŸŸ¡ ç›¸å‘åŒæŒ‡é’ˆ 11 ç››æœ€å¤šæ°´çš„å®¹å™¨ ğŸŸ¡ è´ªå¿ƒ+åŒæŒ‡é’ˆ 15 ä¸‰æ•°ä¹‹å’Œ ğŸŸ¡ æ’åº+åŒæŒ‡é’ˆ ğŸ¨ åŒæŒ‡é’ˆæ¨¡å¼å›¾è§£ï¼š ç›¸å‘åŒæŒ‡é’ˆ (Two Sum, Container) åŒå‘åŒæŒ‡é’ˆ (Fast-Slow) L â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¶ R Slow â”€â”€â–¶ Fast â”€â”€â–¶ â”‚ â”‚ â”‚ â”‚ â–¼ â–¼ â–¼ â–¼ [1, 2, 3, 4, 5, 6, 7] [1, 2, 3, 4, 5, 6, 7] ğŸ§  è®°å¿†å£è¯€ï¼š â€œéªŒåˆ¤ä¸¤ç››ä¸‰ï¼Œç›¸å‘åŠ åŒå‘â€ 3ï¸âƒ£ æ»‘åŠ¨çª—å£ (4é¢˜) # é¢˜ç›® éš¾åº¦ æ ¸å¿ƒæŠ€å·§ 209 é•¿åº¦æœ€å°çš„å­æ•°ç»„ ğŸŸ¡ å¯å˜çª—å£ 3 æ— é‡å¤å­—ç¬¦çš„æœ€é•¿å­ä¸² ğŸŸ¡ å“ˆå¸Œ+æ»‘çª— 30 ä¸²è”æ‰€æœ‰å•è¯çš„å­ä¸² ğŸ”´ å›ºå®šçª—å£ 76 æœ€å°è¦†ç›–å­ä¸² ğŸ”´ å¯å˜çª—å£ ğŸ¨ æ»‘åŠ¨çª—å£æ¨¡æ¿ï¼š def sliding_window(s): left = 0 window = {} # çª—å£å†…å®¹ result = 0 for right in range(len(s)): # 1. å³è¾¹ç•Œæ‰©å¼ ï¼Œæ›´æ–°çª—å£ window[s[right]] = window.get(s[right], 0) + 1 # 2. æ»¡è¶³æ”¶ç¼©æ¡ä»¶æ—¶ï¼Œå·¦è¾¹ç•Œæ”¶ç¼© while need_shrink(window): window[s[left]] -= 1 left += 1 # 3. æ›´æ–°ç»“æœ result = max(result, right - left + 1) return result ğŸ§  è®°å¿†å£è¯€ï¼š â€œå³æ‰©å·¦ç¼©æ›´æ–°æœï¼Œçª—å£æ»‘åŠ¨è§£æœ€ä¼˜â€ 4ï¸âƒ£ å“ˆå¸Œè¡¨ (9é¢˜) # é¢˜ç›® éš¾åº¦ æ ¸å¿ƒæŠ€å·§ 383 èµé‡‘ä¿¡ ğŸŸ¢ å­—ç¬¦è®¡æ•° 205 åŒæ„å­—ç¬¦ä¸² ğŸŸ¢ åŒå‘æ˜ å°„ 290 å•è¯è§„å¾‹ ğŸŸ¢ åŒå‘æ˜ å°„ 242 æœ‰æ•ˆçš„å­—æ¯å¼‚ä½è¯ ğŸŸ¢ å­—ç¬¦è®¡æ•° 49 å­—æ¯å¼‚ä½è¯åˆ†ç»„ ğŸŸ¡ æ’åºä½œkey 1 ä¸¤æ•°ä¹‹å’Œ ğŸŸ¢ è¡¥æ•°æŸ¥æ‰¾ 202 å¿«ä¹æ•° ğŸŸ¢ æ£€æµ‹å¾ªç¯ 219 å­˜åœ¨é‡å¤å…ƒç´  II ğŸŸ¢ æ»‘åŠ¨çª—å£ 128 æœ€é•¿è¿ç»­åºåˆ— ğŸŸ¡ å¹¶æŸ¥é›†æ€æƒ³ ğŸ§  è®°å¿†å£è¯€ï¼š â€œèµåŒå•æœ‰ç»„ï¼Œä¸¤å¿«å­˜æœ€é•¿â€ 5ï¸âƒ£ æ ˆ (7é¢˜) # é¢˜ç›® éš¾åº¦ æ ¸å¿ƒæŠ€å·§ 20 æœ‰æ•ˆçš„æ‹¬å· ğŸŸ¢ åŒ¹é…å‡ºæ ˆ 71 ç®€åŒ–è·¯å¾„ ğŸŸ¡ æ¨¡æ‹Ÿæ ˆ 155 æœ€å°æ ˆ ğŸŸ¡ è¾…åŠ©æ ˆ 150 é€†æ³¢å…°è¡¨è¾¾å¼æ±‚å€¼ ğŸŸ¡ æ“ä½œæ•°æ ˆ 224 åŸºæœ¬è®¡ç®—å™¨ ğŸ”´ é€’å½’/æ ˆ 227 åŸºæœ¬è®¡ç®—å™¨ II ğŸŸ¡ ä¼˜å…ˆçº§æ ˆ 772 åŸºæœ¬è®¡ç®—å™¨ III ğŸ”´ é€’å½’ ğŸ¨ æ ˆçš„æ ¸å¿ƒæ€æƒ³ï¼š åè¿›å…ˆå‡º (LIFO) â”‚ Push â–¼ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ Top â”‚ â—€â”€â”€ Pop â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â”‚ â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤ â”‚ â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 6ï¸âƒ£ é“¾è¡¨ (11é¢˜) # é¢˜ç›® éš¾åº¦ æ ¸å¿ƒæŠ€å·§ 141 ç¯å½¢é“¾è¡¨ ğŸŸ¢ å¿«æ…¢æŒ‡é’ˆ 2 ä¸¤æ•°ç›¸åŠ  ğŸŸ¡ è¿›ä½å¤„ç† 21 åˆå¹¶ä¸¤ä¸ªæœ‰åºé“¾è¡¨ ğŸŸ¢ åŒæŒ‡é’ˆ 138 éšæœºé“¾è¡¨çš„å¤åˆ¶ ğŸŸ¡ å“ˆå¸Œ/äº¤ç»‡ 92 åè½¬é“¾è¡¨ II ğŸŸ¡ å¤´æ’æ³• 25 K ä¸ªä¸€ç»„ç¿»è½¬é“¾è¡¨ ğŸ”´ åˆ†ç»„åè½¬ 19 åˆ é™¤é“¾è¡¨çš„å€’æ•°ç¬¬ N ä¸ªç»“ç‚¹ ğŸŸ¡ å¿«æ…¢æŒ‡é’ˆ 82 åˆ é™¤æ’åºé“¾è¡¨ä¸­çš„é‡å¤å…ƒç´  II ğŸŸ¡ è™šæ‹Ÿå¤´èŠ‚ç‚¹ 61 æ—‹è½¬é“¾è¡¨ ğŸŸ¡ æˆç¯æ–­å¼€ 86 åˆ†éš”é“¾è¡¨ ğŸŸ¡ åŒé“¾è¡¨ 146 LRU ç¼“å­˜ ğŸŸ¡ å“ˆå¸Œ+åŒå‘é“¾è¡¨ ğŸ¨ é“¾è¡¨åè½¬æ ¸å¿ƒä»£ç ï¼š def reverse(head): prev, curr = None, head while curr: next_temp = curr.next # 1. ä¿å­˜ä¸‹ä¸€ä¸ª curr.next = prev # 2. åè½¬æŒ‡å‘ prev = curr # 3. prevå‰è¿› curr = next_temp # 4. currå‰è¿› return prev ğŸ§  è®°å¿†å£è¯€ï¼š â€œå­˜åå‰è¿›ï¼Œå››æ­¥åè½¬â€ 7ï¸âƒ£ äºŒå‰æ ‘ (14é¢˜) # é¢˜ç›® éš¾åº¦ æ ¸å¿ƒæŠ€å·§ 104 äºŒå‰æ ‘çš„æœ€å¤§æ·±åº¦ ğŸŸ¢ DFSé€’å½’ 100 ç›¸åŒçš„æ ‘ ğŸŸ¢ é€’å½’æ¯”è¾ƒ 226 ç¿»è½¬äºŒå‰æ ‘ ğŸŸ¢ é€’å½’äº¤æ¢ 101 å¯¹ç§°äºŒå‰æ ‘ ğŸŸ¢ é•œåƒé€’å½’ 105 ä»å‰åºä¸ä¸­åºéå†åºåˆ—æ„é€ äºŒå‰æ ‘ ğŸŸ¡ åˆ†æ²» 106 ä»ä¸­åºä¸ååºéå†åºåˆ—æ„é€ äºŒå‰æ ‘ ğŸŸ¡ åˆ†æ²» 117 å¡«å……æ¯ä¸ªèŠ‚ç‚¹çš„ä¸‹ä¸€ä¸ªå³ä¾§èŠ‚ç‚¹æŒ‡é’ˆ II ğŸŸ¡ BFSå±‚åº 114 äºŒå‰æ ‘å±•å¼€ä¸ºé“¾è¡¨ ğŸŸ¡ å‰åºéå† 112 è·¯å¾„æ€»å’Œ ğŸŸ¢ DFS 129 æ±‚æ ¹èŠ‚ç‚¹åˆ°å¶èŠ‚ç‚¹æ•°å­—ä¹‹å’Œ ğŸŸ¡ DFS 124 äºŒå‰æ ‘ä¸­çš„æœ€å¤§è·¯å¾„å’Œ ğŸ”´ ååºéå† 173 äºŒå‰æœç´¢æ ‘è¿­ä»£å™¨ ğŸŸ¡ æ ˆæ¨¡æ‹Ÿ 222 å®Œå…¨äºŒå‰æ ‘çš„èŠ‚ç‚¹ä¸ªæ•° ğŸŸ¡ äºŒåˆ†+ä½è¿ç®— 236 äºŒå‰æ ‘çš„æœ€è¿‘å…¬å…±ç¥–å…ˆ ğŸŸ¡ é€’å½’ ğŸ¨ æ ‘çš„éå†æ–¹å¼ï¼š / \\ 2 3 / \\ 4 5 å‰åº(æ ¹å·¦å³): 1 â†’ 2 â†’ 4 â†’ 5 â†’ 3ä¸­åº(å·¦æ ¹å³): 4 â†’ 2 â†’ 5 â†’ 1 â†’ 3ååº(å·¦å³æ ¹): 4 â†’ 5 â†’ 2 â†’ 3 â†’ 1å±‚åº(BFS): 1 â†’ 2 â†’ 3 â†’ 4 â†’ 5 ---### 8ï¸âƒ£ å›¾ä¸æœç´¢ (8é¢˜)| # | é¢˜ç›® | éš¾åº¦ | æ ¸å¿ƒæŠ€å·§ | | 200 | å²›å±¿æ•°é‡ | ğŸŸ¡ | DFS/BFS || 130 | è¢«å›´ç»•çš„åŒºåŸŸ | ğŸŸ¡ | è¾¹ç•ŒDFS || 133 | å…‹éš†å›¾ | ğŸŸ¡ | BFS+å“ˆå¸Œ || 399 | é™¤æ³•æ±‚å€¼ | ğŸŸ¡ | å¸¦æƒå›¾BFS || 207 | è¯¾ç¨‹è¡¨ | ğŸŸ¡ | æ‹“æ‰‘æ’åº || 210 | è¯¾ç¨‹è¡¨ II | ğŸŸ¡ | æ‹“æ‰‘æ’åº || 909 | è›‡æ¢¯æ£‹ | ğŸŸ¡ | BFSæœ€çŸ­è·¯ || 433 | æœ€å°åŸºå› å˜åŒ– | ğŸŸ¡ | BFS | ğŸ¨ BFS æ¨¡æ¿ï¼š from collections import dequedef bfs(start): queue = deque([start]) visited = {start} level = 0 while queue: size = len(queue) for _ in range(size): node = queue.popleft() # å¤„ç†å½“å‰èŠ‚ç‚¹ for neighbor in get_neighbors(node): if neighbor not in visited: visited.add(neighbor) queue.append(neighbor) level += 1 return level 9ï¸âƒ£ å›æº¯ç®—æ³• (7é¢˜) # é¢˜ç›® éš¾åº¦ æ ¸å¿ƒæŠ€å·§ 17 ç”µè¯å·ç çš„å­—æ¯ç»„åˆ ğŸŸ¡ å›æº¯æšä¸¾ 77 ç»„åˆ ğŸŸ¡ å›æº¯+å‰ªæ 46 å…¨æ’åˆ— ğŸŸ¡ å›æº¯ 39 ç»„åˆæ€»å’Œ ğŸŸ¡ å›æº¯ 52 N çš‡å II ğŸ”´ å›æº¯+å‰ªæ 22 æ‹¬å·ç”Ÿæˆ ğŸŸ¡ å›æº¯ 79 å•è¯æœç´¢ ğŸŸ¡ å›æº¯+DFS ğŸ¨ å›æº¯æ¨¡æ¿ï¼š def backtrack(path, choices): if æ»¡è¶³ç»“æŸæ¡ä»¶: result.append(path[:]) return for choice in choices: # åšé€‰æ‹© path.append(choice) # é€’å½’ backtrack(path, new_choices) # æ’¤é”€é€‰æ‹© path.pop() ğŸ§  è®°å¿†å£è¯€ï¼š â€œé€‰æ‹©ã€é€’å½’ã€æ’¤é”€ï¼Œå›æº¯ä¸‰éƒ¨æ›²â€ ğŸ”Ÿ åŠ¨æ€è§„åˆ’ (22é¢˜)ä¸€ç»´ DP (10é¢˜) # é¢˜ç›® éš¾åº¦ æ ¸å¿ƒæŠ€å·§ 70 çˆ¬æ¥¼æ¢¯ ğŸŸ¢ æ–æ³¢é‚£å¥‘ 198 æ‰“å®¶åŠ«èˆ ğŸŸ¡ çŠ¶æ€è½¬ç§» 139 å•è¯æ‹†åˆ† ğŸŸ¡ èƒŒåŒ…å˜å½¢ 322 é›¶é’±å…‘æ¢ ğŸŸ¡ å®Œå…¨èƒŒåŒ… 300 æœ€é•¿é€’å¢å­åºåˆ— ğŸŸ¡ LIS 152 ä¹˜ç§¯æœ€å¤§å­æ•°ç»„ ğŸŸ¡ ç»´æŠ¤æœ€å¤§æœ€å° å¤šç»´ DP (12é¢˜) # é¢˜ç›® éš¾åº¦ æ ¸å¿ƒæŠ€å·§ 120 ä¸‰è§’å½¢æœ€å°è·¯å¾„å’Œ ğŸŸ¡ è‡ªåº•å‘ä¸Š 64 æœ€å°è·¯å¾„å’Œ ğŸŸ¡ ç½‘æ ¼DP 63 ä¸åŒè·¯å¾„ II ğŸŸ¡ ç½‘æ ¼DP 5 æœ€é•¿å›æ–‡å­ä¸² ğŸŸ¡ åŒºé—´DP 72 ç¼–è¾‘è·ç¦» ğŸŸ¡ ç»å…¸DP 97 äº¤é”™å­—ç¬¦ä¸² ğŸŸ¡ äºŒç»´DP 123 ä¹°å–è‚¡ç¥¨çš„æœ€ä½³æ—¶æœº III ğŸ”´ çŠ¶æ€æœºDP ğŸ¨ DP æ€è€ƒæ¡†æ¶ï¼š 1. å®šä¹‰çŠ¶æ€ â”€â”€â–¶ dp[i] è¡¨ç¤ºä»€ä¹ˆï¼Ÿ â”‚ â–¼2. çŠ¶æ€è½¬ç§» â”€â”€â–¶ dp[i] å’Œ dp[i-1] çš„å…³ç³»ï¼Ÿ â”‚ â–¼3. åˆå§‹æ¡ä»¶ â”€â”€â–¶ dp[0] = ? â”‚ â–¼4. è®¡ç®—é¡ºåº â”€â”€â–¶ ä»å°åˆ°å¤§ or ä»å¤§åˆ°å°ï¼Ÿ â”‚ â–¼5. è¿”å›ç»“æœ â”€â”€â–¶ dp[n] or max(dp)? ğŸ§  è®°å¿†å£è¯€ï¼š â€œå®šçŠ¶æ€ã€æ‰¾è½¬ç§»ã€è®¾åˆå€¼ã€å®šé¡ºåºã€å–ç»“æœâ€ ğŸ† åˆ·é¢˜è¿›åº¦è¿½è¸ª åˆ†ç±» æ€»é¢˜æ•° å·²å®Œæˆ è¿›åº¦ æ•°ç»„/å­—ç¬¦ä¸² 15 â¬œ 0% åŒæŒ‡é’ˆ 5 â¬œ 0% æ»‘åŠ¨çª—å£ 4 â¬œ 0% å“ˆå¸Œè¡¨ 9 â¬œ 0% æ ˆ 7 â¬œ 0% é“¾è¡¨ 11 â¬œ 0% äºŒå‰æ ‘ 14 â¬œ 0% å›¾/æœç´¢ 8 â¬œ 0% å›æº¯ 7 â¬œ 0% åŠ¨æ€è§„åˆ’ 22 â¬œ 0% å…¶ä»– 48 â¬œ 0% æ€»è®¡ 150 0 0% ğŸ“š æ¨èå­¦ä¹ èµ„æº LeetCode å®˜æ–¹é¢˜å• - åŸé¢˜é“¾æ¥ ä»£ç éšæƒ³å½• - è¯¦ç»†å›¾è§£ labuladong çš„ç®—æ³•å°æŠ„ - æ¡†æ¶æ€ç»´ ğŸ¯ å¤ä¹ è®¡åˆ’æ¯æ—¥ä»»åŠ¡ï¼šâ”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ ğŸ“… å·¥ä½œæ—¥ï¼š2-3 é¢˜ (1å°æ—¶) â”‚â”‚ ğŸ“… å‘¨æœ«ï¼šå¤ä¹ æœ¬å‘¨é¢˜ç›® + 5é¢˜æ–°é¢˜ â”‚â”‚ ğŸ“… æœˆæœ«ï¼šåˆ†ç±»ä¸“é¡¹çªç ´ + æ¨¡æ‹Ÿé¢è¯• â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ ğŸ’ª åŠ æ²¹ï¼åšæŒåˆ·å®Œè¿™ 150 é¢˜ï¼Œå¤§å‚ Offer åœ¨å‘ä½ æ‹›æ‰‹ï¼","tags":["LeetCode"],"categories":["ç®—æ³•"]},{"title":"ğŸ“¦ LeetCode 150 - æ ˆä¸é˜Ÿåˆ—ä¸“é¢˜","path":"/2026/01/18/leetcode-150-stack/","content":"ğŸ“¦ æ ˆä¸é˜Ÿåˆ—ä¸“é¢˜ (7é¢˜) ğŸ¯ æ ¸å¿ƒç‰¹æ€§ï¼šæ ˆ LIFOï¼ˆåè¿›å…ˆå‡ºï¼‰ï¼Œé˜Ÿåˆ— FIFOï¼ˆå…ˆè¿›å…ˆå‡ºï¼‰ ğŸ—ºï¸ æ ˆçš„æ ¸å¿ƒåº”ç”¨åœºæ™¯â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ æ ˆçš„åº”ç”¨åœºæ™¯ â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ â”‚â”‚ 1. æ‹¬å·åŒ¹é… â”‚â”‚ é‡åˆ°å·¦æ‹¬å·å…¥æ ˆï¼Œé‡åˆ°å³æ‹¬å·å‡ºæ ˆåŒ¹é… â”‚â”‚ â”‚â”‚ 2. è¡¨è¾¾å¼æ±‚å€¼ â”‚â”‚ æ“ä½œæ•°æ ˆ + è¿ç®—ç¬¦æ ˆ â”‚â”‚ â”‚â”‚ 3. å•è°ƒæ ˆ â”‚â”‚ æ‰¾ä¸‹ä¸€ä¸ªæ›´å¤§/æ›´å°å…ƒç´  â”‚â”‚ â”‚â”‚ 4. è·¯å¾„ç®€åŒ– â”‚â”‚ å¤„ç† \".\" å’Œ \"..\" â”‚â”‚ â”‚â”‚ â”Œâ”€â”€â”€â” â”‚â”‚ â”‚ C â”‚ â† Top (åè¿›å…ˆå‡º) â”‚â”‚ â”œâ”€â”€â”€â”¤ â”‚â”‚ â”‚ B â”‚ â”‚â”‚ â”œâ”€â”€â”€â”¤ â”‚â”‚ â”‚ A â”‚ â”‚â”‚ â””â”€â”€â”€â”˜ â”‚â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 1ï¸âƒ£ LC 20. æœ‰æ•ˆçš„æ‹¬å· ğŸŸ¢é¢˜ç›®æè¿°åˆ¤æ–­æ‹¬å·å­—ç¬¦ä¸²æ˜¯å¦æœ‰æ•ˆã€‚ ğŸ¨ å›¾è§£æ€è·¯s = \"([{}])\"å¤„ç†è¿‡ç¨‹:å­—ç¬¦ æ“ä½œ æ ˆçŠ¶æ€( å…¥æ ˆ [(][ å…¥æ ˆ [(, []{ å…¥æ ˆ [(, [, {]} å‡ºæ ˆåŒ¹é… [(, [] âœ“ { åŒ¹é…] å‡ºæ ˆåŒ¹é… [(] âœ“ [ åŒ¹é…) å‡ºæ ˆåŒ¹é… [] âœ“ ( åŒ¹é…æ ˆä¸ºç©º â†’ æœ‰æ•ˆï¼ ğŸ’» ä»£ç å®ç°def isValid(s: str) - bool: stack = [] mapping = {')': '(', ']': '[', '}': '{'} for char in s: if char in mapping: # å³æ‹¬å·ï¼šå‡ºæ ˆåŒ¹é… if not stack or stack.pop() != mapping[char]: return False else: # å·¦æ‹¬å·ï¼šå…¥æ ˆ stack.append(char) return len(stack) == 0 ğŸ§  è®°å¿†å£è¯€ â€œå·¦å…¥å³å‡ºï¼Œç©ºæ ˆæœ‰æ•ˆâ€ 2ï¸âƒ£ LC 71. ç®€åŒ–è·¯å¾„ ğŸŸ¡é¢˜ç›®æè¿°ç®€åŒ– Unix é£æ ¼çš„ç»å¯¹è·¯å¾„ã€‚ ğŸ¨ å›¾è§£æ€è·¯path = \"/a/./b/../../c/\"å¤„ç†è§„åˆ™:. â†’ å½“å‰ç›®å½•ï¼Œå¿½ç•¥.. â†’ ä¸Šçº§ç›®å½•ï¼Œå‡ºæ ˆå…¶ä»– â†’ ç›®å½•åï¼Œå…¥æ ˆå¤„ç†è¿‡ç¨‹:a â†’ å…¥æ ˆ â†’ [a]. â†’ å¿½ç•¥ â†’ [a]b â†’ å…¥æ ˆ â†’ [a, b].. â†’ å‡ºæ ˆ â†’ [a].. â†’ å‡ºæ ˆ â†’ []c â†’ å…¥æ ˆ â†’ [c]ç»“æœ: \"/c\" ğŸ’» ä»£ç å®ç°def simplifyPath(path: str) - str: stack = [] for part in path.split('/'): if part == '..': if stack: stack.pop() elif part and part != '.': stack.append(part) return '/' + '/'.join(stack) ğŸ§  è®°å¿†å£è¯€ â€œç‚¹å¿½ç•¥ï¼ŒåŒç‚¹å‡ºæ ˆï¼Œå…¶ä»–å…¥æ ˆâ€ 3ï¸âƒ£ LC 155. æœ€å°æ ˆ ğŸŸ¡é¢˜ç›®æè¿°è®¾è®¡ä¸€ä¸ªæ”¯æŒ O(1) è·å–æœ€å°å€¼çš„æ ˆã€‚ ğŸ¨ å›¾è§£æ€è·¯ä½¿ç”¨è¾…åŠ©æ ˆåŒæ­¥è®°å½•å½“å‰æœ€å°å€¼ä¸»æ ˆ è¾…åŠ©æ ˆï¼ˆè®°å½•å½“å‰æœ€å°ï¼‰push(5) [5] [5]push(3) [5,3] [5,3] â† 3æ›´å°push(7) [5,3,7] [5,3,3] â† æœ€å°ä»æ˜¯3pop() [5,3] [5,3]getMin() è¿”å› 3 ğŸ’» ä»£ç å®ç°class MinStack: def __init__(self): self.stack = [] self.min_stack = [] def push(self, val: int) - None: self.stack.append(val) # è¾…åŠ©æ ˆï¼šè®°å½•å½“å‰æœ€å°å€¼ if not self.min_stack or val = self.min_stack[-1]: self.min_stack.append(val) else: self.min_stack.append(self.min_stack[-1]) def pop(self) - None: self.stack.pop() self.min_stack.pop() def top(self) - int: return self.stack[-1] def getMin(self) - int: return self.min_stack[-1] ğŸ§  è®°å¿†å£è¯€ â€œè¾…åŠ©æ ˆåŒæ­¥è®°å½•æœ€å°å€¼â€ 4ï¸âƒ£ LC 150. é€†æ³¢å…°è¡¨è¾¾å¼æ±‚å€¼ ğŸŸ¡é¢˜ç›®æè¿°è®¡ç®—é€†æ³¢å…°è¡¨è¾¾å¼ï¼ˆåç¼€è¡¨è¾¾å¼ï¼‰çš„å€¼ã€‚ ğŸ¨ å›¾è§£æ€è·¯tokens = [\"2\",\"1\",\"+\",\"3\",\"*\"]ç­‰ä»·äº: (2 + 1) * 3 = 9å¤„ç†è¿‡ç¨‹ï¼ˆé‡åˆ°è¿ç®—ç¬¦å¼¹å‡ºä¸¤ä¸ªæ“ä½œæ•°ï¼‰:2 â†’ å…¥æ ˆ â†’ [2]1 â†’ å…¥æ ˆ â†’ [2, 1]+ â†’ å¼¹å‡º1,2ï¼Œè®¡ç®—2+1=3 â†’ [3]3 â†’ å…¥æ ˆ â†’ [3, 3]* â†’ å¼¹å‡º3,3ï¼Œè®¡ç®—3*3=9 â†’ [9]ç»“æœ: 9 ğŸ’» ä»£ç å®ç°def evalRPN(tokens: list) - int: stack = [] operators = {'+', '-', '*', '/'} for token in tokens: if token in operators: b, a = stack.pop(), stack.pop() if token == '+': stack.append(a + b) elif token == '-': stack.append(a - b) elif token == '*': stack.append(a * b) else: # æ³¨æ„ï¼šPythoné™¤æ³•å‘é›¶å–æ•´ stack.append(int(a / b)) else: stack.append(int(token)) return stack[0] ğŸ§  è®°å¿†å£è¯€ â€œæ•°å­—å…¥æ ˆï¼Œè¿ç®—ç¬¦å¼¹ä¸¤ä¸ªç®—â€ 5ï¸âƒ£ LC 224. åŸºæœ¬è®¡ç®—å™¨ ğŸ”´é¢˜ç›®æè¿°å®ç°ä¸€ä¸ªåŸºæœ¬çš„è®¡ç®—å™¨ï¼ˆæ”¯æŒ +ã€-ã€æ‹¬å·ï¼‰ã€‚ ğŸ¨ å›¾è§£æ€è·¯s = \"1 + (2 - 3)\"ä½¿ç”¨æ ˆä¿å­˜æ‹¬å·å¤–çš„çŠ¶æ€é‡åˆ° ( â†’ ä¿å­˜å½“å‰ result å’Œ signï¼Œé‡ç½®é‡åˆ° ) â†’ æ¢å¤ä¹‹å‰çš„çŠ¶æ€å¹¶ç´¯åŠ å¤„ç†: 1 + (2 - 3)1 â†’ result = 1+ â†’ sign = 1( â†’ ä¿å­˜(1, 1)ï¼Œé‡ç½® result=02 â†’ result = 2- â†’ sign = -13 â†’ result = 2 + (-1)*3 = -1) â†’ result = 1 + 1*(-1) = 0 ğŸ’» ä»£ç å®ç°def calculate(s: str) - int: stack = [] result = 0 num = 0 sign = 1 for char in s: if char.isdigit(): num = num * 10 + int(char) elif char == '+': result += sign * num num = 0 sign = 1 elif char == '-': result += sign * num num = 0 sign = -1 elif char == '(': # ä¿å­˜å½“å‰çŠ¶æ€ stack.append(result) stack.append(sign) result = 0 sign = 1 elif char == ')': result += sign * num num = 0 # æ¢å¤çŠ¶æ€ result = result * stack.pop() + stack.pop() return result + sign * num ğŸ§  è®°å¿†å£è¯€ â€œæ‹¬å·ä¿å­˜çŠ¶æ€ï¼Œå‡ºæ¥æ¢å¤ç´¯åŠ â€ 6ï¸âƒ£ LC 227. åŸºæœ¬è®¡ç®—å™¨ II ğŸŸ¡é¢˜ç›®æè¿°å®ç°è®¡ç®—å™¨ï¼ˆæ”¯æŒ +ã€-ã€*ã€/ï¼Œæ— æ‹¬å·ï¼‰ã€‚ ğŸ¨ å›¾è§£æ€è·¯s = \"3+2*2\"* / ä¼˜å…ˆçº§é«˜äº + -ç­–ç•¥ï¼šç”¨æ ˆä¿å­˜å¾…åŠ çš„æ•°é‡åˆ° + å…¥æ ˆæ­£æ•°é‡åˆ° - å…¥æ ˆè´Ÿæ•°é‡åˆ° * / å¼¹å‡ºæ ˆé¡¶è®¡ç®—åå…¥æ ˆå¤„ç†: 3 + 2 * 23 â†’ æ ˆ [3]+ â†’ è®°å½• op = +2 â†’ æ ˆ [3, 2]* â†’ è®°å½• op = *2 â†’ å¼¹å‡º2ï¼Œè®¡ç®—2*2=4ï¼Œæ ˆ [3, 4]ç»“æœ: sum([3, 4]) = 7 ğŸ’» ä»£ç å®ç°def calculate(s: str) - int: stack = [] num = 0 op = '+' for i, char in enumerate(s): if char.isdigit(): num = num * 10 + int(char) if char in '+-*/' or i == len(s) - 1: if op == '+': stack.append(num) elif op == '-': stack.append(-num) elif op == '*': stack.append(stack.pop() * num) elif op == '/': stack.append(int(stack.pop() / num)) op = char num = 0 return sum(stack) ğŸ§  è®°å¿†å£è¯€ â€œåŠ å‡å…¥æ ˆï¼Œä¹˜é™¤å…ˆç®—â€ 7ï¸âƒ£ LC 772. åŸºæœ¬è®¡ç®—å™¨ III ğŸ”´é¢˜ç›®æè¿°å®ç°å®Œæ•´è®¡ç®—å™¨ï¼ˆæ”¯æŒ +ã€-ã€*ã€/ã€æ‹¬å·ï¼‰ã€‚ ğŸ¨ å›¾è§£æ€è·¯ç»“åˆ 224 å’Œ 227 çš„æ€è·¯æ–¹æ³•1ï¼šé€’å½’å¤„ç†æ‹¬å·æ–¹æ³•2ï¼šåŒæ ˆï¼ˆæ“ä½œæ•°æ ˆ + è¿ç®—ç¬¦æ ˆï¼‰ ğŸ’» ä»£ç å®ç°ï¼ˆé€’å½’ï¼‰def calculate(s: str) - int: def helper(s, start): stack = [] num = 0 op = '+' i = start while i len(s): char = s[i] if char.isdigit(): num = num * 10 + int(char) if char == '(': num, i = helper(s, i + 1) if char in '+-*/)' or i == len(s) - 1: if op == '+': stack.append(num) elif op == '-': stack.append(-num) elif op == '*': stack.append(stack.pop() * num) elif op == '/': stack.append(int(stack.pop() / num)) if char == ')': return sum(stack), i op = char num = 0 i += 1 return sum(stack), i return helper(s, 0)[0] ğŸ§  è®°å¿†å£è¯€ â€œé‡æ‹¬å·é€’å½’ï¼Œå…¶ä»–åŒ227â€ ğŸ“Š æœ¬ç« æ€»ç»“æ ˆçš„åº”ç”¨æ¨¡å¼ æ¨¡å¼ æ ¸å¿ƒæ€æƒ³ å…¸å‹é¢˜ç›® æ‹¬å·åŒ¹é… å·¦å…¥å³å‡º 20 è·¯å¾„å¤„ç† ç›®å½•å…¥æ ˆï¼Œ..å‡ºæ ˆ 71 è¾…åŠ©æ ˆ åŒæ­¥ç»´æŠ¤é¢å¤–ä¿¡æ¯ 155 è¡¨è¾¾å¼æ±‚å€¼ æ“ä½œæ•°æ ˆ + è¿ç®—ç¬¦å¤„ç† 150, 224, 227 å•è°ƒæ ˆæ¨¡æ¿ï¼ˆè¡¥å……ï¼‰def monotonic_stack(nums): \"\"\"æ‰¾æ¯ä¸ªå…ƒç´ å³è¾¹ç¬¬ä¸€ä¸ªæ›´å¤§çš„å…ƒç´ \"\"\" stack = [] # å­˜ç´¢å¼• result = [-1] * len(nums) for i in range(len(nums)): while stack and nums[i] nums[stack[-1]]: idx = stack.pop() result[idx] = nums[i] stack.append(i) return result ğŸ§  å…¨ç« è®°å¿†å£è¯€æ‹¬å·è·¯å¾„æœ€å°æ ˆé€†æ³¢å…°åè®¡ç®—å™¨ä¸‰é“è®¡ç®—å™¨å‡çº§æ ˆçš„ä¸ƒé¢˜è®°å¿ƒé‡Œæ‹¬å· - æœ‰æ•ˆçš„æ‹¬å· (20)è·¯å¾„ - ç®€åŒ–è·¯å¾„ (71)æœ€å°æ ˆ - æœ€å°æ ˆ (155)é€†æ³¢å…° - é€†æ³¢å…°è¡¨è¾¾å¼æ±‚å€¼ (150)è®¡ç®—å™¨ - åŸºæœ¬è®¡ç®—å™¨ç³»åˆ— (224, 227, 772) ğŸ“– ä¸‹ä¸€ç¯‡ï¼šå †/ä¼˜å…ˆé˜Ÿåˆ—ä¸“é¢˜","tags":["LeetCode"],"categories":["ç®—æ³•"]},{"title":"ğŸ‘† LeetCode 150 - åŒæŒ‡é’ˆä¸“é¢˜","path":"/2026/01/18/leetcode-150-two-pointers/","content":"ğŸ‘† åŒæŒ‡é’ˆä¸“é¢˜ (5é¢˜) ğŸ¯ æ ¸å¿ƒæŠ€å·§ï¼šç›¸å‘åŒæŒ‡é’ˆã€åŒå‘åŒæŒ‡é’ˆã€å¿«æ…¢æŒ‡é’ˆ ğŸ—ºï¸ åŒæŒ‡é’ˆä¸‰å¤§æ¨¡å¼â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”â”‚ åŒæŒ‡é’ˆæ¨¡å¼ â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ ç›¸å‘åŒæŒ‡é’ˆ â”‚ åŒå‘åŒæŒ‡é’ˆ â”‚ èƒŒå‘åŒæŒ‡é’ˆ â”‚â”‚ (å¯¹æ’æŒ‡é’ˆ) â”‚ (å¿«æ…¢æŒ‡é’ˆ) â”‚ (ä¸­å¿ƒæ‰©å±•) â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ L â”€â”€â”€â”€â”€â”€â–¶ â—€â”€â”€ R â”‚ S â”€â”€â–¶ F â”€â”€â–¶ â”‚ â—€â”€â”€ C â”€â”€â–¶ â”‚â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤â”‚ â€¢ ä¸¤æ•°ä¹‹å’Œ â”‚ â€¢ ç§»é™¤å…ƒç´  â”‚ â€¢ æœ€é•¿å›æ–‡å­ä¸² â”‚â”‚ â€¢ ç››æ°´å®¹å™¨ â”‚ â€¢ åˆ é™¤é‡å¤ â”‚ â€¢ å›æ–‡åˆ¤æ–­ â”‚â”‚ â€¢ ä¸‰æ•°ä¹‹å’Œ â”‚ â€¢ é“¾è¡¨å¿«æ…¢ â”‚ â”‚â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ 1ï¸âƒ£ LC 125. éªŒè¯å›æ–‡ä¸² ğŸŸ¢é¢˜ç›®æè¿°åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦ä¸ºå›æ–‡ä¸²ï¼ˆåªè€ƒè™‘å­—æ¯å’Œæ•°å­—ï¼Œå¿½ç•¥å¤§å°å†™ï¼‰ã€‚ ğŸ¨ å›¾è§£æ€è·¯ç›¸å‘åŒæŒ‡é’ˆï¼Œè·³è¿‡éå­—æ¯æ•°å­—å­—ç¬¦s = \"A man, a plan, a canal: Panama\" â†‘ â†‘ L RStep 1: 'A' vs 'a' â†’ ç›¸ç­‰ï¼ŒL++, R--Step 2: ' ' è·³è¿‡ï¼ŒL++Step 3: 'm' vs 'm' â†’ ç›¸ç­‰...æœ€ç»ˆ L = Rï¼Œæ˜¯å›æ–‡ä¸² âœ“ ğŸ’» ä»£ç å®ç°def isPalindrome(s: str) - bool: left, right = 0, len(s) - 1 while left right: # è·³è¿‡éå­—æ¯æ•°å­— while left right and not s[left].isalnum(): left += 1 while left right and not s[right].isalnum(): right -= 1 # æ¯”è¾ƒï¼ˆå¿½ç•¥å¤§å°å†™ï¼‰ if s[left].lower() != s[right].lower(): return False left += 1 right -= 1 return True ğŸ§  è®°å¿†å£è¯€ â€œä¸¤å¤´å¾€ä¸­èµ°ï¼Œä¸åŒå°±è¯´Noâ€ 2ï¸âƒ£ LC 392. åˆ¤æ–­å­åºåˆ— ğŸŸ¢é¢˜ç›®æè¿°åˆ¤æ–­ s æ˜¯å¦ä¸º t çš„å­åºåˆ—ã€‚ ğŸ¨ å›¾è§£æ€è·¯åŒå‘åŒæŒ‡é’ˆï¼ši éå† sï¼Œj éå† ts = \"ace\", t = \"abcde\" â†‘ â†‘ i jj=0: t[0]='a' = s[0], i++, j++j=1: t[1]='b' â‰  s[1]='c', j++j=2: t[2]='c' = s[1], i++, j++j=3: t[3]='d' â‰  s[2]='e', j++j=4: t[4]='e' = s[2], i++, j++i = 3 = len(s) âœ“ æ˜¯å­åºåˆ— ğŸ’» ä»£ç å®ç°def isSubsequence(s: str, t: str) - bool: i, j = 0, 0 while i len(s) and j len(t): if s[i] == t[j]: i += 1 j += 1 return i == len(s) ğŸ§  è®°å¿†å£è¯€ â€œs æŒ‡é’ˆåªåœ¨åŒ¹é…æ—¶å‰è¿›â€ 3ï¸âƒ£ LC 167. ä¸¤æ•°ä¹‹å’Œ II - è¾“å…¥æœ‰åºæ•°ç»„ ğŸŸ¡é¢˜ç›®æè¿°åœ¨æœ‰åºæ•°ç»„ä¸­æ‰¾ä¸¤ä¸ªæ•°ï¼Œä½¿å®ƒä»¬çš„å’Œç­‰äºç›®æ ‡å€¼ã€‚ ğŸ¨ å›¾è§£æ€è·¯æœ‰åºæ•°ç»„ + ä¸¤æ•°ä¹‹å’Œ = ç›¸å‘åŒæŒ‡é’ˆï¼numbers = [2, 7, 11, 15], target = 9 â†‘ â†‘ L Rsum = 2 + 15 = 17 9 â†’ R-- (å’Œå¤ªå¤§ï¼Œå‡å°å³è¾¹)sum = 2 + 11 = 13 9 â†’ R--sum = 2 + 7 = 9 = target âœ“è¿”å› [1, 2] (1-indexed) ğŸ’» ä»£ç å®ç°def twoSum(numbers: list, target: int) - list: left, right = 0, len(numbers) - 1 while left right: total = numbers[left] + numbers[right] if total == target: return [left + 1, right + 1] # 1-indexed elif total target: left += 1 # å’Œå¤ªå°ï¼Œå¢å¤§å·¦è¾¹ else: right -= 1 # å’Œå¤ªå¤§ï¼Œå‡å°å³è¾¹ return [] ğŸ§  è®°å¿†å£è¯€ â€œå°äº†å·¦ç§»ï¼Œå¤§äº†å³ç§»â€ 4ï¸âƒ£ LC 11. ç››æœ€å¤šæ°´çš„å®¹å™¨ ğŸŸ¡é¢˜ç›®æè¿°æ‰¾ä¸¤æ¡çº¿ï¼Œä½¿æ„æˆçš„å®¹å™¨ç››æ°´æœ€å¤šã€‚ ğŸ¨ å›¾è§£æ€è·¯é¢ç§¯ = min(å·¦é«˜, å³é«˜) Ã— å®½åº¦ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ â”‚ [1, 8, 6, 2, 5, 4, 8, 3, 7] â†‘ â†‘ L Rå…³é”®æ´å¯Ÿï¼šç§»åŠ¨è¾ƒçŸ®çš„é‚£è¾¹ï¼ä¸ºä»€ä¹ˆï¼Ÿå› ä¸ºå®½åº¦å¿…ç„¶å‡å°ï¼Œåªæœ‰é«˜åº¦å¢åŠ æ‰å¯èƒ½é¢ç§¯å¢å¤§è€Œé«˜åº¦ç”±çŸ®çš„å†³å®šï¼Œæ‰€ä»¥ç§»åŠ¨çŸ®çš„æ‰æœ‰å¯èƒ½æ‰¾åˆ°æ›´é«˜çš„Step 1: height[L]=1 height[R]=7 area = 1 Ã— 8 = 8 ç§»åŠ¨ L (å› ä¸ºå·¦è¾¹çŸ®)Step 2: height[L]=8 height[R]=7 area = 7 Ã— 7 = 49 ç§»åŠ¨ R... æœ€å¤§é¢ç§¯ = 49 ğŸ’» ä»£ç å®ç°def maxArea(height: list) - int: left, right = 0, len(height) - 1 max_area = 0 while left right: # è®¡ç®—å½“å‰é¢ç§¯ width = right - left h = min(height[left], height[right]) max_area = max(max_area, width * h) # ç§»åŠ¨è¾ƒçŸ®çš„ä¸€è¾¹ if height[left] height[right]: left += 1 else: right -= 1 return max_area ğŸ§  è®°å¿†å£è¯€ â€œçŸ®çš„å…ˆèµ°ï¼Œé«˜çš„ç­‰ç€â€ 5ï¸âƒ£ LC 15. ä¸‰æ•°ä¹‹å’Œ ğŸŸ¡é¢˜ç›®æè¿°æ‰¾å‡ºæ‰€æœ‰å’Œä¸º 0 çš„ä¸‰å…ƒç»„ï¼ˆä¸èƒ½é‡å¤ï¼‰ã€‚ ğŸ¨ å›¾è§£æ€è·¯æ’åº + å›ºå®šä¸€ä¸ªæ•° + åŒæŒ‡é’ˆæ‰¾å¦å¤–ä¸¤ä¸ªnums = [-4, -1, -1, 0, 1, 2] â†‘ i (å›ºå®š) â†‘ â†‘ L Rtarget = -nums[i] = 4æ‰¾ L + R = 4: -1 + 2 = 1 4 â†’ L++ -1 + 2 = 1 4 â†’ L++ 0 + 2 = 2 4 â†’ L++ 1 + 2 = 3 4 â†’ L++ L = Rï¼Œç»“æŸi++ ç»§ç»­...å»é‡æŠ€å·§ï¼š1. nums[i] == nums[i-1] æ—¶è·³è¿‡2. æ‰¾åˆ°è§£åï¼ŒL++/R-- è·³è¿‡é‡å¤å€¼ ğŸ’» ä»£ç å®ç°def threeSum(nums: list) - list: nums.sort() result = [] n = len(nums) for i in range(n - 2): # å»é‡ï¼šè·³è¿‡é‡å¤çš„ç¬¬ä¸€ä¸ªæ•° if i 0 and nums[i] == nums[i - 1]: continue # åŒæŒ‡é’ˆæ‰¾å¦å¤–ä¸¤ä¸ªæ•° left, right = i + 1, n - 1 target = -nums[i] while left right: total = nums[left] + nums[right] if total == target: result.append([nums[i], nums[left], nums[right]]) # å»é‡ï¼šè·³è¿‡é‡å¤çš„ç¬¬äºŒã€ä¸‰ä¸ªæ•° while left right and nums[left] == nums[left + 1]: left += 1 while left right and nums[right] == nums[right - 1]: right -= 1 left += 1 right -= 1 elif total target: left += 1 else: right -= 1 return result ğŸ§  è®°å¿†å£è¯€ â€œæ’åºå›ºå®šä¸€ï¼ŒåŒæŒ‡é’ˆæ‰¾äºŒä¸‰ï¼Œé‡é‡å¤å°±è·³è¿‡â€ ğŸ“Š æœ¬ç« æ€»ç»“åŒæŒ‡é’ˆæ¨¡æ¿é€ŸæŸ¥# æ¨¡æ¿1: ç›¸å‘åŒæŒ‡é’ˆdef two_pointer_opposite(arr): left, right = 0, len(arr) - 1 while left right: if condition: left += 1 else: right -= 1# æ¨¡æ¿2: åŒå‘åŒæŒ‡é’ˆdef two_pointer_same(arr): slow = 0 for fast in range(len(arr)): if condition: arr[slow] = arr[fast] slow += 1 return slow# æ¨¡æ¿3: èƒŒå‘åŒæŒ‡é’ˆï¼ˆä¸­å¿ƒæ‰©å±•ï¼‰def expand_from_center(s, left, right): while left = 0 and right len(s) and s[left] == s[right]: left -= 1 right += 1 return s[left+1:right] é¢˜ç›®æ¨¡å¼è¯†åˆ« ç‰¹å¾ ä½¿ç”¨æ¨¡å¼ å…¸å‹é¢˜ç›® æœ‰åºæ•°ç»„æ±‚å’Œ ç›¸å‘åŒæŒ‡é’ˆ 167, 15 å›æ–‡åˆ¤æ–­ ç›¸å‘åŒæŒ‡é’ˆ 125 æœ€å¤§/æœ€å°å®¹å™¨ ç›¸å‘åŒæŒ‡é’ˆ 11 å­åºåˆ—åŒ¹é… åŒå‘åŒæŒ‡é’ˆ 392 åŸåœ°åˆ é™¤å…ƒç´  åŒå‘åŒæŒ‡é’ˆ 26, 27 å›æ–‡å­ä¸² èƒŒå‘åŒæŒ‡é’ˆ 5, 647 ğŸ§  å…¨ç« è®°å¿†å£è¯€éªŒåˆ¤ä¸¤ç››ä¸‰ï¼ŒåŒæŒ‡é’ˆäº”å…³éªŒ - éªŒè¯å›æ–‡ä¸² (125)åˆ¤ - åˆ¤æ–­å­åºåˆ— (392)ä¸¤ - ä¸¤æ•°ä¹‹å’Œ II (167)ç›› - ç››æœ€å¤šæ°´çš„å®¹å™¨ (11)ä¸‰ - ä¸‰æ•°ä¹‹å’Œ (15) ğŸ“– ä¸‹ä¸€ç¯‡ï¼šæ»‘åŠ¨çª—å£ä¸“é¢˜","tags":["LeetCode"],"categories":["ç®—æ³•"]},{"title":"mmdetection","path":"/2023/03/17/mmdetection/","content":"","tags":["ç›®æ ‡æ£€æµ‹"],"categories":["æŠ€æœ¯"]},{"title":"ä¸€ç§åŸºäºå•åº”æ€§çš„å¤šç›¸æœºè¡Œäººè·Ÿè¸ªç®—æ³•","path":"/2023/01/26/ä¸€ç§åŸºäºå•åº”æ€§çš„å¤šç›¸æœºè¡Œäººè·Ÿè¸ªç®—æ³•/","content":"ä¸€ç§åŸºäºå•åº”æ€§çš„å¤šç›¸æœºè¡Œäººè·Ÿè¸ªç®—æ³•1. æ‘˜è¦åœ¨ä¸€ä¸ªåŒºåŸŸå‘¨å›´å®‰è£…å¤šä¸ªå»‰ä»·çš„è§†é¢‘ç›‘æ§æ‘„åƒå¤´æ˜¯å¾ˆå®¹æ˜“çš„ã€‚ç„¶è€Œï¼Œå¤šæ‘„åƒå¤´è·Ÿè¸ªä»ç„¶æ˜¯ä¸€ä¸ªå‘å±•ä¸­çš„é¢†åŸŸã€‚å¯ä»¥ä½¿ç”¨å¤šä¸ªæ‘„åƒæœºç”Ÿäº§çš„ç›‘æ§äº§å“åŒ…æ‹¬æ‘„åƒæœºæç¤ºã€å¹¿åŸŸäº¤é€šåˆ†æã€å­˜åœ¨é®æŒ¡çš„è·Ÿè¸ªä»¥åŠç°åœºå…¥å£è·Ÿè¸ªã€‚ æ‰€æœ‰è¿™äº›äº§å“éƒ½éœ€è¦è§£å†³ä¸€è‡´çš„æ ‡ç­¾é—®é¢˜ã€‚è¿™æ„å‘³ç€ç»™å„ç§æ‘„åƒæœºä¸­çœŸå®ä¸–ç•Œç›®æ ‡çš„æ‰€æœ‰æŠ•å½±æä¾›ç›¸åŒçš„å…ƒç›®æ ‡è·Ÿè¸ªæ ‡ç­¾ã€‚ æœ¬æ–‡ä»‹ç»äº†ä¸€ç§å¤šæ‘„åƒå¤´äººç¾¤è·Ÿè¸ªç®—æ³•çš„å®ç°ä¸æµ‹è¯•ã€‚é¦–å…ˆï¼Œéƒ¨åˆ†é‡æ–°å®ç°äº†å½¢çŠ¶åŒ¹é…çš„å•æ‘„åƒæœºè·Ÿè¸ªç®—æ³•ï¼Œä»¥ä¾¿åœ¨æµ‹è¯•è§†é¢‘ä¸­å·¥ä½œã€‚å•æ‘„åƒå¤´è·Ÿè¸ªå™¨çš„è¾“å‡ºæ˜¯å¤šæ‘„åƒå¤´è·Ÿè¸ªå™¨çš„è¾“å…¥ã€‚è¯¥ç®—æ³•æ‰¾åˆ°æ¯ä¸ªç›®æ ‡çš„è„šç‰¹å¾:å¯¹åº”äºç›®æ ‡æ­£ä¸‹æ–¹åœ°é¢å¹³é¢ä¸Šçš„ä¸€ä¸ªç‚¹çš„åƒç´ ã€‚è§†é‡çº¿è¢«å‘ç°å¹¶ç”¨äºåˆ›å»ºåˆå§‹å…ƒç›®æ ‡å…³è”ã€‚ç„¶åï¼Œå…ƒç›®æ ‡åœ¨ç§»åŠ¨æ—¶ä¸¢å¼ƒä¸€ç³»åˆ—æ ‡è®°ï¼Œå¹¶æ ¹æ®è¿™äº›æ ‡è®°è®¡ç®—å•åº”æ€§ã€‚ç„¶åï¼ŒåŸºäºåŒæºå…³ç³»çš„è·Ÿè¸ªå™¨ç»†åŒ–å…ƒç›®æ ‡åˆ—è¡¨ï¼Œå¹¶æ ¹æ®éœ€è¦åˆ›å»ºæ–°çš„å…ƒç›®æ ‡ã€‚ æµ‹è¯•è¡¨æ˜ï¼Œè¯¥ç®—æ³•è§£å†³äº†ä¸€è‡´æ€§æ ‡è®°é—®é¢˜ï¼Œå¹¶ä¸”åœ¨å­¦ä¹ è¿‡ç¨‹ä¸­åªéœ€è¦å¾ˆå°‘çš„è¾¹ç¼˜äº‹ä»¶ã€‚åŸºäºåŒæºæ€§çš„åŒ¹é…å™¨è¢«è¯æ˜å®Œå…¨å…‹æœéƒ¨åˆ†å’Œå®Œå…¨çš„ç›®æ ‡é®æŒ¡åœ¨ä¸€å¯¹ç›¸æœºä¹‹ä¸€ã€‚ 2 ä»‹ç»è§†é¢‘ç›‘æ§æ˜¯ä¸€é¡¹è‰°å·¨çš„ä»»åŠ¡ã€‚åŸºäºè®¡ç®—æœºè§†è§‰é¢†åŸŸï¼Œå®ƒæœ¬èº«åªæœ‰å‡ åå¹´çš„å†å²ï¼Œè§†é¢‘é¦ˆç”µçš„è‡ªåŠ¨å¤„ç†é€šå¸¸éœ€è¦ä¸“é—¨çš„ç¼–ç å’Œè§£ç ç¡¬ä»¶ï¼Œå¿«é€Ÿçš„æ•°å­—ä¿¡å·å¤„ç†å™¨ï¼Œä»¥åŠå¤§é‡çš„å­˜å‚¨ä»‹è´¨ã€‚ å¤„ç†å¤šä¸ªè§†é¢‘æµçš„éœ€æ±‚å˜å¾—è¶Šæ¥è¶Šé‡è¦ã€‚æ‘„åƒæœºçš„ä»·æ ¼æŒç»­ä¸‹é™ï¼Œåƒæ ·çš„â€œç½‘ç»œæ‘„åƒæœºâ€å”®ä»·ä¸åˆ°20ç¾å…ƒã€‚å®‰è£…åŒæ ·ä¾¿å®œä¸”ç®€å•ã€‚æ­¤å¤–ï¼Œç¤¾ä¼šå› ç´ ä¹Ÿæœ‰åŠ©äºç›‘æ§æ‘„åƒå¤´çš„æ™®åŠã€‚ä¼¦æ•¦å’Œæ³¢å£«é¡¿ç­‰åŸå¸‚çš„è­¦å¯Ÿéƒ¨é—¨ï¼Œä»¥åŠè´­ç‰©ä¸­å¿ƒå’Œæœºåœºç­‰ç§äººä¼ä¸šï¼Œéƒ½åœ¨åˆ©ç”¨æœ€è¿‘çš„ææ€–ä¸»ä¹‰ï¼Œä¸ºå¢åŠ è§†é¢‘ç›‘æ§æä¾›ç†ç”±ã€‚åœ¨å¤§å¤šæ•°å¤§åŸå¸‚ï¼Œç°åœ¨å¾ˆå®¹æ˜“å‘ç°æ‘„åƒæœºã€‚ä¸€äº›è£…ç½®ç”šè‡³è‡ªå¤¸ä½¿ç”¨å¯¹è¿‘çº¢å¤–æˆ–çƒ­çº¢å¤–æ³¢é•¿æ•æ„Ÿçš„æ‘„åƒæœºçš„å¾®å…‰èƒ½åŠ›ã€‚ å°½ç®¡å¤šæ‘„åƒå¤´ç›‘æ§è£…ç½®è¶Šæ¥è¶Šæ™®éï¼Œä½†å¾ˆå°‘æœ‰ç®—æ³•æå–é¢å¤–çš„ã€æœ‰æ„ä¹‰çš„å¤šæ‘„åƒå¤´è·Ÿè¸ªä¿¡æ¯ã€‚ç¬¬2ç« å°†ä»‹ç»ä¸€äº›åœ¨å•ä¸ªè§†é¢‘æµä¸­è·Ÿè¸ªè¿åŠ¨å¯¹è±¡çš„ç®—æ³•ã€‚å•æ‘„åƒæœºè·Ÿè¸ªé—®é¢˜çš„è§£å†³æ–¹æ¡ˆå·²ç»ç›¸å½“å®Œå–„ã€‚ç„¶è€Œï¼Œå¤šæ‘„åƒå¤´ç›‘æ§ç³»ç»Ÿéœ€è¦èƒ½å¤Ÿå¤„ç†å¤šä¸ªè§†é¢‘æµçš„ç®—æ³•ã€‚ 2.1 åŠ¨æœºç¤ºä¾‹æ‘„åƒæœºAå’Œæ‘„åƒæœºBæ˜¯ä¸ç›¸äº¤çš„â€”â€”å®ƒä»¬è§‚å¯Ÿä¸–ç•Œçš„ä¸åŒåŒºåŸŸï¼Œå¹¶ä¸é‡å ã€‚ç„¶è€Œï¼Œæ‘„åƒæœºAå’ŒCéƒ¨åˆ†é‡å ï¼Œæ‘„åƒæœºBå’ŒCä¹Ÿæ˜¯å¦‚æ­¤ã€‚ä¸¤ä¸ªæ‘„åƒæœºåŒæ—¶å¯ä»¥çœ‹åˆ°ä»»ä½•ä¸€ä¸ªä½äºè¾ƒæš—é‡å åŒºåŸŸçš„ç‰©ä½“ã€‚ ç°åœ¨æ£€æŸ¥è¿™ä¸‰ä¸ªæ‘„åƒæœºçš„è¾“å‡ºã€‚ä¸–ç•Œä¸Šåªæœ‰ä¸¤ä¸ªäººã€‚ç„¶è€Œï¼Œåœ¨è¿™ä¸‰å°ç›¸æœºä¹‹é—´ï¼Œå®ƒä»¬è¢«èµ‹äºˆäº†å››ä¸ªä¸åŒçš„æ ‡ç­¾:A-8, B-2, C-4å’ŒC-5ã€‚ç»™å®šè¿™äº›å¯¹è±¡æ ‡ç­¾ï¼Œæˆ‘ä»¬èƒ½æ‰¾åˆ°çš„æœ€é‡è¦çš„ä¿¡æ¯æ˜¯å“ªäº›æ ‡ç­¾æŒ‡å‘ç›¸åŒçš„ç°å®ä¸–ç•Œå¯¹è±¡ã€‚è¿™æ˜¯ä¸€è‡´çš„æ ‡ç­¾é—®é¢˜ã€‚ åœ¨ä¸€å®šç¨‹åº¦ä¸Šï¼Œäººç±»å¾ˆæ“…é•¿è§£å†³ä¸€è‡´æ€§æ ‡ç­¾é—®é¢˜ã€‚äººå·¥ç›‘æ§æ“ä½œå‘˜å¯ä»¥å¯¹æ‘„åƒæœºåœ¨ä¸–ç•Œä¸Šçš„ä½ç½®ä¿æŒä¸€ä¸ªå¿ƒç†æ¨¡å‹ï¼Œå¹¶ä¸”å³ä½¿ä½¿ç”¨ä¸åŒçš„æ‘„åƒæœºæ¨¡å¼(ä¾‹å¦‚ä¸€ä¸ªRGBæ‘„åƒæœºå’Œä¸€ä¸ªçƒ­çº¢å¤–æ‘„åƒæœº)ï¼Œä¹Ÿç»å¸¸å¯ä»¥åŒ¹é…æ‘„åƒæœºä¸æ‘„åƒæœºçš„ç‰¹å¾ã€‚æ­¤å¤–ï¼Œäººç±»åœ¨åŒ¹é…ç‰©ä½“æ–¹é¢æ¯”è®¡ç®—æœºåšå¾—æ›´å¥½ï¼Œå³ä½¿è¿™äº›ç‰©ä½“ä»ä¸åŒçš„è§’åº¦è§‚å¯Ÿï¼Œå› æ­¤å…·æœ‰ä¸åŒçš„å¤–è§‚ã€‚ç„¶è€Œï¼Œä½¿ç”¨äººç±»æ¥åˆ†æå¤šä¸ªè§†é¢‘æµå¹¶ä¸èƒ½å¾ˆå¥½åœ°æ‰©å±•ï¼Œå› ä¸ºä¸€ä¸ªäººä¸€æ¬¡åªèƒ½çœ‹ä¸€ä¸ªå±å¹•ï¼Œå³ä½¿ä¸€ä¸ªåœºæ™¯å¯èƒ½æœ‰è®¸å¤šç›¸å…³çš„è§†å›¾ã€‚å¦‚æœä½¿ç”¨å¤šä¸ªç›‘è§†æ“ä½œå‘˜ï¼Œæ¯ä¸ªäººè´Ÿè´£ä¸€ä¸ªç‰¹å®šçš„åŒºåŸŸï¼Œé‚£ä¹ˆç³»ç»Ÿå°†éœ€è¦å¼€å‘æ§åˆ¶ã€ç›®æ ‡è·Ÿè¸ªã€ç›®æ ‡åˆ‡æ¢å’Œå¯èƒ½çš„æ“ä½œå‘˜ä¸æ³¨æ„çš„ç¨‹åºã€‚ ç›‘è§†ç³»ç»Ÿçš„ä¸€é¡¹é‡è¦ä»»åŠ¡æ˜¯èƒ½å¤Ÿè·Ÿè¸ªåœ¨ç›‘è§†åŒºåŸŸå†…ç§»åŠ¨çš„ç›®æ ‡ã€‚è®¸å¤šç›¸æœºå¯èƒ½åœ¨ä»»ä½•ç»™å®šçš„æ—¶é—´å†…éƒ½åœ¨è§‚å¯Ÿç›®æ ‡ï¼Œä½†å³ä½¿åªä½¿ç”¨å°‘æ•°ç›¸æœºï¼Œäººç±»ä¹Ÿéœ€è¦æœ‰æ„è¯†åœ°åŠªåŠ›ç¡®å®šè¿™ç»„ç›¸æœºã€‚æ­¤å¤–ï¼Œéšç€ç›®æ ‡çš„ç§»åŠ¨ï¼Œè§‚å¯Ÿç›®æ ‡çš„æ‘„åƒæœºä¹Ÿåœ¨ä¸æ–­å˜åŒ–ã€‚å¦‚æœä¸€è‡´çš„æ ‡ç­¾é—®é¢˜è§£å†³äº†ï¼Œè®¡ç®—æœºçŸ¥é“ç›®æ ‡æ˜¯å¦åº”è¯¥å‡ºç°åœ¨æ¯ä¸ªæ‘„åƒæœºçš„è§†é‡ä¸­ï¼Œé‚£ä¹ˆè®¡ç®—æœºå°±å¯ä»¥è‡ªåŠ¨æç¤ºæ˜¾ç¤ºç›®æ ‡çš„æ­£ç¡®æ‘„åƒæœºç»„ã€‚ å›¾1.2è¯´æ˜äº†å°†å¤šä¸ªæ‘„åƒæœºè§†ä¸ºä¸€ç»„å•ä¸ªæ‘„åƒæœºçš„ç®—æ³•å’Œå°†æ‘„åƒæœºè§†ä¸ºæ›´å¤šä¸œè¥¿çš„ç®—æ³•ä¹‹é—´çš„åŒºåˆ«ã€‚å›¾1.2(a)ä¸­çš„ç®—æ³•åˆ™ä¸æ˜¯è¿™æ ·å…³å¿ƒå¤šå°æ‘„åƒæœºå¯èƒ½æ‹æ‘„ä¸–ç•Œçš„åŒä¸€éƒ¨åˆ†ã€‚ç¬¬äºŒç±»ç®—æ³•å¦‚å›¾1.2(b)æ‰€ç¤ºï¼Œå–å•ä¸ªæ‘„åƒæœºè·Ÿè¸ªå™¨çš„è¾“å‡ºå¹¶å°†å…¶ç»„åˆã€‚åˆ›å»ºäº†æ–°çš„ç›‘è§†åŠŸèƒ½ã€‚ä¸‹é¢å°†æåˆ°è¿™äº›å¤šæ‘„åƒå¤´æ„ŸçŸ¥ç®—æ³•æ‰€åˆ›å»ºçš„ä¸€äº›åŠŸèƒ½ç¤ºä¾‹ã€‚ 2.2 èŒƒå›´-ç›®æ ‡æœ¬è®ºæ–‡æ¶µç›–äº†ä¸€ç§å¤šæ‘„åƒå¤´ç›‘æ§ç®—æ³•çš„å¼€å‘ã€å®ç°å’Œæµ‹è¯•ã€‚è¯¥ç®—æ³•åº”å…·æœ‰ä»¥ä¸‹ç‰¹ç‚¹ï¼š ç‹¬ç«‹äºç›¸æœºå¤–éƒ¨å‚æ•°ï¼Œå³ä½ç½®å’Œæ–¹å‘ã€‚è¯¥ç®—æ³•åº”è¯¥èƒ½å¤Ÿå¹³ç¨³åœ°å¤„ç†å¹¿æ³›ä¸åŒçš„ä¸–ç•Œè§†è§’ã€‚ ç‹¬ç«‹äºç›¸æœºçš„å†…åœ¨å‚æ•°ï¼Œå³ç„¦è·ï¼Œåƒç´ å€¾æ–œå’Œä¸»ç‚¹çš„ä½ç½®ã€‚å¸‚åœºä¸Šæœ‰ä¸åŒçš„ç›¸æœºï¼Œç®—æ³•åº”è¯¥èƒ½å¤Ÿå¤„ç†å¤šä¸ªç„¦è·ï¼Œåˆ†è¾¨ç‡çš„å·®å¼‚ç­‰ç­‰ã€‚ ç‹¬ç«‹äºç›¸æœºæ¨¡å¼ã€‚è¯¥ç®—æ³•åº”è¯¥èƒ½å¤Ÿå¤„ç†ä»»ä½•å•æ‘„åƒå¤´è·Ÿè¸ªå™¨çš„è¾“å‡ºã€‚ç®—æ³•ä¸åº”è¯¥ä¾èµ–äºåº•å±‚çš„æ‘„åƒå¤´ç¡¬ä»¶RGBï¼Œè¿‘çº¢å¤–ï¼Œçƒ­çº¢å¤–ï¼Œæˆ–å…¶ä»–æˆåƒæŠ€æœ¯ã€‚ è§£å†³ä¸€è‡´æ€§æ ‡ç­¾é—®é¢˜ã€‚ä¸€ä¸ªçœŸå®ä¸–ç•Œçš„ç›®æ ‡åº”è¯¥é“¾æ¥åˆ°è¯¥ç›®æ ‡å¯è§çš„æ¯ä¸ªæ‘„åƒæœºä¸­çš„ä¸€ä¸ªå¯¹è±¡æ ‡ç­¾ å¯¹ç›®æ ‡é®æŒ¡å’Œåœºæ™¯å…¥å£çš„é²æ£’æ€§ã€‚å¦‚æœä¸€ä¸ªç›®æ ‡åœ¨åœºæ™¯ä¸­é—´è¿›å…¥ç›‘è§†åŒºåŸŸï¼Œæ¯”å¦‚é€šè¿‡ä¸€æ‰‡é—¨ï¼Œé‚£ä¹ˆè¯¥ç®—æ³•åº”è¯¥æ­£ç¡®åœ°è§£å†³ä¸€è‡´æ€§æ ‡ç­¾é—®é¢˜ã€‚ç±»ä¼¼åœ°ï¼Œå¦‚æœä¸€ä¸ªç›®æ ‡åˆ†è£‚æˆä¸¤ä¸ªï¼Œæ¯”å¦‚ä¸¤ä¸ªäº²å¯†çš„äººèµ°ä¸åŒçš„è·¯ï¼Œç®—æ³•åº”è¯¥è¯†åˆ«å¹¶æ­£ç¡®æ ‡è®°ä¸¤ä¸ªç›®æ ‡ã€‚ è®¾ç½®ç®€å•ã€‚ä¸éœ€è¦ç›¸æœºæ ¡å‡†ã€‚å¦‚æœéœ€è¦ï¼Œè®­ç»ƒåº”å°½å¯èƒ½å°‘åœ°èŠ±è´¹æ—¶é—´ï¼Œå¹¶åº”åœ¨æ­£å¸¸çš„ç°åœºäº¤é€šæ¡ä»¶ä¸‹è¿›è¡Œã€‚è®­ç»ƒåº”è¯¥æ˜¯è‡ªåŠ¨çš„ï¼Œä¸éœ€è¦æ“ä½œå‘˜å¹²é¢„ã€‚ å…·å¤‡æ‘„åƒæç¤ºèƒ½åŠ›ã€‚è¯¥ç®—æ³•åº”è¯¥èƒ½å¤Ÿç¡®å®šå“ªäº›æ‘„åƒæœºåº”è¯¥èƒ½å¤Ÿçœ‹åˆ°ç»™å®šçš„ç›®æ ‡ã€‚ 2.3 èŒƒå›´-é™åˆ¶ç®—æ³•çš„èŒƒå›´é™åˆ¶å¦‚ä¸‹: è¿½è¸ªè¡Œèµ°çš„äººçš„ç®—æ³•åº”ä½¿ç”¨ã€‚è½¦è¾†ã€åŠ¨ç‰©ç­‰å„ç±»è¿åŠ¨ç‰©ä½“ä¸åœ¨æœ¬æ–‡ç ”ç©¶èŒƒå›´ä¹‹å†…ã€‚ è¦å¤„ç†çš„ç›¸æœºå¯¹è‡³å°‘æœ‰éƒ¨åˆ†é‡å çš„è§†é‡ã€‚è¿™å°±è¦æ±‚æ“ä½œå‘˜åœ¨å®‰è£…ç¡¬ä»¶å’Œåˆå§‹åŒ–ç®—æ³•æ—¶åšå‡ºåˆæ­¥åˆ¤æ–­:å†³å®šå“ªäº›æ‘„åƒæœºçœ‹åˆ°çš„æ˜¯ä¸–ç•Œä¸Šç›¸åŒçš„éƒ¨åˆ†ã€‚ æ‘„åƒæœºåº”è¯¥æ˜¯é™æ­¢çš„ã€‚ä¸€æ—¦å®‰è£…ï¼Œç›¸æœºçš„å†…åœ¨å’Œå¤–åœ¨å‚æ•°éƒ½åº”è¯¥æ˜¯å›ºå®šçš„ã€‚è¿™æ„å‘³ç€ç›¸æœºä¸èƒ½å®‰è£…åœ¨å¹³åº•å€¾æ–œç‚®å¡”ä¸Šï¼Œæˆ–è€…å¦‚æœå®ƒæ˜¯ï¼Œç‚®å¡”ä¸èƒ½ç§»åŠ¨ã€‚ æ‘„åƒæœºçš„è¾“å‡ºå›¾åƒå°†æ˜¯ä¸€ä¸ªå®ç”¨çš„å¤§å°ã€‚è¯¥ç®—æ³•å°†ä¸åŒ…æ‹¬å•åƒç´ æ¢æµ‹å™¨(ä¾‹å¦‚çº¢å¤–è¿åŠ¨æ¢æµ‹å™¨ï¼Œå…‰æŸç ´å…‰æ¢æµ‹å™¨)ã€‚è¿™ç§é™åˆ¶æ˜¯å¿…è¦çš„ï¼Œä»¥ç¡®ä¿å•æ‘„åƒæœºè·Ÿè¸ªæ˜¯å¯èƒ½çš„ï¼Œè€Œä¸éœ€è¦å¯¹æ‰€é€‰ç®—æ³•è¿›è¡Œé‡å¤§æ›´æ”¹ã€‚ å¸§ç‡å°†è¶³ä»¥è®©å•æ‘„åƒå¤´è·Ÿè¸ªç®—æ³•æ­£å¸¸å·¥ä½œã€‚ ç›¸æœºåº”è¿‘ä¼¼å¸¸è§„ä¸­å¿ƒæŠ•å½±ç›¸æœºä¸åŸºæœ¬é’ˆå­”å…‰å­¦ã€‚ç›¸æœºå…·æœ‰æå®½çš„è§†åœºè§†ç‚¹-é±¼çœ¼é•œå¤´-æˆ–æ˜¾è‘—æœªæ ¡æ­£çš„æ‰­æ›²å°†ä¸ä½¿ç”¨ã€‚ æœ€é‡è¦çš„æ˜¯ï¼Œç›®æ ‡åº”è¯¥åœ¨åœ°é¢ä¸Šè¡Œèµ°ã€‚ä»»ä½•ä¸¤ä¸ªæ‘„åƒæœºä¹‹é—´çš„é‡å åŒºåŸŸä¸å¾—æœ‰æ˜æ˜¾åç¦»å¹³é¢ã€‚å¤„ç†ä¸˜é™µåœ°åŒºæˆ–å°é˜¶çš„ä»£ç ä¸åŒ…å«åœ¨æœ¬ç®—æ³•ä¸­ã€‚ æ‘„åƒæœºä¸å¾—å®‰è£…åœ¨åœ°å¹³é¢ä¸Šã€‚è¿™å¯ä»¥é˜²æ­¢åœ¨åœºæ™¯å‡ ä½•ä¸­å‡ºç°é€€åŒ–çš„æƒ…å†µï¼Œå¦‚ä¸‹æ–‡æ‰€ç¤º 2.4 å¯¹ é¢†åŸŸçš„è´¡çŒ® å¦‚ä¸Šæ‰€è¿°ï¼Œå¤šæ‘„åƒå¤´è§†é¢‘å¤„ç†æ˜¯ä¸€ä¸ªç›¸å¯¹è¾ƒæ–°çš„é¢†åŸŸã€‚ç®—æ³•ä¸€ç›´åœ¨å¼€å‘ä¸­ï¼Œè¿˜æœ‰å¾ˆå¤šé—®é¢˜æœ‰å¾…è§£å†³ã€‚å¦‚æœæœ¬æ–‡å¼€å‘çš„ç®—æ³•æ»¡è¶³1.2èŠ‚å’Œ1.3èŠ‚ä¸­æè¿°çš„ç›®æ ‡å’Œé™åˆ¶ï¼Œåˆ™å¯ä»¥å®ç°ä»¥ä¸‹åœºæ™¯: è‡ªåŠ¨æç¤º:æ„Ÿå…´è¶£çš„ç›®æ ‡èµ°è¿›ç›‘è§†åŒºåŸŸã€‚ æ“ä½œå‘˜åœ¨ä¸€ä¸ªæ‘„åƒæœºä¸­æ ‡è®°ç›®æ ‡ã€‚å½“ç›®æ ‡åœ¨æ•´ä¸ªåŒºåŸŸå†…ç§»åŠ¨æ—¶ï¼Œè®¡ç®—æœºåœ¨ç®—æ³•çš„é©±åŠ¨ä¸‹ï¼Œè‡ªåŠ¨æ˜¾ç¤ºæ‰€æœ‰å¯è§ç›®æ ‡çš„è§†é¢‘æºã€‚ç›®æ ‡å¯ä»¥ç”¨é¢œè‰²ä¸€è‡´çš„â€œå…‰ç¯â€æˆ–åŒ…å›´æ¡†æ ‡è®°ã€‚è¿™è®©æ“ä½œå‘˜ä¸“æ³¨äºç›®æ ‡çš„è¡ŒåŠ¨ï¼Œè€Œä¸æ˜¯å®ƒåœ¨ä¸–ç•Œä¸Šç›¸å¯¹äºæ¯ä¸ªæ‘„åƒæœºçš„ä½ç½®ã€‚ è·¯å¾„åˆ†æ:ä¸€ä¸ªåŒºåŸŸè¢«ç½®äºç›‘è§†ä¹‹ä¸‹ã€‚è¯¥ç®—æ³•ä¸æ˜¯è¯•å›¾æ‰‹åŠ¨åŒ¹é…äººä»¬ä»ä¸€ä¸ªæ‘„åƒå¤´åˆ°å¦ä¸€ä¸ªæ‘„åƒå¤´çš„è·¯å¾„ï¼Œè€Œæ˜¯è‡ªåŠ¨è¿æ¥äººä»¬é€šè¿‡è¯¥åŒºåŸŸæ‰€èµ°çš„è·¯å¾„ã€‚è¿™ä½¿å¾—æµé‡åˆ†æèƒ½å¤Ÿæ›´å¿«ã€æ›´æœ‰æ•ˆåœ°è¿›è¡Œã€‚ è·Ÿè¸ªé®æŒ¡æ¢å¤ã€‚ä¸ºäº†æ¬ºéª—å½“å‰çš„è®¸å¤šè·Ÿè¸ªç®—æ³•ï¼Œä½ å¯ä»¥ç§»åŠ¨åˆ°é®æŒ¡ç‰©åé¢(ä¾‹å¦‚å»ºç­‘æ”¯æŸ±æˆ–é«˜å¤§çš„åŒä¼™)ï¼Œæ”¹å˜é€Ÿåº¦ï¼Œç„¶åç§»å‡ºé®æŒ¡ç‰©ã€‚é®æŒ¡ç ´åäº†è®¸å¤šå½“å‰çš„è·Ÿè¸ªç®—æ³•ï¼Œå¦‚æœé€Ÿåº¦å˜åŒ–æ˜¾è‘—ï¼Œå¤§å¤šæ•°å…¶ä»–ç®—æ³•å°±ä¼šç ´åã€‚åªè¦ç›®æ ‡åœ¨è‡³å°‘ä¸€ä¸ªæ‘„åƒæœºä¸­ä»ç„¶å¯è§ï¼Œæ¥ä¸‹æ¥ç« èŠ‚ä¸­è®¨è®ºçš„ç®—æ³•å°†ä»é®æŒ¡ä¸­æ¢å¤ï¼Œå¹¶é‡æ–°å»ºç«‹ä¸€è‡´çš„è·Ÿè¸ªæ ‡ç­¾ ç°åœºçš„å…¥å£ã€‚è¯¥ç®—æ³•åº”è¯¥èƒ½å¤Ÿåœ¨äººä»¬å¯ä»¥ä»æ¡†æ¶ä¸­é—´è¿›å…¥çš„åœºæ™¯ä¸­åˆ›å»ºä¸€è‡´çš„è·Ÿè¸ªæ ‡ç­¾ï¼Œä¾‹å¦‚é€šè¿‡ç”µæ¢¯æˆ–é—¨ 2.4.1 å…·ä½“çš„è´¡çŒ®æœ¬æ–‡ä¸ºè§†é¢‘å¤„ç†é¢†åŸŸæä¾›äº†ä»¥ä¸‹å…·ä½“è´¡çŒ®: ä¸€ç§å³ä½¿åœ¨æ‘„åƒæœºæ˜æ˜¾å€¾æ–œæ—¶ä¹Ÿèƒ½æ‰¾åˆ°ç›®æ ‡è„šçš„æ–¹æ³• ä¸€ç§åˆ©ç”¨ç›®æ ‡è¿åŠ¨æ¥å¯»æ‰¾å¹³é¢è¯±å¯¼å•åº”æ€§çš„æ–¹æ³•ï¼Œå³ä½¿å…¥å£å’Œå‡ºå£åœ¨ç©ºé—´ä¸Šæ˜¯æœ‰é™çš„ ä¸€ç§å…·æœ‰ç‰¹å®šè§„åˆ™çš„æ–¹æ³•ï¼Œæè¿°å¦‚ä½•ä½¿ç”¨å¹³é¢è¯±å¯¼å•åº”æ€§æ¥è·¨å¤šä¸ªæ‘„åƒæœºåˆ›å»ºå’Œç»´æŒç›®æ ‡å…³è” ç¬¬ä¸‰ç« è®¨è®ºåŸºç¡€ç†è®ºï¼Œç¬¬å››ç« è®¨è®ºå®ç°ç»†èŠ‚ã€‚è¯•éªŒç»“æœè§ç¬¬5ç« ã€‚ 3. èƒŒæ™¯","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"å•åº”æ€§æ¡†æ¶ä¸‹åŸºäºå›¾åˆ‡çš„å‰æ™¯åˆ†å‰²çš„å¤šæ‘„åƒæœºäººç‰©è·Ÿè¸ª","path":"/2023/01/24/å•åº”æ€§æ¡†æ¶ä¸‹åŸºäºå›¾åˆ‡çš„å‰æ™¯åˆ†å‰²çš„å¤šæ‘„åƒæœºäººç‰©è·Ÿè¸ª/","content":"å•åº”æ€§æ¡†æ¶ä¸‹åŸºäºå›¾åˆ‡çš„å‰æ™¯åˆ†å‰²çš„å¤šæ‘„åƒæœºäººç‰©è·Ÿè¸ª Multi Camera Person Tracking Applying a Graph-Cuts basedForeground Segmentation in a Homography Framework æ‘˜è¦å¯é çš„ç›®æ ‡è·Ÿè¸ªæ˜¯å®ç°è‡ªåŠ¨åŒ–è§†é¢‘ç›‘æ§ç³»ç»Ÿçš„å¿…è¦å‰æã€‚ç”±äºå¤§å¤šæ•°åŸºäºæœºå™¨å­¦ä¹ çš„ç›®æ ‡æ£€æµ‹æ–¹æ³•éœ€è¦è¶³å¤Ÿçš„åº”ç”¨åœºæ™¯æ•°æ®ï¼Œå‰æ™¯åˆ†å‰²æ˜¯ä¸€ç§æµè¡Œçš„æ–¹æ³•æ¥å¯»æ‰¾å¯èƒ½æ„Ÿå…´è¶£çš„åŒºåŸŸã€‚è¿™äº›é€šå¸¸éœ€è¦ä¸€ä¸ªç‰¹å®šçš„å­¦ä¹ é˜¶æ®µï¼Œå¹¶éšç€æ—¶é—´çš„æ¨ç§»è¿›è¡Œé€‚åº”ã€‚åœ¨è¿™é¡¹å·¥ä½œä¸­ï¼Œæˆ‘ä»¬å°†æå‡ºä¸€ç§åŸºäºå›¾åˆ‡å‰²çš„æ–°æ–¹æ³•ï¼Œå®ƒä¼˜äºå¤§å¤šæ•°æ ‡å‡†ç®—æ³•ã€‚äººä»¬æ™®éè®¤ä¸ºï¼Œé®æŒ¡åªèƒ½åœ¨å¤šæ‘„åƒæœºç¯å¢ƒä¸­è§£å†³ã€‚åº”ç”¨å¤šå±‚å•åº”æ€§å¯ä»¥ä½¿æˆ‘ä»¬ä»…åº”ç”¨å‰æ™¯æ•°æ®å°±èƒ½é²æ£’åœ°æ£€æµ‹å’Œè·Ÿè¸ªç›®æ ‡ï¼Œä»è€Œè·å¾—è¾ƒé«˜çš„è·Ÿè¸ªæ€§èƒ½ã€‚ ä»‹ç»","tags":["æŠ€æœ¯"],"categories":["æŠ€æœ¯"]},{"title":"åœ¨æ‰‹æœ¯å®¤å¤šç›¸æœºå¤šè¡Œäººè·Ÿè¸ªå’Œé‡è¯†åˆ«","path":"/2023/01/24/åœ¨æ‰‹æœ¯å®¤å¤šç›¸æœºå¤šè¡Œäººè·Ÿè¸ªå’Œé‡è¯†åˆ«/","content":"åœ¨æ‰‹æœ¯å®¤å¤šç›¸æœºå¤šè¡Œäººè·Ÿè¸ªå’Œé‡è¯†åˆ« Multi-Camera Multi-Person T racking and Re-Identification inan Operating Room æ‘˜è¦å¤šæ‘„åƒå¤´å¤šäºº(MCMP)è·Ÿè¸ªä¸é‡è¯†åˆ«(ReID)æ˜¯å®‰å…¨ã€è¡Œäººåˆ†æç­‰æ–¹é¢çš„é‡è¦ä»»åŠ¡;ç„¶è€Œï¼Œå¤§å¤šæ•°ç ”ç©¶éƒ½é›†ä¸­åœ¨å®¤å¤–åœºæ™¯ï¼Œå› ä¸ºåœ¨ä¸€ä¸ªæœ‰éšœç¢ç‰©çš„æ‹¥æŒ¤æˆ¿é—´é‡Œï¼Œå¤„ç†é®æŒ¡å’Œé”™è¯¯è¯†åˆ«è¦å¤æ‚å¾—å¤šã€‚æ­¤å¤–ï¼Œåœ¨ä¸€ä¸ªæ¡†æ¶å†…å®Œæˆè¿™ä¸¤é¡¹ä»»åŠ¡å…·æœ‰æŒ‘æˆ˜æ€§ã€‚æœ¬æ–‡æå‡ºäº†ä¸€ç§åŸºäºè½¨è¿¹çš„æ–¹æ³•ï¼Œå°†è·Ÿè¸ªå’ŒReIDä»»åŠ¡é›†æˆåœ¨ä¸€èµ·ã€‚é¦–å…ˆï¼Œæ¯å°æ‘„åƒæœºæ•æ‰åˆ°çš„æ‰€æœ‰æ‰‹æœ¯æˆå‘˜çš„å§¿åŠ¿è¢«é€å¸§æ£€æµ‹;ç„¶åï¼Œåˆ©ç”¨æ£€æµ‹åˆ°çš„å§¿æ€æ¥è·Ÿè¸ªæ¯ä¸ªæ‘„åƒæœºçš„æ‰€æœ‰æˆå‘˜çš„è½¨è¿¹;æœ€åï¼Œå°†è¿™äº›ä¸åŒæ‘„åƒæœºçš„è½¨è¿¹èšç±»ï¼Œé€šè¿‡æ‰€æœ‰æ‘„åƒæœºé‡æ–°è¯†åˆ«æ‰‹æœ¯å®¤ä¸­çš„æˆå‘˜ã€‚ä¸å…¶ä»–MCMPè·Ÿè¸ªå’ŒReIDæ–¹æ³•ç›¸æ¯”ï¼Œæœ¬æ–‡æå‡ºçš„æ–¹æ³•ä¸»è¦åˆ©ç”¨è½¨è¿¹ï¼Œå°†æ‰‹æœ¯å®¤åœºæ™¯ä¸­ä¸æ˜“è¯†åˆ«çš„çº¹ç†ç‰¹å¾ä½œä¸ºè¾…åŠ©çº¿ç´¢ã€‚æœ¬æ–‡è¿˜åœ¨ReIDè¿‡ç¨‹ä¸­é›†æˆäº†æ—¶é—´ä¿¡æ¯ï¼Œè¿™æ¯”å½“å‰æœ€å…ˆè¿›çš„æ¡†æ¶(é€å¸§è¿›è¡ŒReID)æ›´å¯é ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬çš„æ¡†æ¶åœ¨éƒ¨ç½²åˆ°æ–°åœºæ™¯ä¹‹å‰ä¸éœ€è¦åŸ¹è®­ã€‚æœ¬æ–‡è¿˜åˆ›å»ºäº†ä¸€ä¸ªå¸¦æœ‰å®é™…æ‰‹æœ¯å®¤è§†é¢‘çš„æ³¨é‡ŠMCMPæ•°æ®é›†ã€‚å®éªŒè¯æ˜äº†æ‰€æå‡ºçš„åŸºäºè½¨è¿¹çš„ReIDç®—æ³•çš„æœ‰æ•ˆæ€§ã€‚æå‡ºçš„æ¡†æ¶åœ¨ReIDä»»åŠ¡ä¸­è¾¾åˆ°äº†85.44%çš„å‡†ç¡®ç‡ï¼Œåœ¨æœ¬æ–‡æå‡ºçš„æ‰‹æœ¯å®¤æ•°æ®é›†ä¸­ä¼˜äºæœ€å…ˆè¿›çš„æ¡†æ¶ã€‚ ä»‹ç»åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬æ—¨åœ¨å»ºç«‹ä¸€ç§åœ¨æ‰‹æœ¯å®¤å†…è·å¾—å¯é çš„å•ç›®äººå‘˜è·Ÿè¸ªå’Œç›¸æœºé—´äººå‘˜ReIDç»“æœçš„æ–¹æ³•ã€‚ä¸ç›´æ¥ç”¨ç¥ç»ç½‘ç»œæå–å›¾åƒç‰¹å¾ç›¸æ¯”ï¼Œæˆ‘ä»¬ä¸»è¦é€šè¿‡è¿åŠ¨è½¨è¿¹è¿›è¡Œåˆ¤æ–­ï¼Œä»¥çº¹ç†ç‰¹å¾ä½œä¸ºè¾…åŠ©çº¿ç´¢ã€‚å› ä¸ºå·¥ä½œäººå‘˜çš„è¿åŠ¨éµå¾ªä¸€å®šçš„è§„åˆ™ï¼Œä»–ä»¬çš„è½¨è¿¹æ¯”éš¾ä»¥åŒºåˆ†çš„çº¹ç†ç‰¹å¾æ›´å¯é ã€‚å³ä½¿æ˜¯ç”±äºé®æŒ¡æˆ–äººä¸ºæ¢æµ‹å¤±è´¥å¯¼è‡´æŸæ‘„åƒå¤´çš„è½¨è¿¹è¢«â€œç ´åâ€ï¼Œåœ¨è·¨æ‘„åƒå¤´ReIDçš„æ­¥éª¤ä¸­ï¼Œä¹Ÿå¯ä»¥æ ¹æ®å…¶ä»–æ‘„åƒå¤´æ›´å®Œæ•´çš„è½¨è¿¹å°†å…¶ç ´åçš„è½¨è¿¹è¿æ¥èµ·æ¥ã€‚ç›¸å¯¹äºåœ¨é—­å¡å‰åå¼ºåˆ¶è¿æ¥å¯èƒ½å±äºåŒä¸€ä¸ªäººçš„ä¸¤æ¡è½¨è¿¹æˆ–å…¶ä»–ç±»å‹çš„æ•…éšœï¼Œæœ¬æ–¹æ³•ä»…è¿½æ±‚æ›´å¥½çš„å•ç›®è·Ÿè¸ªæ€§èƒ½ï¼Œå¯ä»¥è·å¾—æ›´å¥½ã€æ›´å¯é çš„æ•´ä½“æ€§èƒ½ã€‚ è¯¥æ–¹æ³•å¯åˆ†ä¸ºä¸‰ä¸ªæ­¥éª¤:é¦–å…ˆï¼Œåˆ©ç”¨è®­ç»ƒå¥½çš„å§¿æ€ä¼°è®¡ç¥ç»ç½‘ç»œæ£€æµ‹æ¯å°æ‘„åƒæœºæ•æ‰åˆ°çš„æ¯ä¸€å¸§å›¾åƒä¸­å·¥ä½œäººå‘˜çš„å§¿æ€;ç„¶åç­›é€‰å‡ºå§¿æ€ç½®ä¿¡åº¦å¾—åˆ†é«˜çš„å·¥ä½œäººå‘˜ï¼Œæ ¹æ®æ¯ä¸ªäººè„šçš„å›¾åƒåæ ‡å’Œé¢„ä¼°çš„å›¾åƒå¹³é¢åˆ°ä¸–ç•Œåœ°å¹³é¢çš„å•åº”æ€§çŸ©é˜µï¼Œä¼°è®¡å’Œè·Ÿè¸ªæ¯ä¸ªäººåœ¨æ‰‹æœ¯å®¤ä¸­çš„ä½ç½®;æœ€åï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§èšç±»ç®—æ³•ï¼Œé‡æ–°è¯†åˆ«æ‰€æœ‰æ‘„åƒæœºä¸­è¢«æ£€æµ‹åˆ°çš„å·¥ä½œäººå‘˜çš„è½¨è¿¹ï¼Œä»è€Œè·å¾—æ‰‹æœ¯å®¤ä¸­æ‰€æœ‰å·¥ä½œäººå‘˜çš„å®Œæ•´è½¨è¿¹ã€‚ æ–¹æ³•ç”±äºæ‰‹æœ¯å®¤åœºæ™¯çš„ç‰¹æ®Šæ€§ï¼Œæˆ‘ä»¬å¾ˆéš¾ç”¨ç«¯åˆ°ç«¯æ¨¡å‹è§£å†³è·Ÿè¸ªå’ŒReIDé—®é¢˜;å› æ­¤ï¼Œæ ¹æ®å…¶ç‰¹ç‚¹ï¼Œæˆ‘ä»¬å»ºç«‹äº†ä¸€ä¸ªåŒ…å«ä¸‰ä¸ªæ¨¡å‹çš„æ¡†æ¶ï¼Œé€æ­¥å®Œæˆæ•´ä¸ªä»»åŠ¡ã€‚æ–¹æ³•æµç¨‹å¦‚å›¾2æ‰€ç¤ºã€‚åˆ©ç”¨ä¼°è®¡çš„å§¿æ€å’Œè¾¹ç•Œæ¡†åˆ†åˆ«ç”Ÿæˆæ‰€æœ‰æ‘„åƒæœºçš„å‡ æ®µè½¨è¿¹ï¼Œç„¶åç»“åˆæ‰€æœ‰æ‘„åƒæœºçš„ç»“æœè¿æ¥å¹¶é‡æ–°è¯†åˆ«å¾—åˆ°çš„ç ´ç¢è½¨è¿¹ã€‚ 3.1 å§¿æ€ä¼°è®¡å’Œæ£€æµ‹æ¡†äººä½“å§¿æ€ä¼°è®¡çš„ç ”ç©¶åœ¨è®¡ç®—æœºè§†è§‰é¢†åŸŸå·²æµè¡Œå¤šå¹´[27-29]ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨äº†ä¸€ç§æˆç†Ÿä¸”æµè¡Œçš„å§¿æ€ä¼°è®¡ç¥ç»ç½‘ç»œAlphaPose[27]ã€‚å¯¹äºæ¯ä¸€ä¸ªè¢«æ£€æµ‹åˆ°çš„äººï¼ŒAlphaPoseä¸ä»…ç»™å‡ºä¸€å¥—å®Œæ•´çš„å¸¦æœ‰ç½®ä¿¡åº¦åˆ†æ•°çš„å§¿æ€å…³é”®ç‚¹ï¼Œè¿˜ç»™å‡ºäº†æ•´ä¸ªå§¿æ€åŠå…¶åŒ…å›´æ¡†çš„ç½®ä¿¡åº¦åˆ†æ•°ã€‚é€šè¿‡è·å¾—çš„ä¿¡æ¯ï¼Œæˆ‘ä»¬èƒ½å¤Ÿä¼°è®¡æ¯ä¸ªæ‰‹æœ¯æˆå‘˜åœ¨æ‰‹æœ¯å®¤ä¸­çš„ä½ç½®ï¼Œå¹¶åœ¨åç»­æ­¥éª¤ä¸­æå–ä»–ä»¬çš„å›¾åƒç‰¹å¾ã€‚ 3.2 å•ç›®è·Ÿè¸ªå› æ­¤ï¼Œå—Trackpy[30]çš„å¯å‘ï¼Œæˆ‘ä»¬è®¾è®¡äº†ä¸€ç§ç®€æ´å¯é çš„æ–¹æ³•ï¼Œåœ¨å•ç›®è·Ÿè¸ªçš„æ­¥éª¤ä¸­åˆ©ç”¨æ¯ä¸ªäººçš„ä½ç½®ã€‚é¦–å…ˆï¼Œä»æ‰€æœ‰æ£€æµ‹åˆ°çš„å§¿æ€ä¸­ç­›é€‰å‡ºå¯ç”¨çš„å§¿æ€ï¼Œåˆ é™¤é‚£äº›å§¿æ€ä¿¡å¿ƒåˆ†æ•°å°äºT11æˆ–è„šè¸ä¿¡å¿ƒåˆ†æ•°éƒ½å°äºT12çš„å§¿æ€ã€‚ç„¶åï¼Œä¼°è®¡æ¯ä¸ªç­›é€‰å‡ºæ¥çš„æ‰‹æœ¯æˆå‘˜çš„ä½ç½®ï¼Œå¹¶å°†å…¶æ˜ å°„åˆ°ä¸–ç•Œåœ°å¹³é¢ä¸Šï¼Œæˆ‘ä»¬ä¼°è®¡å…¶åœ¨å›¾åƒåæ ‡ç³»ä¸­çš„ä½ç½®ï¼ŒCp = (xp, yp)ï¼Œå…¶ä¸¤ä¸ªè„šè¸åæ ‡åœ¨åŒä¸€å¸§ä¸­ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬åˆ©ç”¨ä¸€ä¸ªäººçš„ä¸¤ä¸ªè„šè¸çš„å¹³å‡åæ ‡æ¥è¡¨ç¤ºä»–ä»¬çš„ä½ç½®: ï¼Œå…¶ä¸­Claå’ŒCraåˆ†åˆ«è¡¨ç¤ºå·¦å³è„šè¸çš„åæ ‡ï¼Œç„¶åæˆ‘ä»¬å°†Cpæ˜ å°„åˆ°ä¸–ç•Œåœ°å¹³é¢ä¸Šï¼Œä½¿ç”¨é¢„å…ˆä¼°è®¡çš„æ‘„åƒæœºå•åº”æ€§çŸ©é˜µH: å…¶ä¸­(Xp, Yp)è¡¨ç¤ºè¿™ä¸ªäººåœ¨åœ°åæ ‡ç³»ä¸­çš„åæ ‡ã€‚ æœ€åï¼Œæˆ‘ä»¬é€å¸§è·Ÿè¸ªæ¯ä¸ªç­›é€‰çš„æ‰‹æœ¯æˆå‘˜ï¼Œå¦‚æœä»–ä»¬åœ¨å½“å‰å¸§å’Œä¸‹ä¸€å¸§çš„ä½ç½®è·ç¦»å°äºé¢„å…ˆè®¾å®šçš„é˜ˆå€¼T13(äººåœ¨åœ°åæ ‡ç³»ä¸­å¯ä»¥è·‘çš„æœ€å¤§è·ç¦»)ï¼Œåˆ™åœ¨ä¸‹ä¸€å¸§ä¸­è·Ÿè¸ªä»–ä»¬ã€‚æ­¤å¤–ï¼Œä¸ºäº†å¤„ç†æŸäººçš„å§¿åŠ¿å¯èƒ½åœ¨æŸäº›å¸§ä¸­ä¸¢å¤±æˆ–è¢«è¿‡æ»¤æ‰çš„æƒ…å†µï¼Œæˆ‘ä»¬è®¾ç½®äº†å¦ä¸€ä¸ªå‚æ•°T14ï¼Œç§°ä¸ºâ€œè®°å¿†å¸§æ•°â€ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬è€ƒè™‘åˆ°ä¸€ä¸ªäººå¯èƒ½ä¼šä¸¢å¤±å‡ å¸§ï¼Œç„¶åå†æ¬¡å‡ºç°çš„å¯èƒ½æ€§ï¼Œæˆ‘ä»¬ä¿ç•™æ¶ˆå¤±çš„å¤–ç§‘æˆå‘˜çš„è·Ÿè¸ªï¼Œå¹¶åœ¨ä»–ä»¬æœ€åä¸€æ¬¡å‡ºç°åä¿ç•™ä»–ä»¬çš„idï¼Œæœ€å¤šä¿ç•™ä¸€äº›å¸§æ•°ã€‚é€šè¿‡è¿™ç§æ–¹æ³•ï¼Œæˆ‘ä»¬è·å¾—äº†æ¯ä¸ªæ‘„åƒæœºçš„æ‰€æœ‰æ‰‹æœ¯æˆå‘˜çš„åˆå§‹è½¨è¿¹ã€‚ 3.3 ç›¸æœºé—´é‡è¯†åˆ«ç”±äºé—­å¡å’Œä¸€äº›å…¶ä»–å¹²æ‰°ï¼Œè·å¾—çš„è½¨è¿¹å°†è¢«åˆ†æˆå‡ ä¸ªæ®µä¸ºæ¯ä¸ªæ‰‹æœ¯æˆå‘˜å¦‚ä¸Šæ‰€è¿°ã€‚åœ¨å•ç›®è·Ÿè¸ªä¸­ï¼Œè¿™ä¸ªé—®é¢˜å¾ˆéš¾è§£å†³ã€‚å¹¸è¿çš„æ˜¯ï¼Œåœ¨æ‰‹æœ¯å®¤åœºæ™¯ä¸­æ”¾ç½®å¤šä¸ªæ‘„åƒæœºæ˜¯å¾ˆæ–¹ä¾¿çš„ï¼Œè¿™æ„å‘³ç€æˆ‘ä»¬å¯ä»¥é€šè¿‡å…¶ä»–æ‘„åƒæœºæ›´å®Œæ•´çš„è½¨è¿¹æ¥å…‹æœè¿™äº›æŒ‘æˆ˜ã€‚æˆ‘ä»¬å¼•å…¥äº†åŸºäºå¯†åº¦çš„ç©ºé—´èšç±»æ–¹æ³•(DBSCAN[31])æ¥æ€»ç»“ä¸åŒæ‘„åƒæœºçš„è½¨è¿¹ï¼Œä¸ºå±äºåŒä¸€äººçš„æ‘„åƒæœºåˆ†é…ç›¸åŒçš„IDã€‚ åŸå§‹DBSCANçš„æ€æƒ³éå¸¸ç®€æ´:å®ƒä»ä¸€ä¸ªæ²¡æœ‰è®¿é—®è¿‡çš„ä»»æ„æ ¸å¿ƒç‚¹å¼€å§‹;æ£€ç´¢è¿™ä¸ªç‚¹çš„eé‚»åŸŸï¼Œå¦‚æœå®ƒåŒ…å«è¶³å¤Ÿå¤šçš„ç‚¹ï¼Œåˆ™å¯åŠ¨ä¸€ä¸ªé›†ç¾¤;å¦åˆ™ï¼Œè¿™ä¸ªç‚¹è¢«æ ‡è®°ä¸ºå™ªå£°ã€‚å—æ­¤æ€æƒ³å¯å‘ï¼Œæˆ‘ä»¬è®¾è®¡äº†æˆ‘ä»¬çš„èšç±»ç®—æ³•:åœ¨åŸå§‹çš„DBSCANä¸­ï¼Œå°†æ ¸å¿ƒå¯¹è±¡å®šä¹‰ä¸ºå…¶é‚»åŸŸå†…çš„é«˜å¯†åº¦ç‚¹ï¼ŒåŸºäºè½¨è¿¹çš„å®Œæ•´æ€§ï¼Œæˆ‘ä»¬å°†æ—¶é—´é•¿åº¦(å•ä½:å¸§)å¤§äºT21çš„è½¨è¿¹å®šä¹‰ä¸ºæˆ‘ä»¬çš„æ¡ˆä¾‹ä¸­çš„æ ¸å¿ƒå¯¹è±¡;å¯¹äºæ ¸å¿ƒè½¨è¿¹pï¼Œå¦‚æœpä¸å¦ä¸€ä¸ªè½¨è¿¹qçš„å¹³å‡è·ç¦»å°äºT22ï¼Œä¸”pä¸qçš„å¹³å‡ç›®æ ‡æ¡†ç›¸ä¼¼åº¦(æœ¬æ–‡é‡‡ç”¨ç›´æ–¹å›¾ç›¸å…³ç³»æ•°)å¤§äºT23ï¼Œåˆ™è®¤ä¸ºqæ˜¯pçš„é‚»åŸŸå¯¹è±¡ã€‚å¯¹äºèšç±»åæ ‡è®°ä¸ºâ€œå™ªå£°â€çš„è½¨è¿¹ï¼Œæˆ‘ä»¬æ ¹æ®å¹³å‡è·ç¦»å’Œå¹³å‡åŒ…å›´ç›’ç›¸ä¼¼åº¦ä¸ºæ¯ä¸ªâ€œå™ªå£°â€è½¨è¿¹å¯»æ‰¾æœ€å¯èƒ½çš„èšç±»ã€‚æ­¤å¤–ï¼Œä¸ºäº†é¿å…é€»è¾‘é”™è¯¯ï¼Œå¯¹åº”äºåŒä¸€è¡Œäººå¹¶æ¥è‡ªåŒä¸€æ‘„åƒæœºçš„è½¨è¿¹ä¸åº”è¯¥åœ¨æ—¶åŸŸé‡å ï¼Œå› ä¸ºè¡Œäººåªèƒ½åœ¨ä¸€ä¸ªæ‘„åƒæœºå¸§ä¸­å‡ºç°ä¸€æ¬¡ã€‚æœ€åï¼Œæˆ‘ä»¬è·å¾—äº†æ‰€æœ‰æ£€æµ‹åˆ°çš„æ‰‹æœ¯æˆå‘˜å’Œå¯¹åº”çš„è½¨è¿¹ï¼Œæ¯ä¸ªæ‘„åƒæœºçš„idã€‚","tags":["æŠ€æœ¯"],"categories":["æŠ€æœ¯"]},{"title":"åŸºäºè‡ªå®šä½æ™ºèƒ½æ‘„åƒæœºç½‘ç»œçš„åˆ†å¸ƒå¼ç›®æ ‡è·Ÿè¸ª","path":"/2023/01/24/åŸºäºè‡ªå®šä½æ™ºèƒ½æ‘„åƒæœºç½‘ç»œçš„åˆ†å¸ƒå¼ç›®æ ‡è·Ÿè¸ª/","content":"åŸºäºè‡ªå®šä½æ™ºèƒ½æ‘„åƒæœºç½‘ç»œçš„åˆ†å¸ƒå¼ç›®æ ‡è·Ÿè¸ª Distributed Target Tracking using Self Localizing SmartCamera Networks æ‘˜è¦é’ˆå¯¹åˆ†å¸ƒå¼æ™ºèƒ½æ‘„åƒæœºï¼Œæå‡ºäº†ä¸€ç§æ–°çš„åˆ†æ•£ç›®æ ‡è·Ÿè¸ªæ–¹æ¡ˆã€‚è¯¥æ–¹æ³•å»ºç«‹åœ¨åˆ†å¸ƒå¼å®šä½åè®®ä¹‹ä¸Šï¼Œè¯¥åè®®å…è®¸æ™ºèƒ½æ‘„åƒå¤´èŠ‚ç‚¹è‡ªåŠ¨è¯†åˆ«å…·æœ‰é‡å åŸŸçš„ç›¸é‚»ä¼ æ„Ÿå™¨ï¼Œå¹¶å»ºç«‹ä¸€ä¸ªé€šä¿¡å›¾ï¼Œåæ˜ èŠ‚ç‚¹å¦‚ä½•ç›¸äº’ä½œç”¨ä»¥èåˆç½‘ç»œä¸­çš„æµ‹é‡ã€‚æ–°åè®®å°†æ£€æµ‹å’Œè·Ÿè¸ªé—®é¢˜å‡åŒ€åœ°åˆ†å¸ƒåœ¨æ•´ä¸ªç½‘ç»œä¸­ï¼Œä»¥æ— ç¼çš„æ–¹å¼è®¡ç®—ä¼ æ„Ÿå™¨çš„åˆ‡æ¢ã€‚è¯¥æ–¹æ³•è¿˜åœ¨ç½‘ç»œä¸­çš„èŠ‚ç‚¹ä¹‹é—´åˆ†é…æœ‰å…³è¢«è·Ÿè¸ªå¯¹è±¡çŠ¶æ€çš„çŸ¥è¯†ã€‚ç„¶åï¼Œè¿™äº›ä¿¡æ¯å¯ä»¥é€šè¿‡åˆ†å¸ƒå¼æŸ¥è¯¢è·å¾—ï¼Œè¯¥æŸ¥è¯¢å…è®¸ç½‘ç»œå‚ä¸è€…è®¢é˜…ä»–ä»¬å¯èƒ½æ„Ÿå…´è¶£çš„ä¸åŒç±»å‹çš„äº‹ä»¶ã€‚æ‰€æå‡ºçš„æ–¹æ¡ˆå·²ç”¨äºä½¿ç”¨è‡ªå®šä¹‰è®¾è®¡çš„æ™ºèƒ½æ‘„åƒæœºèŠ‚ç‚¹çš„é›†åˆå®æ—¶è·Ÿè¸ªç›®æ ‡ã€‚ç»™å‡ºäº†å®éªŒç»“æœã€‚ ä»‹ç»åµŒå…¥å¼æ™ºèƒ½æ‘„åƒå¤´ç³»ç»Ÿæˆæœ¬çš„é™ä½å’Œæ€§èƒ½çš„æé«˜ä½¿å¾—è€ƒè™‘å°†å…¶åº”ç”¨äºå„ç§ç›‘è§†å’Œè·Ÿè¸ªåº”ç”¨ç¨‹åºå…·æœ‰å¸å¼•åŠ›ã€‚åœ¨ä¸ä¹…çš„å°†æ¥ï¼Œå°†æœ‰å¯èƒ½åƒéƒ¨ç½²ç¯æ³¡ä¸€æ ·éƒ¨ç½²å°å‹ã€ä¸æ˜¾çœ¼çš„æ™ºèƒ½æ‘„åƒå¤´ï¼Œå¯¹å¹¿é˜”çš„åŒºåŸŸæä¾›æ— æ‰€ä¸åœ¨çš„è¦†ç›–ã€‚æˆ‘ä»¬å¯ä»¥æƒ³è±¡ä½¿ç”¨è¿™æ ·ä¸€ä¸ªç³»ç»Ÿæ¥è·Ÿè¸ªæœºåœºçš„ä¹˜å®¢ï¼Œä»ä»–ä»¬åˆ°è¾¾è·¯è¾¹å€¼æœºåˆ°ä»–ä»¬ç™»æœºçš„æ—¶é—´ã€‚åŒæ ·åœ°ï¼Œæˆ‘ä»¬å¯ä»¥ä½¿ç”¨è¿™æ ·ä¸€ä¸ªç³»ç»Ÿæ¥ç›‘æ§è€äººæˆ–ä½“å¼±å¤šç—…çš„äººåœ¨ä»–ä»¬å®¶ä¸­çš„æ´»åŠ¨ï¼Œä»¥æé«˜ä»–ä»¬çš„æŠ¤ç†è´¨é‡ã€‚ ä¸ºäº†å®ç°æˆ‘ä»¬çš„æ„¿æ™¯ï¼Œä¸€ä¸ªå¼ºå¤§çš„æ€åŠ¿æ„ŸçŸ¥æ„ŸçŸ¥ä»åˆ†å¸ƒå¼æ‘„åƒæœºçš„é›†åˆï¼Œæˆ‘ä»¬å°†éœ€è¦è§£å†³åˆ†å¸ƒå¼ä¼ æ„Ÿå’Œè·Ÿè¸ªçš„é—®é¢˜ã€‚æ›´å…·ä½“åœ°è¯´ï¼ŒæŒ‘æˆ˜å°†æ˜¯å¯é åœ°æ£€æµ‹ã€å®šä½å’Œè·Ÿè¸ªç›®æ ‡ï¼Œå› ä¸ºä»–ä»¬åœ¨ä¸€ä¸ªç”±å¤šä¸ªåˆ†å¸ƒå¼æ™ºèƒ½æ‘„åƒæœºè¦†ç›–çš„æ‰©å±•åŒºåŸŸç§»åŠ¨ã€‚ ä¸ºäº†éƒ¨ç½²è¿™äº›ç³»ç»Ÿï¼Œæˆ‘ä»¬éœ€è¦å¼€å‘æ£€æµ‹å’Œè·Ÿè¸ªçš„æ–¹æ³•ï¼Œè¿™äº›æ–¹æ³•å¯ä»¥åˆ†å¸ƒåœ¨å¤šä¸ªä¼ æ„Ÿå™¨ä¸Šï¼Œè€Œä¸éœ€è¦è¿‡å¤šçš„é€šä¿¡ã€‚è¿™äº›ç³»ç»Ÿå¿…é¡»æ˜¯å¯æ‰©å±•çš„ï¼Œä»¥å…è®¸éƒ¨ç½²å¯èƒ½æ¶‰åŠåˆ†å¸ƒåœ¨æ‰©å±•åŒºåŸŸçš„æ•°åƒä¸ªæ‘„åƒå¤´ï¼Œå¹¶ä¸”å¿…é¡»å¯¹æ•…éšœå…·æœ‰å¥å£®æ€§ï¼Œä»¥ä¾¿åœ¨å¼‚æ­¥æ·»åŠ æˆ–åˆ é™¤å•ä¸ªä¼ æ„Ÿå™¨æ—¶ï¼Œæ•´ä¸ªç³»ç»Ÿèƒ½å¤Ÿä¼˜é›…åœ°å“åº”ã€‚ æœ¬æ–‡æè¿°äº†ä¸€ç§åŸºäºå»ä¸­å¿ƒåŒ–çš„æ™ºèƒ½æ‘„åƒæœºç½‘ç»œæ£€æµ‹ä¸è·Ÿè¸ªçš„æ–°æ–¹æ³•ã€‚è¿™ç§æ–¹æ³•å»ºç«‹åœ¨å…ˆå‰çš„è‡ªå®šä½å·¥ä½œçš„åŸºç¡€ä¸Šï¼Œè¯¥å·¥ä½œå…è®¸æ™ºèƒ½æ‘„åƒæœºè‡ªåŠ¨æ£€æµ‹å’Œå®šä½å…·æœ‰é‡å å­—æ®µçš„å…¶ä»–æ‘„åƒæœºèŠ‚ç‚¹ï¼Œå¹¶å»ºç«‹åæ˜ èŠ‚ç‚¹å¦‚ä½•ç›¸äº’ä½œç”¨ä»¥èåˆç½‘ç»œä¸­çš„æµ‹é‡çš„é€šä¿¡å›¾ã€‚æˆ‘ä»¬å¼€å‘äº†å…·æœ‰æœ‰é™é€šä¿¡è¦æ±‚çš„æ–°å‹ç½‘ç»œåè®®ï¼Œå…è®¸ç³»ç»Ÿé€šè¿‡ç½‘ç»œå‡åŒ€åˆ†å¸ƒæ£€æµ‹å’Œè·Ÿè¸ªé—®é¢˜ï¼Œä»¥æ— ç¼çš„æ–¹å¼è®¡ç®—ä¼ æ„Ÿå™¨çš„åˆ‡æ¢ã€‚ ç›¸å…³å·¥ä½œ","tags":["æŠ€æœ¯"],"categories":["æŠ€æœ¯"]},{"title":"adaptive_pose","path":"/2023/01/22/adaptive-pose/","content":"AdaptivePose++ : github é—®é¢˜ z[â€˜apâ€™] è™½ç„¶è¾“å‡ºäº†ï¼Œä½†æ˜¯æ²¡æœ‰å‚ä¸æœ€ç»ˆçš„è®¡ç®—ï¼Œ è¿™ä¸ªapæ›´åƒæ˜¯ç½‘ç»œè‡ªå·±å­¦çš„ä¸€ä¸ªä¸­é—´çº§çš„è¿‡ç¨‹ï¼Œè¿™ä¸ªapæ²¡æœ‰ç›‘ç£ èƒ½å­¦åˆ°åƒè®ºæ–‡çš„ç¤ºæ„ä¹ˆã€‚ Resample2Dçš„ä½œç”¨æ˜¯å•¥ï¼Œåœ¨flownet2ä¸­æœåˆ°äº†ï¼Œä½†è¿˜æ˜¯ä¸å¤ªæ¸…æ¥šï¼Œ æ˜¯å°†ä¸åŒä½ç½®çš„ç‰¹å¾è¿›è¡Œèåˆä¹ˆã€‚ self.gradient_mulæ˜¯ä¸ºäº†æ§åˆ¶apå›ä¼ çš„æ¢¯åº¦èŒƒå›´ä¹ˆï¼Œè¿™ä¸ªæ˜¯ç»éªŒå€¼å—ã€‚ æˆ‘ä¹‹å‰åœ¨centernetä¸­åŠ å…¥oks æ›¿æ¢æ‰äº†åŸæ¥çš„RegWeightedL1Loss_coco()ï¼Œ ä½ è¿™æ˜¯é¢å¤–åŠ äº†oksçš„lossï¼Œè¿™æ²¡æœ‰é‡å¤å­¦è¿™ä¸ªå›å½’ä¿¡æ¯ä¹ˆï¼Œ å¦‚æœä¸ºäº†åŠ å¿«æ”¶æ•›å¯ä¸å¯ä»¥å…ˆRegWeightedL1Loss_coco()ï¼Œå†oksã€‚æˆ‘ä¸€ç›´å¾ˆå¥½å¥‡å¦‚æœæ˜¯è‡ªå·±çš„æ•°æ®é›†ï¼Œoksä¸­çš„sigmaä¸€èˆ¬æ€ä¹ˆä¼°è®¡å‘€ã€‚ è¿˜æœ‰oks çš„åº”è¯¥æ˜¯ä»¥ç»å¯¹ä½ç½®ä½œä¸ºè®¡ç®—å§ï¼Œ æˆ‘çœ‹ä»£ç é‡Œå¥½åƒæ˜¯ç›¸å¯¹ä¸­å¿ƒç‚¹çš„åç§»ã€‚ å›ç­” z[â€˜apâ€™] åªæ˜¯ç”¨äºå¯è§†åŒ–ï¼Œæ²¡æœ‰æ˜¾ç¤ºçš„ç›‘ç£ï¼Œadaptiveposeä½¿ç”¨ä¸­å¿ƒç‰¹å¾é¢„æµ‹apåç§»ï¼Œå†å–å‡ºapä½ç½®çš„ç‰¹å¾ç¬¬äºŒè·³åç§»ï¼Œæ•´ä¸ªä¸¤è·³pathæ˜¯æ¢¯åº¦å¯å›ä¼ çš„ï¼Œæ‰€ä»¥ç›¸å½“äºéšå¼ç›‘ç£çš„ã€‚ Resample2Då°±æ˜¯warpæ“ä½œï¼Œé€šè¿‡åŒçº¿æ€§æ’å€¼å–apä½ç½®ç‰¹å¾ã€‚self.gradient_mulè¿™å—æ„æ€è·Ÿé™ä½è¯¥å±‚çš„å­¦ä¹ ç‡ä¸€ä¸ªæ„æ€ã€‚ æˆ‘è¿™è¾¹å®éªŒæ•ˆæœ oks+L1 oks L1ã€‚å…ˆRegWeightedL1Loss_coco()ï¼Œå†oksè¿™ä¸ªæ“ä½œä½ å¯ä»¥è‡ªå·±è¯•è¯•ã€‚è‡ªå·±çš„æ•°æ®é›†å¦‚æœæ˜¯äººä½“å…³é”®ç‚¹ä½ ç›´æ¥æŒ‰ç€cocoå–å¯¹åº”ä½ç½®çš„sigmaå°±å¯ä»¥äº†ï¼Œsigmaè·Ÿæ•°æ®é›†æ— å…³ã€‚off_to_poseä¸­å°†ä¸­å¿ƒåæ ‡åŠ åˆ°åç§»ä¸Šã€‚é¦–å…ˆè®¤ä¸ºæ ‡æ³¨è¿‡ç¨‹ç¬¦åˆé«˜æ–¯åˆ†å¸ƒï¼Œsigma è·Ÿ scaleè¿™ä¿©å‚æ•°ä¹˜ç§¯ï¼Œå°±æ˜¯é«˜æ–¯åˆ†å¸ƒçš„æ–¹å·®ï¼Œç›´è§‰ä¸Šç†è§£å°±æ˜¯å¯¹åå·®çš„å®¹å¿åº¦ï¼Œæ¯”å¦‚åŒæ ·åç§»äº”ä¸ªåƒç´ ï¼Œå¯èƒ½å¯¹äºeyeçš„é¢„æµ‹è¯¯å·®å°±æ˜¯ä¸å¯å®¹å¿çš„ï¼Œå¯¹äºhipçš„é¢„æµ‹è¯¯å·®æ˜¯å¯å®¹å¿çš„ï¼Œå¯¹äºlarge scaleæ˜¯å¯å®¹å¿çš„ï¼Œå¯¹small scaleæ˜¯ä¸å¯å®¹å¿çš„ã€‚cocoä¸Šæä¾›çš„æ ‡æ³¨ï¼Œä¹Ÿæ˜¯è„¸éƒ¨å…³é”®ç‚¹çš„sigmaæœ€å°ï¼Œå…¶ä»–çš„å¤§ä¸€äº›ï¼Œä½ å¯ä»¥æŒ‰ç€è¿™ä¸ªæ€è·¯æ¥ä¼°ç®—ä¸‹ä½ æ‰€ä¼°è®¡çš„ç‚¹çš„sigmaã€‚ ä»£ç è§£æ","tags":["æŠ€æœ¯"],"categories":["æŠ€æœ¯"]},{"title":"2022å¹´å·¥ä½œæš‚æ—¶æ€§æ€»ç»“","path":"/2023/01/22/2022å¹´å·¥ä½œæš‚æ—¶æ€§æ€»ç»“/","content":"2022å¹´å·¥ä½œæš‚æ—¶æ€§æ€»ç»“1. è½¦è·¯ååŒç›®æ ‡æ£€æµ‹é¡¹ç›®å‚ä¸è½¦è·¯ååŒ ç›®æ ‡æ£€æµ‹é¡¹ç›®ä¸­çš„å¼€å‘æ”¯æŒï¼Œäººæœºéç­‰æŒ‡æ ‡è¾¾åˆ°é¡¹ç›®è¦æ±‚ 2.å¤šç›®èåˆä»£ç å¼€å‘å®ç°å¤šç›¸æœºç›®æ ‡èåˆï¼Œ ç›¸æœºåˆ‡æ¢åˆ©ç”¨ä½ç½®è¿›è¡Œè½¨è¿¹å¹³æ»‘ï¼Œ åˆ©ç”¨ç©ç›®ç›®æ ‡è¿‡æ¥äº†é›·è§†å¾å»ºç›®æ ‡ï¼Œç¡®ä¿æ— é‡å¤idè¾“å‡ºï¼Œä»¥ä¸‹æ²™çš„æ•°æ®ä¸ºä¾‹å­ï¼Œ æ ‡æ³¨äº†ä¸€ä¸ªèåˆçš„æ•°æ®é›† 3. è·¯ä¾§åœè½¦æ³Šä½åˆ’çº¿æ”¯æŒ3559a,3519a,ax630 ax620a,sd3403ç­‰å¹³å°ï¼Œå‘æµ‹æŒ‡æ ‡P90+ R 90+ 4.å…¨å›¾ç›®æ ‡å®šä½å’Œæœå‘æœå‘ï¼šæœ‰é™çš„æ•°æ®é›†æ ‡æ³¨(2-3w)ï¼Œç›®æ ‡é«˜å¤§äº1/30ï¼Œæœå‘è§’åº¦è¯¯å·®11Â°ä»¥å†…ï¼Œæ­£ç¡®ç‡ä¸º88.5%ï¼Œä¸”æ£€æµ‹æŒ‡æ ‡æµ®åŠ¨åœ¨2%ä»¥å†… å®šä½ï¼šåœ¨nxå¹³å°ä¸Šå·¥ç¨‹åŒ–å®ç°ï¼Œå·¥ç¨‹è€—æ—¶ç”±86mså‡å°‘åˆ°19msï¼Œ æŒ‡æ ‡å·®å¼‚ä¸å¤§ï¼Œåœ¨è½¦é¡¶ç‚¹ã€è·¯ä¾§åœè½¦æ•°æ®é›†ä»¥åŠè´´åœ°ç‚¹ä¼ªæ ‡ç­¾æ•°æ®ä¸Šæ¢ç´¢äº†å®šä½ä¸åŒçš„å®ç°æ–¹å¼ï¼ŒéªŒè¯å“ªäº›æ˜¯å¯è¡Œçš„å“ªäº›å¯¹å‚æ•°ä¾èµ–æ¯”è¾ƒå¤§ï¼Œå‡å°‘å¯¹äººä¸ºè®¾å®šå‚æ•°çš„ä¾èµ–ã€‚","tags":["æŠ€æœ¯"],"categories":["æŠ€æœ¯"]},{"title":"20220612è®¨è®º","path":"/2023/01/22/20220612/","content":"20220612ç”¨dockerä¾‹å­ æ€ä¹ˆæŠŠä½ çš„imageæ„å»ºçš„å¾ˆå¥½ å®¹é‡å°ï¼Œå¹¶ä¸”æ»¡è¶³å¤§éƒ¨åˆ†çš„åº”ç”¨ shmæ€ä¹ˆé…ç½® dockrfileæ€ä¹ˆå†™ æŒ‚è½½æ€ä¹ˆåš gpuæ€ä¹ˆé€‰å¡ æ€ä¹ˆåœ¨å®¹å™¨é‡Œè”è°ƒï¼Œ å®¹å™¨å‡ºäº†é—®é¢˜æ€ä¹ˆè§£å†³ åº”ç”¨åœ¨k8sä¸Šæœ‰å•¥é—®é¢˜ï¼Œéœ€è¦æ€ä¹ˆä¼˜åŒ–ã€‚ æ¯ä¸ªç‚¹éƒ½æœ‰æ·±åº¦å¯ä»¥åšï¼Œä¾æ®è‡ªå·±çš„æ€ªè¯ï¼Œé€‰æ‹©è‡ªå·±çš„åˆ‡å…¥ç‚¹ å‡å¦‚æ¨¡å‹éƒ½æ˜¯è¿‡æ‹Ÿåˆï¼Œæ€ä¹ˆå®šä½è¿™ä¸ªé—®é¢˜ è¯·é—®æ€ä¹ˆçŸ¥é“å®ƒè¿‡æ‹Ÿåˆçš„ç±»å‹ å› ä¸ºä»€ä¹ˆè¿‡æ‹Ÿåˆçš„ æ¨¡å‹åŸå› è¿˜æ˜¯æ•°æ®åŸå›  è®­ç»ƒç­–ç•¥åŸå›  è¿˜æ˜¯è¯´æå‰æ²¡æœ‰é¢„çƒ­ èŠ±ç‚¹æ—¶é—´ç ”ç©¶æ¯”è¾ƒåº•å±‚çš„ä¸œè¥¿ï¼Œ åˆ«äººä¸ºä»€ä¹ˆè¿™ä¹ˆè®¾è®¡ç®—æ³• æœ¬è´¨ä¾‹å¦‚å¤šç±»åˆ†ç±»å¤šæ ‡ç­¾åˆ†ç±»åˆ†å‰²ï¼Œè¿™äº›ä»–ä»¬çš„æœ¬è´¨æ˜¯åœ¨åšä»€ä¹ˆäº‹ å‡¸ä¼˜åŒ–å¤§æ¦‚çš„ä¸€ä¸ªåŸç† æœ€ä¼˜åŒ–å‡½æ•°çš„ä¸€ä¸ªåŸç† æœ€å°äºŒä¹˜ï¼Œåæœ€å°2ä¹˜ç­‰ç­‰ä¸€ç³»åˆ—çš„ æ‰¾åˆ°è‡ªå·±æ“…é•¿çš„ç‚¹ã€‚","tags":["æŠ€æœ¯"],"categories":["æŠ€æœ¯"]},{"title":"RANSACç®—æ³•","path":"/2023/01/22/RANSACç®—æ³•/","content":"RANSACç®—æ³• RANSAC(RAndom SAmple Consensus,éšæœºé‡‡æ ·ä¸€è‡´)ç®—æ³• RANSAC(RAndom SAmple Consensus,éšæœºé‡‡æ ·ä¸€è‡´)ç®—æ³•æ˜¯ä»ä¸€ç»„å«æœ‰â€œå¤–ç‚¹â€(outliers)çš„æ•°æ®ä¸­æ­£ç¡®ä¼°è®¡æ•°å­¦æ¨¡å‹å‚æ•°çš„è¿­ä»£ç®—æ³•ã€‚â€œå¤–ç‚¹â€ä¸€èˆ¬æŒ‡çš„çš„æ•°æ®ä¸­çš„å™ªå£°ï¼Œæ¯”å¦‚è¯´åŒ¹é…ä¸­çš„è¯¯åŒ¹é…å’Œä¼°è®¡æ›²çº¿ä¸­çš„ç¦»ç¾¤ç‚¹ã€‚æ‰€ä»¥ï¼ŒRANSACä¹Ÿæ˜¯ä¸€ç§â€œå¤–ç‚¹â€æ£€æµ‹ç®—æ³•ã€‚RANSACç®—æ³•æ˜¯ä¸€ç§ä¸ç¡®å®šç®—æ³•ï¼Œå®ƒåªèƒ½åœ¨ä¸€ç§æ¦‚ç‡ä¸‹äº§ç”Ÿç»“æœï¼Œå¹¶ä¸”è¿™ä¸ªæ¦‚ç‡ä¼šéšç€è¿­ä»£æ¬¡æ•°çš„å¢åŠ è€ŒåŠ å¤§ï¼ˆä¹‹åä¼šè§£é‡Šä¸ºä»€ä¹ˆè¿™ä¸ªç®—æ³•æ˜¯è¿™æ ·çš„ï¼‰ã€‚RANSACç®—æœ€æ—©æ˜¯ç”±Fischlerå’ŒBollesåœ¨SRIä¸Šæå‡ºç”¨æ¥è§£å†³LDP(Location Determination Proble)é—®é¢˜çš„ã€‚ å¯¹äºRANSACç®—æ³•æ¥è¯´ä¸€ä¸ªåŸºæœ¬çš„å‡è®¾å°±æ˜¯æ•°æ®æ˜¯ç”±â€œå†…ç‚¹â€å’Œâ€œå¤–ç‚¹â€ç»„æˆçš„ã€‚â€œå†…ç‚¹â€å°±æ˜¯ç»„æˆæ¨¡å‹å‚æ•°çš„æ•°æ®ï¼Œâ€œå¤–ç‚¹â€å°±æ˜¯ä¸é€‚åˆæ¨¡å‹çš„æ•°æ®ã€‚åŒæ—¶RANSACå‡è®¾ï¼šåœ¨ç»™å®šä¸€ç»„å«æœ‰å°‘éƒ¨åˆ†â€œå†…ç‚¹â€çš„æ•°æ®ï¼Œå­˜åœ¨ä¸€ä¸ªç¨‹åºå¯ä»¥ä¼°è®¡å‡ºç¬¦åˆâ€œå†…ç‚¹â€çš„æ¨¡å‹ã€‚ ç®—æ³•åŸºæœ¬æ€æƒ³å’Œæµç¨‹RANSACæ˜¯é€šè¿‡åå¤é€‰æ‹©æ•°æ®é›†å»ä¼°è®¡å‡ºæ¨¡å‹ï¼Œä¸€ç›´è¿­ä»£åˆ°ä¼°è®¡å‡ºè®¤ä¸ºæ¯”è¾ƒå¥½çš„æ¨¡å‹ã€‚å…·ä½“çš„å®ç°æ­¥éª¤å¯ä»¥åˆ†ä¸ºä»¥ä¸‹å‡ æ­¥ï¼š é€‰æ‹©å‡ºå¯ä»¥ä¼°è®¡å‡ºæ¨¡å‹çš„æœ€å°æ•°æ®é›†ï¼›(å¯¹äºç›´çº¿æ‹Ÿåˆæ¥è¯´å°±æ˜¯ä¸¤ä¸ªç‚¹ï¼Œå¯¹äºè®¡ç®—HomographyçŸ©é˜µå°±æ˜¯4ä¸ªç‚¹) ä½¿ç”¨è¿™ä¸ªæ•°æ®é›†æ¥è®¡ç®—å‡ºæ•°æ®æ¨¡å‹ï¼› å°†æ‰€æœ‰æ•°æ®å¸¦å…¥è¿™ä¸ªæ¨¡å‹ï¼Œè®¡ç®—å‡ºâ€œå†…ç‚¹â€çš„æ•°ç›®ï¼›(ç´¯åŠ åœ¨ä¸€å®šè¯¯å·®èŒƒå›´å†…çš„é€‚åˆå½“å‰è¿­ä»£æ¨å‡ºæ¨¡å‹çš„æ•°æ®) æ¯”è¾ƒå½“å‰æ¨¡å‹å’Œä¹‹å‰æ¨å‡ºçš„æœ€å¥½çš„æ¨¡å‹çš„â€œå†…ç‚¹â€œçš„æ•°é‡ï¼Œè®°å½•æœ€å¤§â€œå†…ç‚¹â€æ•°çš„æ¨¡å‹å‚æ•°å’Œâ€œå†…ç‚¹â€æ•°ï¼› é‡å¤1-4æ­¥ï¼Œç›´åˆ°è¿­ä»£ç»“æŸæˆ–è€…å½“å‰æ¨¡å‹å·²ç»è¶³å¤Ÿå¥½äº†(â€œå†…ç‚¹æ•°ç›®å¤§äºä¸€å®šæ•°é‡â€)ã€‚ è¿­ä»£æ¬¡æ•°æ¨å¯¼å‡è®¾â€œå†…ç‚¹â€åœ¨æ•°æ®ä¸­çš„å æ¯”ä¸º é‚£ä¹ˆæˆ‘ä»¬æ¯æ¬¡è®¡ç®—æ¨¡å‹ä½¿ç”¨ ä¸ªç‚¹çš„æƒ…å†µä¸‹ï¼Œé€‰å–çš„ç‚¹è‡³å°‘æœ‰ä¸€ä¸ªå¤–ç‚¹çš„æƒ…å†µå°±æ˜¯ ä¹Ÿå°±æ˜¯è¯´ï¼Œåœ¨è¿­ä»£ æ¬¡çš„æƒ…å†µä¸‹ï¼Œ å°±æ˜¯ æ¬¡è¿­ä»£è®¡ç®—æ¨¡å‹éƒ½è‡³å°‘é‡‡æ ·åˆ°ä¸€ä¸ªâ€œå¤–ç‚¹â€å»è®¡ç®—æ¨¡å‹çš„æ¦‚ç‡ã€‚é‚£ä¹ˆèƒ½é‡‡æ ·åˆ°æ­£ç¡®çš„ ä¸ªç‚¹å»è®¡ç®—å‡ºæ­£ç¡®æ¨¡å‹çš„æ¦‚ç‡å°±æ˜¯ é€šè¿‡ä¸Šå¼ï¼Œå¯ä»¥æ±‚å¾— å†…ç‚¹â€çš„æ¦‚ç‡ é€šå¸¸æ˜¯ä¸€ä¸ªå…ˆéªŒå€¼ã€‚ç„¶å æ˜¯æˆ‘ä»¬å¸Œæœ›RANSACå¾—åˆ°æ­£ç¡®æ¨¡å‹çš„æ¦‚ç‡ã€‚å¦‚æœäº‹å…ˆä¸çŸ¥é“ çš„å€¼ï¼Œå¯ä»¥ä½¿ç”¨è‡ªé€‚åº”è¿­ä»£æ¬¡æ•°çš„æ–¹æ³•ã€‚ä¹Ÿå°±æ˜¯ä¸€å¼€å§‹è®¾å®šä¸€ä¸ªæ— ç©·å¤§çš„è¿­ä»£æ¬¡æ•°ï¼Œç„¶åæ¯æ¬¡æ›´æ–°æ¨¡å‹å‚æ•°ä¼°è®¡çš„æ—¶å€™ï¼Œç”¨å½“å‰çš„â€œå†…ç‚¹â€æ¯”å€¼å½“æˆ æ¥ä¼°ç®—å‡ºè¿­ä»£æ¬¡æ•°ã€‚ ç”¨Pythonå®ç°ç›´çº¿æ‹Ÿåˆimport numpy as npimport matplotlib.pyplot as pltimport randomimport math# æ•°æ®é‡ã€‚SIZE = 50# äº§ç”Ÿæ•°æ®ã€‚np.linspace è¿”å›ä¸€ä¸ªä¸€ç»´æ•°ç»„ï¼ŒSIZEæŒ‡å®šæ•°ç»„é•¿åº¦ã€‚# æ•°ç»„æœ€å°å€¼æ˜¯0ï¼Œæœ€å¤§å€¼æ˜¯10ã€‚æ‰€æœ‰å…ƒç´ é—´éš”ç›¸ç­‰ã€‚X = np.linspace(0, 10, SIZE)Y = 3 * X + 10fig = plt.figure()# ç”»å›¾åŒºåŸŸåˆ†æˆ1è¡Œ1åˆ—ã€‚é€‰æ‹©ç¬¬ä¸€å—åŒºåŸŸã€‚ax1 = fig.add_subplot(1,1, 1)# æ ‡é¢˜ax1.set_title(\"RANSAC\")# è®©æ•£ç‚¹å›¾çš„æ•°æ®æ›´åŠ éšæœºå¹¶ä¸”æ·»åŠ ä¸€äº›å™ªå£°ã€‚random_x = []random_y = []# æ·»åŠ ç›´çº¿éšæœºå™ªå£°for i in range(SIZE): random_x.append(X[i] + random.uniform(-0.5, 0.5)) random_y.append(Y[i] + random.uniform(-0.5, 0.5)) # æ·»åŠ éšæœºå™ªå£°for i in range(SIZE): random_x.append(random.uniform(0,10)) random_y.append(random.uniform(10,40))RANDOM_X = np.array(random_x) # æ•£ç‚¹å›¾çš„æ¨ªè½´ã€‚RANDOM_Y = np.array(random_y) # æ•£ç‚¹å›¾çš„çºµè½´ã€‚# ç”»æ•£ç‚¹å›¾ã€‚ax1.scatter(RANDOM_X, RANDOM_Y)# æ¨ªè½´åç§°ã€‚ax1.set_xlabel(\"x\")# çºµè½´åç§°ã€‚ax1.set_ylabel(\"y\")# ä½¿ç”¨RANSACç®—æ³•ä¼°ç®—æ¨¡å‹# è¿­ä»£æœ€å¤§æ¬¡æ•°ï¼Œæ¯æ¬¡å¾—åˆ°æ›´å¥½çš„ä¼°è®¡ä¼šä¼˜åŒ–itersçš„æ•°å€¼iters = 100000# æ•°æ®å’Œæ¨¡å‹ä¹‹é—´å¯æ¥å—çš„å·®å€¼sigma = 0.25# æœ€å¥½æ¨¡å‹çš„å‚æ•°ä¼°è®¡å’Œå†…ç‚¹æ•°ç›®best_a = 0best_b = 0pretotal = 0# å¸Œæœ›çš„å¾—åˆ°æ­£ç¡®æ¨¡å‹çš„æ¦‚ç‡P = 0.99for i in range(iters): # éšæœºåœ¨æ•°æ®ä¸­çº¢é€‰å‡ºä¸¤ä¸ªç‚¹å»æ±‚è§£æ¨¡å‹ sample_index = random.sample(range(SIZE * 2),2) x_1 = RANDOM_X[sample_index[0]] x_2 = RANDOM_X[sample_index[1]] y_1 = RANDOM_Y[sample_index[0]] y_2 = RANDOM_Y[sample_index[1]] # y = ax + b æ±‚è§£å‡ºaï¼Œb a = (y_2 - y_1) / (x_2 - x_1) b = y_1 - a * x_1 # ç®—å‡ºå†…ç‚¹æ•°ç›® total_inlier = 0 for index in range(SIZE * 2): y_estimate = a * RANDOM_X[index] + b if abs(y_estimate - RANDOM_Y[index]) sigma: total_inlier = total_inlier + 1 # åˆ¤æ–­å½“å‰çš„æ¨¡å‹æ˜¯å¦æ¯”ä¹‹å‰ä¼°ç®—çš„æ¨¡å‹å¥½ if total_inlier pretotal: iters = math.log(1 - P) / math.log(1 - pow(total_inlier / (SIZE * 2), 2)) pretotal = total_inlier best_a = a best_b = b # åˆ¤æ–­æ˜¯å¦å½“å‰æ¨¡å‹å·²ç»ç¬¦åˆè¶…è¿‡ä¸€åŠçš„ç‚¹ if total_inlier SIZE: break# ç”¨æˆ‘ä»¬å¾—åˆ°çš„æœ€ä½³ä¼°è®¡ç”»å›¾Y = best_a * RANDOM_X + best_b# ç›´çº¿å›¾ax1.plot(RANDOM_X, Y)text = \"best_a = \" + str(best_a) + \" best_b = \" + str(best_b)plt.text(5,10, text, fontdict={'size': 8, 'color': 'r'})plt.show()","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"shellå­¦ä¹ ","path":"/2023/01/22/shellå­¦ä¹ /","content":"shellå­¦ä¹ 1. shellæ¦‚è¿°Shell æ˜¯ä¸€ä¸ªå‘½ä»¤è§£é‡Šå™¨ï¼Œ ç”¨äºæ¥æ”¶åº”ç”¨ç¨‹åº/ç”¨æˆ·å‘½ä»¤ï¼Œ ç„¶åè°ƒç”¨æ“ä½œç³»ç»Ÿå†…æ ¸ã€‚ shell è¿˜æ˜¯ä¸€ä¸ªåŠŸèƒ½ç›¸å½“å¼ºå¤§çš„ç¼–ç¨‹è¯­è¨€ï¼Œ æ˜“ç¼–å†™ï¼Œæ˜“è°ƒè¯•ï¼Œçµæ´»æ€§å¼ºã€‚ 2. shellè§£æå™¨/bin/sh/bin/bash/usr/bin/bash/bin/rbash/usr/bin/rbash/usr/bin/sh/bin/dash/usr/bin/dash root@2a63e139ac66:/bin# echo $SHELL/bin/bash ç³»ç»Ÿé»˜è®¤çš„æ˜¯bash 3. Shell è„šæœ¬å…¥é—¨3.1 è„šæœ¬æ ¼å¼è„šæœ¬ä»¥#!/bin/bash å¼€å¤´ï¼ˆæŒ‡å®šè§£æå™¨ï¼‰ 3.2 ç¬¬ä¸€ä¸ªshellè„šæœ¬ï¼š helloworld3.2.1 éœ€æ±‚ï¼š åˆ›å»ºä¸€ä¸ªshellè„šæœ¬ï¼Œ è¾“å‡ºhelloworld#!/bin/bashecho \"helloworld shell\" cd shelldata# ls# pwd/root/shelldata# touch helloworld.sh# vim helloworld.sh### sh helloworld.shhelloworld shell æƒé™ä¸å¤Ÿ # ./ helloworld.sh/bin/sh: 33: ./: Permission denied 3.2.2 è„šæœ¬çš„å¸¸ç”¨æ‰§è¡Œæ–¹å¼ç¬¬ä¸€ç§ï¼š é‡‡ç”¨bashæˆ–è€…sh+è„šæœ¬çš„ç›¸å¯¹è·¯å¾„æˆ–è€…ç»å¯¹è·¯å¾„ï¼ˆä¸ç”¨èµ‹äºˆè„šæœ¬+xæƒé™ï¼‰ ç¬¬äºŒç§ï¼š é‡‡ç”¨è¾“å…¥è„šæœ¬çš„ç»å¯¹è·¯å¾„æˆ–è€…ç›¸å¯¹è·¯å¾„æ‰§è¡Œè„šæœ¬ï¼ˆå¿…é¡»å…·æœ‰å¯æ‰§è¡Œæƒé™+xï¼‰ æ³¨æ„ï¼š ç¬¬ä¸€ç§æ‰§è¡Œæ–¹æ³•ï¼Œ æœ¬è´¨æ˜¯bashè§£æå™¨å¸®ä½ æ‰§è¡Œè„šæœ¬ï¼Œ æ‰€ä»¥è„šæœ¬æœ¬èº«ä¸éœ€è¦æ‰§è¡Œæƒé™ï¼Œ ç¬¬äºŒç§æ‰§è¡Œæ–¹æ³•ï¼Œ æœ¬è´¨æ˜¯è„šæœ¬è‡ªå·±æ‰§è¡Œï¼Œæ‰€ä»¥éœ€è¦æ‰§è¡Œæƒé™ã€‚ 3.3 ç¬¬äºŒä¸ªshellè„šæœ¬ï¼š å¤šå‘½ä»¤å¤„ç†3.3.1 éœ€æ±‚ï¼šåœ¨/home/atguigu/ç›®å½•ä¸‹åˆ›å»ºä¸€ä¸ªbangzhang.txt, åœ¨banzhang.txtä¸­å¢åŠ â€œI Love clsâ€ 3.3.2 æ¡ˆä¾‹å®æ“#!/bin/bashcd /root/shelldata/touch banzhang.txtecho \"I Love cls\" banzhang.txt ç»“æœæ˜¾ç¤º sh-5.1# cat banzhang.txtI Love clsI Love clssh-5.1# rm banzhang.txtsh-5.1# bash hadoop101.shsh-5.1# cat banzhang.txtI Love clssh-5.1# vim hadoop101.sh 4. Shellä¸­çš„å˜é‡4.1 ç³»ç»Ÿå˜é‡4.1.1 å¸¸ç”¨ç³»ç»Ÿå˜é‡$HOMEã€$PWDã€$SHELLã€$USERç­‰ 4.1.2 æ¡ˆä¾‹å®æ“æŸ¥çœ‹ç³»ç»Ÿå˜é‡çš„å€¼ sh-5.1# echo $HOME/rootsh-5.1# echo $PWD/root/shelldatash-5.1# æ˜¾ç¤ºå½“å‰shellä¸­æ‰€æœ‰å˜é‡ BASH=/bin/shBASHOPTS=checkwinsize:cmdhist:complete_fullquote:expand_aliases:extquote:force_fignore:globasciiranges:hostcomplete:interactive_comments:progcomp:promptvars:sourcepathBASH_ALIASES=()BASH_ARGC=()BASH_ARGV=()BASH_CMDS=()BASH_LINENO=()BASH_SOURCE=()BASH_VERSINFO=([0]=\"5\" [1]=\"1\" [2]=\"16\" [3]=\"1\" [4]=\"release\" [5]=\"x86_64-pc-linux-gnu\")BASH_VERSION='5.1.16(1)-release'COLUMNS=177DIRSTACK=()EUID=0GROUPS=()HISTFILE=/root/.bash_historyHISTFILESIZE=500HISTSIZE=500HOME=/rootHOSTNAME=8b25e507bccaHOSTTYPE=x86_64IFS=''LINES=50MACHTYPE=x86_64-pc-linux-gnuMAILCHECK=60OLDPWD=/rootOPTERR=1OPTIND=1OSTYPE=linux-gnuPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binPIPESTATUS=([0]=\"0\")POSIXLY_CORRECT=yPPID=0PS1='\\s-\\v\\$ 'PS2=' 'PS4='+ 'PWD=/root/shelldataSHELL=/bin/bashSHELLOPTS=braceexpand:emacs:hashall:histexpand:history:interactive-comments:monitor:posixSHLVL=1TERM=xtermUID=0_=/root/shelldata 4.2 è‡ªå®šä¹‰å˜é‡4.2.1 åŸºæœ¬è¯­æ³•å®šä¹‰å˜é‡ï¼š å˜é‡=å€¼ sh-5.1# A=1sh-5.1# echo AAsh-5.1# echo $A æ’¤é”€å˜é‡ï¼š unsetå˜é‡ sh-5.1# unset Ash-5.1# echo $Ash-5.1# å£°æ˜ä¸€ä¸ªé™æ€å˜é‡ï¼š readonlyå˜é‡ï¼Œ æ³¨æ„ï¼šä¸èƒ½unset sh-5.1# readonly B=3sh-5.1# echo $Bsh-5.1# unset Bsh: unset: B: cannot unset: readonly variablesh-5.1# 4.2.2 å˜é‡å®šä¹‰è§„åˆ™ å˜é‡åç§°å¯ä»¥ç”±å­—æ¯ã€æ•°å­—å’Œä¸‹åˆ’çº¿æ„æˆï¼Œ ä½†æ˜¯ä¸èƒ½ä»¥æ•°å­—å¼€å¤´ï¼Œ ç¯å¢ƒå˜é‡åå»ºè®®å¤§å†™ ç­‰å·ä¸¤ä¾§ä¸èƒ½æœ‰ç©ºæ ¼ åœ¨bashä¸­ï¼Œ å˜é‡é»˜è®¤ç±»å‹éƒ½æ˜¯å­—ç¬¦ä¸²ç±»å‹ï¼Œ æ— æ³•ç›´æ¥è¿›è¡Œæ•°å€¼è¿ç®—ã€‚ sh-5.1# C=1+1sh-5.1#Display all 505 possibilities? (y or n)sh-5.1# echo $C1+1sh-5.1# å˜é‡çš„å€¼å¦‚æœæœ‰ç©ºæ ¼ï¼Œéœ€è¦ç”¨åŒå¼•å·æˆ–è€…å•å¼•å·æ‹¬èµ·æ¥ã€‚ sh-5.1# D=bangzhang love mmsh: love: command not foundsh-5.1# D='bangzhang love mm'sh-5.1# echo $Dbangzhang love mmsh-5.1# å¯ä»¥æŠŠå˜é‡æå‡ä¸ºå…¨å±€ç¯å¢ƒå˜é‡ï¼Œ å¯ä¾›å…¶ä»–shellç¨‹åºä½¿ç”¨ export å˜é‡å sh: ./helloworld.sh: Permission deniedsh-5.1# chmod +x helloworld.shsh-5.1# ./helloworld.shhelloworld shellsh-5.1# export Dsh-5.1# ./helloworld.shhelloworld shellbangzhang love mm 4.3 ç‰¹æ®Šå˜é‡ï¼š$n4.3.1 åŸºæœ¬è¯­æ³•$nï¼ˆåŠŸèƒ½æè¿°ï¼šnä¸ºæ•°å­—ï¼Œ$0ä»£è¡¨è¯¥è„šæœ¬åç§°ï¼Œ$1-$9ä»£è¡¨ç¬¬ä¸€åˆ°ç¬¬ä¹ä¸ªå‚æ•°ï¼Œ åä»¥ä¸Šçš„å‚æ•°éœ€ç”¨å¤§æ‹¬å·åŒ…å«ï¼Œå¦‚${10}ï¼‰ 4.3.2 æ¡ˆä¾‹å®æ“ è¾“å‡ºè¯¥è„šæœ¬æ–‡ä»¶åç§°ã€è¾“å…¥å‚æ•°1å’Œè¾“å…¥å‚æ•°2çš„å€¼ #!/bin/bashecho \"$0 $1 $2\"~ sh-5.1# vim parameter.shsh-5.1# bash parameter.shparameter.shsh-5.1# bash parameter.sh banzhangparameter.sh banzhangsh-5.1# bash parameter.sh banzhang lobveparameter.sh banzhang lobvesh-5.1# bash parameter.sh banzhang lobve mmparameter.sh banzhang lobvesh-5.1# vim parameter.sh ~ 4.4 ç‰¹æ®Šå˜é‡ï¼š$#4.4.1 åŸºæœ¬è¯­æ³•$# (åŠŸèƒ½æè¿°ï¼š è·å–æ‰€æœ‰è¾“å…¥å‚æ•°çš„ä¸ªæ•°ï¼Œ å¸¸ç”¨äºå¾ªç¯ï¼‰ #!/bin/bashecho \"$0 $1 $2\"echo $#~ Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·sh-5.1# vim parameter.shsh-5.1# lsbanzhang.txt hadoop101.bahs hadoop101.sh helloworld.sh parameter.shsh-5.1# chmod +x helloworld.shsh-5.1# chmod 777 parameter.shsh-5.1# ./parameter.sh cls xyz 111./parameter.sh cls xyzsh-5.1# 4.5 ç‰¹æ®Šå˜é‡ï¼š$*ã€$@$* ï¼ˆåŠŸèƒ½æè¿°ï¼š è¿™ä¸ªå˜é‡ä»£è¡¨å‘½ä»¤è¡Œä¸­æ‰€æœ‰çš„å‚æ•°ï¼Œ$*æŠŠæ‰€æœ‰çš„å‚æ•°çœ‹æˆä¸€ä¸ªæ•´ä½“ï¼‰ $@ï¼ˆåŠŸèƒ½æè¿°ï¼š è¿™ä¸ªå˜é‡ä¹Ÿä»£è¡¨å‘½ä»¤è¡Œä¸­æ‰€æœ‰çš„å‚æ•°ï¼Œ åªä¸è¿‡$@æŠŠæ¯ä¸ªå‚æ•°åŒºåˆ†å¯¹å¾…ï¼‰ #!/bin/bashecho \"$0 $1 $2\"echo $#echo $*echo $@Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·sh-5.1# ./parameter.sh banhh lobe 1111./parameter.sh banhh lobebanhh lobe 1111banhh lobe 1111 4.6 ç‰¹æ®Šå˜é‡$?$? (åŠŸèƒ½æè¿°ï¼š æœ€åä¸€æ¬¡æ‰§è¡Œçš„å‘½ä»¤çš„è¿”å›çŠ¶æ€ã€‚å¦‚æœè¿™ä¸ªå˜é‡çš„å€¼ä¸º0è¡¨ç¤ºä¸Šä¸ªå‘½ä»¤æ‰§è¡Œæ­£ç¡®ï¼›å¦‚æœä¸ºéé›¶ï¼ˆå…·ä½“å“ªä¸ªå€¼ï¼Œç”±å‘½ä»¤è‡ªå·±å†³å®š)ï¼Œè¯æ˜ä¸Šä¸€ä¸ªå‘½ä»¤æ‰§è¡Œä¸æ­£ç¡®äº†ã€‚ è¯æ˜helloworldè„šæœ¬ æ˜¯å¦æ­£ç¡®æ‰§è¡Œsh-5.1# ./helloworld.shhelloworld shellsh-5.1# echo $?sh-5.1# ç¬¬5ç«  è¿ç®—ç¬¦5.1 åŸºæœ¬è¯­æ³•â€œ$((è¿ç®—å¼))â€ æˆ–â€œ$[è¿ç®—å¼]â€ expr +,- ,\\*,/,% åŠ ã€å‡ã€ä¹˜ã€é™¤ï¼Œå–ä½™ æ³¨æ„ï¼š exprè¿ç®—ç¬¦é—´è¦æœ‰ç©ºæ ¼ è®¡ç®—3+2çš„å€¼ sh-5.1# expr 2 + 3 è®¡ç®—3-2 çš„å€¼ sh-5.1# expr 3 - 2sh-5.1# è®¡ç®—ï¼ˆ2+3ï¼‰*4çš„å€¼ sh: 1: command not foundsh-5.1# expr 'expr 2 + 3' \\* 4expr: non-integer argumenté‡‡ç”¨ $[è¿ç®—å¼]æ–¹å¼sh-5.1# s=$[(2+3)*4]sh-5.1# echo $s ç¬¬å…­ç«  æ¡ä»¶åˆ¤æ–­6.1 åŸºæœ¬è¯­æ³•[ condition ] ï¼ˆæ³¨æ„ condition å‰åè¦æœ‰ç©ºæ ¼ï¼‰ æ³¨æ„ï¼š æ¡ä»¶éç©ºå³ä¸ºtrueï¼Œ [ atguigu ] è¿”å›trueï¼Œ []è¿”å›false 6.2 å¸¸ç”¨åˆ¤æ–­æ¡ä»¶ ä¸¤ä¸ªæ•´æ•°ä¹‹é—´æ¯”è¾ƒ = å­—ç¬¦ä¸²æ¯”è¾ƒ -lt å°äºï¼ˆless than) -le å°äºç­‰äº(less equal) -eq ç­‰äºï¼ˆequalï¼‰ -gt å¤§äº (greater than) -ge å¤§äºç­‰äºï¼ˆgreater equalï¼‰ -ne ä¸ç­‰äº(Not equal) æŒ‰ç…§æ–‡ä»¶æƒé™è¿›è¡Œåˆ¤æ–­ â€‹ -r æœ‰è¯»çš„æƒé™(read) -w æœ‰å†™çš„æƒé™(write) â€‹ -x æœ‰æ‰§è¡Œçš„æƒé™(execute) æŒ‰ç…§æ–‡ä»¶ç±»å‹è¿›è¡Œåˆ¤æ–­ -f æ–‡ä»¶å­˜åœ¨ å¹¶ä¸”æ˜¯ä¸€ä¸ªå¸¸è§„çš„æ–‡ä»¶(file) -e æ–‡ä»¶å­˜åœ¨ï¼ˆexistenceï¼‰ -d æ–‡ä»¶å­˜åœ¨å¹¶ä¸”æ˜¯ä¸€ä¸ªç›®å½•(directory) 23 æ˜¯å¦å¤§äºç­‰äº22sh-5.1# [ 23 -ge 22 ]sh-5.1# echo $?helloworld.sh æ˜¯å¦å…·æœ‰å†™æƒé™sh-5.1# [ -w helloworld.sh ]sh-5.1# echo $? å¤šæ¡ä»¶åˆ¤æ–­( è¡¨ç¤ºå‰é¢ä¸€æ¡å‘½ä»¤æ‰§è¡ŒæˆåŠŸæ—¶å€™ï¼Œæ‰æ‰§è¡Œåä¸€æ¡å‘½ä»¤ï¼Œ || è¡¨ç¤ºä¸Šä¸€æ¡å‘½ä»¤æ‰§è¡Œå¤±è´¥åï¼Œæ‰æ‰§è¡Œä¸‹ä¸€æ¡å‘½ä»¤) sh-5.1# [ condition ] echo OK || echo notokOKsh-5.1# [ ] echo OK || echo notoknotok ç¬¬ä¸ƒç«  æµç¨‹æ§åˆ¶7.1 if åˆ¤æ–­if [ æ¡ä»¶åˆ¤æ–­å¼ ]:then â€‹ ç¨‹åº fi æˆ–è€… if [ æ¡ä»¶åˆ¤æ–­å¼ ] â€‹ then â€‹ ç¨‹åº fi æ³¨æ„äº‹é¡¹ï¼š [ æ¡ä»¶åˆ¤æ–­å¼ ]ï¼Œ ä¸­æ‹¬å·å’Œæ¡ä»¶åˆ¤æ–­å¼ä¹‹é—´å¿…é¡»æœ‰ç©ºæ ¼ if åè¦æœ‰ç©ºæ ¼ è¾“å…¥ä¸€ä¸ªæ•°å­—ï¼Œ å¦‚æœæ˜¯1 ï¼Œ åˆ™è¾“å‡ºbanzhang zhenshuaiï¼Œ å¦‚æœæ˜¯2ï¼Œåˆ™è¾“å‡ºshell meiï¼Œå¦‚æœæ˜¯å…¶ä»–ï¼Œåˆ™ä»€ä¹ˆä¹Ÿä¸è¾“å‡º#!/bin/bashif [ $1 -eq 1 ]then echo \"banzhangzhenshuai\"elif [ $1 -eq 2 ]then echo \"shell mei\"fish-5.1# bash if.shif.sh: line 3: [: -eq: unary operator expectedif.sh: line 7: [: -eq: unary operator expectedsh-5.1# bash if.sh 1banzhangzhenshuaish-5.1# bash if.sh \\2shell meish-5.1# bash if.sh 2shell mei 7.2 case æ–­å¥case $å˜é‡å in â€œå€¼1â€ï¼‰ â€‹ å¦‚æœå˜é‡çš„å€¼ç­‰äº1ï¼Œåˆ™æ‰§è¡Œç¨‹åº1 ;; â€œå€¼2â€ï¼‰ â€‹ å¦‚æœå˜é‡çš„å€¼ç­‰äº2ï¼Œåˆ™æ‰§è¡Œç¨‹åº2 ;; â€¦çœç•¥å…¶ä»–åˆ†æ”¯â€¦ *ï¼‰ å¦‚æœå˜é‡çš„å€¼éƒ½ä¸æ˜¯ä»¥ä¸Šçš„å€¼ï¼Œåˆ™æ‰§è¡Œæ¬¡ç¨‹åº ;; esac æ³¨æ„äº‹é¡¹ï¼š caseè¡Œç»“å°¾å¿…é¡»ä¸ºå•è¯â€œinâ€ ï¼Œ æ¯ä¸€ä¸ªæ¨¡å¼åŒ¹é…å¿…é¡»ä»¥å³æ‹¬å·â€œï¼‰â€ ç»“æŸã€‚ åŒåˆ†å·â€œ;;â€è¡¨ç¤ºå‘½ä»¤åºåˆ—ç»“æŸï¼Œç›¸å½“äºjavaä¸­çš„break æœ€åçš„â€œ*ï¼‰â€è¡¨ç¤ºé»˜è®¤æ¨¡å¼ï¼Œ ç›¸å½“äºjavaä¸­çš„default è¾“å…¥ä¸€ä¸ªæ•°å­—ï¼Œå¦‚æœæ˜¯1 åˆ™è¾“å‡ºbangzag ï¼Œå¦‚æœæ˜¯2 åˆ™è¾“å‡ºjavaï¼Œå¦‚æœæ˜¯å…¶ä»–åˆ™è¾“å‡ºpython#!/bin/bashcase $1 in1) echo \"c++\";;2) echo \"java\";;*) echo \"python\";;esacsh-5.1# vim case.shsh-5.1# bash case.sh 2case.sh: line 10: syntax error: unexpected end of filesh-5.1# vim case.shsh-5.1# bash case.sh 2javash-5.1# bash case.sh 3pythonsh-5.1# bash case.sh 1c++ 7.3 forå¾ªç¯for (( åˆå§‹å€¼;å¾ªç¯æ§åˆ¶æ¡ä»¶;å˜é‡å˜åŒ– )) do â€‹ ç¨‹åº done ä»1 åŠ åˆ°100#!/bin/bashs=0for(( i=0;i=100;i++ ))do s=$[$s+$i]doneecho $ssh-5.1# vim for1.shsh-5.1# bash for1.sh è¯­æ³•2 for å˜é‡ in å€¼1 å€¼2 å€¼3â€¦ do â€‹ ç¨‹åº done æ‰“å°æ‰€æœ‰è¾“å…¥å‚æ•°#!/bin/bashfor i in $*do echo \"shell love $i\"donefor k in \"$*\"do echo \"shell love $k\"donefor j in \"$@\"do echo \"shell love $j\"donesh-5.1# touch for2.shsh-5.1# vim for2.shsh-5.1# sh for2.sh 1 2 3shell love 1shell love 2shell love 3sh-5.1# sh for2.sh 1 2 3shell love 1shell love 2shell love 3shell love 1 2 3shell love 1shell love 2shell love 3 7.4 whileå¾ªç¯while [ æ¡ä»¶åˆ¤æ–­å¼ ] do ç¨‹åº done ä»1+åˆ°100#!/bin/bashs=0i=1while [ $i -le 100 ]do s=$[$s + $i] i=$[$i + 1]doneecho $s~ sh-5.1# bash while.sh ç¬¬å…«ç«  read è¯»å–æ§åˆ¶å°è¾“å…¥read(é€‰é¡¹)(å‚æ•°) é€‰é¡¹ï¼š -pï¼š æŒ‡å®šè¯»å–å€¼æ—¶çš„æç¤ºç¬¦ï¼š -t : æŒ‡å®šè¯»å–å€¼æ—¶ç­‰å¾…çš„æ—¶é—´ï¼ˆç§’ï¼‰ å‚æ•° â€‹ å˜é‡ï¼šæŒ‡å®šè¯»å–å€¼å¾—å˜é‡å æç¤º7så†…è¯»å–æ§åˆ¶å°è¾“å…¥çš„åç§° #!/bin/bashread -t 7 -p \"Enter your name i 7 seconds \" NAMEecho $NAME~ sh-5.1# bash read.sh shellEnter your name i 7 seconds shellshell ç¬¬ä¹ç«  å‡½æ•°9.1 ç³»ç»Ÿå‡½æ•°basenameåŸºæœ¬è¯­æ³• basename [string/pathname] [suffix] (åŠŸèƒ½æè¿°ï¼š basename çš„å‘½ä»¤ä¼šåˆ æ‰æ‰€æœ‰å‰ç¼€åŒ…æ‹¬æœ€åä¸€ä¸ªï¼ˆâ€™/â€˜)å­—ç¬¦ï¼Œç„¶åå°†å­—ç¬¦ä¸²æ˜¾ç¤ºå‡ºæ¥ é€‰é¡¹ï¼š suffixä¸ºåç¼€ï¼Œ å¦‚æœsuffixè¢«æŒ‡å®šäº†ï¼Œ basennameä¼šå°†pathnameæˆ–è€…stringä¸­çš„suffixå»æ‰ã€‚ æˆªå–è¯¥/homeshell/banzhang.txtè·¯å¾„çš„æ–‡ä»¶åç§°sh-5.1# basename /homeshell/banzhang.txt .txtbanzhang dirname æ–‡ä»¶ç»å¯¹è·¯å¾„ ( åŠŸèƒ½æè¿°ï¼š ä»ç»™å®šçš„åŒ…å«ç»å¯¹è·¯å¾„ä¸­å–å‡ºæ–‡ä»¶åï¼ˆéç›®å½•çš„éƒ¨åˆ†)ï¼Œç„¶åè¿”å›å‰©ä¸‹çš„è·¯å¾„) è·å–æˆªå–è¯¥/homeshell/banzhang.txtè·¯å¾„çš„æ–‡ä»¶è·¯å¾„sh-5.1# dirname /homeshell/banzhang.txt/homeshell 9.2 è‡ªå®šä¹‰å‡½æ•°åŸºæœ¬è¯­æ³• [ function ] funname[()] { â€‹ Actionï¼› â€‹ [return int;] } ç»éªŒæŠ€å·§ 2.1 å¿…é¡»åœ¨è°ƒç”¨åœ°æ–¹ä¹‹å‰ï¼Œå…ˆå£°æ˜å‡½æ•°ï¼Œ shellè„šæœ¬æ˜¯é€è¡Œè¿è¡Œçš„ï¼Œ ä¸ä¼šåƒå…¶ä»–è¯­è¨€ä¸€æ · å…ˆç¼–è¯‘ã€‚ 2.2 å‡½æ•°è¿”å›å€¼ï¼Œ åªèƒ½é€šè¿‡$?ç³»ç»Ÿå˜é‡æˆ–å¾—ï¼Œå¯ä»¥æ˜¾ç¤ºåŠ :return è¿”å›ï¼Œå¦‚æœä¸åŠ ï¼Œå°†ä»¥æœ€åä¸€æ¡å‘½ä»¤è¿è¡Œç»“æœä½œä¸ºè¿”å›å€¼ï¼Œreturnåè·Ÿæ•°å€¼n(0-255) è®¡ç®—ä¸¤ä¸ªè¾“å…¥å‚æ•°çš„å’Œ#!/bin/bashfunction sum(){ s=0; s=$[$1+$2] echo $s}read -p \"input your paramter1: \" P1read -p \"input your paramter1: \" P2sum $P1 $P2sh-5.1# vim sum.shsh-5.1# bash sum.shinput your paramter1:input your paramter1:sum.sh: line 6: +: syntax error: operand expected (error token is \"+\")sh-5.1# bash sum.shinput your paramter1: 10input your paramter1: 20 ç¬¬åç«  shellå·¥å…·ï¼ˆé‡ç‚¹ï¼‰10.1 cutcutçš„å·¥ä½œå°±æ˜¯â€œå‰ªâ€ï¼Œ å…·ä½“çš„è¯´å°±æ˜¯åœ¨æ–‡ä»¶ä¸­è´Ÿè´£å‰ªåˆ‡æ•°æ®ç”¨çš„ã€‚cutå‘½ä»¤ä»æ–‡ä»¶çš„æ¯ä¸€è¡Œå‰ªåˆ‡å­—èŠ‚ã€å­—ç¬¦å’Œå­—æ®µå¹¶å°†è¿™äº›å€¼è¾“å‡º åŸºæœ¬ç”¨æ³• â€‹ cut[é€‰é¡¹å‚æ•°] filenames â€‹ è¯´æ˜: é»˜è®¤åˆ†å‰²ç¬¦æ˜¯åˆ¶è¡¨ç¬¦ é€‰é¡¹å‚æ•°è¯´æ˜ æ•°æ®å‡†å¤‡sh-5.1# touch cut.txtsh-5.1# vim cut.txtsh-5.1# cat cut.txtdong shengguan zhengwo wola lalei leish-5.1# cut -d \" \" -f 1 cut.txtdongguanwolaleiåœ¨cutæ–‡ä»¶ä¸­åˆ‡å‰²å‡ºguanh-5.1# cat cut.txt | grep guanguan zhengsh-5.1# cat cut.txt | grep guan | cut -d \" \" -f 1guané€‰å–ç³»ç»ŸPATHå˜é‡å€¼ï¼Œ ç¬¬äºŒä¸ªâ€œï¼šâ€å¼€å§‹åçš„æ‰€æœ‰è·¯å¾„ï¼šsh-5.1# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binsh-5.1# echo $PATH | cut -d : -f 3-/usr/sbin:/usr/bin:/sbin:/binåˆ‡å‰²ifconfigåæ‰“å°çš„ipåœ°å€å¤±è´¥äº†ã€‚sh-5.1# ifconfig eth0 | grep \"inet\" | cut -d '' -f 2 inet 172.17.0.2 netmask 255.255.0.0 broadcast 172.17.255.255sh-5.1# 10.2 sed sedæ˜¯ä¸€ç§æµç¼–è¾‘å™¨ï¼Œ å®ƒä¸€æ¬¡å¤„ç†ä¸€è¡Œå†…å®¹ï¼Œ å¤„ç†æ—¶ï¼Œå§å½“å‰å¤„ç†çš„è¡Œå­˜å‚¨åœ¨ä¸´æ—¶ç¼“å­˜åŒºä¸­ï¼Œ ç§°ä¸ºï¼šæ¨¡å¼ç©ºé—´ï¼Œæ¥ç€ç”¨sedå‘½ä»¤å¤„ç†ç¼“å†²åŒºä¸­çš„å†…å®¹ï¼Œ å¤„ç†å®Œæˆåï¼ŒæŠŠç¼“å†²åŒºçš„å†…å®¹é€å¾€å±å¹•ï¼Œ æ¥ç€å¤„ç†ä¸‹ä¸€è¡Œï¼Œ è¿™æ ·ä¸æ–­é‡å¤ï¼Œ ç›´åˆ°æ–‡ä»¶æœ«å°¾ã€‚æ–‡ä»¶å†…å®¹æ²¡æœ‰æ”¹å˜ï¼Œ é™¤éä½ ä½¿ç”¨é‡å®šå‘å­˜å‚¨è¾“å‡ºã€‚ åŸºæœ¬ç”¨æ³• sed[é€‰é¡¹å‚æ•°] â€œcommandâ€ filename 10.2.2 å®æˆ˜å°†mei nv è¿™ä¸ªå•è¯æ’å…¥åˆ°sed.txt ç¬¬äºŒè¡Œä¸‹ï¼Œ æ‰“å°sh-5.1# touch sed.txtsh-5.1# vim sed.txtsh-5.1# sed '2a mei nv' sed.txtdong shenot pythonmei nvll llko okåˆ é™¤æ–‡ä»¶ä¸­åŒ…å«æ‰€æœ‰woçš„è¡Œsh-5.1# sed \"/m/e\" sed.txtdong shenot pythonll llko okå°†sedä¸­woæ›¿æ¢ä¸ºnish-5.1# sed \"s/ot/to/g\" sed.txtdong shento pythonll llko okæ³¨æ„ï¼šgè¡¨ç¤ºglobal ï¼Œå…¨éƒ¨æ›¿æ¢å°†sedä¸­ç¬¬äºŒè¡Œåˆ é™¤å¹¶å°†woæ›¿æ¢ä¸ºnish-5.1# sed -e \"2d\" -e \"s/to/ot/g\" sed.txtdong shenll llko ok 10.3 awkä¸€ä¸ªå¼ºå¤§çš„æ–‡æœ¬åˆ†æå·¥å…·ï¼Œ æŠŠæ–‡ä»¶é€è¡Œçš„è¯»å…¥ï¼Œä»¥ç©ºæ ¼ä¸ºé»˜è®¤åˆ†éš”ç¬¦å°†æ¯è¡Œåˆ‡ç‰‡ï¼Œ åˆ‡å¼€çš„éƒ¨åˆ†å†è¿›åˆ†æå¤„ç†ã€‚ åŸºæœ¬ç”¨æ³• awk [é€‰é¡¹å‚æ•°] â€˜pattern1{action1} pattern2{action2} â€¦.â€™ filename pattern: è¡¨ç¤ºAWKåœ¨æ•°æ®ä¸­æŸ¥æ‰¾çš„å†…å®¹ï¼Œå°±æ˜¯åŒ¹é…æ¨¡å¼ action:åœ¨æ‰¾åˆ°åŒ¹é…æ—¶ï¼Œæ‰€æ‰§è¡Œçš„ä¸€ç³»åˆ—å‘½ä»¤ å®æ“æ•°æ®å‡†å¤‡ sh-5.1# cp /etc/passwd ./sh-5.1# lsbanzhang.txt case.sh cut.txt for1.sh for2.sh hadoop101.bahs hadoop101.sh helloworld.sh if.sh parameter.sh passwd read.sh sed.txt sum.sh while.shsh-5.1# awk -F: '/^root/{print $7}' passwd /bin/bash/bin/bashsh-5.1# æœç´¢passwdæ–‡ä»¶ä»¥rootå…³é”®è¯å¼€å¤´çš„æ‰€æœ‰è¡Œï¼Œå¹¶è¾“å‡ºè¯¥è¡Œçš„ç¬¬7åˆ— æœç´¢passwdæ–‡ä»¶ä»¥rootå…³é”®è¯å¼€å¤´çš„æ‰€æœ‰è¡Œï¼Œå¹¶è¾“å‡ºè¯¥è¡Œçš„ç¬¬7åˆ—å’Œç¬¬ä¸€åˆ—ï¼Œä¸­é—´ä»¥â€œï¼Œâ€ åˆ†éš” sh-5.1# awk -F: '/^root/{print $1â€œï¼Œâ€$7}' passwd /bin/bash æ³¨æ„åªæœ‰åŒ¹é…äº†patternçš„è¡Œæ‰ä¼šæ‰§è¡Œaction åªæ˜¾ç¤ºã€etc/passwdçš„ç¬¬ä¸€åˆ—å’Œç¬¬ä¸ƒåˆ—ï¼Œä»¥é€—å·åˆ†éš”ï¼Œä¸”åœ¨æ‰€æœ‰è¡Œå‰é¢é“å»ºåˆ—åuserï¼Œ shellåœ¨æœ€åä¸€è¡Œå åŠ â€œdahaige , /bin/zuishuaiâ€. awk -F: 'BEGIN{print \"user,shell\"}{print $1\",\"$7} END{print \"dahaiuge,/bin/zuishuai\"}' passwd æ³¨æ„LBEGINåœ¨æ‰€æœ‰æ•°æ®è¯»å–è¡Œä¹‹å‰æ‰§è¡Œï¼Œ ENDåœ¨æ‰€æœ‰æ•°æ®æ‰§è¡Œå®Œä¹‹åæ‰§è¡Œ å°†passwdä¸­çš„ç”¨æˆ·idå¢åŠ æ•°å€¼1å¹¶è¾“å‡º awk -vi1-F: '{print $3+i}' passwd awkçš„å†…ç½®å˜é‡ å®æ“ ç»Ÿè®¡passwdæ–‡ä»¶åï¼Œæ¯è¡Œçš„è¡Œå·ï¼Œæ¯è¡Œçš„åˆ—æ•° awk -F: '{print \"filename:\" FILENAME \", linenumer:\" NR \",columns:\" NF}' passwd åˆ‡å‰²ip ifconfig eth0 | grep \"inet addr\" | awk -F : '{print $2}' | awk -F \" \" '{print $1}' æŸ¥è¯¢sed.txtä¸­ç©ºè¡Œæ‰€åœ¨çš„è¡Œå· awk '/^$/{print NR}' sed.txt 10.4 sortsort å‘½ä»¤åœ¨LInuxé‡Œéå¸¸æ¸¸æ³³ï¼Œå®ƒå°†æ–‡ä»¶æ’åºï¼Œå¹¶å°†æ’åºç»“æœæ ‡å‡†è¾“å‡º å®æ“ æŒ‰ç…§â€œï¼šâ€ åˆ†éš”ä¹‹åæ’åº sort-t ï¼š -nrk 2 sort.sh ç¬¬åä¸€ç«  ä¼ä¸šé¢è¯•é¢˜11.1 äº¬ä¸œé—®é¢˜1 ï¼šä½¿ç”¨linuxå‘½ä»¤æŸ¥è¯¢file1ä¸­ç©ºè¡Œæ‰€åœ¨çš„è¡Œå· ç­”æ¡ˆï¼š awk '/^$/{print NR}' sed.txt é—®é¢˜2ï¼š æœ‰æ–‡ä»¶chengji.txtå†…å®¹å¦‚ä¸‹ï¼š å¼ ä¸‰ 40 æå›› 50 ç‹äº” 60 ä½¿ç”¨linuxå‘½ä»¤è®¡ç®—ç¬¬äºŒåˆ—çš„å’Œå¹¶è¾“å‡º cat chenji.TXT| AWK -F \" \" '{sum+=$2} END{print sum}' 11.2 æœç‹å’Œè®¯ç½‘é—®é¢˜1ï¼š shellè„šæœ¬é‡Œå¦‚ä½•æ£€æŸ¥ä¸€ä¸ªæ–‡ä»¶æ˜¯å¦å­˜åœ¨ï¼Œå¦‚æœä¸å­˜åœ¨è¯¥å¦‚ä½•å¤„ç†ï¼Ÿ #!/bin/bashif [ -f file.txt ]; then echo \"æ–‡ä»¶å­˜åœ¨ï¼\"else echo \"æ–‡ä»¶ä¸å­˜åœ¨ï¼\"fi 11.3 æ–°æµªé—®é¢˜1ï¼š ç”¨shellå†™ä¸€ä¸ªè„šæœ¬ï¼Œå¯¹æ–‡æœ¬ä¸­æ— åºçš„ä¸€åˆ—æ•°æ®æ’åº sort -n test.txt|awk '{a+=$0;print$0}END{print \"SUM-\"a}' 11.4 é‡‘å’Œç½‘ç»œé—®é¢˜1ï¼š è¯·ç”¨shellè„šæœ¬å†™å‡º æŸ¥æ‰¾å½“å‰æ–‡ä»¶å¤¹ï¼Œ(/home) ä¸‹æ‰€æœ‰æ–‡æœ¬æ–‡ä»¶å†…å®¹åŒ…å«æœ‰å­—ç¬¦â€œshenâ€çš„æ–‡ä»¶åç§° grep -r \"shen\" /home |cut -d \":\" -f 1 ç•™ä¸‹æ–‡ä»¶å","tags":["æŠ€æœ¯"],"categories":["æŠ€æœ¯"]},{"title":"å•åº”çŸ©é˜µHæ±‚è§£","path":"/2023/01/22/å•åº”çŸ©é˜µHæ±‚è§£/","content":"å•åº”çŸ©é˜µHæ±‚è§£æ‰€éœ€æ±‚è§£çš„å•åº”çŸ©é˜µï¼š$$H_{3 \\times 3}=\\left[\\begin{array}{lll}h_{11} h_{12} h_{13} \\h_{21} h_{22} h_{23} \\h_{31} h_{32} h_{33}\\end{array}\\right]$$å•åº”å˜æ¢å…³ç³»ï¼š$$\\mathrm{s}\\left[\\begin{array}{c}x^{\\prime} \\y^{\\prime} \\1\\end{array}\\right]=H\\left[\\begin{array}{l}x \\y \\1\\end{array}\\right]=\\left[\\begin{array}{lll}h_{11} h_{12} h_{13} \\h_{21} h_{22} h_{23} \\h_{31} h_{32} h_{33}\\end{array}\\right]\\left[\\begin{array}{l}x \\y \\1\\end{array}\\right]$$ä¸ºå‡å°‘è‡ªç”±åº¦ï¼Œ ä»¤h33 =1, sä¸ºå°ºåº¦å› å­ã€‚$$h_{31} x+h_{32} y+h_{33}=h_{31} x+h_{32} y+1$$ $$\\begin{aligned}x_{i}^{\\prime} =\\frac{h_{11} x_{i}+h_{12} y_{i}+h_{13}}{h_{31} x_{i}+h_{32} y_{i}+h_{33}} \\y_{i}^{\\prime} =\\frac{h_{21} x_{i}+h_{22} y_{i}+h_{23}}{h_{31} x_{i}+h_{32} y_{i}+h_{33}}\\end{aligned}$$ åœ¨å›¾åƒä¸Šå–åœ¨çœŸå®ä¸–ç•Œæ„æˆçŸ©å½¢çš„é¡ºæ—¶é’ˆçš„å››ä¸ªè§’ç‚¹ã€‚ (x1,y1),(x2,y2),(x3,y3),(x4,y4)ä»¥åŠæ„æˆç›´è§’çš„ä¸‰ä¸ªç‚¹ (x5,y5),(x6,y6),(x7,y7), å…¶ä¸­ï¼ˆx6,y6)ä¸ºç›´è§’ç‚¹ é¢„è®¾æœŸæœ›å˜æ¢åçš„çŸ©å½¢ï¼Œç¬¬ä¸€ä¸ªç„¦ç‚¹çš„ä½ç½®ï¼ˆx1â€™,y1â€™)(x2â€™,y2â€™) è®¾è®¡æ–¹ç¨‹ç»„ï¼š ä»¤ï¼ˆx1ï¼Œy1) (x2,y2) å˜æ¢åˆ°é¢„è®¾ä½ç½®ï¼ˆx1â€™,y1â€™)(x2â€™,y2â€™)$$\\begin{aligned}x_{1}^{\\prime} =\\frac{h_{11} x_{1}+h_{12} y_{1}+h_{13}}{h_{31} x_{1}+h_{32} y_{1}+h_{33}} \\x_{2}^{\\prime} =\\frac{h_{11} x_{2}+h_{12} y_{2}+h_{13}}{h_{31} x_{2}+h_{32} y_{2}+h_{33}} \\y_{1}^{\\prime} =\\frac{h_{21} x_{1}+h_{22} y_{1}+h_{23}}{h_{31} x_{1}+h_{32} y_{1}+h_{33}} \\y_{2}^{\\prime} =\\frac{h_{21} x_{2}+h_{22} y_{2}+h_{23}}{h_{31} x_{2}+h_{32} y_{2}+h_{33}}\\end{aligned}$$ä»¤ï¼ˆx1â€™,y1â€™)å’Œ(x4â€™,y4â€™)æ„æˆçš„ç›´çº¿å‚ç›´ä¸ï¼ˆx1â€™,y1â€™)(x2â€™,y2â€™) æ„æˆçš„ç›´çº¿$$\\frac{y_{2}^{\\prime}-y_{1}^{\\prime}}{x_{2}^{\\prime}-x_{1}^{\\prime}} \\cdot \\frac{y_{4}^{\\prime}-y_{1}^{\\prime}}{x_{4}^{\\prime}-x_{1}^{\\prime}}=-1$$ä»¤xâ€™4åˆ°x1â€™çš„è·ç¦»ç­‰äºx3â€™ åˆ°x2â€™çš„è·ç¦»ï¼Œ y4â€™åˆ°y1â€™çš„è·ç¦»ç­‰äºy3â€™åˆ°y2â€˜çš„è·ç¦»ï¼Œä½¿å¾—(x4â€™,y4â€™)å’Œ(x3â€™,y3â€™)åœ¨ï¼ˆx1â€™,y1â€™)å’Œ(x2â€™,y2â€™) æ„æˆçš„ç›´çº¿çš„åŒä¸€ä¾§ï¼Œ ä¸”ç»“åˆä¸Šä¸€ä¸ªçº¦æŸï¼Œ ä½¿å¾—(x2â€™,y2â€™) å’Œ(x3â€™,y3â€™)ç›´çº¿å¹³è¡Œä¸ï¼ˆx1â€™,y1â€™)å’Œ(x4â€™,y4â€™) æ„æˆçš„ç›´çº¿ï¼Œå³(x2â€™,y2â€™) å’Œ(x3â€™,y3â€™) å‚ç›´ä¸ ï¼ˆx1â€™,y1â€™)å’Œ(x2â€™,y2â€™) æ„æˆçš„ç›´çº¿$$\\begin{aligned}x_{4}^{\\prime}-x_{3}^{\\prime}=x_{1}^{\\prime}-x_{2}^{\\prime} \\y_{4}^{\\prime}-y_{3}^{\\prime}=y_{1}^{\\prime}-y_{2}^{\\prime}\\end{aligned}$$ä»¤(x5â€™,y5â€™) å’Œ(x6â€™,y6â€™) å‚ç›´ä¸ï¼ˆx6â€™,y6â€™)å’Œ(x7â€™,y7â€™) æ„æˆçš„ç›´çº¿$$\\frac{y_{5}^{\\prime}-y_{6}^{\\prime}}{x_{5}^{\\prime}-x_{6}^{\\prime}} \\cdot \\frac{y_{7}^{\\prime}-y_{6}^{\\prime}}{x_{7}^{\\prime}-x_{6}^{\\prime}}=-1$$è”ç«‹ä¸Šè¿°8ä¸ªæ–¹ç¨‹å’Œh33=1å¯æ±‚å‡ºå•åº”çŸ©é˜µH æ­¤å¤–ä¸ºäº†ç®€åŒ–æ±‚è§£ï¼Œ å¯ä½¿å¾—é¢„è®¾æœŸæœ›çš„å˜æ¢åçš„çŸ©å½¢ç¬¬ä¸€ä¸ªè§’ç‚¹ï¼ˆx1â€™,y1â€™)ï¼Œç¬¬äºŒä¸ªè§’ç‚¹çš„ä½ç½®ï¼ˆx2â€™,y2â€™)çš„y1å’Œy2 æˆ–è€…x1å’Œx2 ç›¸ç­‰ï¼Œ åªéœ€è¦x3=x2 ,x4=x1 å³å¯ä»¥ä¿è¯å‚ç›´å…³ç³»ï¼Œ è®¡ç®—å°†å¤§å¤§ç®€åŒ– é€šè¿‡å¤šé€‰å‡ ç»„ç‚¹ï¼Œ ç»“åˆRANSACç­‰ç®—æ³•å¯è¿›è¡Œå…¨å±€ä¼˜åŒ–ã€‚","tags":["æŠ€æœ¯"],"categories":["æŠ€æœ¯"]},{"title":"æ™ºæ…§äº¤é€šé¡¹ç›®","path":"/2023/01/22/æ™ºæ…§äº¤é€šé¡¹ç›®/","content":"æ™ºæ…§äº¤é€šé¡¹ç›®1. é¡¹ç›®ç®€ä»‹å­¦ä¹ ç›®æ ‡ï¼š äº†è§£æ™ºæ…§äº¤é€šé¡¹ç›®çš„æ¶æ„ äº†è§£æ™ºæ…§äº¤é€šé¡¹ç›®ä¸­çš„æ¨¡å— å®Œæˆæ™ºæ…§äº¤é€šé¡¹ç›®çš„ç¯å¢ƒæ­å»º ä¸»è¦è®²è§£è®¡ç®—æœºè§†è§‰åœ¨äº¤é€šé¢†åŸŸçš„ç›¸å…³åº”ç”¨ï¼Œ åŒ…æ‹¬è½¦é“çº¿æ£€æµ‹ã€å¤šç›®æ ‡è½¦è¾†è¿½è¸ªä»¥åŠæµé‡ç»Ÿè®¡æ–¹æ³• å¤šç›®æ ‡è½¦è¾†è¿½è¸ªå’Œè®¡æ•°ï¼šSORTæ–¹æ³•ã€åŒˆç‰™åˆ©ç®—æ³•ã€‚å¡å°”æ›¼æ»¤æ³¢ã€è™šæ‹Ÿçº¿åœˆç­‰ è½¦é“çº¿æ£€æµ‹ï¼šå¼ æ°ç›¸æœºæ ¡æ­£ã€ä»¿å°„å˜æ¢ç­‰ã€‚ é¡¹ç›®çš„æ¶æ„ ç”¨æˆ·å±‚ æœåŠ¡å±‚ 2. ç¯å¢ƒå®‰è£…å·¥å…·åŒ…å¦‚ä¸‹ï¼š Numpy Numba Scipy h5py pandas opencv-python moviepy Filterpy å®‰è£…æ–¹æ³• 3.è½¦æµé‡æ£€æµ‹å®ç°å­¦ä¹ ç›®æ ‡ äº†è§£å¤šç›®æ ‡è·Ÿè·Ÿè¸ªçš„å®ç°æ–¹æ³• çŸ¥é“è½¦æµé‡ç»Ÿè®¡çš„æ–¹æ³• è½¦æµé‡ç»Ÿè®¡ä¸»è¦æœ‰ä»¥ä¸‹å‡ ç§æ–¹å¼ï¼š äººå·¥ç»Ÿè®¡ï¼Œ è€—æ—¶è€—åŠ›ï¼Œä¸”ç»Ÿè®¡ç»“æœä¸å…·æœ‰å¯éªŒè¯æ€§ ä¼ æ„Ÿå™¨è®¡æ•°ï¼Œ å®¹æ˜“æ”¶åˆ°å¤–ç•Œç¯å¢ƒå› ç´ å¹²æ‰° åŸºäºè§†é¢‘çš„è½¦æµé‡ç»Ÿè®¡ï¼Œ æœ¬é¡¹ç›®æ‰€ä½¿ç”¨çš„æ–¹æ³• ä¸»è¦åŒ…æ‹¬ä»¥ä¸‹å‡ ä¸ªæ­¥éª¤ï¼š ä½¿ç”¨yolov3æ¨¡å‹è¿›è¡Œç›®æ ‡æ£€æµ‹ ä½¿ç”¨sortç®—æ³•è¿›è¡Œç›®æ ‡è·Ÿè¸ªï¼Œ ä½¿ç”¨å¡å°”æ›¼æ»¤æ³¢è¿›è¡Œç›®æ ‡ä½ç½®é¢„æµ‹ï¼Œå¹¶åˆ©ç”¨åŒˆç‰™åˆ©ç®—æ³•å¯¹æ¯”ç›®æ ‡çš„ç›¸ä¼¼åº¦ï¼Œ å®Œæˆè½¦è¾†ç›®æ ‡è·Ÿè¸ª åˆ©ç”¨è™šæ‹Ÿçº¿åœˆçš„æ€æƒ³æ¥å®ç°è½¦è¾†ç›®æ ‡çš„æŠ€æœ¯ï¼Œå®Œæˆè½¦æµé‡çš„ç»Ÿè®¡ã€‚ é¡¹ç›®æµç¨‹å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š 3.0 é¡¹ç›®ä»‹ç»3.1 å¤šç›®æ ‡è·Ÿè¸ªå­¦ä¹ ç›®æ ‡ï¼š äº†è§£å¤šç›®æ ‡è·Ÿç€çš„å¸¸è§åˆ†ç±»æ–¹æ³• äº†è§£å¤šç›®æ ‡è·Ÿè¸ªä¸­å¸¸ç”¨çš„è¿åŠ¨æ¨¡å‹ çŸ¥é“å¤šç›®æ ‡è·Ÿç€çš„å¸¸ç”¨ç®—æ³• 1. å¤šç›®æ ‡è·Ÿè¸ªåˆ†ç±»å¤šç›®æ ‡è·Ÿè¸ªï¼Œå³MOTï¼Œä¹Ÿå°±æ˜¯åœ¨ä¸€æ®µè§†é¢‘ä¸­åŒæ—¶è·Ÿè¸ªå¤šä¸ªç›®æ ‡ã€‚MOTä¸»è¦åº”ç”¨åœ¨å®‰é˜²ç›‘æ§å’Œè‡ªåŠ¨é©¾é©¶ç­‰é¢†åŸŸä¸­ã€‚ å¤šç›®æ ‡è·Ÿè¸ªå¯ä»¥çœ‹åšå¤šå˜é‡ä¼°è®¡é—®é¢˜ï¼Œ å³ç»™å®šä¸€ä¸ªå›¾åƒåºåˆ— 1.1 åˆå§‹åŒ–æ–¹æ³• å¤šç›®æ ‡è·Ÿè¸ªé—®é¢˜ï¼Œå¹¶ä¸æ˜¯æ‰€æœ‰ç›®æ ‡éƒ½ä¼šåœ¨ç¬¬ä¸€å¸§å‡ºç°ï¼Œä¹Ÿå¹¶ä¸æ˜¯æ‰€æœ‰ç›®æ ‡éƒ½ä¼šåœ¨å‡ºç°åœ¨æ¯ä¸€å¸§ï¼Œã€‚é‚£å¦‚ä½•å¯¹å‡ºç°çš„ç›®æ ‡è¿›è¡Œåˆå§‹åŒ–ï¼Œ å¯ä»¥ä½œä¸ºè·Ÿè¸ªç®—æ³•çš„åˆ†ç±»è¡¨é’ˆï¼Œ å¸¸è§çš„åˆå§‹åŒ–æ–¹æ³•åˆ†ä¸ºä¸¤å¤§ç±»ï¼Œä¸€ä¸ªæ˜¯Detection-based-trackingï¼ˆDBTï¼‰ï¼Œå¦ä¸€ä¸ªæ˜¯Detection-free-tracking(DFT)ã€‚ 1.2 å¤„ç†æ¨¡å¼ MOTä¹Ÿå­˜åœ¨ç€ä¸åŒçš„å¤„ç†æ¨¡å¼ï¼Œ Onlineå’ŒOfflineä¸¤å¤§ç±»ï¼Œ å…¶ä¸»è¦åŒºåˆ«åœ¨äºæ˜¯å¦ç”¨äº†åç»­å¸§ä¿¡æ¯ï¼Œ ä¸‹å›¾å½¢è±¡çš„è§£é‡Šäº†Onlineå’Œofflineçš„åŒºåˆ« Online Tracking Online Tracking æ˜¯å¯¹è§†é¢‘å¸§è¿›è¡Œé€å¸§è¿›è¡Œå¤„ç†ï¼Œ å½“å‰å¸§çš„è·Ÿè¸ªä»…ä»…åˆ©ç”¨è¿‡å»çš„ä¿¡æ¯ã€‚ Offline tracking ä¸åŒäºOnline Trackingï¼Œ Offline Trackingä¼šåˆ©ç”¨å‰åè§†é¢‘å¸§å¯¹å½“å‰å¸§è¿›è¡Œç›®æ ‡è·Ÿè¸ªï¼Œ è¿™ç§æ–¹å¼åªé€‚åˆè§†é¢‘ï¼Œ å¦‚æœåº”ç”¨äºæ‘„åƒå¤´ï¼Œåˆ™ä¼šæœ‰æ»åæ•ˆåº”ï¼Œ é€šå¸¸é‡‡ç”¨æ—¶é—´çª—çš„æ–¹å¼è¿›è¡Œå¤„ç†ï¼ŒèŠ‚çœå†…å­˜å’ŒåŠ é€Ÿ 2. è¿åŠ¨æ¨¡å‹ä¸ºäº†ç®€åŒ–å¤šç›®æ ‡è·Ÿè¸ªçš„éš¾åº¦ï¼Œ å¼•å…¥è¿åŠ¨æ¨¡å‹ç±»ç®€åŒ–æ±‚è§£è¿‡ç¨‹ï¼Œè¿åŠ¨æ¨¡å‹æ•æ‰ç›®æ ‡çš„åŠ¨æ€è¡Œä¸ºï¼Œå®ƒä¼°è®¡ç›®æ ‡åœ¨æœªæ¥å¸§ä¸­çš„æ½œåœ¨ä½ç½®ï¼Œ ä»è€Œå‡å°‘æœç´¢ç©ºé—´ã€‚åœ¨å¤§å¤šæ•°æƒ…å†µä¸‹ï¼Œ å‡è®¾ç›®æ ‡åœ¨ç°å®ä¸­æ˜¯å¹³ç¼“è¿åŠ¨çš„ï¼Œ é‚£ä¹ˆå›¾åƒç©ºé—´ä¹Ÿæ˜¯å¦‚æ­¤ï¼Œå¯¹äºè½¦è¾†çš„è¿åŠ¨ï¼Œ å¤§è‡´å¯ä»¥åˆ†ä¸ºçº¿æ€§å’Œéçº¿æ€§ä¸¤ç§è¿åŠ¨ï¼š çº¿æ€§è¿åŠ¨ï¼š çº¿æ€§è¿åŠ¨æ˜¯ç›®æ ‡æœ€ä¸»æµçš„æ¨¡å‹ï¼Œå‡è®¾ç›®æ ‡çš„è¿åŠ¨å±æ€§å¹³ç¨³ï¼ˆé€Ÿåº¦ï¼ŒåŠ é€Ÿåº¦ï¼Œä½ç½®ï¼‰ éçº¿æ€§è¿åŠ¨ï¼šè™½ç„¶çº¿æ€§è¿åŠ¨æ¨¡å‹æ¯”è¾ƒå¸¸ç”¨ï¼Œ ä½†ç”±äºå­˜åœ¨å®ƒè§£å†³ä¸äº†çš„é—®é¢˜ï¼Œéçº¿æ€§æ¨¡å‹éšä¹‹äº§ç”Ÿï¼Œå®ƒå¯ä»¥ä½¿trackletsé—´è¿åŠ¨ç›¸ä¼¼åº¦è®¡ç®—å¾—æ›´åŠ å‡†ç¡®ã€‚ 3. è·Ÿè¸ªæ–¹æ³•å¤šç›®æ ‡è·Ÿè¸ªåŸºäºç¥ç»ç½‘ç»œçš„ç®—æ³•ï¼Œ ç«¯åˆ°ç«¯çš„ç®—æ³•å¹¶ä¸å¤šï¼Œä¸»è¦è¿˜åœ¨å®éªŒå®¤åˆ·æ¦œé˜¶æ®µï¼Œæ¨¡å‹å¤æ‚ï¼Œé€Ÿåº¦æ…¢ï¼Œ æ•ˆæœä¸å¥½ã€‚ ä¸»è¦ä»‹ç»ä¸¤ç§ä¸»æµçš„ç®—æ³•ï¼š 3.1 åŸºäºKalmanå’ŒKMç®—æ³•çš„åç«¯ä¼˜åŒ–ç®—æ³• è¯¥ç±»ç®—æ³•èƒ½è¾¾åˆ°å®æ—¶æ€§ï¼Œ ä½†ä¾èµ–äºæ£€æµ‹ç®—æ³•æ•ˆæœæ‘‡å·ï¼Œç‰¹å¾åŒºåˆ†è¦å¥½ï¼Œ ï¼ˆè¾“å‡ºæœ€ç»ˆç»“æœçš„å¥½åï¼Œä¾èµ–äºè¾ƒå¼ºçš„æ£€æµ‹ç®—æ³•ï¼Œè€ŒåŸºäºå¡å°”æ›¼åŠ åŒˆç‰™åˆ©åŒ¹é…çš„è·Ÿè¸ªç®—æ³•ä½œç”¨äºèƒ½å¤Ÿè¾“å‡ºæ£€æµ‹ç›®æ ‡çš„idï¼Œå…¶æ¬¡èƒ½å¤Ÿä¿è¯è¿½è¸ªç®—æ³•çš„å®æ—¶æ€§ï¼Œè¿™æ ·è¿½è¸ªæ•ˆæœä¼šå¥½ï¼Œidåˆ‡æ¢å°‘ä»£è¡¨çš„ç®—æ³•æ˜¯SORT/deepSort SORTæ˜¯ä¸€ç§å®ç”¨çš„å¤šç›®æ ‡è·Ÿè¸ªç®—æ³•ï¼Œå¼•å…¥çº¿æ€§é€Ÿåº¦æ¨¡å‹äºå¡å°”æ›¼æ»¤æ³¢æ¥è¿›è¡Œä½ç½®é¢„æµ‹ï¼Œå†æ— åˆé€‚åŒ¹é…æ£€æµ‹æ¡†çš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨è¿åŠ¨æ¨¡å‹æ¥é¢„æµ‹ç‰©ä½“çš„ä½ç½®ã€‚åŒˆç‰™åˆ©ç®—æ³•æ˜¯ä¸€ç§å¯»æ‰¾äºŒåˆ†å›¾çš„æœ€å¤§åŒ¹é…çš„ç®—æ³•ï¼Œåœ¨å¤šç›®æ ‡è·Ÿè¸ªé—®é¢˜ä¸­å¯ä»¥ç®€å•ç†è§£ä¸ºå¯»æ‰¾å‰åä¸¤å¸§çš„è‹¥å¹²ç›®æ ‡çš„åŒ¹é…æœ€ä¼˜è§£çš„ä¸€ç§ç®—æ³•ã€‚è€Œå¡å°”æ›¼æ»¤æ³¢å¯ä»¥çœ‹åšæ˜¯ä¸€ç§è¿åŠ¨æ¨¡å‹ï¼Œç”¨æ¥è§£å†³å¯¹ç›®æ ‡è½¨è¿¹é¢„æµ‹ï¼Œå¹¶ä½¿ç”¨ç½®ä¿¡åº¦æ¯”è¾ƒé«˜çš„è·Ÿè¸ªç»“æœè¿›è¡Œé¢„æµ‹ç»“æœçš„ä¿®æ­£ã€‚ å¤šç›®æ ‡è·Ÿè¸ªä¸€èˆ¬æ¥åœ¨ç›®æ ‡æ£€æµ‹åï¼Œ åœ¨å·¥ä¸šç•Œç›®æ ‡æ£€æµ‹é‡‡ç”¨æ¯”è¾ƒå¤šçš„æ˜¯yoloæ£€æµ‹ç®—æ³•ã€‚å…ˆå®ç°ç›®æ ‡æ£€æµ‹ç½‘ç»œï¼Œ æ£€æµ‹çš„è¾“å‡ºç»“æœä¸»è¦æ˜¯å°†æ£€æµ‹æ¡†çš„ä½ç½®ä¿¡æ¯è¾“å…¥åˆ°å¤šç›®æ ‡è·Ÿè¸ªç®—æ³•ä¸­ã€‚ 3.2 åŸºäºå¤šçº¿ç¨‹çš„å•ç›®æ ‡è·Ÿè¸ªçš„å¤šç›®æ ‡è·Ÿè¸ªç®—æ³• è¿™ç±»ç®—æ³•ç‰¹ç‚¹æ˜¯è·Ÿè¸ªæ•ˆæœå¾ˆå¥½ï¼Œå› ä¸ºæ¯ä¸€ä¸ªç±»çš„ç‰©ä½“éƒ½å•ç‹¬åˆ†é…äº†ä¸€ä¸ªè·Ÿè¸ªå™¨ï¼Œ ä½†è¯¥ç®—æ³•å¯¹å°ºåº¦å˜æ¢çš„è¦æ±‚å¤§ï¼Œå‚æ•°è°ƒè¯•éœ€è¦åˆç†ï¼ŒåŒæ—¶è€—è´¹cpuèµ„æºï¼Œå®æ—¶æ€§ä¸é«˜ï¼Œ ä»£è¡¨ç®—æ³•æ˜¯åˆ©ç”¨KCFè¿›è¡Œç›®æ ‡è·Ÿè¸ªã€‚ å¤šç›®æ ‡è·Ÿè¸ªæœ¬è´¨ä¸Šæ˜¯å¤šä¸ªç›®æ ‡åŒæ—¶è¿åŠ¨çš„é—®é¢˜ï¼Œ æ‰€ä»¥æœ‰æå‡ºå°†å•ç›®æ ‡è·Ÿè¸ªå™¨å¼•å…¥åˆ°å¤šç›®æ ‡è¿½è¸ªçš„é—®é¢˜ï¼Œä¸ºæ¯ä¸€ä¸ªç›®æ ‡åˆ†é…ä¸€ä¸ªè·Ÿè¸ªå™¨ã€‚ç„¶åæ£€æµ‹åœ°ä½¿ç”¨åŒ¹é…ç®—æ³•æ¥ä¿®æ­£é‚£äº›è·Ÿè¸ªå¤±è´¥æˆ–è€…æ–°å‡ºç°çš„ç›®æ ‡ï¼Œä»£è¡¨æ€§çš„å•ç›®æ ‡è·Ÿè¸ªç®—æ³•ä¸ºæ ¸ç›¸å…³æ»¤æ³¢ç®—æ³•ï¼ˆKCFï¼‰ï¼Œåœ¨ç²¾åº¦æ ¸é€Ÿåº¦ä¸ŠåŒæ—¶è¾¾åˆ°å¾ˆé«˜çš„æ°´å¹³ï¼Œæ˜¯å½“æ—¶å•ç›®æ ‡è·Ÿè¸ªæœ€ä¼˜ç§€çš„ç®—æ³•ä¹‹ä¸€ï¼Œåæ¥çš„å¾ˆå¤šç®—æ³•éƒ½æ˜¯åŸºäºæ­¤åšçš„æ”¹è¿›ã€‚ å®é™…åº”ç”¨è¿‡ç¨‹ä¼šä¸ºæ¯ä¸ªç›®æ ‡åˆ†é…ä¸€ä¸ªKCFè·Ÿè¸ªå™¨å¹¶é‡‡ç”¨å¤šçº¿ç¨‹çš„æ–¹å¼æ¥ç»„ç»‡è¿™äº›è·Ÿè¸ªå™¨ï¼ŒåŒæ—¶å› ä¸ºå®é™…ç¡¬ä»¶æ¡ä»¶çš„é™åˆ¶ï¼Œä¸å¯èƒ½æä¾›å¼ºå¤§çš„è®¡ç®—èµ„æºï¼Œä¼šé‡‡ç”¨æ£€æµ‹å™¨ä¸è·Ÿè¸ªäº¤æ›¿ç­–ç•¥ï¼Œç”±äºæ£€æµ‹å¸§ç‡ä¸é«˜ï¼Œ è·Ÿè¸ªæ•ˆæœæ»åæ€§ï¼Œ å®ç”¨æ•ˆæœä¸å¤§ã€‚ 3.2 è¾…åŠ©åŠŸèƒ½ yoloçš„æ•°æ®æ ¼å¼ iouçš„è¯¦è§£ 3.3 å¡å°”æ›¼æ»¤æ³¢å™¨3.4 å¡å°”æ›¼æ»¤æ³¢å™¨å®æˆ˜3.5 ç›®æ ‡ä¼°è®¡æ¨¡å‹-å¡å°”æ›¼æ»¤æ³¢3.6 åŒˆç‰™åˆ©ç®—æ³•3.7 æ•°æ®å…³è”3.8 SORT/deepSORT3.9 å¤šç›®æ ‡è·Ÿç€å®ç°3.10 yoloæ¨¡å‹3.12 åŸºäºyoloçš„ç›®æ ‡æ£€æµ‹3.12 è½¦æµé‡ç»Ÿè®¡3.13 è§†é¢‘ä¸­çš„è½¦æµé‡ç»Ÿè®¡","tags":["æŠ€æœ¯"],"categories":["æŠ€æœ¯"]},{"title":"ä»è§†é¢‘æ ‡å®šåˆ°SLAM","path":"/2023/01/22/æµ™æ±Ÿå¤§å­¦è§†è§‰SLAMå…¬å¼€è¯¾-ä»è§†é¢‘æ ‡å®šåˆ°SLAM/","content":"æµ™æ±Ÿå¤§å­¦è§†è§‰SLAMå…¬å¼€è¯¾-ä»è§†é¢‘æ ‡å®šåˆ°SLAMCAMERA Pinhole Camera ï¼ˆé’ˆå­”æ‘„åƒï¼‰ è§’åº¦ è·ç¦»æ¶ˆå¤± å¹³è¡Œä¸ä¿ç•™ï¼Œé•¿åº¦ä¸åœ¨ç»å¯¹ ç»´åº¦å‹ç¼©ï¼Œä¸¢å¤±äº†éå¸¸å¤šçš„ä¿¡æ¯ åˆ¶ä½œä¸€ä¸ªç›¸æœº å¦‚ä½•åšä¸€ä¸ªç›¸æœºï¼Œ Shringking the Apertureï¼ˆæ”¶ç¼©å…‰åœˆï¼‰ å­”å¤ªå°ï¼Œ æ›å…‰æ—¶é—´å¤ªå°‘ï¼Œ å­”å¤ªå°ï¼Œ å…‰çš„æ³¢é•¿ä¸èƒ½è¢«å¿½ç•¥ä¼šå‘ç”Ÿå…‰çš„å¹²æ¶‰ç°è±¡ã€‚ é•œå¤´çš„ä½œç”¨æ˜¯å•¥å‘¢ï¼Ÿ ç”¨å‡¸é€é•œ æŠŠå…‰èšé›†èµ·æ¥ï¼Œèšç„¦èµ·æ¥åˆ°ä¸€ä¸ªç‚¹ä¸Šï¼Œ æ›å…‰æ—¶é—´ç®€çŸ­ï¼Œ è¿˜æ˜¯ä¼šæ•£å¼€ï¼Œ é•œé¢çš„è§’åº¦æœ‰å…³ åªæœ‰åˆ°é•œå¤´å›ºå®šè·ç¦»ï¼Œæ‰èƒ½æ¸…æ™°æˆåƒã€‚ï¼Œåªæœ‰ä¸€å®šèŒƒå›´å†…æ‰æ¸…æ™°ã€‚ depth of filedã€‚ å…‰åœˆçš„ä½œç”¨ å…‰åœˆçš„é€Ÿåº¦ï¼Œ å…‰åœˆè¶Šå°ï¼Œ æ¯”è¾ƒé•¿æ—¶é—´æ›å…‰ï¼Œ å…‰åœˆè¶Šå¤§F = 1/ç›´å¾„ filed of view è·Ÿç›¸æœºçš„ç„¦è·ç›´æ¥ç›¸å…³ã€‚","tags":["æŠ€æœ¯"],"categories":["æŠ€æœ¯"]},{"title":"ç›®æ ‡æ£€æµ‹ä¸­çš„åŒ¹é…æœºåˆ¶","path":"/2023/01/22/ç›®æ ‡æ£€æµ‹ä¸­çš„åŒ¹é…æœºåˆ¶/","content":"ç›®æ ‡æ£€æµ‹ä¸­çš„åŒ¹é…æœºåˆ¶1. FCOSç›¸è¾ƒäºreatinnet FOs çš„æ­£è´Ÿæ ·æœ¬åŒ¹é…æœºåˆ¶ä¼šäº§ç”Ÿæ›´å¤šçš„æ­£æ ·æœ¬ 2. ATSSåŒ¹é…æ­¥éª¤ è®¡ç®—æ¯ä¸ªgt boxä¸å¤šå°ºåº¦è¾“å‡ºå±‚ anchorä¹‹é—´çš„IoU è®¡ç®—æ¯ä¸ªgt boxä¸å¤šå°ºåº¦è¾“å‡ºå±‚anchorä¸­å¿ƒåæ ‡çš„L2è·ç¦» éå†å±‚ ï¼Œ éå†gt box ï¼Œtopk(k é»˜è®¤æ˜¯9 ï¼‰L2 æœ€å°è·ç¦»çš„anchorï¼Œ ä¸€å…±æœ‰Lå±‚ä¸ª L*K, æ¯ä¸ªgt å¯¹äºæ¯ä¸ªgt boxï¼Œ è®¡ç®—anchorä¸boxä¹‹é—´çš„ï¼ˆIoUï¼‰çš„ï¼ˆmeanï¼Œ varï¼‰ å‡å€¼å’Œæ–¹å·®ï¼Œç›¸åŠ ä¹‹å’Œä½œä¸ºé˜ˆå€¼ å¯¹äºæ¯ä¸ªgt box å¤§äºé˜ˆå€¼çš„ä½œä¸ºæ­£æ ·æœ¬ï¼Œ å…¶ä»–ä½œä¸ºè´Ÿæ ·æœ¬ è‹¥topkè¿‡å¤§ï¼Œå¯¼è‡´ä¸åœ¨å†…éƒ¨ï¼Œ åˆ™è¿‡æ»¤æ‰ 3. OTA(Optimal Transport Assignment)4. simOTAä¼˜ç‚¹ simOTAèƒ½å¤Ÿè‡ªåŠ¨åˆ†ææ¯ä¸ªgtã€æ‰€éœ€è¦å¤šå°‘æ­£æ ·æœ¬ èƒ½è‡ªåŠ¨å†³å®šæ¯ä¸ªgt ä»å“ªä¸ªç‰¹å¾å›¾æ¥è´Ÿè´£æ£€æµ‹ ç›¸æ¯”OTA, simOTAæ›´å¿« ç›¸æ¯”äºOTAï¼Œ é¿å…é¢å¤–è¶…å‚æ•° ä»£ç é€»è¾‘ è®¡ç®—æ­£åŒºåŸŸ è®¡ç®—anchorä¸gtçš„iou åœ¨å€™é€‰åŒºåŸŸè®¡ç®—cost åˆ©ç”¨iouç¡®å®šæ¯ä¸ªgtçš„dynamic_k gté€‰å–costæœ€å°dynamic_kä¸ªanchoræ­£æ ·æœ¬ï¼Œ å…¶ä½™ä¸ºè´Ÿ ä½¿ç”¨æ­£è´Ÿæ ·æœ¬è®¡ç®—loss","tags":["æŠ€æœ¯"],"categories":["æŠ€æœ¯"]},{"title":"matlabåŸºç¡€","path":"/2020/02/05/20200205-practise-matlab/","content":"%%% ç¬¬ä¸€é¢˜% ç»™å®šNå’ŒAï¼ŒNæ˜¯ä¸€ä¸ªæ•´æ•°ï¼ŒAæ˜¯ä¸€ä¸ªå‘é‡ï¼Œä¾‹å¦‚N = 3; A = [ 4 5 6 7]ï¼Œè¯·ä½¿ç”¨matlabå‘½ä»¤å°†Aä¸­çš„æ¯ä¸€ä¸ª% å…ƒç´ é‡å¤Næ¬¡ï¼Œç„¶åå½¢æˆä¸€ä¸ªå‘é‡ï¼Œç¤ºä¾‹è®¡ç®—ç»“æœå¦‚ä¸‹ï¼šN = 3; A = [ 4 5 6 7];B=A(ones(1,N),:); % æ³¨æ„è¿™é‡Œä¸‹æ ‡çš„å®éªŒï¼Œå¯ä»¥ä½¿ç”¨ç›¸åŒçš„ä¸‹æ ‡å¤šæ¬¡B(:) %%% å‡è®¾xæ˜¯ä¸€ä¸ªå‘é‡ï¼Œä¾‹å¦‚% x = [ 4 4 5 5 5 6 7 7 8 8 8 8 ]% ç°åœ¨æˆ‘ä»¬æƒ³å¾—åˆ°å¦‚ä¸‹ä¸¤ä¸ªå‘é‡% l = [ 2 3 1 2 4 ]; % xæ¯ä¸ªå…ƒç´ é‡å¤çš„ä¸ªæ•°% v = [ 4 5 6 7 8 ]; % xä¸­é‡å¤å…ƒç´ çš„å€¼x = [ 4 4 5 5 5 6 7 7 8 8 8 8 ]i = [find(x(1:end-1) ~= x(2:end)) length(x)]l = diff([0 i])v = x(i) %%% æ±‚å»ºç«‹ä»¥ä¸‹tablesæ•°ç»„% table N riqi xianxing Ndanhao Nshuanghao% ç¬¬ä¸€åˆ—ï¼Œè¡Œç¼–å·% ç¬¬äºŒåˆ—ï¼Œ2017å¹´æ¯å¤©çš„æ—¥æœŸï¼Œdatetimeæ ¼å¼% ç¬¬ä¸‰åˆ—ï¼Œå¦‚æœæ—¥æœŸä¸ºå•å·ï¼Œé‚£ä¹ˆå†™æ–‡æœ¬â€œå•å·é€šè¡Œâ€ï¼Œå¦‚æœæ—¥æœŸæ˜¯åŒå·ï¼Œé‚£ä¹ˆå†™æ–‡æœ¬â€œåŒå·é€šè¡Œâ€ï¼Œå¦‚æœæ˜¯å‘¨% æœ«ï¼Œåˆ™å†™â€œå•åŒé€šè¡Œâ€% ç¬¬å››åˆ—ï¼Œå†™å½“å‰æ—¥æœŸå•å·è½¦ä¸€å…±é€šè¡Œäº†å¤šå°‘å¤©% ç¬¬äº”åˆ—ï¼Œå†™å½“å‰æ—¥æœŸåŒå·è½¦ä¸€å…±é€šè¡Œäº†å¤šå°‘å¤©NDays = yeardays(2017) %2017å¹´çš„å¤©æ•°N=[1:NDays]';riqi=datetime(2017,1,1)+N-1;%æ³¨æ„è¿™é‡Œï¼Œå¾ˆå¤šæ—¶å€™å¯ä»¥é‡‡ç”¨å…ˆé¢„ç½®ä¸€ç§ç»Ÿä¸€ç­”æ¡ˆï¼Œç„¶åå…¶ä¸­æŸäº›ç»“æœå¯ä»¥ä¾æ¬¡è¦†ç›–åŸæœ‰ç­”æ¡ˆxianxing = repmat('å•å·é€šè¡Œ',NDays,1);a = logical(mod(riqi.Day,2)); %æ—¥æœŸæ˜¯å¦ä¸ºå•æ•°xianxing(~a,:)= repmat('åŒå·é€šè¡Œ',sum(~a),1);tf = isweekend(riqi);xianxing(tf,:) = repmat('å•åŒé€šè¡Œ',sum(tf),1);idxdanhao=all(xianxing=='å•å·é€šè¡Œ',2)|all(xianxing=='å•åŒé€šè¡Œ',2)idxshuanghao=all(xianxing=='åŒå·é€šè¡Œ',2)|all(xianxing=='å•åŒé€šè¡Œ',2)Ndanhao=cumsum(idxdanhao);Nshuanghao=cumsum(idxshuanghao);XianXiangData = table(N,riqi,xianxing,Ndanhao,Nshuanghao); %%% é€šè¿‡load('data.mat')å‘½ä»¤è½½å…¥å½“å‰ç›®å½•ä¸‹çš„æ•°æ®æ–‡ä»¶ï¼Œdata.matï¼Œç„¶åè¿›è¡Œä»¥ä¸‹å¤„ç†ï¼š% dataæ•°æ®æ˜¯ä¸€ä¸ªæ‰‹å†™æ•°å­—å›¾åƒçš„ç°åº¦æ•°æ®ï¼Œä¸€å…±2000è¡Œ785åˆ—ï¼Œ2000è¡¨ç¤ºæ˜¯2000å‰¯å›¾ç‰‡ï¼Œç¬¬ä¸€åˆ—æ˜¯å½“å‰å›¾ç‰‡% çš„æ•°å­—æ˜¯å‡ ï¼ˆ0-9èŒƒå›´ï¼Œå•ä¸ªæ•´æ•°ï¼‰ï¼Œç¬¬2åˆ—è‡³ç¬¬785åˆ—æ˜¯å›¾åƒçš„ç°åº¦æ•°æ®ï¼Œå›¾ç‰‡åŸå§‹å¤§å°ä¸º28Ã—28=784ä¸ªåƒç´ % ç‚¹ï¼Œç„¶åæŒ‰ç…§ä»¥ä¸‹ç¼–å·æ–¹å¼(å›¾ç‰‡æœªæ˜¾ç¤ºå®Œå…¨)ï¼Œå°†784å„ç‚¹è½¬æ¢ä¸ºäº†1è¡Œï¼Œä½œä¸ºäº†dataæ•°æ®ä¸­çš„ä¸€è¡Œ% ç°åœ¨è¦æ±‚å°†dataç¬¬ä¸€åˆ—æå–ä¸ºä¸€ä¸ªåˆ—å‘é‡ï¼Œå‘½åä¸ºtrainTï¼Œå¦å¤–ç¬¬2åˆ—è‡³ç¬¬785åˆ—æ•°æ®è½¬æ¢ä¸º28Ã—28Ã—2000çš„% ä¸‰ç»´æ•°ç»„ï¼Œç¬¬ä¸€é¡µä¸ºç¬¬ä¸€ä¸ªæ•°å­—çš„å›¾åƒï¼Œè¦æ±‚æŒ‰ç…§å›¾ç‰‡ä¸­çš„é¡ºåºï¼Œå°†ç¬¬ä¸€è¡Œä¸­çš„ç°åº¦æ•°æ®å­˜æˆæ–°çš„28Ã—28çš„æ•°% ç»„ï¼Œå‘½åä¸ºtrainDdata=load('data.mat');data1 = data.data(:,2:end);trainId = reshape(data1',28,28,2000);trainId = permute(trainId,[2 1 3]); %ç»´æ•°çš„è½¬æ¢trianT = data.data(:,1);figureimshow(uint8(trainId(:,:,7))) %%% åœ¨åŒä¸€ä¸ªå›¾å†…ç»˜åˆ¶ä¸¤ä¸ªåœ†çš„æ›²çº¿ï¼Œä¸€ä¸ªåŠå¾„ä¸º1ï¼Œä¸€ä¸ªåŠå¾„ä¸º2ï¼Œåœ¨å³æ–¹å¤–ä¾§ä¸­éƒ¨æ·»åŠ å›¾ä¾‹ï¼Œâ€œå°åœ†â€å’Œâ€å¤§% åœ†â€œï¼Œç»˜å›¾åŒºåŸŸè®¾ç½®ä¸ºæ­£æ–¹å½¢ï¼Œæ ‡é¢˜è®¾ç½®ä¸ºâ€œä¸¤ä¸ªåœ†â€ï¼Œæ¨ªè½´æ ‡ç­¾ä¸ºxï¼Œçºµè½´æ ‡ç­¾ä¸ºy% åŒåæ ‡è½´å›¾% è½½å…¥æ•°æ®datahis.matï¼Œ% ç„¶åç»˜åˆ¶datahis0.t_hisä¸ºæ¨ªåæ ‡ï¼Œdatahis0.tempä¸ºå·¦ä¾§çºµåæ ‡ï¼Œdatahis0.humä¸ºå³ä¾§çºµåæ ‡çš„åŒåæ ‡è½´å›¾å½¢% åœ¨å›¾åƒä¸‹æ–¹å¤–ä¾§æ·»åŠ å›¾ä¾‹â€œæ¸©åº¦â€ï¼Œâ€œæ¹¿åº¦â€ï¼Œæ¨ªè½´æ ‡ç­¾è®¾ç½®ä¸ºæ—¶é—´ï¼Œå·¦ä¾§yè½´è®¾ç½®æ ‡ç­¾ä¸ºæ¸©åº¦ï¼Œå³ä¾§yè½´æ ‡ç­¾% è®¾ç½®ä¸ºæ¹¿åº¦% datahis0.t_hisä¸ºæ—¶é—´ï¼Œéœ€è¦è½¬æ¢ä¸ºdatetimeç±»å‹ï¼Œç„¶åé€‰å–1æœˆ10æ—¥çš„æ•°æ®è¿›è¡Œç”»å›¾clearfigureload('datahis.mat')t = datetime(datevec(datahis0.t_his));idx=t=datetime(2017,1,10)t=datetime(2017,1,11);t1 = t(idx);temp1 = datahis0.temp(idx);hum1 = datahis0.hum(idx);yyaxis leftplot(t1,temp1)xlabel('æ—¶é—´')ylabel('æ¸©åº¦')yyaxis rightplot(t1,hum1)ylabel('æ¸©åº¦')legend('æ¸©åº¦','æ¹¿åº¦','Location','southoutside','Orientation','horizontal') %%% å¤šå­å›¾çš„ç»˜åˆ¶% ç»™å®šä»¥ä¸‹æ•°æ®% x = 0:0.01:20; % xåæ ‡% y1 = 200*exp(-0.05*x).*sin(x); % Y1% y2 = 0.8*exp(-0.5*x).*sin(10*x); % Y2% y3 = 100*exp(-0.5*x).*sin(5*x); % Y3% y1ï¼Œy2ï¼Œy3éœ€è¦åˆ†åˆ«ç»˜åˆ¶ä¸€ä¸ªå­å›¾ï¼Œå…¶ä¸­y1å æ®å·¦ä¾§ä¸€åŠä½ç½®ï¼Œy2å æ®å³ä¾§ä¸Šæ–¹ï¼Œy3å æ®å³ä¾§ä¸‹æ–¹clearfigurex= 0:0.01:20; % xåæ ‡y1 = 200*exp(-0.05*x).*sin(x); % Y1y2 = 0.8*exp(-0.5*x).*sin(10*x); % Y2y3 = 100*exp(-0.5*x).*sin(5*x); % Y3subplot(2,2,[1 3])plot(x,y1)subplot(2,2,2)plot(x, y2)subplot(2,2,4)plot(x,y3) %%% æ›²é¢å›¾% ç»˜åˆ¶å‡½æ•°çš„ç½‘æ ¼å›¾,x,yçš„å–å€¼èŒƒå›´ä¸º-2åˆ°2figureclearx = -2:0.1:2;y=x;[X,Y]=meshgrid(x,y)Z = sin(X.^2+Y.^2);mesh(X,Y,Z) %%% ä¸‰ç»´é¥¼å½¢å›¾% ç»˜åˆ¶ä¸‰ç»´é¥¼å½¢å›¾ï¼Œå„å…ƒç´ æ‰€å æ•°å€¼ä¸ºï¼š [6 3 7 5 1 2 4]ï¼Œçªå‡ºæ˜¾ç¤ºç¬¬1ï¼Œ3ä¸ªå…ƒç´ ï¼Œ7ä¸ªå…ƒç´ çš„æ ‡ç­¾åˆ†åˆ«ä¸º% 'å‘¨ä¸€'åˆ°'å‘¨æ—¥'figurex =[6 3 7 5 1 2 4];labels = {'å‘¨ä¸€','å‘¨äºŒ','å‘¨ä¸‰','å‘¨å››','å‘¨äº”','å‘¨å…­','å‘¨æ—¥'};explode=[1 0 1 0 0 0 0] %çªå‡ºæ˜¾ç¤ºå‘é‡xçš„å…ƒç´ pie3(x,explode,labels) %%% ç»˜åˆ¶å¦‚ä¸‹åŒçºµè½´æŸ±å½¢å›¾ã€‚æ•°æ®ä¸ºï¼š% x=1:20;% y1=sin(x)+2;% y2=(x-10).^2;% æ³¨æ„æŸ±å½¢å›¾ç­‰å›¾å½¢ä¹Ÿæ˜¯å¯ä»¥ä½¿ç”¨åŒåæ ‡è½´ç»˜åˆ¶çš„clearfigurex=1:20;y1=sin(x)+2;y2=(x-10).^2;yyaxis leftbar(x+0.2,y1,0.3,'b') %æ³¨æ„å›¾å½¢å åŠ ä½ç½®çš„è°ƒæ•´ylabel('sin')yyaxis rightbar(x-0.2,y2,0.3,'r')ylabel('x^2')legend('sin','x^2','Location','southoutside','Orientation','horizontal')title('åŒçºµè½´æŸ±å½¢å›¾')xlabel('x') %%% ç»˜åˆ¶ä»¥ä¸‹æ•°æ®å¯¹åº”æ›²çº¿ï¼Œå¹¶å¢åŠ æ¨ªè½´æ ‡ç­¾åŠå¯¹åº”æ›²çº¿ä¸Šçš„æ•°å€¼åšæ ‡è®°ã€‚å¹¶åœ¨å›¾ä¸­å†™æ ‡è®°å…¬å¼æ–‡æœ¬% x=2:20;% alpha=x.^2;% beta=log(x);% y=alpha./beta;clearfigurex=2:20;alpha=x.^2;beta=log(x);y=alpha./beta;plot(x,y)hold onplot(sqrt(23),sqrt(23).^2./log(sqrt(23)),'o')text(6,100,'$$\\frac{\\alpha}{\\beta}$$','Interpreter','latex');%å­—ç¬¦çš„è¾“å‡ºxticks(sort([2:2:20 sqrt(23)]))h1=gca;h1.XTickLabel{3}='$$\\sqrt{23}$$'; %å­—ç¬¦çš„è¾“å‡ºh1.TickLabelInterpreter='latex'; %%% ç»˜åˆ¶å¦‚ä¸‹å›¾å½¢% t1 = datetime(2014,1:12,1);% temp = [0 2 12 11 15 25 23 27 25 24 12 8];clearfiguret1 = datetime(2014,1:12,1);temp = [0 2 12 11 15 25 23 27 25 24 12 8];h = plot(t1,temp,':*');ax = h.Parent;title('A Year of Temperatures on the 1st of the Month')ylabel('Degrees Celcius ^{\\circ}') % ä¸Šè§’æ ‡çš„ä½¿ç”¨ï¼Œæ‘„æ°åº¦ç¬¦å·yt1 = ax.YTickLabel %åæ ‡è½´æ ‡ç­¾ä¸ºå…ƒèƒæ•°ç»„ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œä¿®æ”¹ytld = strcat(yt1,'^{\\circ}') %ä¸Šè§’æ ‡ax.YTickLabel = ytld; %%% æŒ‰ç…§è¦æ±‚ç»˜å›¾% t=(pi*(0:1000)/1000)';% y=sin(t);% ä»¥tä¸ºæ¨ªè½´æ•°æ®ï¼Œyä¸ºçºµè½´æ•°æ®ç»˜åˆ¶å›¾å½¢ï¼Œè¦æ±‚ï¼š% 1ï¼Œå°†çºµåæ ‡è½´è®¾ç½®åœ¨å³æ–¹ï¼Œ% 2ï¼Œå°†æ¨ªè½´æ ‡ç­¾è®¾ç½®ä¸º[0 1/4*pi 1/2*pi 3/4*pi pi]ï¼Œåæ ‡è½´èŒƒå›´è®¾ç½®ä¸º0ï½pi% 3ï¼Œè¾“å‡ºå›¾å½¢çš„figureçª—å£ä½ç½®åæ ‡% 4ï¼Œè¾“å‡ºå›¾å½¢axesçš„ä½ç½®åæ ‡clearfiguret=(pi*(0:1000)/1000)';y=sin(t);h = plot(t,y)set(gca,'YAxisLocation','right')xticks([0 1/4*pi 1/2*pi 3/4*pi pi])xticklabels({'0','1/4 \\pi','1/2 \\pi','3/4 \\pi','\\pi'})xlim([0 pi]) %%% æŒ‰ç…§ä»¥ä¸‹ä½ç½®ç»˜åˆ¶å›¾å½¢% å…¶ä¸­éœ€è¦ç”¨åˆ°çš„æ•°æ®å¦‚ä¸‹ï¼Œå¦å¤–å›¾ç‰‡åæ ‡çš„ä½ç½®ä¸ç¤ºä¾‹å›¾ä¸­å¤§è‡´ç›¸ç­‰å³å¯% % å›¾1% x=linspace(0.2*pi,20);% y=sin(x);% % å›¾2% t=0:pi/100:20*pi;% x=sin(t);% y=cos(t);% z=t.*sin(t).*cos(t);% % å›¾3% [x,y]=meshgrid(-8:0.5:8);% z=sin(sqrt(x.^2+y.^2))./sqrt(x.^2+y.^2+eps);clearfigure% å›¾1x=linspace(0.2*pi,20);y=sin(x);axes('Position',[0.6,0.2,0.2,0.7],'GridLineStyle','-');plot(y,x);grid on% å›¾2axes('Position',[0.1,0.2,0.5,0.5]);t=0:pi/100:20*pi;x=sin(t);y=cos(t);z=t.*sin(t).*cos(t);plot3(x,y,z);% å›¾3axes('Position',[0.1,0.6,0.25,0.3]);[x,y]=meshgrid(-8:0.5:8);z=sin(sqrt(x.^2+y.^2))./sqrt(x.^2+y.^2+eps);mesh(x,y,z) %%% ä»¥ä¸‹æ•°æ®è¿›è¡Œæ‹Ÿåˆ% load ex1.mat% cftoolload ex1.mat %%% ä½¿ç”¨å‘½ä»¤å¯¼å…¥æ–‡ä»¶2016010600.txtä¸­çš„æ•°æ®ï¼Œæå–ç¬¬4åˆ—æ—¥æœŸæ•°æ®å’Œç¬¬5åˆ—æ—¶åˆ»æ•°æ®æ•°æ®ï¼Œåˆå¹¶ç”Ÿæˆä¸€ä¸ª% datetimeæ•°ç»„ï¼Œå‘½åä¸ºTcleara = readtable('2016010600.txt');t1 = a.Var4;t2 = a.Var5;TPvec_y = floor(t1/10000);TPvec_mon = floor((t1-TPvec_y*10000)/100);TPvec_d = mod(t1,100)TPvec_h = floor(t2/100);TPvec_min=mod(t2,100);TPvec_s = zeros(size(t1,1),1);t_vec=[TPvec_y TPvec_mon TPvec_d TPvec_h TPvec_min TPvec_s];T = datetime(t_vec) %%% ç”Ÿæˆä¸2016010600.txtæ–‡ä»¶ä¸­ç¬¬6åˆ—æ•°æ®æ¦‚ç‡åˆ†å¸ƒç›¸åŒçš„éšæœºæ•°ï¼Œæ•°é‡10000ä¸ª.clearfigurea = readtable('2016010600.txt')v = a.Var6;histogram(v,20)[N,edges] = histcounts(v,20);rm = RDrnd(N,10000);figurehistogram(rm,20)","tags":["MATLAB"],"categories":["å…¶ä»–"]},{"title":"matlabåŸºç¡€","path":"/2020/02/05/practise-matlab/","content":"1 2 3 4 5 6 7 | %% % ç¬¬ä¸€é¢˜ % ç»™å®šNå’ŒAï¼ŒNæ˜¯ä¸€ä¸ªæ•´æ•°ï¼ŒAæ˜¯ä¸€ä¸ªå‘é‡ï¼Œä¾‹å¦‚N = 3; A = [ 4 5 6 7]ï¼Œè¯·ä½¿ç”¨matlabå‘½ä»¤å°†Aä¸­çš„æ¯ä¸€ä¸ª % å…ƒç´ é‡å¤Næ¬¡ï¼Œç„¶åå½¢æˆä¸€ä¸ªå‘é‡ï¼Œç¤ºä¾‹è®¡ç®—ç»“æœå¦‚ä¸‹ï¼š N = 3; A = [ 4 5 6 7]; B=A(ones(1,N),:); % æ³¨æ„è¿™é‡Œä¸‹æ ‡çš„å®éªŒï¼Œå¯ä»¥ä½¿ç”¨ç›¸åŒçš„ä¸‹æ ‡å¤šæ¬¡ B(:) â€”|â€” 1 2 3 4 5 6 7 8 9 10 | %% % å‡è®¾xæ˜¯ä¸€ä¸ªå‘é‡ï¼Œä¾‹å¦‚ % x = [ 4 4 5 5 5 6 7 7 8 8 8 8 ] % ç°åœ¨æˆ‘ä»¬æƒ³å¾—åˆ°å¦‚ä¸‹ä¸¤ä¸ªå‘é‡ % l = [ 2 3 1 2 4 ]; % xæ¯ä¸ªå…ƒç´ é‡å¤çš„ä¸ªæ•° % v = [ 4 5 6 7 8 ]; % xä¸­é‡å¤å…ƒç´ çš„å€¼ x = [ 4 4 5 5 5 6 7 7 8 8 8 8 ] i = [find(x(1:end-1) ~= x(2:end)) length(x)] l = diff([0 i]) v = x(i) â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 | %% % æ±‚å»ºç«‹ä»¥ä¸‹tablesæ•°ç»„ % table N riqi xianxing Ndanhao Nshuanghao % ç¬¬ä¸€åˆ—ï¼Œè¡Œç¼–å· % ç¬¬äºŒåˆ—ï¼Œ2017å¹´æ¯å¤©çš„æ—¥æœŸï¼Œdatetimeæ ¼å¼ % ç¬¬ä¸‰åˆ—ï¼Œå¦‚æœæ—¥æœŸä¸ºå•å·ï¼Œé‚£ä¹ˆå†™æ–‡æœ¬â€œå•å·é€šè¡Œâ€ï¼Œå¦‚æœæ—¥æœŸæ˜¯åŒå·ï¼Œé‚£ä¹ˆå†™æ–‡æœ¬â€œåŒå·é€šè¡Œâ€ï¼Œå¦‚æœæ˜¯å‘¨ % æœ«ï¼Œåˆ™å†™â€œå•åŒé€šè¡Œâ€ % ç¬¬å››åˆ—ï¼Œå†™å½“å‰æ—¥æœŸå•å·è½¦ä¸€å…±é€šè¡Œäº†å¤šå°‘å¤© % ç¬¬äº”åˆ—ï¼Œå†™å½“å‰æ—¥æœŸåŒå·è½¦ä¸€å…±é€šè¡Œäº†å¤šå°‘å¤© NDays = yeardays(2017) %2017å¹´çš„å¤©æ•° N=[1:NDays]'; riqi=datetime(2017,1,1)+N-1; %æ³¨æ„è¿™é‡Œï¼Œå¾ˆå¤šæ—¶å€™å¯ä»¥é‡‡ç”¨å…ˆé¢„ç½®ä¸€ç§ç»Ÿä¸€ç­”æ¡ˆï¼Œç„¶åå…¶ä¸­æŸäº›ç»“æœå¯ä»¥ä¾æ¬¡è¦†ç›–åŸæœ‰ç­”æ¡ˆ xianxing = repmat('å•å·é€šè¡Œ',NDays,1); a = logical(mod(riqi.Day,2)); %æ—¥æœŸæ˜¯å¦ä¸ºå•æ•° xianxing(~a,:)= repmat('åŒå·é€šè¡Œ',sum(~a),1); tf = isweekend(riqi); xianxing(tf,:) = repmat('å•åŒé€šè¡Œ',sum(tf),1); idxdanhao=all(xianxing=='å•å·é€šè¡Œ',2)|all(xianxing=='å•åŒé€šè¡Œ',2) idxshuanghao=all(xianxing=='åŒå·é€šè¡Œ',2)|all(xianxing=='å•åŒé€šè¡Œ',2) Ndanhao=cumsum(idxdanhao); Nshuanghao=cumsum(idxshuanghao); XianXiangData = table(N,riqi,xianxing,Ndanhao,Nshuanghao); â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | %% % é€šè¿‡load('data.mat')å‘½ä»¤è½½å…¥å½“å‰ç›®å½•ä¸‹çš„æ•°æ®æ–‡ä»¶ï¼Œdata.matï¼Œç„¶åè¿›è¡Œä»¥ä¸‹å¤„ç†ï¼š % dataæ•°æ®æ˜¯ä¸€ä¸ªæ‰‹å†™æ•°å­—å›¾åƒçš„ç°åº¦æ•°æ®ï¼Œä¸€å…±2000è¡Œ785åˆ—ï¼Œ2000è¡¨ç¤ºæ˜¯2000å‰¯å›¾ç‰‡ï¼Œç¬¬ä¸€åˆ—æ˜¯å½“å‰å›¾ç‰‡ % çš„æ•°å­—æ˜¯å‡ ï¼ˆ0-9èŒƒå›´ï¼Œå•ä¸ªæ•´æ•°ï¼‰ï¼Œç¬¬2åˆ—è‡³ç¬¬785åˆ—æ˜¯å›¾åƒçš„ç°åº¦æ•°æ®ï¼Œå›¾ç‰‡åŸå§‹å¤§å°ä¸º28Ã—28=784ä¸ªåƒç´  % ç‚¹ï¼Œç„¶åæŒ‰ç…§ä»¥ä¸‹ç¼–å·æ–¹å¼(å›¾ç‰‡æœªæ˜¾ç¤ºå®Œå…¨)ï¼Œå°†784å„ç‚¹è½¬æ¢ä¸ºäº†1è¡Œï¼Œä½œä¸ºäº†dataæ•°æ®ä¸­çš„ä¸€è¡Œ % ç°åœ¨è¦æ±‚å°†dataç¬¬ä¸€åˆ—æå–ä¸ºä¸€ä¸ªåˆ—å‘é‡ï¼Œå‘½åä¸ºtrainTï¼Œå¦å¤–ç¬¬2åˆ—è‡³ç¬¬785åˆ—æ•°æ®è½¬æ¢ä¸º28Ã—28Ã—2000çš„ % ä¸‰ç»´æ•°ç»„ï¼Œç¬¬ä¸€é¡µä¸ºç¬¬ä¸€ä¸ªæ•°å­—çš„å›¾åƒï¼Œè¦æ±‚æŒ‰ç…§å›¾ç‰‡ä¸­çš„é¡ºåºï¼Œå°†ç¬¬ä¸€è¡Œä¸­çš„ç°åº¦æ•°æ®å­˜æˆæ–°çš„28Ã—28çš„æ•° % ç»„ï¼Œå‘½åä¸ºtrainD data=load('data.mat'); data1 = data.data(:,2:end); trainId = reshape(data1',28,28,2000); trainId = permute(trainId,[2 1 3]); %ç»´æ•°çš„è½¬æ¢ trianT = data.data(:,1); figure imshow(uint8(trainId(:,:,7))) â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 | %% % åœ¨åŒä¸€ä¸ªå›¾å†…ç»˜åˆ¶ä¸¤ä¸ªåœ†çš„æ›²çº¿ï¼Œä¸€ä¸ªåŠå¾„ä¸º1ï¼Œä¸€ä¸ªåŠå¾„ä¸º2ï¼Œåœ¨å³æ–¹å¤–ä¾§ä¸­éƒ¨æ·»åŠ å›¾ä¾‹ï¼Œâ€œå°åœ†â€å’Œâ€å¤§ % åœ†â€œï¼Œç»˜å›¾åŒºåŸŸè®¾ç½®ä¸ºæ­£æ–¹å½¢ï¼Œæ ‡é¢˜è®¾ç½®ä¸ºâ€œä¸¤ä¸ªåœ†â€ï¼Œæ¨ªè½´æ ‡ç­¾ä¸ºxï¼Œçºµè½´æ ‡ç­¾ä¸ºy % åŒåæ ‡è½´å›¾ % è½½å…¥æ•°æ®datahis.matï¼Œ % ç„¶åç»˜åˆ¶datahis0.t_hisä¸ºæ¨ªåæ ‡ï¼Œdatahis0.tempä¸ºå·¦ä¾§çºµåæ ‡ï¼Œdatahis0.humä¸ºå³ä¾§çºµåæ ‡çš„åŒåæ ‡è½´å›¾å½¢ % åœ¨å›¾åƒä¸‹æ–¹å¤–ä¾§æ·»åŠ å›¾ä¾‹â€œæ¸©åº¦â€ï¼Œâ€œæ¹¿åº¦â€ï¼Œæ¨ªè½´æ ‡ç­¾è®¾ç½®ä¸ºæ—¶é—´ï¼Œå·¦ä¾§yè½´è®¾ç½®æ ‡ç­¾ä¸ºæ¸©åº¦ï¼Œå³ä¾§yè½´æ ‡ç­¾ % è®¾ç½®ä¸ºæ¹¿åº¦ % datahis0.t_hisä¸ºæ—¶é—´ï¼Œéœ€è¦è½¬æ¢ä¸ºdatetimeç±»å‹ï¼Œç„¶åé€‰å–1æœˆ10æ—¥çš„æ•°æ®è¿›è¡Œç”»å›¾ clear figure load('datahis.mat') t = datetime(datevec(datahis0.t_his)); idx=t=datetime(2017,1,10)t=datetime(2017,1,11); t1 = t(idx); temp1 = datahis0.temp(idx); hum1 = datahis0.hum(idx); yyaxis left plot(t1,temp1) xlabel('æ—¶é—´') ylabel('æ¸©åº¦') yyaxis right plot(t1,hum1) ylabel('æ¸©åº¦') legend('æ¸©åº¦','æ¹¿åº¦','Location','southoutside','Orientation','horizontal') â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 | %% % å¤šå­å›¾çš„ç»˜åˆ¶ % ç»™å®šä»¥ä¸‹æ•°æ® % x = 0:0.01:20; % xåæ ‡ % y1 = 200*exp(-0.05*x).*sin(x); % Y1 % y2 = 0.8*exp(-0.5*x).*sin(10*x); % Y2 % y3 = 100*exp(-0.5*x).*sin(5*x); % Y3 % y1ï¼Œy2ï¼Œy3éœ€è¦åˆ†åˆ«ç»˜åˆ¶ä¸€ä¸ªå­å›¾ï¼Œå…¶ä¸­y1å æ®å·¦ä¾§ä¸€åŠä½ç½®ï¼Œy2å æ®å³ä¾§ä¸Šæ–¹ï¼Œy3å æ®å³ä¾§ä¸‹æ–¹ clear figure x= 0:0.01:20; % xåæ ‡ y1 = 200*exp(-0.05*x).*sin(x); % Y1 y2 = 0.8*exp(-0.5*x).*sin(10*x); % Y2 y3 = 100*exp(-0.5*x).*sin(5*x); % Y3 subplot(2,2,[1 3]) plot(x,y1) subplot(2,2,2) plot(x, y2) subplot(2,2,4) plot(x,y3) â€”|â€” 1 2 3 4 5 6 7 8 9 10 | %% % æ›²é¢å›¾ % ç»˜åˆ¶å‡½æ•°çš„ç½‘æ ¼å›¾,x,yçš„å–å€¼èŒƒå›´ä¸º-2åˆ°2 figure clear x = -2:0.1:2; y=x; [X,Y]=meshgrid(x,y) Z = sin(X.^2+Y.^2); mesh(X,Y,Z) â€”|â€” 1 2 3 4 5 6 7 8 9 10 | %% % ä¸‰ç»´é¥¼å½¢å›¾ % ç»˜åˆ¶ä¸‰ç»´é¥¼å½¢å›¾ï¼Œå„å…ƒç´ æ‰€å æ•°å€¼ä¸ºï¼š [6 3 7 5 1 2 4]ï¼Œçªå‡ºæ˜¾ç¤ºç¬¬1ï¼Œ3ä¸ªå…ƒç´ ï¼Œ7ä¸ªå…ƒç´ çš„æ ‡ç­¾åˆ†åˆ«ä¸º % 'å‘¨ä¸€'åˆ°'å‘¨æ—¥' figure x =[6 3 7 5 1 2 4]; labels = {'å‘¨ä¸€','å‘¨äºŒ','å‘¨ä¸‰','å‘¨å››','å‘¨äº”','å‘¨å…­','å‘¨æ—¥'}; explode=[1 0 1 0 0 0 0] %çªå‡ºæ˜¾ç¤ºå‘é‡xçš„å…ƒç´  pie3(x,explode,labels) â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 | %% % ç»˜åˆ¶å¦‚ä¸‹åŒçºµè½´æŸ±å½¢å›¾ã€‚æ•°æ®ä¸ºï¼š % x=1:20; % y1=sin(x)+2; % y2=(x-10).^2; % æ³¨æ„æŸ±å½¢å›¾ç­‰å›¾å½¢ä¹Ÿæ˜¯å¯ä»¥ä½¿ç”¨åŒåæ ‡è½´ç»˜åˆ¶çš„ clear figure x=1:20; y1=sin(x)+2; y2=(x-10).^2; yyaxis left bar(x+0.2,y1,0.3,'b') %æ³¨æ„å›¾å½¢å åŠ ä½ç½®çš„è°ƒæ•´ ylabel('sin') yyaxis right bar(x-0.2,y2,0.3,'r') ylabel('x^2') legend('sin','x^2','Location','southoutside','Orientation','horizontal') title('åŒçºµè½´æŸ±å½¢å›¾') xlabel('x') â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 | %% % ç»˜åˆ¶ä»¥ä¸‹æ•°æ®å¯¹åº”æ›²çº¿ï¼Œå¹¶å¢åŠ æ¨ªè½´æ ‡ç­¾åŠå¯¹åº”æ›²çº¿ä¸Šçš„æ•°å€¼åšæ ‡è®°ã€‚å¹¶åœ¨å›¾ä¸­å†™æ ‡è®°å…¬å¼æ–‡æœ¬ % x=2:20; % alpha=x.^2; % beta=log(x); % y=alpha./beta; clear figure x=2:20; alpha=x.^2; beta=log(x); y=alpha./beta; plot(x,y) hold on plot(sqrt(23),sqrt(23).^2./log(sqrt(23)),'o') text(6,100,'$$\\frac{\\alpha}{\\beta}$$','Interpreter','latex');%å­—ç¬¦çš„è¾“å‡º xticks(sort([2:2:20 sqrt(23)])) h1=gca; h1.XTickLabel{3}='$$\\sqrt{23}$$'; %å­—ç¬¦çš„è¾“å‡º h1.TickLabelInterpreter='latex'; â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | %% % ç»˜åˆ¶å¦‚ä¸‹å›¾å½¢ % t1 = datetime(2014,1:12,1); % temp = [0 2 12 11 15 25 23 27 25 24 12 8]; clear figure t1 = datetime(2014,1:12,1); temp = [0 2 12 11 15 25 23 27 25 24 12 8]; h = plot(t1,temp,':*'); ax = h.Parent; title('A Year of Temperatures on the 1st of the Month') ylabel('Degrees Celcius ^{\\circ}') % ä¸Šè§’æ ‡çš„ä½¿ç”¨ï¼Œæ‘„æ°åº¦ç¬¦å· yt1 = ax.YTickLabel %åæ ‡è½´æ ‡ç­¾ä¸ºå…ƒèƒæ•°ç»„ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šè¿›è¡Œä¿®æ”¹ ytld = strcat(yt1,'^{\\circ}') %ä¸Šè§’æ ‡ ax.YTickLabel = ytld; â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 | %% % æŒ‰ç…§è¦æ±‚ç»˜å›¾ % t=(pi*(0:1000)/1000)'; % y=sin(t); % ä»¥tä¸ºæ¨ªè½´æ•°æ®ï¼Œyä¸ºçºµè½´æ•°æ®ç»˜åˆ¶å›¾å½¢ï¼Œè¦æ±‚ï¼š % 1ï¼Œå°†çºµåæ ‡è½´è®¾ç½®åœ¨å³æ–¹ï¼Œ % 2ï¼Œå°†æ¨ªè½´æ ‡ç­¾è®¾ç½®ä¸º[0 1/4*pi 1/2*pi 3/4*pi pi]ï¼Œåæ ‡è½´èŒƒå›´è®¾ç½®ä¸º0ï½pi % 3ï¼Œè¾“å‡ºå›¾å½¢çš„figureçª—å£ä½ç½®åæ ‡ % 4ï¼Œè¾“å‡ºå›¾å½¢axesçš„ä½ç½®åæ ‡ clear figure t=(pi*(0:1000)/1000)'; y=sin(t); h = plot(t,y) set(gca,'YAxisLocation','right') xticks([0 1/4*pi 1/2*pi 3/4*pi pi]) xticklabels({'0','1/4 \\pi','1/2 \\pi','3/4 \\pi','\\pi'}) xlim([0 pi]) â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 | %% % æŒ‰ç…§ä»¥ä¸‹ä½ç½®ç»˜åˆ¶å›¾å½¢ % å…¶ä¸­éœ€è¦ç”¨åˆ°çš„æ•°æ®å¦‚ä¸‹ï¼Œå¦å¤–å›¾ç‰‡åæ ‡çš„ä½ç½®ä¸ç¤ºä¾‹å›¾ä¸­å¤§è‡´ç›¸ç­‰å³å¯ % % å›¾1 % x=linspace(0.2*pi,20); % y=sin(x); % % å›¾2 % t=0:pi/100:20*pi; % x=sin(t); % y=cos(t); % z=t.*sin(t).*cos(t); % % å›¾3 % [x,y]=meshgrid(-8:0.5:8); % z=sin(sqrt(x.^2+y.^2))./sqrt(x.^2+y.^2+eps); clear figure % å›¾1 x=linspace(0.2*pi,20); y=sin(x); axes('Position',[0.6,0.2,0.2,0.7],'GridLineStyle','-'); plot(y,x); grid on % å›¾2 axes('Position',[0.1,0.2,0.5,0.5]); t=0:pi/100:20*pi; x=sin(t); y=cos(t); z=t.*sin(t).*cos(t); plot3(x,y,z); % å›¾3 axes('Position',[0.1,0.6,0.25,0.3]); [x,y]=meshgrid(-8:0.5:8); z=sin(sqrt(x.^2+y.^2))./sqrt(x.^2+y.^2+eps); mesh(x,y,z) â€”|â€” 1 2 3 4 5 | %% % ä»¥ä¸‹æ•°æ®è¿›è¡Œæ‹Ÿåˆ % load ex1.mat % cftool load ex1.mat â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | %% % ä½¿ç”¨å‘½ä»¤å¯¼å…¥æ–‡ä»¶2016010600.txtä¸­çš„æ•°æ®ï¼Œæå–ç¬¬4åˆ—æ—¥æœŸæ•°æ®å’Œç¬¬5åˆ—æ—¶åˆ»æ•°æ®æ•°æ®ï¼Œåˆå¹¶ç”Ÿæˆä¸€ä¸ª % datetimeæ•°ç»„ï¼Œå‘½åä¸ºT clear a = readtable('2016010600.txt'); t1 = a.Var4; t2 = a.Var5; TPvec_y = floor(t1/10000); TPvec_mon = floor((t1-TPvec_y*10000)/100); TPvec_d = mod(t1,100) TPvec_h = floor(t2/100); TPvec_min=mod(t2,100); TPvec_s = zeros(size(t1,1),1); t_vec=[TPvec_y TPvec_mon TPvec_d TPvec_h TPvec_min TPvec_s]; T = datetime(t_vec) â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 | %% % ç”Ÿæˆä¸2016010600.txtæ–‡ä»¶ä¸­ç¬¬6åˆ—æ•°æ®æ¦‚ç‡åˆ†å¸ƒç›¸åŒçš„éšæœºæ•°ï¼Œæ•°é‡10000ä¸ª. clear figure a = readtable('2016010600.txt') v = a.Var6; histogram(v,20) [N,edges] = histcounts(v,20); rm = RDrnd(N,10000); figure histogram(rm,20) â€”|â€”","tags":["OpenCV"],"categories":["å·¥å…·"]},{"title":"Attentionæœºåˆ¶ä¸­SEnet CBAMä»¥åŠDual poolingçš„pytorchå®ç°","path":"/2019/12/27/20191227-SENet-code/","content":"æœ¬æ¥è‡ªå·±å†™äº†ï¼Œå…³äºSENetçš„æ³¨æ„åŠ›æˆªæ­¢ï¼Œä½†æ˜¯åœ¨å‡†å¤‡å†™å…¶ä»–æ³¨æ„åŠ›æœºåˆ¶ä»£ç çš„æ—¶å€™ï¼Œçœ‹åˆ°ä¸€ç¯‡æ–‡ç« æ€»ç»“çš„å¾ˆå¥½ï¼Œæ‰€ä»¥å¯¹æ­¤ç¯‡æ–‡ç« è¿›è¡Œæ¬è¿ï¼Œä»¥ä¾›è‡ªå·±æŸ¥é˜…ï¼Œå¹¶åŠ ä¸Šè‡ªå·±çš„ç†è§£ã€‚ [TOC] 1.SENETä¸­çš„channel-wiseåŠ æƒçš„å®ç°å®ç°ä»£ç å‚è€ƒè‡ªï¼šsenet.pytorchä»£ç å¦‚ä¸‹ï¼šSEnet æ¨¡å— from torch import nnclass SELayer(nn.Module): def __init__(self, channel, reduction=16): super(SELayer, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.fc = nn.Sequential( nn.Linear(channel, channel // reduction, bias=False), nn.ReLU(inplace=True), nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid() ) def forward(self, x): b, c, _, _ = x.size() y = self.avg_pool(x).view(b, c) y = self.fc(y).view(b, c, 1, 1) return x * y. (x) ä»¥ä¸Šä»£ç è®¾è®¡åˆ°çš„APIï¼š AdaptiveAvgPool2d: è‡ªé€‚åº”å¹³å‡æ± åŒ–ï¼Œå‚æ•°ä¸ºï¼ˆn,mï¼‰åˆ™å°†åŸæ¥çš„featureï¼ˆw,hï¼‰é€šè¿‡poolingå¾—åˆ°ï¼ˆn,mï¼‰çš„featureï¼Œå¦‚æœæ˜¯ï¼ˆnï¼‰,åˆ™å°†åŸæ¥çš„featureä»ï¼ˆw,hï¼‰é€šè¿‡poolingå¾—åˆ°ï¼ˆn,nï¼‰ Sequential: torchå®¹å™¨ï¼Œå­˜æ”¾ç½‘ç»œå±‚ç­‰å†…å®¹ã€‚ Linear: çº¿æ€§å±‚ï¼Œå‚æ•°ä¸ºï¼ˆin, outï¼‰,å°†åŸæœ‰çš„inä¸ªfeatureè½¬ä¸ºoutä¸ªfeature ReLU: æ¿€æ´»å±‚ï¼Œ inplaceè¿›è¡ŒåŸåœ°æ“ä½œï¼ŒèŠ‚çœå†…å­˜ Sigmoid: æ¿€æ´»å±‚ï¼Œå°†è¾“å…¥å‹ç¼©åˆ°0-1åˆ†æforwardè¿›è¡Œæ¨¡å‹çš„æ„å»ºï¼š xæ˜¯è¾“å…¥çš„feature,ä¸€èˆ¬å„ä¸ªé€šé“æ„ä¹‰å¦‚ä¸‹ï¼šï¼ˆbatch sizeï¼Œchannel, width , heightï¼‰,è¿™é‡Œè·å–äº†batch(b), channel xé€šè¿‡AdaptiveAvgPool2d(1)ä»¥åå°†å¾—åˆ°ï¼ˆbatch size, channel, 1, 1ï¼‰, ç„¶åviewï¼ˆb,cï¼‰æ„æ€æ˜¯æŒ‰ç…§b,cè¿›è¡Œå±•å¼€ ```pythonIn [1]: import torchIn [2]: x = torch.zeros((16,256,256,256))In [3]: import torch.nn as nnIn [4]: avg_pool = nn.AdaptiveAvgPool2d(1)In [5]: avg_pool(x).shapeOut[5]: torch.Size([16, 256, 1, 1])In [6]: avg_pool(x).view((16,256)).shapeOut[6]: torch.Size([16, 256])In [7]: avg_pool(x).squeeze().shape # squeeze()å‡½æ•°ä¹Ÿå¯ä»¥å°†æ‰€æœ‰é€šé“ä¸ªæ•°ä¸º1çš„è¿›è¡ŒæŒ¤å‹Out[7]: torch.Size([16, 256]) ç„¶åå½¢çŠ¶ä¸ºã€16, 256ã€‘çš„tensorç»è¿‡fc: (1) Linear: from 256(channel) to 256/16 (2) ReLuï¼šè¿›è¡Œä¸€æ¬¡æ¿€æ´»å‡½æ•° (3) Linear: from 256/16 to 256(channel) (4) Sigmoid: æ¿€æ´»åˆ°0-1ï¼Œä»£è¡¨æ¯ä¸ªé€šé“çš„é‡è¦æ€§ ç„¶åé€šè¿‡viewæ“ä½œè½¬åŒ–ä¸ºã€16,256,1,1ã€‘å½¢çŠ¶çš„tensor ç°åœ¨yå¾—åˆ°çš„æ˜¯æ¯ä¸€ä¸ªé€šé“å¯¹åº”çš„åˆ†æ•°ï¼ˆ0-1ï¼‰ï¼Œç„¶åéœ€è¦å°†å…¶ä¸é€šé“å†…å®¹ç›¸ä¹˜ï¼Œå…·ä½“æ“ä½œä½¿ç”¨åˆ°äº†tensorçš„å†…ç½®å‡½æ•°expand_as(æŠŠä¸€ä¸ªtensorå˜æˆå’Œå‡½æ•°æ‹¬å·å†…ä¸€æ ·å½¢çŠ¶çš„tensorï¼Œç”¨æ³•ä¸expandç±»ä¼¼ï¼Œç›¸å½“äºexpand(tensor.size()) xæ˜¯ã€16,256,256,256ã€‘å½¢çŠ¶çš„ç‰¹å¾å›¾ï¼Œyæ˜¯ã€16,256,1,1ã€‘å¤§å°çš„channel-wiseåˆ†æ•°ï¼Œç„¶åéœ€è¦å°†å…¶ç›¸ä¹˜ b.expand_as(a)å°±æ˜¯å°†bè¿›è¡Œæ‰©å……ï¼Œæ‰©å……åˆ°açš„ç»´åº¦ï¼Œéœ€è¦è¯´æ˜çš„æ˜¯açš„ä½ç»´åº¦éœ€è¦æ¯”bå¤§ï¼Œä¾‹å¦‚bçš„shapeæ˜¯31ï¼Œå¦‚æœaçš„shapeæ˜¯32ä¸ä¼šå‡ºé”™ï¼Œä½†æ˜¯æ˜¯2*2å°±ä¼šæŠ¥é”™äº†ã€‚å°±æ˜¯å¿…é¡»æœ‰ä¸€ä¸ªç»´åº¦æ˜¯1ï¼Œç„¶åç”¨äºæ‰©å±•ï¼š123In [8]: tensor1 = torch.ones((3,4,1,1))In [9]: tensor1.expand([3,4,5,5]).shapeOut[9]: torch.Size([3, 4, 5, 5]) è¿™æ ·é€šè¿‡expand_aså°±èƒ½å¾—åˆ°ã€16,256,256,256ã€‘å¤§å°çš„tensorï¼Œå…¶ä¸­256*256éƒ½æ˜¯å¯¹åº”é€šé“çš„1åˆ†æ•°ï¼Œç„¶åä¸åŸå…ˆçš„featureç›¸ä¹˜ï¼Œå°±èƒ½å¾—åˆ°channel-wiseåˆ†æ•°è®¡ç®—åçš„featureã€‚åœ¨resetnetä¸­çš„blockæ’å…¥senetæ¨¡å— ```python class CifarSEBasicBlock(nn.Module): def __init__(self, inplanes, planes, stride=1, reduction=16): super(CifarSEBasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = nn.BatchNorm2d(planes) self.se = SELayer(planes, reduction) if inplanes != planes: self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes)) else: self.downsample = lambda x: x self.stride = stride def forward(self, x): residual = self.downsample(x) out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.se(out) out += residual out = self.relu(out) return out æ­£å¸¸çš„resentçš„BasicBlock class BasicBlock(nn.Module): def __init__(self, inplanes, planes, stride=1): super(BasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = nn.BatchNorm2d(planes) if inplanes != planes: self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes)) else: self.downsample = lambda x: x self.stride = stride baseline:0.888se+baseline:0.892 2.CBAMä¸­çš„é€šé“æ³¨æ„åŠ›æœºåˆ¶channel-attention-moduleè·Ÿä»¥ä¸Šå†…å®¹æƒ³æ³•æœ‰ä¸€ç‚¹åƒï¼Œç»™æ¯ä¸ªchannelè¿›è¡Œæ‰“åˆ†ï¼Œå…·ä½“å®ç°å¦‚ä¸‹ï¼šå‚è€ƒæ¥æºï¼šCBMA.pytorch class ChannelAttention(nn.Module): def __init__(self, in_planes, ratio=16): super(ChannelAttention, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.max_pool = nn.AdaptiveMaxPool2d(1) self.fc1 = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False) self.relu1 = nn.ReLU() self.fc2 = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False) self.sigmoid = nn.Sigmoid() def forward(self, x): avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x)))) max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x)))) out = avg_out + max_out return self.sigmoid(out) APIè·Ÿä¸Šè¾¹ç±»ä¼¼ï¼Œåªæ·»åŠ äº†å·ç§¯ï¼Œä¹Ÿå¾ˆç®€å•ã€‚éœ€è¦è¯´æ˜çš„æ˜¯è²Œä¼¼Linearå’ŒConv2dä¸­çš„å‚æ•°å¾ˆç›¸ä¼¼ï¼Œä½†æ˜¯å®é™…ä¸Šï¼Œä¸¤è€…è¿˜æ˜¯å¾ˆä¸ä¸€æ ·çš„ï¼ŒLinearæ¥å—çš„æ˜¯çº¿æ€§çš„2ç»´æ•°ç»„ï¼ˆbatch, ä¸€ç»´ç‰¹å¾ï¼‰ï¼ŒCon2dæ¥å—çš„æ˜¯4ç»´æ•°ç»„ï¼ˆbatch, é€šé“ï¼Œw, hï¼‰ã€‚forwardå‡½æ•°ï¼š ç¬¬ä¸€è¡Œï¼Œè¿›è¡Œäº†adaptiveAvgPoolingï¼Œ conv2d, relu, conv2d ç¬¬äºŒè¡Œï¼Œè¿›è¡Œäº†AdaptiveMaxPooling, conv2d, relu, conv2d ç¬¬ä¸‰è¡Œï¼Œå°†ä¸¤ä¸ªå‘é‡è¿›è¡Œç›¸åŠ  ç¬¬å››è¡Œï¼Œå°†å¯¹åº”ç»“æœè¿›è¡Œæ¿€æ´»ï¼Œå¾—åˆ°é€šé“æ³¨æ„åŠ›åˆ†æ•° 3.CBAMä¸­çš„ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶å‚è€ƒæ¥æºï¼šCBMA.pytorch class SpatialAttention(nn.Module):def __init__(self, kernel_size=7):super(SpatialAttention, self).__init__()assert kernel_size in (3, 7), 'kernel size must be 3 or 7'padding = 3 if kernel_size == 7 else 1self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)self.sigmoid = nn.Sigmoid()def forward(self, x):avg_out = torch.mean(x, dim=1, keepdim=True)max_out, _ = torch.max(x, dim=1, keepdim=True)x = torch.cat([avg_out, max_out], dim=1)x = self.conv1(x)return self.sigmoid(x) Spatial attention moduleä¸­æ”¯æŒkernel_size=3æˆ–è€…7ï¼Œé»˜è®¤è®¾ç½®ä¸º7ã€‚ä»¥ä¸Šæ¶‰åŠåˆ°çš„API: torch.mean: æ±‚å¹³å‡å€¼ï¼ŒdimæŒ‡çš„æ˜¯æ²¿ç€æŸä¸€ä¸ªé€šé“è¿›è¡Œè®¡ç®—å¹³å‡å€¼ã€‚è¿™é‡Œdim=1ï¼Œè¯´æ˜æ²¿ç€é€šé“channelè¿›è¡Œå¹³å‡ï¼Œå¯¹æ‰€æœ‰channelçš„featureä¸Šç›¸åº”çš„åƒç´ è¿›è¡Œæ±‚å¹³å‡å€¼ã€‚ torch.max: åŒä¸Šï¼Œè¿›è¡Œæ±‚æœ€å¤§å€¼ã€‚forwardå‡½æ•°ï¼š ç¬¬ä¸€è¡Œï¼šæ²¿ç€é€šé“ç»´åº¦è¿›è¡Œè¿›è¡Œå¹³å‡ï¼Œå¾—åˆ°ä¸€ä¸ªï¼ˆbatch, 1, w, hï¼‰çš„feature ç¬¬äºŒè¡Œï¼šæ²¿ç€é€šé“ç»´åº¦è¿›è¡Œæ±‚æœ€å¤§å€¼ï¼Œå¾—åˆ°ä¸€ä¸ªï¼ˆbatch, 1, w, hï¼‰çš„feature ç¬¬ä¸‰è¡Œï¼šå°†ä¸¤ä¸ªfeatureé€šè¿‡catçš„æ–¹å¼æ‹¼æ¥èµ·æ¥ï¼Œå¾—åˆ°ä¸€ä¸ªï¼ˆbatch, 2, w, hï¼‰çš„feature ç¬¬å››è¡Œï¼šå¯¹è¿™ä¸ªfeatureè¿›è¡Œå·ç§¯ä¹‹æ‰€ä»¥è®¾ç½®å¦‚æœkernel_size=7çš„æ—¶å€™padding=3æ˜¯å› ä¸ºéœ€è¦å°†out_featureå’Œin_featureç›¸ç­‰ï¼Œå¯ä»¥å¸¦å…¥å…¬å¼è¿›è¡Œè®¡ç®—ã€‚ ç¬¬äº”è¡Œï¼šè¿›è¡Œæ¿€æ´»ï¼Œå°†å¾—åˆ†çº¦æŸè‡³[0-1] 4.CBAMä¸­çš„èåˆå‚è€ƒä»£ç ï¼šCBMA.pytorchåœ¨resnetä¸­ä¸»è¦æ˜¯ç”¨åœ¨basicBlockä¸­ï¼Œä»£ç å¦‚ä¸‹ï¼š class BasicBlock(nn.Module): expansion = 1 def __init__(self, inplanes, planes, stride=1, downsample=None): super(BasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = nn.BatchNorm2d(planes) self.ca = ChannelAttention(planes) self.sa = SpatialAttention() self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.ca(out) * out # å¹¿æ’­æœºåˆ¶ out = self.sa(out) * out # å¹¿æ’­æœºåˆ¶ if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return out resnet50+cbam: 0.902 5.dual poolingçš„pytorchå®ç°max poolingæ›´æ³¨é‡é‡è¦çš„å±€éƒ¨ç‰¹å¾, average poolingæ›´å…³æ³¨å…¨å±€ç‰¹å¾.ä¸¤è€…concatå¯ä»¥ä¸°å¯Œç‰¹å¾å±‚.å‚è€ƒé“¾æ¥:GaryLIU class res18(nn.Module): def __init__(self, num_classes): super(res18, self).__init__() self.base = resnet18(pretrained=True) self.feature = nn.Sequential( self.base.conv1, self.base.bn1, self.base.relu, self.base.maxpool, self.base.layer1, self.base.layer2, self.base.layer3, self.base.layer4 ) self.avg_pool = nn.AdaptiveAvgPool2d(1) self.max_pool = nn.AdaptiveMaxPool2d(1) self.reduce_layer = nn.Conv2d(1024, 512, 1) self.fc = nn.Sequential( nn.Dropout(0.5), nn.Linear(512, num_classes) ) def forward(self, x): bs = x.shape[0] x = self.feature(x) x1 = self.avg_pool(x) x2 = self.max_pool(x) x = torch.cat([x1, x2], dim=1) x = self.reduce_layer(x).view(bs, -1) logits = self.fc(x) return logits è¿™ç§æ˜¯åœ¨æ¨¡å‹å±‚è¿›è¡Œæ”¹é€ çš„ä¸€ç§å°trickäº†ï¼Œå¸¸è§çš„åšæ³•ï¼šglobal max/average pooling + fc layerï¼Œè¿™é‡Œè¯•concat(global max-pooling, global average pooling) + fc layerï¼Œå…¶å®å°±æ˜¯ä¸ºäº†ä¸°å¯Œç‰¹å¾å±‚ï¼Œmax poolingæ›´åŠ å…³æ³¨é‡è¦çš„å±€éƒ¨ç‰¹å¾ï¼Œè€Œaverage poolingè¯•æ›´åŠ å…³æ³¨å…¨å±€çš„ç‰¹å¾ã€‚ä¸ä¸€å®šæœ‰æ•ˆï¼Œæˆ‘è¯•è¿‡ä¸å°‘æ¬¡ï¼Œæœ‰æ•ˆçš„æ¬¡æ•°æ¯”è¾ƒå°‘ï¼Œä½†ä¸å°‘äººå–œæ¬¢è¿™æ ·ç”¨.-grayä»¥ä¸Šå°±æ˜¯dual poolingçš„å®ç°ï¼Œå…·ä½“åˆ†æå¦‚ä¸‹ï¼š ç¬¬ä¸€è¡Œï¼šå¾—åˆ°batch-size ç¬¬äºŒè¡Œï¼šå¾—åˆ°feature, grayå¤§ä½¬è¿™é‡Œç”¨çš„æ˜¯ä¸€ä¸ªsequentialå°†æ‰€æœ‰çš„æ¨¡å—è£…è½½è¿›æ¥ï¼Œå…¶å®ä¹Ÿå¯ä»¥ç”¨è¿™ç§æ–¹æ³•ï¼š 1 | self.base_model = nn.Sequential(*list(model_ft.children())[:-3]) # å–é™¤äº†åä¸‰ä¸ªå…¨éƒ¨çš„å±‚ â€”|â€” childrenæ–¹æ³•é‡Œå°±æ˜¯è¿”å›å½“å‰æ¨¡å‹å­æ¨¡å—çš„è¿­ä»£å™¨ï¼Œå¯ä»¥æŸ¥çœ‹æºä»£ç ï¼Œç„¶åé€‰æ‹©å°†å…¶ä¸­ä¸€éƒ¨åˆ†å»æ‰ï¼Œæ¯”å¦‚fcå±‚ç­‰ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨grayå¤§ä½¬çš„è¿™ç§æ–¹æ³•ã€‚æŸ¥æ‰¾çš„è¿‡ç¨‹ä¸­æ‰¾åˆ°ä¸€ä¸ªä¸­é—´å±‚å¯è§†åŒ–çš„ç®€å•ä»£ç ï¼šhttps://www.jianshu.com/p/0a23db1df55a ç¬¬å››ï¼Œäº”è¡Œï¼Œé€šè¿‡avg_pool,max_poolå¾—åˆ°å¯¹åº”çš„feature ç¬¬å…­è¡Œï¼Œè¿›è¡Œconcateæ“ä½œï¼Œè¿›è¡Œæ‹¼æ¥ ç¬¬ä¸ƒè¡Œï¼Œä½¿ç”¨äº†ä¸€ä¸ªå·ç§¯å±‚è¿›è¡Œé™ç»´é€šé“ï¼Œå¹¶è¿›è¡Œviewå±•å¼€æˆä¸€ç»´å‘é‡ã€‚ ç¬¬å…«å±‚ï¼Œè¿›è¡Œå…¨è¿æ¥å±‚çš„åˆ†ç±»ã€‚å‚è€ƒé“¾æ¥ï¼šhttps://blog.csdn.net/DD_PP_JJ/article/details/103318617","tags":["æ·±åº¦å­¦ä¹ "],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"SpatialSense","path":"/2019/12/27/20191227-SpatialSense/","content":"æ‘˜è¦ç†è§£å›¾åƒä¸­ç‰©ä½“ä¹‹é—´çš„ç©ºé—´å…³ç³»æ˜¯ä¸€é¡¹å…·æœ‰æƒŠäººæŒ‘æˆ˜æ€§çš„ä»»åŠ¡(å›¾1)ã€‚æ¤…å­å¯èƒ½â€œåœ¨â€ä¸€ä¸ªäººçš„â€œåé¢â€ï¼Œå³ä½¿å®ƒå‡ºç°åœ¨äººçš„å·¦è¾¹(å–å†³äºäººé¢å¯¹çš„æ–¹å‘)ã€‚å¦‚æœæœ‰ç¬¬ä¸‰ä¸ªå­¦ç”Ÿåœ¨ä»–ä»¬ä¹‹é—´ï¼Œé‚£ä¹ˆä¸¤ä¸ªçœ‹èµ·æ¥å¾ˆè¿‘çš„å­¦ç”Ÿå®é™…ä¸Šå¯èƒ½å¹¶ä¸â€œæŒ¨ç€â€ã€‚æˆ‘ä»¬ä»‹ç»äº†spatial alsenseï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç ”ç©¶ç©ºé—´å…³ç³»è¯†åˆ«çš„æ•°æ®é›†ï¼Œå®ƒæ•æ‰äº†å¹¿æ³›çš„æ­¤ç±»æŒ‘æˆ˜ï¼Œå…è®¸å¯¹è®¡ç®—æœºè§†è§‰æŠ€æœ¯è¿›è¡Œé€‚å½“çš„åŸºå‡†æµ‹è¯•ã€‚ç©ºé—´æ„ŸçŸ¥æ˜¯é€šè¿‡å¯¹æŠ—æ€§çš„ä¼—åŒ…æ¥æ„å»ºçš„ï¼Œåœ¨ä¼—åŒ…ä¸­ï¼Œäººç±»æ³¨é‡Šè€…çš„ä»»åŠ¡æ˜¯å‘ç°ç©ºé—´å…³ç³»ï¼Œè¿™äº›å…³ç³»å¾ˆéš¾ç”¨ç®€å•çš„çº¿ç´¢æ¥é¢„æµ‹ï¼Œæ¯”å¦‚äºŒç»´ç©ºé—´ç»“æ„æˆ–è¯­è¨€å…ˆéªŒã€‚ä¸ç°æœ‰çš„æ•°æ®é›†ç›¸æ¯”ï¼Œå¯¹æŠ—æ€§ä¼—åŒ…å¤§å¤§å‡å°‘äº†æ•°æ®é›†çš„åå€šï¼Œå¹¶åœ¨é•¿å°¾æŠ½å–äº†æ›´æœ‰è¶£çš„å…³ç³»æ ·æœ¬ã€‚åœ¨ç©ºé—´æ„Ÿæ–¹é¢ï¼Œæœ€å…ˆè¿›çš„è¯†åˆ«æ¨¡å‹ä¸ç®€å•çš„åŸºçº¿ç›¸æ¯”ï¼Œè¡¨ç°å¾—æ›´ä¸ºå‡ºè‰²ï¼Œè¿™è¡¨æ˜å®ƒä»¬ä¾èµ–äºç›´æ¥çš„çº¿ç´¢ï¼Œè€Œä¸æ˜¯å¯¹è¿™ä¸ªå¤æ‚çš„ä»»åŠ¡è¿›è¡Œå……åˆ†çš„æ¨ç†ã€‚ç©ºé—´æ„Ÿè§‰åŸºå‡†æµ‹è¯•ä¸ºæé«˜è®¡ç®—æœºè§†è§‰ç³»ç»Ÿçš„ç©ºé—´æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€æ¡é€”å¾„ã€‚ Introduction","tags":["è®¡ç®—æœºè§†è§‰"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"Attentionæœºåˆ¶ä¸­SEnet CBAMä»¥åŠDual poolingçš„pytorchå®ç°","path":"/2019/12/27/SENet-code/","content":"æœ¬æ¥è‡ªå·±å†™äº†ï¼Œå…³äºSENetçš„æ³¨æ„åŠ›æˆªæ­¢ï¼Œä½†æ˜¯åœ¨å‡†å¤‡å†™å…¶ä»–æ³¨æ„åŠ›æœºåˆ¶ä»£ç çš„æ—¶å€™ï¼Œçœ‹åˆ°ä¸€ç¯‡æ–‡ç« æ€»ç»“çš„å¾ˆå¥½ï¼Œæ‰€ä»¥å¯¹æ­¤ç¯‡æ–‡ç« è¿›è¡Œæ¬è¿ï¼Œä»¥ä¾›è‡ªå·±æŸ¥é˜…ï¼Œå¹¶åŠ ä¸Šè‡ªå·±çš„ç†è§£ã€‚ [TOC] 1.SENETä¸­çš„channel-wiseåŠ æƒçš„å®ç°å®ç°ä»£ç å‚è€ƒè‡ªï¼šsenet.pytorchä»£ç å¦‚ä¸‹ï¼šSEnet æ¨¡å— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | from torch import nn class SELayer(nn.Module): def __init__(self, channel, reduction=16): super(SELayer, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.fc = nn.Sequential( nn.Linear(channel, channel // reduction, bias=False), nn.ReLU(inplace=True), nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid() ) def forward(self, x): b, c, _, _ = x.size() y = self.avg_pool(x).view(b, c) y = self.fc(y).view(b, c, 1, 1) return x * y. (x) â€”|â€” ä»¥ä¸Šä»£ç è®¾è®¡åˆ°çš„APIï¼š AdaptiveAvgPool2d: è‡ªé€‚åº”å¹³å‡æ± åŒ–ï¼Œå‚æ•°ä¸ºï¼ˆn,mï¼‰åˆ™å°†åŸæ¥çš„featureï¼ˆw,hï¼‰é€šè¿‡poolingå¾—åˆ°ï¼ˆn,mï¼‰çš„featureï¼Œå¦‚æœæ˜¯ï¼ˆnï¼‰,åˆ™å°†åŸæ¥çš„featureä»ï¼ˆw,hï¼‰é€šè¿‡poolingå¾—åˆ°ï¼ˆn,nï¼‰ Sequential: torchå®¹å™¨ï¼Œå­˜æ”¾ç½‘ç»œå±‚ç­‰å†…å®¹ã€‚ Linear: çº¿æ€§å±‚ï¼Œå‚æ•°ä¸ºï¼ˆin, outï¼‰,å°†åŸæœ‰çš„inä¸ªfeatureè½¬ä¸ºoutä¸ªfeature ReLU: æ¿€æ´»å±‚ï¼Œ inplaceè¿›è¡ŒåŸåœ°æ“ä½œï¼ŒèŠ‚çœå†…å­˜ Sigmoid: æ¿€æ´»å±‚ï¼Œå°†è¾“å…¥å‹ç¼©åˆ°0-1åˆ†æforwardè¿›è¡Œæ¨¡å‹çš„æ„å»ºï¼š xæ˜¯è¾“å…¥çš„feature,ä¸€èˆ¬å„ä¸ªé€šé“æ„ä¹‰å¦‚ä¸‹ï¼šï¼ˆbatch sizeï¼Œchannel, width , heightï¼‰,è¿™é‡Œè·å–äº†batch(b), channel xé€šè¿‡AdaptiveAvgPool2d(1)ä»¥åå°†å¾—åˆ°ï¼ˆbatch size, channel, 1, 1ï¼‰, ç„¶åviewï¼ˆb,cï¼‰æ„æ€æ˜¯æŒ‰ç…§b,cè¿›è¡Œå±•å¼€ 1 2 3 4 5 6 7 8 9 10 | In [1]: import torch In [2]: x = torch.zeros((16,256,256,256)) In [3]: import torch.nn as nn In [4]: avg_pool = nn.AdaptiveAvgPool2d(1) In [5]: avg_pool(x).shape Out[5]: torch.Size([16, 256, 1, 1]) In [6]: avg_pool(x).view((16,256)).shape Out[6]: torch.Size([16, 256]) In [7]: avg_pool(x).squeeze().shape # squeeze()å‡½æ•°ä¹Ÿå¯ä»¥å°†æ‰€æœ‰é€šé“ä¸ªæ•°ä¸º1çš„è¿›è¡ŒæŒ¤å‹ Out[7]: torch.Size([16, 256]) â€”|â€” ç„¶åå½¢çŠ¶ä¸ºã€16, 256ã€‘çš„tensorç»è¿‡fc: (1) Linear: from 256(channel) to 256/16 (2) ReLuï¼šè¿›è¡Œä¸€æ¬¡æ¿€æ´»å‡½æ•° (3) Linear: from 256/16 to 256(channel) (4) Sigmoid: æ¿€æ´»åˆ°0-1ï¼Œä»£è¡¨æ¯ä¸ªé€šé“çš„é‡è¦æ€§ ç„¶åé€šè¿‡viewæ“ä½œè½¬åŒ–ä¸ºã€16,256,1,1ã€‘å½¢çŠ¶çš„tensor ç°åœ¨yå¾—åˆ°çš„æ˜¯æ¯ä¸€ä¸ªé€šé“å¯¹åº”çš„åˆ†æ•°ï¼ˆ0-1ï¼‰ï¼Œç„¶åéœ€è¦å°†å…¶ä¸é€šé“å†…å®¹ç›¸ä¹˜ï¼Œå…·ä½“æ“ä½œä½¿ç”¨åˆ°äº†tensorçš„å†…ç½®å‡½æ•°expand_as(æŠŠä¸€ä¸ªtensorå˜æˆå’Œå‡½æ•°æ‹¬å·å†…ä¸€æ ·å½¢çŠ¶çš„tensorï¼Œç”¨æ³•ä¸expandç±»ä¼¼ï¼Œç›¸å½“äºexpand(tensor.size()) xæ˜¯ã€16,256,256,256ã€‘å½¢çŠ¶çš„ç‰¹å¾å›¾ï¼Œyæ˜¯ã€16,256,1,1ã€‘å¤§å°çš„channel-wiseåˆ†æ•°ï¼Œç„¶åéœ€è¦å°†å…¶ç›¸ä¹˜ b.expand_as(a)å°±æ˜¯å°†bè¿›è¡Œæ‰©å……ï¼Œæ‰©å……åˆ°açš„ç»´åº¦ï¼Œéœ€è¦è¯´æ˜çš„æ˜¯açš„ä½ç»´åº¦éœ€è¦æ¯”bå¤§ï¼Œä¾‹å¦‚bçš„shapeæ˜¯31ï¼Œå¦‚æœaçš„shapeæ˜¯32ä¸ä¼šå‡ºé”™ï¼Œä½†æ˜¯æ˜¯2*2å°±ä¼šæŠ¥é”™äº†ã€‚å°±æ˜¯å¿…é¡»æœ‰ä¸€ä¸ªç»´åº¦æ˜¯1ï¼Œç„¶åç”¨äºæ‰©å±•ï¼š 1 2 3 | In [8]: tensor1 = torch.ones((3,4,1,1)) In [9]: tensor1.expand([3,4,5,5]).shape Out[9]: torch.Size([3, 4, 5, 5]) â€”|â€” è¿™æ ·é€šè¿‡expand_aså°±èƒ½å¾—åˆ°ã€16,256,256,256ã€‘å¤§å°çš„tensorï¼Œå…¶ä¸­256*256éƒ½æ˜¯å¯¹åº”é€šé“çš„1åˆ†æ•°ï¼Œç„¶åä¸åŸå…ˆçš„featureç›¸ä¹˜ï¼Œå°±èƒ½å¾—åˆ°channel-wiseåˆ†æ•°è®¡ç®—åçš„featureã€‚ åœ¨resetnetä¸­çš„blockæ’å…¥senetæ¨¡å— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 | class CifarSEBasicBlock(nn.Module): def __init__(self, inplanes, planes, stride=1, reduction=16): super(CifarSEBasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = nn.BatchNorm2d(planes) self.se = SELayer(planes, reduction) if inplanes != planes: self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes)) else: self.downsample = lambda x: x self.stride = stride def forward(self, x): residual = self.downsample(x) out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.se(out) out += residual out = self.relu(out) return out â€”|â€” æ­£å¸¸çš„resentçš„BasicBlock 1 2 3 4 5 6 7 8 9 10 11 12 13 14 | class BasicBlock(nn.Module): def __init__(self, inplanes, planes, stride=1): super(BasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = nn.BatchNorm2d(planes) if inplanes != planes: self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes)) else: self.downsample = lambda x: x self.stride = stride â€”|â€” baseline:0.888se+baseline:0.892 2.CBAMä¸­çš„é€šé“æ³¨æ„åŠ›æœºåˆ¶channel-attention-moduleè·Ÿä»¥ä¸Šå†…å®¹æƒ³æ³•æœ‰ä¸€ç‚¹åƒï¼Œç»™æ¯ä¸ªchannelè¿›è¡Œæ‰“åˆ†ï¼Œå…·ä½“å®ç°å¦‚ä¸‹ï¼šå‚è€ƒæ¥æºï¼šCBMA.pytorch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | class ChannelAttention(nn.Module): def __init__(self, in_planes, ratio=16): super(ChannelAttention, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.max_pool = nn.AdaptiveMaxPool2d(1) self.fc1 = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False) self.relu1 = nn.ReLU() self.fc2 = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False) self.sigmoid = nn.Sigmoid() def forward(self, x): avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x)))) max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x)))) out = avg_out + max_out return self.sigmoid(out) â€”|â€” APIè·Ÿä¸Šè¾¹ç±»ä¼¼ï¼Œåªæ·»åŠ äº†å·ç§¯ï¼Œä¹Ÿå¾ˆç®€å•ã€‚éœ€è¦è¯´æ˜çš„æ˜¯è²Œä¼¼Linearå’ŒConv2dä¸­çš„å‚æ•°å¾ˆç›¸ä¼¼ï¼Œä½†æ˜¯å®é™…ä¸Šï¼Œä¸¤è€…è¿˜æ˜¯å¾ˆä¸ä¸€æ ·çš„ï¼ŒLinearæ¥å—çš„æ˜¯çº¿æ€§çš„2ç»´æ•°ç»„ï¼ˆbatch, ä¸€ç»´ç‰¹å¾ï¼‰ï¼ŒCon2dæ¥å—çš„æ˜¯4ç»´æ•°ç»„ï¼ˆbatch, é€šé“ï¼Œw, hï¼‰ã€‚forwardå‡½æ•°ï¼š ç¬¬ä¸€è¡Œï¼Œè¿›è¡Œäº†adaptiveAvgPoolingï¼Œ conv2d, relu, conv2d ç¬¬äºŒè¡Œï¼Œè¿›è¡Œäº†AdaptiveMaxPooling, conv2d, relu, conv2d ç¬¬ä¸‰è¡Œï¼Œå°†ä¸¤ä¸ªå‘é‡è¿›è¡Œç›¸åŠ  ç¬¬å››è¡Œï¼Œå°†å¯¹åº”ç»“æœè¿›è¡Œæ¿€æ´»ï¼Œå¾—åˆ°é€šé“æ³¨æ„åŠ›åˆ†æ•° 3.CBAMä¸­çš„ç©ºé—´æ³¨æ„åŠ›æœºåˆ¶å‚è€ƒæ¥æºï¼šCBMA.pytorch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 | class SpatialAttention(nn.Module): def __init__(self, kernel_size=7): super(SpatialAttention, self).__init__() assert kernel_size in (3, 7), 'kernel size must be 3 or 7' padding = 3 if kernel_size == 7 else 1 self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False) self.sigmoid = nn.Sigmoid() def forward(self, x): avg_out = torch.mean(x, dim=1, keepdim=True) max_out, _ = torch.max(x, dim=1, keepdim=True) x = torch.cat([avg_out, max_out], dim=1) x = self.conv1(x) return self.sigmoid(x) â€”|â€” Spatial attention moduleä¸­æ”¯æŒkernel_size=3æˆ–è€…7ï¼Œé»˜è®¤è®¾ç½®ä¸º7ã€‚ä»¥ä¸Šæ¶‰åŠåˆ°çš„API: torch.mean: æ±‚å¹³å‡å€¼ï¼ŒdimæŒ‡çš„æ˜¯æ²¿ç€æŸä¸€ä¸ªé€šé“è¿›è¡Œè®¡ç®—å¹³å‡å€¼ã€‚è¿™é‡Œdim=1ï¼Œè¯´æ˜æ²¿ç€é€šé“channelè¿›è¡Œå¹³å‡ï¼Œå¯¹æ‰€æœ‰channelçš„featureä¸Šç›¸åº”çš„åƒç´ è¿›è¡Œæ±‚å¹³å‡å€¼ã€‚ torch.max: åŒä¸Šï¼Œè¿›è¡Œæ±‚æœ€å¤§å€¼ã€‚forwardå‡½æ•°ï¼š ç¬¬ä¸€è¡Œï¼šæ²¿ç€é€šé“ç»´åº¦è¿›è¡Œè¿›è¡Œå¹³å‡ï¼Œå¾—åˆ°ä¸€ä¸ªï¼ˆbatch, 1, w, hï¼‰çš„feature ç¬¬äºŒè¡Œï¼šæ²¿ç€é€šé“ç»´åº¦è¿›è¡Œæ±‚æœ€å¤§å€¼ï¼Œå¾—åˆ°ä¸€ä¸ªï¼ˆbatch, 1, w, hï¼‰çš„feature ç¬¬ä¸‰è¡Œï¼šå°†ä¸¤ä¸ªfeatureé€šè¿‡catçš„æ–¹å¼æ‹¼æ¥èµ·æ¥ï¼Œå¾—åˆ°ä¸€ä¸ªï¼ˆbatch, 2, w, hï¼‰çš„feature ç¬¬å››è¡Œï¼šå¯¹è¿™ä¸ªfeatureè¿›è¡Œå·ç§¯ä¹‹æ‰€ä»¥è®¾ç½®å¦‚æœkernel_size=7çš„æ—¶å€™padding=3æ˜¯å› ä¸ºéœ€è¦å°†out_featureå’Œin_featureç›¸ç­‰ï¼Œå¯ä»¥å¸¦å…¥å…¬å¼è¿›è¡Œè®¡ç®—ã€‚ ç¬¬äº”è¡Œï¼šè¿›è¡Œæ¿€æ´»ï¼Œå°†å¾—åˆ†çº¦æŸè‡³[0-1] 4.CBAMä¸­çš„èåˆå‚è€ƒä»£ç ï¼šCBMA.pytorchåœ¨resnetä¸­ä¸»è¦æ˜¯ç”¨åœ¨basicBlockä¸­ï¼Œä»£ç å¦‚ä¸‹ï¼š 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 | class BasicBlock(nn.Module): expansion = 1 def __init__(self, inplanes, planes, stride=1, downsample=None): super(BasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = nn.BatchNorm2d(planes) self.ca = ChannelAttention(planes) self.sa = SpatialAttention() self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.ca(out) * out # å¹¿æ’­æœºåˆ¶ out = self.sa(out) * out # å¹¿æ’­æœºåˆ¶ if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return out â€”|â€” resnet50+cbam: 0.902 5.dual poolingçš„pytorchå®ç°max poolingæ›´æ³¨é‡é‡è¦çš„å±€éƒ¨ç‰¹å¾, average poolingæ›´å…³æ³¨å…¨å±€ç‰¹å¾.ä¸¤è€…concatå¯ä»¥ä¸°å¯Œç‰¹å¾å±‚.å‚è€ƒé“¾æ¥:GaryLIU 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 | class res18(nn.Module): def __init__(self, num_classes): super(res18, self).__init__() self.base = resnet18(pretrained=True) self.feature = nn.Sequential( self.base.conv1, self.base.bn1, self.base.relu, self.base.maxpool, self.base.layer1, self.base.layer2, self.base.layer3, self.base.layer4 ) self.avg_pool = nn.AdaptiveAvgPool2d(1) self.max_pool = nn.AdaptiveMaxPool2d(1) self.reduce_layer = nn.Conv2d(1024, 512, 1) self.fc = nn.Sequential( nn.Dropout(0.5), nn.Linear(512, num_classes) ) def forward(self, x): bs = x.shape[0] x = self.feature(x) x1 = self.avg_pool(x) x2 = self.max_pool(x) x = torch.cat([x1, x2], dim=1) x = self.reduce_layer(x).view(bs, -1) logits = self.fc(x) return logits â€”|â€” è¿™ç§æ˜¯åœ¨æ¨¡å‹å±‚è¿›è¡Œæ”¹é€ çš„ä¸€ç§å°trickäº†ï¼Œå¸¸è§çš„åšæ³•ï¼šglobal max/average pooling + fc layerï¼Œè¿™é‡Œè¯•concat(global max-pooling, global average pooling) + fc layerï¼Œå…¶å®å°±æ˜¯ä¸ºäº†ä¸°å¯Œç‰¹å¾å±‚ï¼Œmax poolingæ›´åŠ å…³æ³¨é‡è¦çš„å±€éƒ¨ç‰¹å¾ï¼Œè€Œaverage poolingè¯•æ›´åŠ å…³æ³¨å…¨å±€çš„ç‰¹å¾ã€‚ä¸ä¸€å®šæœ‰æ•ˆï¼Œæˆ‘è¯•è¿‡ä¸å°‘æ¬¡ï¼Œæœ‰æ•ˆçš„æ¬¡æ•°æ¯”è¾ƒå°‘ï¼Œä½†ä¸å°‘äººå–œæ¬¢è¿™æ ·ç”¨.-grayä»¥ä¸Šå°±æ˜¯dual poolingçš„å®ç°ï¼Œå…·ä½“åˆ†æå¦‚ä¸‹ï¼š ç¬¬ä¸€è¡Œï¼šå¾—åˆ°batch-size ç¬¬äºŒè¡Œï¼šå¾—åˆ°feature, grayå¤§ä½¬è¿™é‡Œç”¨çš„æ˜¯ä¸€ä¸ªsequentialå°†æ‰€æœ‰çš„æ¨¡å—è£…è½½è¿›æ¥ï¼Œå…¶å®ä¹Ÿå¯ä»¥ç”¨è¿™ç§æ–¹æ³•ï¼š 1 | self.base_model = nn.Sequential(*list(model_ft.children())[:-3]) # å–é™¤äº†åä¸‰ä¸ªå…¨éƒ¨çš„å±‚ â€”|â€” childrenæ–¹æ³•é‡Œå°±æ˜¯è¿”å›å½“å‰æ¨¡å‹å­æ¨¡å—çš„è¿­ä»£å™¨ï¼Œå¯ä»¥æŸ¥çœ‹æºä»£ç ï¼Œç„¶åé€‰æ‹©å°†å…¶ä¸­ä¸€éƒ¨åˆ†å»æ‰ï¼Œæ¯”å¦‚fcå±‚ç­‰ï¼Œä¹Ÿå¯ä»¥ä½¿ç”¨grayå¤§ä½¬çš„è¿™ç§æ–¹æ³•ã€‚æŸ¥æ‰¾çš„è¿‡ç¨‹ä¸­æ‰¾åˆ°ä¸€ä¸ªä¸­é—´å±‚å¯è§†åŒ–çš„ç®€å•ä»£ç ï¼šhttps://www.jianshu.com/p/0a23db1df55a ç¬¬å››ï¼Œäº”è¡Œï¼Œé€šè¿‡avg_pool,max_poolå¾—åˆ°å¯¹åº”çš„feature ç¬¬å…­è¡Œï¼Œè¿›è¡Œconcateæ“ä½œï¼Œè¿›è¡Œæ‹¼æ¥ ç¬¬ä¸ƒè¡Œï¼Œä½¿ç”¨äº†ä¸€ä¸ªå·ç§¯å±‚è¿›è¡Œé™ç»´é€šé“ï¼Œå¹¶è¿›è¡Œviewå±•å¼€æˆä¸€ç»´å‘é‡ã€‚ ç¬¬å…«å±‚ï¼Œè¿›è¡Œå…¨è¿æ¥å±‚çš„åˆ†ç±»ã€‚å‚è€ƒé“¾æ¥ï¼šhttps://blog.csdn.net/DD_PP_JJ/article/details/103318617","tags":["PyTorch"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"SpatialSense","path":"/2019/12/27/SpatialSense/","content":"æ‘˜è¦ç†è§£å›¾åƒä¸­ç‰©ä½“ä¹‹é—´çš„ç©ºé—´å…³ç³»æ˜¯ä¸€é¡¹å…·æœ‰æƒŠäººæŒ‘æˆ˜æ€§çš„ä»»åŠ¡(å›¾1)ã€‚æ¤…å­å¯èƒ½â€œåœ¨â€ä¸€ä¸ªäººçš„â€œåé¢â€ï¼Œå³ä½¿å®ƒå‡ºç°åœ¨äººçš„å·¦è¾¹(å–å†³äºäººé¢å¯¹çš„æ–¹å‘)ã€‚å¦‚æœæœ‰ç¬¬ä¸‰ä¸ªå­¦ç”Ÿåœ¨ä»–ä»¬ä¹‹é—´ï¼Œé‚£ä¹ˆä¸¤ä¸ªçœ‹èµ·æ¥å¾ˆè¿‘çš„å­¦ç”Ÿå®é™…ä¸Šå¯èƒ½å¹¶ä¸â€œæŒ¨ç€â€ã€‚æˆ‘ä»¬ä»‹ç»äº†spatial alsenseï¼Œè¿™æ˜¯ä¸€ä¸ªä¸“é—¨ç ”ç©¶ç©ºé—´å…³ç³»è¯†åˆ«çš„æ•°æ®é›†ï¼Œå®ƒæ•æ‰äº†å¹¿æ³›çš„æ­¤ç±»æŒ‘æˆ˜ï¼Œå…è®¸å¯¹è®¡ç®—æœºè§†è§‰æŠ€æœ¯è¿›è¡Œé€‚å½“çš„åŸºå‡†æµ‹è¯•ã€‚ç©ºé—´æ„ŸçŸ¥æ˜¯é€šè¿‡å¯¹æŠ—æ€§çš„ä¼—åŒ…æ¥æ„å»ºçš„ï¼Œåœ¨ä¼—åŒ…ä¸­ï¼Œäººç±»æ³¨é‡Šè€…çš„ä»»åŠ¡æ˜¯å‘ç°ç©ºé—´å…³ç³»ï¼Œè¿™äº›å…³ç³»å¾ˆéš¾ç”¨ç®€å•çš„çº¿ç´¢æ¥é¢„æµ‹ï¼Œæ¯”å¦‚äºŒç»´ç©ºé—´ç»“æ„æˆ–è¯­è¨€å…ˆéªŒã€‚ä¸ç°æœ‰çš„æ•°æ®é›†ç›¸æ¯”ï¼Œå¯¹æŠ—æ€§ä¼—åŒ…å¤§å¤§å‡å°‘äº†æ•°æ®é›†çš„åå€šï¼Œå¹¶åœ¨é•¿å°¾æŠ½å–äº†æ›´æœ‰è¶£çš„å…³ç³»æ ·æœ¬ã€‚åœ¨ç©ºé—´æ„Ÿæ–¹é¢ï¼Œæœ€å…ˆè¿›çš„è¯†åˆ«æ¨¡å‹ä¸ç®€å•çš„åŸºçº¿ç›¸æ¯”ï¼Œè¡¨ç°å¾—æ›´ä¸ºå‡ºè‰²ï¼Œè¿™è¡¨æ˜å®ƒä»¬ä¾èµ–äºç›´æ¥çš„çº¿ç´¢ï¼Œè€Œä¸æ˜¯å¯¹è¿™ä¸ªå¤æ‚çš„ä»»åŠ¡è¿›è¡Œå……åˆ†çš„æ¨ç†ã€‚ç©ºé—´æ„Ÿè§‰åŸºå‡†æµ‹è¯•ä¸ºæé«˜è®¡ç®—æœºè§†è§‰ç³»ç»Ÿçš„ç©ºé—´æ¨ç†èƒ½åŠ›æä¾›äº†ä¸€æ¡é€”å¾„ã€‚ Introduction","tags":["OpenCV"],"categories":["å…¶ä»–"]},{"title":"ADVERSARIALAL_AUTOAUGMENT","path":"/2019/12/26/20191226-ADVERSARIALAL-AUTOAUGMENT/","content":"æ•°æ®å¢å¹¿ï¼ˆDAï¼Œdata augmentationï¼‰å·²è¢«å¹¿æ³›ç”¨äºæ”¹å–„è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œçš„æ³›åŒ–æ€§ã€‚æœ€è¿‘ï¼Œäººä¸ºè®¾è®¡çš„æ•°æ®å¢å¹¿å·²é€æ¸è¢«è‡ªåŠ¨å­¦ä¹ çš„å¢å¹¿ç­–ç•¥æ‰€å–ä»£ã€‚é€šè¿‡åœ¨ç²¾å¿ƒè®¾è®¡çš„æ•°æ®å¢å¹¿æœç´¢ç©ºé—´ä¸­æ‰¾åˆ°æœ€ä½³ç­–ç•¥ï¼ŒAutoAugmentå¯ä»¥æ˜¾è‘—æé«˜å›¾åƒåˆ†ç±»ä»»åŠ¡çš„éªŒè¯å‡†ç¡®æ€§ã€‚ä½†æ˜¯ï¼Œè¿™ç§æ–¹æ³•åœ¨å¤§è§„æ¨¡é—®é¢˜ä¸Šåœ¨è®¡ç®—ä¸Šå¹¶ä¸å®ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¯¹æŠ—æ–¹æ³•ï¼Œä»¥å¾—å‡ºä¸€ç§è®¡ç®—ä¸Šå¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œç§°ä¸ºAdversarial AutoAugmentï¼ˆå¯¹æŠ—è‡ªåŠ¨å¢å¹¿ï¼‰ï¼Œå¯ä»¥åŒæ—¶ä¼˜åŒ–ç›®æ ‡ç›¸å…³å¯¹è±¡å’Œå¢å¹¿ç­–ç•¥æœç´¢æŸå¤±ã€‚å¢å¹¿ç­–ç•¥ç½‘ç»œè¯•å›¾é€šè¿‡ç”Ÿæˆå¯¹æŠ—æ€§å¢å¹¿ç­–ç•¥æ¥å¢åŠ ç›®æ ‡ç½‘ç»œçš„è®­ç»ƒæŸå¤±ï¼Œè€Œç›®æ ‡ç½‘ç»œå¯ä»¥ä»è¾ƒéš¾çš„ç¤ºä¾‹ä¸­å­¦ä¹ æ›´å¼ºå¤§çš„åŠŸèƒ½ï¼Œä»¥æé«˜é€šç”¨æ€§ã€‚ä¸å…ˆå‰çš„å·¥ä½œç›¸åï¼Œæˆ‘ä»¬åœ¨ç›®æ ‡ç½‘ç»œè®­ç»ƒä¸­é‡æ–°ä½¿ç”¨è®¡ç®—ä»¥è¿›è¡Œç­–ç•¥è¯„ä¼°ï¼Œè€Œæ— éœ€å¯¹ç›®æ ‡ç½‘ç»œè¿›è¡Œå†è®­ç»ƒã€‚ä¸AutoAugmentç›¸æ¯”ï¼Œè¿™ä½¿ImageNetçš„è®¡ç®—æˆæœ¬é™ä½äº†çº¦12å€ï¼Œæ—¶é—´å¼€é”€ç¼©çŸ­äº†11å€ã€‚æˆ‘ä»¬åœ¨ImageNetä¸Šæ˜¾ç¤ºäº†æˆ‘ä»¬åœ¨CIFAR-10 / CIFAR-100ä¸Šçš„æ–¹æ³•çš„å®éªŒç»“æœï¼Œå¹¶å±•ç¤ºäº†ç›¸å¯¹äºæœ€æ–°æŠ€æœ¯çš„æ˜¾è‘—æ€§èƒ½æ”¹è¿›ã€‚åœ¨CIFAR-10ä¸Šï¼Œæˆ‘ä»¬å®ç°äº†top-1æµ‹è¯•è¯¯å·®ä¸º1.36ï¼…ï¼Œè¿™æ˜¯ç›®å‰è¡¨ç°æœ€ä½³çš„å•ä¸€æ¨¡å‹ã€‚åœ¨ImageNetä¸Šï¼Œåœ¨æ²¡æœ‰é¢å¤–æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åœ¨ResNet-50ä¸Šè¾¾åˆ°äº†top-1ç²¾åº¦çš„é¢†å…ˆæ€§èƒ½ï¼Œåœ¨ResNet-50-Dä¸Šè¾¾åˆ°äº†80.00ï¼…ã€‚","tags":["æ·±åº¦å­¦ä¹ "],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"ADVERSARIALAL_AUTOAUGMENT","path":"/2019/12/26/ADVERSARIALAL-AUTOAUGMENT/","content":"æ•°æ®å¢å¹¿ï¼ˆDAï¼Œdata augmentationï¼‰å·²è¢«å¹¿æ³›ç”¨äºæ”¹å–„è®­ç»ƒæ·±åº¦ç¥ç»ç½‘ç»œçš„æ³›åŒ–æ€§ã€‚æœ€è¿‘ï¼Œäººä¸ºè®¾è®¡çš„æ•°æ®å¢å¹¿å·²é€æ¸è¢«è‡ªåŠ¨å­¦ä¹ çš„å¢å¹¿ç­–ç•¥æ‰€å–ä»£ã€‚é€šè¿‡åœ¨ç²¾å¿ƒè®¾è®¡çš„æ•°æ®å¢å¹¿æœç´¢ç©ºé—´ä¸­æ‰¾åˆ°æœ€ä½³ç­–ç•¥ï¼ŒAutoAugmentå¯ä»¥æ˜¾è‘—æé«˜å›¾åƒåˆ†ç±»ä»»åŠ¡çš„éªŒè¯å‡†ç¡®æ€§ã€‚ä½†æ˜¯ï¼Œè¿™ç§æ–¹æ³•åœ¨å¤§è§„æ¨¡é—®é¢˜ä¸Šåœ¨è®¡ç®—ä¸Šå¹¶ä¸å®ç”¨ã€‚åœ¨æœ¬æ–‡ä¸­ï¼Œæˆ‘ä»¬å¼€å‘äº†ä¸€ç§å¯¹æŠ—æ–¹æ³•ï¼Œä»¥å¾—å‡ºä¸€ç§è®¡ç®—ä¸Šå¯è¡Œçš„è§£å†³æ–¹æ¡ˆï¼Œç§°ä¸ºAdversarial AutoAugmentï¼ˆå¯¹æŠ—è‡ªåŠ¨å¢å¹¿ï¼‰ï¼Œå¯ä»¥åŒæ—¶ä¼˜åŒ–ç›®æ ‡ç›¸å…³å¯¹è±¡å’Œå¢å¹¿ç­–ç•¥æœç´¢æŸå¤±ã€‚å¢å¹¿ç­–ç•¥ç½‘ç»œè¯•å›¾é€šè¿‡ç”Ÿæˆå¯¹æŠ—æ€§å¢å¹¿ç­–ç•¥æ¥å¢åŠ ç›®æ ‡ç½‘ç»œçš„è®­ç»ƒæŸå¤±ï¼Œè€Œç›®æ ‡ç½‘ç»œå¯ä»¥ä»è¾ƒéš¾çš„ç¤ºä¾‹ä¸­å­¦ä¹ æ›´å¼ºå¤§çš„åŠŸèƒ½ï¼Œä»¥æé«˜é€šç”¨æ€§ã€‚ä¸å…ˆå‰çš„å·¥ä½œç›¸åï¼Œæˆ‘ä»¬åœ¨ç›®æ ‡ç½‘ç»œè®­ç»ƒä¸­é‡æ–°ä½¿ç”¨è®¡ç®—ä»¥è¿›è¡Œç­–ç•¥è¯„ä¼°ï¼Œè€Œæ— éœ€å¯¹ç›®æ ‡ç½‘ç»œè¿›è¡Œå†è®­ç»ƒã€‚ä¸AutoAugmentç›¸æ¯”ï¼Œè¿™ä½¿ImageNetçš„è®¡ç®—æˆæœ¬é™ä½äº†çº¦12å€ï¼Œæ—¶é—´å¼€é”€ç¼©çŸ­äº†11å€ã€‚æˆ‘ä»¬åœ¨ImageNetä¸Šæ˜¾ç¤ºäº†æˆ‘ä»¬åœ¨CIFAR-10 / CIFAR-100ä¸Šçš„æ–¹æ³•çš„å®éªŒç»“æœï¼Œå¹¶å±•ç¤ºäº†ç›¸å¯¹äºæœ€æ–°æŠ€æœ¯çš„æ˜¾è‘—æ€§èƒ½æ”¹è¿›ã€‚åœ¨CIFAR-10ä¸Šï¼Œæˆ‘ä»¬å®ç°äº†top-1æµ‹è¯•è¯¯å·®ä¸º1.36ï¼…ï¼Œè¿™æ˜¯ç›®å‰è¡¨ç°æœ€ä½³çš„å•ä¸€æ¨¡å‹ã€‚åœ¨ImageNetä¸Šï¼Œåœ¨æ²¡æœ‰é¢å¤–æ•°æ®çš„æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬åœ¨ResNet-50ä¸Šè¾¾åˆ°äº†top-1ç²¾åº¦çš„é¢†å…ˆæ€§èƒ½ï¼Œåœ¨ResNet-50-Dä¸Šè¾¾åˆ°äº†80.00ï¼…ã€‚","tags":["OpenCV"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"Semantic_segmentation_overview","path":"/2019/12/24/20191224-Semantic-segmentation-overview/","content":"è¯­ä¹‰åˆ†å‰²æ˜¯è¿‘å¹´æ¥å‡ºç°çš„åŸºæœ¬é—®é¢˜ä¹‹ä¸€ï¼Œå› æ­¤æˆä¸ºè®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ é¢†åŸŸçš„çƒ­é—¨è¯é¢˜ã€‚ é€šç”¨è¯­ä¹‰åˆ†å‰²å›¾åƒé›† PASCAL Visual Object Classes (VOC) Common Objects in Context (COCO)With 200K labelled images, 1.5 million object instances, and 80object categories Other General Purpose Semantic Segmentation Image Sets YouTube-Objects SIFT-flow Urban Street Semantic Segmentation Image Sets Cityscapes CamVid KITTI SYNTHIA Before Fully Convolutional NetworksPre-Deep Learning Approachesä¼ ç»Ÿå›¾åƒåˆ†å‰²ä¸è¯­ä¹‰åˆ†å‰²çš„åŒºåˆ«åœ¨äºè¯­ä¹‰ç‰¹å¾åœ¨å›¾åƒåˆ†å‰²è¿‡ç¨‹ä¸­çš„åº”ç”¨ã€‚ä¼ ç»Ÿçš„å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œå¦‚é˜ˆå€¼ã€èšç±»å’ŒåŒºåŸŸå¢é•¿ç­‰(æœ‰å…³ä¼ ç»Ÿå›¾åƒåˆ†å‰²æŠ€æœ¯çš„è°ƒæŸ¥ï¼Œè¯·å‚é˜…[29])ä½¿ç”¨æ‰‹å·¥åˆ¶ä½œçš„ä½çº§ç‰¹å¾(å³åœ¨å›¾åƒä¸­å®šä½ç‰©ä½“çš„è¾¹ç•Œã€‚å› æ­¤ï¼Œåœ¨éœ€è¦å›¾åƒè¯­ä¹‰ä¿¡æ¯è¿›è¡Œåƒç´ çº§åˆ†å‰²çš„æƒ…å†µä¸‹ï¼Œä¾‹å¦‚åœ¨ç›¸ä¼¼ç‰©ä½“ç›¸äº’é®æŒ¡çš„æƒ…å†µä¸‹ï¼Œè¿™äº›æ–¹æ³•æ˜¯å¿…è¦çš„ã€‚å…³äºæ·±åº¦CNNsæµè¡Œä¹‹å‰çš„è¯­ä¹‰åˆ†å‰²å·¥ä½œï¼Œæœ‰å¤šç§æ–¹æ³•[30,31ï¼Œ32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]ä½¿ç”¨çš„å›¾å½¢æ¨¡å‹ï¼Œå¦‚é©¬å°”å¯å¤«éšæœºåŸŸ(MRF)ï¼Œæ¡ä»¶éšæœºåœº(CRF)æˆ–åŸºäºæ£®æ—(æœ‰æ—¶è¢«ç§°ä¸ºâ€œæ•´ä½“â€)çš„æ–¹æ³•ï¼Œä»¥ä¾¿åœ¨åƒç´ çº§æ‰¾åˆ°åœºæ™¯æ ‡ç­¾ã€‚å…¶ä¸»è¦æ€æƒ³æ˜¯é€šè¿‡è§‚å¯Ÿç›¸é‚»åƒç´ ä¹‹é—´çš„ä¾èµ–å…³ç³»æ¥æ‰¾åˆ°ä¸€ä¸ªæ¨è®ºã€‚æ¢å¥è¯è¯´ï¼Œè¿™äº›æ–¹æ³•å°†å›¾åƒçš„è¯­ä¹‰å»ºæ¨¡ä¸ºç›¸é‚»åƒç´ ä¹‹é—´çš„ä¸€ç§â€œå…ˆéªŒâ€ä¿¡æ¯ã€‚å¦ä¸€ç»„ç ”ç©¶ï¼Œæœ‰æ—¶è¢«ç§°ä¸ºâ€œåˆ†å±‚æ¨¡å‹â€[44,45,46]ï¼Œä½¿ç”¨ä¸€ä¸ªé¢„å…ˆè®­ç»ƒçš„å’Œç‹¬ç«‹çš„å¯¹è±¡æ¢æµ‹å™¨çš„ç»„åˆï¼Œä»¥ä¾¿ä»å›¾åƒä¸­æå–è¯­ä¹‰ä¿¡æ¯ã€‚ç”±äºå•ä¸ªçš„å¯¹è±¡æ£€æµ‹å™¨æœªèƒ½æ­£ç¡®åœ°å¯¹åŒºåŸŸè¿›è¡Œåˆ†ç±»ï¼Œæˆ–è€…ç”±äºè¿™äº›æ–¹æ³•å—åˆ°â€œæ‰‹å·¥é€‰æ‹©â€çš„æ£€æµ‹å™¨åº“æ‰€æä¾›çš„å¯¹è±¡ç±»çš„æœ‰é™æ•°é‡çš„é™åˆ¶ï¼Œå› æ­¤ä¸å½“ä»Šæœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒä»¬çš„æ€§èƒ½ç›¸å¯¹è¾ƒä½ã€‚è™½ç„¶ä¸Šè¿°çš„å‰æ·±åº¦å­¦ä¹ æ—¶ä»£çš„æ–¹æ³•ä¸å†æ˜¯é¦–é€‰çš„åˆ†å‰²æ–¹æ³•ï¼Œä¸€äº›å›¾å½¢æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯CRFsï¼Œç›®å‰æ­£åœ¨ä½¿ç”¨çš„æœ€å…ˆè¿›çš„æ–¹æ³•ä½œä¸ºåå¤„ç†(ç»†åŒ–)å±‚ï¼Œç›®çš„æ˜¯æé«˜è¯­ä¹‰åˆ†å‰²çš„æ€§èƒ½ï¼Œå…·ä½“ç»†èŠ‚å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­è®¨è®º Refinement Methodsæ·±åº¦ç¥ç»ç½‘ç»œå…·æœ‰å¾ˆå¼ºçš„å±€éƒ¨ç‰¹å¾æå–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬ç¼ºä¹åˆ©ç”¨å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå› æ­¤æ— æ³•å¯¹ç›¸é‚»åƒç´ é¢„æµ‹ä¹‹é—´çš„äº¤äº’è¿›è¡Œå»ºæ¨¡ã€‚å¦ä¸€æ–¹é¢ï¼Œå‰æ·±åº¦å­¦ä¹ æ—¶ä»£æµè¡Œçš„åˆ†å‰²æ–¹æ³•ï¼Œå›¾å½¢æ¨¡å‹ï¼Œéå¸¸é€‚åˆè¿™ç±»ä»»åŠ¡ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒä»¬ç›®å‰è¢«ç”¨ä½œè®¸å¤šæ·±åº¦åŸºäºcnnçš„è¯­ä¹‰åˆ†å‰²æ¶æ„çš„ç»†åŒ–å±‚ã€‚æ­£å¦‚åœ¨å‰ä¸€èŠ‚ä¸­æåˆ°çš„ï¼Œä½¿ç”¨å›¾å½¢æ¨¡å‹è¿›è¡Œåˆ†å‰²èƒŒåçš„æ€æƒ³æ˜¯é€šè¿‡è§‚å¯Ÿç›¸é‚»åƒç´ ä¹‹é—´çš„ä½å±‚æ¬¡å…³ç³»æ¥å¯»æ‰¾ä¸€ä¸ªæ¨è®ºã€‚åœ¨å›¾2ä¸­ï¼Œå¯ä»¥çœ‹åˆ°ä½¿ç”¨åŸºäºå›¾å½¢æ¨¡å‹çš„ç»†åˆ†å¯¹åˆ†å‰²ç»“æœçš„å½±å“ã€‚åˆ†ç±»å™¨(è§å›¾2.b)ä¸èƒ½æ­£ç¡®åˆ†å‰²ä¸åŒç±»æ ‡ç­¾ç›¸é‚»çš„åƒç´ ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºcrfçš„ç»†åˆ†[42]æ¥æ”¹è¿›åƒç´ çº§çš„åˆ†å‰²ç»“æœã€‚åŸºäºcrfçš„æ–¹æ³•è¢«å¹¿æ³›ç”¨äºæ·±åº¦è¯­ä¹‰åˆ†å‰²æ–¹æ³•çš„ç»†åŒ–CRFs[50]æ˜¯ä¸€ç§æœ‰åŒºåˆ«çš„æ— å‘æ¦‚ç‡å›¾å½¢æ¨¡å‹ã€‚å®ƒä»¬è¢«ç”¨æ¥å¯¹è§‚æµ‹ä¹‹é—´å·²çŸ¥çš„å…³ç³»è¿›è¡Œç¼–ç ï¼Œå¹¶æ„å»ºä¸€è‡´çš„è§£é‡Šã€‚å®ƒä»¬ç”¨ä½œç»†åŒ–å±‚çš„åŸå› æ˜¯ï¼Œä¸ä¸è€ƒè™‘ç›¸é‚»åƒç´ ç›¸ä¼¼æ€§çš„ç¦»æ•£åˆ†ç±»å™¨ä¸åŒï¼ŒCRFå¯ä»¥åˆ©ç”¨è¿™äº›ä¿¡æ¯ã€‚ä¸å…¶ä»–å›¾å½¢åŒ–æ¨¡å‹(å¦‚éšé©¬å°”ç§‘å¤«æ¨¡å‹)ç›¸æ¯”ï¼ŒCRFsçš„ä¸»è¦ä¼˜ç‚¹æ˜¯å®ƒä»¬çš„æ¡ä»¶æ€§è´¨å’Œé¿å…æ ‡ç­¾åå·®[50]é—®é¢˜çš„èƒ½åŠ›ã€‚å°½ç®¡æœ‰ç›¸å½“æ•°é‡çš„æ–¹æ³•(è§è¡¨1)ä½¿ç”¨CRFsè¿›è¡Œç»†åŒ–ï¼Œä½†è¿™äº›æ¨¡å‹åœ¨ç›¸å¯¹è¾ƒæ–°çš„æ–¹æ³•ä¸­å¼€å§‹å˜å¾—ä¸å—æ¬¢è¿ï¼Œå› ä¸ºå®ƒä»¬çš„é€Ÿåº¦éå¸¸æ…¢ï¼Œè€Œä¸”éå¸¸éš¾ä»¥ä¼˜åŒ–ã€‚ Early Deep Learning ApproachesFCNåœ¨2014å¹´å‡ºç°ï¼Œä½¿ç”¨tanh ç›¸è¾ƒäºproposal of a ReLU layer å¾ˆéš¾å»åŒºåˆ†ï¼Œå› æ­¤ï¼Œè®­ç»ƒè¿™æ ·çš„ç³»ç»Ÿè¢«è®¤ä¸ºæ˜¯ä¸é€‚åˆè®¡ç®—çš„ï¼Œç”šè‡³å¯¹å¤§è§„æ¨¡æ•°æ®æ˜¯ä¸å¯è¡Œçš„ã€‚ç„¶è€Œï¼Œç¬¬ä¸€ä¸ªæˆç†Ÿçš„æ–¹æ³•åªæ˜¯ç®€å•åœ°å°è¯•è½¬æ¢åˆ†ç±»ç½‘ç»œï¼Œå¦‚AlexNetå’ŒVGGé€šè¿‡å¾®è°ƒå…¨è¿æ¥å±‚æ¥ç»†åˆ†ç½‘ç»œã€‚ä»–ä»¬åœ¨è®­ç»ƒé˜¶æ®µé­å—äº†è¿‡åº¦æ‹Ÿåˆå’Œå®Œå…¨è¿æ¥å±‚çš„æ—¶é—´é™åˆ¶ã€‚æ­¤å¤–ï¼Œä½¿ç”¨çš„CNNsä¸å¤Ÿæ·±ï¼Œæ— æ³•åˆ›å»ºæŠ½è±¡çš„ç‰¹å¾ï¼Œè¿™ä¸å›¾åƒçš„è¯­ä¹‰æœ‰å…³ã€‚åœ¨ä¸€äº›æ—©æœŸçš„æ·±åº¦å­¦ä¹ ç ”ç©¶ä¸­ï¼Œç ”ç©¶äººå‘˜æ‹’ç»ä½¿ç”¨å®Œå…¨è¿æ¥çš„å±‚æ¥è¿›è¡Œå†³ç­–ï¼Œè€Œæ˜¯ä½¿ç”¨ä¸åŒçš„ç»“æ„ï¼Œå¦‚å‘¨æœŸæ€§çš„æ¶æ„[57]æˆ–ä½¿ç”¨æ¥è‡ªä¸€ä¸ªå•ç‹¬è®¡ç®—çš„åˆ†æ®µå®¶æ—çš„æ ‡è®°ã€‚é€šè¿‡æå‡ºå…¨è¿æ¥å±‚FCNè¿™æ ·çš„ç»“æ„çš„å¿…è¦æ€§çš„ç¬¬ä¸€ä¸ªè¿¹è±¡ï¼Œä¸å‡ºæ‰€æ–™ï¼Œå®ƒä»¬è¢«FCNå–ä»£ã€‚ç”±äºä»–ä»¬çš„åˆ†å‰²ç»“æœè¢«è®¤ä¸ºæ˜¯ä¸ä»¤äººæ»¡æ„çš„ï¼Œè¿™äº›ç ”ç©¶é€šå¸¸ä½¿ç”¨ä¸€ä¸ªç»†åŒ–çš„è¿‡ç¨‹ï¼Œè¦ä¹ˆä½œä¸ºä¸€ä¸ªåå¤„ç†å±‚[52,53,54,56]ï¼Œæˆ–ä½œä¸ºä¸€ä¸ªæ›¿ä»£æ¶æ„ï¼Œä»¥å®Œå…¨è¿æ¥çš„å†³ç­–å±‚Refinement methods varied such as Markov random fieldsï¼Œnearest neighbour-based approachï¼Œä½¿ç”¨æ ¡å‡†å±‚[54]ï¼Œä½¿ç”¨è¶…çº§åƒç´ [55,56]ï¼Œæˆ–æ™®é€šCNNsçš„é€’å½’ç½‘ç»œã€‚ç»†åŒ–å±‚ä»ç„¶è¢«åfcnæ–¹æ³•æ‰€ä½¿ç”¨ï¼Œå…¶ç›®çš„æ˜¯æé«˜ç±»äº¤å‰åŒºåŸŸçš„åƒç´ çº§æ ‡è®°æ€§èƒ½ã€‚tips: 4FCN [11] ] was officially published in 2017. However the same group first shared the idea online as pre-printed literature in 014 [51]. Fully Convolutional Networks for Semantic SegmentationFCN(2017) æå‡ºäº†ä»CNNs (DCNN)ä¸­æ‹†é™¤å…¨è¿é€šå±‚çš„è®¾æƒ³.â€˜FCN-32sâ€™, â€˜FCN16sâ€™, and â€˜FCN8sâ€™ all transfer-learnt using the VGG architectureFCNæ¶æ„åœ¨å¾ˆå¤šæ–¹é¢éƒ½è¢«è®¤ä¸ºæ˜¯é©å‘½æ€§çš„, FCNä¸åŒ…æ‹¬å…¨è¿æ¥å±‚ è¯¥ç»“æ„å…è®¸ä¸ºä»»ä½•åˆ†è¾¨ç‡çš„å›¾åƒç”Ÿæˆåˆ†å‰²å›¾ã€‚ä½¿ç”¨åå·ç§¯å±‚ï¼Œå¯ä»¥å°†ç²—æ·±å·ç§¯å±‚è¾“å‡ºæå‡åˆ°ä»»æ„åˆ†è¾¨ç‡çš„ç¨ å¯†åƒç´ ã€‚ æå‡ºäº†DCNNsçš„skipæ¶æ„ã€‚è·³è¿‡æ¶æ„(æˆ–è¿æ¥)åœ¨DCNNsä¸­æä¾›ä¸ç›¸é‚»å±‚ä¹‹é—´çš„é“¾æ¥ã€‚ä»…ä»…é€šè¿‡å¯¹æœªè¿æ¥å±‚çš„è¾“å‡ºè¿›è¡Œæ±‚å’Œæˆ–è¿æ¥ï¼Œè¿™äº›è¿æ¥å°±å¯ä»¥ä½¿ä¿¡æ¯æµåŠ¨ï¼Œå¦åˆ™ï¼Œç”±äºä½“ç³»ç»“æ„çš„é€‰æ‹©(å¦‚æœ€å¤§æ± åŒ–å±‚æˆ–è¾å­¦)ï¼Œè¿™äº›ä¿¡æ¯å°±ä¼šä¸¢å¤±ã€‚æœ€å¸¸è§çš„åšæ³•æ˜¯åœ¨max-poolingå±‚ä¹‹å‰ä½¿ç”¨skipè¿æ¥ï¼Œå®ƒé€šè¿‡é€‰æ‹©ç‰¹å®šåŒºåŸŸçš„æœ€å¤§å€¼å¯¹å±‚è¾“å‡ºè¿›è¡Œé‡‡æ ·ã€‚æ± åŒ–å±‚æœ‰åŠ©äºæ¶æ„åˆ›å»ºç‰¹æ€§å±‚æ¬¡ï¼Œä½†ä¹Ÿä¼šå¯¼è‡´å±€éƒ¨ä¿¡æ¯çš„ä¸¢å¤±ï¼Œè€Œè¿™äº›å±€éƒ¨ä¿¡æ¯å¯¹äºè¯­ä¹‰åˆ†å‰²æ˜¯æœ‰ä»·å€¼çš„ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹è±¡è¾¹ç•Œã€‚è·³è¿‡è¿æ¥é€šè¿‡ç»•è¿‡æ± åŒ–å±‚æ¥ä¿å­˜è¿™äº›ä¿¡æ¯å¹¶å°†å…¶è½¬å‘åˆ°æ›´æ·±å±‚ã€‚å®é™…ä¸Šï¼Œåœ¨[11]ä¸­ä½¿ç”¨è·³è½¬è¿æ¥è¢«è®¤ä¸ºæ˜¯ç›¸å½“åŸå§‹çš„ã€‚â€œFCN-8sâ€å’Œâ€œFCN-16sâ€ç½‘ç»œåœ¨ä¸åŒçš„å±‚åŒ…å«è¿™äº›è·³è¿‡è¿æ¥ã€‚å¯¹äºç›¸åŒçš„æ¶æ„ï¼Œå³â€œFCN-4sâ€å’Œâ€œæ›´å¯†é›†çš„è·³è¿‡è¿æ¥â€ã€‚â€œFCN-2sâ€ä¹Ÿè¢«ç”¨äºå„ç§åº”ç”¨[61,62]ã€‚è¿™ä¸€æ€æƒ³æœ€ç»ˆæ¼”å˜ä¸ºç”¨äºè¯­ä¹‰åˆ†å‰²çš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„[63,27]ï¼Œä¸‹æ–‡å°†å¯¹æ­¤è¿›è¡Œä»‹ç»ã€‚ Post-FCN Approachesdrawbacks of FCNsï¼š ç‰¹æ€§å±‚æ¬¡ç»“æ„ä¸­æ ‡ç­¾æœ¬åœ°åŒ–çš„ä½æ•ˆä¸¢å¤±ã€æ— æ³•å¤„ç†å…¨å±€ä¸Šä¸‹æ–‡çŸ¥è¯†ä»¥åŠç¼ºä¹å¤šå°ºåº¦å¤„ç†æœºåˆ¶ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†è¯­ä¹‰åˆ†å‰²ä¸Šä¸‹æ–‡ä¸­çš„å°ºåº¦ä¸å˜æ€§ï¼Œæœ€åè®¨è®ºäº†åŸºäºå¯¹è±¡æ£€æµ‹çš„æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³åŒæ—¶æ£€æµ‹å¯¹è±¡å®ä¾‹çš„è¯­ä¹‰åˆ†å‰²é—®é¢˜ã€‚ Techniques for Fine-grained Localisationæ ¹æ®å®šä¹‰ï¼Œè¯­ä¹‰åˆ†å‰²æ˜¯ä¸€ä¸ªå¯†é›†çš„è¿‡ç¨‹ï¼Œå› æ­¤å®ƒéœ€è¦åœ¨åƒç´ çº§å¯¹ç±»æ ‡ç­¾è¿›è¡Œç»†ç²’åº¦çš„æœ¬åœ°åŒ–ã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººæ‰‹æœ¯ä¸­ï¼Œè¯­ä¹‰åˆ†å‰²ä¸­çš„åƒç´ é”™è¯¯å¯èƒ½ä¼šå¯¼è‡´ç”Ÿå­˜æˆ–æ­»äº¡çš„æƒ…å†µã€‚å±‚æ¬¡ç‰¹æ€§åˆ›å»ºçš„æ± (å³ã€‚ï¼Œæœ€å¤§æ± )å±‚å¯ä»¥éƒ¨åˆ†å¤±å»æœ¬åœ°åŒ–ã€‚æ­¤å¤–ï¼Œç”±äºä»–ä»¬FCNså®Œå…¨æ˜¯å·ç§¯æ€§è´¨çš„ï¼Œå®ƒæœ¬èº«å¹¶ä¸å…·å¤‡åœ¨å›¾åƒä¸­å¯¹å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯å»ºæ¨¡çš„èƒ½åŠ›ï¼Œè¿™åœ¨ç±»æ ‡ç­¾æœ¬åœ°åŒ–æ–¹é¢ä¹Ÿéå¸¸æœ‰æ•ˆã€‚å› æ­¤ï¼Œè¿™ä¸¤ä¸ªé—®é¢˜åœ¨æœ¬è´¨ä¸Šå’Œæœ¬è´¨ä¸Šæ˜¯ç›¸äº’äº¤ç»‡çš„ä¸‹é¢æˆ‘ä»¬å°†è®¨è®ºæ—¨åœ¨å…‹æœè¿™äº›é—®é¢˜å’Œæä¾›æ›´å¥½çš„æœ¬åœ°åŒ–çš„ä¸åŒæ–¹æ³•ç±»çš„æ ‡ç­¾ã€‚ Encoder-Decoder Architectureç¼–è¯‘ç å™¨ï¼Œ ç±»ä¼¼U-netï¼Œå…·æœ‰å¼€åˆ›æ–°çš„ç ”ç©¶ã€‚ç¼–ç å™¨ä½¿ç”¨æ± åŒ–å±‚é€æ¸ç¼©å‡è¾“å…¥æ•°æ®çš„ç©ºé—´ç»´åº¦ï¼Œè€Œè§£ç å™¨é€šè¿‡åå·ç§¯å±‚ç­‰ç½‘ç»œå±‚é€æ­¥æ¢å¤ç›®æ ‡çš„ç»†èŠ‚å’Œç›¸åº”çš„ç©ºé—´ç»´åº¦ã€‚ä»ç¼–ç å™¨åˆ°è§£ç å™¨ä¹‹é—´ï¼Œé€šå¸¸å­˜åœ¨ç›´æ¥çš„ä¿¡æ¯è¿æ¥ï¼Œæ¥å¸®åŠ©è§£ç å™¨æ›´å¥½åœ°æ¢å¤ç›®æ ‡ç»†èŠ‚ã€‚U-Netï¼ŒSeg-Net éƒ½æ˜¯éå¸¸å‡ºåçš„ç½‘ç»œã€‚åœ¨è¿™ç§ç»“æ„ä¸­ï¼Œç”±ç¼–ç å™¨éƒ¨åˆ†ç›¸é‚»çš„ä½åˆ†è¾¨ç‡ç‰¹å¾æ˜ å°„æä¾›çš„å¼ºç›¸å…³è¯­ä¹‰ä¿¡æ¯å¿…é¡»ç»è¿‡é¢å¤–çš„ä¸­é—´å±‚æ‰èƒ½åˆ°è¾¾ç›¸åŒçš„è¯‘ç å±‚ã€‚è¿™é€šå¸¸ä¼šå¯¼è‡´ä¸€å®šç¨‹åº¦çš„ä¿¡æ¯è¡°å‡ã€‚ç„¶è€Œï¼ŒU-Netæ¶æ„å·²ç»è¢«è¯æ˜å¯¹äºä¸åŒåº”ç”¨çš„åˆ†å‰²éå¸¸æœ‰ç”¨ï¼Œä¾‹å¦‚å«æ˜Ÿå›¾åƒã€‚ Spatial Pyramid Pooling(ç©ºé—´é‡‘å­—å¡”æ± åŒ–)Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. åœ¨2006å¹´é¦–æ¬¡è¢«æå‡ºï¼Œ ä¸ºçš„æ˜¯è§£å†³å•è¯è¢‹ç³»ç»Ÿå¤±å»äº†ç‰¹å¾ä¹‹é—´çš„ç©ºé—´å…³ç³»ã€‚é¦–æ¬¡åº”ç”¨åœ¨æ·±åº¦å­¦ä¹ æ˜¯2015å¹´çš„SPPNetè¿™ç¯‡æ–‡ç« ã€‚æ— è®ºè¾“å…¥å¤§å°å¦‚ä½•ï¼Œéƒ½å¯ä»¥åœ¨ç©ºé—´é‡‘å­—å¡”æ±‡èšç½‘ç»œä¸­åˆ›å»ºæ·±åº¦ç‰¹å¾çš„ç©ºé—´é‡‘å­—å¡”è¡¨ç¤ºã€‚SPP-Netæœ€é‡è¦çš„è´¡çŒ®æ˜¯å®ƒå…è®¸è¾“å…¥ä¸åŒå¤§å°çš„æ•°æ®ã€‚ä¸åŒå¤§å°çš„å›¾åƒè¾“å…¥åˆ°å·ç§¯å±‚ä¸­ï¼Œä¸å¯é¿å…åœ°ä¼šäº§ç”Ÿä¸åŒå¤§å°çš„ç‰¹å¾å›¾ã€‚ç„¶è€Œ,å¦‚æœæ± åŒ–å±‚åˆšå¥½åœ¨å†³ç­–å±‚ä¹‹å‰ï¼Œå…·æœ‰ä¸è¾“å…¥å¤§å°æˆæ¯”ä¾‹çš„æ­¥é•¿å€¼ï¼Œåˆ™åˆ›å»ºç‰¹å¾æ˜ å°„è¿™ä¸€å±‚å°†è¢«å›ºå®šã€‚CNNä¸­çš„SPPå±‚æ„å»ºäº†ä¸åŒå±‚æ¬¡ç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚å› æ­¤ï¼Œå®ƒä¸EDç»“æ„ä¸­çš„è·³è¿‡è¿æ¥éå¸¸ç›¸ä¼¼ï¼Œåè€…ä¹Ÿå…è®¸ç‰¹æ€§å±‚æ¬¡ç»“æ„ä¹‹é—´çš„ä¿¡æ¯æµã€‚ï¼šSPPå±‚ç”¨äºè¯­ä¹‰åˆ†å‰²æœ€å¸¸è§çš„ç”¨æ³•æ˜¯åœ¨[67]ä¸­æå‡ºçš„ï¼Œæ¯”å¦‚SPPå±‚è¢«é™„åŠ åˆ°æœ€åä¸€ä¸ªå·ç§¯å±‚ï¼Œå¹¶åé¦ˆç»™åƒç´ çº§åˆ†ç±»å™¨ã€‚ Feature Concatenation(ç‰¹å¾è¿æ¥)è¿™ä¸ªæƒ³æ³•æ˜¯åŸºäºèåˆä»ä¸åŒæ¥æºæå–çš„ç‰¹å¾ã€‚ Dilated Convolution(æ‰©å¼ å·ç§¯ï¼Œç©ºæ´å·ç§¯)æ‰©å±•å·ç§¯çš„æ€æƒ³å®é™…ä¸Šå¾ˆç®€å•:ä½¿ç”¨è¿ç»­çš„å·ç§¯æ»¤æ³¢å™¨ï¼Œä¸€ä¸ªæœ‰æ•ˆçš„æ¥æ”¶åŸŸåªèƒ½éšå±‚çº¿æ€§å¢é•¿;ç„¶è€Œï¼Œå¦‚æœä½¿ç”¨åœ¨æ»¤æ³¢å™¨ä¸­æœ‰é—´éš™çš„è†¨èƒ€å·ç§¯(è§å›¾4.c)ï¼Œæœ‰æ•ˆæ¥å—åŸŸå°†å¢é•¿å¾—æ›´å¿«[70]ã€‚å› æ­¤ï¼Œåœ¨æ²¡æœ‰æ± æˆ–å­é‡‡æ ·çš„æƒ…å†µä¸‹ï¼Œåˆ›å»ºäº†å·ç§¯å±‚çš„çŸ©å½¢æ£±é•œã€‚æ‰©å¼ å·ç§¯æ˜¯ä¸€ç§éå¸¸æœ‰æ•ˆå’Œå¼ºå¤§çš„æ–¹æ³•æ¥è¯¦ç»†ä¿å­˜ç‰¹å¾å›¾åˆ†è¾¨ç‡ã€‚ç¼ºç‚¹åœ¨äºå¯¹GPUå­˜å‚¨å’Œè®¡ç®—çš„è¦æ±‚æ›´é«˜ï¼Œå› ä¸ºç‰¹å¾å›¾åˆ†è¾¨ç‡ä¸ä¼šåœ¨ç‰¹å¾å±‚æ¬¡ç»“æ„ä¸­ç¼©å°ã€‚ Conditional Random Fields(æ¡ä»¶éšæœºåœº)cnnè‡ªç„¶ç¼ºä¹ç‰¹åˆ«çš„â€˜å…³æ³¨â€™ç±»äº¤å‰åŒºåŸŸçš„æœºåˆ¶ã€‚åœ¨è¿™äº›åŒºåŸŸå‘¨å›´ï¼Œé€šè¿‡è§‚å¯ŸCNNå±‚çš„ç›¸é‚»feature mapsä¹‹é—´çš„ä½å±‚å…³ç³»ï¼Œä½¿ç”¨å›¾å½¢åŒ–æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚å› æ­¤ï¼Œå›¾å½¢æ¨¡å‹(ä¸»è¦æ˜¯crf)è¢«ç”¨ä½œæ·±åº¦è¯­ä¹‰åˆ†å‰²æ¶æ„çš„ç»†åŒ–å±‚ã€‚ä¸åœ¨[72]ä¸­ä¸€æ ·ï¼ŒCRFså°†ä½çº§äº¤äº’ä¸æ¥è‡ªå¤šç±»äº¤äº’çš„è¾“å‡ºè¿æ¥èµ·æ¥ï¼Œå¹¶ä»¥è¿™ç§æ–¹å¼æ„å»ºå…¨å±€ä¸Šä¸‹æ–‡çŸ¥è¯†ã€‚CRFsä½œä¸ºä¸€ç§ç»†åŒ–å±‚ï¼Œç›®å‰å­˜åœ¨å¤šç§åˆ©ç”¨CRFså¯¹CNNsè¿›è¡Œæ·±åº¦å¤„ç†çš„æ–¹æ³•ï¼Œå¦‚å·ç§¯CRFs[47]ã€ç¨ å¯†CRF[42]ã€CRN-as-RNNç­‰[73]ã€‚å°½ç®¡CRFsæœ‰åŠ©äºæ„å»ºä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œä»è€Œåœ¨ç±»æ ‡ç­¾ä¸­æ›´å¥½åœ°æœ¬åœ°åŒ–ï¼Œè¡¨1æ˜¾ç¤ºäº†åœ¨â€œCRFæ¨¡å‹â€é€‰é¡¹å¡ä¸‹åˆ†ç±»çš„CRFsï¼Œä»¥ä¾¿å°†å®ƒä»¬ä¸å®é™…çš„CNNæ¶æ„æ‰©å±•åŒºåˆ†å¼€ã€‚ Recurrent Approachesé€’å½’ç¥ç»ç½‘ç»œå¤„ç†æ—¶é—´ä¿¡æ¯çš„èƒ½åŠ›æœ‰åŠ©äºæé«˜åˆ†å‰²ç²¾åº¦ã€‚ä¾‹å¦‚ï¼Œ[74]ä½¿ç”¨ConvLSTMå±‚æ¥æ”¹è¿›å›¾åƒåºåˆ—ä¸­çš„è¯­ä¹‰åˆ†å‰²ç»“æœã€‚ç„¶è€Œï¼Œä¹Ÿæœ‰ä¸€äº›æ–¹æ³•åœ¨é™æ€å›¾åƒä¸Šä½¿ç”¨å¾ªç¯ç»“æ„ã€‚åœ¨[13]ä¸­ï¼Œç ”ç©¶äººå‘˜åˆ©ç”¨LSTMchainsæ¥ç¼ ç»•å¤šä¸ªå°ºåº¦ï¼Œä»è€Œå¾—åˆ°åƒç´ çº§çš„åˆ†å‰²æ”¹è¿›ã€‚ä¹Ÿæœ‰å°†CNNså’ŒRNNsèåˆçš„æ··åˆæ–¹æ³•ã€‚è¿™æ–¹é¢çš„ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­æ˜¯æ‰€è°“çš„ReSegæ¨¡å‹[75]ï¼Œå…¶ä¸­ï¼Œè¾“å…¥å›¾åƒè¢«é¦ˆé€åˆ°ä¸€ä¸ªç±»ä¼¼äºvggçš„CNNç¼–ç å™¨ï¼Œç„¶åé€šè¿‡é€’å½’å±‚(å³ReNetæ¶æ„)è¿›è¡Œå¤„ç†ï¼Œä»¥ä¾¿æ›´å¥½åœ°å®šä½åƒç´ æ ‡ç­¾ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¯­ä¹‰åˆ†å‰²ä¸å­˜åœ¨å•çº¯çš„é€’å½’ç»“æ„ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºè¯­ä¹‰åˆ†å‰²éœ€è¦ä¸€ä¸ªåˆæ­¥çš„åŸºäºcnnçš„ç‰¹å¾ç¼–ç æ–¹æ¡ˆã€‚ç›®å‰ï¼Œæœ‰ä¸€ç§ç‰¹å®šç±»å‹çš„RNNï¼Œå³â€œæ³¨æ„æ¨¡å—â€ï¼Œæœ‰å¢é•¿çš„è¶‹åŠ¿ã€‚åœ¨è¿™äº›æ¨¡å—ä¸­ï¼ŒRNNåœ¨æŠ€æœ¯ä¸Šèåˆäº†æ³¨æ„åŠ›[76]ï¼Œåœ¨é¢„æµ‹è¾“å‡ºåºåˆ—çš„æŸä¸ªéƒ¨åˆ†æ—¶ï¼Œå°†æ³¨æ„åŠ›é›†ä¸­åœ¨è¾“å…¥çš„æŸä¸ªåŒºåŸŸã€‚å› æ­¤ï¼Œå®ƒä»¬ä¹Ÿè¢«ç”¨äºè¯­ä¹‰åˆ†å‰²[77,78,79]ã€‚ Scale-Invariance(å°ºåº¦å˜åŒ–)æ ¹æ®å®šä¹‰ï¼Œå°ºåº¦ä¸å˜æ€§æ˜¯æŒ‡ä¸€ä¸ªæ–¹æ³•å¤„ç†è¾“å…¥æ—¶ä¸ä¾èµ–äºç›¸å¯¹å°ºåº¦çš„èƒ½åŠ›ã€‚æˆ–å›¾åƒåˆ†è¾¨ç‡ã€‚å°½ç®¡å®ƒå¯¹äºæŸäº›åº”ç”¨ç¨‹åºæ¥è¯´æ˜¯æå…¶é‡è¦çš„ï¼Œä½†æ˜¯è¿™ç§èƒ½åŠ›é€šå¸¸è¢«å¿½è§†ï¼Œæˆ–è€…ä¸æ–¹æ³•åŒ…å«å¤šå°ºåº¦ä¿¡æ¯çš„èƒ½åŠ›ç›¸æ··æ·†ã€‚ä¸€ç§æ–¹æ³•å¯ä»¥ä½¿ç”¨å¤šå°ºåº¦ä¿¡æ¯æ¥æé«˜å…¶åƒç´ çº§åˆ†å‰²èƒ½åŠ›ï¼Œä½†ä»ç„¶ä¾èµ–äºå°ºåº¦æˆ–åˆ†è¾¨ç‡ã€‚ æ–‡çŒ®ï¼š A SURVEY ON DEEP LEARNING-BASED ARCHITECTURES FOR SEMANTIC SEGMENTATION ON 2D IMAGES","tags":["æ·±åº¦å­¦ä¹ "],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"Semantic_segmentation_overview","path":"/2019/12/24/Semantic-segmentation-overview/","content":"è¯­ä¹‰åˆ†å‰²æ˜¯è¿‘å¹´æ¥å‡ºç°çš„åŸºæœ¬é—®é¢˜ä¹‹ä¸€ï¼Œå› æ­¤æˆä¸ºè®¡ç®—æœºè§†è§‰å’Œæœºå™¨å­¦ä¹ é¢†åŸŸçš„çƒ­é—¨è¯é¢˜ã€‚ é€šç”¨è¯­ä¹‰åˆ†å‰²å›¾åƒé›† PASCAL Visual Object Classes (VOC) Common Objects in Context (COCO)With 200K labelled images, 1.5 million object instances, and 80object categories Other General Purpose Semantic Segmentation Image Sets YouTube-Objects SIFT-flow Urban Street Semantic Segmentation Image Sets Cityscapes CamVid KITTI SYNTHIA Before Fully Convolutional NetworksPre-Deep Learning Approachesä¼ ç»Ÿå›¾åƒåˆ†å‰²ä¸è¯­ä¹‰åˆ†å‰²çš„åŒºåˆ«åœ¨äºè¯­ä¹‰ç‰¹å¾åœ¨å›¾åƒåˆ†å‰²è¿‡ç¨‹ä¸­çš„åº”ç”¨ã€‚ä¼ ç»Ÿçš„å›¾åƒåˆ†å‰²æ–¹æ³•ï¼Œå¦‚é˜ˆå€¼ã€èšç±»å’ŒåŒºåŸŸå¢é•¿ç­‰(æœ‰å…³ä¼ ç»Ÿå›¾åƒåˆ†å‰²æŠ€æœ¯çš„è°ƒæŸ¥ï¼Œè¯·å‚é˜…[29])ä½¿ç”¨æ‰‹å·¥åˆ¶ä½œçš„ä½çº§ç‰¹å¾(å³åœ¨å›¾åƒä¸­å®šä½ç‰©ä½“çš„è¾¹ç•Œã€‚å› æ­¤ï¼Œåœ¨éœ€è¦å›¾åƒè¯­ä¹‰ä¿¡æ¯è¿›è¡Œåƒç´ çº§åˆ†å‰²çš„æƒ…å†µä¸‹ï¼Œä¾‹å¦‚åœ¨ç›¸ä¼¼ç‰©ä½“ç›¸äº’é®æŒ¡çš„æƒ…å†µä¸‹ï¼Œè¿™äº›æ–¹æ³•æ˜¯å¿…è¦çš„ã€‚å…³äºæ·±åº¦CNNsæµè¡Œä¹‹å‰çš„è¯­ä¹‰åˆ†å‰²å·¥ä½œï¼Œæœ‰å¤šç§æ–¹æ³•[30,31ï¼Œ32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]ä½¿ç”¨çš„å›¾å½¢æ¨¡å‹ï¼Œå¦‚é©¬å°”å¯å¤«éšæœºåŸŸ(MRF)ï¼Œæ¡ä»¶éšæœºåœº(CRF)æˆ–åŸºäºæ£®æ—(æœ‰æ—¶è¢«ç§°ä¸ºâ€œæ•´ä½“â€)çš„æ–¹æ³•ï¼Œä»¥ä¾¿åœ¨åƒç´ çº§æ‰¾åˆ°åœºæ™¯æ ‡ç­¾ã€‚å…¶ä¸»è¦æ€æƒ³æ˜¯é€šè¿‡è§‚å¯Ÿç›¸é‚»åƒç´ ä¹‹é—´çš„ä¾èµ–å…³ç³»æ¥æ‰¾åˆ°ä¸€ä¸ªæ¨è®ºã€‚æ¢å¥è¯è¯´ï¼Œè¿™äº›æ–¹æ³•å°†å›¾åƒçš„è¯­ä¹‰å»ºæ¨¡ä¸ºç›¸é‚»åƒç´ ä¹‹é—´çš„ä¸€ç§â€œå…ˆéªŒâ€ä¿¡æ¯ã€‚å¦ä¸€ç»„ç ”ç©¶ï¼Œæœ‰æ—¶è¢«ç§°ä¸ºâ€œåˆ†å±‚æ¨¡å‹â€[44,45,46]ï¼Œä½¿ç”¨ä¸€ä¸ªé¢„å…ˆè®­ç»ƒçš„å’Œç‹¬ç«‹çš„å¯¹è±¡æ¢æµ‹å™¨çš„ç»„åˆï¼Œä»¥ä¾¿ä»å›¾åƒä¸­æå–è¯­ä¹‰ä¿¡æ¯ã€‚ç”±äºå•ä¸ªçš„å¯¹è±¡æ£€æµ‹å™¨æœªèƒ½æ­£ç¡®åœ°å¯¹åŒºåŸŸè¿›è¡Œåˆ†ç±»ï¼Œæˆ–è€…ç”±äºè¿™äº›æ–¹æ³•å—åˆ°â€œæ‰‹å·¥é€‰æ‹©â€çš„æ£€æµ‹å™¨åº“æ‰€æä¾›çš„å¯¹è±¡ç±»çš„æœ‰é™æ•°é‡çš„é™åˆ¶ï¼Œå› æ­¤ä¸å½“ä»Šæœ€å…ˆè¿›çš„æ–¹æ³•ç›¸æ¯”ï¼Œå®ƒä»¬çš„æ€§èƒ½ç›¸å¯¹è¾ƒä½ã€‚è™½ç„¶ä¸Šè¿°çš„å‰æ·±åº¦å­¦ä¹ æ—¶ä»£çš„æ–¹æ³•ä¸å†æ˜¯é¦–é€‰çš„åˆ†å‰²æ–¹æ³•ï¼Œä¸€äº›å›¾å½¢æ¨¡å‹ï¼Œç‰¹åˆ«æ˜¯CRFsï¼Œç›®å‰æ­£åœ¨ä½¿ç”¨çš„æœ€å…ˆè¿›çš„æ–¹æ³•ä½œä¸ºåå¤„ç†(ç»†åŒ–)å±‚ï¼Œç›®çš„æ˜¯æé«˜è¯­ä¹‰åˆ†å‰²çš„æ€§èƒ½ï¼Œå…·ä½“ç»†èŠ‚å°†åœ¨ä¸‹ä¸€èŠ‚ä¸­è®¨è®º Refinement Methodsæ·±åº¦ç¥ç»ç½‘ç»œå…·æœ‰å¾ˆå¼ºçš„å±€éƒ¨ç‰¹å¾æå–èƒ½åŠ›ã€‚ç„¶è€Œï¼Œå®ƒä»¬ç¼ºä¹åˆ©ç”¨å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯çš„èƒ½åŠ›ï¼Œå› æ­¤æ— æ³•å¯¹ç›¸é‚»åƒç´ é¢„æµ‹ä¹‹é—´çš„äº¤äº’è¿›è¡Œå»ºæ¨¡ã€‚å¦ä¸€æ–¹é¢ï¼Œå‰æ·±åº¦å­¦ä¹ æ—¶ä»£æµè¡Œçš„åˆ†å‰²æ–¹æ³•ï¼Œå›¾å½¢æ¨¡å‹ï¼Œéå¸¸é€‚åˆè¿™ç±»ä»»åŠ¡ã€‚è¿™å°±æ˜¯ä¸ºä»€ä¹ˆå®ƒä»¬ç›®å‰è¢«ç”¨ä½œè®¸å¤šæ·±åº¦åŸºäºcnnçš„è¯­ä¹‰åˆ†å‰²æ¶æ„çš„ç»†åŒ–å±‚ã€‚æ­£å¦‚åœ¨å‰ä¸€èŠ‚ä¸­æåˆ°çš„ï¼Œä½¿ç”¨å›¾å½¢æ¨¡å‹è¿›è¡Œåˆ†å‰²èƒŒåçš„æ€æƒ³æ˜¯é€šè¿‡è§‚å¯Ÿç›¸é‚»åƒç´ ä¹‹é—´çš„ä½å±‚æ¬¡å…³ç³»æ¥å¯»æ‰¾ä¸€ä¸ªæ¨è®ºã€‚åœ¨å›¾2ä¸­ï¼Œå¯ä»¥çœ‹åˆ°ä½¿ç”¨åŸºäºå›¾å½¢æ¨¡å‹çš„ç»†åˆ†å¯¹åˆ†å‰²ç»“æœçš„å½±å“ã€‚åˆ†ç±»å™¨(è§å›¾2.b)ä¸èƒ½æ­£ç¡®åˆ†å‰²ä¸åŒç±»æ ‡ç­¾ç›¸é‚»çš„åƒç´ ã€‚åœ¨æœ¬ä¾‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨åŸºäºcrfçš„ç»†åˆ†[42]æ¥æ”¹è¿›åƒç´ çº§çš„åˆ†å‰²ç»“æœã€‚åŸºäºcrfçš„æ–¹æ³•è¢«å¹¿æ³›ç”¨äºæ·±åº¦è¯­ä¹‰åˆ†å‰²æ–¹æ³•çš„ç»†åŒ–CRFs[50]æ˜¯ä¸€ç§æœ‰åŒºåˆ«çš„æ— å‘æ¦‚ç‡å›¾å½¢æ¨¡å‹ã€‚å®ƒä»¬è¢«ç”¨æ¥å¯¹è§‚æµ‹ä¹‹é—´å·²çŸ¥çš„å…³ç³»è¿›è¡Œç¼–ç ï¼Œå¹¶æ„å»ºä¸€è‡´çš„è§£é‡Šã€‚å®ƒä»¬ç”¨ä½œç»†åŒ–å±‚çš„åŸå› æ˜¯ï¼Œä¸ä¸è€ƒè™‘ç›¸é‚»åƒç´ ç›¸ä¼¼æ€§çš„ç¦»æ•£åˆ†ç±»å™¨ä¸åŒï¼ŒCRFå¯ä»¥åˆ©ç”¨è¿™äº›ä¿¡æ¯ã€‚ä¸å…¶ä»–å›¾å½¢åŒ–æ¨¡å‹(å¦‚éšé©¬å°”ç§‘å¤«æ¨¡å‹)ç›¸æ¯”ï¼ŒCRFsçš„ä¸»è¦ä¼˜ç‚¹æ˜¯å®ƒä»¬çš„æ¡ä»¶æ€§è´¨å’Œé¿å…æ ‡ç­¾åå·®[50]é—®é¢˜çš„èƒ½åŠ›ã€‚å°½ç®¡æœ‰ç›¸å½“æ•°é‡çš„æ–¹æ³•(è§è¡¨1)ä½¿ç”¨CRFsè¿›è¡Œç»†åŒ–ï¼Œä½†è¿™äº›æ¨¡å‹åœ¨ç›¸å¯¹è¾ƒæ–°çš„æ–¹æ³•ä¸­å¼€å§‹å˜å¾—ä¸å—æ¬¢è¿ï¼Œå› ä¸ºå®ƒä»¬çš„é€Ÿåº¦éå¸¸æ…¢ï¼Œè€Œä¸”éå¸¸éš¾ä»¥ä¼˜åŒ–ã€‚ Early Deep Learning ApproachesFCNåœ¨2014å¹´å‡ºç°ï¼Œä½¿ç”¨tanh ç›¸è¾ƒäºproposal of a ReLU layer å¾ˆéš¾å»åŒºåˆ†ï¼Œå› æ­¤ï¼Œè®­ç»ƒè¿™æ ·çš„ç³»ç»Ÿè¢«è®¤ä¸ºæ˜¯ä¸é€‚åˆè®¡ç®—çš„ï¼Œç”šè‡³å¯¹å¤§è§„æ¨¡æ•°æ®æ˜¯ä¸å¯è¡Œçš„ã€‚ç„¶è€Œï¼Œç¬¬ä¸€ä¸ªæˆç†Ÿçš„æ–¹æ³•åªæ˜¯ç®€å•åœ°å°è¯•è½¬æ¢åˆ†ç±»ç½‘ç»œï¼Œå¦‚AlexNetå’ŒVGGé€šè¿‡å¾®è°ƒå…¨è¿æ¥å±‚æ¥ç»†åˆ†ç½‘ç»œã€‚ä»–ä»¬åœ¨è®­ç»ƒé˜¶æ®µé­å—äº†è¿‡åº¦æ‹Ÿåˆå’Œå®Œå…¨è¿æ¥å±‚çš„æ—¶é—´é™åˆ¶ã€‚æ­¤å¤–ï¼Œä½¿ç”¨çš„CNNsä¸å¤Ÿæ·±ï¼Œæ— æ³•åˆ›å»ºæŠ½è±¡çš„ç‰¹å¾ï¼Œè¿™ä¸å›¾åƒçš„è¯­ä¹‰æœ‰å…³ã€‚åœ¨ä¸€äº›æ—©æœŸçš„æ·±åº¦å­¦ä¹ ç ”ç©¶ä¸­ï¼Œç ”ç©¶äººå‘˜æ‹’ç»ä½¿ç”¨å®Œå…¨è¿æ¥çš„å±‚æ¥è¿›è¡Œå†³ç­–ï¼Œè€Œæ˜¯ä½¿ç”¨ä¸åŒçš„ç»“æ„ï¼Œå¦‚å‘¨æœŸæ€§çš„æ¶æ„[57]æˆ–ä½¿ç”¨æ¥è‡ªä¸€ä¸ªå•ç‹¬è®¡ç®—çš„åˆ†æ®µå®¶æ—çš„æ ‡è®°ã€‚é€šè¿‡æå‡ºå…¨è¿æ¥å±‚FCNè¿™æ ·çš„ç»“æ„çš„å¿…è¦æ€§çš„ç¬¬ä¸€ä¸ªè¿¹è±¡ï¼Œä¸å‡ºæ‰€æ–™ï¼Œå®ƒä»¬è¢«FCNå–ä»£ã€‚ç”±äºä»–ä»¬çš„åˆ†å‰²ç»“æœè¢«è®¤ä¸ºæ˜¯ä¸ä»¤äººæ»¡æ„çš„ï¼Œè¿™äº›ç ”ç©¶é€šå¸¸ä½¿ç”¨ä¸€ä¸ªç»†åŒ–çš„è¿‡ç¨‹ï¼Œè¦ä¹ˆä½œä¸ºä¸€ä¸ªåå¤„ç†å±‚[52,53,54,56]ï¼Œæˆ–ä½œä¸ºä¸€ä¸ªæ›¿ä»£æ¶æ„ï¼Œä»¥å®Œå…¨è¿æ¥çš„å†³ç­–å±‚Refinement methods varied such as Markov random fieldsï¼Œnearest neighbour-based approachï¼Œä½¿ç”¨æ ¡å‡†å±‚[54]ï¼Œä½¿ç”¨è¶…çº§åƒç´ [55,56]ï¼Œæˆ–æ™®é€šCNNsçš„é€’å½’ç½‘ç»œã€‚ç»†åŒ–å±‚ä»ç„¶è¢«åfcnæ–¹æ³•æ‰€ä½¿ç”¨ï¼Œå…¶ç›®çš„æ˜¯æé«˜ç±»äº¤å‰åŒºåŸŸçš„åƒç´ çº§æ ‡è®°æ€§èƒ½ã€‚tips: 4FCN [11] ] was officially published in 2017. However the same group first shared the idea online as pre-printed literature in 014 [51]. Fully Convolutional Networks for Semantic SegmentationFCN(2017) æå‡ºäº†ä»CNNs (DCNN)ä¸­æ‹†é™¤å…¨è¿é€šå±‚çš„è®¾æƒ³.â€˜FCN-32sâ€™, â€˜FCN16sâ€™, and â€˜FCN8sâ€™ all transfer-learnt using the VGG architectureFCNæ¶æ„åœ¨å¾ˆå¤šæ–¹é¢éƒ½è¢«è®¤ä¸ºæ˜¯é©å‘½æ€§çš„, FCNä¸åŒ…æ‹¬å…¨è¿æ¥å±‚ è¯¥ç»“æ„å…è®¸ä¸ºä»»ä½•åˆ†è¾¨ç‡çš„å›¾åƒç”Ÿæˆåˆ†å‰²å›¾ã€‚ä½¿ç”¨åå·ç§¯å±‚ï¼Œå¯ä»¥å°†ç²—æ·±å·ç§¯å±‚è¾“å‡ºæå‡åˆ°ä»»æ„åˆ†è¾¨ç‡çš„ç¨ å¯†åƒç´ ã€‚ æå‡ºäº†DCNNsçš„skipæ¶æ„ã€‚è·³è¿‡æ¶æ„(æˆ–è¿æ¥)åœ¨DCNNsä¸­æä¾›ä¸ç›¸é‚»å±‚ä¹‹é—´çš„é“¾æ¥ã€‚ä»…ä»…é€šè¿‡å¯¹æœªè¿æ¥å±‚çš„è¾“å‡ºè¿›è¡Œæ±‚å’Œæˆ–è¿æ¥ï¼Œè¿™äº›è¿æ¥å°±å¯ä»¥ä½¿ä¿¡æ¯æµåŠ¨ï¼Œå¦åˆ™ï¼Œç”±äºä½“ç³»ç»“æ„çš„é€‰æ‹©(å¦‚æœ€å¤§æ± åŒ–å±‚æˆ–è¾å­¦)ï¼Œè¿™äº›ä¿¡æ¯å°±ä¼šä¸¢å¤±ã€‚æœ€å¸¸è§çš„åšæ³•æ˜¯åœ¨max-poolingå±‚ä¹‹å‰ä½¿ç”¨skipè¿æ¥ï¼Œå®ƒé€šè¿‡é€‰æ‹©ç‰¹å®šåŒºåŸŸçš„æœ€å¤§å€¼å¯¹å±‚è¾“å‡ºè¿›è¡Œé‡‡æ ·ã€‚æ± åŒ–å±‚æœ‰åŠ©äºæ¶æ„åˆ›å»ºç‰¹æ€§å±‚æ¬¡ï¼Œä½†ä¹Ÿä¼šå¯¼è‡´å±€éƒ¨ä¿¡æ¯çš„ä¸¢å¤±ï¼Œè€Œè¿™äº›å±€éƒ¨ä¿¡æ¯å¯¹äºè¯­ä¹‰åˆ†å‰²æ˜¯æœ‰ä»·å€¼çš„ï¼Œç‰¹åˆ«æ˜¯åœ¨å¯¹è±¡è¾¹ç•Œã€‚è·³è¿‡è¿æ¥é€šè¿‡ç»•è¿‡æ± åŒ–å±‚æ¥ä¿å­˜è¿™äº›ä¿¡æ¯å¹¶å°†å…¶è½¬å‘åˆ°æ›´æ·±å±‚ã€‚å®é™…ä¸Šï¼Œåœ¨[11]ä¸­ä½¿ç”¨è·³è½¬è¿æ¥è¢«è®¤ä¸ºæ˜¯ç›¸å½“åŸå§‹çš„ã€‚â€œFCN-8sâ€å’Œâ€œFCN-16sâ€ç½‘ç»œåœ¨ä¸åŒçš„å±‚åŒ…å«è¿™äº›è·³è¿‡è¿æ¥ã€‚å¯¹äºç›¸åŒçš„æ¶æ„ï¼Œå³â€œFCN-4sâ€å’Œâ€œæ›´å¯†é›†çš„è·³è¿‡è¿æ¥â€ã€‚â€œFCN-2sâ€ä¹Ÿè¢«ç”¨äºå„ç§åº”ç”¨[61,62]ã€‚è¿™ä¸€æ€æƒ³æœ€ç»ˆæ¼”å˜ä¸ºç”¨äºè¯­ä¹‰åˆ†å‰²çš„ç¼–ç å™¨-è§£ç å™¨ç»“æ„[63,27]ï¼Œä¸‹æ–‡å°†å¯¹æ­¤è¿›è¡Œä»‹ç»ã€‚ Post-FCN Approachesdrawbacks of FCNsï¼š ç‰¹æ€§å±‚æ¬¡ç»“æ„ä¸­æ ‡ç­¾æœ¬åœ°åŒ–çš„ä½æ•ˆä¸¢å¤±ã€æ— æ³•å¤„ç†å…¨å±€ä¸Šä¸‹æ–‡çŸ¥è¯†ä»¥åŠç¼ºä¹å¤šå°ºåº¦å¤„ç†æœºåˆ¶ã€‚æˆ‘ä»¬è¿˜è®¨è®ºäº†è¯­ä¹‰åˆ†å‰²ä¸Šä¸‹æ–‡ä¸­çš„å°ºåº¦ä¸å˜æ€§ï¼Œæœ€åè®¨è®ºäº†åŸºäºå¯¹è±¡æ£€æµ‹çš„æ–¹æ³•ï¼Œè¿™æ˜¯ä¸€ç§æ–°çš„è§£å†³æ–¹æ¡ˆï¼Œæ—¨åœ¨è§£å†³åŒæ—¶æ£€æµ‹å¯¹è±¡å®ä¾‹çš„è¯­ä¹‰åˆ†å‰²é—®é¢˜ã€‚ Techniques for Fine-grained Localisationæ ¹æ®å®šä¹‰ï¼Œè¯­ä¹‰åˆ†å‰²æ˜¯ä¸€ä¸ªå¯†é›†çš„è¿‡ç¨‹ï¼Œå› æ­¤å®ƒéœ€è¦åœ¨åƒç´ çº§å¯¹ç±»æ ‡ç­¾è¿›è¡Œç»†ç²’åº¦çš„æœ¬åœ°åŒ–ã€‚ä¾‹å¦‚ï¼Œåœ¨æœºå™¨äººæ‰‹æœ¯ä¸­ï¼Œè¯­ä¹‰åˆ†å‰²ä¸­çš„åƒç´ é”™è¯¯å¯èƒ½ä¼šå¯¼è‡´ç”Ÿå­˜æˆ–æ­»äº¡çš„æƒ…å†µã€‚å±‚æ¬¡ç‰¹æ€§åˆ›å»ºçš„æ± (å³ã€‚ï¼Œæœ€å¤§æ± )å±‚å¯ä»¥éƒ¨åˆ†å¤±å»æœ¬åœ°åŒ–ã€‚æ­¤å¤–ï¼Œç”±äºä»–ä»¬FCNså®Œå…¨æ˜¯å·ç§¯æ€§è´¨çš„ï¼Œå®ƒæœ¬èº«å¹¶ä¸å…·å¤‡åœ¨å›¾åƒä¸­å¯¹å…¨å±€ä¸Šä¸‹æ–‡ä¿¡æ¯å»ºæ¨¡çš„èƒ½åŠ›ï¼Œè¿™åœ¨ç±»æ ‡ç­¾æœ¬åœ°åŒ–æ–¹é¢ä¹Ÿéå¸¸æœ‰æ•ˆã€‚å› æ­¤ï¼Œè¿™ä¸¤ä¸ªé—®é¢˜åœ¨æœ¬è´¨ä¸Šå’Œæœ¬è´¨ä¸Šæ˜¯ç›¸äº’äº¤ç»‡çš„ä¸‹é¢æˆ‘ä»¬å°†è®¨è®ºæ—¨åœ¨å…‹æœè¿™äº›é—®é¢˜å’Œæä¾›æ›´å¥½çš„æœ¬åœ°åŒ–çš„ä¸åŒæ–¹æ³•ç±»çš„æ ‡ç­¾ã€‚ Encoder-Decoder Architectureç¼–è¯‘ç å™¨ï¼Œ ç±»ä¼¼U-netï¼Œå…·æœ‰å¼€åˆ›æ–°çš„ç ”ç©¶ã€‚ç¼–ç å™¨ä½¿ç”¨æ± åŒ–å±‚é€æ¸ç¼©å‡è¾“å…¥æ•°æ®çš„ç©ºé—´ç»´åº¦ï¼Œè€Œè§£ç å™¨é€šè¿‡åå·ç§¯å±‚ç­‰ç½‘ç»œå±‚é€æ­¥æ¢å¤ç›®æ ‡çš„ç»†èŠ‚å’Œç›¸åº”çš„ç©ºé—´ç»´åº¦ã€‚ä»ç¼–ç å™¨åˆ°è§£ç å™¨ä¹‹é—´ï¼Œé€šå¸¸å­˜åœ¨ç›´æ¥çš„ä¿¡æ¯è¿æ¥ï¼Œæ¥å¸®åŠ©è§£ç å™¨æ›´å¥½åœ°æ¢å¤ç›®æ ‡ç»†èŠ‚ã€‚U-Netï¼ŒSeg-Net éƒ½æ˜¯éå¸¸å‡ºåçš„ç½‘ç»œã€‚åœ¨è¿™ç§ç»“æ„ä¸­ï¼Œç”±ç¼–ç å™¨éƒ¨åˆ†ç›¸é‚»çš„ä½åˆ†è¾¨ç‡ç‰¹å¾æ˜ å°„æä¾›çš„å¼ºç›¸å…³è¯­ä¹‰ä¿¡æ¯å¿…é¡»ç»è¿‡é¢å¤–çš„ä¸­é—´å±‚æ‰èƒ½åˆ°è¾¾ç›¸åŒçš„è¯‘ç å±‚ã€‚è¿™é€šå¸¸ä¼šå¯¼è‡´ä¸€å®šç¨‹åº¦çš„ä¿¡æ¯è¡°å‡ã€‚ç„¶è€Œï¼ŒU-Netæ¶æ„å·²ç»è¢«è¯æ˜å¯¹äºä¸åŒåº”ç”¨çš„åˆ†å‰²éå¸¸æœ‰ç”¨ï¼Œä¾‹å¦‚å«æ˜Ÿå›¾åƒã€‚ Spatial Pyramid Pooling(ç©ºé—´é‡‘å­—å¡”æ± åŒ–)Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. åœ¨2006å¹´é¦–æ¬¡è¢«æå‡ºï¼Œ ä¸ºçš„æ˜¯è§£å†³å•è¯è¢‹ç³»ç»Ÿå¤±å»äº†ç‰¹å¾ä¹‹é—´çš„ç©ºé—´å…³ç³»ã€‚é¦–æ¬¡åº”ç”¨åœ¨æ·±åº¦å­¦ä¹ æ˜¯2015å¹´çš„SPPNetè¿™ç¯‡æ–‡ç« ã€‚æ— è®ºè¾“å…¥å¤§å°å¦‚ä½•ï¼Œéƒ½å¯ä»¥åœ¨ç©ºé—´é‡‘å­—å¡”æ±‡èšç½‘ç»œä¸­åˆ›å»ºæ·±åº¦ç‰¹å¾çš„ç©ºé—´é‡‘å­—å¡”è¡¨ç¤ºã€‚SPP-Netæœ€é‡è¦çš„è´¡çŒ®æ˜¯å®ƒå…è®¸è¾“å…¥ä¸åŒå¤§å°çš„æ•°æ®ã€‚ä¸åŒå¤§å°çš„å›¾åƒè¾“å…¥åˆ°å·ç§¯å±‚ä¸­ï¼Œä¸å¯é¿å…åœ°ä¼šäº§ç”Ÿä¸åŒå¤§å°çš„ç‰¹å¾å›¾ã€‚ç„¶è€Œ,å¦‚æœæ± åŒ–å±‚åˆšå¥½åœ¨å†³ç­–å±‚ä¹‹å‰ï¼Œå…·æœ‰ä¸è¾“å…¥å¤§å°æˆæ¯”ä¾‹çš„æ­¥é•¿å€¼ï¼Œåˆ™åˆ›å»ºç‰¹å¾æ˜ å°„è¿™ä¸€å±‚å°†è¢«å›ºå®šã€‚CNNä¸­çš„SPPå±‚æ„å»ºäº†ä¸åŒå±‚æ¬¡ç‰¹å¾ä¹‹é—´çš„å…³ç³»ã€‚å› æ­¤ï¼Œå®ƒä¸EDç»“æ„ä¸­çš„è·³è¿‡è¿æ¥éå¸¸ç›¸ä¼¼ï¼Œåè€…ä¹Ÿå…è®¸ç‰¹æ€§å±‚æ¬¡ç»“æ„ä¹‹é—´çš„ä¿¡æ¯æµã€‚ï¼šSPPå±‚ç”¨äºè¯­ä¹‰åˆ†å‰²æœ€å¸¸è§çš„ç”¨æ³•æ˜¯åœ¨[67]ä¸­æå‡ºçš„ï¼Œæ¯”å¦‚SPPå±‚è¢«é™„åŠ åˆ°æœ€åä¸€ä¸ªå·ç§¯å±‚ï¼Œå¹¶åé¦ˆç»™åƒç´ çº§åˆ†ç±»å™¨ã€‚ Feature Concatenation(ç‰¹å¾è¿æ¥)è¿™ä¸ªæƒ³æ³•æ˜¯åŸºäºèåˆä»ä¸åŒæ¥æºæå–çš„ç‰¹å¾ã€‚ Dilated Convolution(æ‰©å¼ å·ç§¯ï¼Œç©ºæ´å·ç§¯)æ‰©å±•å·ç§¯çš„æ€æƒ³å®é™…ä¸Šå¾ˆç®€å•:ä½¿ç”¨è¿ç»­çš„å·ç§¯æ»¤æ³¢å™¨ï¼Œä¸€ä¸ªæœ‰æ•ˆçš„æ¥æ”¶åŸŸåªèƒ½éšå±‚çº¿æ€§å¢é•¿;ç„¶è€Œï¼Œå¦‚æœä½¿ç”¨åœ¨æ»¤æ³¢å™¨ä¸­æœ‰é—´éš™çš„è†¨èƒ€å·ç§¯(è§å›¾4.c)ï¼Œæœ‰æ•ˆæ¥å—åŸŸå°†å¢é•¿å¾—æ›´å¿«[70]ã€‚å› æ­¤ï¼Œåœ¨æ²¡æœ‰æ± æˆ–å­é‡‡æ ·çš„æƒ…å†µä¸‹ï¼Œåˆ›å»ºäº†å·ç§¯å±‚çš„çŸ©å½¢æ£±é•œã€‚æ‰©å¼ å·ç§¯æ˜¯ä¸€ç§éå¸¸æœ‰æ•ˆå’Œå¼ºå¤§çš„æ–¹æ³•æ¥è¯¦ç»†ä¿å­˜ç‰¹å¾å›¾åˆ†è¾¨ç‡ã€‚ç¼ºç‚¹åœ¨äºå¯¹GPUå­˜å‚¨å’Œè®¡ç®—çš„è¦æ±‚æ›´é«˜ï¼Œå› ä¸ºç‰¹å¾å›¾åˆ†è¾¨ç‡ä¸ä¼šåœ¨ç‰¹å¾å±‚æ¬¡ç»“æ„ä¸­ç¼©å°ã€‚ Conditional Random Fields(æ¡ä»¶éšæœºåœº)cnnè‡ªç„¶ç¼ºä¹ç‰¹åˆ«çš„â€˜å…³æ³¨â€™ç±»äº¤å‰åŒºåŸŸçš„æœºåˆ¶ã€‚åœ¨è¿™äº›åŒºåŸŸå‘¨å›´ï¼Œé€šè¿‡è§‚å¯ŸCNNå±‚çš„ç›¸é‚»feature mapsä¹‹é—´çš„ä½å±‚å…³ç³»ï¼Œä½¿ç”¨å›¾å½¢åŒ–æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚å› æ­¤ï¼Œå›¾å½¢æ¨¡å‹(ä¸»è¦æ˜¯crf)è¢«ç”¨ä½œæ·±åº¦è¯­ä¹‰åˆ†å‰²æ¶æ„çš„ç»†åŒ–å±‚ã€‚ä¸åœ¨[72]ä¸­ä¸€æ ·ï¼ŒCRFså°†ä½çº§äº¤äº’ä¸æ¥è‡ªå¤šç±»äº¤äº’çš„è¾“å‡ºè¿æ¥èµ·æ¥ï¼Œå¹¶ä»¥è¿™ç§æ–¹å¼æ„å»ºå…¨å±€ä¸Šä¸‹æ–‡çŸ¥è¯†ã€‚CRFsä½œä¸ºä¸€ç§ç»†åŒ–å±‚ï¼Œç›®å‰å­˜åœ¨å¤šç§åˆ©ç”¨CRFså¯¹CNNsè¿›è¡Œæ·±åº¦å¤„ç†çš„æ–¹æ³•ï¼Œå¦‚å·ç§¯CRFs[47]ã€ç¨ å¯†CRF[42]ã€CRN-as-RNNç­‰[73]ã€‚å°½ç®¡CRFsæœ‰åŠ©äºæ„å»ºä¸Šä¸‹æ–‡çŸ¥è¯†ï¼Œä»è€Œåœ¨ç±»æ ‡ç­¾ä¸­æ›´å¥½åœ°æœ¬åœ°åŒ–ï¼Œè¡¨1æ˜¾ç¤ºäº†åœ¨â€œCRFæ¨¡å‹â€é€‰é¡¹å¡ä¸‹åˆ†ç±»çš„CRFsï¼Œä»¥ä¾¿å°†å®ƒä»¬ä¸å®é™…çš„CNNæ¶æ„æ‰©å±•åŒºåˆ†å¼€ã€‚ Recurrent Approachesé€’å½’ç¥ç»ç½‘ç»œå¤„ç†æ—¶é—´ä¿¡æ¯çš„èƒ½åŠ›æœ‰åŠ©äºæé«˜åˆ†å‰²ç²¾åº¦ã€‚ä¾‹å¦‚ï¼Œ[74]ä½¿ç”¨ConvLSTMå±‚æ¥æ”¹è¿›å›¾åƒåºåˆ—ä¸­çš„è¯­ä¹‰åˆ†å‰²ç»“æœã€‚ç„¶è€Œï¼Œä¹Ÿæœ‰ä¸€äº›æ–¹æ³•åœ¨é™æ€å›¾åƒä¸Šä½¿ç”¨å¾ªç¯ç»“æ„ã€‚åœ¨[13]ä¸­ï¼Œç ”ç©¶äººå‘˜åˆ©ç”¨LSTMchainsæ¥ç¼ ç»•å¤šä¸ªå°ºåº¦ï¼Œä»è€Œå¾—åˆ°åƒç´ çº§çš„åˆ†å‰²æ”¹è¿›ã€‚ä¹Ÿæœ‰å°†CNNså’ŒRNNsèåˆçš„æ··åˆæ–¹æ³•ã€‚è¿™æ–¹é¢çš„ä¸€ä¸ªå¾ˆå¥½çš„ä¾‹å­æ˜¯æ‰€è°“çš„ReSegæ¨¡å‹[75]ï¼Œå…¶ä¸­ï¼Œè¾“å…¥å›¾åƒè¢«é¦ˆé€åˆ°ä¸€ä¸ªç±»ä¼¼äºvggçš„CNNç¼–ç å™¨ï¼Œç„¶åé€šè¿‡é€’å½’å±‚(å³ReNetæ¶æ„)è¿›è¡Œå¤„ç†ï¼Œä»¥ä¾¿æ›´å¥½åœ°å®šä½åƒç´ æ ‡ç­¾ã€‚æ®æˆ‘ä»¬æ‰€çŸ¥ï¼Œè¯­ä¹‰åˆ†å‰²ä¸å­˜åœ¨å•çº¯çš„é€’å½’ç»“æ„ï¼Œè¿™ä¸»è¦æ˜¯å› ä¸ºè¯­ä¹‰åˆ†å‰²éœ€è¦ä¸€ä¸ªåˆæ­¥çš„åŸºäºcnnçš„ç‰¹å¾ç¼–ç æ–¹æ¡ˆã€‚ç›®å‰ï¼Œæœ‰ä¸€ç§ç‰¹å®šç±»å‹çš„RNNï¼Œå³â€œæ³¨æ„æ¨¡å—â€ï¼Œæœ‰å¢é•¿çš„è¶‹åŠ¿ã€‚åœ¨è¿™äº›æ¨¡å—ä¸­ï¼ŒRNNåœ¨æŠ€æœ¯ä¸Šèåˆäº†æ³¨æ„åŠ›[76]ï¼Œåœ¨é¢„æµ‹è¾“å‡ºåºåˆ—çš„æŸä¸ªéƒ¨åˆ†æ—¶ï¼Œå°†æ³¨æ„åŠ›é›†ä¸­åœ¨è¾“å…¥çš„æŸä¸ªåŒºåŸŸã€‚å› æ­¤ï¼Œå®ƒä»¬ä¹Ÿè¢«ç”¨äºè¯­ä¹‰åˆ†å‰²[77,78,79]ã€‚ Scale-Invariance(å°ºåº¦å˜åŒ–)æ ¹æ®å®šä¹‰ï¼Œå°ºåº¦ä¸å˜æ€§æ˜¯æŒ‡ä¸€ä¸ªæ–¹æ³•å¤„ç†è¾“å…¥æ—¶ä¸ä¾èµ–äºç›¸å¯¹å°ºåº¦çš„èƒ½åŠ›ã€‚æˆ–å›¾åƒåˆ†è¾¨ç‡ã€‚å°½ç®¡å®ƒå¯¹äºæŸäº›åº”ç”¨ç¨‹åºæ¥è¯´æ˜¯æå…¶é‡è¦çš„ï¼Œä½†æ˜¯è¿™ç§èƒ½åŠ›é€šå¸¸è¢«å¿½è§†ï¼Œæˆ–è€…ä¸æ–¹æ³•åŒ…å«å¤šå°ºåº¦ä¿¡æ¯çš„èƒ½åŠ›ç›¸æ··æ·†ã€‚ä¸€ç§æ–¹æ³•å¯ä»¥ä½¿ç”¨å¤šå°ºåº¦ä¿¡æ¯æ¥æé«˜å…¶åƒç´ çº§åˆ†å‰²èƒ½åŠ›ï¼Œä½†ä»ç„¶ä¾èµ–äºå°ºåº¦æˆ–åˆ†è¾¨ç‡ã€‚ æ–‡çŒ®ï¼š A SURVEY ON DEEP LEARNING-BASED ARCHITECTURES FOR SEMANTIC SEGMENTATION ON 2D IMAGES","tags":["OpenCV"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"äºŒå‰æ ‘æ‰“å°","path":"/2019/12/19/20191219-äºŒå‰æ ‘æ‰“å°/","content":"äºŒå‰æ ‘æŒ‰å±‚éå† é’ˆå¯¹äºŒå‰æ ‘çš„å®½åº¦ä¼˜å…ˆéå† å®½åº¦ä¼˜å…ˆéå†å¸¸ä½¿ç”¨é˜Ÿåˆ—ç»“æ„ é¢è¯•ä¸­ï¼Œè¯¥ç±»é¢˜ç›®å¸¸å¸¸å¯¹æ¢è¡Œæœ‰æ‰€è¦æ±‚last: è¡¨ç¤ºæ­£åœ¨æ‰“å°çš„å½“å‰è¡Œçš„æœ€å³èŠ‚ç‚¹nlastï¼šè¡¨ç¤ºä¸‹ä¸€è¡Œçš„æœ€å³èŠ‚ç‚¹ äºŒå‰æ ‘çš„åºåˆ—åŒ–å’Œååºåˆ—åŒ– äºŒå‰æ ‘-å­—ç¬¦ä¸²ï¼ˆåºåˆ—åŒ– å­—ç¬¦ä¸²-äºŒå‰æ ‘ï¼ˆååºåˆ—åŒ– åºåˆ—åŒ–çš„æ–¹å¼ï¼š æ ¹æ®å…ˆåºéå†åºåˆ—åŒ– æ ¹æ®ä¸­åºéå†åºåˆ—åŒ– æ ¹æ®ååºéå†åºåˆ—åŒ– æŒ‰å±‚åºåˆ—åŒ–","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"äºŒå‰æ ‘æ‰“å°","path":"/2019/12/19/äºŒå‰æ ‘æ‰“å°/","content":"äºŒå‰æ ‘æŒ‰å±‚éå† é’ˆå¯¹äºŒå‰æ ‘çš„å®½åº¦ä¼˜å…ˆéå† å®½åº¦ä¼˜å…ˆéå†å¸¸ä½¿ç”¨é˜Ÿåˆ—ç»“æ„ é¢è¯•ä¸­ï¼Œè¯¥ç±»é¢˜ç›®å¸¸å¸¸å¯¹æ¢è¡Œæœ‰æ‰€è¦æ±‚last: è¡¨ç¤ºæ­£åœ¨æ‰“å°çš„å½“å‰è¡Œçš„æœ€å³èŠ‚ç‚¹nlastï¼šè¡¨ç¤ºä¸‹ä¸€è¡Œçš„æœ€å³èŠ‚ç‚¹ äºŒå‰æ ‘çš„åºåˆ—åŒ–å’Œååºåˆ—åŒ– äºŒå‰æ ‘-å­—ç¬¦ä¸²ï¼ˆåºåˆ—åŒ– å­—ç¬¦ä¸²-äºŒå‰æ ‘ï¼ˆååºåˆ—åŒ– åºåˆ—åŒ–çš„æ–¹å¼ï¼š æ ¹æ®å…ˆåºéå†åºåˆ—åŒ– æ ¹æ®ä¸­åºéå†åºåˆ—åŒ– æ ¹æ®ååºéå†åºåˆ—åŒ– æŒ‰å±‚åºåˆ—åŒ–","tags":["BFS"],"categories":["ç®—æ³•"]},{"title":"éšé©¬å°”å¯å¤«HMM","path":"/2019/11/28/20191128-HMM/","content":"é©¬å°”ç§‘å¤«æ¨¡å‹ çŠ¶æ€ä¹‹é—´å¯ä»¥å‘ç”Ÿè½¬æ¢ï¼Œæ˜¨å¤©å’Œä»Šå¤©è½¬æ¢çš„æƒ…å†µï¼š ä»Šå¤©èƒ½å¾—åˆ°æ˜å¤©çš„æƒ…å†µï¼Œæ˜å¤©èƒ½å¾—åˆ°åå¤©çš„æƒ…å†µï¼Œä»¥æ­¤ç±»æ¨å¯ä»¥æ— é™çš„ç©ä¸‹å» è¿™é‡Œæˆ‘ä»¬å°±å®šä¹‰å¥½äº†ä¸€ä¸ªä¸€é˜¶é©¬å°”ç§‘å¤«æ¨¡å‹ï¼šçŠ¶æ€ï¼šæ™´å¤©ï¼Œå¤šäº‘ï¼Œé›·é›¨çŠ¶æ€è½¬æ¢æ¦‚ç‡ï¼šä¸‰ç§å¤©æ°”çŠ¶æ€é—´çš„è½¬æ¢æ¦‚ç‡åˆå§‹æ¦‚ç‡ï¼šæ™´å¤© è®¡ç®—ä»Šå¤©(t=1)çš„å¤©æ°”çŠ¶å†µï¼šä»Šå¤©ä¸ºæ™´å¤©çš„æ¦‚ç‡=åˆå§‹æ™´å¤©æ¦‚ç‡Xæ™´å¤©è½¬æ™´å¤©æ¦‚ç‡ +åˆå§‹å¤šäº‘æ¦‚ç‡Xå¤šäº‘è½¬æ™´å¤©æ¦‚ç‡ +åˆå§‹é›·é›¨æ¦‚ç‡Xé›·é›¨è½¬æ™´å¤©æ¦‚ç‡ã€‚ éšé©¬å°”ç§‘å¤«æ¨¡å‹ å½“å‰çš„çŠ¶æ€åªå’Œå‰ä¸€çŠ¶æ€æœ‰å…³ï¼š æŸä¸ªè§‚æµ‹åªå’Œç”Ÿæˆå®ƒçš„çŠ¶æ€æœ‰å…³ï¼š éšé©¬å°”ç§‘å¤«æ¨¡å‹çš„ç»„æˆ ä¸‰ä¸ªå¿…å¤‡ï¼šåˆå§‹æ¦‚ç‡(Ï€)ï¼Œéšè—çŠ¶æ€è½¬ç§»æ¦‚ç‡çŸ©é˜µ(A)ï¼Œç”Ÿæˆè§‚æµ‹çŠ¶æ€æ¦‚ç‡çŸ©é˜µ(B)ã€‚ éšè—çŠ¶æ€ä¸è§‚å¯ŸçŠ¶æ€ï¼ˆBçŸ©é˜µï¼‰: è¦è§£å†³çš„é—®é¢˜: æ¨¡å‹ä¸º ç»™å®šæ¨¡å‹åŠè§‚æµ‹åºåˆ— è®¡ç®—å…¶å‡ºç°çš„æ¦‚ç‡ ç»™å®šè§‚æµ‹åºåˆ—æ±‚è§£å‚æ•°ä½¿å¾—æœ€å¤§ å·²çŸ¥æ¨¡å‹å’Œè§‚æµ‹åºåˆ—æ±‚çŠ¶æ€åºåˆ—ï¼Œä½¿å¾—æœ€å¤§ æ±‚è§‚æµ‹åºåˆ—çš„æ¦‚ç‡ æš´åŠ›æ±‚è§£ï¼šæˆ‘ä»¬è¦æ±‚çš„æ˜¯åœ¨ç»™å®šæ¨¡å‹ä¸‹è§‚æµ‹åºåˆ—å‡ºç°çš„æ¦‚ç‡ï¼Œé‚£å¦‚æœæˆ‘èƒ½æŠŠæ‰€æœ‰çš„éšè—åºåˆ—éƒ½ç»™åˆ—å‡ºæ¥ï¼Œä¹Ÿå°±å¯ä»¥çŸ¥é“è”åˆæ¦‚ç‡åˆ†å¸ƒ ç°åœ¨è¦æ±‚çš„ç›®æ ‡å°±å¾ˆæ˜ç¡®äº†ã€‚åœ¨ç»™å®šæ¨¡å‹ä¸‹ï¼Œä¸€ä¸ªéšè—åºåˆ—å‡ºç°çš„æ¦‚ç‡ï¼Œé‚£å°±ç”±åˆå§‹çŠ¶æ€æ…¢æ…¢è½¬æ¢å˜›ã€‚å‡ºç°çš„æ¦‚ç‡ä¸ºï¼š å‰å‘ç®—æ³•ç»™å®štæ—¶åˆ»çš„éšè—çŠ¶æ€ä¸ºiï¼Œè§‚æµ‹åºåˆ—ä¸ºo1,o2â€¦otçš„æ¦‚ç‡å«åšå‰å‘æ¦‚ç‡ï¼š","tags":["æ·±åº¦å­¦ä¹ "],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"éšé©¬å°”å¯å¤«HMM","path":"/2019/11/28/HMM/","content":"é©¬å°”ç§‘å¤«æ¨¡å‹ çŠ¶æ€ä¹‹é—´å¯ä»¥å‘ç”Ÿè½¬æ¢ï¼Œæ˜¨å¤©å’Œä»Šå¤©è½¬æ¢çš„æƒ…å†µï¼š ä»Šå¤©èƒ½å¾—åˆ°æ˜å¤©çš„æƒ…å†µï¼Œæ˜å¤©èƒ½å¾—åˆ°åå¤©çš„æƒ…å†µï¼Œä»¥æ­¤ç±»æ¨å¯ä»¥æ— é™çš„ç©ä¸‹å» è¿™é‡Œæˆ‘ä»¬å°±å®šä¹‰å¥½äº†ä¸€ä¸ªä¸€é˜¶é©¬å°”ç§‘å¤«æ¨¡å‹ï¼šçŠ¶æ€ï¼šæ™´å¤©ï¼Œå¤šäº‘ï¼Œé›·é›¨çŠ¶æ€è½¬æ¢æ¦‚ç‡ï¼šä¸‰ç§å¤©æ°”çŠ¶æ€é—´çš„è½¬æ¢æ¦‚ç‡åˆå§‹æ¦‚ç‡ï¼šæ™´å¤© è®¡ç®—ä»Šå¤©(t=1)çš„å¤©æ°”çŠ¶å†µï¼šä»Šå¤©ä¸ºæ™´å¤©çš„æ¦‚ç‡=åˆå§‹æ™´å¤©æ¦‚ç‡Xæ™´å¤©è½¬æ™´å¤©æ¦‚ç‡ +åˆå§‹å¤šäº‘æ¦‚ç‡Xå¤šäº‘è½¬æ™´å¤©æ¦‚ç‡ +åˆå§‹é›·é›¨æ¦‚ç‡Xé›·é›¨è½¬æ™´å¤©æ¦‚ç‡ã€‚ éšé©¬å°”ç§‘å¤«æ¨¡å‹ å½“å‰çš„çŠ¶æ€åªå’Œå‰ä¸€çŠ¶æ€æœ‰å…³ï¼š æŸä¸ªè§‚æµ‹åªå’Œç”Ÿæˆå®ƒçš„çŠ¶æ€æœ‰å…³ï¼š éšé©¬å°”ç§‘å¤«æ¨¡å‹çš„ç»„æˆ ä¸‰ä¸ªå¿…å¤‡ï¼šåˆå§‹æ¦‚ç‡(Ï€)ï¼Œéšè—çŠ¶æ€è½¬ç§»æ¦‚ç‡çŸ©é˜µ(A)ï¼Œç”Ÿæˆè§‚æµ‹çŠ¶æ€æ¦‚ç‡çŸ©é˜µ(B)ã€‚ éšè—çŠ¶æ€ä¸è§‚å¯ŸçŠ¶æ€ï¼ˆBçŸ©é˜µï¼‰: è¦è§£å†³çš„é—®é¢˜: æ¨¡å‹ä¸º ç»™å®šæ¨¡å‹åŠè§‚æµ‹åºåˆ— è®¡ç®—å…¶å‡ºç°çš„æ¦‚ç‡ ç»™å®šè§‚æµ‹åºåˆ—æ±‚è§£å‚æ•°ä½¿å¾—æœ€å¤§ å·²çŸ¥æ¨¡å‹å’Œè§‚æµ‹åºåˆ—æ±‚çŠ¶æ€åºåˆ—ï¼Œä½¿å¾—æœ€å¤§ æ±‚è§‚æµ‹åºåˆ—çš„æ¦‚ç‡ æš´åŠ›æ±‚è§£ï¼šæˆ‘ä»¬è¦æ±‚çš„æ˜¯åœ¨ç»™å®šæ¨¡å‹ä¸‹è§‚æµ‹åºåˆ—å‡ºç°çš„æ¦‚ç‡ï¼Œé‚£å¦‚æœæˆ‘èƒ½æŠŠæ‰€æœ‰çš„éšè—åºåˆ—éƒ½ç»™åˆ—å‡ºæ¥ï¼Œä¹Ÿå°±å¯ä»¥çŸ¥é“è”åˆæ¦‚ç‡åˆ†å¸ƒ ç°åœ¨è¦æ±‚çš„ç›®æ ‡å°±å¾ˆæ˜ç¡®äº†ã€‚åœ¨ç»™å®šæ¨¡å‹ä¸‹ï¼Œä¸€ä¸ªéšè—åºåˆ—å‡ºç°çš„æ¦‚ç‡ï¼Œé‚£å°±ç”±åˆå§‹çŠ¶æ€æ…¢æ…¢è½¬æ¢å˜›ã€‚å‡ºç°çš„æ¦‚ç‡ä¸ºï¼š å‰å‘ç®—æ³•ç»™å®štæ—¶åˆ»çš„éšè—çŠ¶æ€ä¸ºiï¼Œè§‚æµ‹åºåˆ—ä¸ºo1,o2â€¦otçš„æ¦‚ç‡å«åšå‰å‘æ¦‚ç‡ï¼š","tags":["ç¼–ç¨‹"],"categories":["å…¶ä»–"]},{"title":"GAN","path":"/2019/11/26/20191126-GAN/","content":"å¯¹æŠ—ç”Ÿæˆç½‘ç»œ GAN(Generative Adversarial Nets) Adversarial Nets Framework ç”Ÿæˆå™¨ä¸åˆ¤åˆ«å™¨çŠ¶æ€ç›¸ç­‰ æŸå¤±å‡½æ•°å®ƒåšçš„æ˜¯å»æœ€å¤§åŒ– Dçš„åŒºåˆ†åº¦ï¼Œæœ€å°åŒ–Gå’Œrealæ•°æ®é›†çš„æ•°æ®åˆ†å¸ƒ åˆ¤åˆ«æ¨¡å‹ï¼š D1 å’Œ D2 ç›¸åŒçš„ï¼Œæ˜¯åˆ¤åˆ«å™¨ï¼ŒGæ˜¯ç”Ÿæˆå™¨ ç”Ÿæˆæ¨¡å‹ï¼š å…ˆè®­ç»ƒåˆ¤åˆ«å™¨ï¼Œåœ¨è®­ç»ƒç”Ÿæˆå™¨ã€‚ # -*- coding: utf-8 -*-# @Author: Your name# @Date: 2019-11-26 09:12:52# @Last Modified by: Your name# @Last Modified time: 2019-11-26 09:12:52import argparseimport numpy as npfrom scipy.stats import normimport tensorflow as tfimport matplotlib.pyplot as pltfrom matplotlib import animationimport seaborn as snssns.set(color_codes=True)seed = 42np.random.seed(seed)tf.set_random_seed(seed)class DataDistribution(object): def __init__(self): self.mu = 4 self.sigma = 0.5 def sample(self, N): samples = np.random.normal(self.mu, self.sigma, N) # ç”Ÿæˆé«˜æ–¯åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦éšæœºæ•° å‡å€¼ï¼Œæ ‡å‡†å·® samples.sort() return samplesclass GeneratorDistribution(object): def __init__(self, range): self.range = range def sample(self, N): return np.linspace(-self.range, self.range, N) + \\ np.random.random(N) * 0.01 # ç”Ÿæˆéšæœºæ•°# å¯¹çº¿æ€§ç›¸ä¹˜è¿›è¡Œåˆå§‹åŒ–def linear(input, output_dim, scope=None, stddev=1.0): norm = tf.random_normal_initializer(stddev=stddev) const = tf.constant_initializer(0.0) with tf.variable_scope(scope or 'linear'): # å®šä¹‰å‘½åç©ºé—´ w = tf.get_variable('w', [input.get_shape()[1], output_dim], initializer=norm) b = tf.get_variable('b', [output_dim], initializer=const) return tf.matmul(input, w) + b# ç”Ÿæˆå™¨def generator(input, h_dim): # è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯è®¡ç®—æ¿€æ´»å‡½æ•°softplusï¼Œå³log( exp( features ) + 1) h0 = tf.nn.softplus(linear(input, h_dim, 'g0')) h1 = linear(h0, 1, 'g1') return h1# åˆ¤åˆ«å™¨def discriminator(input, h_dim): h0 = tf.tanh(linear(input, h_dim * 2, 'd0')) h1 = tf.tanh(linear(h0, h_dim * 2, 'd1')) h2 = tf.tanh(linear(h1, h_dim * 2, scope='d2')) h3 = tf.sigmoid(linear(h2, 1, scope='d3')) return h3# ä¼˜åŒ–å™¨def optimizer(loss, var_list, initial_learning_rate): decay = 0.95 num_decay_steps = 150 batch = tf.Variable(0) learning_rate = tf.train.exponential_decay( initial_learning_rate, batch, num_decay_steps, decay, staircase=True ) optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize( loss, global_step=batch, var_list=var_list ) return optimizerclass GAN(object): def __init__(self, data, gen, num_steps, batch_size, log_every): self.data = data self.gen = gen self.num_steps = num_steps self.batch_size = batch_size self.log_every = log_every self.mlp_hidden_size = 4 self.learning_rate = 0.03 self._create_model() def _create_model(self): with tf.variable_scope('D_pre'): self.pre_input = tf.placeholder(tf.float32, shape=(self.batch_size, 1)) self.pre_labels = tf.placeholder(tf.float32, shape=(self.batch_size, 1)) D_pre = discriminator(self.pre_input, self.mlp_hidden_size) self.pre_loss = tf.reduce_mean(tf.square(D_pre - self.pre_labels)) self.pre_opt = optimizer(self.pre_loss, None, self.learning_rate) # This defines the generator network - it takes samples from a noise # distribution as input, and passes them through an MLP. with tf.variable_scope('Gen'): self.z = tf.placeholder(tf.float32, shape=(self.batch_size, 1)) self.G = generator(self.z, self.mlp_hidden_size) # The discriminator tries to tell the difference between samples from the # true data distribution (self.x) and the generated samples (self.z). # # Here we create two copies of the discriminator network (that share parameters), # as you cannot use the same network with different inputs in TensorFlow. with tf.variable_scope('Disc') as scope: self.x = tf.placeholder(tf.float32, shape=(self.batch_size, 1)) self.D1 = discriminator(self.x, self.mlp_hidden_size) scope.reuse_variables() self.D2 = discriminator(self.G, self.mlp_hidden_size) # Define the loss for discriminator and generator networks (see the original # paper for details), and create optimizers for both self.loss_d = tf.reduce_mean(-tf.log(self.D1) - tf.log(1 - self.D2)) self.loss_g = tf.reduce_mean(-tf.log(self.D2)) self.d_pre_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='D_pre') self.d_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Disc') self.g_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Gen') self.opt_d = optimizer(self.loss_d, self.d_params, self.learning_rate) self.opt_g = optimizer(self.loss_g, self.g_params, self.learning_rate) def train(self): with tf.Session() as session: tf.global_variables_initializer().run() # pretraining discriminator num_pretrain_steps = 1000 for step in range(num_pretrain_steps): d = (np.random.random(self.batch_size) - 0.5) * 10.0 labels = norm.pdf(d, loc=self.data.mu, scale=self.data.sigma) # norm.pdf:æ­£æ€æ¦‚ç‡å¯†åº¦å‡½æ•° pretrain_loss, _ = session.run([self.pre_loss, self.pre_opt], { self.pre_input: np.reshape(d, (self.batch_size, 1)), self.pre_labels: np.reshape(labels, (self.batch_size, 1)) }) self.weightsD = session.run(self.d_pre_params) # copy weights from pre-training over to new D network for i, v in enumerate(self.d_params): session.run(v.assign(self.weightsD[i])) for step in range(self.num_steps): # update discriminator x = self.data.sample(self.batch_size) z = self.gen.sample(self.batch_size) loss_d, _ = session.run([self.loss_d, self.opt_d], { self.x: np.reshape(x, (self.batch_size, 1)), self.z: np.reshape(z, (self.batch_size, 1)) }) # update generator z = self.gen.sample(self.batch_size) loss_g, _ = session.run([self.loss_g, self.opt_g], { self.z: np.reshape(z, (self.batch_size, 1)) }) if step % self.log_every == 0: print('{}: {}\\t{}'.format(step, loss_d, loss_g)) if step % 100 == 0 or step==0 or step == self.num_steps -1 : self._plot_distributions(session) def _samples(self, session, num_points=10000, num_bins=100): xs = np.linspace(-self.gen.range, self.gen.range, num_points) bins = np.linspace(-self.gen.range, self.gen.range, num_bins) # data distribution d = self.data.sample(num_points) pd, _ = np.histogram(d, bins=bins, density=True) # generated samples zs = np.linspace(-self.gen.range, self.gen.range, num_points) g = np.zeros((num_points, 1)) for i in range(num_points // self.batch_size): g[self.batch_size * i:self.batch_size * (i + 1)] = session.run(self.G, { self.z: np.reshape( zs[self.batch_size * i:self.batch_size * (i + 1)], (self.batch_size, 1) ) }) pg, _ = np.histogram(g, bins=bins, density=True) return pd, pg def _plot_distributions(self, session): pd, pg = self._samples(session) p_x = np.linspace(-self.gen.range, self.gen.range, len(pd)) f, ax = plt.subplots(1) ax.set_ylim(0, 1) plt.plot(p_x, pd, label='real data') plt.plot(p_x, pg, label='generated data') plt.title('1D Generative Adversarial Network') plt.xlabel('Data values') plt.ylabel('Probability density') plt.legend() plt.show()def main(args): model = GAN( DataDistribution(), GeneratorDistribution(range=8), args.num_steps, args.batch_size, args.log_every, ) model.train()def parse_args(): parser = argparse.ArgumentParser() parser.add_argument('--num-steps', type=int, default=12000, help='the number of training steps to take') parser.add_argument('--batch-size', type=int, default=12, help='the batch size') parser.add_argument('--log-every', type=int, default=10, help='print loss after this many steps') return parser.parse_args()if __name__ == '__main__': main(parse_args()) DCGAN å°†poolingå±‚convolutionsæ›¿ä»£ å¯¹äºåˆ¤åˆ«æ¨¡å‹ï¼šå®¹è®¸ç½‘ç»œå­¦ä¹ è‡ªå·±çš„ç©ºé—´ä¸‹é‡‡æ · å¯¹äºç”Ÿæˆæ¨¡å‹ï¼šå®¹è®¸å®ƒå­¦ä¹ è‡ªå·±çš„ç©ºé—´ä¸Šé‡‡æ · åœ¨generatorå’Œdiscriminatorä¸Šéƒ½ä½¿ç”¨batchnorm è§£å†³åˆå§‹åŒ–å·®çš„é—®é¢˜ å¸®åŠ©æ¢¯åº¦ä¼ æ’­åˆ°æ¯ä¸€å±‚ é˜²æ­¢generatoræŠŠæ‰€æœ‰çš„æ ·æœ¬éƒ½æ”¶æ•›åˆ°åŒä¸€ä¸ªç‚¹ã€‚ åœ¨CNNä¸­ç§»é™¤å…¨è¿æ¥å±‚ åœ¨generatorçš„é™¤äº†è¾“å‡ºå±‚å¤–çš„æ‰€æœ‰å±‚ä½¿ç”¨ReLUï¼Œè¾“å‡ºå±‚é‡‡ç”¨tanhã€‚ åœ¨discriminatorçš„æ‰€æœ‰å±‚ä¸Šä½¿ç”¨LeakyReLU 100ç»´çš„å‘é‡è½¬ä¸ºä¸ºç‰¹å¾å›¾ç›¸ä¼¼çš„ä¸œè¥¿ï¼Œ å†å°†è¿™ä¸ªå‘é‡reshape ã€‚ä½¿ç”¨åå·ç§¯æ“ä½œã€‚ è¾“å…¥å›¾ç‰‡ï¼Œå¾—åˆ°ä¸€ä¸ªå€¼æ˜¯0æˆ–è€…1ï¼Œè¿™ä¸ªæ˜¯åˆ¤åˆ«ç½‘ç»œ è¿™ä¸ªæ˜¯ç”Ÿæˆç½‘ç»œã€‚model.py from __future__ import divisionimport osimport timeimport mathfrom glob import globimport tensorflow as tfimport numpy as npfrom six.moves import xrangefrom ops import *from utils import *def conv_out_size_same(size, stride): return math.ceil(float(size) / float(stride))class DCGAN(object): def __init__(self, sess, input_height=108, input_width=108, is_crop=True, batch_size=64, sample_num = 64, output_height=64, output_width=64, y_dim=None, z_dim=100, gf_dim=64, df_dim=64, gfc_dim=1024, dfc_dim=1024, c_dim=3, dataset_name='default', input_fname_pattern='*.jpg', checkpoint_dir=None, sample_dir=None): \"\"\" # sample number æµ‹è¯•å™ªéŸ³çš„è¾“å‡ºï¼Œyä»£è¡¨label Args: sess: TensorFlow session batch_size: The size of batch. Should be specified before training. y_dim: (optional) Dimension of dim for y. [None] z_dim: (optional) Dimension of dim for Z. [100] gf_dim: (optional) Dimension of gen filters in first conv layer. [64] df_dim: (optional) Dimension of discrim filters in first conv layer. [64] gfc_dim: (optional) Dimension of gen units for for fully connected layer. [1024] dfc_dim: (optional) Dimension of discrim units for fully connected layer. [1024] c_dim: (optional) Dimension of image color. For grayscale input, set to 1. [3] \"\"\" self.sess = sess self.is_crop = is_crop self.is_grayscale = (c_dim == 1) self.batch_size = batch_size self.sample_num = sample_num self.input_height = input_height self.input_width = input_width self.output_height = output_height self.output_width = output_width self.y_dim = y_dim # null self.z_dim = z_dim # å™ªéŸ³ç‚¹çš„ç»´åº¦ 100 self.gf_dim = gf_dim # æœ€ç»ˆå¤šå°‘ä¸ªfilterçš„ä¸ªæ•° åŸºæ•° self.df_dim = df_dim # 64 self.gfc_dim = gfc_dim# ç”Ÿæˆå’Œåˆ¤åˆ«çš„å…¨è¿æ¥ 1024 self.dfc_dim = dfc_dim # 1024 self.c_dim = c_dim# ç”Ÿæˆçš„æ˜¯å½©è‰²å›¾ 3 # batch normalization : deals with poor initialization helps gradient flow self.d_bn1 = batch_norm(name='d_bn1')# bacthåœ¨reluä¹‹å‰å·ç§¯ä¹‹å self.d_bn2 = batch_norm(name='d_bn2') if not self.y_dim: self.d_bn3 = batch_norm(name='d_bn3') self.g_bn0 = batch_norm(name='g_bn0') self.g_bn1 = batch_norm(name='g_bn1') self.g_bn2 = batch_norm(name='g_bn2') if not self.y_dim: self.g_bn3 = batch_norm(name='g_bn3') self.dataset_name = dataset_name self.input_fname_pattern = input_fname_pattern self.checkpoint_dir = checkpoint_dir self.build_model() def build_model(self): if self.y_dim: self.y= tf.placeholder(tf.float32, [self.batch_size, self.y_dim], name='y') if self.is_crop: image_dims = [self.output_height, self.output_width, self.c_dim] else: image_dims = [self.input_height, self.input_height, self.c_dim] self.inputs = tf.placeholder( tf.float32, [self.batch_size] + image_dims, name='real_images') self.sample_inputs = tf.placeholder( # 64 108 108 3ï¼Œiamge_dim 108 108 3 tf.float32, [self.sample_num] + image_dims, name='sample_inputs') inputs = self.inputs # 64 108 108 3 sample_inputs = self.sample_inputs self.z = tf.placeholder( tf.float32, [None, self.z_dim], name='z') ## ç”Ÿæˆç½‘ç»œç»„æœ€å¼€å§‹çš„è¾“å…¥ï¼Œfloat32 # Bï¼Œ 100 self.z_sum = histogram_summary(\"z\", self.z) # åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œå½“éœ€è¦æŸ¥çœ‹ä¸€ä¸ªå¼ é‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å€¼çš„åˆ†å¸ƒæƒ…å†µæ—¶ï¼Œå¯é€šè¿‡tf.summary.histogram()å°†å…¶åˆ†å¸ƒæƒ…å†µä»¥ç›´æ–¹å›¾çš„å½¢å¼åœ¨TensorBoardç›´æ–¹å›¾ä»ªè¡¨æ¿ä¸Šæ˜¾ç¤ºï¼ if self.y_dim: self.G = self.generator(self.z, self.y) self.D, self.D_logits = \\ self.discriminator(inputs, self.y, reuse=False) self.sampler = self.sampler(self.z, self.y) self.D_, self.D_logits_ = \\ self.discriminator(self.G, self.y, reuse=True) else: self.G = self.generator(self.z) # 64 64 64 3 self.D, self.D_logits = self.discriminator(inputs) # 64 108 108 3 self.sampler = self.sampler(self.z) self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True) self.d_sum = histogram_summary(\"d\", self.D) self.d__sum = histogram_summary(\"d_\", self.D_) self.G_sum = image_summary(\"G\", self.G) # tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits,l) self.d_loss_real = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( logits=self.D_logits, labels=tf.ones_like(self.D))) self.d_loss_fake = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( logits=self.D_logits_, labels=tf.zeros_like(self.D_))) self.g_loss = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( logits=self.D_logits_, labels=tf.ones_like(self.D_))) self.d_loss_real_sum = scalar_summary(\"d_loss_real\", self.d_loss_real) self.d_loss_fake_sum = scalar_summary(\"d_loss_fake\", self.d_loss_fake) self.d_loss = self.d_loss_real + self.d_loss_fake self.g_loss_sum = scalar_summary(\"g_loss\", self.g_loss) self.d_loss_sum = scalar_summary(\"d_loss\", self.d_loss) t_vars = tf.trainable_variables() self.d_vars = [var for var in t_vars if 'd_' in var.name] self.g_vars = [var for var in t_vars if 'g_' in var.name] self.saver = tf.train.Saver() def train(self, config): \"\"\"Train DCGAN\"\"\" if config.dataset == 'mnist': data_X, data_y = self.load_mnist() else: data = glob(os.path.join(\"./data\", config.dataset, self.input_fname_pattern)) #np.random.shuffle(data) d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\ .minimize(self.d_loss, var_list=self.d_vars) g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\ .minimize(self.g_loss, var_list=self.g_vars) try: tf.global_variables_initializer().run() except: tf.initialize_all_variables().run() self.g_sum = merge_summary([self.z_sum, self.d__sum, self.G_sum, self.d_loss_fake_sum, self.g_loss_sum]) self.d_sum = merge_summary( [self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum]) self.writer = SummaryWriter(\"./logs\", self.sess.graph) sample_z = np.random.uniform(-1, 1, size=(self.sample_num , self.z_dim)) if config.dataset == 'mnist': sample_inputs = data_X[0:self.sample_num] sample_labels = data_y[0:self.sample_num] else: sample_files = data[0:self.sample_num] sample = [ get_image(sample_file, input_height=self.input_height, input_width=self.input_width, resize_height=self.output_height, resize_width=self.output_width, is_crop=self.is_crop, is_grayscale=self.is_grayscale) for sample_file in sample_files] if (self.is_grayscale): sample_inputs = np.array(sample).astype(np.float32)[:, :, :, None] else: sample_inputs = np.array(sample).astype(np.float32) counter = 1 start_time = time.time() if self.load(self.checkpoint_dir): print(\" [*] Load SUCCESS\") else: print(\" [!] Load failed...\") for epoch in xrange(config.epoch): if config.dataset == 'mnist': batch_idxs = min(len(data_X), config.train_size) // config.batch_size else: data = glob(os.path.join( \"./data\", config.dataset, self.input_fname_pattern)) batch_idxs = min(len(data), config.train_size) // config.batch_size for idx in xrange(0, batch_idxs): if config.dataset == 'mnist': batch_images = data_X[idx*config.batch_size:(idx+1)*config.batch_size] batch_labels = data_y[idx*config.batch_size:(idx+1)*config.batch_size] else: batch_files = data[idx*config.batch_size:(idx+1)*config.batch_size] batch = [ get_image(batch_file, input_height=self.input_height, input_width=self.input_width, resize_height=self.output_height, resize_width=self.output_width, is_crop=self.is_crop, is_grayscale=self.is_grayscale) for batch_file in batch_files] if (self.is_grayscale): batch_images = np.array(batch).astype(np.float32)[:, :, :, None] else: batch_images = np.array(batch).astype(np.float32) # ä¸€ä¸ªå‡åŒ€åˆ†å¸ƒ[low,high)ä¸­éšæœºé‡‡æ · ä»+1å’Œ-1ä¹‹é—´éšæ‰é‡‡æ · batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \\ .astype(np.float32) if config.dataset == 'mnist': # Update D network _, summary_str = self.sess.run([d_optim, self.d_sum], feed_dict={ self.inputs: batch_images, self.z: batch_z, self.y:batch_labels, }) self.writer.add_summary(summary_str, counter) # Update G network _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z, self.y:batch_labels, }) self.writer.add_summary(summary_str, counter) # Run g_optim twice to make sure that d_loss does not go to zero (different from paper) _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z, self.y:batch_labels }) self.writer.add_summary(summary_str, counter) errD_fake = self.d_loss_fake.eval({ self.z: batch_z, self.y:batch_labels }) errD_real = self.d_loss_real.eval({ self.inputs: batch_images, self.y:batch_labels }) errG = self.g_loss.eval({ self.z: batch_z, self.y: batch_labels }) else: # Update D network _, summary_str = self.sess.run([d_optim, self.d_sum], feed_dict={ self.inputs: batch_images, self.z: batch_z }) self.writer.add_summary(summary_str, counter) # Update G network _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z }) self.writer.add_summary(summary_str, counter) # Run g_optim twice to make sure that d_loss does not go to zero (different from paper) _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z }) self.writer.add_summary(summary_str, counter) errD_fake = self.d_loss_fake.eval({ self.z: batch_z }) errD_real = self.d_loss_real.eval({ self.inputs: batch_images }) errG = self.g_loss.eval({self.z: batch_z}) counter += 1 print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f\" \\ % (epoch, idx, batch_idxs, time.time() - start_time, errD_fake+errD_real, errG)) if np.mod(counter, 100) == 1: if config.dataset == 'mnist': samples, d_loss, g_loss = self.sess.run( [self.sampler, self.d_loss, self.g_loss], feed_dict={ self.z: sample_z, self.inputs: sample_inputs, self.y:sample_labels, } ) save_images(samples, [8, 8], './{}/train_{:02d}_{:04d}.png'.format(config.sample_dir, epoch, idx)) print(\"[Sample] d_loss: %.8f, g_loss: %.8f\" % (d_loss, g_loss)) else: try: samples, d_loss, g_loss = self.sess.run( [self.sampler, self.d_loss, self.g_loss], feed_dict={ self.z: sample_z, self.inputs: sample_inputs, }, ) save_images(samples, [8, 8], './{}/train_{:02d}_{:04d}.png'.format(config.sample_dir, epoch, idx)) print(\"[Sample] d_loss: %.8f, g_loss: %.8f\" % (d_loss, g_loss)) except: print(\"one pic error!...\") if counter//10 == 2: self.save(config.checkpoint_dir, counter) def discriminator(self, image, y=None, reuse=False): with tf.variable_scope(\"discriminator\") as scope: if reuse: scope.reuse_variables() if not self.y_dim: h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv')) h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name='d_h1_conv'))) h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name='d_h2_conv'))) h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name='d_h3_conv'))) # 64 14 14 256 -64 7 7 512 aa = tf.reshape(h3, [self.batch_size, -1]) h4 = linear(aa, 1, 'd_h3_lin') return tf.nn.sigmoid(h4), h4 else: yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) x = conv_cond_concat(image, yb) h0 = lrelu(conv2d(x, self.c_dim + self.y_dim, name='d_h0_conv')) h0 = conv_cond_concat(h0, yb) h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim + self.y_dim, name='d_h1_conv'))) h1 = tf.reshape(h1, [self.batch_size, -1]) h1 = concat([h1, y], 1) h2 = lrelu(self.d_bn2(linear(h1, self.dfc_dim, 'd_h2_lin'))) h2 = concat([h2, y], 1) h3 = linear(h2, 1, 'd_h3_lin') return tf.nn.sigmoid(h3), h3 def generator(self, z, y=None): with tf.variable_scope(\"generator\") as scope: if not self.y_dim: s_h, s_w = self.output_height, self.output_width s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2) # å…ˆæŠŠç‰¹å¾å›¾å¤§å°ç¡®å®šå‡ºæ¥ s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2) s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2) s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2) # project `z` and reshape self.z_, self.h0_w, self.h0_b = linear( z, self.gf_dim*8*s_h16*s_w16, 'g_h0_lin', with_w=True) self.h0 = tf.reshape( self.z_, [-1, s_h16, s_w16, self.gf_dim * 8]) h0 = tf.nn.relu(self.g_bn0(self.h0)) self.h1, self.h1_w, self.h1_b = deconv2d( h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name='g_h1', with_w=True) h1 = tf.nn.relu(self.g_bn1(self.h1)) h2, self.h2_w, self.h2_b = deconv2d( h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name='g_h2', with_w=True) h2 = tf.nn.relu(self.g_bn2(h2)) h3, self.h3_w, self.h3_b = deconv2d( h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name='g_h3', with_w=True) h3 = tf.nn.relu(self.g_bn3(h3)) h4, self.h4_w, self.h4_b = deconv2d( h3, [self.batch_size, s_h, s_w, self.c_dim], name='g_h4', with_w=True) return tf.nn.tanh(h4) else: s_h, s_w = self.output_height, self.output_width s_h2, s_h4 = int(s_h/2), int(s_h/4) s_w2, s_w4 = int(s_w/2), int(s_w/4) # yb = tf.expand_dims(tf.expand_dims(y, 1),2) yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) z = concat([z, y], 1) h0 = tf.nn.relu( self.g_bn0(linear(z, self.gfc_dim, 'g_h0_lin'))) h0 = concat([h0, y], 1) h1 = tf.nn.relu(self.g_bn1( linear(h0, self.gf_dim*2*s_h4*s_w4, 'g_h1_lin'))) h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2]) h1 = conv_cond_concat(h1, yb) h2 = tf.nn.relu(self.g_bn2(deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name='g_h2'))) h2 = conv_cond_concat(h2, yb) return tf.nn.sigmoid( deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name='g_h3')) def sampler(self, z, y=None): with tf.variable_scope(\"generator\") as scope: scope.reuse_variables() if not self.y_dim: s_h, s_w = self.output_height, self.output_width s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2) s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2) s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2) s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2) # project `z` and reshape h0 = tf.reshape( linear(z, self.gf_dim*8*s_h16*s_w16, 'g_h0_lin'), [-1, s_h16, s_w16, self.gf_dim * 8]) h0 = tf.nn.relu(self.g_bn0(h0, train=False)) h1 = deconv2d(h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name='g_h1') h1 = tf.nn.relu(self.g_bn1(h1, train=False)) h2 = deconv2d(h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name='g_h2') h2 = tf.nn.relu(self.g_bn2(h2, train=False)) h3 = deconv2d(h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name='g_h3') h3 = tf.nn.relu(self.g_bn3(h3, train=False)) h4 = deconv2d(h3, [self.batch_size, s_h, s_w, self.c_dim], name='g_h4') return tf.nn.tanh(h4) else: s_h, s_w = self.output_height, self.output_width s_h2, s_h4 = int(s_h/2), int(s_h/4) s_w2, s_w4 = int(s_w/2), int(s_w/4) # yb = tf.reshape(y, [-1, 1, 1, self.y_dim]) yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) z = concat([z, y], 1) h0 = tf.nn.relu(self.g_bn0(linear(z, self.gfc_dim, 'g_h0_lin'))) h0 = concat([h0, y], 1) h1 = tf.nn.relu(self.g_bn1( linear(h0, self.gf_dim*2*s_h4*s_w4, 'g_h1_lin'), train=False)) h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2]) h1 = conv_cond_concat(h1, yb) h2 = tf.nn.relu(self.g_bn2( deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name='g_h2'), train=False)) h2 = conv_cond_concat(h2, yb) return tf.nn.sigmoid(deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name='g_h3')) def load_mnist(self): data_dir = os.path.join(\"./data\", self.dataset_name) fd = open(os.path.join(data_dir,'train-images-idx3-ubyte')) loaded = np.fromfile(file=fd,dtype=np.uint8) trX = loaded[16:].reshape((60000,28,28,1)).astype(np.float) fd = open(os.path.join(data_dir,'train-labels-idx1-ubyte')) loaded = np.fromfile(file=fd,dtype=np.uint8) trY = loaded[8:].reshape((60000)).astype(np.float) fd = open(os.path.join(data_dir,'t10k-images-idx3-ubyte')) loaded = np.fromfile(file=fd,dtype=np.uint8) teX = loaded[16:].reshape((10000,28,28,1)).astype(np.float) fd = open(os.path.join(data_dir,'t10k-labels-idx1-ubyte')) loaded = np.fromfile(file=fd,dtype=np.uint8) teY = loaded[8:].reshape((10000)).astype(np.float) trY = np.asarray(trY) teY = np.asarray(teY) X = np.concatenate((trX, teX), axis=0) y = np.concatenate((trY, teY), axis=0).astype(np.int) seed = 547 np.random.seed(seed) np.random.shuffle(X) np.random.seed(seed) np.random.shuffle(y) y_vec = np.zeros((len(y), self.y_dim), dtype=np.float) for i, label in enumerate(y): y_vec[i,y[i]] = 1.0 return X/255.,y_vec @property def model_dir(self): return \"{}_{}_{}_{}\".format( self.dataset_name, self.batch_size, self.output_height, self.output_width) def save(self, checkpoint_dir, step): model_name = \"DCGAN.model\" checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir) if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir) self.saver.save(self.sess, os.path.join(checkpoint_dir, model_name), global_step=step) def load(self, checkpoint_dir): print(\" [*] Reading checkpoints...\") checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir) ckpt = tf.train.get_checkpoint_state(checkpoint_dir) if ckpt and ckpt.model_checkpoint_path: ckpt_name = os.path.basename(ckpt.model_checkpoint_path) self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name)) print(\" [*] Success to read {}\".format(ckpt_name)) return True else: print(\" [*] Failed to find a checkpoint\") return False","tags":["æ·±åº¦å­¦ä¹ "],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"GAN","path":"/2019/11/26/GAN/","content":"å¯¹æŠ—ç”Ÿæˆç½‘ç»œ GAN(Generative Adversarial Nets) Adversarial Nets Framework ç”Ÿæˆå™¨ä¸åˆ¤åˆ«å™¨çŠ¶æ€ç›¸ç­‰ æŸå¤±å‡½æ•°å®ƒåšçš„æ˜¯å»æœ€å¤§åŒ– Dçš„åŒºåˆ†åº¦ï¼Œæœ€å°åŒ–Gå’Œrealæ•°æ®é›†çš„æ•°æ®åˆ†å¸ƒ åˆ¤åˆ«æ¨¡å‹ï¼š D1 å’Œ D2 ç›¸åŒçš„ï¼Œæ˜¯åˆ¤åˆ«å™¨ï¼ŒGæ˜¯ç”Ÿæˆå™¨ ç”Ÿæˆæ¨¡å‹ï¼š å…ˆè®­ç»ƒåˆ¤åˆ«å™¨ï¼Œåœ¨è®­ç»ƒç”Ÿæˆå™¨ã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 | # -*- coding: utf-8 -*- # @Author: Your name # @Date: 2019-11-26 09:12:52 # @Last Modified by: Your name # @Last Modified time: 2019-11-26 09:12:52 import argparse import numpy as np from scipy.stats import norm import tensorflow as tf import matplotlib.pyplot as plt from matplotlib import animation import seaborn as sns sns.set(color_codes=True) seed = 42 np.random.seed(seed) tf.set_random_seed(seed) class DataDistribution(object): def __init__(self): self.mu = 4 self.sigma = 0.5 def sample(self, N): samples = np.random.normal(self.mu, self.sigma, N) # ç”Ÿæˆé«˜æ–¯åˆ†å¸ƒçš„æ¦‚ç‡å¯†åº¦éšæœºæ•° å‡å€¼ï¼Œæ ‡å‡†å·® samples.sort() return samples class GeneratorDistribution(object): def __init__(self, range): self.range = range def sample(self, N): return np.linspace(-self.range, self.range, N) + \\ np.random.random(N) * 0.01 # ç”Ÿæˆéšæœºæ•° # å¯¹çº¿æ€§ç›¸ä¹˜è¿›è¡Œåˆå§‹åŒ– def linear(input, output_dim, scope=None, stddev=1.0): norm = tf.random_normal_initializer(stddev=stddev) const = tf.constant_initializer(0.0) with tf.variable_scope(scope or 'linear'): # å®šä¹‰å‘½åç©ºé—´ w = tf.get_variable('w', [input.get_shape()[1], output_dim], initializer=norm) b = tf.get_variable('b', [output_dim], initializer=const) return tf.matmul(input, w) + b # ç”Ÿæˆå™¨ def generator(input, h_dim): # è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯è®¡ç®—æ¿€æ´»å‡½æ•°softplusï¼Œå³log( exp( features ) + 1) h0 = tf.nn.softplus(linear(input, h_dim, 'g0')) h1 = linear(h0, 1, 'g1') return h1 # åˆ¤åˆ«å™¨ def discriminator(input, h_dim): h0 = tf.tanh(linear(input, h_dim * 2, 'd0')) h1 = tf.tanh(linear(h0, h_dim * 2, 'd1')) h2 = tf.tanh(linear(h1, h_dim * 2, scope='d2')) h3 = tf.sigmoid(linear(h2, 1, scope='d3')) return h3 # ä¼˜åŒ–å™¨ def optimizer(loss, var_list, initial_learning_rate): decay = 0.95 num_decay_steps = 150 batch = tf.Variable(0) learning_rate = tf.train.exponential_decay( initial_learning_rate, batch, num_decay_steps, decay, staircase=True ) optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize( loss, global_step=batch, var_list=var_list ) return optimizer class GAN(object): def __init__(self, data, gen, num_steps, batch_size, log_every): self.data = data self.gen = gen self.num_steps = num_steps self.batch_size = batch_size self.log_every = log_every self.mlp_hidden_size = 4 self.learning_rate = 0.03 self._create_model() def _create_model(self): with tf.variable_scope('D_pre'): self.pre_input = tf.placeholder(tf.float32, shape=(self.batch_size, 1)) self.pre_labels = tf.placeholder(tf.float32, shape=(self.batch_size, 1)) D_pre = discriminator(self.pre_input, self.mlp_hidden_size) self.pre_loss = tf.reduce_mean(tf.square(D_pre - self.pre_labels)) self.pre_opt = optimizer(self.pre_loss, None, self.learning_rate) # This defines the generator network - it takes samples from a noise # distribution as input, and passes them through an MLP. with tf.variable_scope('Gen'): self.z = tf.placeholder(tf.float32, shape=(self.batch_size, 1)) self.G = generator(self.z, self.mlp_hidden_size) # The discriminator tries to tell the difference between samples from the # true data distribution (self.x) and the generated samples (self.z). # # Here we create two copies of the discriminator network (that share parameters), # as you cannot use the same network with different inputs in TensorFlow. with tf.variable_scope('Disc') as scope: self.x = tf.placeholder(tf.float32, shape=(self.batch_size, 1)) self.D1 = discriminator(self.x, self.mlp_hidden_size) scope.reuse_variables() self.D2 = discriminator(self.G, self.mlp_hidden_size) # Define the loss for discriminator and generator networks (see the original # paper for details), and create optimizers for both self.loss_d = tf.reduce_mean(-tf.log(self.D1) - tf.log(1 - self.D2)) self.loss_g = tf.reduce_mean(-tf.log(self.D2)) self.d_pre_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='D_pre') self.d_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Disc') self.g_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Gen') self.opt_d = optimizer(self.loss_d, self.d_params, self.learning_rate) self.opt_g = optimizer(self.loss_g, self.g_params, self.learning_rate) def train(self): with tf.Session() as session: tf.global_variables_initializer().run() # pretraining discriminator num_pretrain_steps = 1000 for step in range(num_pretrain_steps): d = (np.random.random(self.batch_size) - 0.5) * 10.0 labels = norm.pdf(d, loc=self.data.mu, scale=self.data.sigma) # norm.pdf:æ­£æ€æ¦‚ç‡å¯†åº¦å‡½æ•° pretrain_loss, _ = session.run([self.pre_loss, self.pre_opt], { self.pre_input: np.reshape(d, (self.batch_size, 1)), self.pre_labels: np.reshape(labels, (self.batch_size, 1)) }) self.weightsD = session.run(self.d_pre_params) # copy weights from pre-training over to new D network for i, v in enumerate(self.d_params): session.run(v.assign(self.weightsD[i])) for step in range(self.num_steps): # update discriminator x = self.data.sample(self.batch_size) z = self.gen.sample(self.batch_size) loss_d, _ = session.run([self.loss_d, self.opt_d], { self.x: np.reshape(x, (self.batch_size, 1)), self.z: np.reshape(z, (self.batch_size, 1)) }) # update generator z = self.gen.sample(self.batch_size) loss_g, _ = session.run([self.loss_g, self.opt_g], { self.z: np.reshape(z, (self.batch_size, 1)) }) if step % self.log_every == 0: print('{}: {}\\t{}'.format(step, loss_d, loss_g)) if step % 100 == 0 or step==0 or step == self.num_steps -1 : self._plot_distributions(session) def _samples(self, session, num_points=10000, num_bins=100): xs = np.linspace(-self.gen.range, self.gen.range, num_points) bins = np.linspace(-self.gen.range, self.gen.range, num_bins) # data distribution d = self.data.sample(num_points) pd, _ = np.histogram(d, bins=bins, density=True) # generated samples zs = np.linspace(-self.gen.range, self.gen.range, num_points) g = np.zeros((num_points, 1)) for i in range(num_points // self.batch_size): g[self.batch_size * i:self.batch_size * (i + 1)] = session.run(self.G, { self.z: np.reshape( zs[self.batch_size * i:self.batch_size * (i + 1)], (self.batch_size, 1) ) }) pg, _ = np.histogram(g, bins=bins, density=True) return pd, pg def _plot_distributions(self, session): pd, pg = self._samples(session) p_x = np.linspace(-self.gen.range, self.gen.range, len(pd)) f, ax = plt.subplots(1) ax.set_ylim(0, 1) plt.plot(p_x, pd, label='real data') plt.plot(p_x, pg, label='generated data') plt.title('1D Generative Adversarial Network') plt.xlabel('Data values') plt.ylabel('Probability density') plt.legend() plt.show() def main(args): model = GAN( DataDistribution(), GeneratorDistribution(range=8), args.num_steps, args.batch_size, args.log_every, ) model.train() def parse_args(): parser = argparse.ArgumentParser() parser.add_argument('--num-steps', type=int, default=12000, help='the number of training steps to take') parser.add_argument('--batch-size', type=int, default=12, help='the batch size') parser.add_argument('--log-every', type=int, default=10, help='print loss after this many steps') return parser.parse_args() if __name__ == '__main__': main(parse_args()) â€”|â€” DCGAN å°†poolingå±‚convolutionsæ›¿ä»£ å¯¹äºåˆ¤åˆ«æ¨¡å‹ï¼šå®¹è®¸ç½‘ç»œå­¦ä¹ è‡ªå·±çš„ç©ºé—´ä¸‹é‡‡æ · å¯¹äºç”Ÿæˆæ¨¡å‹ï¼šå®¹è®¸å®ƒå­¦ä¹ è‡ªå·±çš„ç©ºé—´ä¸Šé‡‡æ · åœ¨generatorå’Œdiscriminatorä¸Šéƒ½ä½¿ç”¨batchnorm è§£å†³åˆå§‹åŒ–å·®çš„é—®é¢˜ å¸®åŠ©æ¢¯åº¦ä¼ æ’­åˆ°æ¯ä¸€å±‚ é˜²æ­¢generatoræŠŠæ‰€æœ‰çš„æ ·æœ¬éƒ½æ”¶æ•›åˆ°åŒä¸€ä¸ªç‚¹ã€‚ åœ¨CNNä¸­ç§»é™¤å…¨è¿æ¥å±‚ åœ¨generatorçš„é™¤äº†è¾“å‡ºå±‚å¤–çš„æ‰€æœ‰å±‚ä½¿ç”¨ReLUï¼Œè¾“å‡ºå±‚é‡‡ç”¨tanhã€‚ åœ¨discriminatorçš„æ‰€æœ‰å±‚ä¸Šä½¿ç”¨LeakyReLU 100ç»´çš„å‘é‡è½¬ä¸ºä¸ºç‰¹å¾å›¾ç›¸ä¼¼çš„ä¸œè¥¿ï¼Œ å†å°†è¿™ä¸ªå‘é‡reshape ã€‚ä½¿ç”¨åå·ç§¯æ“ä½œã€‚ è¾“å…¥å›¾ç‰‡ï¼Œå¾—åˆ°ä¸€ä¸ªå€¼æ˜¯0æˆ–è€…1ï¼Œè¿™ä¸ªæ˜¯åˆ¤åˆ«ç½‘ç»œ è¿™ä¸ªæ˜¯ç”Ÿæˆç½‘ç»œã€‚model.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 | from __future__ import division import os import time import math from glob import glob import tensorflow as tf import numpy as np from six.moves import xrange from ops import * from utils import * def conv_out_size_same(size, stride): return math.ceil(float(size) / float(stride)) class DCGAN(object): def __init__(self, sess, input_height=108, input_width=108, is_crop=True, batch_size=64, sample_num = 64, output_height=64, output_width=64, y_dim=None, z_dim=100, gf_dim=64, df_dim=64, gfc_dim=1024, dfc_dim=1024, c_dim=3, dataset_name='default', input_fname_pattern='*.jpg', checkpoint_dir=None, sample_dir=None): \"\"\" # sample number æµ‹è¯•å™ªéŸ³çš„è¾“å‡ºï¼Œyä»£è¡¨label Args: sess: TensorFlow session batch_size: The size of batch. Should be specified before training. y_dim: (optional) Dimension of dim for y. [None] z_dim: (optional) Dimension of dim for Z. [100] gf_dim: (optional) Dimension of gen filters in first conv layer. [64] df_dim: (optional) Dimension of discrim filters in first conv layer. [64] gfc_dim: (optional) Dimension of gen units for for fully connected layer. [1024] dfc_dim: (optional) Dimension of discrim units for fully connected layer. [1024] c_dim: (optional) Dimension of image color. For grayscale input, set to 1. [3] \"\"\" self.sess = sess self.is_crop = is_crop self.is_grayscale = (c_dim == 1) self.batch_size = batch_size self.sample_num = sample_num self.input_height = input_height self.input_width = input_width self.output_height = output_height self.output_width = output_width self.y_dim = y_dim # null self.z_dim = z_dim # å™ªéŸ³ç‚¹çš„ç»´åº¦ 100 self.gf_dim = gf_dim # æœ€ç»ˆå¤šå°‘ä¸ªfilterçš„ä¸ªæ•° åŸºæ•° self.df_dim = df_dim # 64 self.gfc_dim = gfc_dim# ç”Ÿæˆå’Œåˆ¤åˆ«çš„å…¨è¿æ¥ 1024 self.dfc_dim = dfc_dim # 1024 self.c_dim = c_dim# ç”Ÿæˆçš„æ˜¯å½©è‰²å›¾ 3 # batch normalization : deals with poor initialization helps gradient flow self.d_bn1 = batch_norm(name='d_bn1')# bacthåœ¨reluä¹‹å‰å·ç§¯ä¹‹å self.d_bn2 = batch_norm(name='d_bn2') if not self.y_dim: self.d_bn3 = batch_norm(name='d_bn3') self.g_bn0 = batch_norm(name='g_bn0') self.g_bn1 = batch_norm(name='g_bn1') self.g_bn2 = batch_norm(name='g_bn2') if not self.y_dim: self.g_bn3 = batch_norm(name='g_bn3') self.dataset_name = dataset_name self.input_fname_pattern = input_fname_pattern self.checkpoint_dir = checkpoint_dir self.build_model() def build_model(self): if self.y_dim: self.y= tf.placeholder(tf.float32, [self.batch_size, self.y_dim], name='y') if self.is_crop: image_dims = [self.output_height, self.output_width, self.c_dim] else: image_dims = [self.input_height, self.input_height, self.c_dim] self.inputs = tf.placeholder( tf.float32, [self.batch_size] + image_dims, name='real_images') self.sample_inputs = tf.placeholder( # 64 108 108 3ï¼Œiamge_dim 108 108 3 tf.float32, [self.sample_num] + image_dims, name='sample_inputs') inputs = self.inputs # 64 108 108 3 sample_inputs = self.sample_inputs self.z = tf.placeholder( tf.float32, [None, self.z_dim], name='z') ## ç”Ÿæˆç½‘ç»œç»„æœ€å¼€å§‹çš„è¾“å…¥ï¼Œfloat32 # Bï¼Œ 100 self.z_sum = histogram_summary(\"z\", self.z) # åœ¨è®­ç»ƒç¥ç»ç½‘ç»œæ—¶ï¼Œå½“éœ€è¦æŸ¥çœ‹ä¸€ä¸ªå¼ é‡åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­å€¼çš„åˆ†å¸ƒæƒ…å†µæ—¶ï¼Œå¯é€šè¿‡tf.summary.histogram()å°†å…¶åˆ†å¸ƒæƒ…å†µä»¥ç›´æ–¹å›¾çš„å½¢å¼åœ¨TensorBoardç›´æ–¹å›¾ä»ªè¡¨æ¿ä¸Šæ˜¾ç¤ºï¼ if self.y_dim: self.G = self.generator(self.z, self.y) self.D, self.D_logits = \\ self.discriminator(inputs, self.y, reuse=False) self.sampler = self.sampler(self.z, self.y) self.D_, self.D_logits_ = \\ self.discriminator(self.G, self.y, reuse=True) else: self.G = self.generator(self.z) # 64 64 64 3 self.D, self.D_logits = self.discriminator(inputs) # 64 108 108 3 self.sampler = self.sampler(self.z) self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True) self.d_sum = histogram_summary(\"d\", self.D) self.d__sum = histogram_summary(\"d_\", self.D_) self.G_sum = image_summary(\"G\", self.G) # tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits,l) self.d_loss_real = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( logits=self.D_logits, labels=tf.ones_like(self.D))) self.d_loss_fake = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( logits=self.D_logits_, labels=tf.zeros_like(self.D_))) self.g_loss = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( logits=self.D_logits_, labels=tf.ones_like(self.D_))) self.d_loss_real_sum = scalar_summary(\"d_loss_real\", self.d_loss_real) self.d_loss_fake_sum = scalar_summary(\"d_loss_fake\", self.d_loss_fake) self.d_loss = self.d_loss_real + self.d_loss_fake self.g_loss_sum = scalar_summary(\"g_loss\", self.g_loss) self.d_loss_sum = scalar_summary(\"d_loss\", self.d_loss) t_vars = tf.trainable_variables() self.d_vars = [var for var in t_vars if 'd_' in var.name] self.g_vars = [var for var in t_vars if 'g_' in var.name] self.saver = tf.train.Saver() def train(self, config): \"\"\"Train DCGAN\"\"\" if config.dataset == 'mnist': data_X, data_y = self.load_mnist() else: data = glob(os.path.join(\"./data\", config.dataset, self.input_fname_pattern)) #np.random.shuffle(data) d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\ .minimize(self.d_loss, var_list=self.d_vars) g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\ .minimize(self.g_loss, var_list=self.g_vars) try: tf.global_variables_initializer().run() except: tf.initialize_all_variables().run() self.g_sum = merge_summary([self.z_sum, self.d__sum, self.G_sum, self.d_loss_fake_sum, self.g_loss_sum]) self.d_sum = merge_summary( [self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum]) self.writer = SummaryWriter(\"./logs\", self.sess.graph) sample_z = np.random.uniform(-1, 1, size=(self.sample_num , self.z_dim)) if config.dataset == 'mnist': sample_inputs = data_X[0:self.sample_num] sample_labels = data_y[0:self.sample_num] else: sample_files = data[0:self.sample_num] sample = [ get_image(sample_file, input_height=self.input_height, input_width=self.input_width, resize_height=self.output_height, resize_width=self.output_width, is_crop=self.is_crop, is_grayscale=self.is_grayscale) for sample_file in sample_files] if (self.is_grayscale): sample_inputs = np.array(sample).astype(np.float32)[:, :, :, None] else: sample_inputs = np.array(sample).astype(np.float32) counter = 1 start_time = time.time() if self.load(self.checkpoint_dir): print(\" [*] Load SUCCESS\") else: print(\" [!] Load failed...\") for epoch in xrange(config.epoch): if config.dataset == 'mnist': batch_idxs = min(len(data_X), config.train_size) // config.batch_size else: data = glob(os.path.join( \"./data\", config.dataset, self.input_fname_pattern)) batch_idxs = min(len(data), config.train_size) // config.batch_size for idx in xrange(0, batch_idxs): if config.dataset == 'mnist': batch_images = data_X[idx*config.batch_size:(idx+1)*config.batch_size] batch_labels = data_y[idx*config.batch_size:(idx+1)*config.batch_size] else: batch_files = data[idx*config.batch_size:(idx+1)*config.batch_size] batch = [ get_image(batch_file, input_height=self.input_height, input_width=self.input_width, resize_height=self.output_height, resize_width=self.output_width, is_crop=self.is_crop, is_grayscale=self.is_grayscale) for batch_file in batch_files] if (self.is_grayscale): batch_images = np.array(batch).astype(np.float32)[:, :, :, None] else: batch_images = np.array(batch).astype(np.float32) # ä¸€ä¸ªå‡åŒ€åˆ†å¸ƒ[low,high)ä¸­éšæœºé‡‡æ · ä»+1å’Œ-1ä¹‹é—´éšæ‰é‡‡æ · batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \\ .astype(np.float32) if config.dataset == 'mnist': # Update D network _, summary_str = self.sess.run([d_optim, self.d_sum], feed_dict={ self.inputs: batch_images, self.z: batch_z, self.y:batch_labels, }) self.writer.add_summary(summary_str, counter) # Update G network _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z, self.y:batch_labels, }) self.writer.add_summary(summary_str, counter) # Run g_optim twice to make sure that d_loss does not go to zero (different from paper) _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z, self.y:batch_labels }) self.writer.add_summary(summary_str, counter) errD_fake = self.d_loss_fake.eval({ self.z: batch_z, self.y:batch_labels }) errD_real = self.d_loss_real.eval({ self.inputs: batch_images, self.y:batch_labels }) errG = self.g_loss.eval({ self.z: batch_z, self.y: batch_labels }) else: # Update D network _, summary_str = self.sess.run([d_optim, self.d_sum], feed_dict={ self.inputs: batch_images, self.z: batch_z }) self.writer.add_summary(summary_str, counter) # Update G network _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z }) self.writer.add_summary(summary_str, counter) # Run g_optim twice to make sure that d_loss does not go to zero (different from paper) _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z }) self.writer.add_summary(summary_str, counter) errD_fake = self.d_loss_fake.eval({ self.z: batch_z }) errD_real = self.d_loss_real.eval({ self.inputs: batch_images }) errG = self.g_loss.eval({self.z: batch_z}) counter += 1 print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f\" \\ % (epoch, idx, batch_idxs, time.time() - start_time, errD_fake+errD_real, errG)) if np.mod(counter, 100) == 1: if config.dataset == 'mnist': samples, d_loss, g_loss = self.sess.run( [self.sampler, self.d_loss, self.g_loss], feed_dict={ self.z: sample_z, self.inputs: sample_inputs, self.y:sample_labels, } ) save_images(samples, [8, 8], './{}/train_{:02d}_{:04d}.png'.format(config.sample_dir, epoch, idx)) print(\"[Sample] d_loss: %.8f, g_loss: %.8f\" % (d_loss, g_loss)) else: try: samples, d_loss, g_loss = self.sess.run( [self.sampler, self.d_loss, self.g_loss], feed_dict={ self.z: sample_z, self.inputs: sample_inputs, }, ) save_images(samples, [8, 8], './{}/train_{:02d}_{:04d}.png'.format(config.sample_dir, epoch, idx)) print(\"[Sample] d_loss: %.8f, g_loss: %.8f\" % (d_loss, g_loss)) except: print(\"one pic error!...\") if counter//10 == 2: self.save(config.checkpoint_dir, counter) def discriminator(self, image, y=None, reuse=False): with tf.variable_scope(\"discriminator\") as scope: if reuse: scope.reuse_variables() if not self.y_dim: h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv')) h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name='d_h1_conv'))) h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name='d_h2_conv'))) h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name='d_h3_conv'))) # 64 14 14 256 -64 7 7 512 aa = tf.reshape(h3, [self.batch_size, -1]) h4 = linear(aa, 1, 'd_h3_lin') return tf.nn.sigmoid(h4), h4 else: yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) x = conv_cond_concat(image, yb) h0 = lrelu(conv2d(x, self.c_dim + self.y_dim, name='d_h0_conv')) h0 = conv_cond_concat(h0, yb) h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim + self.y_dim, name='d_h1_conv'))) h1 = tf.reshape(h1, [self.batch_size, -1]) h1 = concat([h1, y], 1) h2 = lrelu(self.d_bn2(linear(h1, self.dfc_dim, 'd_h2_lin'))) h2 = concat([h2, y], 1) h3 = linear(h2, 1, 'd_h3_lin') return tf.nn.sigmoid(h3), h3 def generator(self, z, y=None): with tf.variable_scope(\"generator\") as scope: if not self.y_dim: s_h, s_w = self.output_height, self.output_width s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2) # å…ˆæŠŠç‰¹å¾å›¾å¤§å°ç¡®å®šå‡ºæ¥ s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2) s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2) s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2) # project `z` and reshape self.z_, self.h0_w, self.h0_b = linear( z, self.gf_dim*8*s_h16*s_w16, 'g_h0_lin', with_w=True) self.h0 = tf.reshape( self.z_, [-1, s_h16, s_w16, self.gf_dim * 8]) h0 = tf.nn.relu(self.g_bn0(self.h0)) self.h1, self.h1_w, self.h1_b = deconv2d( h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name='g_h1', with_w=True) h1 = tf.nn.relu(self.g_bn1(self.h1)) h2, self.h2_w, self.h2_b = deconv2d( h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name='g_h2', with_w=True) h2 = tf.nn.relu(self.g_bn2(h2)) h3, self.h3_w, self.h3_b = deconv2d( h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name='g_h3', with_w=True) h3 = tf.nn.relu(self.g_bn3(h3)) h4, self.h4_w, self.h4_b = deconv2d( h3, [self.batch_size, s_h, s_w, self.c_dim], name='g_h4', with_w=True) return tf.nn.tanh(h4) else: s_h, s_w = self.output_height, self.output_width s_h2, s_h4 = int(s_h/2), int(s_h/4) s_w2, s_w4 = int(s_w/2), int(s_w/4) # yb = tf.expand_dims(tf.expand_dims(y, 1),2) yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) z = concat([z, y], 1) h0 = tf.nn.relu( self.g_bn0(linear(z, self.gfc_dim, 'g_h0_lin'))) h0 = concat([h0, y], 1) h1 = tf.nn.relu(self.g_bn1( linear(h0, self.gf_dim*2*s_h4*s_w4, 'g_h1_lin'))) h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2]) h1 = conv_cond_concat(h1, yb) h2 = tf.nn.relu(self.g_bn2(deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name='g_h2'))) h2 = conv_cond_concat(h2, yb) return tf.nn.sigmoid( deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name='g_h3')) def sampler(self, z, y=None): with tf.variable_scope(\"generator\") as scope: scope.reuse_variables() if not self.y_dim: s_h, s_w = self.output_height, self.output_width s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2) s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2) s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2) s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2) # project `z` and reshape h0 = tf.reshape( linear(z, self.gf_dim*8*s_h16*s_w16, 'g_h0_lin'), [-1, s_h16, s_w16, self.gf_dim * 8]) h0 = tf.nn.relu(self.g_bn0(h0, train=False)) h1 = deconv2d(h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name='g_h1') h1 = tf.nn.relu(self.g_bn1(h1, train=False)) h2 = deconv2d(h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name='g_h2') h2 = tf.nn.relu(self.g_bn2(h2, train=False)) h3 = deconv2d(h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name='g_h3') h3 = tf.nn.relu(self.g_bn3(h3, train=False)) h4 = deconv2d(h3, [self.batch_size, s_h, s_w, self.c_dim], name='g_h4') return tf.nn.tanh(h4) else: s_h, s_w = self.output_height, self.output_width s_h2, s_h4 = int(s_h/2), int(s_h/4) s_w2, s_w4 = int(s_w/2), int(s_w/4) # yb = tf.reshape(y, [-1, 1, 1, self.y_dim]) yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) z = concat([z, y], 1) h0 = tf.nn.relu(self.g_bn0(linear(z, self.gfc_dim, 'g_h0_lin'))) h0 = concat([h0, y], 1) h1 = tf.nn.relu(self.g_bn1( linear(h0, self.gf_dim*2*s_h4*s_w4, 'g_h1_lin'), train=False)) h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2]) h1 = conv_cond_concat(h1, yb) h2 = tf.nn.relu(self.g_bn2( deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name='g_h2'), train=False)) h2 = conv_cond_concat(h2, yb) return tf.nn.sigmoid(deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name='g_h3')) def load_mnist(self): data_dir = os.path.join(\"./data\", self.dataset_name) fd = open(os.path.join(data_dir,'train-images-idx3-ubyte')) loaded = np.fromfile(file=fd,dtype=np.uint8) trX = loaded[16:].reshape((60000,28,28,1)).astype(np.float) fd = open(os.path.join(data_dir,'train-labels-idx1-ubyte')) loaded = np.fromfile(file=fd,dtype=np.uint8) trY = loaded[8:].reshape((60000)).astype(np.float) fd = open(os.path.join(data_dir,'t10k-images-idx3-ubyte')) loaded = np.fromfile(file=fd,dtype=np.uint8) teX = loaded[16:].reshape((10000,28,28,1)).astype(np.float) fd = open(os.path.join(data_dir,'t10k-labels-idx1-ubyte')) loaded = np.fromfile(file=fd,dtype=np.uint8) teY = loaded[8:].reshape((10000)).astype(np.float) trY = np.asarray(trY) teY = np.asarray(teY) X = np.concatenate((trX, teX), axis=0) y = np.concatenate((trY, teY), axis=0).astype(np.int) seed = 547 np.random.seed(seed) np.random.shuffle(X) np.random.seed(seed) np.random.shuffle(y) y_vec = np.zeros((len(y), self.y_dim), dtype=np.float) for i, label in enumerate(y): y_vec[i,y[i]] = 1.0 return X/255.,y_vec @property def model_dir(self): return \"{}_{}_{}_{}\".format( self.dataset_name, self.batch_size, self.output_height, self.output_width) def save(self, checkpoint_dir, step): model_name = \"DCGAN.model\" checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir) if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir) self.saver.save(self.sess, os.path.join(checkpoint_dir, model_name), global_step=step) def load(self, checkpoint_dir): print(\" [*] Reading checkpoints...\") checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir) ckpt = tf.train.get_checkpoint_state(checkpoint_dir) if ckpt and ckpt.model_checkpoint_path: ckpt_name = os.path.basename(ckpt.model_checkpoint_path) self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name)) print(\" [*] Success to read {}\".format(ckpt_name)) return True else: print(\" [*] Failed to find a checkpoint\") return False â€”|â€”","tags":["Python"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"DFS_BFS","path":"/2019/11/17/20191117-DFS-BFS/","content":"#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/11/17 10:28# @Author : caius# @Site :# @File : BFS.py# @Software: PyCharmgraph = { \"A\":{\"B\", \"C\"}, \"B\":{\"A\",\"C\",\"D\"}, \"C\":{\"A\",\"B\",\"D\",\"E\"}, \"D\":{\"B\",\"C\",\"E\",\"F\"}, \"E\":{\"C\",\"D\"}, \"F\":{\"D\"}}# å­—å…¸çš„åŸºæœ¬ç”¨æ³•# keys: A B C D E F# graph[\"E\"} \"c\". \"D\"def BFS(graph, s): # é˜Ÿåˆ—å…ˆè¿›å…ˆå‡º queue=[] queue.append(s) seen = set()# ä»£è¡¨è¿™ä¸ªä¸œè¥¿æ˜¯ä¸ªset seen.add(s) parrent ={} parrent={s:None} while(len(queue)0): vertex = queue.pop(0) nodes = graph[vertex] for w in nodes: if w not in seen: queue.append(w) seen.add(w) parrent[w] = vertex print(vertex) return parrentdef DFS(graph, s): # é˜Ÿåˆ—å…ˆè¿›å…ˆå‡º stack=[] stack.append(s) seen = set()# ä»£è¡¨è¿™ä¸ªä¸œè¥¿æ˜¯ä¸ªset seen.add(s) while(len(stack)0): vertex = stack.pop() nodes = graph[vertex] for w in nodes: if w not in seen: stack.append(w) seen.add(w) print(vertex)DFS(graph,\"E\")parrent = BFS(graph,'E')for key in parrent: print(key,parrent[key])v = \"B\"count=-1while v!= None: print(v) v = parrent[v] count+=1print(\"count: {} æ¬¡\".format(count))","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"Duplicate","path":"/2019/11/17/20191117-Duplicate/","content":"æ˜¯å¦å­˜åœ¨ç›¸åŒå…ƒç´ ï¼Œpython3ç”¨å­—å…¸çš„æ–¹å¼è§£å†³ class Solution(object): def containsDuplicate(self, nums): \"\"\" :type nums: List[int] :rtype: bool \"\"\" # æ–¹æ³•3ï¼šæ•°å­—å­˜å­—å…¸ dic = {} for i in nums: dic[i] = dic.get(i, 0) + 1 if dic[i] 1: return True return False æœ€é•¿å›æ–‡å­ä¸² class Solution: def longestPalindrome(self, s: str) - str: # ä¸¤ç§åˆ¤æ–­æ¡ä»¶ # DP åŠ¨æ€è§„åˆ’ # CABAC # B # ABA # CABAC palindrome = '' for i in range(len(s)): aa1 = self.getlongestpalindrome(s,i,i) len1 = len(aa1) if len1len(palindrome): palindrome = aa1 aa2 = self.getlongestpalindrome(s,i,i+1) len2 = len(aa2) if len2len(palindrome): palindrome = aa2 return palindrome def getlongestpalindrome(self, s, l, r): while l = 0 and rlen(s) and s[l]==s[r]: l -= 1 r += 1 return s[l+1:r]~ class Solution: def containsNearbyDuplicate(self, nums: List[int], k: int) - bool: # ç”¨dictionaryæ¥åšæ¯”è¾ƒå®¹æ˜“ä¸€ç‚¹ lookup = {} for i , num in enumerate(nums): if num not in lookup: lookup[num] = i # å°†numå­˜åˆ°dicé‡Œé¢ else: if i-lookup[num]=k: return True lookup[num] = i return False~","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"dynamic planning","path":"/2019/11/17/20191117-dynamic/","content":"arr= [1,2,4,1,7,8,3]def rec_opt(arr,i): if i == 0: return arr[0] elif i == 1: return max(arr[0],arr[1]) else: A = rec_opt(arr,i-2)+arr[i] B = rec_opt(arr,i-1) return max(A,B)print(rec_opt(arr,6)) é€’å½’ä¼šäº§ç”Ÿå¾ˆå¤šçš„é‡å çš„å­é—®é¢˜ï¼Œè¿ç®—è§„æ¨¡2^n#éé€’å½’çš„æ–¹æ³•import numpy as npdef dp_opt(arr): opt = np.zeros(len(arr)) opt[0] = arr[0] opt[1] = max(arr[0], arr[1]) for i in range(2, len(arr)): A = opt[i-2] +arr[i] B = opt[i-1] opt[i] = max(A,B) return opt[len(arr)-1]dp_opt(arr) arr = [3,34,4,12,5,2]def rec_subset(arr, i,s): if s == 0: return True elif i == 0: return arr[0] == s elif arr[i]s: return rec_subset(arr,i-1,s) else: A= rec_subset(arr,i-1, s-arr[i]) B = rec_subset(arr, i-1,s) return A or Brec_subset(arr, len(arr)-1,9)~ # éé€’å½’çš„æ–¹æ³•ï¼Œç”¨äºŒç»´æ•°ç»„æ¥ä¿å­˜def dp_subset(arr,s): subset = np.zeros((len(arr),s+1),dtype=bool) subset[:,0] = True subset[0,:] = Fasle subset[0,arr[0]] = True for i in range(1,len(arr)): for ss in range(1,s+1): if arr[i] ss: subset[i,ss] = subset[i-1,ss] else: A= subset(i-1, s-arr[i]) B = subset(i-1,s) subset[i,s] = A or B r ,c =subset.shape return subset[r-1,c-1]rec_subset(arr, len(arr)-1,9)","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"python_learn","path":"/2019/11/17/20191117-python-learn/","content":"æŸ¥æ‰¾å›¾ä¸­ä¸¤ä¸ªèŠ‚ç‚¹çš„æœ€å°çš„è·ç¦»è¿™é‡Œé¢ä½¿ç”¨äº†pythonçš„ä¼˜å…ˆé˜Ÿåˆ—ï¼Œè¿™é‡Œçš„é˜Ÿåˆ—æŒ‰ç…§åé¢çš„æ•°å€¼å¤§å°è¿›è¡Œæ’åºï¼Œè€Œä¸æ˜¯åƒæ™®é€šçš„é˜Ÿåˆ—ä¸€æ ·å…ˆè¿›å…ˆå‡ºã€‚åé¢çš„æ•°å€¼ï¼Œæ˜¯èŠ‚ç‚¹åˆ°å‡ºå‘èŠ‚ç‚¹çš„è·ç¦»é•¿åº¦ã€‚ #!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/11/17 11:50# @Author : caius# @Site :# @File : find_min_bfs.py# @Software: PyCharmimport heapqimport math# pqueue = []# heapq.heappush(pqueue,(1,\"A\"))# heapq.heappush(pqueue,(7,\"B\"))# heapq.heappush(pqueue,(3,\"C\"))# heapq.heappush(pqueue,(6,\"D\"))# heapq.heappush(pqueue,(2,\"E\"))graph = { \"A\":{\"B\": 5, \"C\": 1}, \"B\":{\"A\": 5,\"C\": 2,\"D\": 1}, \"C\":{\"A\": 1,\"B\": 2,\"D\": 4,\"E\": 8}, \"D\":{\"B\": 1,\"C\": 4,\"E\": 3,\"F\": 6}, \"E\":{\"C\": 8,\"D\": 3}, \"F\":{\"D\": 6}}def init_distance(graph,s): distance = {s:0} for vertex in graph: if vertex != s: distance[vertex] = math.inf return distancedef dijkstra(graph, s): pqueue = [] heapq.heappush(pqueue,(0,s)) seen =set() parent ={s:None} distance = init_distance(graph,s) while(len(pqueue)0): pair = heapq.heappop(pqueue) # æ‹¿åˆ°ä¸€å¯¹ç‚¹ï¼Œpair dist = pair[0] vertex = pair[1] seen.add(vertex) nodes = graph[vertex].keys() for w in nodes: if w not in seen: if dist+graph[vertex][w] distance[w]: heapq.heappush(pqueue,(dist+graph[vertex][w],w)) parent[w] = vertex distance[w] = dist+graph[vertex][w] return parent,distanceparent, distance = dijkstra(graph,\"A\")print(parent)print(distance) python è£…é¥°å™¨è£…é¥°å™¨(Decorators)æ˜¯ Python çš„ä¸€ä¸ªé‡è¦éƒ¨åˆ†ã€‚ç®€å•åœ°è¯´ï¼šä»–ä»¬æ˜¯ä¿®æ”¹å…¶ä»–å‡½æ•°çš„åŠŸèƒ½çš„å‡½æ•°ã€‚ä»–ä»¬æœ‰åŠ©äºè®©æˆ‘ä»¬çš„ä»£ç æ›´ç®€çŸ­ï¼Œä¹Ÿæ›´Pythonicï¼ˆPythonèŒƒå„¿ï¼‰ã€‚è£…é¥°å™¨å¯ä»¥è®©ä½ çš„ä»£ç æ›´ç®€æ´ã€‚ #!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/11/17 9:59# @Author : caius# @Site :# @File : deco.py.py# @Software: PyCharmimport time# è£…é¥°å™¨def display_time(func): def wrapper(*args): t1 = time.time() result = func(*args) t2 = time.time() print(\"Total time: {:.4} s\".format(t2-t1)) return result return wrapper# è¾“å‡ºè´¨æ•°def is_prime(num): if num2: return False elif num==2: return True else: for i in range(2, num): if num%i ==0: return False return True@display_timedef prime_nums(): for i in range(2,10000): if is_prime(i): print(i)@display_timedef count_prime_nums(maxnum): count = 0 for i in range(2, maxnum): if is_prime(i): count += 1 return countcount = count_prime_nums(5000)print(count)~ ç”¨turtle ç”»å›¾#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/11/17 13:27# @Author : caius# @Site :# @File : draw.py# @Software: PyCharmfrom turtle import *import math# turtle ç”»å›¾# forward(100)# left(90)# forward(100)# left(90)# forward(100)# left(90)# forward(100)# left(90)# exitonclick()# ä¸ç‚¹å‡»çª—å£çš„è¯å°±ä¸ä¼šé€€å‡º# # ç”»ç­‰è¾¹ä¸‰è§’å½¢# forward(100)# left(120)# forward(100)# left(120)# forward(100)# left(120)# exitonclick()# ä¸ç‚¹å‡»çª—å£çš„è¯å°±ä¸ä¼šé€€å‡º# ç”»äº”è§’æ˜Ÿ# forward(100)# right(180-36)# forward(100)# right(180-36)# forward(100)# right(180-36)# forward(100)# right(180-36)# forward(100)# right(180-36)# for i in range(5):# forward(100)# right(180-36)#angle = 360/8length = 100speed(0)for i in range(8): if i %2==0: color('yellow') else: color('red') begin_fill() forward(100) left(angle) forward(length) left(180-angle) forward(length) left(angle) forward(length) left(180-angle) end_fill() left(angle)forward(length)left(180-(180-angle)/2)alpha = angle*3.1415926536 /180step = 2*length*math.sin(alpha/2)color('blue')begin_fill()for i in range(8): forward(step) left(angle)end_fill()exitonclick()# ä¸ç‚¹å‡»çª—å£çš„è¯å°±ä¸ä¼šé€€å‡º #!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/11/17 14:20# @Author : caius# @Site :# @File : Lsystem2.py# @Software: PyCharmfrom turtle import *length = 7angle = 60def split_path(path): i = 0 list = [] while i len(path): if path[i] == \"F\": list.append(path[i:i+2]) i = i+2 else: list.append(path[i]) i = i+1 return listdef apply_rule(path, rules): lst = split_path(path) for i in range(len(lst)): symbol = lst[i] if symbol in rules: lst[i] = rules[symbol] path =\"\".join(symbol for symbol in lst) return pathrules={ \"Fl\": \"Fr+Fl+Fr\", \"Fr\":\"Fl-Fr-Fl\"}def draw_patj(path): lst = split_path(path) for symbol in lst: if symbol ==\"Fl\" or symbol=='Fr': forward(length) elif symbol==\"-\": left(angle) elif symbol=='+': right(angle)speed(0)path = 'Fr'# speed(0)##lst = split_path(path)for i in range(6): path = apply_rule(path,rules)print(path)draw_patj(path)exitonclick()~ python ç±»çš„æ„é€ #!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/11/17 14:52# @Author : caius# @Site :# @File : Bank.py# @Software: PyCharmclass BankAccount: # Constructor æ„é€ å™¨ def __init__(self,accountNumber, accountName, balance): self.accountNumber = accountNumber self.accountName = accountName self.balance = balance def __str__(self): return \"(name: {}, balance: {})\".format(self.accountName,self.balance)#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/11/17 14:56# @Author : caius# @Site :# @File : main.py# @Software: PyCharmfrom Bank import BankAccountb1 = BankAccount(\"56789\",\"Tony\", 100.0)print((b1))","tags":["Python"],"categories":["Python"]},{"title":"rnn","path":"/2019/11/17/20191117-rnn/","content":"RNNåŠRNNçš„å‡ ä¸ªå˜ä½“åºåˆ—å½¢çš„æ•°æ®ä¸å¤ªå¥½ç”¨åŸå§‹çš„ç¥ç»ç½‘ç»œå¤„ç†ï¼Œä¸ºäº†å»ºæ¨¡åºåˆ—é—®é¢˜ï¼ŒRNNå¼•å…¥äº†éšçŠ¶æ€hï¼ˆhiddenï¼‰çš„æ¦‚å¿µï¼Œhå¯ä»¥å¯¹åºåˆ—å½¢çš„æ•°æ®æå–ç‰¹å¾ï¼Œæ¥ç€è½¬æ¢ä¸ºè¾“å‡ºã€‚ åœ†åœˆæˆ–æ–¹å—è¡¨ç¤ºçš„æ˜¯å‘é‡ ä¸€ä¸ªç®­å¤´å°±è¡¨ç¤ºå¯¹è¯¥å‘é‡åšä¸€æ¬¡å˜æ¢ã€‚å¦‚ä¸Šå›¾ä¸­h0å’Œx1åˆ†åˆ«æœ‰ä¸€ä¸ªç®­å¤´è¿æ¥ï¼Œå°±è¡¨ç¤ºå¯¹h0å’Œx1å„åšäº†ä¸€æ¬¡å˜æ¢ Tipsï¼šUã€Wã€béƒ½æ˜¯ä¸€æ ·çš„ï¼Œæ¯ä¸ªæ­¥éª¤çš„å‚æ•°éƒ½æ˜¯å…±äº«çš„ï¼Œè¿™æ˜¯RNNçš„é‡è¦ç‰¹ç‚¹ã€‚ ä¾æ¬¡è®¡ç®—å‰©ä¸‹æ¥çš„ï¼ˆä½¿ç”¨ç›¸åŒçš„å‚æ•°Uã€Wã€bï¼‰ï¼šè¾“å‡ºå€¼çš„æ–¹æ³•å°±æ˜¯ç›´æ¥é€šè¿‡hè¿›è¡Œè®¡ç®—ï¼šä¸€ä¸ªç®­å¤´å°±è¡¨ç¤ºå¯¹å¯¹åº”çš„å‘é‡åšä¸€æ¬¡ç±»ä¼¼äºf(Wx+b)çš„å˜æ¢ï¼Œè¿™é‡Œçš„è¿™ä¸ªç®­å¤´å°±è¡¨ç¤ºå¯¹h1è¿›è¡Œä¸€æ¬¡å˜æ¢ï¼Œå¾—åˆ°è¾“å‡ºy1è¾“å…¥æ˜¯x1, x2, â€¦..xnï¼Œè¾“å‡ºä¸ºy1, y2, â€¦ynï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè¾“å…¥å’Œè¾“å‡ºåºåˆ—å¿…é¡»è¦æ˜¯ç­‰é•¿çš„ã€‚ä¸€äº›é—®é¢˜é€‚åˆç”¨ç»å…¸çš„RNNç»“æ„å»ºæ¨¡ï¼Œå¦‚ï¼š è®¡ç®—è§†é¢‘ä¸­æ¯ä¸€å¸§çš„åˆ†ç±»æ ‡ç­¾ã€‚å› ä¸ºè¦å¯¹æ¯ä¸€å¸§è¿›è¡Œè®¡ç®—ï¼Œå› æ­¤è¾“å…¥å’Œè¾“å‡ºåºåˆ—ç­‰é•¿ã€‚ è¾“å…¥ä¸ºå­—ç¬¦ï¼Œè¾“å‡ºä¸ºä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡ã€‚è¿™å°±æ˜¯è‘—åçš„Char RNNï¼ˆè¯¦ç»†ä»‹ç»è¯·å‚è€ƒï¼šThe Unreasonable Effectiveness of Recurrent Neural Networksï¼‰ã€‚ N vs 1 å¤šè¾“å…¥å•è¾“å‡ºè¿™ç§ç»“æ„é€šå¸¸ç”¨æ¥å¤„ç†åºåˆ—åˆ†ç±»é—®é¢˜ã€‚å¦‚è¾“å…¥ä¸€æ®µæ–‡å­—åˆ¤åˆ«å®ƒæ‰€å±çš„ç±»åˆ«ï¼Œè¾“å…¥ä¸€ä¸ªå¥å­åˆ¤æ–­å…¶æƒ…æ„Ÿå€¾å‘ï¼Œè¾“å…¥ä¸€æ®µè§†é¢‘å¹¶åˆ¤æ–­å®ƒçš„ç±»åˆ«ç­‰ç­‰ã€‚ 1 VS N å•è¾“å…¥å¤šè¾“å‡ºè¾“å…¥ä¸æ˜¯åºåˆ—è€Œè¾“å‡ºä¸ºåºåˆ—çš„æƒ…å†µæ€ä¹ˆå¤„ç†ï¼Ÿæˆ‘ä»¬å¯ä»¥åªåœ¨åºåˆ—å¼€å§‹è¿›è¡Œè¾“å…¥è®¡ç®—ï¼šè¿˜æœ‰ä¸€ç§ç»“æ„æ˜¯æŠŠè¾“å…¥xä½œä¸ºæ¯ä¸ªé˜¶æ®µçš„è¾“å…¥ï¼šè¿™ç§ç»“æ„å¤„ç†çš„é—®é¢˜ï¼š ä»å›¾åƒç”Ÿæˆæ–‡å­—ï¼ˆimage captionï¼‰ï¼Œæ­¤æ—¶è¾“å…¥çš„Xå°±æ˜¯å›¾åƒçš„ç‰¹å¾ï¼Œè€Œè¾“å‡ºçš„yåºåˆ—å°±æ˜¯ä¸€æ®µå¥å­ ä»ç±»åˆ«ç”Ÿæˆè¯­éŸ³æˆ–éŸ³ä¹ç­‰ N vs MRNNæœ€é‡è¦çš„ä¸€ä¸ªå˜ç§ï¼šN vs Mã€‚è¿™ç§ç»“æ„åˆå«Encoder-Decoderæ¨¡å‹ï¼Œä¹Ÿå¯ä»¥ç§°ä¹‹ä¸ºSeq2Seqæ¨¡å‹ã€‚ä¸ºæ­¤ï¼ŒEncoder-Decoderç»“æ„å…ˆå°†è¾“å…¥æ•°æ®ç¼–ç æˆä¸€ä¸ªä¸Šä¸‹æ–‡å‘é‡cï¼šcæœ‰å¤šç§æ–¹å¼ï¼Œæœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯æŠŠEncoderçš„æœ€åä¸€ä¸ªéšçŠ¶æ€èµ‹å€¼ç»™cï¼Œè¿˜å¯ä»¥å¯¹æœ€åçš„éšçŠ¶æ€åšä¸€ä¸ªå˜æ¢å¾—åˆ°cï¼Œä¹Ÿå¯ä»¥å¯¹æ‰€æœ‰çš„éšçŠ¶æ€åšå˜æ¢ã€‚ æ‹¿åˆ°cä¹‹åï¼Œå°±ç”¨å¦ä¸€ä¸ªRNNç½‘ç»œå¯¹å…¶è¿›è¡Œè§£ç ï¼Œè¿™éƒ¨åˆ†RNNç½‘ç»œè¢«ç§°ä¸ºDecoderã€‚å…·ä½“åšæ³•å°±æ˜¯å°†cå½“åšä¹‹å‰çš„åˆå§‹çŠ¶æ€h0è¾“å…¥åˆ°Decoderä¸­ï¼šè¿˜æœ‰ä¸€ç§åšæ³•æ˜¯å°†cå½“åšæ¯ä¸€æ­¥çš„è¾“å…¥ï¼šEncoder-Decoderç»“æ„ä¸é™åˆ¶è¾“å…¥å’Œè¾“å‡ºçš„åºåˆ—é•¿åº¦ï¼Œå› æ­¤åº”ç”¨çš„èŒƒå›´éå¸¸å¹¿æ³›ï¼Œæ¯”å¦‚ï¼š æœºå™¨ç¿»è¯‘ã€‚Encoder-Decoderçš„æœ€ç»å…¸åº”ç”¨ï¼Œäº‹å®ä¸Šè¿™ä¸€ç»“æ„å°±æ˜¯åœ¨æœºå™¨ç¿»è¯‘é¢†åŸŸæœ€å…ˆæå‡ºçš„ æ–‡æœ¬æ‘˜è¦ã€‚è¾“å…¥æ˜¯ä¸€æ®µæ–‡æœ¬åºåˆ—ï¼Œè¾“å‡ºæ˜¯è¿™æ®µæ–‡æœ¬åºåˆ—çš„æ‘˜è¦åºåˆ— é˜…è¯»ç†è§£ã€‚å°†è¾“å…¥çš„æ–‡ç« å’Œé—®é¢˜åˆ†åˆ«ç¼–ç ï¼Œå†å¯¹å…¶è¿›è¡Œè§£ç å¾—åˆ°é—®é¢˜çš„ç­”æ¡ˆã€‚ è¯­éŸ³è¯†åˆ«ã€‚è¾“å…¥æ˜¯è¯­éŸ³ä¿¡å·åºåˆ—ï¼Œè¾“å‡ºæ˜¯æ–‡å­—åºåˆ—ã€‚ â€¦â€¦â€¦ tensorflow å®ç°RNNRNNCellRNNCell æ˜¯TensorFlowä¸­å®ç°RNNçš„åŸºæœ¬å•å…ƒï¼Œæ¯ä¸ªRNNCelléƒ½æœ‰ä¸€ä¸ªcallæ–¹æ³•ï¼Œä½¿ç”¨æ–¹å¼æ˜¯ï¼š(output, next_state) = call(input, state)å€ŸåŠ©å›¾ç‰‡æ¥è¯´å¯èƒ½æ›´å®¹æ˜“ç†è§£ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåˆå§‹çŠ¶æ€h0ï¼Œè¿˜æœ‰è¾“å…¥x1ï¼Œè°ƒç”¨call(x1, h0)åå°±å¯ä»¥å¾—åˆ°(output1, h1)ï¼šå†è°ƒç”¨ä¸€æ¬¡call(x2, h1)å°±å¯ä»¥å¾—åˆ°(output2, h2)ï¼šä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯è°ƒç”¨ä¸€æ¬¡RNNCellçš„callæ–¹æ³•ï¼Œå°±ç›¸å½“äºåœ¨æ—¶é—´ä¸Šâ€œæ¨è¿›äº†ä¸€æ­¥â€ï¼Œè¿™å°±æ˜¯RNNCellçš„åŸºæœ¬åŠŸèƒ½ã€‚RNNCellåªæ˜¯ä¸€ä¸ªæŠ½è±¡ç±»ï¼Œå…¶ä»–çš„RNNcelléƒ½ä¼šç»§æ‰¿è¯¥æ–¹æ³•ï¼Œç„¶åå…·ä½“å®ç°å…¶ä¸­çš„call()å‡½æ•°ã€‚ä¸»è¦æœ‰state_sizeå’Œoutput_sizeä¸¤ä¸ªå±æ€§ï¼Œåˆ†åˆ«ä»£è¡¨äº†éšè—å±‚å’Œè¾“å‡ºå±‚çš„ç»´åº¦ã€‚ç„¶åå°±æ˜¯zero_state()å’Œcall()ä¸¤ä¸ªå‡½æ•°ï¼Œåˆ†åˆ«ç”¨äºåˆå§‹åŒ–åˆå§‹çŠ¶æ€h0ä¸ºå…¨é›¶å‘é‡å’Œå®šä¹‰å®é™…çš„RNNCellçš„æ“ä½œï¼ˆæ¯”å¦‚RNNå°±æ˜¯ä¸€ä¸ªæ¿€æ´»ï¼ŒGRUçš„ä¸¤ä¸ªé—¨ï¼ŒLSTMçš„ä¸‰ä¸ªé—¨æ§ç­‰ï¼Œä¸åŒçš„RNNçš„åŒºåˆ«ä¸»è¦ä½“ç°åœ¨è¿™ä¸ªå‡½æ•°ï¼‰ã€‚ BasicRNNCellæŠŠstate_sizeå’Œoutput_sizeå®šä¹‰æˆç›¸åŒï¼Œè€Œä¸”htå’Œoutputä¹Ÿæ˜¯ç›¸åŒçš„æœ€æ™®é€šçš„RNNå®šä¹‰æ–¹å¼ã€‚ä¹Ÿå°±æ˜¯è¯´output = new_state = f(W input + U state + B) GRUç›¸æ¯”BasicRNNCellåªæ”¹å˜äº†callå‡½æ•°éƒ¨åˆ†ï¼Œå¢åŠ äº†é‡ç½®é—¨å’Œæ›´æ–°é—¨ä¸¤éƒ¨åˆ†ï¼Œåˆ†åˆ«ç”±rå’Œuè¡¨ç¤ºã€‚ç„¶åcè¡¨ç¤ºè¦æ›´æ–°çš„çŠ¶æ€å€¼ã€‚å…¶å¯¹åº”çš„å…¬å¼å¦‚å¦‚ä¸‹æ‰€ç¤ºï¼š r = f(W1 input + U1 state + B1)r=f(W1âˆ—input+U1âˆ—state+B1)u = f(W2 input + U2 state + B2)u=f(W2âˆ—input+U2âˆ—state+B2)c = f(W3 input + U3 r _state + B3)c=f(W3âˆ—input+U3âˆ—râˆ—state+B3)h_new = u _ h + (1 - u) * chnâ€‹ew=uâˆ—h+(1âˆ’u)âˆ—c BasicLSTMCellç›¸æ¯”GRUï¼ŒLSTMåˆå¤šäº†ä¸€ä¸ªè¾“å‡ºé—¨ï¼Œè€Œä¸”åˆæ–°å¢æ·»äº†ä¸€ä¸ªCè¡¨ç¤ºå…¶å†…éƒ¨çŠ¶æ€ï¼Œç„¶åå°†hå’Œcä»¥tupleçš„å½¢å¼è¿”å›ä½œä¸ºLSTMå†…éƒ¨çš„çŠ¶æ€å˜é‡ã€‚ æˆ‘ä»¬ç”¨çš„æ—¶å€™éƒ½æ˜¯ç”¨çš„å®ƒçš„ä¸¤ä¸ªå­ç±»BasicRNNCellå’ŒBasicLSTMCellã€‚é¡¾åæ€ä¹‰ï¼Œå‰è€…æ˜¯RNNçš„åŸºç¡€ç±»ï¼Œåè€…æ˜¯LSTMçš„åŸºç¡€ç±»ã€‚çœ‹ä¸‹RNNCellã€BasicRNNCellã€BasicLSTMCellè¿™ä¸‰ä¸ªç±»çš„æ³¨é‡Šéƒ¨åˆ†ï¼Œåº”è¯¥å°±å¯ä»¥ç†è§£å®ƒä»¬çš„åŠŸèƒ½äº†ã€‚é™¤äº†callæ–¹æ³•å¤–ï¼Œå¯¹äºRNNCellï¼Œè¿˜æœ‰ä¸¤ä¸ªç±»å±æ€§æ¯”è¾ƒé‡è¦ï¼š state_size output_sizeå‰è€…æ˜¯éšå±‚çš„å¤§å°ï¼Œåè€…æ˜¯è¾“å‡ºçš„å¤§å°ã€‚æ¯”å¦‚æˆ‘ä»¬é€šå¸¸æ˜¯å°†ä¸€ä¸ªbatché€å…¥æ¨¡å‹è®¡ç®—ï¼Œè®¾è¾“å…¥æ•°æ®çš„å½¢çŠ¶ä¸º(batch_size, input_size)ï¼Œé‚£ä¹ˆè®¡ç®—æ—¶å¾—åˆ°çš„éšå±‚çŠ¶æ€å°±æ˜¯(batch_size, state_size)ï¼Œè¾“å‡ºå°±æ˜¯(batch_size, output_size)ã€‚ In [2]: import numpy as npIn [3]: cell = tf.nn.rnn_cell.BasicRNNCell(num_units=128) # state_size = 128In [4]: print(cell.state_size) # 128 In [5]: inputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 æ˜¯ batch_size In [6]: h0 = cell.zero_state(32, np.float32) # é€šè¿‡zero_stateå¾—åˆ°ä¸€ä¸ªå…¨0çš„åˆå§‹çŠ¶æ€ï¼Œå½¢çŠ¶ä¸º(batch_size, state_size) In [7]: output, h1 = cell.call(inputs, h0) #è°ƒç”¨callå‡½æ•° --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) ipython-input-7-378fe3b1c400 in module ---- 1 output, h1 = cell.call(inputs, h0) #è°ƒç”¨callå‡½æ•° ~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)```python 350 gate_inputs = math_ops.matmul( -- 351 array_ops.concat([inputs, state], 1), self._kernel) 352 gate_inputs = nn_ops.bias_add(gate_inputs, self._bias) 353 output = self._activation(gate_inputs) AttributeError: 'BasicRNNCell' object has no attribute '_kernel' In [8]: output, h1 = cell.__call__(inputs, h0) #è°ƒç”¨callå‡½æ•° In [9]: print(h1.shape) # (32, 128) (32, 128) In [10]: BasicLSTMCell In [10]: lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=128)In [11]: inputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 æ˜¯ batch_sizeIn [12]: h0 = lstm_cell.zero_state(32, np.float32) # é€šè¿‡zero_stateå¾—åˆ°ä¸€ä¸ªå…¨0çš„åˆå§‹çŠ¶æ€In [13]: output, h1 = lstm_cell.__call__(inputs, h0)In [14]: print(h1.h) # shape=(32, 128)Tensor(\"basic_lstm_cell/Mul_2:0\", shape=(32, 128), dtype=float32)In [15]: print(h1.c) # shape=(32, 128)Tensor(\"basic_lstm_cell/Add_1:0\", shape=(32, 128), dtype=float32) å‚è€ƒé“¾æ¥ï¼šhttps://zhuanlan.zhihu.com/p/28054589https://zhuanlan.zhihu.com/p/28196873","tags":["æ·±åº¦å­¦ä¹ "],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"DFS_BFS","path":"/2019/11/17/DFS-BFS/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 | #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/17 10:28 # @Author : caius # @Site : # @File : BFS.py # @Software: PyCharm graph = { \"A\":{\"B\", \"C\"}, \"B\":{\"A\",\"C\",\"D\"}, \"C\":{\"A\",\"B\",\"D\",\"E\"}, \"D\":{\"B\",\"C\",\"E\",\"F\"}, \"E\":{\"C\",\"D\"}, \"F\":{\"D\"} } # å­—å…¸çš„åŸºæœ¬ç”¨æ³• # keys: A B C D E F # graph[\"E\"} \"c\". \"D\" def BFS(graph, s): # é˜Ÿåˆ—å…ˆè¿›å…ˆå‡º queue=[] queue.append(s) seen = set()# ä»£è¡¨è¿™ä¸ªä¸œè¥¿æ˜¯ä¸ªset seen.add(s) parrent ={} parrent={s:None} while(len(queue)0): vertex = queue.pop(0) nodes = graph[vertex] for w in nodes: if w not in seen: queue.append(w) seen.add(w) parrent[w] = vertex print(vertex) return parrent def DFS(graph, s): # é˜Ÿåˆ—å…ˆè¿›å…ˆå‡º stack=[] stack.append(s) seen = set()# ä»£è¡¨è¿™ä¸ªä¸œè¥¿æ˜¯ä¸ªset seen.add(s) while(len(stack)0): vertex = stack.pop() nodes = graph[vertex] for w in nodes: if w not in seen: stack.append(w) seen.add(w) print(vertex) DFS(graph,\"E\") parrent = BFS(graph,'E') for key in parrent: print(key,parrent[key]) v = \"B\" count=-1 while v!= None: print(v) v = parrent[v] count+=1 print(\"count: {} æ¬¡\".format(count)) â€”|â€”","tags":["Python"],"categories":["ç®—æ³•"]},{"title":"Duplicate","path":"/2019/11/17/Duplicate/","content":"æ˜¯å¦å­˜åœ¨ç›¸åŒå…ƒç´ ï¼Œpython3ç”¨å­—å…¸çš„æ–¹å¼è§£å†³ 1 2 3 4 5 6 7 8 9 10 11 12 13 | class Solution(object): def containsDuplicate(self, nums): \"\"\" :type nums: List[int] :rtype: bool \"\"\" # æ–¹æ³•3ï¼šæ•°å­—å­˜å­—å…¸ dic = {} for i in nums: dic[i] = dic.get(i, 0) + 1 if dic[i] 1: return True return False â€”|â€” æœ€é•¿å›æ–‡å­ä¸² 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 | class Solution: def longestPalindrome(self, s: str) - str: # ä¸¤ç§åˆ¤æ–­æ¡ä»¶ # DP åŠ¨æ€è§„åˆ’ # CABAC # B # ABA # CABAC palindrome = '' for i in range(len(s)): aa1 = self.getlongestpalindrome(s,i,i) len1 = len(aa1) if len1len(palindrome): palindrome = aa1 aa2 = self.getlongestpalindrome(s,i,i+1) len2 = len(aa2) if len2len(palindrome): palindrome = aa2 return palindrome def getlongestpalindrome(self, s, l, r): while l = 0 and rlen(s) and s[l]==s[r]: l -= 1 r += 1 return s[l+1:r] ~ â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | class Solution: def containsNearbyDuplicate(self, nums: List[int], k: int) - bool: # ç”¨dictionaryæ¥åšæ¯”è¾ƒå®¹æ˜“ä¸€ç‚¹ lookup = {} for i , num in enumerate(nums): if num not in lookup: lookup[num] = i # å°†numå­˜åˆ°dicé‡Œé¢ else: if i-lookup[num]=k: return True lookup[num] = i return False ~ â€”|â€”","tags":["Python"],"categories":["ç®—æ³•"]},{"title":"dynamic planning","path":"/2019/11/17/dynamic/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 | arr= [1,2,4,1,7,8,3] def rec_opt(arr,i): if i == 0: return arr[0] elif i == 1: return max(arr[0],arr[1]) else: A = rec_opt(arr,i-2)+arr[i] B = rec_opt(arr,i-1) return max(A,B) print(rec_opt(arr,6)) â€”|â€” é€’å½’ä¼šäº§ç”Ÿå¾ˆå¤šçš„é‡å çš„å­é—®é¢˜ï¼Œè¿ç®—è§„æ¨¡2^n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | #éé€’å½’çš„æ–¹æ³• import numpy as np def dp_opt(arr): opt = np.zeros(len(arr)) opt[0] = arr[0] opt[1] = max(arr[0], arr[1]) for i in range(2, len(arr)): A = opt[i-2] +arr[i] B = opt[i-1] opt[i] = max(A,B) return opt[len(arr)-1] dp_opt(arr) â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | arr = [3,34,4,12,5,2] def rec_subset(arr, i,s): if s == 0: return True elif i == 0: return arr[0] == s elif arr[i]s: return rec_subset(arr,i-1,s) else: A= rec_subset(arr,i-1, s-arr[i]) B = rec_subset(arr, i-1,s) return A or B rec_subset(arr, len(arr)-1,9) ~ â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 | # éé€’å½’çš„æ–¹æ³•ï¼Œç”¨äºŒç»´æ•°ç»„æ¥ä¿å­˜ def dp_subset(arr,s): subset = np.zeros((len(arr),s+1),dtype=bool) subset[:,0] = True subset[0,:] = Fasle subset[0,arr[0]] = True for i in range(1,len(arr)): for ss in range(1,s+1): if arr[i] ss: subset[i,ss] = subset[i-1,ss] else: A= subset(i-1, s-arr[i]) B = subset(i-1,s) subset[i,s] = A or B r ,c =subset.shape return subset[r-1,c-1] rec_subset(arr, len(arr)-1,9) â€”|â€”","tags":["Python"],"categories":["ç®—æ³•"]},{"title":"python_learn","path":"/2019/11/17/python-learn/","content":"æŸ¥æ‰¾å›¾ä¸­ä¸¤ä¸ªèŠ‚ç‚¹çš„æœ€å°çš„è·ç¦»è¿™é‡Œé¢ä½¿ç”¨äº†pythonçš„ä¼˜å…ˆé˜Ÿåˆ—ï¼Œè¿™é‡Œçš„é˜Ÿåˆ—æŒ‰ç…§åé¢çš„æ•°å€¼å¤§å°è¿›è¡Œæ’åºï¼Œè€Œä¸æ˜¯åƒæ™®é€šçš„é˜Ÿåˆ—ä¸€æ ·å…ˆè¿›å…ˆå‡ºã€‚åé¢çš„æ•°å€¼ï¼Œæ˜¯èŠ‚ç‚¹åˆ°å‡ºå‘èŠ‚ç‚¹çš„è·ç¦»é•¿åº¦ã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 | #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/17 11:50 # @Author : caius # @Site : # @File : find_min_bfs.py # @Software: PyCharm import heapq import math # pqueue = [] # heapq.heappush(pqueue,(1,\"A\")) # heapq.heappush(pqueue,(7,\"B\")) # heapq.heappush(pqueue,(3,\"C\")) # heapq.heappush(pqueue,(6,\"D\")) # heapq.heappush(pqueue,(2,\"E\")) graph = { \"A\":{\"B\": 5, \"C\": 1}, \"B\":{\"A\": 5,\"C\": 2,\"D\": 1}, \"C\":{\"A\": 1,\"B\": 2,\"D\": 4,\"E\": 8}, \"D\":{\"B\": 1,\"C\": 4,\"E\": 3,\"F\": 6}, \"E\":{\"C\": 8,\"D\": 3}, \"F\":{\"D\": 6} } def init_distance(graph,s): distance = {s:0} for vertex in graph: if vertex != s: distance[vertex] = math.inf return distance def dijkstra(graph, s): pqueue = [] heapq.heappush(pqueue,(0,s)) seen =set() parent ={s:None} distance = init_distance(graph,s) while(len(pqueue)0): pair = heapq.heappop(pqueue) # æ‹¿åˆ°ä¸€å¯¹ç‚¹ï¼Œpair dist = pair[0] vertex = pair[1] seen.add(vertex) nodes = graph[vertex].keys() for w in nodes: if w not in seen: if dist+graph[vertex][w] distance[w]: heapq.heappush(pqueue,(dist+graph[vertex][w],w)) parent[w] = vertex distance[w] = dist+graph[vertex][w] return parent,distance parent, distance = dijkstra(graph,\"A\") print(parent) print(distance) â€”|â€” python è£…é¥°å™¨è£…é¥°å™¨(Decorators)æ˜¯ Python çš„ä¸€ä¸ªé‡è¦éƒ¨åˆ†ã€‚ç®€å•åœ°è¯´ï¼šä»–ä»¬æ˜¯ä¿®æ”¹å…¶ä»–å‡½æ•°çš„åŠŸèƒ½çš„å‡½æ•°ã€‚ä»–ä»¬æœ‰åŠ©äºè®©æˆ‘ä»¬çš„ä»£ç æ›´ç®€çŸ­ï¼Œä¹Ÿæ›´Pythonicï¼ˆPythonèŒƒå„¿ï¼‰ã€‚è£…é¥°å™¨å¯ä»¥è®©ä½ çš„ä»£ç æ›´ç®€æ´ã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 | #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/17 9:59 # @Author : caius # @Site : # @File : deco.py.py # @Software: PyCharm import time # è£…é¥°å™¨ def display_time(func): def wrapper(*args): t1 = time.time() result = func(*args) t2 = time.time() print(\"Total time: {:.4} s\".format(t2-t1)) return result return wrapper # è¾“å‡ºè´¨æ•° def is_prime(num): if num2: return False elif num==2: return True else: for i in range(2, num): if num%i ==0: return False return True @display_time def prime_nums(): for i in range(2,10000): if is_prime(i): print(i) @display_time def count_prime_nums(maxnum): count = 0 for i in range(2, maxnum): if is_prime(i): count += 1 return count count = count_prime_nums(5000) print(count) ~ â€”|â€” ç”¨turtle ç”»å›¾1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 | #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/17 13:27 # @Author : caius # @Site : # @File : draw.py # @Software: PyCharm from turtle import * import math # turtle ç”»å›¾ # forward(100) # left(90) # forward(100) # left(90) # forward(100) # left(90) # forward(100) # left(90) # exitonclick()# ä¸ç‚¹å‡»çª—å£çš„è¯å°±ä¸ä¼šé€€å‡º # # ç”»ç­‰è¾¹ä¸‰è§’å½¢ # forward(100) # left(120) # forward(100) # left(120) # forward(100) # left(120) # exitonclick()# ä¸ç‚¹å‡»çª—å£çš„è¯å°±ä¸ä¼šé€€å‡º # ç”»äº”è§’æ˜Ÿ # forward(100) # right(180-36) # forward(100) # right(180-36) # forward(100) # right(180-36) # forward(100) # right(180-36) # forward(100) # right(180-36) # for i in range(5): # forward(100) # right(180-36) # angle = 360/8 length = 100 speed(0) for i in range(8): if i %2==0: color('yellow') else: color('red') begin_fill() forward(100) left(angle) forward(length) left(180-angle) forward(length) left(angle) forward(length) left(180-angle) end_fill() left(angle) forward(length) left(180-(180-angle)/2) alpha = angle*3.1415926536 /180 step = 2*length*math.sin(alpha/2) color('blue') begin_fill() for i in range(8): forward(step) left(angle) end_fill() exitonclick()# ä¸ç‚¹å‡»çª—å£çš„è¯å°±ä¸ä¼šé€€å‡º â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 | #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/17 14:20 # @Author : caius # @Site : # @File : Lsystem2.py # @Software: PyCharm from turtle import * length = 7 angle = 60 def split_path(path): i = 0 list = [] while i len(path): if path[i] == \"F\": list.append(path[i:i+2]) i = i+2 else: list.append(path[i]) i = i+1 return list def apply_rule(path, rules): lst = split_path(path) for i in range(len(lst)): symbol = lst[i] if symbol in rules: lst[i] = rules[symbol] path =\"\".join(symbol for symbol in lst) return path rules={ \"Fl\": \"Fr+Fl+Fr\", \"Fr\":\"Fl-Fr-Fl\" } def draw_patj(path): lst = split_path(path) for symbol in lst: if symbol ==\"Fl\" or symbol=='Fr': forward(length) elif symbol==\"-\": left(angle) elif symbol=='+': right(angle) speed(0) path = 'Fr' # speed(0) # #lst = split_path(path) for i in range(6): path = apply_rule(path,rules) print(path) draw_patj(path) exitonclick() ~ â€”|â€” python ç±»çš„æ„é€ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 | #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/17 14:52 # @Author : caius # @Site : # @File : Bank.py # @Software: PyCharm class BankAccount: # Constructor æ„é€ å™¨ def __init__(self,accountNumber, accountName, balance): self.accountNumber = accountNumber self.accountName = accountName self.balance = balance def __str__(self): return \"(name: {}, balance: {})\".format(self.accountName,self.balance) #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/17 14:56 # @Author : caius # @Site : # @File : main.py # @Software: PyCharm from Bank import BankAccount b1 = BankAccount(\"56789\",\"Tony\", 100.0) print((b1)) â€”|â€”","tags":["Python"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"rnn","path":"/2019/11/17/rnn/","content":"RNNåŠRNNçš„å‡ ä¸ªå˜ä½“åºåˆ—å½¢çš„æ•°æ®ä¸å¤ªå¥½ç”¨åŸå§‹çš„ç¥ç»ç½‘ç»œå¤„ç†ï¼Œä¸ºäº†å»ºæ¨¡åºåˆ—é—®é¢˜ï¼ŒRNNå¼•å…¥äº†éšçŠ¶æ€hï¼ˆhiddenï¼‰çš„æ¦‚å¿µï¼Œhå¯ä»¥å¯¹åºåˆ—å½¢çš„æ•°æ®æå–ç‰¹å¾ï¼Œæ¥ç€è½¬æ¢ä¸ºè¾“å‡ºã€‚ åœ†åœˆæˆ–æ–¹å—è¡¨ç¤ºçš„æ˜¯å‘é‡ ä¸€ä¸ªç®­å¤´å°±è¡¨ç¤ºå¯¹è¯¥å‘é‡åšä¸€æ¬¡å˜æ¢ã€‚å¦‚ä¸Šå›¾ä¸­h0å’Œx1åˆ†åˆ«æœ‰ä¸€ä¸ªç®­å¤´è¿æ¥ï¼Œå°±è¡¨ç¤ºå¯¹h0å’Œx1å„åšäº†ä¸€æ¬¡å˜æ¢ Tipsï¼šUã€Wã€béƒ½æ˜¯ä¸€æ ·çš„ï¼Œæ¯ä¸ªæ­¥éª¤çš„å‚æ•°éƒ½æ˜¯å…±äº«çš„ï¼Œè¿™æ˜¯RNNçš„é‡è¦ç‰¹ç‚¹ã€‚ ä¾æ¬¡è®¡ç®—å‰©ä¸‹æ¥çš„ï¼ˆä½¿ç”¨ç›¸åŒçš„å‚æ•°Uã€Wã€bï¼‰ï¼šè¾“å‡ºå€¼çš„æ–¹æ³•å°±æ˜¯ç›´æ¥é€šè¿‡hè¿›è¡Œè®¡ç®—ï¼šä¸€ä¸ªç®­å¤´å°±è¡¨ç¤ºå¯¹å¯¹åº”çš„å‘é‡åšä¸€æ¬¡ç±»ä¼¼äºf(Wx+b)çš„å˜æ¢ï¼Œè¿™é‡Œçš„è¿™ä¸ªç®­å¤´å°±è¡¨ç¤ºå¯¹h1è¿›è¡Œä¸€æ¬¡å˜æ¢ï¼Œå¾—åˆ°è¾“å‡ºy1è¾“å…¥æ˜¯x1, x2, â€¦..xnï¼Œè¾“å‡ºä¸ºy1, y2, â€¦ynï¼Œä¹Ÿå°±æ˜¯è¯´ï¼Œè¾“å…¥å’Œè¾“å‡ºåºåˆ—å¿…é¡»è¦æ˜¯ç­‰é•¿çš„ã€‚ä¸€äº›é—®é¢˜é€‚åˆç”¨ç»å…¸çš„RNNç»“æ„å»ºæ¨¡ï¼Œå¦‚ï¼š è®¡ç®—è§†é¢‘ä¸­æ¯ä¸€å¸§çš„åˆ†ç±»æ ‡ç­¾ã€‚å› ä¸ºè¦å¯¹æ¯ä¸€å¸§è¿›è¡Œè®¡ç®—ï¼Œå› æ­¤è¾“å…¥å’Œè¾“å‡ºåºåˆ—ç­‰é•¿ã€‚ è¾“å…¥ä¸ºå­—ç¬¦ï¼Œè¾“å‡ºä¸ºä¸‹ä¸€ä¸ªå­—ç¬¦çš„æ¦‚ç‡ã€‚è¿™å°±æ˜¯è‘—åçš„Char RNNï¼ˆè¯¦ç»†ä»‹ç»è¯·å‚è€ƒï¼šThe Unreasonable Effectiveness of Recurrent Neural Networksï¼‰ã€‚ N vs 1 å¤šè¾“å…¥å•è¾“å‡ºè¿™ç§ç»“æ„é€šå¸¸ç”¨æ¥å¤„ç†åºåˆ—åˆ†ç±»é—®é¢˜ã€‚å¦‚è¾“å…¥ä¸€æ®µæ–‡å­—åˆ¤åˆ«å®ƒæ‰€å±çš„ç±»åˆ«ï¼Œè¾“å…¥ä¸€ä¸ªå¥å­åˆ¤æ–­å…¶æƒ…æ„Ÿå€¾å‘ï¼Œè¾“å…¥ä¸€æ®µè§†é¢‘å¹¶åˆ¤æ–­å®ƒçš„ç±»åˆ«ç­‰ç­‰ã€‚ 1 VS N å•è¾“å…¥å¤šè¾“å‡ºè¾“å…¥ä¸æ˜¯åºåˆ—è€Œè¾“å‡ºä¸ºåºåˆ—çš„æƒ…å†µæ€ä¹ˆå¤„ç†ï¼Ÿæˆ‘ä»¬å¯ä»¥åªåœ¨åºåˆ—å¼€å§‹è¿›è¡Œè¾“å…¥è®¡ç®—ï¼šè¿˜æœ‰ä¸€ç§ç»“æ„æ˜¯æŠŠè¾“å…¥xä½œä¸ºæ¯ä¸ªé˜¶æ®µçš„è¾“å…¥ï¼šè¿™ç§ç»“æ„å¤„ç†çš„é—®é¢˜ï¼š ä»å›¾åƒç”Ÿæˆæ–‡å­—ï¼ˆimage captionï¼‰ï¼Œæ­¤æ—¶è¾“å…¥çš„Xå°±æ˜¯å›¾åƒçš„ç‰¹å¾ï¼Œè€Œè¾“å‡ºçš„yåºåˆ—å°±æ˜¯ä¸€æ®µå¥å­ ä»ç±»åˆ«ç”Ÿæˆè¯­éŸ³æˆ–éŸ³ä¹ç­‰ N vs MRNNæœ€é‡è¦çš„ä¸€ä¸ªå˜ç§ï¼šN vs Mã€‚è¿™ç§ç»“æ„åˆå«Encoder-Decoderæ¨¡å‹ï¼Œä¹Ÿå¯ä»¥ç§°ä¹‹ä¸ºSeq2Seqæ¨¡å‹ã€‚ä¸ºæ­¤ï¼ŒEncoder-Decoderç»“æ„å…ˆå°†è¾“å…¥æ•°æ®ç¼–ç æˆä¸€ä¸ªä¸Šä¸‹æ–‡å‘é‡cï¼šcæœ‰å¤šç§æ–¹å¼ï¼Œæœ€ç®€å•çš„æ–¹æ³•å°±æ˜¯æŠŠEncoderçš„æœ€åä¸€ä¸ªéšçŠ¶æ€èµ‹å€¼ç»™cï¼Œè¿˜å¯ä»¥å¯¹æœ€åçš„éšçŠ¶æ€åšä¸€ä¸ªå˜æ¢å¾—åˆ°cï¼Œä¹Ÿå¯ä»¥å¯¹æ‰€æœ‰çš„éšçŠ¶æ€åšå˜æ¢ã€‚ æ‹¿åˆ°cä¹‹åï¼Œå°±ç”¨å¦ä¸€ä¸ªRNNç½‘ç»œå¯¹å…¶è¿›è¡Œè§£ç ï¼Œè¿™éƒ¨åˆ†RNNç½‘ç»œè¢«ç§°ä¸ºDecoderã€‚å…·ä½“åšæ³•å°±æ˜¯å°†cå½“åšä¹‹å‰çš„åˆå§‹çŠ¶æ€h0è¾“å…¥åˆ°Decoderä¸­ï¼šè¿˜æœ‰ä¸€ç§åšæ³•æ˜¯å°†cå½“åšæ¯ä¸€æ­¥çš„è¾“å…¥ï¼šEncoder-Decoderç»“æ„ä¸é™åˆ¶è¾“å…¥å’Œè¾“å‡ºçš„åºåˆ—é•¿åº¦ï¼Œå› æ­¤åº”ç”¨çš„èŒƒå›´éå¸¸å¹¿æ³›ï¼Œæ¯”å¦‚ï¼š æœºå™¨ç¿»è¯‘ã€‚Encoder-Decoderçš„æœ€ç»å…¸åº”ç”¨ï¼Œäº‹å®ä¸Šè¿™ä¸€ç»“æ„å°±æ˜¯åœ¨æœºå™¨ç¿»è¯‘é¢†åŸŸæœ€å…ˆæå‡ºçš„ æ–‡æœ¬æ‘˜è¦ã€‚è¾“å…¥æ˜¯ä¸€æ®µæ–‡æœ¬åºåˆ—ï¼Œè¾“å‡ºæ˜¯è¿™æ®µæ–‡æœ¬åºåˆ—çš„æ‘˜è¦åºåˆ— é˜…è¯»ç†è§£ã€‚å°†è¾“å…¥çš„æ–‡ç« å’Œé—®é¢˜åˆ†åˆ«ç¼–ç ï¼Œå†å¯¹å…¶è¿›è¡Œè§£ç å¾—åˆ°é—®é¢˜çš„ç­”æ¡ˆã€‚ è¯­éŸ³è¯†åˆ«ã€‚è¾“å…¥æ˜¯è¯­éŸ³ä¿¡å·åºåˆ—ï¼Œè¾“å‡ºæ˜¯æ–‡å­—åºåˆ—ã€‚ â€¦â€¦â€¦ tensorflow å®ç°RNNRNNCellRNNCell æ˜¯TensorFlowä¸­å®ç°RNNçš„åŸºæœ¬å•å…ƒï¼Œæ¯ä¸ªRNNCelléƒ½æœ‰ä¸€ä¸ªcallæ–¹æ³•ï¼Œä½¿ç”¨æ–¹å¼æ˜¯ï¼š(output, next_state) = call(input, state)å€ŸåŠ©å›¾ç‰‡æ¥è¯´å¯èƒ½æ›´å®¹æ˜“ç†è§£ã€‚å‡è®¾æˆ‘ä»¬æœ‰ä¸€ä¸ªåˆå§‹çŠ¶æ€h0ï¼Œè¿˜æœ‰è¾“å…¥x1ï¼Œè°ƒç”¨call(x1, h0)åå°±å¯ä»¥å¾—åˆ°(output1, h1)ï¼šå†è°ƒç”¨ä¸€æ¬¡call(x2, h1)å°±å¯ä»¥å¾—åˆ°(output2, h2)ï¼šä¹Ÿå°±æ˜¯è¯´ï¼Œæ¯è°ƒç”¨ä¸€æ¬¡RNNCellçš„callæ–¹æ³•ï¼Œå°±ç›¸å½“äºåœ¨æ—¶é—´ä¸Šâ€œæ¨è¿›äº†ä¸€æ­¥â€ï¼Œè¿™å°±æ˜¯RNNCellçš„åŸºæœ¬åŠŸèƒ½ã€‚RNNCellåªæ˜¯ä¸€ä¸ªæŠ½è±¡ç±»ï¼Œå…¶ä»–çš„RNNcelléƒ½ä¼šç»§æ‰¿è¯¥æ–¹æ³•ï¼Œç„¶åå…·ä½“å®ç°å…¶ä¸­çš„call()å‡½æ•°ã€‚ä¸»è¦æœ‰state_sizeå’Œoutput_sizeä¸¤ä¸ªå±æ€§ï¼Œåˆ†åˆ«ä»£è¡¨äº†éšè—å±‚å’Œè¾“å‡ºå±‚çš„ç»´åº¦ã€‚ç„¶åå°±æ˜¯zero_state()å’Œcall()ä¸¤ä¸ªå‡½æ•°ï¼Œåˆ†åˆ«ç”¨äºåˆå§‹åŒ–åˆå§‹çŠ¶æ€h0ä¸ºå…¨é›¶å‘é‡å’Œå®šä¹‰å®é™…çš„RNNCellçš„æ“ä½œï¼ˆæ¯”å¦‚RNNå°±æ˜¯ä¸€ä¸ªæ¿€æ´»ï¼ŒGRUçš„ä¸¤ä¸ªé—¨ï¼ŒLSTMçš„ä¸‰ä¸ªé—¨æ§ç­‰ï¼Œä¸åŒçš„RNNçš„åŒºåˆ«ä¸»è¦ä½“ç°åœ¨è¿™ä¸ªå‡½æ•°ï¼‰ã€‚ BasicRNNCellæŠŠstate_sizeå’Œoutput_sizeå®šä¹‰æˆç›¸åŒï¼Œè€Œä¸”htå’Œoutputä¹Ÿæ˜¯ç›¸åŒçš„æœ€æ™®é€šçš„RNNå®šä¹‰æ–¹å¼ã€‚ä¹Ÿå°±æ˜¯è¯´output = new_state = f(W input + U state + B) GRUç›¸æ¯”BasicRNNCellåªæ”¹å˜äº†callå‡½æ•°éƒ¨åˆ†ï¼Œå¢åŠ äº†é‡ç½®é—¨å’Œæ›´æ–°é—¨ä¸¤éƒ¨åˆ†ï¼Œåˆ†åˆ«ç”±rå’Œuè¡¨ç¤ºã€‚ç„¶åcè¡¨ç¤ºè¦æ›´æ–°çš„çŠ¶æ€å€¼ã€‚å…¶å¯¹åº”çš„å…¬å¼å¦‚å¦‚ä¸‹æ‰€ç¤ºï¼š r = f(W1 input + U1 state + B1)r=f(W1âˆ—input+U1âˆ—state+B1)u = f(W2 input + U2 state + B2)u=f(W2âˆ—input+U2âˆ—state+B2)c = f(W3 input + U3 r _state + B3)c=f(W3âˆ—input+U3âˆ—râˆ—state+B3)h_new = u _ h + (1 - u) * chnâ€‹ew=uâˆ—h+(1âˆ’u)âˆ—c BasicLSTMCellç›¸æ¯”GRUï¼ŒLSTMåˆå¤šäº†ä¸€ä¸ªè¾“å‡ºé—¨ï¼Œè€Œä¸”åˆæ–°å¢æ·»äº†ä¸€ä¸ªCè¡¨ç¤ºå…¶å†…éƒ¨çŠ¶æ€ï¼Œç„¶åå°†hå’Œcä»¥tupleçš„å½¢å¼è¿”å›ä½œä¸ºLSTMå†…éƒ¨çš„çŠ¶æ€å˜é‡ã€‚ æˆ‘ä»¬ç”¨çš„æ—¶å€™éƒ½æ˜¯ç”¨çš„å®ƒçš„ä¸¤ä¸ªå­ç±»BasicRNNCellå’ŒBasicLSTMCellã€‚é¡¾åæ€ä¹‰ï¼Œå‰è€…æ˜¯RNNçš„åŸºç¡€ç±»ï¼Œåè€…æ˜¯LSTMçš„åŸºç¡€ç±»ã€‚çœ‹ä¸‹RNNCellã€BasicRNNCellã€BasicLSTMCellè¿™ä¸‰ä¸ªç±»çš„æ³¨é‡Šéƒ¨åˆ†ï¼Œåº”è¯¥å°±å¯ä»¥ç†è§£å®ƒä»¬çš„åŠŸèƒ½äº†ã€‚é™¤äº†callæ–¹æ³•å¤–ï¼Œå¯¹äºRNNCellï¼Œè¿˜æœ‰ä¸¤ä¸ªç±»å±æ€§æ¯”è¾ƒé‡è¦ï¼š state_size output_sizeå‰è€…æ˜¯éšå±‚çš„å¤§å°ï¼Œåè€…æ˜¯è¾“å‡ºçš„å¤§å°ã€‚æ¯”å¦‚æˆ‘ä»¬é€šå¸¸æ˜¯å°†ä¸€ä¸ªbatché€å…¥æ¨¡å‹è®¡ç®—ï¼Œè®¾è¾“å…¥æ•°æ®çš„å½¢çŠ¶ä¸º(batch_size, input_size)ï¼Œé‚£ä¹ˆè®¡ç®—æ—¶å¾—åˆ°çš„éšå±‚çŠ¶æ€å°±æ˜¯(batch_size, state_size)ï¼Œè¾“å‡ºå°±æ˜¯(batch_size, output_size)ã€‚ 1234567891011121314151617181920212223242526272829303132 | In [2]: import numpy as np In [3]: cell = tf.nn.rnn_cell.BasicRNNCell(num_units=128) # state_size = 128 In [4]: print(cell.state_size) # 128 128 In [5]: inputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 æ˜¯ batch_size In [6]: h0 = cell.zero_state(32, np.float32) # é€šè¿‡zero_stateå¾—åˆ°ä¸€ä¸ªå…¨0çš„åˆå§‹çŠ¶æ€ï¼Œå½¢çŠ¶ä¸º(batch_size, state_size) In [7]: output, h1 = cell.call(inputs, h0) #è°ƒç”¨callå‡½æ•° --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) ipython-input-7-378fe3b1c400 in module ---- 1 output, h1 = cell.call(inputs, h0) #è°ƒç”¨callå‡½æ•° ~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state) 349 350 gate_inputs = math_ops.matmul( -- 351 array_ops.concat([inputs, state], 1), self._kernel) 352 gate_inputs = nn_ops.bias_add(gate_inputs, self._bias) 353 output = self._activation(gate_inputs) AttributeError: 'BasicRNNCell' object has no attribute '_kernel' In [8]: output, h1 = cell.__call__(inputs, h0) #è°ƒç”¨callå‡½æ•° In [9]: print(h1.shape) # (32, 128) (32, 128) In [10]: â€”|â€” BasicLSTMCell 1 2 3 4 5 6 7 8 9 10 11 12 13 | In [10]: lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=128) In [11]: inputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 æ˜¯ batch_size In [12]: h0 = lstm_cell.zero_state(32, np.float32) # é€šè¿‡zero_stateå¾—åˆ°ä¸€ä¸ªå…¨0çš„åˆå§‹çŠ¶æ€ In [13]: output, h1 = lstm_cell.__call__(inputs, h0) In [14]: print(h1.h) # shape=(32, 128) Tensor(\"basic_lstm_cell/Mul_2:0\", shape=(32, 128), dtype=float32) In [15]: print(h1.c) # shape=(32, 128) Tensor(\"basic_lstm_cell/Add_1:0\", shape=(32, 128), dtype=float32) â€”|â€” å‚è€ƒé“¾æ¥ï¼šhttps://zhuanlan.zhihu.com/p/28054589https://zhuanlan.zhihu.com/p/28196873","tags":["TensorFlow"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"æ–‡æœ¬æ£€æµ‹è°ƒç ”","path":"/2019/11/13/20191113-æ–‡æœ¬æ£€æµ‹è°ƒç ”/","content":"æ–‡æœ¬æ£€æµ‹é—®é¢˜ï¼Œå¹¿ä¹‰ä¸Šæ¥è¯´å¯ä»¥çœ‹åšæ˜¯ä¸€ä¸ªç›®æ ‡æ£€æµ‹çš„é—®é¢˜ï¼Œä½†æ˜¯ç›¸å½“äºç›®æ ‡æ£€æµ‹è¦ç®€å•çš„å¤šã€‚å› ä¸ºç›®æ ‡æ£€æµ‹å¾€å¾€é™¤äº†èƒŒæ™¯è¿˜æœ‰å…¶ä»–çš„ç±»ï¼Œè€Œæ–‡æœ¬æ£€æµ‹ï¼Œåªéœ€è¦æ£€æµ‹èƒŒæ™¯å’Œæ–‡æœ¬ç±»ä¸¤ä¸ªé—®é¢˜ã€‚å› æ­¤å¯ä»¥é‡‡ç”¨ç›®æ ‡æ£€æµ‹æˆ–è€…åˆ†å‰²çš„æ–¹æ³•æ¥è¿›è¡Œæ–‡æœ¬æ£€æµ‹ã€‚è€Œè§†é¢‘ä¸­çš„æ–‡æœ¬æ£€æµ‹ï¼Œä¹Ÿå¯ä»¥çœ‹åšæ˜¯è§†é¢‘ä¸­çš„ç›®æ ‡æ£€æµ‹ä¸­çš„ä¸€ç§ï¼Œæ„Ÿè§‰åº”è¯¥ä¹Ÿå¯ä»¥ç”¨è§†é¢‘ä¸­çš„ç›®æ ‡æ£€æµ‹+è·Ÿè¸ªæ¥åšã€‚ é€šå¸¸ç›®æ ‡è·Ÿè¸ªé¢ä¸´çš„æå¤§éš¾ç‚¹ï¼šç‰©ä½“å˜å½¢ã€äº®åº¦å˜åŒ–ã€å¿«é€Ÿç§»åŠ¨ã€èƒŒæ™¯å¹²æ‰°è¦†ç›–ã€‚å…¶ä¸­æœ€ä¸»è¦çš„ä¸‰ä¸ªéš¾é¢˜åˆ†åˆ«æ˜¯ç›®æ ‡èƒŒæ™¯çš„å˜åŒ–ï¼Œç‰©ä½“æœ¬èº«çš„å˜åŒ–ï¼Œå…‰ç…§å¼ºåº¦çš„å˜åŒ–ã€‚ å…‰æµæ³•å¸§é—´å·®åˆ†æ³•èƒŒæ™¯å·®åˆ†æ³•","tags":["è®¡ç®—æœºè§†è§‰"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"Pythonï¼šloggingæ¨¡å—","path":"/2019/11/04/20191104-Pythonï¼šloggingæ¨¡å—/","content":"In [12]: import logging ...: ...: logging.basicConfig(level=logging.DEBUG, ...: filename='output.log', ...: datefmt='%Y/%m/%d %H:%M:%S', ...: format='%(asctime)s - %(name)s - %(levelname)s - %(lineno)d - %(module)s - %(message)s') ...: logger = logging.getLogger(__name__) ...: ...: logger.info('This is a log info') ...: logger.debug('Debugging') ...: logger.warning('Warning exists') ...: logger.info('Finish') 2019-11-04 13:00:45,976 - main - INFO - This is a log info2019-11-04 13:00:45,977 - main - WARNING - Warning exists2019-11-04 13:00:45,977 - main - INFO - Finish è®¾ç½®levelç­‰çº§ï¼Œä»è€Œæ§åˆ¶logè¾“å‡ºçš„çº§åˆ«ã€‚ In [13]: import logging ...: ...: logging.basicConfig(level=logging.DEBUG, ...: filename='output.log', ...: datefmt='%Y/%m/%d %H:%M:%S', ...: format='%(asctime)s - %(name)s - %(levelname)s - %(lineno)d - %(module)s - %(message)s') ...: logger = logging.getLogger(__name__) ...: logger.setLevel(level=logging.DEBUG) ...: logger.info('This is a log info') ...: logger.debug('Debugging') ...: logger.warning('Warning exists') ...: logger.info('Finish') å¦‚æœä¸è®¾ç½®loggerçš„Levelçš„è¯ï¼Œ debugâ€™çš„ä¿¡æ¯ä¹Ÿä¸ä¼šè¢«è¾“å‡ºã€‚ éœ€è¦è®¾ç½® logger.setLevel(level=logging.DEBUG)ï¼Œç„¶åä¿¡æ¯å°±å¯ä»¥æ­£å¸¸çš„æ˜¾ç¤ºå‡ºæ¥äº†ã€‚ 2019-11-04 13:10:01,634 - main - INFO - This is a log info2019-11-04 13:10:01,634 - main - DEBUG - Debugging2019-11-04 13:10:01,635 - main - WARNING - Warning exists2019-11-04 13:10:01,639 - main - INFO - Finish CSDNåšå®¢åœ°å€ï¼šhttps://blog.csdn.net/eilot_c/article/details/102894687","tags":["Python"],"categories":["Python"]},{"title":"Pythonï¼šloggingæ¨¡å—","path":"/2019/11/04/Pythonï¼šloggingæ¨¡å—/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 | In [12]: import logging ...: ...: logging.basicConfig(level=logging.DEBUG, ...: filename='output.log', ...: datefmt='%Y/%m/%d %H:%M:%S', ...: format='%(asctime)s - %(name)s - %(levelname)s - %(lineno)d - %(module)s - %(message)s') ...: logger = logging.getLogger(__name__) ...: ...: logger.info('This is a log info') ...: logger.debug('Debugging') ...: logger.warning('Warning exists') ...: logger.info('Finish') â€”|â€” 2019-11-04 13:00:45,976 - main - INFO - This is a log info2019-11-04 13:00:45,977 - main - WARNING - Warning exists2019-11-04 13:00:45,977 - main - INFO - Finish è®¾ç½®levelç­‰çº§ï¼Œä»è€Œæ§åˆ¶logè¾“å‡ºçš„çº§åˆ«ã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 | In [13]: import logging ...: ...: logging.basicConfig(level=logging.DEBUG, ...: filename='output.log', ...: datefmt='%Y/%m/%d %H:%M:%S', ...: format='%(asctime)s - %(name)s - %(levelname)s - %(lineno)d - %(module)s - %(message)s') ...: logger = logging.getLogger(__name__) ...: logger.setLevel(level=logging.DEBUG) ...: logger.info('This is a log info') ...: logger.debug('Debugging') ...: logger.warning('Warning exists') ...: logger.info('Finish') â€”|â€” å¦‚æœä¸è®¾ç½®loggerçš„Levelçš„è¯ï¼Œ debugâ€™çš„ä¿¡æ¯ä¹Ÿä¸ä¼šè¢«è¾“å‡ºã€‚ éœ€è¦è®¾ç½® logger.setLevel(level=logging.DEBUG)ï¼Œç„¶åä¿¡æ¯å°±å¯ä»¥æ­£å¸¸çš„æ˜¾ç¤ºå‡ºæ¥äº†ã€‚ 2019-11-04 13:10:01,634 - main - INFO - This is a log info2019-11-04 13:10:01,634 - main - DEBUG - Debugging2019-11-04 13:10:01,635 - main - WARNING - Warning exists2019-11-04 13:10:01,639 - main - INFO - Finish CSDNåšå®¢åœ°å€ï¼šhttps://blog.csdn.net/eilot_c/article/details/102894687","tags":["Python"],"categories":["Python"]},{"title":"attention_mechanism_xmind","path":"/2019/10/31/20191031-attention-mechanism-xmind/","content":"","tags":["æ·±åº¦å­¦ä¹ "],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"object_track_xmind","path":"/2019/10/31/20191031-pyobject-track-xmind/","content":"","tags":["æŠ€æœ¯"],"categories":["æŠ€æœ¯"]},{"title":"object_detection_xmind","path":"/2019/10/31/20191031-object-dectection-xmind/","content":"","tags":["ç›®æ ‡æ£€æµ‹"],"categories":["æŠ€æœ¯"]},{"title":"pytorchåŸºç¡€å¯¼å›¾","path":"/2019/10/31/20191031-pytorchchchâ€”â€”-xmind/","content":"","tags":["æ·±åº¦å­¦ä¹ "],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"attention_mechanism_xmind","path":"/2019/10/31/attention-mechanism-xmind/","content":"","tags":["Attention"],"categories":["å…¶ä»–"]},{"title":"object_detection_xmind","path":"/2019/10/31/object-dectection-xmind/","content":"","tags":["ç›®æ ‡æ£€æµ‹"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"object_track_xmind","path":"/2019/10/31/pyobject-track-xmind/","content":"","tags":["ç›®æ ‡è·Ÿè¸ª"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"pytorchåŸºç¡€å¯¼å›¾","path":"/2019/10/31/pytorchchchâ€”â€”-xmind/","content":"","tags":["PyTorch"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"knnæ‰‹å†™æ•°å­—è¯†åˆ«","path":"/2019/10/20/20191020-knnæ‰‹å†™æ•°å­—è¯†åˆ«/","content":"Kè¿‘é‚»ç®—æ³•ç®—æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œç»™å®šä¸€ä¸ªè®­ç»ƒæ•°æ®é›†ï¼Œå¯¹äºæ–°çš„è¾“å…¥å®ä¾‹ï¼Œåœ¨è®­ç»ƒé›†ä¸­æ‰¾åˆ°ä¸è¯¥å®ä¾‹æœ€è¿‘çš„Kä¸ªå®ä¾‹ï¼Œè¿™Kä¸ªå®ä¾‹çš„å¤šæ•°å±äºæŸä¸ªç±»ï¼Œå°±æŠŠè¿™ä¸ªè¾“å…¥å½’ä¸ºå“ªä¸ªç±»ä¸­ã€‚ # 1 é‡è¦# 2 KNN CNN 2ç§# 3 æ ·æœ¬# 4 æ—§ç“¶è£…æ–°é…’ ï¼šæ•°å­—è¯†åˆ«çš„ä¸åŒ# 4.1 ç½‘ç»œ 4ã€‚2 æ¯ä¸€çº§ 4.3 å…ˆåŸç† åä»£ç # æœ¬è´¨ï¼šknn test æ ·æœ¬ Kä¸ª max4 3ä¸ª1 -ã€‹1# 1 load Data 1.1 éšæœºæ•° 1.2 4ç»„ è®­ç»ƒ æµ‹è¯• ï¼ˆå›¾ç‰‡ å’Œ æ ‡ç­¾ï¼‰# 2 knn test train distance 5*500 = 2500 784=28*28# 3 knn kä¸ªæœ€è¿‘çš„å›¾ç‰‡5 500 1-ã€‹500train ï¼ˆ4ï¼‰# 4 kä¸ªæœ€è¿‘çš„å›¾ç‰‡- parse centent label# 5 label -ã€‹ æ•°å­— p9 æµ‹è¯•å›¾ç‰‡-ã€‹æ•°æ®# 6 æ£€æµ‹æ¦‚ç‡ç»Ÿè®¡import tensorflow as tfimport numpy as npimport randomfrom tensorflow.examples.tutorials.mnist import input_data# load data 2 one_hot : 1 0000 1 fileNamemnist = input_data.read_data_sets('MNIST_data',one_hot=True)# å±æ€§è®¾ç½®trainNum = 55000testNum = 10000trainSize = 500testSize = 5k = 4# data åˆ†è§£ 1 trainSize 2èŒƒå›´0-trainNum 3 replace=FalsetrainIndex = np.random.choice(trainNum,trainSize,replace=False)testIndex = np.random.choice(testNum,testSize,replace=False)trainData = mnist.train.images[trainIndex]# è®­ç»ƒå›¾ç‰‡trainLabel = mnist.train.labels[trainIndex]# è®­ç»ƒæ ‡ç­¾testData = mnist.test.images[testIndex]testLabel = mnist.test.labels[testIndex]# 28*28 = 784print('trainData.shape=',trainData.shape)#500*784 1 å›¾ç‰‡ä¸ªæ•° 2 784?print('trainLabel.shape=',trainLabel.shape)#500*10print('testData.shape=',testData.shape)#5*784print('testLabel.shape=',testLabel.shape)#5*10print('testLabel=',testLabel)# 4 :testData [0] 3:testData[1] 6# tf input 784-imagetrainDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32)trainLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32)testDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32)testLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32)#knn distance 5*785. 5*1*784# 5 500 784 (3D) 2500*784f1 = tf.expand_dims(testDataInput,1) # ç»´åº¦æ‰©å±• (?, 1, 784) # 0å…¶å®ä»£è¡¨çš„ç¬¬ä¸€ç»´åº¦ï¼Œé‚£ä¹ˆ1ä»£è¡¨ç¬¬äºŒç»´åº¦ï¼Œ2ä»£è¡¨ç¬¬ä¸‰ç»´åº¦ã€‚ä»¥æ­¤ç±»æ¨ã€‚print()f2 = tf.subtract(trainDataInput,f1)# 784 sum(784) è¿”å› x-y çš„å…ƒç´ .f3 = tf.reduce_sum(tf.abs(f2),reduction_indices=2)# å®Œæˆæ•°æ®ç´¯åŠ  784 abs # axis=0æ—¶ï¼ŒæŒ‰ç¬¬ä¸€ä¸ªç»´åº¦æ±‚å’Œï¼Œ# 5*500f4 = tf.negative(f3)# å–å å–è´Ÿå€¼print('f1.shape=',f1.shape)#500*784 1 å›¾ç‰‡ä¸ªæ•° 2 784?print('f2.shape=',f2.shape)#500*10 (?, ?, 784)print('f3.shape=',f3.shape)#5*784 (?, ?)print('f4.shape=',f4.shape)#5*10 (?, ?)f5,f6 = tf.nn.top_k(f4,k=4) # é€‰å–f4 æœ€å¤§çš„å››ä¸ªå€¼ è¿”å›å€¼å’Œå…¶ç´¢å¼•ä½ç½®# f3 æœ€å°çš„å››ä¸ªå€¼# f6 index-trainLabelInputf7 = tf.gather(trainLabelInput,f6) # æ ¹æ®ç´¢å¼•æŠ½å–å…¶ä¸­çš„å¼ é‡# f8 num reduce_sum reduction_indices=1 'ç«–ç›´'f8 = tf.reduce_sum(f7,reduction_indices=1)# tf.argmax é€‰å–åœ¨æŸä¸€ä¸ªæœ€å¤§çš„å€¼ indexf9 = tf.argmax(f8,dimension=1)# f9 - test5 image - 5 numwith tf.Session() as sess: # f1 - testData 5å¼ å›¾ç‰‡ p1 = sess.run(f1,feed_dict={testDataInput:testData[0:5]}) print('p1=',p1.shape)# p1= (5, 1, 784) p2 = sess.run(f2,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print('p2=',p2.shape)#p2= (5, 500, 784) (1,100) p3 = sess.run(f3,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print('p3=',p3.shape)#p3= (5, 500) print('p3[0,0]=',p3[0,0]) #130.451 knn distance p3[0,0]= 155.812 p4 = sess.run(f4,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print('p4=',p4.shape) print('p4[0,0]',p4[0,0]) p5,p6 = sess.run((f5,f6),feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) #p5= (5, 4) æ¯ä¸€å¼ æµ‹è¯•å›¾ç‰‡ï¼ˆ5å¼ ï¼‰åˆ†åˆ«å¯¹åº”4å¼ æœ€è¿‘è®­ç»ƒå›¾ç‰‡ #p6= (5, 4) print('p5=',p5.shape) print('p6=',p6.shape) print('p5[0,0]',p5[0]) print('p6[0,0]',p6[0])# p6 index p7 = sess.run(f7,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print('p7=',p7.shape)#p7= (5, 4, 10) print('p7[]',p7) p8 = sess.run(f8,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print('p8=',p8.shape) print('p8[]=',p8) p9 = sess.run(f9,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print('p9=',p9.shape) print('p9[]=',p9) p10 = np.argmax(testLabel[0:5],axis=1) print('p10[]=',p10)j = 0for i in range(0,5): if p10[i] == p9[i]: j = j+1print('ac=',j*100/5) å‚è€ƒé“¾æ¥ï¼šOpenCV+TensorFlow å…¥é—¨äººå·¥æ™ºèƒ½å›¾åƒå¤„ç† æ‰‹å†™æ•°å­—è¯†åˆ«","tags":["æœºå™¨å­¦ä¹ "],"categories":["Python"]},{"title":"knnæ‰‹å†™æ•°å­—è¯†åˆ«","path":"/2019/10/20/knnæ‰‹å†™æ•°å­—è¯†åˆ«/","content":"Kè¿‘é‚»ç®—æ³•ç®—æ³•çš„æ ¸å¿ƒæ€æƒ³æ˜¯ï¼Œç»™å®šä¸€ä¸ªè®­ç»ƒæ•°æ®é›†ï¼Œå¯¹äºæ–°çš„è¾“å…¥å®ä¾‹ï¼Œåœ¨è®­ç»ƒé›†ä¸­æ‰¾åˆ°ä¸è¯¥å®ä¾‹æœ€è¿‘çš„Kä¸ªå®ä¾‹ï¼Œè¿™Kä¸ªå®ä¾‹çš„å¤šæ•°å±äºæŸä¸ªç±»ï¼Œå°±æŠŠè¿™ä¸ªè¾“å…¥å½’ä¸ºå“ªä¸ªç±»ä¸­ã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 | # 1 é‡è¦ # 2 KNN CNN 2ç§ # 3 æ ·æœ¬ # 4 æ—§ç“¶è£…æ–°é…’ ï¼šæ•°å­—è¯†åˆ«çš„ä¸åŒ # 4.1 ç½‘ç»œ 4ã€‚2 æ¯ä¸€çº§ 4.3 å…ˆåŸç† åä»£ç  # æœ¬è´¨ï¼šknn test æ ·æœ¬ Kä¸ª max4 3ä¸ª1 -ã€‹1 # 1 load Data 1.1 éšæœºæ•° 1.2 4ç»„ è®­ç»ƒ æµ‹è¯• ï¼ˆå›¾ç‰‡ å’Œ æ ‡ç­¾ï¼‰ # 2 knn test train distance 5*500 = 2500 784=28*28 # 3 knn kä¸ªæœ€è¿‘çš„å›¾ç‰‡5 500 1-ã€‹500train ï¼ˆ4ï¼‰ # 4 kä¸ªæœ€è¿‘çš„å›¾ç‰‡- parse centent label # 5 label -ã€‹ æ•°å­— p9 æµ‹è¯•å›¾ç‰‡-ã€‹æ•°æ® # 6 æ£€æµ‹æ¦‚ç‡ç»Ÿè®¡ import tensorflow as tf import numpy as np import random from tensorflow.examples.tutorials.mnist import input_data # load data 2 one_hot : 1 0000 1 fileName mnist = input_data.read_data_sets('MNIST_data',one_hot=True) # å±æ€§è®¾ç½® trainNum = 55000 testNum = 10000 trainSize = 500 testSize = 5 k = 4 # data åˆ†è§£ 1 trainSize 2èŒƒå›´0-trainNum 3 replace=False trainIndex = np.random.choice(trainNum,trainSize,replace=False) testIndex = np.random.choice(testNum,testSize,replace=False) trainData = mnist.train.images[trainIndex]# è®­ç»ƒå›¾ç‰‡ trainLabel = mnist.train.labels[trainIndex]# è®­ç»ƒæ ‡ç­¾ testData = mnist.test.images[testIndex] testLabel = mnist.test.labels[testIndex] # 28*28 = 784 print('trainData.shape=',trainData.shape)#500*784 1 å›¾ç‰‡ä¸ªæ•° 2 784? print('trainLabel.shape=',trainLabel.shape)#500*10 print('testData.shape=',testData.shape)#5*784 print('testLabel.shape=',testLabel.shape)#5*10 print('testLabel=',testLabel)# 4 :testData [0] 3:testData[1] 6 # tf input 784-image trainDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32) trainLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32) testDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32) testLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32) #knn distance 5*785. 5*1*784 # 5 500 784 (3D) 2500*784 f1 = tf.expand_dims(testDataInput,1) # ç»´åº¦æ‰©å±• (?, 1, 784) # 0å…¶å®ä»£è¡¨çš„ç¬¬ä¸€ç»´åº¦ï¼Œé‚£ä¹ˆ1ä»£è¡¨ç¬¬äºŒç»´åº¦ï¼Œ2ä»£è¡¨ç¬¬ä¸‰ç»´åº¦ã€‚ä»¥æ­¤ç±»æ¨ã€‚ print() f2 = tf.subtract(trainDataInput,f1)# 784 sum(784) è¿”å› x-y çš„å…ƒç´ . f3 = tf.reduce_sum(tf.abs(f2),reduction_indices=2)# å®Œæˆæ•°æ®ç´¯åŠ  784 abs # axis=0æ—¶ï¼ŒæŒ‰ç¬¬ä¸€ä¸ªç»´åº¦æ±‚å’Œï¼Œ # 5*500 f4 = tf.negative(f3)# å–å å–è´Ÿå€¼ print('f1.shape=',f1.shape)#500*784 1 å›¾ç‰‡ä¸ªæ•° 2 784? print('f2.shape=',f2.shape)#500*10 (?, ?, 784) print('f3.shape=',f3.shape)#5*784 (?, ?) print('f4.shape=',f4.shape)#5*10 (?, ?) f5,f6 = tf.nn.top_k(f4,k=4) # é€‰å–f4 æœ€å¤§çš„å››ä¸ªå€¼ è¿”å›å€¼å’Œå…¶ç´¢å¼•ä½ç½® # f3 æœ€å°çš„å››ä¸ªå€¼ # f6 index-trainLabelInput f7 = tf.gather(trainLabelInput,f6) # æ ¹æ®ç´¢å¼•æŠ½å–å…¶ä¸­çš„å¼ é‡ # f8 num reduce_sum reduction_indices=1 'ç«–ç›´' f8 = tf.reduce_sum(f7,reduction_indices=1) # tf.argmax é€‰å–åœ¨æŸä¸€ä¸ªæœ€å¤§çš„å€¼ index f9 = tf.argmax(f8,dimension=1) # f9 - test5 image - 5 num with tf.Session() as sess: # f1 - testData 5å¼ å›¾ç‰‡ p1 = sess.run(f1,feed_dict={testDataInput:testData[0:5]}) print('p1=',p1.shape)# p1= (5, 1, 784) p2 = sess.run(f2,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print('p2=',p2.shape)#p2= (5, 500, 784) (1,100) p3 = sess.run(f3,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print('p3=',p3.shape)#p3= (5, 500) print('p3[0,0]=',p3[0,0]) #130.451 knn distance p3[0,0]= 155.812 p4 = sess.run(f4,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print('p4=',p4.shape) print('p4[0,0]',p4[0,0]) p5,p6 = sess.run((f5,f6),feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) #p5= (5, 4) æ¯ä¸€å¼ æµ‹è¯•å›¾ç‰‡ï¼ˆ5å¼ ï¼‰åˆ†åˆ«å¯¹åº”4å¼ æœ€è¿‘è®­ç»ƒå›¾ç‰‡ #p6= (5, 4) print('p5=',p5.shape) print('p6=',p6.shape) print('p5[0,0]',p5[0]) print('p6[0,0]',p6[0])# p6 index p7 = sess.run(f7,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print('p7=',p7.shape)#p7= (5, 4, 10) print('p7[]',p7) p8 = sess.run(f8,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print('p8=',p8.shape) print('p8[]=',p8) p9 = sess.run(f9,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print('p9=',p9.shape) print('p9[]=',p9) p10 = np.argmax(testLabel[0:5],axis=1) print('p10[]=',p10) j = 0 for i in range(0,5): if p10[i] == p9[i]: j = j+1 print('ac=',j*100/5) â€”|â€” å‚è€ƒé“¾æ¥ï¼šOpenCV+TensorFlow å…¥é—¨äººå·¥æ™ºèƒ½å›¾åƒå¤„ç† æ‰‹å†™æ•°å­—è¯†åˆ«","tags":["TensorFlow"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"tensorflowè‡ªå®šä¹‰ç½‘ç»œæ¨¡å‹","path":"/2019/10/17/20191017-tensorflowè‡ªå®šä¹‰ç½‘ç»œæ¨¡å‹/","content":"SlimTF-Slim æ¨¡å—æ˜¯ TensorFlow ä¸­æœ€å¥½ç”¨çš„ API ä¹‹ä¸€ã€‚å°¤å…¶æ˜¯é‡Œé¢å¼•å…¥çš„ arg_scopeã€model_variablesã€repeatã€stackã€‚TF-Slim æ˜¯ TensorFlow ä¸­ä¸€ä¸ªç”¨æ¥æ„å»ºã€è®­ç»ƒã€è¯„ä¼°å¤æ‚æ¨¡å‹çš„è½»é‡åŒ–åº“ã€‚TF-Slim æ¨¡å—å¯ä»¥å’Œ TensorFlow ä¸­å…¶å®ƒAPIæ··åˆä½¿ç”¨ã€‚ Slimæ¨¡å—çš„å¯¼å…¥import tensorflow.contrib.slim as slim Slim æ„å»ºæ¨¡å‹å¯ä»¥ç”¨ slimã€variablesã€layers å’Œ scopes æ¥ååˆ†ç®€æ´åœ°å®šä¹‰æ¨¡å‹ã€‚ä¸‹é¢å¯¹å„ä¸ªéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼š Slimå˜é‡ï¼ˆVariablesï¼‰weights = slim.variable('weights', shape=[10, 10, 3 , 3], initializer=tf.truncated_normal_initializer(stddev=0.1), regularizer=slim.l2_regularizer(0.05), device='/CPU:0')~ Slim å±‚ï¼ˆLayersï¼‰ä½¿ç”¨åŸºç¡€ï¼ˆplainï¼‰çš„ TensorFlow ä»£ç ï¼š input = ...with tf.name_scope('conv1_1') as scope: kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32, stddev=1e-1), name='weights') conv = tf.nn.conv2d(input, kernel, [1, 1, 1, 1], padding='SAME') biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32), trainable=True, name='biases') bias = tf.nn.bias_add(conv, biases) conv1 = tf.nn.relu(bias, name=scope) ä¸ºäº†é¿å…ä»£ç çš„é‡å¤ã€‚Slim æä¾›äº†å¾ˆå¤šæ–¹ä¾¿çš„ç¥ç»ç½‘ç»œ layers çš„é«˜å±‚ opã€‚ä¾‹å¦‚ï¼šä¸ä¸Šé¢çš„ä»£ç å¯¹åº”çš„ Slim ç‰ˆçš„ä»£ç ï¼š input = ...net = slim.conv2d(input, 128, [3, 3], scope='conv1_1') slim.arg_scopeï¼ˆï¼‰ å‡½æ•°çš„ä½¿ç”¨è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯ç»™list_opsä¸­çš„å†…å®¹è®¾ç½®é»˜è®¤å€¼ã€‚ä½†æ˜¯æ¯ä¸ªlist_opsä¸­çš„æ¯ä¸ªæˆå‘˜éœ€è¦ç”¨@add_arg_scopeä¿®é¥°æ‰è¡Œã€‚æ‰€ä»¥ä½¿ç”¨slim.arg_scopeï¼ˆï¼‰æœ‰ä¸¤ä¸ªæ­¥éª¤ï¼š ä½¿ç”¨@slim.add_arg_scopeä¿®é¥°ç›®æ ‡å‡½æ•° ç”¨ slim.arg_scopeï¼ˆï¼‰ä¸ºç›®æ ‡å‡½æ•°è®¾ç½®é»˜è®¤å‚æ•°.ä¾‹å¦‚å¦‚ä¸‹ä»£ç ï¼›é¦–å…ˆç”¨@slim.add_arg_scopeä¿®é¥°ç›®æ ‡å‡½æ•°fun1ï¼ˆï¼‰ï¼Œç„¶ååˆ©ç”¨slim.arg_scopeï¼ˆï¼‰ä¸ºå®ƒè®¾ç½®é»˜è®¤å‚æ•°ã€‚ ```pythonimport tensorflow as tfslim =tf.contrib.slim@slim.add_arg_scopedef fun1(a=0,b=0):return (a+b)with slim.arg_scope([fun1],a=10):x=fun1(b=30)print(x) è¿è¡Œç»“æœ:40å‚è€ƒé“¾æ¥ï¼šhttps://blog.csdn.net/u013921430/article/details/80915696 å…¶ä»–ç”¨æ³•è§å‚è€ƒé“¾æ¥https://blog.csdn.net/wanttifa/article/details/90208398 æŸ¥çœ‹ckptä¸­å˜é‡çš„å‡ ç§æ–¹æ³•æŸ¥çœ‹ckptä¸­å˜é‡çš„æ–¹æ³•æœ‰ä¸‰ç§ï¼š åœ¨æœ‰modelçš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨tf.train.Saverè¿›è¡Œrestore ä½¿ç”¨tf.train.NewCheckpointReaderç›´æ¥è¯»å–ckptæ–‡ä»¶ï¼Œè¿™ç§æ–¹æ³•ä¸éœ€è¦modelã€‚ ä½¿ç”¨toolsé‡Œçš„freeze_graphæ¥è¯»å–ckptTips: å¦‚æœæ¨¡å‹ä¿å­˜ä¸º.ckptçš„æ–‡ä»¶ï¼Œåˆ™ä½¿ç”¨è¯¥æ–‡ä»¶å°±å¯ä»¥æŸ¥çœ‹.ckptæ–‡ä»¶é‡Œçš„å˜é‡ã€‚ckptè·¯å¾„ä¸º model.ckpt å¦‚æœæ¨¡å‹ä¿å­˜ä¸º.ckpt-xxx-data (å›¾ç»“æ„)ã€.ckpt-xxx.index (å‚æ•°å)ã€.ckpt-xxx-meta (å‚æ•°å€¼)æ–‡ä»¶ï¼Œåˆ™éœ€è¦åŒæ—¶æ‹¥æœ‰è¿™ä¸‰ä¸ªæ–‡ä»¶æ‰è¡Œã€‚å¹¶ä¸”ckptçš„è·¯å¾„ä¸º model.ckpt-xxx 1.åŸºäºmodelæ¥è¯»å–ckptæ–‡ä»¶é‡Œçš„å˜é‡1.é¦–å…ˆå»ºç«‹èµ·model2.ä»ckptä¸­æ¢å¤å˜é‡ 1 2 3 4 5 6 7 8 9 10 with tf.Graph().as_default() as g: #å»ºç«‹model images, labels = cifar10.inputs(eval_data=eval_data) logits = cifar10.inference(images) top_k_op = tf.nn.in_top_k(logits, labels, 1) #ä»ckptä¸­æ¢å¤å˜é‡ sess = tf.Session() saver = tf.train.Saver() #saver = tf.train.Saver(â€¦variablesâ€¦) # æ¢å¤éƒ¨åˆ†å˜é‡æ—¶ï¼Œåªéœ€è¦åœ¨Saveré‡ŒæŒ‡å®šè¦æ¢å¤çš„å˜é‡ save_path = â€˜ckptçš„è·¯å¾„â€™ saver.restore(sess, save_path) # ä»ckptä¸­æ¢å¤å˜é‡ æ³¨æ„ï¼šåŸºäºmodelæ¥è¯»å–ckptä¸­å˜é‡æ—¶ï¼Œmodelå’Œckptå¿…é¡»åŒ¹é…ã€‚### 2.ä½¿ç”¨tf.train.NewCheckpointReaderç›´æ¥è¯»å–ckptæ–‡ä»¶é‡Œçš„å˜é‡ï¼Œä½¿ç”¨tools.inspect_checkpointé‡Œçš„print_tensors_in_checkpoint_fileå‡½æ•°æ‰“å°ckpté‡Œçš„ä¸œè¥¿ ```python #ä½¿ç”¨NewCheckpointReaderæ¥è¯»å–ckpté‡Œçš„å˜é‡ from tensorflow.python import pywrap_tensorflow checkpoint_path = os.path.join(model_dir, \"model.ckpt\") reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path) #tf.train.NewCheckpointReader var_to_shape_map = reader.get_variable_to_shape_map() for key in var_to_shape_map: print(\"tensor_name: \", key) #print(reader.get_tensor(key)) #ä½¿ç”¨print_tensors_in_checkpoint_fileæ‰“å°ckpté‡Œçš„å†…å®¹ from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file print_tensors_in_checkpoint_file(file_name, #ckptæ–‡ä»¶åå­— tensor_name, # å¦‚æœä¸ºNone,åˆ™é»˜è®¤ä¸ºckpté‡Œçš„æ‰€æœ‰å˜é‡ all_tensors, # bool æ˜¯å¦æ‰“å°æ‰€æœ‰çš„tensorï¼Œè¿™é‡Œæ‰“å°å‡ºçš„æ˜¯tensorçš„å€¼ï¼Œä¸€èˆ¬ä¸æ¨èè¿™é‡Œè®¾ç½®ä¸ºFalse all_tensor_names) # bool æ˜¯å¦æ‰“å°æ‰€æœ‰çš„tensorçš„name #ä¸Šé¢çš„æ‰“å°ckptçš„å†…éƒ¨ä½¿ç”¨çš„æ˜¯pywrap_tensorflow.NewCheckpointReaderæ‰€ä»¥è¦æŒæ¡NewCheckpointReader 3.ä½¿ç”¨toolsé‡Œçš„freeze_graphæ¥è¯»å–ckptfrom tensorflow.python.tools import freeze_graphfreeze_graph(input_graph, #=some_graph_def.pb input_saver, input_binary, input_checkpoint, #=model.ckpt output_node_names, #=softmax restore_op_name, filename_tensor_name, output_graph, #='./tmp/frozen_graph.pb' clear_devices, initializer_nodes, variable_names_whitelist='', variable_names_blacklist='', input_meta_graph=None, input_saved_model_dir=None, saved_model_tags='serve', checkpoint_version=2)#freeze_graph_test.pyè®²è¿°äº†æ€ä¹ˆä½¿ç”¨freeze_grapgã€‚ å‚è€ƒé“¾æ¥ï¼šhttps://www.jb51.net/article/142183.htm control_dependenciestf.control_dependencies(control_inputs)Wrapper for Graph.control_dependencies() using the default graph.See Graph.control_dependencies() for more details.æ­¤å‡½æ•°æŒ‡å®šæŸäº›æ“ä½œæ‰§è¡Œçš„ä¾èµ–å…³ç³»è¿”å›ä¸€ä¸ªæ§åˆ¶ä¾èµ–çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œä½¿ç”¨ with å…³é”®å­—å¯ä»¥è®©åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ç¯å¢ƒä¸­çš„æ“ä½œéƒ½åœ¨ control_inputs æ‰§è¡Œ 1 with tf.control_dependencies([a, b]):2 c = ....3 d = ... åœ¨æ‰§è¡Œå®Œ aï¼Œb æ“ä½œä¹‹åï¼Œæ‰èƒ½æ‰§è¡Œ cï¼Œd æ“ä½œã€‚æ„æ€å°±æ˜¯ cï¼Œd æ“ä½œä¾èµ– aï¼Œb æ“ä½œ 1 with tf.control_dependencies([train_step, variable_averages_op]):2 train_op = tf.no_op(name='train') tf.no_op()è¡¨ç¤ºæ‰§è¡Œå®Œ train_step, variable_averages_op æ“ä½œä¹‹åä»€ä¹ˆéƒ½ä¸åšå‚è€ƒé“¾æ¥ï¼šhttp://www.tensorfly.cn/tfdoc/api_docs/python/framework.html#Graph.control_dependencies TensorBoardåœ¨TensorBoardä¸­å¯è§†åŒ–å›¾å½¢æ„å»ºæ‚¨çš„ç½‘ç»œï¼Œåˆ›å»ºä¸€ä¸ªä¼šè¯(session)ï¼Œç„¶ååˆ›å»ºä¸€ä¸ªTensorFlow File Writerå¯¹è±¡File Writerå®šä¹‰å­˜å‚¨TensorBoardæ–‡ä»¶çš„è·¯å¾„ï¼Œä»¥åŠTensorFlow graphå¯¹è±¡sess.graphæ˜¯ç¬¬äºŒä¸ªå‚æ•°ã€‚ writer = tf.summary.FileWriter(STORE_PATH, sess.graph) å½“åˆ›å»ºä¸€ä¸ªTensorFlowç½‘ç»œåï¼Œå®šä¹‰å¹¶è¿è¡ŒFile Writeræ—¶ï¼Œå°±å¯ä»¥å¯åŠ¨TensorBoardæ¥å¯è§†åŒ–å›¾å½¢ã€‚è¦å®šä¹‰File Writerå¹¶å°†å›¾å½¢å‘é€ç»™å®ƒï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤: # start the sessionwith tf.Session() as sess:writer = tf.summary.FileWriter(STORE_PATH, sess.graph) å¯åŠ¨TensorBoardtensorboard --logdir=STORE_PATH åç§°ç©ºé—´ï¼ˆNamespacesï¼‰åç§°ç©ºé—´æ˜¯ä¸€ç§ä½œç”¨åŸŸï¼Œå¯ä»¥ç”¨å®ƒæ¥åŒ…å›´å›¾å½¢ç»„ä»¶ï¼Œä»¥ä¾¿å°†å®ƒä»¬ç»„åˆåœ¨ä¸€èµ·ã€‚é€šè¿‡è¿™æ ·çš„æ“ä½œï¼Œåç§°ç©ºé—´ä¸­çš„ç»†èŠ‚å°†è¢«æŠ˜å æˆTensorBoardè®¡ç®—å›¾å½¢å¯è§†åŒ–ä¸­çš„å•ä¸ªåç§°ç©ºé—´èŠ‚ç‚¹ã€‚è¦åœ¨TensorFlowä¸­åˆ›å»ºåç§°ç©ºé—´ï¼Œå¯ä»¥ä½¿ç”¨Python withåŠŸèƒ½ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š with tf.name_scope(\"layer_1\"):# now declare the weights connecting the input to the hidden layer W1 = tf.Variable(tf.random_normal([784, 300], stddev=0.01), name='W') b1 = tf.Variable(tf.random_normal([300]), name='b') hidden_logits = tf.add(tf.matmul(x_sc, W1), b1) hidden_out = tf.nn.sigmoid(hidden_logits) è¿˜å¯ä»¥ä½¿ç”¨tf.variable_scope()ä»£æ›¿tf.name_scope()ã€‚å˜é‡ä½œç”¨åŸŸæ˜¯TensorFlowä¸­çš„get_variable()å˜é‡å…±äº«æœºåˆ¶çš„ä¸€éƒ¨åˆ†ã€‚ æ ‡é‡æ€»ç»“ï¼ˆScalar summariesï¼‰åœ¨ç½‘ç»œä¸­çš„ä»»ä½•ä½ç½®ï¼Œéƒ½å¯ä»¥è®°å½•æ ‡é‡(å³å•ä¸ªå®å€¼)æ•°é‡ï¼Œä»¥ä¾¿åœ¨TensorBoardä¸­æ˜¾ç¤ºã€‚è¿™å¯¹äºè·Ÿè¸ªè¯¸å¦‚è®­ç»ƒå‡†ç¡®ç‡çš„æé«˜æˆ–æŸå¤±å‡½æ•°çš„å‡å°‘ï¼Œæˆ–ç ”ç©¶åˆ†å¸ƒçš„æ ‡å‡†å·®ç­‰æ–¹é¢éƒ½å¾ˆæœ‰ç”¨ã€‚æ‰§è¡Œèµ·æ¥å¾ˆå®¹æ˜“ã€‚ä¾‹å¦‚ï¼Œä¸‹é¢çš„ä»£ç å±•ç¤ºäº†å¦‚ä½•åœ¨è¿™ä¸ªå›¾ä¸­è®°å½•accuracyæ ‡é‡: # add a summary to store theaccuracytf.summary.scalar('acc_summary', accuracy) ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯è¦åœ¨TensorBoardå¯è§†åŒ–ä¸­ç»™å‡ºæ ‡é‡çš„åç§°ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯è¦è®°å½•çš„æ“ä½œ(å¿…é¡»è¿”å›ä¸€ä¸ªå®å€¼)ã€‚scalar()è°ƒç”¨çš„è¾“å‡ºæ˜¯ä¸€ä¸ªæ“ä½œã€‚åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘æ²¡æœ‰å°†è¿™ä¸ªæ“ä½œåˆ†é…ç»™Pythonä¸­çš„ä»»ä½•å˜é‡ï¼Œä½†æ˜¯å¦‚æœç”¨æˆ·æ„¿æ„ï¼Œå¯ä»¥è¿™æ ·åšã€‚ç„¶è€Œï¼Œä¸TensorFlowä¸­çš„å…¶ä»–æ“ä½œä¸€æ ·ï¼Œè¿™äº›æ±‡æ€»æ“ä½œåœ¨è¿è¡Œä¹‹å‰ä¸ä¼šæ‰§è¡Œä»»ä½•æ“ä½œã€‚æ ¹æ®å¼€å‘äººå‘˜æƒ³è¦è§‚å¯Ÿçš„å†…å®¹ï¼Œåœ¨ä»»ä½•ç»™å®šçš„å›¾ä¸­é€šå¸¸éƒ½ä¼šè¿è¡Œè®¸å¤šå¯è§†åŒ–å‡½æ•°ï¼Œå› æ­¤æœ‰ä¸€ä¸ªæ–¹ä¾¿çš„åŠ©æ‰‹å‡½æ•°merge_all()ã€‚è¿™å°†æŠŠå›¾ä¸­çš„æ‰€æœ‰å‡½æ•°è°ƒç”¨åˆå¹¶åœ¨ä¸€èµ·ï¼Œè¿™æ ·æ‚¨åªéœ€è°ƒç”¨mergeæ“ä½œï¼Œå®ƒå°†ä¸ºæ‚¨æ”¶é›†æ‰€æœ‰å…¶ä»–å‡½æ•°æ“ä½œå¹¶è®°å½•æ•°æ®ã€‚å®ƒæ˜¯è¿™æ ·çš„: merged = tf.summary.merge_all() å›¾åƒå¯è§†åŒ–# add summaryif reuse_variables is None: tf.summary.image('input', images) tf.summary.image('score_map', score_maps) tf.summary.image('score_map_pred', f_score * 255) tf.summary.image('geo_map_0', geo_maps[:, :, :, 0:1]) tf.summary.image('geo_map_0_pred', f_geometry[:, :, :, 0:1]) tf.summary.image('training_masks', training_masks) tf.summary.scalar('model_loss', model_loss) tf.summary.scalar('total_loss', total_loss) æ–‡æœ¬æ£€æµ‹æ¨¡å‹EASTçš„æ­å»ºæ•°æ®åŠ è½½def load_annoataion(p): ''' load annotation from the text file :param p: :return: ''' text_polys = [] text_tags = [] if not os.path.exists(p): return np.array(text_polys, dtype=np.float32) with open(p, 'r') as f: reader = csv.reader(f) for line in reader: label = line[-1] # strip BOM. \\ufeff for python3, \\xef\\xbb\\bf for python2 line = [i.strip('\\ufeff').strip('\\xef\\xbb\\xbf') for i in line] x1, y1, x2, y2, x3, y3, x4, y4 = list(map(float, line[:8])) text_polys.append([[x1, y1], [x2, y2], [x3, y3], [x4, y4]]) if label == '*' or label == '###': text_tags.append(True) else: text_tags.append(False) return np.array(text_polys, dtype=np.float32), np.array(text_tags, dtype=np.bool)def generator(input_size=512, batch_size=32, background_ratio=3./8, random_scale=np.array([0.5, 1, 2.0, 3.0]), vis=False): image_list = np.array(get_images()) print('{} training images in {}'.format( image_list.shape[0], FLAGS.training_data_path)) index = np.arange(0, image_list.shape[0]) while True: np.random.shuffle(index) images = [] image_fns = [] score_maps = [] geo_maps = [] training_masks = [] for i in index: try: im_fn = image_list[i] im = cv2.imread(im_fn) # print im_fn h, w, _ = im.shape txt_fn = im_fn.replace(os.path.basename(im_fn).split('.')[1], 'txt') if not os.path.exists(txt_fn): print('text file {} does not exists'.format(txt_fn)) continue text_polys, text_tags = load_annoataion(txt_fn) text_polys, text_tags = check_and_validate_polys(text_polys, text_tags, (h, w)) # if text_polys.shape[0] == 0: # continue # random scale this image rd_scale = np.random.choice(random_scale) im = cv2.resize(im, dsize=None, fx=rd_scale, fy=rd_scale) text_polys *= rd_scale # print rd_scale # random crop a area from image if np.random.rand() background_ratio: # crop background im, text_polys, text_tags = crop_area(im, text_polys, text_tags, crop_background=True) if text_polys.shape[0] 0: # cannot find background continue # pad and resize image new_h, new_w, _ = im.shape max_h_w_i = np.max([new_h, new_w, input_size]) im_padded = np.zeros((max_h_w_i, max_h_w_i, 3), dtype=np.uint8) im_padded[:new_h, :new_w, :] = im.copy() im = cv2.resize(im_padded, dsize=(input_size, input_size)) score_map = np.zeros((input_size, input_size), dtype=np.uint8) geo_map_channels = 5 if FLAGS.geometry == 'RBOX' else 8 geo_map = np.zeros((input_size, input_size, geo_map_channels), dtype=np.float32) training_mask = np.ones((input_size, input_size), dtype=np.uint8) else: im, text_polys, text_tags = crop_area(im, text_polys, text_tags, crop_background=False) if text_polys.shape[0] == 0: continue h, w, _ = im.shape # pad the image to the training input size or the longer side of image new_h, new_w, _ = im.shape max_h_w_i = np.max([new_h, new_w, input_size]) im_padded = np.zeros((max_h_w_i, max_h_w_i, 3), dtype=np.uint8) im_padded[:new_h, :new_w, :] = im.copy() im = im_padded # resize the image to input size new_h, new_w, _ = im.shape resize_h = input_size resize_w = input_size im = cv2.resize(im, dsize=(resize_w, resize_h)) resize_ratio_3_x = resize_w/float(new_w) resize_ratio_3_y = resize_h/float(new_h) text_polys[:, :, 0] *= resize_ratio_3_x text_polys[:, :, 1] *= resize_ratio_3_y new_h, new_w, _ = im.shape score_map, geo_map, training_mask = generate_rbox((new_h, new_w), text_polys, text_tags) if vis: fig, axs = plt.subplots(3, 2, figsize=(20, 30)) # axs[0].imshow(im[:, :, ::-1]) # axs[0].set_xticks([]) # axs[0].set_yticks([]) # for poly in text_polys: # poly_h = min(abs(poly[3, 1] - poly[0, 1]), abs(poly[2, 1] - poly[1, 1])) # poly_w = min(abs(poly[1, 0] - poly[0, 0]), abs(poly[2, 0] - poly[3, 0])) # axs[0].add_artist(Patches.Polygon( # poly * 4, facecolor='none', edgecolor='green', linewidth=2, linestyle='-', fill=True)) # axs[0].text(poly[0, 0] * 4, poly[0, 1] * 4, '{:.0f}-{:.0f}'.format(poly_h * 4, poly_w * 4), # color='purple') # axs[1].imshow(score_map) # axs[1].set_xticks([]) # axs[1].set_yticks([]) axs[0, 0].imshow(im[:, :, ::-1]) axs[0, 0].set_xticks([]) axs[0, 0].set_yticks([]) for poly in text_polys: poly_h = min(abs(poly[3, 1] - poly[0, 1]), abs(poly[2, 1] - poly[1, 1])) poly_w = min(abs(poly[1, 0] - poly[0, 0]), abs(poly[2, 0] - poly[3, 0])) axs[0, 0].add_artist(Patches.Polygon( poly, facecolor='none', edgecolor='green', linewidth=2, linestyle='-', fill=True)) axs[0, 0].text(poly[0, 0], poly[0, 1], '{:.0f}-{:.0f}'.format(poly_h, poly_w), color='purple') axs[0, 1].imshow(score_map[::, ::]) axs[0, 1].set_xticks([]) axs[0, 1].set_yticks([]) axs[1, 0].imshow(geo_map[::, ::, 0]) axs[1, 0].set_xticks([]) axs[1, 0].set_yticks([]) axs[1, 1].imshow(geo_map[::, ::, 1]) axs[1, 1].set_xticks([]) axs[1, 1].set_yticks([]) axs[2, 0].imshow(geo_map[::, ::, 2]) axs[2, 0].set_xticks([]) axs[2, 0].set_yticks([]) axs[2, 1].imshow(training_mask[::, ::]) axs[2, 1].set_xticks([]) axs[2, 1].set_yticks([]) plt.tight_layout() plt.show() plt.close() images.append(im[:, :, ::-1].astype(np.float32)) image_fns.append(im_fn) score_maps.append(score_map[::4, ::4, np.newaxis].astype(np.float32)) geo_maps.append(geo_map[::4, ::4, :].astype(np.float32)) training_masks.append(training_mask[::4, ::4, np.newaxis].astype(np.float32)) if len(images) == batch_size: yield images, image_fns, score_maps, geo_maps, training_masks images = [] image_fns = [] score_maps = [] geo_maps = [] training_masks = [] except Exception as e: import traceback traceback.print_exc() continue ç½‘ç»œæ¨¡å‹çš„æ­å»ºdef model(images, weight_decay=1e-5, is_training=True): ''' define the model, we use slim's implemention of resnet ''' images = mean_image_subtraction(images) with slim.arg_scope(resnet_v1.resnet_arg_scope(weight_decay=weight_decay)): logits, end_points = resnet_v1.resnet_v1_50(images, is_training=is_training, scope='resnet_v1_50') with tf.variable_scope('feature_fusion', values=[end_points.values]): batch_norm_params = { 'decay': 0.997, 'epsilon': 1e-5, 'scale': True, 'is_training': is_training } with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu, normalizer_fn=slim.batch_norm, normalizer_params=batch_norm_params, weights_regularizer=slim.l2_regularizer(weight_decay)): f = [end_points['pool5'], end_points['pool4'], end_points['pool3'], end_points['pool2']] for i in range(4): print('Shape of f_{} {}'.format(i, f[i].shape)) g = [None, None, None, None] h = [None, None, None, None] num_outputs = [None, 128, 64, 32] for i in range(4): if i == 0: h[i] = f[i] else: c1_1 = slim.conv2d(tf.concat([g[i-1], f[i]], axis=-1), num_outputs[i], 1) h[i] = slim.conv2d(c1_1, num_outputs[i], 3) if i = 2: g[i] = unpool(h[i]) else: g[i] = slim.conv2d(h[i], num_outputs[i], 3) print('Shape of h_{} {}, g_{} {}'.format(i, h[i].shape, i, g[i].shape)) # here we use a slightly different way for regression part, # we first use a sigmoid to limit the regression range, and also # this is do with the angle map F_score = slim.conv2d(g[3], 1, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None) # 4 channel of axis aligned bbox and 1 channel rotation angle geo_map = slim.conv2d(g[3], 4, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None) * FLAGS.text_scale angle_map = (slim.conv2d(g[3], 1, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None) - 0.5) * np.pi/2 # angle is between [-45, 45] F_geometry = tf.concat([geo_map, angle_map], axis=-1) return F_score, F_geometry losså‡½æ•°çš„è®¾è®¡def loss(y_true_cls, y_pred_cls, y_true_geo, y_pred_geo, training_mask): ''' define the loss used for training, contraning two part, the first part we use dice loss instead of weighted logloss, the second part is the iou loss defined in the paper :param y_true_cls: ground truth of text :param y_pred_cls: prediction os text :param y_true_geo: ground truth of geometry :param y_pred_geo: prediction of geometry :param training_mask: mask used in training, to ignore some text annotated by ### :return: ''' classification_loss = dice_coefficient(y_true_cls, y_pred_cls, training_mask) # scale classification loss to match the iou loss part classification_loss *= 0.01 # d1 - top, d2-right, d3-bottom, d4-left d1_gt, d2_gt, d3_gt, d4_gt, theta_gt = tf.split(value=y_true_geo, num_or_size_splits=5, axis=3) d1_pred, d2_pred, d3_pred, d4_pred, theta_pred = tf.split(value=y_pred_geo, num_or_size_splits=5, axis=3) area_gt = (d1_gt + d3_gt) * (d2_gt + d4_gt) area_pred = (d1_pred + d3_pred) * (d2_pred + d4_pred) w_union = tf.minimum(d2_gt, d2_pred) + tf.minimum(d4_gt, d4_pred) h_union = tf.minimum(d1_gt, d1_pred) + tf.minimum(d3_gt, d3_pred) area_intersect = w_union * h_union area_union = area_gt + area_pred - area_intersect L_AABB = -tf.log((area_intersect + 1.0)/(area_union + 1.0)) L_theta = 1 - tf.cos(theta_pred - theta_gt) tf.summary.scalar('geometry_AABB', tf.reduce_mean(L_AABB * y_true_cls * training_mask)) tf.summary.scalar('geometry_theta', tf.reduce_mean(L_theta * y_true_cls * training_mask)) L_g = L_AABB + 20 * L_theta return tf.reduce_mean(L_g * y_true_cls * training_mask) + classification_loss traindef main(argv=None): import os os.environ['CUDA_VISIBLE_DEVICES'] = FLAGS.gpu_list config = None config.batch_size = FLAGS.batch_size_per_gpu * FLAGS.num_gpus if not tf.gfile.Exists(FLAGS.checkpoint_path): tf.gfile.MkDir(FLAGS.checkpoint_path) else: if not FLAGS.restore: tf.gfile.DeleteRecursively(FLAGS.checkpoint_path) tf.gfile.MkDir(FLAGS.checkpoint_path) input_images = tf.placeholder(tf.float32, shape=[None, None, None, 3], name='input_images') input_score_maps = tf.placeholder(tf.float32, shape=[None, None, None, 1], name='input_score_maps') if FLAGS.geometry == 'RBOX': input_geo_maps = tf.placeholder(tf.float32, shape=[None, None, None, 5], name='input_geo_maps') else: input_geo_maps = tf.placeholder(tf.float32, shape=[None, None, None, 8], name='input_geo_maps') input_training_masks = tf.placeholder(tf.float32, shape=[None, None, None, 1], name='input_training_masks') global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False) learning_rate = tf.train.exponential_decay(FLAGS.learning_rate, global_step, decay_steps=10000, decay_rate=0.94, staircase=True) # add summary tf.summary.scalar('learning_rate', learning_rate) opt = tf.train.AdamOptimizer(learning_rate) # opt = tf.train.MomentumOptimizer(learning_rate, 0.9) # split input_images_split = tf.split(input_images, len(gpus)) input_score_maps_split = tf.split(input_score_maps, len(gpus)) input_geo_maps_split = tf.split(input_geo_maps, len(gpus)) input_training_masks_split = tf.split(input_training_masks, len(gpus)) tower_grads = [] reuse_variables = None for i, gpu_id in enumerate(gpus): with tf.device('/gpu:%d' % gpu_id): with tf.name_scope('model_%d' % gpu_id) as scope: iis = input_images_split[i] isms = input_score_maps_split[i] igms = input_geo_maps_split[i] itms = input_training_masks_split[i] total_loss, model_loss = tower_loss(iis, isms, igms, itms, reuse_variables) batch_norm_updates_op = tf.group(*tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope)) reuse_variables = True grads = opt.compute_gradients(total_loss) tower_grads.append(grads) grads = average_gradients(tower_grads) apply_gradient_op = opt.apply_gradients(grads, global_step=global_step) summary_op = tf.summary.merge_all() # save moving average variable_averages = tf.train.ExponentialMovingAverage( FLAGS.moving_average_decay, global_step) variables_averages_op = variable_averages.apply(tf.trainable_variables()) # batch norm updates with tf.control_dependencies([variables_averages_op, apply_gradient_op, batch_norm_updates_op]): train_op = tf.no_op(name='train_op') saver = tf.train.Saver(tf.global_variables()) summary_writer = tf.summary.FileWriter(FLAGS.checkpoint_path, tf.get_default_graph()) init = tf.global_variables_initializer() if FLAGS.pretrained_model_path is not None: variable_restore_op = slim.assign_from_checkpoint_fn(FLAGS.pretrained_model_path, slim.get_trainable_variables(), ignore_missing_vars=True) with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess: if FLAGS.restore: print('continue training from previous checkpoint') ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_path) saver.restore(sess, ckpt) else: sess.run(init) if FLAGS.pretrained_model_path is not None: variable_restore_op(sess) # data_generator = icdar.get_batch(num_workers=FLAGS.num_readers, # input_size=FLAGS.input_size, # batch_size=FLAGS.batch_size_per_gpu * len(gpus)) train_data_generator = icdar_single.get_batch_seq(num_workers=FLAGS.num_readers, config=config, is_training=True) start = time.time() for step in range(FLAGS.max_steps): data = next(train_data_generator) ml, tl, _ = sess.run([model_loss, total_loss, train_op], feed_dict={input_images: data[0], input_score_maps: data[2], input_geo_maps: data[3], input_training_masks: data[4]}) if np.isnan(tl): print('Loss diverged, stop training') break if step % 10 == 0: avg_time_per_step = (time.time() - start)/10 avg_examples_per_second = (10 * FLAGS.batch_size_per_gpu * len(gpus))/(time.time() - start) start = time.time() print('Step {:06d}, model loss {:.4f}, total loss {:.4f}, {:.2f} seconds/step, {:.2f} examples/second'.format( step, ml, tl, avg_time_per_step, avg_examples_per_second)) if step % FLAGS.save_checkpoint_steps == 0: saver.save(sess, FLAGS.checkpoint_path + 'model.ckpt', global_step=global_step) if step % FLAGS.save_summary_steps == 0: _, tl, summary_str = sess.run([train_op, total_loss, summary_op], feed_dict={input_images: data[0], input_score_maps: data[2], input_geo_maps: data[3], input_training_masks: data[4]}) summary_writer.add_summary(summary_str, global_step=step) å‚è€ƒé“¾æ¥ï¼šhttps://github.com/argman/EAST","tags":["æ·±åº¦å­¦ä¹ "],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"tensorflowè‡ªå®šä¹‰ç½‘ç»œæ¨¡å‹","path":"/2019/10/17/tensorflowè‡ªå®šä¹‰ç½‘ç»œæ¨¡å‹/","content":"SlimTF-Slim æ¨¡å—æ˜¯ TensorFlow ä¸­æœ€å¥½ç”¨çš„ API ä¹‹ä¸€ã€‚å°¤å…¶æ˜¯é‡Œé¢å¼•å…¥çš„ arg_scopeã€model_variablesã€repeatã€stackã€‚TF-Slim æ˜¯ TensorFlow ä¸­ä¸€ä¸ªç”¨æ¥æ„å»ºã€è®­ç»ƒã€è¯„ä¼°å¤æ‚æ¨¡å‹çš„è½»é‡åŒ–åº“ã€‚TF-Slim æ¨¡å—å¯ä»¥å’Œ TensorFlow ä¸­å…¶å®ƒAPIæ··åˆä½¿ç”¨ã€‚ Slimæ¨¡å—çš„å¯¼å…¥1 | import tensorflow.contrib.slim as slim â€”|â€” Slim æ„å»ºæ¨¡å‹å¯ä»¥ç”¨ slimã€variablesã€layers å’Œ scopes æ¥ååˆ†ç®€æ´åœ°å®šä¹‰æ¨¡å‹ã€‚ä¸‹é¢å¯¹å„ä¸ªéƒ¨åˆ†è¿›è¡Œäº†è¯¦ç»†æè¿°ï¼š Slimå˜é‡ï¼ˆVariablesï¼‰1 2 3 4 5 6 | weights = slim.variable('weights', shape=[10, 10, 3 , 3], initializer=tf.truncated_normal_initializer(stddev=0.1), regularizer=slim.l2_regularizer(0.05), device='/CPU:0') ~ â€”|â€” Slim å±‚ï¼ˆLayersï¼‰ä½¿ç”¨åŸºç¡€ï¼ˆplainï¼‰çš„ TensorFlow ä»£ç ï¼š 1 2 3 4 5 6 7 8 9 | input = ... with tf.name_scope('conv1_1') as scope: kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32, stddev=1e-1), name='weights') conv = tf.nn.conv2d(input, kernel, [1, 1, 1, 1], padding='SAME') biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32), trainable=True, name='biases') bias = tf.nn.bias_add(conv, biases) conv1 = tf.nn.relu(bias, name=scope) â€”|â€” ä¸ºäº†é¿å…ä»£ç çš„é‡å¤ã€‚Slim æä¾›äº†å¾ˆå¤šæ–¹ä¾¿çš„ç¥ç»ç½‘ç»œ layers çš„é«˜å±‚ opã€‚ä¾‹å¦‚ï¼šä¸ä¸Šé¢çš„ä»£ç å¯¹åº”çš„ Slim ç‰ˆçš„ä»£ç ï¼š 1 2 | input = ... net = slim.conv2d(input, 128, [3, 3], scope='conv1_1') â€”|â€” slim.arg_scopeï¼ˆï¼‰ å‡½æ•°çš„ä½¿ç”¨è¿™ä¸ªå‡½æ•°çš„ä½œç”¨æ˜¯ç»™list_opsä¸­çš„å†…å®¹è®¾ç½®é»˜è®¤å€¼ã€‚ä½†æ˜¯æ¯ä¸ªlist_opsä¸­çš„æ¯ä¸ªæˆå‘˜éœ€è¦ç”¨@add_arg_scopeä¿®é¥°æ‰è¡Œã€‚æ‰€ä»¥ä½¿ç”¨slim.arg_scopeï¼ˆï¼‰æœ‰ä¸¤ä¸ªæ­¥éª¤ï¼š ä½¿ç”¨@slim.add_arg_scopeä¿®é¥°ç›®æ ‡å‡½æ•° ç”¨ slim.arg_scopeï¼ˆï¼‰ä¸ºç›®æ ‡å‡½æ•°è®¾ç½®é»˜è®¤å‚æ•°.ä¾‹å¦‚å¦‚ä¸‹ä»£ç ï¼›é¦–å…ˆç”¨@slim.add_arg_scopeä¿®é¥°ç›®æ ‡å‡½æ•°fun1ï¼ˆï¼‰ï¼Œç„¶ååˆ©ç”¨slim.arg_scopeï¼ˆï¼‰ä¸ºå®ƒè®¾ç½®é»˜è®¤å‚æ•°ã€‚ 1 2 3 4 5 6 7 8 9 10 | import tensorflow as tf slim =tf.contrib.slim @slim.add_arg_scope def fun1(a=0,b=0): return (a+b) with slim.arg_scope([fun1],a=10): x=fun1(b=30) print(x) â€”|â€” è¿è¡Œç»“æœ:40å‚è€ƒé“¾æ¥ï¼šhttps://blog.csdn.net/u013921430/article/details/80915696 å…¶ä»–ç”¨æ³•è§å‚è€ƒé“¾æ¥https://blog.csdn.net/wanttifa/article/details/90208398 æŸ¥çœ‹ckptä¸­å˜é‡çš„å‡ ç§æ–¹æ³•æŸ¥çœ‹ckptä¸­å˜é‡çš„æ–¹æ³•æœ‰ä¸‰ç§ï¼š åœ¨æœ‰modelçš„æƒ…å†µä¸‹ï¼Œä½¿ç”¨tf.train.Saverè¿›è¡Œrestore ä½¿ç”¨tf.train.NewCheckpointReaderç›´æ¥è¯»å–ckptæ–‡ä»¶ï¼Œè¿™ç§æ–¹æ³•ä¸éœ€è¦modelã€‚ ä½¿ç”¨toolsé‡Œçš„freeze_graphæ¥è¯»å–ckptTips: å¦‚æœæ¨¡å‹ä¿å­˜ä¸º.ckptçš„æ–‡ä»¶ï¼Œåˆ™ä½¿ç”¨è¯¥æ–‡ä»¶å°±å¯ä»¥æŸ¥çœ‹.ckptæ–‡ä»¶é‡Œçš„å˜é‡ã€‚ckptè·¯å¾„ä¸º model.ckpt å¦‚æœæ¨¡å‹ä¿å­˜ä¸º.ckpt-xxx-data (å›¾ç»“æ„)ã€.ckpt-xxx.index (å‚æ•°å)ã€.ckpt-xxx-meta (å‚æ•°å€¼)æ–‡ä»¶ï¼Œåˆ™éœ€è¦åŒæ—¶æ‹¥æœ‰è¿™ä¸‰ä¸ªæ–‡ä»¶æ‰è¡Œã€‚å¹¶ä¸”ckptçš„è·¯å¾„ä¸º model.ckpt-xxx 1.åŸºäºmodelæ¥è¯»å–ckptæ–‡ä»¶é‡Œçš„å˜é‡1.é¦–å…ˆå»ºç«‹èµ·model2.ä»ckptä¸­æ¢å¤å˜é‡ 1 2 3 4 5 6 7 8 9 10 | with tf.Graph().as_default() as g: #å»ºç«‹model images, labels = cifar10.inputs(eval_data=eval_data) logits = cifar10.inference(images) top_k_op = tf.nn.in_top_k(logits, labels, 1) #ä»ckptä¸­æ¢å¤å˜é‡ sess = tf.Session() saver = tf.train.Saver() #saver = tf.train.Saver(...variables...) # æ¢å¤éƒ¨åˆ†å˜é‡æ—¶ï¼Œåªéœ€è¦åœ¨Saveré‡ŒæŒ‡å®šè¦æ¢å¤çš„å˜é‡ save_path = 'ckptçš„è·¯å¾„' saver.restore(sess, save_path) # ä»ckptä¸­æ¢å¤å˜é‡ â€”|â€” æ³¨æ„ï¼šåŸºäºmodelæ¥è¯»å–ckptä¸­å˜é‡æ—¶ï¼Œmodelå’Œckptå¿…é¡»åŒ¹é…ã€‚ 2.ä½¿ç”¨tf.train.NewCheckpointReaderç›´æ¥è¯»å–ckptæ–‡ä»¶é‡Œçš„å˜é‡ï¼Œä½¿ç”¨tools.inspect_checkpointé‡Œçš„print_tensors_in_checkpoint_fileå‡½æ•°æ‰“å°ckpté‡Œçš„ä¸œè¥¿1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 | #ä½¿ç”¨NewCheckpointReaderæ¥è¯»å–ckpté‡Œçš„å˜é‡ from tensorflow.python import pywrap_tensorflow checkpoint_path = os.path.join(model_dir, \"model.ckpt\") reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path) #tf.train.NewCheckpointReader var_to_shape_map = reader.get_variable_to_shape_map() for key in var_to_shape_map: print(\"tensor_name: \", key) #print(reader.get_tensor(key)) #ä½¿ç”¨print_tensors_in_checkpoint_fileæ‰“å°ckpté‡Œçš„å†…å®¹ from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file print_tensors_in_checkpoint_file(file_name, #ckptæ–‡ä»¶åå­— tensor_name, # å¦‚æœä¸ºNone,åˆ™é»˜è®¤ä¸ºckpté‡Œçš„æ‰€æœ‰å˜é‡ all_tensors, # bool æ˜¯å¦æ‰“å°æ‰€æœ‰çš„tensorï¼Œè¿™é‡Œæ‰“å°å‡ºçš„æ˜¯tensorçš„å€¼ï¼Œä¸€èˆ¬ä¸æ¨èè¿™é‡Œè®¾ç½®ä¸ºFalse all_tensor_names) # bool æ˜¯å¦æ‰“å°æ‰€æœ‰çš„tensorçš„name #ä¸Šé¢çš„æ‰“å°ckptçš„å†…éƒ¨ä½¿ç”¨çš„æ˜¯pywrap_tensorflow.NewCheckpointReaderæ‰€ä»¥è¦æŒæ¡NewCheckpointReader â€”|â€” 3.ä½¿ç”¨toolsé‡Œçš„freeze_graphæ¥è¯»å–ckpt1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 | from tensorflow.python.tools import freeze_graph freeze_graph(input_graph, #=some_graph_def.pb input_saver, input_binary, input_checkpoint, #=model.ckpt output_node_names, #=softmax restore_op_name, filename_tensor_name, output_graph, #='./tmp/frozen_graph.pb' clear_devices, initializer_nodes, variable_names_whitelist='', variable_names_blacklist='', input_meta_graph=None, input_saved_model_dir=None, saved_model_tags='serve', checkpoint_version=2) #freeze_graph_test.pyè®²è¿°äº†æ€ä¹ˆä½¿ç”¨freeze_grapgã€‚ â€”|â€” å‚è€ƒé“¾æ¥ï¼šhttps://www.jb51.net/article/142183.htm control_dependenciestf.control_dependencies(control_inputs)Wrapper for Graph.control_dependencies() using the default graph.See Graph.control_dependencies() for more details.æ­¤å‡½æ•°æŒ‡å®šæŸäº›æ“ä½œæ‰§è¡Œçš„ä¾èµ–å…³ç³»è¿”å›ä¸€ä¸ªæ§åˆ¶ä¾èµ–çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œä½¿ç”¨ with å…³é”®å­—å¯ä»¥è®©åœ¨è¿™ä¸ªä¸Šä¸‹æ–‡ç¯å¢ƒä¸­çš„æ“ä½œéƒ½åœ¨ control_inputs æ‰§è¡Œ 1 2 3 | 1 with tf.control_dependencies([a, b]): 2 c = .... 3 d = ... â€”|â€” åœ¨æ‰§è¡Œå®Œ aï¼Œb æ“ä½œä¹‹åï¼Œæ‰èƒ½æ‰§è¡Œ cï¼Œd æ“ä½œã€‚æ„æ€å°±æ˜¯ cï¼Œd æ“ä½œä¾èµ– aï¼Œb æ“ä½œ 1 2 | 1 with tf.control_dependencies([train_step, variable_averages_op]): 2 train_op = tf.no_op(name='train') â€”|â€” tf.no_op()è¡¨ç¤ºæ‰§è¡Œå®Œ train_step, variable_averages_op æ“ä½œä¹‹åä»€ä¹ˆéƒ½ä¸åšå‚è€ƒé“¾æ¥ï¼šhttp://www.tensorfly.cn/tfdoc/api_docs/python/framework.html#Graph.control_dependencies TensorBoardåœ¨TensorBoardä¸­å¯è§†åŒ–å›¾å½¢æ„å»ºæ‚¨çš„ç½‘ç»œï¼Œåˆ›å»ºä¸€ä¸ªä¼šè¯(session)ï¼Œç„¶ååˆ›å»ºä¸€ä¸ªTensorFlow File Writerå¯¹è±¡File Writerå®šä¹‰å­˜å‚¨TensorBoardæ–‡ä»¶çš„è·¯å¾„ï¼Œä»¥åŠTensorFlow graphå¯¹è±¡sess.graphæ˜¯ç¬¬äºŒä¸ªå‚æ•°ã€‚ 1 | writer = tf.summary.FileWriter(STORE_PATH, sess.graph) â€”|â€” å½“åˆ›å»ºä¸€ä¸ªTensorFlowç½‘ç»œåï¼Œå®šä¹‰å¹¶è¿è¡ŒFile Writeræ—¶ï¼Œå°±å¯ä»¥å¯åŠ¨TensorBoardæ¥å¯è§†åŒ–å›¾å½¢ã€‚è¦å®šä¹‰File Writerå¹¶å°†å›¾å½¢å‘é€ç»™å®ƒï¼Œè¿è¡Œä»¥ä¸‹å‘½ä»¤: 1 2 3 | # start the session with tf.Session() as sess: writer = tf.summary.FileWriter(STORE_PATH, sess.graph) â€”|â€” å¯åŠ¨TensorBoard1 | tensorboard --logdir=STORE_PATH â€”|â€” åç§°ç©ºé—´ï¼ˆNamespacesï¼‰åç§°ç©ºé—´æ˜¯ä¸€ç§ä½œç”¨åŸŸï¼Œå¯ä»¥ç”¨å®ƒæ¥åŒ…å›´å›¾å½¢ç»„ä»¶ï¼Œä»¥ä¾¿å°†å®ƒä»¬ç»„åˆåœ¨ä¸€èµ·ã€‚é€šè¿‡è¿™æ ·çš„æ“ä½œï¼Œåç§°ç©ºé—´ä¸­çš„ç»†èŠ‚å°†è¢«æŠ˜å æˆTensorBoardè®¡ç®—å›¾å½¢å¯è§†åŒ–ä¸­çš„å•ä¸ªåç§°ç©ºé—´èŠ‚ç‚¹ã€‚è¦åœ¨TensorFlowä¸­åˆ›å»ºåç§°ç©ºé—´ï¼Œå¯ä»¥ä½¿ç”¨Python withåŠŸèƒ½ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š 1 2 3 4 5 6 | with tf.name_scope(\"layer_1\"): # now declare the weights connecting the input to the hidden layer W1 = tf.Variable(tf.random_normal([784, 300], stddev=0.01), name='W') b1 = tf.Variable(tf.random_normal([300]), name='b') hidden_logits = tf.add(tf.matmul(x_sc, W1), b1) hidden_out = tf.nn.sigmoid(hidden_logits) â€”|â€” è¿˜å¯ä»¥ä½¿ç”¨tf.variable_scope()ä»£æ›¿tf.name_scope()ã€‚å˜é‡ä½œç”¨åŸŸæ˜¯TensorFlowä¸­çš„get_variable()å˜é‡å…±äº«æœºåˆ¶çš„ä¸€éƒ¨åˆ†ã€‚ æ ‡é‡æ€»ç»“ï¼ˆScalar summariesï¼‰åœ¨ç½‘ç»œä¸­çš„ä»»ä½•ä½ç½®ï¼Œéƒ½å¯ä»¥è®°å½•æ ‡é‡(å³å•ä¸ªå®å€¼)æ•°é‡ï¼Œä»¥ä¾¿åœ¨TensorBoardä¸­æ˜¾ç¤ºã€‚è¿™å¯¹äºè·Ÿè¸ªè¯¸å¦‚è®­ç»ƒå‡†ç¡®ç‡çš„æé«˜æˆ–æŸå¤±å‡½æ•°çš„å‡å°‘ï¼Œæˆ–ç ”ç©¶åˆ†å¸ƒçš„æ ‡å‡†å·®ç­‰æ–¹é¢éƒ½å¾ˆæœ‰ç”¨ã€‚æ‰§è¡Œèµ·æ¥å¾ˆå®¹æ˜“ã€‚ä¾‹å¦‚ï¼Œä¸‹é¢çš„ä»£ç å±•ç¤ºäº†å¦‚ä½•åœ¨è¿™ä¸ªå›¾ä¸­è®°å½•accuracyæ ‡é‡: 1 2 | # add a summary to store the accuracytf.summary.scalar('acc_summary', accuracy) â€”|â€” ç¬¬ä¸€ä¸ªå‚æ•°æ˜¯è¦åœ¨TensorBoardå¯è§†åŒ–ä¸­ç»™å‡ºæ ‡é‡çš„åç§°ï¼Œç¬¬äºŒä¸ªå‚æ•°æ˜¯è¦è®°å½•çš„æ“ä½œ(å¿…é¡»è¿”å›ä¸€ä¸ªå®å€¼)ã€‚scalar()è°ƒç”¨çš„è¾“å‡ºæ˜¯ä¸€ä¸ªæ“ä½œã€‚åœ¨ä¸Šé¢çš„ä»£ç ä¸­ï¼Œæˆ‘æ²¡æœ‰å°†è¿™ä¸ªæ“ä½œåˆ†é…ç»™Pythonä¸­çš„ä»»ä½•å˜é‡ï¼Œä½†æ˜¯å¦‚æœç”¨æˆ·æ„¿æ„ï¼Œå¯ä»¥è¿™æ ·åšã€‚ç„¶è€Œï¼Œä¸TensorFlowä¸­çš„å…¶ä»–æ“ä½œä¸€æ ·ï¼Œè¿™äº›æ±‡æ€»æ“ä½œåœ¨è¿è¡Œä¹‹å‰ä¸ä¼šæ‰§è¡Œä»»ä½•æ“ä½œã€‚æ ¹æ®å¼€å‘äººå‘˜æƒ³è¦è§‚å¯Ÿçš„å†…å®¹ï¼Œåœ¨ä»»ä½•ç»™å®šçš„å›¾ä¸­é€šå¸¸éƒ½ä¼šè¿è¡Œè®¸å¤šå¯è§†åŒ–å‡½æ•°ï¼Œå› æ­¤æœ‰ä¸€ä¸ªæ–¹ä¾¿çš„åŠ©æ‰‹å‡½æ•°merge_all()ã€‚è¿™å°†æŠŠå›¾ä¸­çš„æ‰€æœ‰å‡½æ•°è°ƒç”¨åˆå¹¶åœ¨ä¸€èµ·ï¼Œè¿™æ ·æ‚¨åªéœ€è°ƒç”¨mergeæ“ä½œï¼Œå®ƒå°†ä¸ºæ‚¨æ”¶é›†æ‰€æœ‰å…¶ä»–å‡½æ•°æ“ä½œå¹¶è®°å½•æ•°æ®ã€‚å®ƒæ˜¯è¿™æ ·çš„: 1 | merged = tf.summary.merge_all() â€”|â€” å›¾åƒå¯è§†åŒ–1 2 3 4 5 6 7 8 9 10 | # add summary if reuse_variables is None: tf.summary.image('input', images) tf.summary.image('score_map', score_maps) tf.summary.image('score_map_pred', f_score * 255) tf.summary.image('geo_map_0', geo_maps[:, :, :, 0:1]) tf.summary.image('geo_map_0_pred', f_geometry[:, :, :, 0:1]) tf.summary.image('training_masks', training_masks) tf.summary.scalar('model_loss', model_loss) tf.summary.scalar('total_loss', total_loss) â€”|â€” æ–‡æœ¬æ£€æµ‹æ¨¡å‹EASTçš„æ­å»ºæ•°æ®åŠ è½½1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 | def load_annoataion(p): ''' load annotation from the text file :param p: :return: ''' text_polys = [] text_tags = [] if not os.path.exists(p): return np.array(text_polys, dtype=np.float32) with open(p, 'r') as f: reader = csv.reader(f) for line in reader: label = line[-1] # strip BOM. \\ufeff for python3, \\xef\\xbb\\bf for python2 line = [i.strip('\\ufeff').strip('\\xef\\xbb\\xbf') for i in line] x1, y1, x2, y2, x3, y3, x4, y4 = list(map(float, line[:8])) text_polys.append([[x1, y1], [x2, y2], [x3, y3], [x4, y4]]) if label == '*' or label == '###': text_tags.append(True) else: text_tags.append(False) return np.array(text_polys, dtype=np.float32), np.array(text_tags, dtype=np.bool) def generator(input_size=512, batch_size=32, background_ratio=3./8, random_scale=np.array([0.5, 1, 2.0, 3.0]), vis=False): image_list = np.array(get_images()) print('{} training images in {}'.format( image_list.shape[0], FLAGS.training_data_path)) index = np.arange(0, image_list.shape[0]) while True: np.random.shuffle(index) images = [] image_fns = [] score_maps = [] geo_maps = [] training_masks = [] for i in index: try: im_fn = image_list[i] im = cv2.imread(im_fn) # print im_fn h, w, _ = im.shape txt_fn = im_fn.replace(os.path.basename(im_fn).split('.')[1], 'txt') if not os.path.exists(txt_fn): print('text file {} does not exists'.format(txt_fn)) continue text_polys, text_tags = load_annoataion(txt_fn) text_polys, text_tags = check_and_validate_polys(text_polys, text_tags, (h, w)) # if text_polys.shape[0] == 0: # continue # random scale this image rd_scale = np.random.choice(random_scale) im = cv2.resize(im, dsize=None, fx=rd_scale, fy=rd_scale) text_polys *= rd_scale # print rd_scale # random crop a area from image if np.random.rand() background_ratio: # crop background im, text_polys, text_tags = crop_area(im, text_polys, text_tags, crop_background=True) if text_polys.shape[0] 0: # cannot find background continue # pad and resize image new_h, new_w, _ = im.shape max_h_w_i = np.max([new_h, new_w, input_size]) im_padded = np.zeros((max_h_w_i, max_h_w_i, 3), dtype=np.uint8) im_padded[:new_h, :new_w, :] = im.copy() im = cv2.resize(im_padded, dsize=(input_size, input_size)) score_map = np.zeros((input_size, input_size), dtype=np.uint8) geo_map_channels = 5 if FLAGS.geometry == 'RBOX' else 8 geo_map = np.zeros((input_size, input_size, geo_map_channels), dtype=np.float32) training_mask = np.ones((input_size, input_size), dtype=np.uint8) else: im, text_polys, text_tags = crop_area(im, text_polys, text_tags, crop_background=False) if text_polys.shape[0] == 0: continue h, w, _ = im.shape # pad the image to the training input size or the longer side of image new_h, new_w, _ = im.shape max_h_w_i = np.max([new_h, new_w, input_size]) im_padded = np.zeros((max_h_w_i, max_h_w_i, 3), dtype=np.uint8) im_padded[:new_h, :new_w, :] = im.copy() im = im_padded # resize the image to input size new_h, new_w, _ = im.shape resize_h = input_size resize_w = input_size im = cv2.resize(im, dsize=(resize_w, resize_h)) resize_ratio_3_x = resize_w/float(new_w) resize_ratio_3_y = resize_h/float(new_h) text_polys[:, :, 0] *= resize_ratio_3_x text_polys[:, :, 1] *= resize_ratio_3_y new_h, new_w, _ = im.shape score_map, geo_map, training_mask = generate_rbox((new_h, new_w), text_polys, text_tags) if vis: fig, axs = plt.subplots(3, 2, figsize=(20, 30)) # axs[0].imshow(im[:, :, ::-1]) # axs[0].set_xticks([]) # axs[0].set_yticks([]) # for poly in text_polys: # poly_h = min(abs(poly[3, 1] - poly[0, 1]), abs(poly[2, 1] - poly[1, 1])) # poly_w = min(abs(poly[1, 0] - poly[0, 0]), abs(poly[2, 0] - poly[3, 0])) # axs[0].add_artist(Patches.Polygon( # poly * 4, facecolor='none', edgecolor='green', linewidth=2, linestyle='-', fill=True)) # axs[0].text(poly[0, 0] * 4, poly[0, 1] * 4, '{:.0f}-{:.0f}'.format(poly_h * 4, poly_w * 4), # color='purple') # axs[1].imshow(score_map) # axs[1].set_xticks([]) # axs[1].set_yticks([]) axs[0, 0].imshow(im[:, :, ::-1]) axs[0, 0].set_xticks([]) axs[0, 0].set_yticks([]) for poly in text_polys: poly_h = min(abs(poly[3, 1] - poly[0, 1]), abs(poly[2, 1] - poly[1, 1])) poly_w = min(abs(poly[1, 0] - poly[0, 0]), abs(poly[2, 0] - poly[3, 0])) axs[0, 0].add_artist(Patches.Polygon( poly, facecolor='none', edgecolor='green', linewidth=2, linestyle='-', fill=True)) axs[0, 0].text(poly[0, 0], poly[0, 1], '{:.0f}-{:.0f}'.format(poly_h, poly_w), color='purple') axs[0, 1].imshow(score_map[::, ::]) axs[0, 1].set_xticks([]) axs[0, 1].set_yticks([]) axs[1, 0].imshow(geo_map[::, ::, 0]) axs[1, 0].set_xticks([]) axs[1, 0].set_yticks([]) axs[1, 1].imshow(geo_map[::, ::, 1]) axs[1, 1].set_xticks([]) axs[1, 1].set_yticks([]) axs[2, 0].imshow(geo_map[::, ::, 2]) axs[2, 0].set_xticks([]) axs[2, 0].set_yticks([]) axs[2, 1].imshow(training_mask[::, ::]) axs[2, 1].set_xticks([]) axs[2, 1].set_yticks([]) plt.tight_layout() plt.show() plt.close() images.append(im[:, :, ::-1].astype(np.float32)) image_fns.append(im_fn) score_maps.append(score_map[::4, ::4, np.newaxis].astype(np.float32)) geo_maps.append(geo_map[::4, ::4, :].astype(np.float32)) training_masks.append(training_mask[::4, ::4, np.newaxis].astype(np.float32)) if len(images) == batch_size: yield images, image_fns, score_maps, geo_maps, training_masks images = [] image_fns = [] score_maps = [] geo_maps = [] training_masks = [] except Exception as e: import traceback traceback.print_exc() continue â€”|â€” ç½‘ç»œæ¨¡å‹çš„æ­å»º1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 | def model(images, weight_decay=1e-5, is_training=True): ''' define the model, we use slim's implemention of resnet ''' images = mean_image_subtraction(images) with slim.arg_scope(resnet_v1.resnet_arg_scope(weight_decay=weight_decay)): logits, end_points = resnet_v1.resnet_v1_50(images, is_training=is_training, scope='resnet_v1_50') with tf.variable_scope('feature_fusion', values=[end_points.values]): batch_norm_params = { 'decay': 0.997, 'epsilon': 1e-5, 'scale': True, 'is_training': is_training } with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu, normalizer_fn=slim.batch_norm, normalizer_params=batch_norm_params, weights_regularizer=slim.l2_regularizer(weight_decay)): f = [end_points['pool5'], end_points['pool4'], end_points['pool3'], end_points['pool2']] for i in range(4): print('Shape of f_{} {}'.format(i, f[i].shape)) g = [None, None, None, None] h = [None, None, None, None] num_outputs = [None, 128, 64, 32] for i in range(4): if i == 0: h[i] = f[i] else: c1_1 = slim.conv2d(tf.concat([g[i-1], f[i]], axis=-1), num_outputs[i], 1) h[i] = slim.conv2d(c1_1, num_outputs[i], 3) if i = 2: g[i] = unpool(h[i]) else: g[i] = slim.conv2d(h[i], num_outputs[i], 3) print('Shape of h_{} {}, g_{} {}'.format(i, h[i].shape, i, g[i].shape)) # here we use a slightly different way for regression part, # we first use a sigmoid to limit the regression range, and also # this is do with the angle map F_score = slim.conv2d(g[3], 1, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None) # 4 channel of axis aligned bbox and 1 channel rotation angle geo_map = slim.conv2d(g[3], 4, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None) * FLAGS.text_scale angle_map = (slim.conv2d(g[3], 1, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None) - 0.5) * np.pi/2 # angle is between [-45, 45] F_geometry = tf.concat([geo_map, angle_map], axis=-1) return F_score, F_geometry â€”|â€” losså‡½æ•°çš„è®¾è®¡1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 | def loss(y_true_cls, y_pred_cls, y_true_geo, y_pred_geo, training_mask): ''' define the loss used for training, contraning two part, the first part we use dice loss instead of weighted logloss, the second part is the iou loss defined in the paper :param y_true_cls: ground truth of text :param y_pred_cls: prediction os text :param y_true_geo: ground truth of geometry :param y_pred_geo: prediction of geometry :param training_mask: mask used in training, to ignore some text annotated by ### :return: ''' classification_loss = dice_coefficient(y_true_cls, y_pred_cls, training_mask) # scale classification loss to match the iou loss part classification_loss *= 0.01 # d1 - top, d2-right, d3-bottom, d4-left d1_gt, d2_gt, d3_gt, d4_gt, theta_gt = tf.split(value=y_true_geo, num_or_size_splits=5, axis=3) d1_pred, d2_pred, d3_pred, d4_pred, theta_pred = tf.split(value=y_pred_geo, num_or_size_splits=5, axis=3) area_gt = (d1_gt + d3_gt) * (d2_gt + d4_gt) area_pred = (d1_pred + d3_pred) * (d2_pred + d4_pred) w_union = tf.minimum(d2_gt, d2_pred) + tf.minimum(d4_gt, d4_pred) h_union = tf.minimum(d1_gt, d1_pred) + tf.minimum(d3_gt, d3_pred) area_intersect = w_union * h_union area_union = area_gt + area_pred - area_intersect L_AABB = -tf.log((area_intersect + 1.0)/(area_union + 1.0)) L_theta = 1 - tf.cos(theta_pred - theta_gt) tf.summary.scalar('geometry_AABB', tf.reduce_mean(L_AABB * y_true_cls * training_mask)) tf.summary.scalar('geometry_theta', tf.reduce_mean(L_theta * y_true_cls * training_mask)) L_g = L_AABB + 20 * L_theta return tf.reduce_mean(L_g * y_true_cls * training_mask) + classification_loss â€”|â€” train1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 | def main(argv=None): import os os.environ['CUDA_VISIBLE_DEVICES'] = FLAGS.gpu_list config = None config.batch_size = FLAGS.batch_size_per_gpu * FLAGS.num_gpus if not tf.gfile.Exists(FLAGS.checkpoint_path): tf.gfile.MkDir(FLAGS.checkpoint_path) else: if not FLAGS.restore: tf.gfile.DeleteRecursively(FLAGS.checkpoint_path) tf.gfile.MkDir(FLAGS.checkpoint_path) input_images = tf.placeholder(tf.float32, shape=[None, None, None, 3], name='input_images') input_score_maps = tf.placeholder(tf.float32, shape=[None, None, None, 1], name='input_score_maps') if FLAGS.geometry == 'RBOX': input_geo_maps = tf.placeholder(tf.float32, shape=[None, None, None, 5], name='input_geo_maps') else: input_geo_maps = tf.placeholder(tf.float32, shape=[None, None, None, 8], name='input_geo_maps') input_training_masks = tf.placeholder(tf.float32, shape=[None, None, None, 1], name='input_training_masks') global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False) learning_rate = tf.train.exponential_decay(FLAGS.learning_rate, global_step, decay_steps=10000, decay_rate=0.94, staircase=True) # add summary tf.summary.scalar('learning_rate', learning_rate) opt = tf.train.AdamOptimizer(learning_rate) # opt = tf.train.MomentumOptimizer(learning_rate, 0.9) # split input_images_split = tf.split(input_images, len(gpus)) input_score_maps_split = tf.split(input_score_maps, len(gpus)) input_geo_maps_split = tf.split(input_geo_maps, len(gpus)) input_training_masks_split = tf.split(input_training_masks, len(gpus)) tower_grads = [] reuse_variables = None for i, gpu_id in enumerate(gpus): with tf.device('/gpu:%d' % gpu_id): with tf.name_scope('model_%d' % gpu_id) as scope: iis = input_images_split[i] isms = input_score_maps_split[i] igms = input_geo_maps_split[i] itms = input_training_masks_split[i] total_loss, model_loss = tower_loss(iis, isms, igms, itms, reuse_variables) batch_norm_updates_op = tf.group(*tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope)) reuse_variables = True grads = opt.compute_gradients(total_loss) tower_grads.append(grads) grads = average_gradients(tower_grads) apply_gradient_op = opt.apply_gradients(grads, global_step=global_step) summary_op = tf.summary.merge_all() # save moving average variable_averages = tf.train.ExponentialMovingAverage( FLAGS.moving_average_decay, global_step) variables_averages_op = variable_averages.apply(tf.trainable_variables()) # batch norm updates with tf.control_dependencies([variables_averages_op, apply_gradient_op, batch_norm_updates_op]): train_op = tf.no_op(name='train_op') saver = tf.train.Saver(tf.global_variables()) summary_writer = tf.summary.FileWriter(FLAGS.checkpoint_path, tf.get_default_graph()) init = tf.global_variables_initializer() if FLAGS.pretrained_model_path is not None: variable_restore_op = slim.assign_from_checkpoint_fn(FLAGS.pretrained_model_path, slim.get_trainable_variables(), ignore_missing_vars=True) with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess: if FLAGS.restore: print('continue training from previous checkpoint') ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_path) saver.restore(sess, ckpt) else: sess.run(init) if FLAGS.pretrained_model_path is not None: variable_restore_op(sess) # data_generator = icdar.get_batch(num_workers=FLAGS.num_readers, # input_size=FLAGS.input_size, # batch_size=FLAGS.batch_size_per_gpu * len(gpus)) train_data_generator = icdar_single.get_batch_seq(num_workers=FLAGS.num_readers, config=config, is_training=True) start = time.time() for step in range(FLAGS.max_steps): data = next(train_data_generator) ml, tl, _ = sess.run([model_loss, total_loss, train_op], feed_dict={input_images: data[0], input_score_maps: data[2], input_geo_maps: data[3], input_training_masks: data[4]}) if np.isnan(tl): print('Loss diverged, stop training') break if step % 10 == 0: avg_time_per_step = (time.time() - start)/10 avg_examples_per_second = (10 * FLAGS.batch_size_per_gpu * len(gpus))/(time.time() - start) start = time.time() print('Step {:06d}, model loss {:.4f}, total loss {:.4f}, {:.2f} seconds/step, {:.2f} examples/second'.format( step, ml, tl, avg_time_per_step, avg_examples_per_second)) if step % FLAGS.save_checkpoint_steps == 0: saver.save(sess, FLAGS.checkpoint_path + 'model.ckpt', global_step=global_step) if step % FLAGS.save_summary_steps == 0: _, tl, summary_str = sess.run([train_op, total_loss, summary_op], feed_dict={input_images: data[0], input_score_maps: data[2], input_geo_maps: data[3], input_training_masks: data[4]}) summary_writer.add_summary(summary_str, global_step=step) â€”|â€” å‚è€ƒé“¾æ¥ï¼šhttps://github.com/argman/EAST","tags":["TensorFlow"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"å›¾åƒå¢å¼ºå¸¸ç”¨å‡½æ•°","path":"/2019/10/14/20191014-å›¾åƒå¢å¼ºå¸¸ç”¨å‡½æ•°/","content":"å¯¹ç›®æ ‡æ£€æµ‹ä¸€äº›å¸¸ç”¨çš„æ•°æ®å¢å¼ºå‡½æ•° # -*- coding: utf-8 -*-import cv2import numbersimport mathimport randomimport numpy as npfrom skimage.util import random_noise# åœ¨åŸå›¾ä¸Šç”»å‡ºç›®æ ‡æ¡†def show_pic(img, bboxes=None, name='pic'): ''' è¾“å…¥: img:å›¾åƒarray bboxes:å›¾åƒçš„æ‰€æœ‰boudning box list, æ ¼å¼ä¸º[[x_min, y_min, x_max, y_max]....] names:æ¯ä¸ªboxå¯¹åº”çš„åç§° ''' show_img = img.copy() if not isinstance(bboxes, np.ndarray): bboxes = np.array(bboxes) for point in bboxes.astype(np.int): cv2.line(show_img, tuple(point[0]), tuple(point[1]), (255, 0, 0), 2) # tuple æ˜¯åœ¨åŸæœ‰æ•°æ®ä¸ŠåŠ å°æ‹¬å· cv2.line(show_img, tuple(point[1]), tuple(point[2]), (255, 0, 0), 2) cv2.line(show_img, tuple(point[2]), tuple(point[3]), (255, 0, 0), 2) cv2.line(show_img, tuple(point[3]), tuple(point[0]), (255, 0, 0), 2) # cv2.namedWindow(name, 0) # 1è¡¨ç¤ºåŸå›¾ # cv2.moveWindow(name, 0, 0) # cv2.resizeWindow(name, 1200, 800) # å¯è§†åŒ–çš„å›¾ç‰‡å¤§å° cv2.imshow(name, show_img)# å›¾åƒå‡ä¸ºcv2è¯»å–class DataAugment(): def __init__(self): pass def add_noise(self, im: np.ndarray): \"\"\" å¯¹å›¾ç‰‡åŠ å™ªå£° :param img: å›¾åƒarray :return: åŠ å™ªå£°åçš„å›¾åƒarray,ç”±äºè¾“å‡ºçš„åƒç´ æ˜¯åœ¨[0,1]ä¹‹é—´,æ‰€ä»¥å¾—ä¹˜ä»¥255 \"\"\" return (random_noise(im, mode='gaussian', clip=True) * 255).astype(im.dtype) def random_scale(self, im: np.ndarray, text_polys: np.ndarray, scales: np.ndarray or list) - tuple: \"\"\" ä»scalesä¸­éšæœºé€‰æ‹©ä¸€ä¸ªå°ºåº¦ï¼Œå¯¹å›¾ç‰‡å’Œæ–‡æœ¬æ¡†è¿›è¡Œç¼©æ”¾ :param im: åŸå›¾ :param text_polys: æ–‡æœ¬æ¡† :param scales: å°ºåº¦ :return: ç»è¿‡ç¼©æ”¾çš„å›¾ç‰‡å’Œæ–‡æœ¬ \"\"\" tmp_text_polys = text_polys.copy() rd_scale = float(np.random.choice(scales)) im = cv2.resize(im, dsize=None, fx=rd_scale, fy=rd_scale) tmp_text_polys *= rd_scale return im, tmp_text_polys def random_rotate_img_bbox(self, img, text_polys, degrees: numbers.Number or list or tuple or np.ndarray, same_size=False): \"\"\" ä»ç»™å®šçš„è§’åº¦ä¸­é€‰æ‹©ä¸€ä¸ªè§’åº¦ï¼Œå¯¹å›¾ç‰‡å’Œæ–‡æœ¬æ¡†è¿›è¡Œæ—‹è½¬ :param img: å›¾ç‰‡ :param text_polys: æ–‡æœ¬æ¡† :param degrees: è§’åº¦ï¼Œå¯ä»¥æ˜¯ä¸€ä¸ªæ•°å€¼æˆ–è€…list :param same_size: æ˜¯å¦ä¿æŒå’ŒåŸå›¾ä¸€æ ·å¤§ :return: æ—‹è½¬åçš„å›¾ç‰‡å’Œè§’åº¦ \"\"\" if isinstance(degrees, numbers.Number): if degrees 0: raise ValueError(\"If degrees is a single number, it must be positive.\") degrees = (-degrees, degrees) elif isinstance(degrees, list) or isinstance(degrees, tuple) or isinstance(degrees, np.ndarray): if len(degrees) != 2: raise ValueError(\"If degrees is a sequence, it must be of len 2.\") degrees = degrees else: raise Exception('degrees must in Number or list or tuple or np.ndarray') # ---------------------- æ—‹è½¬å›¾åƒ ---------------------- w = img.shape[1] h = img.shape[0] angle = np.random.uniform(degrees[0], degrees[1]) if same_size: nw = w nh = h else: # è§’åº¦å˜å¼§åº¦ rangle = np.deg2rad(angle) # è®¡ç®—æ—‹è½¬ä¹‹åå›¾åƒçš„w, h nw = (abs(np.sin(rangle) * h) + abs(np.cos(rangle) * w)) nh = (abs(np.cos(rangle) * h) + abs(np.sin(rangle) * w)) # æ„é€ ä»¿å°„çŸ©é˜µ rot_mat = cv2.getRotationMatrix2D((nw * 0.5, nh * 0.5), angle, 1) # è®¡ç®—åŸå›¾ä¸­å¿ƒç‚¹åˆ°æ–°å›¾ä¸­å¿ƒç‚¹çš„åç§»é‡ rot_move = np.dot(rot_mat, np.array([(nw - w) * 0.5, (nh - h) * 0.5, 0])) # æ›´æ–°ä»¿å°„çŸ©é˜µ rot_mat[0, 2] += rot_move[0] rot_mat[1, 2] += rot_move[1] # ä»¿å°„å˜æ¢ rot_img = cv2.warpAffine(img, rot_mat, (int(math.ceil(nw)), int(math.ceil(nh))), flags=cv2.INTER_LANCZOS4) # ---------------------- çŸ«æ­£bboxåæ ‡ ---------------------- # rot_matæ˜¯æœ€ç»ˆçš„æ—‹è½¬çŸ©é˜µ # è·å–åŸå§‹bboxçš„å››ä¸ªä¸­ç‚¹ï¼Œç„¶åå°†è¿™å››ä¸ªç‚¹è½¬æ¢åˆ°æ—‹è½¬åçš„åæ ‡ç³»ä¸‹ rot_text_polys = list() for bbox in text_polys: point1 = np.dot(rot_mat, np.array([bbox[0, 0], bbox[0, 1], 1])) point2 = np.dot(rot_mat, np.array([bbox[1, 0], bbox[1, 1], 1])) point3 = np.dot(rot_mat, np.array([bbox[2, 0], bbox[2, 1], 1])) point4 = np.dot(rot_mat, np.array([bbox[3, 0], bbox[3, 1], 1])) rot_text_polys.append([point1, point2, point3, point4]) return rot_img, np.array(rot_text_polys, dtype=np.float32) def random_crop(self, imgs, img_size): h, w = imgs[0].shape[0:2] th, tw = img_size if w == tw and h == th: return imgs # labelä¸­å­˜åœ¨æ–‡æœ¬å®ä¾‹ï¼Œå¹¶ä¸”æŒ‰ç…§æ¦‚ç‡è¿›è¡Œè£å‰ª if np.max(imgs[1][:, :, -1]) 0 and random.random() 3.0 / 8.0: # æ–‡æœ¬å®ä¾‹çš„top leftç‚¹ tl = np.min(np.where(imgs[1][:, :, -1] 0), axis=1) - img_size tl[tl 0] = 0 # æ–‡æœ¬å®ä¾‹çš„ bottom right ç‚¹ br = np.max(np.where(imgs[1][:, :, -1] 0), axis=1) - img_size br[br 0] = 0 # ä¿è¯é€‰åˆ°å³ä¸‹è§’ç‚¹æ˜¯ï¼Œæœ‰è¶³å¤Ÿçš„è·ç¦»è¿›è¡Œcrop br[0] = min(br[0], h - th) br[1] = min(br[1], w - tw) i = random.randint(tl[0], br[0]) j = random.randint(tl[1], br[1]) else: i = random.randint(0, h - th) j = random.randint(0, w - tw) # return i, j, th, tw for idx in range(len(imgs)): if len(imgs[idx].shape) == 3: imgs[idx] = imgs[idx][i:i + th, j:j + tw, :] else: imgs[idx] = imgs[idx][i:i + th, j:j + tw] return imgs def resize(self, im: np.ndarray, text_polys: np.ndarray, input_size: numbers.Number or list or tuple or np.ndarray, keep_ratio: bool = False) - tuple: \"\"\" å¯¹å›¾ç‰‡å’Œæ–‡æœ¬æ¡†è¿›è¡Œresize :param im: å›¾ç‰‡ :param text_polys: æ–‡æœ¬æ¡† :param input_size: resizeå°ºå¯¸,æ•°å­—æˆ–è€…listçš„å½¢å¼ï¼Œå¦‚æœä¸ºlistå½¢å¼ï¼Œå°±æ˜¯[w,h] :param keep_ratio: æ˜¯å¦ä¿æŒé•¿å®½æ¯” :return: resizeåçš„å›¾ç‰‡å’Œæ–‡æœ¬æ¡† \"\"\" if isinstance(input_size, numbers.Number): if input_size 0: raise ValueError(\"If input_size is a single number, it must be positive.\") input_size = (input_size, input_size) elif isinstance(input_size, list) or isinstance(input_size, tuple) or isinstance(input_size, np.ndarray): if len(input_size) != 2: raise ValueError(\"If input_size is a sequence, it must be of len 2.\") input_size = (input_size[0], input_size[1]) else: raise Exception('input_size must in Number or list or tuple or np.ndarray') if keep_ratio: # å°†å›¾ç‰‡çŸ­è¾¹padåˆ°å’Œé•¿è¾¹ä¸€æ · h, w, c = im.shape max_h = max(h, input_size[0]) max_w = max(w, input_size[1]) im_padded = np.zeros((max_h, max_w, c), dtype=np.uint8) im_padded[:h, :w] = im.copy() im = im_padded text_polys = text_polys.astype(np.float32) h, w, _ = im.shape im = cv2.resize(im, input_size) w_scale = input_size[0] / float(w) h_scale = input_size[1] / float(h) text_polys[:, :, 0] *= w_scale text_polys[:, :, 1] *= h_scale return im, text_polys def horizontal_flip(self, im: np.ndarray, text_polys: np.ndarray) - tuple: \"\"\" å¯¹å›¾ç‰‡å’Œæ–‡æœ¬æ¡†è¿›è¡Œæ°´å¹³ç¿»è½¬ :param im: å›¾ç‰‡ :param text_polys: æ–‡æœ¬æ¡† :return: æ°´å¹³ç¿»è½¬ä¹‹åçš„å›¾ç‰‡å’Œæ–‡æœ¬æ¡† \"\"\" flip_text_polys = text_polys.copy() flip_im = cv2.flip(im, 1) h, w, _ = flip_im.shape flip_text_polys[:, :, 0] = w - flip_text_polys[:, :, 0] return flip_im, flip_text_polys def vertical_flip(self, im: np.ndarray, text_polys: np.ndarray) - tuple: \"\"\" å¯¹å›¾ç‰‡å’Œæ–‡æœ¬æ¡†è¿›è¡Œç«–ç›´ç¿»è½¬ :param im: å›¾ç‰‡ :param text_polys: æ–‡æœ¬æ¡† :return: ç«–ç›´ç¿»è½¬ä¹‹åçš„å›¾ç‰‡å’Œæ–‡æœ¬æ¡† \"\"\" flip_text_polys = text_polys.copy() flip_im = cv2.flip(im, 0) h, w, _ = flip_im.shape flip_text_polys[:, :, 1] = h - flip_text_polys[:, :, 1] return flip_im, flip_text_polys def test(self, im: np.ndarray, text_polys: np.ndarray): print('éšæœºå°ºåº¦ç¼©æ”¾') t_im, t_text_polys = self.random_scale(im, text_polys, [0.5, 1, 2, 3]) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'random_scale') print('éšæœºæ—‹è½¬') t_im, t_text_polys = self.random_rotate_img_bbox(im, text_polys, 10) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'random_rotate_img_bbox') print('éšæœºè£å‰ª') t_im, t_text_polys = self.random_crop_img_bboxes(im, text_polys) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'random_crop_img_bboxes') print('æ°´å¹³ç¿»è½¬') t_im, t_text_polys = self.horizontal_flip(im, text_polys) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'horizontal_flip') print('ç«–ç›´ç¿»è½¬') t_im, t_text_polys = self.vertical_flip(im, text_polys) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'vertical_flip') show_pic(im, text_polys, 'vertical_flip_ori') print('åŠ å™ªå£°') t_im = self.add_noise(im) print(t_im.shape) show_pic(t_im, text_polys, 'add_noise') show_pic(im, text_polys, 'add_noise_ori')","tags":["è®¡ç®—æœºè§†è§‰"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"å›¾åƒå¢å¼ºå¸¸ç”¨å‡½æ•°","path":"/2019/10/14/å›¾åƒå¢å¼ºå¸¸ç”¨å‡½æ•°/","content":"å¯¹ç›®æ ‡æ£€æµ‹ä¸€äº›å¸¸ç”¨çš„æ•°æ®å¢å¼ºå‡½æ•° 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 | # -*- coding: utf-8 -*- import cv2 import numbers import math import random import numpy as np from skimage.util import random_noise # åœ¨åŸå›¾ä¸Šç”»å‡ºç›®æ ‡æ¡† def show_pic(img, bboxes=None, name='pic'): ''' è¾“å…¥: img:å›¾åƒarray bboxes:å›¾åƒçš„æ‰€æœ‰boudning box list, æ ¼å¼ä¸º[[x_min, y_min, x_max, y_max]....] names:æ¯ä¸ªboxå¯¹åº”çš„åç§° ''' show_img = img.copy() if not isinstance(bboxes, np.ndarray): bboxes = np.array(bboxes) for point in bboxes.astype(np.int): cv2.line(show_img, tuple(point[0]), tuple(point[1]), (255, 0, 0), 2) # tuple æ˜¯åœ¨åŸæœ‰æ•°æ®ä¸ŠåŠ å°æ‹¬å· cv2.line(show_img, tuple(point[1]), tuple(point[2]), (255, 0, 0), 2) cv2.line(show_img, tuple(point[2]), tuple(point[3]), (255, 0, 0), 2) cv2.line(show_img, tuple(point[3]), tuple(point[0]), (255, 0, 0), 2) # cv2.namedWindow(name, 0) # 1è¡¨ç¤ºåŸå›¾ # cv2.moveWindow(name, 0, 0) # cv2.resizeWindow(name, 1200, 800) # å¯è§†åŒ–çš„å›¾ç‰‡å¤§å° cv2.imshow(name, show_img) # å›¾åƒå‡ä¸ºcv2è¯»å– class DataAugment(): def __init__(self): pass def add_noise(self, im: np.ndarray): \"\"\" å¯¹å›¾ç‰‡åŠ å™ªå£° :param img: å›¾åƒarray :return: åŠ å™ªå£°åçš„å›¾åƒarray,ç”±äºè¾“å‡ºçš„åƒç´ æ˜¯åœ¨[0,1]ä¹‹é—´,æ‰€ä»¥å¾—ä¹˜ä»¥255 \"\"\" return (random_noise(im, mode='gaussian', clip=True) * 255).astype(im.dtype) def random_scale(self, im: np.ndarray, text_polys: np.ndarray, scales: np.ndarray or list) - tuple: \"\"\" ä»scalesä¸­éšæœºé€‰æ‹©ä¸€ä¸ªå°ºåº¦ï¼Œå¯¹å›¾ç‰‡å’Œæ–‡æœ¬æ¡†è¿›è¡Œç¼©æ”¾ :param im: åŸå›¾ :param text_polys: æ–‡æœ¬æ¡† :param scales: å°ºåº¦ :return: ç»è¿‡ç¼©æ”¾çš„å›¾ç‰‡å’Œæ–‡æœ¬ \"\"\" tmp_text_polys = text_polys.copy() rd_scale = float(np.random.choice(scales)) im = cv2.resize(im, dsize=None, fx=rd_scale, fy=rd_scale) tmp_text_polys *= rd_scale return im, tmp_text_polys def random_rotate_img_bbox(self, img, text_polys, degrees: numbers.Number or list or tuple or np.ndarray, same_size=False): \"\"\" ä»ç»™å®šçš„è§’åº¦ä¸­é€‰æ‹©ä¸€ä¸ªè§’åº¦ï¼Œå¯¹å›¾ç‰‡å’Œæ–‡æœ¬æ¡†è¿›è¡Œæ—‹è½¬ :param img: å›¾ç‰‡ :param text_polys: æ–‡æœ¬æ¡† :param degrees: è§’åº¦ï¼Œå¯ä»¥æ˜¯ä¸€ä¸ªæ•°å€¼æˆ–è€…list :param same_size: æ˜¯å¦ä¿æŒå’ŒåŸå›¾ä¸€æ ·å¤§ :return: æ—‹è½¬åçš„å›¾ç‰‡å’Œè§’åº¦ \"\"\" if isinstance(degrees, numbers.Number): if degrees 0: raise ValueError(\"If degrees is a single number, it must be positive.\") degrees = (-degrees, degrees) elif isinstance(degrees, list) or isinstance(degrees, tuple) or isinstance(degrees, np.ndarray): if len(degrees) != 2: raise ValueError(\"If degrees is a sequence, it must be of len 2.\") degrees = degrees else: raise Exception('degrees must in Number or list or tuple or np.ndarray') # ---------------------- æ—‹è½¬å›¾åƒ ---------------------- w = img.shape[1] h = img.shape[0] angle = np.random.uniform(degrees[0], degrees[1]) if same_size: nw = w nh = h else: # è§’åº¦å˜å¼§åº¦ rangle = np.deg2rad(angle) # è®¡ç®—æ—‹è½¬ä¹‹åå›¾åƒçš„w, h nw = (abs(np.sin(rangle) * h) + abs(np.cos(rangle) * w)) nh = (abs(np.cos(rangle) * h) + abs(np.sin(rangle) * w)) # æ„é€ ä»¿å°„çŸ©é˜µ rot_mat = cv2.getRotationMatrix2D((nw * 0.5, nh * 0.5), angle, 1) # è®¡ç®—åŸå›¾ä¸­å¿ƒç‚¹åˆ°æ–°å›¾ä¸­å¿ƒç‚¹çš„åç§»é‡ rot_move = np.dot(rot_mat, np.array([(nw - w) * 0.5, (nh - h) * 0.5, 0])) # æ›´æ–°ä»¿å°„çŸ©é˜µ rot_mat[0, 2] += rot_move[0] rot_mat[1, 2] += rot_move[1] # ä»¿å°„å˜æ¢ rot_img = cv2.warpAffine(img, rot_mat, (int(math.ceil(nw)), int(math.ceil(nh))), flags=cv2.INTER_LANCZOS4) # ---------------------- çŸ«æ­£bboxåæ ‡ ---------------------- # rot_matæ˜¯æœ€ç»ˆçš„æ—‹è½¬çŸ©é˜µ # è·å–åŸå§‹bboxçš„å››ä¸ªä¸­ç‚¹ï¼Œç„¶åå°†è¿™å››ä¸ªç‚¹è½¬æ¢åˆ°æ—‹è½¬åçš„åæ ‡ç³»ä¸‹ rot_text_polys = list() for bbox in text_polys: point1 = np.dot(rot_mat, np.array([bbox[0, 0], bbox[0, 1], 1])) point2 = np.dot(rot_mat, np.array([bbox[1, 0], bbox[1, 1], 1])) point3 = np.dot(rot_mat, np.array([bbox[2, 0], bbox[2, 1], 1])) point4 = np.dot(rot_mat, np.array([bbox[3, 0], bbox[3, 1], 1])) rot_text_polys.append([point1, point2, point3, point4]) return rot_img, np.array(rot_text_polys, dtype=np.float32) def random_crop(self, imgs, img_size): h, w = imgs[0].shape[0:2] th, tw = img_size if w == tw and h == th: return imgs # labelä¸­å­˜åœ¨æ–‡æœ¬å®ä¾‹ï¼Œå¹¶ä¸”æŒ‰ç…§æ¦‚ç‡è¿›è¡Œè£å‰ª if np.max(imgs[1][:, :, -1]) 0 and random.random() 3.0 / 8.0: # æ–‡æœ¬å®ä¾‹çš„top leftç‚¹ tl = np.min(np.where(imgs[1][:, :, -1] 0), axis=1) - img_size tl[tl 0] = 0 # æ–‡æœ¬å®ä¾‹çš„ bottom right ç‚¹ br = np.max(np.where(imgs[1][:, :, -1] 0), axis=1) - img_size br[br 0] = 0 # ä¿è¯é€‰åˆ°å³ä¸‹è§’ç‚¹æ˜¯ï¼Œæœ‰è¶³å¤Ÿçš„è·ç¦»è¿›è¡Œcrop br[0] = min(br[0], h - th) br[1] = min(br[1], w - tw) i = random.randint(tl[0], br[0]) j = random.randint(tl[1], br[1]) else: i = random.randint(0, h - th) j = random.randint(0, w - tw) # return i, j, th, tw for idx in range(len(imgs)): if len(imgs[idx].shape) == 3: imgs[idx] = imgs[idx][i:i + th, j:j + tw, :] else: imgs[idx] = imgs[idx][i:i + th, j:j + tw] return imgs def resize(self, im: np.ndarray, text_polys: np.ndarray, input_size: numbers.Number or list or tuple or np.ndarray, keep_ratio: bool = False) - tuple: \"\"\" å¯¹å›¾ç‰‡å’Œæ–‡æœ¬æ¡†è¿›è¡Œresize :param im: å›¾ç‰‡ :param text_polys: æ–‡æœ¬æ¡† :param input_size: resizeå°ºå¯¸,æ•°å­—æˆ–è€…listçš„å½¢å¼ï¼Œå¦‚æœä¸ºlistå½¢å¼ï¼Œå°±æ˜¯[w,h] :param keep_ratio: æ˜¯å¦ä¿æŒé•¿å®½æ¯” :return: resizeåçš„å›¾ç‰‡å’Œæ–‡æœ¬æ¡† \"\"\" if isinstance(input_size, numbers.Number): if input_size 0: raise ValueError(\"If input_size is a single number, it must be positive.\") input_size = (input_size, input_size) elif isinstance(input_size, list) or isinstance(input_size, tuple) or isinstance(input_size, np.ndarray): if len(input_size) != 2: raise ValueError(\"If input_size is a sequence, it must be of len 2.\") input_size = (input_size[0], input_size[1]) else: raise Exception('input_size must in Number or list or tuple or np.ndarray') if keep_ratio: # å°†å›¾ç‰‡çŸ­è¾¹padåˆ°å’Œé•¿è¾¹ä¸€æ · h, w, c = im.shape max_h = max(h, input_size[0]) max_w = max(w, input_size[1]) im_padded = np.zeros((max_h, max_w, c), dtype=np.uint8) im_padded[:h, :w] = im.copy() im = im_padded text_polys = text_polys.astype(np.float32) h, w, _ = im.shape im = cv2.resize(im, input_size) w_scale = input_size[0] / float(w) h_scale = input_size[1] / float(h) text_polys[:, :, 0] *= w_scale text_polys[:, :, 1] *= h_scale return im, text_polys def horizontal_flip(self, im: np.ndarray, text_polys: np.ndarray) - tuple: \"\"\" å¯¹å›¾ç‰‡å’Œæ–‡æœ¬æ¡†è¿›è¡Œæ°´å¹³ç¿»è½¬ :param im: å›¾ç‰‡ :param text_polys: æ–‡æœ¬æ¡† :return: æ°´å¹³ç¿»è½¬ä¹‹åçš„å›¾ç‰‡å’Œæ–‡æœ¬æ¡† \"\"\" flip_text_polys = text_polys.copy() flip_im = cv2.flip(im, 1) h, w, _ = flip_im.shape flip_text_polys[:, :, 0] = w - flip_text_polys[:, :, 0] return flip_im, flip_text_polys def vertical_flip(self, im: np.ndarray, text_polys: np.ndarray) - tuple: \"\"\" å¯¹å›¾ç‰‡å’Œæ–‡æœ¬æ¡†è¿›è¡Œç«–ç›´ç¿»è½¬ :param im: å›¾ç‰‡ :param text_polys: æ–‡æœ¬æ¡† :return: ç«–ç›´ç¿»è½¬ä¹‹åçš„å›¾ç‰‡å’Œæ–‡æœ¬æ¡† \"\"\" flip_text_polys = text_polys.copy() flip_im = cv2.flip(im, 0) h, w, _ = flip_im.shape flip_text_polys[:, :, 1] = h - flip_text_polys[:, :, 1] return flip_im, flip_text_polys def test(self, im: np.ndarray, text_polys: np.ndarray): print('éšæœºå°ºåº¦ç¼©æ”¾') t_im, t_text_polys = self.random_scale(im, text_polys, [0.5, 1, 2, 3]) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'random_scale') print('éšæœºæ—‹è½¬') t_im, t_text_polys = self.random_rotate_img_bbox(im, text_polys, 10) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'random_rotate_img_bbox') print('éšæœºè£å‰ª') t_im, t_text_polys = self.random_crop_img_bboxes(im, text_polys) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'random_crop_img_bboxes') print('æ°´å¹³ç¿»è½¬') t_im, t_text_polys = self.horizontal_flip(im, text_polys) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'horizontal_flip') print('ç«–ç›´ç¿»è½¬') t_im, t_text_polys = self.vertical_flip(im, text_polys) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'vertical_flip') show_pic(im, text_polys, 'vertical_flip_ori') print('åŠ å™ªå£°') t_im = self.add_noise(im) print(t_im.shape) show_pic(t_im, text_polys, 'add_noise') show_pic(im, text_polys, 'add_noise_ori') â€”|â€”","tags":["Python"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"ocrç­”é¢˜å¡è¯†åˆ«","path":"/2019/10/13/20191013-ocrç­”é¢˜å¡è¯†åˆ«/","content":"import cv2def sort_contours(cnts, method=\"left-to-right\"): reverse = False i = 0 if method == \"right-to-left\" or method == \"bottom-to-top\": reverse = True if method == \"top-to-bottom\" or method == \"bottom-to-top\": i = 1 boundingBoxes = [cv2.boundingRect(c) for c in cnts] #ç”¨ä¸€ä¸ªæœ€å°çš„çŸ©å½¢ï¼ŒæŠŠæ‰¾åˆ°çš„å½¢çŠ¶åŒ…èµ·æ¥x,y,h,w (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes), key=lambda b: b[1][i], reverse=reverse)) return cnts, boundingBoxesdef resize(image, width=None, height=None, inter=cv2.INTER_AREA): dim = None (h, w) = image.shape[:2] if width is None and height is None: return image if width is None: r = height / float(h) dim = (int(w * r), height) else: r = width / float(w) dim = (width, int(h * r)) resized = cv2.resize(image, dim, interpolation=inter) return resized # å¯¼å…¥å·¥å…·åŒ…from imutils import contoursimport numpy as npimport argparseimport cv2import myutils_beifen# # è®¾ç½®å‚æ•°# ap = argparse.ArgumentParser()# ap.add_argument(\"-i\", \"--image\", required=True,# help=\"path to input image\")# ap.add_argument(\"-t\", \"--template\", required=True,# help=\"path to template OCR-A image\")# args = vars(ap.parse_args())# æŒ‡å®šä¿¡ç”¨å¡ç±»å‹FIRST_NUMBER = {\t\"3\": \"American Express\",\t\"4\": \"Visa\",\t\"5\": \"MasterCard\",\t\"6\": \"Discover Card\"}# ç»˜å›¾å±•ç¤ºdef cv_show(name,img):\tcv2.imshow(name, img)\tcv2.waitKey(0)\tcv2.destroyAllWindows()# è¯»å–ä¸€ä¸ªæ¨¡æ¿å›¾åƒ# img = cv2.imread(args[\"template\"])img = cv2.imread('images/ocr_a_reference.png')cv_show('img',img)# ç°åº¦å›¾ref = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)cv_show('ref',ref)# äºŒå€¼å›¾åƒref = cv2.threshold(ref, 10, 255, cv2.THRESH_BINARY_INV)[1]cv_show('ref',ref)# è®¡ç®—è½®å»“#cv2.findContours()å‡½æ•°æ¥å—çš„å‚æ•°ä¸ºäºŒå€¼å›¾ï¼Œå³é»‘ç™½çš„ï¼ˆä¸æ˜¯ç°åº¦å›¾ï¼‰,cv2.RETR_EXTERNALåªæ£€æµ‹å¤–è½®å»“ï¼Œcv2.CHAIN_APPROX_SIMPLEåªä¿ç•™ç»ˆç‚¹åæ ‡#è¿”å›çš„listä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯å›¾åƒä¸­çš„ä¸€ä¸ªè½®å»“ref_, refCnts, hierarchy = cv2.findContours(ref.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)cv2.drawContours(img,refCnts,-1,(0,0,255),3) # -1è¡¨ç¤ºç”»æ‰€æœ‰çš„è½®å»“cv_show('img',img)print (np.array(refCnts).shape)refCnts = myutils.sort_contours(refCnts, method=\"left-to-right\")[0] #æ’åºï¼Œä»å·¦åˆ°å³ï¼Œä»ä¸Šåˆ°ä¸‹digits = {}# éå†æ¯ä¸€ä¸ªè½®å»“for (i, c) in enumerate(refCnts):\t# è®¡ç®—å¤–æ¥çŸ©å½¢å¹¶ä¸”resizeæˆåˆé€‚å¤§å°\t(x, y, w, h) = cv2.boundingRect(c)\troi = ref[y:y + h, x:x + w]\troi = cv2.resize(roi, (57, 88))\t# æ¯ä¸€ä¸ªæ•°å­—å¯¹åº”æ¯ä¸€ä¸ªæ¨¡æ¿\tdigits[i] = roi# åˆå§‹åŒ–å·ç§¯æ ¸rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9, 3))sqKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))#è¯»å–è¾“å…¥å›¾åƒï¼Œé¢„å¤„ç†# image = cv2.imread(args[\"image\"])image = cv2.imread('images/credit_card_02.png')cv_show('image',image)image = myutils.resize(image, width=300)gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)cv_show('gray',gray)#ç¤¼å¸½æ“ä½œï¼Œçªå‡ºæ›´æ˜äº®çš„åŒºåŸŸtophat = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, rectKernel)cv_show('tophat',tophat)#gradX = cv2.Sobel(tophat, ddepth=cv2.CV_32F, dx=1, dy=0, #ksize=-1ç›¸å½“äºç”¨3*3çš„\tksize=-1)gradX = np.absolute(gradX)(minVal, maxVal) = (np.min(gradX), np.max(gradX))gradX = (255 * ((gradX - minVal) / (maxVal - minVal)))gradX = gradX.astype(\"uint8\")print (np.array(gradX).shape)cv_show('gradX',gradX)#é€šè¿‡é—­æ“ä½œï¼ˆå…ˆè†¨èƒ€ï¼Œå†è…èš€ï¼‰å°†æ•°å­—è¿åœ¨ä¸€èµ·gradX = cv2.morphologyEx(gradX, cv2.MORPH_CLOSE, rectKernel)cv_show('gradX',gradX)#THRESH_OTSUä¼šè‡ªåŠ¨å¯»æ‰¾åˆé€‚çš„é˜ˆå€¼ï¼Œé€‚åˆåŒå³°ï¼Œéœ€æŠŠé˜ˆå€¼å‚æ•°è®¾ç½®ä¸º0thresh = cv2.threshold(gradX, 0, 255,\tcv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]cv_show('thresh',thresh)#å†æ¥ä¸€ä¸ªé—­æ“ä½œthresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, sqKernel) #å†æ¥ä¸€ä¸ªé—­æ“ä½œcv_show('thresh',thresh)# è®¡ç®—è½®å»“thresh_, threshCnts, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\tcv2.CHAIN_APPROX_SIMPLE)cnts = threshCntscur_img = image.copy()cv2.drawContours(cur_img,cnts,-1,(0,0,255),3)cv_show('img',cur_img)locs = []# éå†è½®å»“for (i, c) in enumerate(cnts):\t# è®¡ç®—çŸ©å½¢\t(x, y, w, h) = cv2.boundingRect(c)\tar = w / float(h)\t# é€‰æ‹©åˆé€‚çš„åŒºåŸŸï¼Œæ ¹æ®å®é™…ä»»åŠ¡æ¥ï¼Œè¿™é‡Œçš„åŸºæœ¬éƒ½æ˜¯å››ä¸ªæ•°å­—ä¸€ç»„\tif ar 2.5 and ar 4.0: if (w 40 and w 55) and (h 10 and h 20): #ç¬¦åˆçš„ç•™ä¸‹æ¥ locs.append((x, y, w, h))# å°†ç¬¦åˆçš„è½®å»“ä»å·¦åˆ°å³æ’åºlocs = sorted(locs, key=lambda x:x[0])output = []# éå†æ¯ä¸€ä¸ªè½®å»“ä¸­çš„æ•°å­—for (i, (gX, gY, gW, gH)) in enumerate(locs):\t# initialize the list of group digits\tgroupOutput = []\t# æ ¹æ®åæ ‡æå–æ¯ä¸€ä¸ªç»„\tgroup = gray[gY - 5:gY + gH + 5, gX - 5:gX + gW + 5]\tcv_show('group',group)\t# é¢„å¤„ç†\tgroup = cv2.threshold(group, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\tcv_show('group',group)\t# è®¡ç®—æ¯ä¸€ç»„çš„è½®å»“\tgroup_,digitCnts,hierarchy = cv2.findContours(group.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\tdigitCnts = contours.sort_contours(digitCnts, method=\"left-to-right\")[0]\t# è®¡ç®—æ¯ä¸€ç»„ä¸­çš„æ¯ä¸€ä¸ªæ•°å€¼\tfor c in digitCnts: # æ‰¾åˆ°å½“å‰æ•°å€¼çš„è½®å»“ï¼Œresizeæˆåˆé€‚çš„çš„å¤§å° (x, y, w, h) = cv2.boundingRect(c) roi = group[y:y + h, x:x + w] roi = cv2.resize(roi, (57, 88)) # æ¨¡æ¿åŒ¹é…è¦æ±‚å¤§å°ä¸€è‡´ cv_show('roi',roi) # è®¡ç®—åŒ¹é…å¾—åˆ† scores = [] # åœ¨æ¨¡æ¿ä¸­è®¡ç®—æ¯ä¸€ä¸ªå¾—åˆ† for (digit, digitROI) in digits.items(): # æ¨¡æ¿åŒ¹é… result = cv2.matchTemplate(roi, digitROI, cv2.TM_CCOEFF) (_, score, _, _) = cv2.minMaxLoc(result) scores.append(score) # å¾—åˆ°æœ€åˆé€‚çš„æ•°å­— groupOutput.append(str(np.argmax(scores)))\t# ç”»å‡ºæ¥\tcv2.rectangle(image, (gX - 5, gY - 5), (gX + gW + 5, gY + gH + 5), (0, 0, 255), 1)\tcv2.putText(image, \"\".join(groupOutput), (gX, gY - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 0, 255), 2)\t# å¾—åˆ°ç»“æœ\toutput.extend(groupOutput)# æ‰“å°ç»“æœprint(\"Credit Card Type: {}\".format(FIRST_NUMBER[output[0]]))print(\"Credit Card #: {}\".format(\"\".join(output)))cv2.imshow(\"Image\", image)cv2.waitKey(0) å‚è€ƒèµ„æ–™ï¼š å”å®‡è¿ª OpenCVè®¡ç®—æœºè§†è§‰å®æˆ˜(Pythonç‰ˆ)","tags":["è®¡ç®—æœºè§†è§‰"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"ocré€è§†å˜æ¢","path":"/2019/10/13/20191013-ocré€è§†å˜æ¢/","content":"# å¯¼å…¥å·¥å…·åŒ…import numpy as npimport argparseimport cv2# è®¾ç½®å‚æ•°ap = argparse.ArgumentParser()ap.add_argument(\"-i\", \"--image\", required = False,\thelp = \"Path to the image to be scanned\",default='images/page.jpg')args = vars(ap.parse_args())def order_points(pts):\t# ä¸€å…±4ä¸ªåæ ‡ç‚¹\trect = np.zeros((4, 2), dtype = \"float32\")\t# æŒ‰é¡ºåºæ‰¾åˆ°å¯¹åº”åæ ‡0123åˆ†åˆ«æ˜¯ å·¦ä¸Šï¼Œå³ä¸Šï¼Œå³ä¸‹ï¼Œå·¦ä¸‹\t# è®¡ç®—å·¦ä¸Šï¼Œå³ä¸‹\ts = pts.sum(axis = 1)\trect[0] = pts[np.argmin(s)]\trect[2] = pts[np.argmax(s)]\t# è®¡ç®—å³ä¸Šå’Œå·¦ä¸‹\tdiff = np.diff(pts, axis = 1)\trect[1] = pts[np.argmin(diff)]\trect[3] = pts[np.argmax(diff)]\treturn rectdef four_point_transform(image, pts):\t# è·å–è¾“å…¥åæ ‡ç‚¹\trect = order_points(pts)\t(tl, tr, br, bl) = rect\t# è®¡ç®—è¾“å…¥çš„wå’Œhå€¼\twidthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\twidthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\tmaxWidth = max(int(widthA), int(widthB))\theightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\theightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\tmaxHeight = max(int(heightA), int(heightB))\t# å˜æ¢åå¯¹åº”åæ ‡ä½ç½®\tdst = np.array([ [0, 0], [maxWidth - 1, 0], [maxWidth - 1, maxHeight - 1], [0, maxHeight - 1]], dtype = \"float32\")\t# è®¡ç®—å˜æ¢çŸ©é˜µ\tM = cv2.getPerspectiveTransform(rect, dst)\twarped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\t# è¿”å›å˜æ¢åç»“æœ\treturn warpeddef resize(image, width=None, height=None, inter=cv2.INTER_AREA):\tdim = None\t(h, w) = image.shape[:2]\tif width is None and height is None: return image\tif width is None: r = height / float(h) dim = (int(w * r), height)\telse: r = width / float(w) dim = (width, int(h * r))\tresized = cv2.resize(image, dim, interpolation=inter)\treturn resized# è¯»å–è¾“å…¥image = cv2.imread(args[\"image\"])#åæ ‡ä¹Ÿä¼šç›¸åŒå˜åŒ–ratio = image.shape[0] / 500.0orig = image.copy()image = resize(orig, height = 500)# é¢„å¤„ç†gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)gray = cv2.GaussianBlur(gray, (5, 5), 0)edged = cv2.Canny(gray, 75, 200)# å±•ç¤ºé¢„å¤„ç†ç»“æœprint(\"STEP 1: è¾¹ç¼˜æ£€æµ‹\")cv2.imshow(\"Image\", image)cv2.imshow(\"Edged\", edged)cv2.waitKey(0)cv2.destroyAllWindows()# è½®å»“æ£€æµ‹cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[1]cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]# éå†è½®å»“for c in cnts:\t# è®¡ç®—è½®å»“è¿‘ä¼¼\tperi = cv2.arcLength(c, True)\t# Cè¡¨ç¤ºè¾“å…¥çš„ç‚¹é›†\t# epsilonè¡¨ç¤ºä»åŸå§‹è½®å»“åˆ°è¿‘ä¼¼è½®å»“çš„æœ€å¤§è·ç¦»ï¼Œå®ƒæ˜¯ä¸€ä¸ªå‡†ç¡®åº¦å‚æ•°\t# Trueè¡¨ç¤ºå°é—­çš„\tapprox = cv2.approxPolyDP(c, 0.02 * peri, True)# è¶Šå°è¶Šç²¾å‡†ï¼Œé•¿åº¦çš„ç™¾åˆ†ä¹‹å¤šå°‘ä½œä¸ºç²¾åº¦\t# 4ä¸ªç‚¹çš„æ—¶å€™å°±æ‹¿å‡ºæ¥\tif len(approx) == 4: screenCnt = approx break# å±•ç¤ºç»“æœprint(\"STEP 2: è·å–è½®å»“\")cv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 2)cv2.imshow(\"Outline\", image)cv2.waitKey(0)cv2.destroyAllWindows()# é€è§†å˜æ¢warped = four_point_transform(orig, screenCnt.reshape(4, 2) * ratio)# äºŒå€¼å¤„ç†warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)ref = cv2.threshold(warped, 100, 255, cv2.THRESH_BINARY)[1]cv2.imwrite('scan.jpg', ref)# å±•ç¤ºç»“æœprint(\"STEP 3: å˜æ¢\")cv2.imshow(\"Original\", resize(orig, height = 650))cv2.imshow(\"Scanned\", resize(ref, height = 650))cv2.waitKey(0) å‚è€ƒèµ„æ–™ï¼š å”å®‡è¿ª OpenCVè®¡ç®—æœºè§†è§‰å®æˆ˜(Pythonç‰ˆ)","tags":["è®¡ç®—æœºè§†è§‰"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"parkåœè½¦åœºé¡¹ç›®å®æˆ˜","path":"/2019/10/13/20191013-parkåœè½¦åœºé¡¹ç›®å®æˆ˜/","content":"park ç±»å¯¹è±¡# coding=utf-8import matplotlib.pyplot as pltimport cv2import os, globimport numpy as npclass Parking: # æ˜¾ç¤ºå›¾ç‰‡ def show_images(self, images, cmap=None): cols = 2 rows = (len(images)+1)//cols plt.figure(figsize=(15,12)) for i, image in enumerate(images): plt.subplot(rows, cols, i+1) cmap = 'gray' if len(image.shape)==2 else cmap plt.imshow(image, cmap=cmap) plt.xticks([]) plt.yticks([]) plt.tight_layout(pad=0, h_pad=0, w_pad=0) plt.show() def cv_show(self, name, img): cv2.imshow(name,img) cv2.waitKey(0) cv2.destroyAllWindows() def select_rgb_white_yello(self, image): # è¿‡æ»¤æ‰èƒŒæ™¯ lower = np.uint8([120,120,120]) upper = np.uint8([255,255,255]) # lower_redå’Œé«˜äºupper_redçš„éƒ¨åˆ†åˆ†åˆ«å˜æˆ0ï¼Œlower_redï½upper_redä¹‹é—´çš„å€¼å˜æˆ255,ç›¸å½“äºè¿‡æ»¤èƒŒæ™¯ white_mask = cv2.inRange(image,lower, upper) self.cv_show('white_mask', white_mask) masked = cv2.bitwise_and(image, image, mask = white_mask) self.cv_show('masked', masked) return masked def convert_gray_sacle(self,image): return cv2.cvtColor(image,cv2.COLOR_RGB2GRAY) # æ£€æµ‹è¾¹ç¼˜ def detect_edges(self, image, low_threshold=50, high_threshold=200): return cv2.Canny(image, low_threshold, high_threshold) def filter_region(self, image, vertices): \"\"\" å‰”é™¤ä¸éœ€è¦çš„åœ°æ–¹ \"\"\" mask = np.zeros_like(image) if len(mask.shape) == 2: cv2.fillPoly(mask, vertices, 255) self.cv_show('mask', mask) return cv2.bitwise_and(image, mask) # æ‰‹åŠ¨é€‰æ‹©åŒºåŸŸ def select_region(self,image): \"\"\" æ‰‹åŠ¨é€‰æ‹©åŒºåŸŸ \"\"\" # first, define the polygon by vertices rows, cols = image.shape[:2] pt_1 = [cols*0.05, rows*0.90] pt_2 = [cols*0.05, rows*0.70] pt_3 = [cols*0.30, rows*0.55] pt_4 = [cols*0.6, rows*0.15] pt_5 = [cols*0.90, rows*0.15] pt_6 = [cols*0.90, rows*0.90] vertices = np.array([[pt_1, pt_2, pt_3, pt_4, pt_5, pt_6]], dtype=np.int32) point_img = image.copy() point_img = cv2.cvtColor(point_img, cv2.COLOR_GRAY2RGB) for point in vertices[0]: cv2.circle(point_img, (point[0],point[1]), 10, (0,0,255), 4) self.cv_show('point_img',point_img) return self.filter_region(image, vertices) def hough_line(self, image): # è¾“å…¥çš„å›¾åƒéœ€è¦æ˜¯è¾¹ç¼˜æ£€æµ‹åçš„ç»“æœ # minLineLengh(çº¿çš„æœ€çŸ­é•¿åº¦ï¼Œæ¯”è¿™ä¸ªçŸ­çš„éƒ½è¢«å¿½ç•¥)å’ŒMaxLineCapï¼ˆä¸¤æ¡ç›´çº¿ä¹‹é—´çš„æœ€å¤§é—´éš”ï¼Œå°äºæ­¤å€¼ï¼Œè®¤ä¸ºæ˜¯ä¸€æ¡ç›´çº¿ï¼‰ # rhoè·ç¦»ç²¾åº¦,thetaè§’åº¦ç²¾åº¦,threshodè¶…è¿‡è®¾å®šé˜ˆå€¼æ‰è¢«æ£€æµ‹å‡ºçº¿æ®µ return cv2.HoughLinesP(image, rho=0.1, theta=np.pi/10, threshold=15, minLineLength=9, maxLineGap=4) def draw_lines(self, image, lines, color=[255,0,0], thickness=2, make_copy=True): # è¿‡æ»¤éœå¤«å˜æ¢æ£€æµ‹å¾—åˆ°ç›´çº¿ if make_copy: image = np.copy(image) cleaned = [] for line in lines: for x1, y1, x2, y2 in line: if abs(y2-y1) =1 and abs(x2-x1) =25 and abs(x2-x1) = 55: cleaned.append((x1,y1,x2,y2)) cv2.line(image, (x1, y1), (x2, y2), color, thickness) print('No lines detected: ', len(cleaned)) return image def identify_blocks(self, image, lines, make_copy=True): if make_copy: new_image = np.copy(image) # step 1: è¿‡æ»¤éƒ¨åˆ†ç›´çº¿ cleaned = [] for line in lines: for x1,y1,x2,y2 in line: if abs(y2-y1) =1 and abs(x2-x1) =25 and abs(x2-x1) = 55: cleaned.append((x1,y1,x2,y2)) # step 2: å¯¹ç›´çº¿æŒ‰ç…§x1è¿›è¡Œæ’åº import operator list1 = sorted(cleaned, key=operator.itemgetter(0,1)) # b=operator.itemgetter(1,0) //å®šä¹‰å‡½æ•°bï¼Œè·å–å¯¹è±¡çš„ç¬¬1ä¸ªåŸŸå’Œç¬¬0ä¸ªçš„å€¼ # b(a) # (2, 1) # step 3: æ‰¾åˆ°å¤šä¸ªåˆ—ï¼Œç›¸å½“äºæ¯åˆ—æ˜¯ä¸€æ’è½¦ clusters = {} dIndex = 0 clus_dist = 10 for i in range(len(list1)-1): distance = abs(list1[i+1][0] - list1[i][0]) if distance = clus_dist: if not dIndex in clusters.keys(): clusters[dIndex] = [] clusters[dIndex].append(list1[i]) clusters[dIndex].append(list1[i + 1]) else: dIndex += 1 # step 4: å¾—åˆ°åæ ‡ rects = {} i = 0 for key in clusters: all_list = clusters[key] cleaned = list(set(all_list)) if len(cleaned) 5: cleaned = sorted(cleaned, key=lambda tup: tup[1]) avg_y1 = cleaned[0][1] avg_y2 = cleaned[-1][1] avg_x1 = 0 avg_x2 = 0 for tup in cleaned: avg_x1 += tup[0] avg_x2 += tup[2] avg_x1 = avg_x1/len(cleaned) avg_x2 = avg_x2/len(cleaned) rects[i] = (avg_x1, avg_y1, avg_x2, avg_y2) i += 1 print(\"Num Parking Lanes: \", len(rects)) # step 5: æŠŠçŸ©å½¢ç”»å‡ºæ¥ buff = 7 for key in rects: tup_topLeft = (int(rects[key][0] - buff), int(rects[key][1])) tup_botRight = (int(rects[key][2] + buff), int(rects[key][3])) cv2.rectangle(new_image, tup_topLeft,tup_botRight,(0,255,0),3) return new_image, rects def draw_parking(self, image, rects, make_copy=True, color=[255,0,0],thickness=2, save=True): if make_copy: new_image = np.copy(image) gap = 15.5 spot_dict = {} # å­—å…¸ï¼šä¸€ä¸ªè½¦ä½å¯¹åº”ä¸€ä¸ªä½ç½® tot_spots = 0 #å¾®è°ƒ adj_y1 = {0: 20, 1:-10, 2:0, 3:-11, 4:28, 5:5, 6:-15, 7:-15, 8:-10, 9:-30, 10:9, 11:-32} adj_y2 = {0: 30, 1: 50, 2:15, 3:10, 4:-15, 5:15, 6:15, 7:-20, 8:15, 9:15, 10:0, 11:30} adj_x1 = {0: -8, 1:-15, 2:-15, 3:-15, 4:-15, 5:-15, 6:-15, 7:-15, 8:-10, 9:-10, 10:-10, 11:0} adj_x2 = {0: 0, 1: 15, 2:15, 3:15, 4:15, 5:15, 6:15, 7:15, 8:10, 9:10, 10:10, 11:0} for key in rects: tup = rects[key] x1 = int(tup[0]+ adj_x1[key]) x2 = int(tup[2]+ adj_x2[key]) y1 = int(tup[1] + adj_y1[key]) y2 = int(tup[3] + adj_y2[key]) cv2.rectangle(new_image, (x1, y1),(x2,y2),(0,255,0),2) num_splits = int(abs(y2-y1)//gap) for i in range(0, num_splits+1): y = int(y1 + i*gap) cv2.line(new_image, (x1, y), (x2, y), color, thickness) if key 0 and key len(rects) -1 : #ç«–ç›´çº¿ x = int((x1 + x2)/2) cv2.line(new_image, (x, y1), (x, y2), color, thickness) # è®¡ç®—æ•°é‡ if key == 0 or key == (len(rects) -1): tot_spots += num_splits +1 else: tot_spots += 2*(num_splits +1) # å­—å…¸å¯¹åº”å¥½ if key == 0 or key == (len(rects) -1): for i in range(0, num_splits+1): cur_len = len(spot_dict) y = int(y1 + i*gap) spot_dict[(x1, y, x2, y+gap)] = cur_len +1 else: for i in range(0, num_splits+1): cur_len = len(spot_dict) y = int(y1 + i*gap) x = int((x1 + x2)/2) spot_dict[(x1, y, x, y+gap)] = cur_len +1 spot_dict[(x, y, x2, y+gap)] = cur_len +2 print(\"total parking spaces: \", tot_spots, cur_len) if save: filename = 'with_parking.jpg' cv2.imwrite(filename, new_image) return new_image, spot_dict def assign_spots_map(self,image, spot_dict, make_copy = True, color=[255, 0, 0], thickness=2): if make_copy: new_image = np.copy(image) for spot in spot_dict.keys(): (x1, y1, x2, y2) = spot cv2.rectangle(new_image, (int(x1),int(y1)), (int(x2),int(y2)), color, thickness) return new_image def save_images_for_cnn(self,image, spot_dict, folder_name ='cnn_data'): for spot in spot_dict.keys(): (x1, y1, x2, y2) = spot (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2)) #è£å‰ªå›¾åƒ spot_img = image[y1:y2, x1:x2] spot_img = cv2.resize(spot_img, (0,0), fx=2.0, fy=2.0) spot_id = spot_dict[spot] filename = 'spot' + str(spot_id) +'.jpg' print(spot_img.shape, filename, (x1,x2,y1,y2)) cv2.imwrite(os.path.join(folder_name, filename), spot_img) def make_prediction(self,image, model, class_dictionary): # é¢„å¤„ç† img = image/255. # è½¬æ¢æˆ4D tensor image = np.expand_dims(img,axis=0) # ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œè®­ç»ƒ class_predicted = model.predict(image) inID = np.argmax(class_predicted[0]) label = class_dictionary[inID] return label def predict_on_image(self, image, spot_dict, model, class_dictionary, make_copy=True, color = [0,255,0], alpha=0.5): if make_copy: new_image = np.copy(image) overlay = np.copy(image) self.cv_show('new_image',new_image) cnt_empty = 0 all_spots = 0 for spot in spot_dict.key(): all_spots += 1 (x1, y1, x2, y2) = spot (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2)) spot_img = image[y1:y2, x1:x2] spot_img = cv2.resize(spot_img, (48, 48)) label = self.make_prediction(spot_img,model,class_dictionary) if label == 'empty': cv2.rectangle(overlay, (int(x1),int(y1)), (int(x2),int(y2)), color, -1) cnt_empty += 1 cv2.addWeighted(overlay, alpha, new_image, 1 - alpha, 0, new_image) # å›¾åƒèåˆ cv2.putText(new_image, \"Available: %d spots\" %cnt_empty, (30, 95), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) cv2.putText(new_image, \"Total: %d spots\" %all_spots, (30, 125), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) save = False if save: filename = 'with_marking.jpg' cv2.imwrite(filename, new_image) self.cv_show('new_image',new_image) return new_image def predict_on_video(self,video_name,final_spot_dict, model,class_dictionary,ret=True): cap = cv2.VideoCapture(video_name) count = 0 while ret: ret, image = cap.read() count += 1 if count == 5: count = 0 new_image = np.copy(image) overlay = np.copy(image) cnt_empty = 0 all_spots = 0 color = [0, 255, 0] alpha=0.5 for spot in final_spot_dict.keys(): all_spots += 1 (x1, y1, x2, y2) = spot (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2)) spot_img = image[y1:y2, x1:x2] spot_img = cv2.resize(spot_img, (48,48)) label = self.make_prediction(spot_img,model,class_dictionary) if label == 'empty': cv2.rectangle(overlay, (int(x1),int(y1)), (int(x2),int(y2)), color, -1) cnt_empty += 1 cv2.addWeighted(overlay, alpha, new_image, 1 - alpha, 0, new_image) cv2.putText(new_image, \"Available: %d spots\" %cnt_empty, (30, 95), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) cv2.putText(new_image, \"Total: %d spots\" %all_spots, (30, 125), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) cv2.imshow('frame', new_image) if cv2.waitKey(10) 0xFF == ord('q'): break cv2.destroyAllWindows() cap.release() testæ¨¡å—from __future__ import divisionimport matplotlib.pyplot as pltimport cv2import os, globimport numpy as npfrom PIL import Imagefrom keras.applications.imagenet_utils import preprocess_inputfrom keras.models import load_modelfrom keras.preprocessing import imagefrom Parking import Parkingimport picklecwd = os.getcwd()def img_process(test_images,park): white_yellow_images = list(map(park.select_rgb_white_yellow, test_images)) park.show_images(white_yellow_images) gray_images = list(map(park.convert_gray_scale, white_yellow_images)) park.show_images(gray_images) edge_images = list(map(lambda image: park.detect_edges(image), gray_images)) park.show_images(edge_images) roi_images = list(map(park.select_region, edge_images)) park.show_images(roi_images) list_of_lines = list(map(park.hough_lines, roi_images)) line_images = [] for image, lines in zip(test_images, list_of_lines): line_images.append(park.draw_lines(image, lines)) park.show_images(line_images) rect_images = [] rect_coords = [] for image, lines in zip(test_images, list_of_lines): new_image, rects = park.identify_blocks(image, lines) rect_images.append(new_image) rect_coords.append(rects) park.show_images(rect_images) delineated = [] spot_pos = [] for image, rects in zip(test_images, rect_coords): new_image, spot_dict = park.draw_parking(image, rects) delineated.append(new_image) spot_pos.append(spot_dict) park.show_images(delineated) final_spot_dict = spot_pos[1] print(len(final_spot_dict)) with open('spot_dict.pickle', 'wb') as handle: pickle.dump(final_spot_dict, handle, protocol=pickle.HIGHEST_PROTOCOL) park.save_images_for_cnn(test_images[0],final_spot_dict) return final_spot_dictdef keras_model(weights_path): model = load_model(weights_path) return modeldef img_test(test_images,final_spot_dict,model,class_dictionary): for i in range (len(test_images)): predicted_images = park.predict_on_image(test_images[i],final_spot_dict,model,class_dictionary)def video_test(video_name,final_spot_dict,model,class_dictionary): name = video_name cap = cv2.VideoCapture(name) park.predict_on_video(name,final_spot_dict,model,class_dictionary,ret=True)if __name__ == '__main__': test_images = [plt.imread(path) for path in glob.glob('test_images/*.jpg')] weights_path = 'car1.h5' video_name = 'parking_video.mp4' class_dictionary = {} class_dictionary[0] = 'empty' class_dictionary[1] = 'occupied' park = Parking() park.show_images(test_images) final_spot_dict = img_process(test_images,park) model = keras_model(weights_path) img_test(test_images,final_spot_dict,model,class_dictionary) video_test(video_name,final_spot_dict,model,class_dictionary) åˆ©ç”¨cnnè®­ç»ƒå‡ºä¸€ä¸ªäºŒåˆ†ç±»ç½‘ç»œimport numpyimport osfrom keras import applicationsfrom keras.preprocessing.image import ImageDataGeneratorfrom keras import optimizersfrom keras.models import Sequential, Modelfrom keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2Dfrom keras import backend as kfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStoppingfrom keras.models import Sequentialfrom keras.layers.normalization import BatchNormalizationfrom keras.layers.convolutional import Conv2Dfrom keras.layers.convolutional import MaxPooling2Dfrom keras.initializers import TruncatedNormalfrom keras.layers.core import Activationfrom keras.layers.core import Flattenfrom keras.layers.core import Dropoutfrom keras.layers.core import Densefiles_train = 0files_validation = 0cwd = os.getcwd()folder = 'train_data/train'for sub_folder in os.listdir(folder): path, dirs, files = next(os.walk(os.path.join(folder,sub_folder))) files_train += len(files)folder = 'train_data/test'for sub_folder in os.listdir(folder): path, dirs, files = next(os.walk(os.path.join(folder,sub_folder))) files_validation += len(files)print(files_train,files_validation)img_width, img_height = 48, 48train_data_dir = \"train_data/train\"validation_data_dir = \"train_data/test\"nb_train_samples = files_trainnb_validation_samples = files_validationbatch_size = 32epochs = 15num_classes = 2model = applications.VGG16(weights='imagenet', include_top=False, input_shape = (img_width, img_height, 3))for layer in model.layers[:10]: layer.trainable = Falsex = model.outputx = Flatten()(x)predictions = Dense(num_classes, activation=\"softmax\")(x)model_final = Model(input = model.input, output = predictions)model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])train_datagen = ImageDataGenerator(rescale = 1./255,horizontal_flip = True,fill_mode = \"nearest\",zoom_range = 0.1,width_shift_range = 0.1,height_shift_range=0.1,rotation_range=5)test_datagen = ImageDataGenerator(rescale = 1./255,horizontal_flip = True,fill_mode = \"nearest\",zoom_range = 0.1,width_shift_range = 0.1,height_shift_range=0.1,rotation_range=5)train_generator = train_datagen.flow_from_directory(train_data_dir,target_size = (img_height, img_width),batch_size = batch_size,class_mode = \"categorical\")validation_generator = test_datagen.flow_from_directory(validation_data_dir,target_size = (img_height, img_width),class_mode = \"categorical\")checkpoint = ModelCheckpoint(\"car1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')history_object = model_final.fit_generator(train_generator,samples_per_epoch = nb_train_samples,epochs = epochs,validation_data = validation_generator,nb_val_samples = nb_validation_samples,callbacks = [checkpoint, early]) å‚è€ƒèµ„æ–™ï¼š å”å®‡è¿ª OpenCVè®¡ç®—æœºè§†è§‰å®æˆ˜(Pythonç‰ˆ)","tags":["è®¡ç®—æœºè§†è§‰"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"å›¾åƒå¤„ç†-1","path":"/2019/10/13/20191013-å›¾åƒå¤„ç†-1/","content":"ç°åº¦å›¾import cv2 #opencvè¯»å–çš„æ ¼å¼æ˜¯BGRimport numpy as npimport matplotlib.pyplot as plt#Matplotlibæ˜¯RGB%matplotlib inlineimg=cv2.imread('cat.jpg')img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)img_gray.shapecv2.imshow(\"img_gray\", img_gray)cv2.waitKey(0)cv2.destroyAllWindows() HSV H - è‰²è°ƒï¼ˆä¸»æ³¢é•¿ï¼‰ã€‚ S - é¥±å’Œåº¦ï¼ˆçº¯åº¦/é¢œè‰²çš„é˜´å½±ï¼‰ã€‚ Vå€¼ï¼ˆå¼ºåº¦ï¼‰ hsv=cv2.cvtColor(img,cv2.COLOR_BGR2HSV)cv2.imshow(\"hsv\", hsv)cv2.waitKey(0)cv2.destroyAllWindows() å›¾åƒé˜ˆå€¼ret, dst = cv2.threshold(src, thresh, maxval, type) srcï¼š è¾“å…¥å›¾ï¼Œåªèƒ½è¾“å…¥å•é€šé“å›¾åƒï¼Œé€šå¸¸æ¥è¯´ä¸ºç°åº¦å›¾ dstï¼š è¾“å‡ºå›¾ threshï¼š é˜ˆå€¼ maxvalï¼š å½“åƒç´ å€¼è¶…è¿‡äº†é˜ˆå€¼ï¼ˆæˆ–è€…å°äºé˜ˆå€¼ï¼Œæ ¹æ®typeæ¥å†³å®šï¼‰ï¼Œæ‰€èµ‹äºˆçš„å€¼ typeï¼šäºŒå€¼åŒ–æ“ä½œçš„ç±»å‹ï¼ŒåŒ…å«ä»¥ä¸‹5ç§ç±»å‹ï¼š cv2.THRESH_BINARYï¼› cv2.THRESH_BINARY_INVï¼› cv2.THRESH_TRUNCï¼› cv2.THRESH_TOZEROï¼› cv2.THRESH_TOZERO_INV cv2.THRESH_BINARY è¶…è¿‡é˜ˆå€¼éƒ¨åˆ†å–maxvalï¼ˆæœ€å¤§å€¼ï¼‰ï¼Œå¦åˆ™å–0 cv2.THRESH_BINARY_INV THRESH_BINARYçš„åè½¬ cv2.THRESH_TRUNC å¤§äºé˜ˆå€¼éƒ¨åˆ†è®¾ä¸ºé˜ˆå€¼ï¼Œå¦åˆ™ä¸å˜ cv2.THRESH_TOZERO å¤§äºé˜ˆå€¼éƒ¨åˆ†ä¸æ”¹å˜ï¼Œå¦åˆ™è®¾ä¸º0 cv2.THRESH_TOZERO_INV THRESH_TOZEROçš„åè½¬ | ret, thresh1 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_BINARY) ret, thresh2 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_BINARY_INV) # INV åè½¬ ret, thresh3 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_TRUNC) ret, thresh4 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_TOZERO) ret, thresh5 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_TOZERO_INV) titles = ['Original Image', 'BINARY', 'BINARY_INV', 'TRUNC', 'TOZERO', 'TOZERO_INV'] images = [img, thresh1, thresh2, thresh3, thresh4, thresh5] for i in range(6): plt.subplot(2, 3, i + 1), plt.imshow(images[i], 'gray') plt.title(titles[i]) plt.xticks([]), plt.yticks([]) plt.show() â€”|â€” å›¾åƒå¹³æ»‘img = cv2.imread('lenaNoise.png')cv2.imshow('img', img)cv2.waitKey(0)cv2.destroyAllWindows()# å‡å€¼æ»¤æ³¢# ç®€å•çš„å¹³å‡å·ç§¯æ“ä½œblur = cv2.blur(img, (3, 3))cv2.imshow('blur', blur)cv2.waitKey(0)cv2.destroyAllWindows()# æ–¹æ¡†æ»¤æ³¢# åŸºæœ¬å’Œå‡å€¼ä¸€æ ·ï¼Œå¯ä»¥é€‰æ‹©å½’ä¸€åŒ– ï¼Œ-1è¡¨ç¤ºé¢œè‰²é€šé“æ˜¯ä¸€è‡³çš„box = cv2.boxFilter(img,-1,(3,3), normalize=True)cv2.imshow('box', box)cv2.waitKey(0)cv2.destroyAllWindows()# æ–¹æ¡†æ»¤æ³¢# åŸºæœ¬å’Œå‡å€¼ä¸€æ ·ï¼Œå¯ä»¥é€‰æ‹©å½’ä¸€åŒ–,å®¹æ˜“è¶Šç•Œ ï¼Œåšå½’ä¸€åŒ–çš„ç»“æœè·Ÿå‡å€¼æ»¤æ³¢æ˜¯ä¸€æ ·çš„box = cv2.boxFilter(img,-1,(3,3), normalize=False)cv2.imshow('box', box)cv2.waitKey(0)cv2.destroyAllWindows()# é«˜æ–¯æ»¤æ³¢# é«˜æ–¯æ¨¡ç³Šçš„å·ç§¯æ ¸é‡Œçš„æ•°å€¼æ˜¯æ»¡è¶³é«˜æ–¯åˆ†å¸ƒï¼Œç›¸å½“äºæ›´é‡è§†ä¸­é—´çš„aussian = cv2.GaussianBlur(img, (5, 5), 1)cv2.imshow('aussian', aussian)cv2.waitKey(0)cv2.destroyAllWindows()# ä¸­å€¼æ»¤æ³¢# ç›¸å½“äºç”¨ä¸­å€¼ä»£æ›¿median = cv2.medianBlur(img, 5) # ä¸­å€¼æ»¤æ³¢cv2.imshow('median', median)cv2.waitKey(0)cv2.destroyAllWindows()# å±•ç¤ºæ‰€æœ‰çš„res = np.hstack((blur,aussian,median))#print (res)cv2.imshow('median vs average', res)cv2.waitKey(0)cv2.destroyAllWindows() å½¢æ€å­¦-è…èš€æ“ä½œimg = cv2.imread('dige.png')cv2.imshow('img', img)cv2.waitKey(0)cv2.destroyAllWindows()kernel = np.ones((3,3),np.uint8)erosion = cv2.erode(img,kernel,iterations = 1) # erodeå‡½æ•°cv2.imshow('erosion', erosion)cv2.waitKey(0)cv2.destroyAllWindows()pie = cv2.imread('pie.png')cv2.imshow('pie', pie)cv2.waitKey(0)cv2.destroyAllWindows()kernel = np.ones((30,30),np.uint8)erosion_1 = cv2.erode(pie,kernel,iterations = 1)erosion_2 = cv2.erode(pie,kernel,iterations = 2)erosion_3 = cv2.erode(pie,kernel,iterations = 3)res = np.hstack((erosion_1,erosion_2,erosion_3))cv2.imshow('res', res)cv2.waitKey(0)cv2.destroyAllWindows() å½¢æ€å­¦-è†¨èƒ€æ“ä½œimg = cv2.imread('dige.png')cv2.imshow('img', img)cv2.waitKey(0)cv2.destroyAllWindows()kernel = np.ones((3,3),np.uint8)dige_erosion = cv2.erode(img,kernel,iterations = 1)cv2.imshow('erosion', erosion)cv2.waitKey(0)cv2.destroyAllWindows()kernel = np.ones((3,3),np.uint8)dige_dilate = cv2.dilate(dige_erosion,kernel,iterations = 1)cv2.imshow('dilate', dige_dilate)cv2.waitKey(0)cv2.destroyAllWindows()pie = cv2.imread('pie.png')kernel = np.ones((30,30),np.uint8)dilate_1 = cv2.dilate(pie,kernel,iterations = 1)dilate_2 = cv2.dilate(pie,kernel,iterations = 2)dilate_3 = cv2.dilate(pie,kernel,iterations = 3)res = np.hstack((dilate_1,dilate_2,dilate_3))cv2.imshow('res', res)cv2.waitKey(0)cv2.destroyAllWindows() å¼€è¿ç®—ä¸é—­è¿ç®—# å¼€ï¼šå…ˆè…èš€ï¼Œå†è†¨èƒ€img = cv2.imread('dige.png')kernel = np.ones((5,5),np.uint8)opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)cv2.imshow('opening', opening)cv2.waitKey(0)cv2.destroyAllWindows()# é—­ï¼šå…ˆè†¨èƒ€ï¼Œå†è…èš€img = cv2.imread('dige.png')kernel = np.ones((5,5),np.uint8)closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)cv2.imshow('closing', closing)cv2.waitKey(0)cv2.destroyAllWindows() æ¢¯åº¦è¿ç®—# æ¢¯åº¦=è†¨èƒ€-è…èš€pie = cv2.imread('pie.png')kernel = np.ones((7,7),np.uint8)dilate = cv2.dilate(pie,kernel,iterations = 5)erosion = cv2.erode(pie,kernel,iterations = 5)res = np.hstack((dilate,erosion))cv2.imshow('res', res)cv2.waitKey(0)cv2.destroyAllWindows()gradient = cv2.morphologyEx(pie, cv2.MORPH_GRADIENT, kernel)cv2.imshow('gradient', gradient)cv2.waitKey(0)cv2.destroyAllWindows() ç¤¼å¸½ä¸é»‘å¸½ ç¤¼å¸½ = åŸå§‹è¾“å…¥-å¼€è¿ç®—ç»“æœ é»‘å¸½ = é—­è¿ç®—-åŸå§‹è¾“å…¥ | #ç¤¼å¸½ img = cv2.imread('dige.png') tophat = cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel) cv2.imshow('tophat', tophat) cv2.waitKey(0) cv2.destroyAllWindows() #é»‘å¸½ img = cv2.imread('dige.png') blackhat = cv2.morphologyEx(img,cv2.MORPH_BLACKHAT, kernel) cv2.imshow('blackhat ', blackhat ) cv2.waitKey(0) cv2.destroyAllWindows() â€”|â€” å‚è€ƒèµ„æ–™ï¼š å”å®‡è¿ª OpenCVè®¡ç®—æœºè§†è§‰å®æˆ˜(Pythonç‰ˆ)","tags":["è®¡ç®—æœºè§†è§‰"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"ocrç­”é¢˜å¡è¯†åˆ«","path":"/2019/10/13/ocrç­”é¢˜å¡è¯†åˆ«/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 | import cv2 def sort_contours(cnts, method=\"left-to-right\"): reverse = False i = 0 if method == \"right-to-left\" or method == \"bottom-to-top\": reverse = True if method == \"top-to-bottom\" or method == \"bottom-to-top\": i = 1 boundingBoxes = [cv2.boundingRect(c) for c in cnts] #ç”¨ä¸€ä¸ªæœ€å°çš„çŸ©å½¢ï¼ŒæŠŠæ‰¾åˆ°çš„å½¢çŠ¶åŒ…èµ·æ¥x,y,h,w (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes), key=lambda b: b[1][i], reverse=reverse)) return cnts, boundingBoxes def resize(image, width=None, height=None, inter=cv2.INTER_AREA): dim = None (h, w) = image.shape[:2] if width is None and height is None: return image if width is None: r = height / float(h) dim = (int(w * r), height) else: r = width / float(w) dim = (width, int(h * r)) resized = cv2.resize(image, dim, interpolation=inter) return resized â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 | # å¯¼å…¥å·¥å…·åŒ… from imutils import contours import numpy as np import argparse import cv2 import myutils_beifen # # è®¾ç½®å‚æ•° # ap = argparse.ArgumentParser() # ap.add_argument(\"-i\", \"--image\", required=True, # help=\"path to input image\") # ap.add_argument(\"-t\", \"--template\", required=True, # help=\"path to template OCR-A image\") # args = vars(ap.parse_args()) # æŒ‡å®šä¿¡ç”¨å¡ç±»å‹ FIRST_NUMBER = { \"3\": \"American Express\", \"4\": \"Visa\", \"5\": \"MasterCard\", \"6\": \"Discover Card\" } # ç»˜å›¾å±•ç¤º def cv_show(name,img): cv2.imshow(name, img) cv2.waitKey(0) cv2.destroyAllWindows() # è¯»å–ä¸€ä¸ªæ¨¡æ¿å›¾åƒ # img = cv2.imread(args[\"template\"]) img = cv2.imread('images/ocr_a_reference.png') cv_show('img',img) # ç°åº¦å›¾ ref = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv_show('ref',ref) # äºŒå€¼å›¾åƒ ref = cv2.threshold(ref, 10, 255, cv2.THRESH_BINARY_INV)[1] cv_show('ref',ref) # è®¡ç®—è½®å»“ #cv2.findContours()å‡½æ•°æ¥å—çš„å‚æ•°ä¸ºäºŒå€¼å›¾ï¼Œå³é»‘ç™½çš„ï¼ˆä¸æ˜¯ç°åº¦å›¾ï¼‰,cv2.RETR_EXTERNALåªæ£€æµ‹å¤–è½®å»“ï¼Œcv2.CHAIN_APPROX_SIMPLEåªä¿ç•™ç»ˆç‚¹åæ ‡ #è¿”å›çš„listä¸­æ¯ä¸ªå…ƒç´ éƒ½æ˜¯å›¾åƒä¸­çš„ä¸€ä¸ªè½®å»“ ref_, refCnts, hierarchy = cv2.findContours(ref.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE) cv2.drawContours(img,refCnts,-1,(0,0,255),3) # -1è¡¨ç¤ºç”»æ‰€æœ‰çš„è½®å»“ cv_show('img',img) print (np.array(refCnts).shape) refCnts = myutils.sort_contours(refCnts, method=\"left-to-right\")[0] #æ’åºï¼Œä»å·¦åˆ°å³ï¼Œä»ä¸Šåˆ°ä¸‹ digits = {} # éå†æ¯ä¸€ä¸ªè½®å»“ for (i, c) in enumerate(refCnts): # è®¡ç®—å¤–æ¥çŸ©å½¢å¹¶ä¸”resizeæˆåˆé€‚å¤§å° (x, y, w, h) = cv2.boundingRect(c) roi = ref[y:y + h, x:x + w] roi = cv2.resize(roi, (57, 88)) # æ¯ä¸€ä¸ªæ•°å­—å¯¹åº”æ¯ä¸€ä¸ªæ¨¡æ¿ digits[i] = roi # åˆå§‹åŒ–å·ç§¯æ ¸ rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9, 3)) sqKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5)) #è¯»å–è¾“å…¥å›¾åƒï¼Œé¢„å¤„ç† # image = cv2.imread(args[\"image\"]) image = cv2.imread('images/credit_card_02.png') cv_show('image',image) image = myutils.resize(image, width=300) gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) cv_show('gray',gray) #ç¤¼å¸½æ“ä½œï¼Œçªå‡ºæ›´æ˜äº®çš„åŒºåŸŸ tophat = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, rectKernel) cv_show('tophat',tophat) # gradX = cv2.Sobel(tophat, ddepth=cv2.CV_32F, dx=1, dy=0, #ksize=-1ç›¸å½“äºç”¨3*3çš„ ksize=-1) gradX = np.absolute(gradX) (minVal, maxVal) = (np.min(gradX), np.max(gradX)) gradX = (255 * ((gradX - minVal) / (maxVal - minVal))) gradX = gradX.astype(\"uint8\") print (np.array(gradX).shape) cv_show('gradX',gradX) #é€šè¿‡é—­æ“ä½œï¼ˆå…ˆè†¨èƒ€ï¼Œå†è…èš€ï¼‰å°†æ•°å­—è¿åœ¨ä¸€èµ· gradX = cv2.morphologyEx(gradX, cv2.MORPH_CLOSE, rectKernel) cv_show('gradX',gradX) #THRESH_OTSUä¼šè‡ªåŠ¨å¯»æ‰¾åˆé€‚çš„é˜ˆå€¼ï¼Œé€‚åˆåŒå³°ï¼Œéœ€æŠŠé˜ˆå€¼å‚æ•°è®¾ç½®ä¸º0 thresh = cv2.threshold(gradX, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1] cv_show('thresh',thresh) #å†æ¥ä¸€ä¸ªé—­æ“ä½œ thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, sqKernel) #å†æ¥ä¸€ä¸ªé—­æ“ä½œ cv_show('thresh',thresh) # è®¡ç®—è½®å»“ thresh_, threshCnts, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) cnts = threshCnts cur_img = image.copy() cv2.drawContours(cur_img,cnts,-1,(0,0,255),3) cv_show('img',cur_img) locs = [] # éå†è½®å»“ for (i, c) in enumerate(cnts): # è®¡ç®—çŸ©å½¢ (x, y, w, h) = cv2.boundingRect(c) ar = w / float(h) # é€‰æ‹©åˆé€‚çš„åŒºåŸŸï¼Œæ ¹æ®å®é™…ä»»åŠ¡æ¥ï¼Œè¿™é‡Œçš„åŸºæœ¬éƒ½æ˜¯å››ä¸ªæ•°å­—ä¸€ç»„ if ar 2.5 and ar 4.0: if (w 40 and w 55) and (h 10 and h 20): #ç¬¦åˆçš„ç•™ä¸‹æ¥ locs.append((x, y, w, h)) # å°†ç¬¦åˆçš„è½®å»“ä»å·¦åˆ°å³æ’åº locs = sorted(locs, key=lambda x:x[0]) output = [] # éå†æ¯ä¸€ä¸ªè½®å»“ä¸­çš„æ•°å­— for (i, (gX, gY, gW, gH)) in enumerate(locs): # initialize the list of group digits groupOutput = [] # æ ¹æ®åæ ‡æå–æ¯ä¸€ä¸ªç»„ group = gray[gY - 5:gY + gH + 5, gX - 5:gX + gW + 5] cv_show('group',group) # é¢„å¤„ç† group = cv2.threshold(group, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1] cv_show('group',group) # è®¡ç®—æ¯ä¸€ç»„çš„è½®å»“ group_,digitCnts,hierarchy = cv2.findContours(group.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) digitCnts = contours.sort_contours(digitCnts, method=\"left-to-right\")[0] # è®¡ç®—æ¯ä¸€ç»„ä¸­çš„æ¯ä¸€ä¸ªæ•°å€¼ for c in digitCnts: # æ‰¾åˆ°å½“å‰æ•°å€¼çš„è½®å»“ï¼Œresizeæˆåˆé€‚çš„çš„å¤§å° (x, y, w, h) = cv2.boundingRect(c) roi = group[y:y + h, x:x + w] roi = cv2.resize(roi, (57, 88)) # æ¨¡æ¿åŒ¹é…è¦æ±‚å¤§å°ä¸€è‡´ cv_show('roi',roi) # è®¡ç®—åŒ¹é…å¾—åˆ† scores = [] # åœ¨æ¨¡æ¿ä¸­è®¡ç®—æ¯ä¸€ä¸ªå¾—åˆ† for (digit, digitROI) in digits.items(): # æ¨¡æ¿åŒ¹é… result = cv2.matchTemplate(roi, digitROI, cv2.TM_CCOEFF) (_, score, _, _) = cv2.minMaxLoc(result) scores.append(score) # å¾—åˆ°æœ€åˆé€‚çš„æ•°å­— groupOutput.append(str(np.argmax(scores))) # ç”»å‡ºæ¥ cv2.rectangle(image, (gX - 5, gY - 5), (gX + gW + 5, gY + gH + 5), (0, 0, 255), 1) cv2.putText(image, \"\".join(groupOutput), (gX, gY - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 0, 255), 2) # å¾—åˆ°ç»“æœ output.extend(groupOutput) # æ‰“å°ç»“æœ print(\"Credit Card Type: {}\".format(FIRST_NUMBER[output[0]])) print(\"Credit Card #: {}\".format(\"\".join(output))) cv2.imshow(\"Image\", image) cv2.waitKey(0) â€”|â€” å‚è€ƒèµ„æ–™ï¼š å”å®‡è¿ª OpenCVè®¡ç®—æœºè§†è§‰å®æˆ˜(Pythonç‰ˆ)","tags":["OpenCV"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"ocré€è§†å˜æ¢","path":"/2019/10/13/ocré€è§†å˜æ¢/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 | # å¯¼å…¥å·¥å…·åŒ… import numpy as np import argparse import cv2 # è®¾ç½®å‚æ•° ap = argparse.ArgumentParser() ap.add_argument(\"-i\", \"--image\", required = False, help = \"Path to the image to be scanned\",default='images/page.jpg') args = vars(ap.parse_args()) def order_points(pts): # ä¸€å…±4ä¸ªåæ ‡ç‚¹ rect = np.zeros((4, 2), dtype = \"float32\") # æŒ‰é¡ºåºæ‰¾åˆ°å¯¹åº”åæ ‡0123åˆ†åˆ«æ˜¯ å·¦ä¸Šï¼Œå³ä¸Šï¼Œå³ä¸‹ï¼Œå·¦ä¸‹ # è®¡ç®—å·¦ä¸Šï¼Œå³ä¸‹ s = pts.sum(axis = 1) rect[0] = pts[np.argmin(s)] rect[2] = pts[np.argmax(s)] # è®¡ç®—å³ä¸Šå’Œå·¦ä¸‹ diff = np.diff(pts, axis = 1) rect[1] = pts[np.argmin(diff)] rect[3] = pts[np.argmax(diff)] return rect def four_point_transform(image, pts): # è·å–è¾“å…¥åæ ‡ç‚¹ rect = order_points(pts) (tl, tr, br, bl) = rect # è®¡ç®—è¾“å…¥çš„wå’Œhå€¼ widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2)) widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2)) maxWidth = max(int(widthA), int(widthB)) heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2)) heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2)) maxHeight = max(int(heightA), int(heightB)) # å˜æ¢åå¯¹åº”åæ ‡ä½ç½® dst = np.array([ [0, 0], [maxWidth - 1, 0], [maxWidth - 1, maxHeight - 1], [0, maxHeight - 1]], dtype = \"float32\") # è®¡ç®—å˜æ¢çŸ©é˜µ M = cv2.getPerspectiveTransform(rect, dst) warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight)) # è¿”å›å˜æ¢åç»“æœ return warped def resize(image, width=None, height=None, inter=cv2.INTER_AREA): dim = None (h, w) = image.shape[:2] if width is None and height is None: return image if width is None: r = height / float(h) dim = (int(w * r), height) else: r = width / float(w) dim = (width, int(h * r)) resized = cv2.resize(image, dim, interpolation=inter) return resized # è¯»å–è¾“å…¥ image = cv2.imread(args[\"image\"]) #åæ ‡ä¹Ÿä¼šç›¸åŒå˜åŒ– ratio = image.shape[0] / 500.0 orig = image.copy() image = resize(orig, height = 500) # é¢„å¤„ç† gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) gray = cv2.GaussianBlur(gray, (5, 5), 0) edged = cv2.Canny(gray, 75, 200) # å±•ç¤ºé¢„å¤„ç†ç»“æœ print(\"STEP 1: è¾¹ç¼˜æ£€æµ‹\") cv2.imshow(\"Image\", image) cv2.imshow(\"Edged\", edged) cv2.waitKey(0) cv2.destroyAllWindows() # è½®å»“æ£€æµ‹ cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[1] cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5] # éå†è½®å»“ for c in cnts: # è®¡ç®—è½®å»“è¿‘ä¼¼ peri = cv2.arcLength(c, True) # Cè¡¨ç¤ºè¾“å…¥çš„ç‚¹é›† # epsilonè¡¨ç¤ºä»åŸå§‹è½®å»“åˆ°è¿‘ä¼¼è½®å»“çš„æœ€å¤§è·ç¦»ï¼Œå®ƒæ˜¯ä¸€ä¸ªå‡†ç¡®åº¦å‚æ•° # Trueè¡¨ç¤ºå°é—­çš„ approx = cv2.approxPolyDP(c, 0.02 * peri, True)# è¶Šå°è¶Šç²¾å‡†ï¼Œé•¿åº¦çš„ç™¾åˆ†ä¹‹å¤šå°‘ä½œä¸ºç²¾åº¦ # 4ä¸ªç‚¹çš„æ—¶å€™å°±æ‹¿å‡ºæ¥ if len(approx) == 4: screenCnt = approx break # å±•ç¤ºç»“æœ print(\"STEP 2: è·å–è½®å»“\") cv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 2) cv2.imshow(\"Outline\", image) cv2.waitKey(0) cv2.destroyAllWindows() # é€è§†å˜æ¢ warped = four_point_transform(orig, screenCnt.reshape(4, 2) * ratio) # äºŒå€¼å¤„ç† warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY) ref = cv2.threshold(warped, 100, 255, cv2.THRESH_BINARY)[1] cv2.imwrite('scan.jpg', ref) # å±•ç¤ºç»“æœ print(\"STEP 3: å˜æ¢\") cv2.imshow(\"Original\", resize(orig, height = 650)) cv2.imshow(\"Scanned\", resize(ref, height = 650)) cv2.waitKey(0) â€”|â€” å‚è€ƒèµ„æ–™ï¼š å”å®‡è¿ª OpenCVè®¡ç®—æœºè§†è§‰å®æˆ˜(Pythonç‰ˆ)","tags":["Python"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"parkåœè½¦åœºé¡¹ç›®å®æˆ˜","path":"/2019/10/13/parkåœè½¦åœºé¡¹ç›®å®æˆ˜/","content":"park ç±»å¯¹è±¡1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 | # coding=utf-8 import matplotlib.pyplot as plt import cv2 import os, glob import numpy as np class Parking: # æ˜¾ç¤ºå›¾ç‰‡ def show_images(self, images, cmap=None): cols = 2 rows = (len(images)+1)//cols plt.figure(figsize=(15,12)) for i, image in enumerate(images): plt.subplot(rows, cols, i+1) cmap = 'gray' if len(image.shape)==2 else cmap plt.imshow(image, cmap=cmap) plt.xticks([]) plt.yticks([]) plt.tight_layout(pad=0, h_pad=0, w_pad=0) plt.show() def cv_show(self, name, img): cv2.imshow(name,img) cv2.waitKey(0) cv2.destroyAllWindows() def select_rgb_white_yello(self, image): # è¿‡æ»¤æ‰èƒŒæ™¯ lower = np.uint8([120,120,120]) upper = np.uint8([255,255,255]) # lower_redå’Œé«˜äºupper_redçš„éƒ¨åˆ†åˆ†åˆ«å˜æˆ0ï¼Œlower_redï½upper_redä¹‹é—´çš„å€¼å˜æˆ255,ç›¸å½“äºè¿‡æ»¤èƒŒæ™¯ white_mask = cv2.inRange(image,lower, upper) self.cv_show('white_mask', white_mask) masked = cv2.bitwise_and(image, image, mask = white_mask) self.cv_show('masked', masked) return masked def convert_gray_sacle(self,image): return cv2.cvtColor(image,cv2.COLOR_RGB2GRAY) # æ£€æµ‹è¾¹ç¼˜ def detect_edges(self, image, low_threshold=50, high_threshold=200): return cv2.Canny(image, low_threshold, high_threshold) def filter_region(self, image, vertices): \"\"\" å‰”é™¤ä¸éœ€è¦çš„åœ°æ–¹ \"\"\" mask = np.zeros_like(image) if len(mask.shape) == 2: cv2.fillPoly(mask, vertices, 255) self.cv_show('mask', mask) return cv2.bitwise_and(image, mask) # æ‰‹åŠ¨é€‰æ‹©åŒºåŸŸ def select_region(self,image): \"\"\" æ‰‹åŠ¨é€‰æ‹©åŒºåŸŸ \"\"\" # first, define the polygon by vertices rows, cols = image.shape[:2] pt_1 = [cols*0.05, rows*0.90] pt_2 = [cols*0.05, rows*0.70] pt_3 = [cols*0.30, rows*0.55] pt_4 = [cols*0.6, rows*0.15] pt_5 = [cols*0.90, rows*0.15] pt_6 = [cols*0.90, rows*0.90] vertices = np.array([[pt_1, pt_2, pt_3, pt_4, pt_5, pt_6]], dtype=np.int32) point_img = image.copy() point_img = cv2.cvtColor(point_img, cv2.COLOR_GRAY2RGB) for point in vertices[0]: cv2.circle(point_img, (point[0],point[1]), 10, (0,0,255), 4) self.cv_show('point_img',point_img) return self.filter_region(image, vertices) def hough_line(self, image): # è¾“å…¥çš„å›¾åƒéœ€è¦æ˜¯è¾¹ç¼˜æ£€æµ‹åçš„ç»“æœ # minLineLengh(çº¿çš„æœ€çŸ­é•¿åº¦ï¼Œæ¯”è¿™ä¸ªçŸ­çš„éƒ½è¢«å¿½ç•¥)å’ŒMaxLineCapï¼ˆä¸¤æ¡ç›´çº¿ä¹‹é—´çš„æœ€å¤§é—´éš”ï¼Œå°äºæ­¤å€¼ï¼Œè®¤ä¸ºæ˜¯ä¸€æ¡ç›´çº¿ï¼‰ # rhoè·ç¦»ç²¾åº¦,thetaè§’åº¦ç²¾åº¦,threshodè¶…è¿‡è®¾å®šé˜ˆå€¼æ‰è¢«æ£€æµ‹å‡ºçº¿æ®µ return cv2.HoughLinesP(image, rho=0.1, theta=np.pi/10, threshold=15, minLineLength=9, maxLineGap=4) def draw_lines(self, image, lines, color=[255,0,0], thickness=2, make_copy=True): # è¿‡æ»¤éœå¤«å˜æ¢æ£€æµ‹å¾—åˆ°ç›´çº¿ if make_copy: image = np.copy(image) cleaned = [] for line in lines: for x1, y1, x2, y2 in line: if abs(y2-y1) =1 and abs(x2-x1) =25 and abs(x2-x1) = 55: cleaned.append((x1,y1,x2,y2)) cv2.line(image, (x1, y1), (x2, y2), color, thickness) print('No lines detected: ', len(cleaned)) return image def identify_blocks(self, image, lines, make_copy=True): if make_copy: new_image = np.copy(image) # step 1: è¿‡æ»¤éƒ¨åˆ†ç›´çº¿ cleaned = [] for line in lines: for x1,y1,x2,y2 in line: if abs(y2-y1) =1 and abs(x2-x1) =25 and abs(x2-x1) = 55: cleaned.append((x1,y1,x2,y2)) # step 2: å¯¹ç›´çº¿æŒ‰ç…§x1è¿›è¡Œæ’åº import operator list1 = sorted(cleaned, key=operator.itemgetter(0,1)) # b=operator.itemgetter(1,0) //å®šä¹‰å‡½æ•°bï¼Œè·å–å¯¹è±¡çš„ç¬¬1ä¸ªåŸŸå’Œç¬¬0ä¸ªçš„å€¼ # b(a) # (2, 1) # step 3: æ‰¾åˆ°å¤šä¸ªåˆ—ï¼Œç›¸å½“äºæ¯åˆ—æ˜¯ä¸€æ’è½¦ clusters = {} dIndex = 0 clus_dist = 10 for i in range(len(list1)-1): distance = abs(list1[i+1][0] - list1[i][0]) if distance = clus_dist: if not dIndex in clusters.keys(): clusters[dIndex] = [] clusters[dIndex].append(list1[i]) clusters[dIndex].append(list1[i + 1]) else: dIndex += 1 # step 4: å¾—åˆ°åæ ‡ rects = {} i = 0 for key in clusters: all_list = clusters[key] cleaned = list(set(all_list)) if len(cleaned) 5: cleaned = sorted(cleaned, key=lambda tup: tup[1]) avg_y1 = cleaned[0][1] avg_y2 = cleaned[-1][1] avg_x1 = 0 avg_x2 = 0 for tup in cleaned: avg_x1 += tup[0] avg_x2 += tup[2] avg_x1 = avg_x1/len(cleaned) avg_x2 = avg_x2/len(cleaned) rects[i] = (avg_x1, avg_y1, avg_x2, avg_y2) i += 1 print(\"Num Parking Lanes: \", len(rects)) # step 5: æŠŠçŸ©å½¢ç”»å‡ºæ¥ buff = 7 for key in rects: tup_topLeft = (int(rects[key][0] - buff), int(rects[key][1])) tup_botRight = (int(rects[key][2] + buff), int(rects[key][3])) cv2.rectangle(new_image, tup_topLeft,tup_botRight,(0,255,0),3) return new_image, rects def draw_parking(self, image, rects, make_copy=True, color=[255,0,0],thickness=2, save=True): if make_copy: new_image = np.copy(image) gap = 15.5 spot_dict = {} # å­—å…¸ï¼šä¸€ä¸ªè½¦ä½å¯¹åº”ä¸€ä¸ªä½ç½® tot_spots = 0 #å¾®è°ƒ adj_y1 = {0: 20, 1:-10, 2:0, 3:-11, 4:28, 5:5, 6:-15, 7:-15, 8:-10, 9:-30, 10:9, 11:-32} adj_y2 = {0: 30, 1: 50, 2:15, 3:10, 4:-15, 5:15, 6:15, 7:-20, 8:15, 9:15, 10:0, 11:30} adj_x1 = {0: -8, 1:-15, 2:-15, 3:-15, 4:-15, 5:-15, 6:-15, 7:-15, 8:-10, 9:-10, 10:-10, 11:0} adj_x2 = {0: 0, 1: 15, 2:15, 3:15, 4:15, 5:15, 6:15, 7:15, 8:10, 9:10, 10:10, 11:0} for key in rects: tup = rects[key] x1 = int(tup[0]+ adj_x1[key]) x2 = int(tup[2]+ adj_x2[key]) y1 = int(tup[1] + adj_y1[key]) y2 = int(tup[3] + adj_y2[key]) cv2.rectangle(new_image, (x1, y1),(x2,y2),(0,255,0),2) num_splits = int(abs(y2-y1)//gap) for i in range(0, num_splits+1): y = int(y1 + i*gap) cv2.line(new_image, (x1, y), (x2, y), color, thickness) if key 0 and key len(rects) -1 : #ç«–ç›´çº¿ x = int((x1 + x2)/2) cv2.line(new_image, (x, y1), (x, y2), color, thickness) # è®¡ç®—æ•°é‡ if key == 0 or key == (len(rects) -1): tot_spots += num_splits +1 else: tot_spots += 2*(num_splits +1) # å­—å…¸å¯¹åº”å¥½ if key == 0 or key == (len(rects) -1): for i in range(0, num_splits+1): cur_len = len(spot_dict) y = int(y1 + i*gap) spot_dict[(x1, y, x2, y+gap)] = cur_len +1 else: for i in range(0, num_splits+1): cur_len = len(spot_dict) y = int(y1 + i*gap) x = int((x1 + x2)/2) spot_dict[(x1, y, x, y+gap)] = cur_len +1 spot_dict[(x, y, x2, y+gap)] = cur_len +2 print(\"total parking spaces: \", tot_spots, cur_len) if save: filename = 'with_parking.jpg' cv2.imwrite(filename, new_image) return new_image, spot_dict def assign_spots_map(self,image, spot_dict, make_copy = True, color=[255, 0, 0], thickness=2): if make_copy: new_image = np.copy(image) for spot in spot_dict.keys(): (x1, y1, x2, y2) = spot cv2.rectangle(new_image, (int(x1),int(y1)), (int(x2),int(y2)), color, thickness) return new_image def save_images_for_cnn(self,image, spot_dict, folder_name ='cnn_data'): for spot in spot_dict.keys(): (x1, y1, x2, y2) = spot (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2)) #è£å‰ªå›¾åƒ spot_img = image[y1:y2, x1:x2] spot_img = cv2.resize(spot_img, (0,0), fx=2.0, fy=2.0) spot_id = spot_dict[spot] filename = 'spot' + str(spot_id) +'.jpg' print(spot_img.shape, filename, (x1,x2,y1,y2)) cv2.imwrite(os.path.join(folder_name, filename), spot_img) def make_prediction(self,image, model, class_dictionary): # é¢„å¤„ç† img = image/255. # è½¬æ¢æˆ4D tensor image = np.expand_dims(img,axis=0) # ç”¨è®­ç»ƒå¥½çš„æ¨¡å‹è¿›è¡Œè®­ç»ƒ class_predicted = model.predict(image) inID = np.argmax(class_predicted[0]) label = class_dictionary[inID] return label def predict_on_image(self, image, spot_dict, model, class_dictionary, make_copy=True, color = [0,255,0], alpha=0.5): if make_copy: new_image = np.copy(image) overlay = np.copy(image) self.cv_show('new_image',new_image) cnt_empty = 0 all_spots = 0 for spot in spot_dict.key(): all_spots += 1 (x1, y1, x2, y2) = spot (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2)) spot_img = image[y1:y2, x1:x2] spot_img = cv2.resize(spot_img, (48, 48)) label = self.make_prediction(spot_img,model,class_dictionary) if label == 'empty': cv2.rectangle(overlay, (int(x1),int(y1)), (int(x2),int(y2)), color, -1) cnt_empty += 1 cv2.addWeighted(overlay, alpha, new_image, 1 - alpha, 0, new_image) # å›¾åƒèåˆ cv2.putText(new_image, \"Available: %d spots\" %cnt_empty, (30, 95), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) cv2.putText(new_image, \"Total: %d spots\" %all_spots, (30, 125), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) save = False if save: filename = 'with_marking.jpg' cv2.imwrite(filename, new_image) self.cv_show('new_image',new_image) return new_image def predict_on_video(self,video_name,final_spot_dict, model,class_dictionary,ret=True): cap = cv2.VideoCapture(video_name) count = 0 while ret: ret, image = cap.read() count += 1 if count == 5: count = 0 new_image = np.copy(image) overlay = np.copy(image) cnt_empty = 0 all_spots = 0 color = [0, 255, 0] alpha=0.5 for spot in final_spot_dict.keys(): all_spots += 1 (x1, y1, x2, y2) = spot (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2)) spot_img = image[y1:y2, x1:x2] spot_img = cv2.resize(spot_img, (48,48)) label = self.make_prediction(spot_img,model,class_dictionary) if label == 'empty': cv2.rectangle(overlay, (int(x1),int(y1)), (int(x2),int(y2)), color, -1) cnt_empty += 1 cv2.addWeighted(overlay, alpha, new_image, 1 - alpha, 0, new_image) cv2.putText(new_image, \"Available: %d spots\" %cnt_empty, (30, 95), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) cv2.putText(new_image, \"Total: %d spots\" %all_spots, (30, 125), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) cv2.imshow('frame', new_image) if cv2.waitKey(10) 0xFF == ord('q'): break cv2.destroyAllWindows() cap.release() â€”|â€” testæ¨¡å—1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 | from __future__ import division import matplotlib.pyplot as plt import cv2 import os, glob import numpy as np from PIL import Image from keras.applications.imagenet_utils import preprocess_input from keras.models import load_model from keras.preprocessing import image from Parking import Parking import pickle cwd = os.getcwd() def img_process(test_images,park): white_yellow_images = list(map(park.select_rgb_white_yellow, test_images)) park.show_images(white_yellow_images) gray_images = list(map(park.convert_gray_scale, white_yellow_images)) park.show_images(gray_images) edge_images = list(map(lambda image: park.detect_edges(image), gray_images)) park.show_images(edge_images) roi_images = list(map(park.select_region, edge_images)) park.show_images(roi_images) list_of_lines = list(map(park.hough_lines, roi_images)) line_images = [] for image, lines in zip(test_images, list_of_lines): line_images.append(park.draw_lines(image, lines)) park.show_images(line_images) rect_images = [] rect_coords = [] for image, lines in zip(test_images, list_of_lines): new_image, rects = park.identify_blocks(image, lines) rect_images.append(new_image) rect_coords.append(rects) park.show_images(rect_images) delineated = [] spot_pos = [] for image, rects in zip(test_images, rect_coords): new_image, spot_dict = park.draw_parking(image, rects) delineated.append(new_image) spot_pos.append(spot_dict) park.show_images(delineated) final_spot_dict = spot_pos[1] print(len(final_spot_dict)) with open('spot_dict.pickle', 'wb') as handle: pickle.dump(final_spot_dict, handle, protocol=pickle.HIGHEST_PROTOCOL) park.save_images_for_cnn(test_images[0],final_spot_dict) return final_spot_dict def keras_model(weights_path): model = load_model(weights_path) return model def img_test(test_images,final_spot_dict,model,class_dictionary): for i in range (len(test_images)): predicted_images = park.predict_on_image(test_images[i],final_spot_dict,model,class_dictionary) def video_test(video_name,final_spot_dict,model,class_dictionary): name = video_name cap = cv2.VideoCapture(name) park.predict_on_video(name,final_spot_dict,model,class_dictionary,ret=True) if __name__ == '__main__': test_images = [plt.imread(path) for path in glob.glob('test_images/*.jpg')] weights_path = 'car1.h5' video_name = 'parking_video.mp4' class_dictionary = {} class_dictionary[0] = 'empty' class_dictionary[1] = 'occupied' park = Parking() park.show_images(test_images) final_spot_dict = img_process(test_images,park) model = keras_model(weights_path) img_test(test_images,final_spot_dict,model,class_dictionary) video_test(video_name,final_spot_dict,model,class_dictionary) â€”|â€” åˆ©ç”¨cnnè®­ç»ƒå‡ºä¸€ä¸ªäºŒåˆ†ç±»ç½‘ç»œ1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 | import numpy import os from keras import applications from keras.preprocessing.image import ImageDataGenerator from keras import optimizers from keras.models import Sequential, Model from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D from keras import backend as k from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping from keras.models import Sequential from keras.layers.normalization import BatchNormalization from keras.layers.convolutional import Conv2D from keras.layers.convolutional import MaxPooling2D from keras.initializers import TruncatedNormal from keras.layers.core import Activation from keras.layers.core import Flatten from keras.layers.core import Dropout from keras.layers.core import Dense files_train = 0 files_validation = 0 cwd = os.getcwd() folder = 'train_data/train' for sub_folder in os.listdir(folder): path, dirs, files = next(os.walk(os.path.join(folder,sub_folder))) files_train += len(files) folder = 'train_data/test' for sub_folder in os.listdir(folder): path, dirs, files = next(os.walk(os.path.join(folder,sub_folder))) files_validation += len(files) print(files_train,files_validation) img_width, img_height = 48, 48 train_data_dir = \"train_data/train\" validation_data_dir = \"train_data/test\" nb_train_samples = files_train nb_validation_samples = files_validation batch_size = 32 epochs = 15 num_classes = 2 model = applications.VGG16(weights='imagenet', include_top=False, input_shape = (img_width, img_height, 3)) for layer in model.layers[:10]: layer.trainable = False x = model.output x = Flatten()(x) predictions = Dense(num_classes, activation=\"softmax\")(x) model_final = Model(input = model.input, output = predictions) model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"]) train_datagen = ImageDataGenerator( rescale = 1./255, horizontal_flip = True, fill_mode = \"nearest\", zoom_range = 0.1, width_shift_range = 0.1, height_shift_range=0.1, rotation_range=5) test_datagen = ImageDataGenerator( rescale = 1./255, horizontal_flip = True, fill_mode = \"nearest\", zoom_range = 0.1, width_shift_range = 0.1, height_shift_range=0.1, rotation_range=5) train_generator = train_datagen.flow_from_directory( train_data_dir, target_size = (img_height, img_width), batch_size = batch_size, class_mode = \"categorical\") validation_generator = test_datagen.flow_from_directory( validation_data_dir, target_size = (img_height, img_width), class_mode = \"categorical\") checkpoint = ModelCheckpoint(\"car1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1) early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto') history_object = model_final.fit_generator( train_generator, samples_per_epoch = nb_train_samples, epochs = epochs, validation_data = validation_generator, nb_val_samples = nb_validation_samples, callbacks = [checkpoint, early]) â€”|â€” å‚è€ƒèµ„æ–™ï¼š å”å®‡è¿ª OpenCVè®¡ç®—æœºè§†è§‰å®æˆ˜(Pythonç‰ˆ)","tags":["ç¼–ç¨‹"],"categories":["å…¶ä»–"]},{"title":"å›¾åƒå¤„ç†-1","path":"/2019/10/13/å›¾åƒå¤„ç†-1/","content":"ç°åº¦å›¾1 2 3 4 5 6 7 8 9 10 11 12 | import cv2 #opencvè¯»å–çš„æ ¼å¼æ˜¯BGR import numpy as np import matplotlib.pyplot as plt#Matplotlibæ˜¯RGB %matplotlib inline img=cv2.imread('cat.jpg') img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) img_gray.shape cv2.imshow(\"img_gray\", img_gray) cv2.waitKey(0) cv2.destroyAllWindows() â€”|â€” HSV H - è‰²è°ƒï¼ˆä¸»æ³¢é•¿ï¼‰ã€‚ S - é¥±å’Œåº¦ï¼ˆçº¯åº¦/é¢œè‰²çš„é˜´å½±ï¼‰ã€‚ Vå€¼ï¼ˆå¼ºåº¦ï¼‰ 1 2 3 4 5 | hsv=cv2.cvtColor(img,cv2.COLOR_BGR2HSV) cv2.imshow(\"hsv\", hsv) cv2.waitKey(0) cv2.destroyAllWindows() â€”|â€” å›¾åƒé˜ˆå€¼ret, dst = cv2.threshold(src, thresh, maxval, type) srcï¼š è¾“å…¥å›¾ï¼Œåªèƒ½è¾“å…¥å•é€šé“å›¾åƒï¼Œé€šå¸¸æ¥è¯´ä¸ºç°åº¦å›¾ dstï¼š è¾“å‡ºå›¾ threshï¼š é˜ˆå€¼ maxvalï¼š å½“åƒç´ å€¼è¶…è¿‡äº†é˜ˆå€¼ï¼ˆæˆ–è€…å°äºé˜ˆå€¼ï¼Œæ ¹æ®typeæ¥å†³å®šï¼‰ï¼Œæ‰€èµ‹äºˆçš„å€¼ typeï¼šäºŒå€¼åŒ–æ“ä½œçš„ç±»å‹ï¼ŒåŒ…å«ä»¥ä¸‹5ç§ç±»å‹ï¼š cv2.THRESH_BINARYï¼› cv2.THRESH_BINARY_INVï¼› cv2.THRESH_TRUNCï¼› cv2.THRESH_TOZEROï¼› cv2.THRESH_TOZERO_INV cv2.THRESH_BINARY è¶…è¿‡é˜ˆå€¼éƒ¨åˆ†å–maxvalï¼ˆæœ€å¤§å€¼ï¼‰ï¼Œå¦åˆ™å–0 cv2.THRESH_BINARY_INV THRESH_BINARYçš„åè½¬ cv2.THRESH_TRUNC å¤§äºé˜ˆå€¼éƒ¨åˆ†è®¾ä¸ºé˜ˆå€¼ï¼Œå¦åˆ™ä¸å˜ cv2.THRESH_TOZERO å¤§äºé˜ˆå€¼éƒ¨åˆ†ä¸æ”¹å˜ï¼Œå¦åˆ™è®¾ä¸º0 cv2.THRESH_TOZERO_INV THRESH_TOZEROçš„åè½¬ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 | ret, thresh1 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_BINARY) ret, thresh2 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_BINARY_INV) # INV åè½¬ ret, thresh3 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_TRUNC) ret, thresh4 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_TOZERO) ret, thresh5 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_TOZERO_INV) titles = ['Original Image', 'BINARY', 'BINARY_INV', 'TRUNC', 'TOZERO', 'TOZERO_INV'] images = [img, thresh1, thresh2, thresh3, thresh4, thresh5] for i in range(6): plt.subplot(2, 3, i + 1), plt.imshow(images[i], 'gray') plt.title(titles[i]) plt.xticks([]), plt.yticks([]) plt.show() â€”|â€” å›¾åƒå¹³æ»‘1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 | img = cv2.imread('lenaNoise.png') cv2.imshow('img', img) cv2.waitKey(0) cv2.destroyAllWindows() # å‡å€¼æ»¤æ³¢ # ç®€å•çš„å¹³å‡å·ç§¯æ“ä½œ blur = cv2.blur(img, (3, 3)) cv2.imshow('blur', blur) cv2.waitKey(0) cv2.destroyAllWindows() # æ–¹æ¡†æ»¤æ³¢ # åŸºæœ¬å’Œå‡å€¼ä¸€æ ·ï¼Œå¯ä»¥é€‰æ‹©å½’ä¸€åŒ– ï¼Œ-1è¡¨ç¤ºé¢œè‰²é€šé“æ˜¯ä¸€è‡³çš„ box = cv2.boxFilter(img,-1,(3,3), normalize=True) cv2.imshow('box', box) cv2.waitKey(0) cv2.destroyAllWindows() # æ–¹æ¡†æ»¤æ³¢ # åŸºæœ¬å’Œå‡å€¼ä¸€æ ·ï¼Œå¯ä»¥é€‰æ‹©å½’ä¸€åŒ–,å®¹æ˜“è¶Šç•Œ ï¼Œåšå½’ä¸€åŒ–çš„ç»“æœè·Ÿå‡å€¼æ»¤æ³¢æ˜¯ä¸€æ ·çš„ box = cv2.boxFilter(img,-1,(3,3), normalize=False) cv2.imshow('box', box) cv2.waitKey(0) cv2.destroyAllWindows() # é«˜æ–¯æ»¤æ³¢ # é«˜æ–¯æ¨¡ç³Šçš„å·ç§¯æ ¸é‡Œçš„æ•°å€¼æ˜¯æ»¡è¶³é«˜æ–¯åˆ†å¸ƒï¼Œç›¸å½“äºæ›´é‡è§†ä¸­é—´çš„ aussian = cv2.GaussianBlur(img, (5, 5), 1) cv2.imshow('aussian', aussian) cv2.waitKey(0) cv2.destroyAllWindows() # ä¸­å€¼æ»¤æ³¢ # ç›¸å½“äºç”¨ä¸­å€¼ä»£æ›¿ median = cv2.medianBlur(img, 5) # ä¸­å€¼æ»¤æ³¢ cv2.imshow('median', median) cv2.waitKey(0) cv2.destroyAllWindows() # å±•ç¤ºæ‰€æœ‰çš„ res = np.hstack((blur,aussian,median)) #print (res) cv2.imshow('median vs average', res) cv2.waitKey(0) cv2.destroyAllWindows() â€”|â€” å½¢æ€å­¦-è…èš€æ“ä½œ1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 | img = cv2.imread('dige.png') cv2.imshow('img', img) cv2.waitKey(0) cv2.destroyAllWindows() kernel = np.ones((3,3),np.uint8) erosion = cv2.erode(img,kernel,iterations = 1) # erodeå‡½æ•° cv2.imshow('erosion', erosion) cv2.waitKey(0) cv2.destroyAllWindows() pie = cv2.imread('pie.png') cv2.imshow('pie', pie) cv2.waitKey(0) cv2.destroyAllWindows() kernel = np.ones((30,30),np.uint8) erosion_1 = cv2.erode(pie,kernel,iterations = 1) erosion_2 = cv2.erode(pie,kernel,iterations = 2) erosion_3 = cv2.erode(pie,kernel,iterations = 3) res = np.hstack((erosion_1,erosion_2,erosion_3)) cv2.imshow('res', res) cv2.waitKey(0) cv2.destroyAllWindows() â€”|â€” å½¢æ€å­¦-è†¨èƒ€æ“ä½œ1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 | img = cv2.imread('dige.png') cv2.imshow('img', img) cv2.waitKey(0) cv2.destroyAllWindows() kernel = np.ones((3,3),np.uint8) dige_erosion = cv2.erode(img,kernel,iterations = 1) cv2.imshow('erosion', erosion) cv2.waitKey(0) cv2.destroyAllWindows() kernel = np.ones((3,3),np.uint8) dige_dilate = cv2.dilate(dige_erosion,kernel,iterations = 1) cv2.imshow('dilate', dige_dilate) cv2.waitKey(0) cv2.destroyAllWindows() pie = cv2.imread('pie.png') kernel = np.ones((30,30),np.uint8) dilate_1 = cv2.dilate(pie,kernel,iterations = 1) dilate_2 = cv2.dilate(pie,kernel,iterations = 2) dilate_3 = cv2.dilate(pie,kernel,iterations = 3) res = np.hstack((dilate_1,dilate_2,dilate_3)) cv2.imshow('res', res) cv2.waitKey(0) cv2.destroyAllWindows() â€”|â€” å¼€è¿ç®—ä¸é—­è¿ç®—1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 | # å¼€ï¼šå…ˆè…èš€ï¼Œå†è†¨èƒ€ img = cv2.imread('dige.png') kernel = np.ones((5,5),np.uint8) opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel) cv2.imshow('opening', opening) cv2.waitKey(0) cv2.destroyAllWindows() # é—­ï¼šå…ˆè†¨èƒ€ï¼Œå†è…èš€ img = cv2.imread('dige.png') kernel = np.ones((5,5),np.uint8) closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel) cv2.imshow('closing', closing) cv2.waitKey(0) cv2.destroyAllWindows() â€”|â€” æ¢¯åº¦è¿ç®—1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 | # æ¢¯åº¦=è†¨èƒ€-è…èš€ pie = cv2.imread('pie.png') kernel = np.ones((7,7),np.uint8) dilate = cv2.dilate(pie,kernel,iterations = 5) erosion = cv2.erode(pie,kernel,iterations = 5) res = np.hstack((dilate,erosion)) cv2.imshow('res', res) cv2.waitKey(0) cv2.destroyAllWindows() gradient = cv2.morphologyEx(pie, cv2.MORPH_GRADIENT, kernel) cv2.imshow('gradient', gradient) cv2.waitKey(0) cv2.destroyAllWindows() â€”|â€” ç¤¼å¸½ä¸é»‘å¸½ ç¤¼å¸½ = åŸå§‹è¾“å…¥-å¼€è¿ç®—ç»“æœ é»‘å¸½ = é—­è¿ç®—-åŸå§‹è¾“å…¥ 1 2 3 4 5 6 7 8 9 10 11 12 | #ç¤¼å¸½ img = cv2.imread('dige.png') tophat = cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel) cv2.imshow('tophat', tophat) cv2.waitKey(0) cv2.destroyAllWindows() #é»‘å¸½ img = cv2.imread('dige.png') blackhat = cv2.morphologyEx(img,cv2.MORPH_BLACKHAT, kernel) cv2.imshow('blackhat ', blackhat ) cv2.waitKey(0) cv2.destroyAllWindows() â€”|â€” å‚è€ƒèµ„æ–™ï¼š å”å®‡è¿ª OpenCVè®¡ç®—æœºè§†è§‰å®æˆ˜(Pythonç‰ˆ)","tags":["Python"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"zhongjie","path":"/2019/09/30/20190930-zhongjie/","content":"ä¸€åˆ‡ç»ˆæ­¢äºä»Šå¤©å§ï¼","tags":["å…¶ä»–"],"categories":["å…¶ä»–"]},{"title":"zhongjie","path":"/2019/09/30/zhongjie/","content":"ä¸€åˆ‡ç»ˆæ­¢äºä»Šå¤©å§ï¼","tags":["ç¼–ç¨‹"],"categories":["å…¶ä»–"]},{"title":"mosse","path":"/2019/09/14/20190914-mosse/","content":"MOSSEMOSSE(Minimum Output Sum of Squared Error) æ˜¯2010å¹´ çš„CVPRï¼Œå®ƒçš„å…¨åå«åšVisual Object Tracking using Adaptive Correlation Filtersã€‚ MOSSE æ˜¯ç¬¬ä¸€ç¯‡å°†correlation filter(CF) å¼•å…¥object tracking çš„è®ºæ–‡ï¼Œå®ƒä¹Ÿæ˜¯CSKå’ŒKCF/DCFç­‰ç®—æ³•çš„åŸºç¡€ã€‚ CF(ç›¸å…³æ»¤æ³¢)ç›¸å…³ä¸€èˆ¬åˆ†ä¸ºè‡ªç›¸å…³å’Œäº’ç›¸å…³ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸€èˆ¬æŒ‡çš„æ˜¯äº’ç›¸å…³ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸¤ä¸ªä¿¡å·få’Œg fâˆ—è¡¨ç¤ºfçš„å…±è½­ï¼Œäº’ç›¸å…³çš„ç›´æ¥è§£é‡Šå°±æ˜¯è¡¡é‡ä¸¤ä¸ªä¿¡å·åœ¨æŸä¸ªæ—¶åˆ»Ï„æ—¶çš„ç›¸ä¼¼ç¨‹åº¦ã€‚å‡è®¾få’Œgçš„å½¢çŠ¶ä¸€æ ·ï¼Œé‚£ä¹ˆä¸€å®šæ˜¯få’Œgå¯¹é½çš„æ—¶å€™äºŒè€…çš„ç›¸ä¼¼ç¨‹åº¦æœ€å¤§ï¼Œæ­¤æ—¶è¾¾åˆ°æœ€å¤§çš„è¾“å‡ºå“åº”ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼šå·ç§¯è®¡ç®—å’Œç›¸å…³è®¡ç®—çš„å…³ç³» Two-dimensional correlation is equivalent to two-dimensional convolution with the filter matrix rotated 180 degrees. è®ºæ–‡è§£è¯» å°†CFåº”ç”¨åœ¨trackingæ–¹é¢æœ€åŸºæœ¬çš„æ€æƒ³å°±æ˜¯ï¼Œè®¾è®¡ä¸€ä¸ªæ»¤æ³¢æ¨¡æ¿ï¼Œä½¿å¾—è¯¥æ¨¡æ¿ä¸è·Ÿè¸ªç›®æ ‡çš„ROIåšå·ç§¯è¿ç®—ï¼Œå¾—åˆ°æœ€å¤§çš„è¾“å‡ºå“åº”ã€‚ gè¡¨ç¤ºè¾“å‡ºå“åº” fè¡¨ç¤ºè¾“å…¥åŸå§‹å›¾ç‰‡çš„ç°åº¦å›¾åƒ hè¡¨ç¤ºæ»¤æ³¢æ¨¡æ¿ä¸ºäº†ç®€åŒ–è®¡ç®—ï¼Œå°†æ—¶åŸŸçš„å·ç§¯è½¬åŒ–ä¸ºé¢‘åŸŸçš„ç‚¹ä¹˜ç§¯ã€‚æ—¶åŸŸå…¬å¼è¡¨ç¤ºï¼šé¢‘åŸŸå…¬å¼è¡¨ç¤ºï¼šæ‰€ä»¥ç›®æ ‡Hçš„è®¡ç®—ä¸ºï¼šåœ¨è·Ÿè¸ªçš„å…‰ç…§ç­‰å…¶ä»–å› ç´ çš„å½±å“ä¸‹ï¼Œä¸ºäº†æé«˜æ»¤æ³¢æ¨¡æ¿çš„é²æ£’æ€§ï¼Œåœ¨æ–‡ç« ä¸­ä½œè€…å¯¹GroundTruthè¿›è¡Œéšæœºä»¿å°„å˜æ¢å¾—åˆ°ä¸€ç³»åˆ—çš„è®­ç»ƒæ ·æœ¬fiï¼Œgiæ˜¯ç”±é«˜æ–¯å‡½æ•°äº§ç”Ÿçš„å¹¶ä¸”å…¶å³°å€¼ä½ç½®æ˜¯åœ¨fiçš„ä¸­å¿ƒ,æˆ‘ä»¬åŒæ—¶è€ƒè™‘må¸§ä½œä¸ºå‚è€ƒï¼Œè¿™å°±æ˜¯MOSSEæ¨¡å‹çš„æ€æƒ³ï¼Œæœ€ç»ˆè¯¥æ¨¡å‹çš„ç›®æ ‡å‡½æ•°è¡¨ç¤ºä¸ºï¼šå°†ç›®æ ‡å‡½æ•°æœ€å°åŒ–ï¼Œå¯¹ä¸Šå¼åœ¨é¢‘åŸŸè¿›è¡Œæ±‚å¯¼ï¼ˆå¤æ•°åŸŸä¸åŒäºå®æ•°åŸŸï¼‰ï¼Œå¾—åˆ°ï¼šåœ¨è·Ÿè¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åªéœ€è¦å°†ä»¥ä¸Šæ¨¡æ¿ä¸å½“å‰å¸§ä¸æ»¤æ³¢æ¨¡æ¿åšç›¸å…³æ“ä½œï¼Œåœ¨è¾“å‡ºå“åº”ä¸­æ‰¾åˆ°æœ€å¤§å€¼çš„ä½ç½®ï¼Œè¯¥ä½ç½®å°±æ˜¯ç›®æ ‡åœ¨å½“å‰å¸§ä¸­çš„ä½ç½®ã€‚æœ¬æ–‡çš„å‚æ•°æ›´æ–°çš„ç­–ç•¥ä¸ºï¼šå…¶ä¸­ï¼ŒÎ·æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œä¸ºç»éªŒå€¼ã€‚ ç¼ºç‚¹ï¼š è¾“å…¥çš„ç‰¹å¾ä¸ºå•é€šé“ç°åº¦å›¾åƒï¼Œç‰¹å¾è¡¨è¾¾èƒ½åŠ›æœ‰é™ æ²¡æœ‰å°ºåº¦æ›´æ–°ï¼Œå¯¹äºå°ºåº¦å˜åŒ–çš„è·Ÿè¸ªç›®æ ‡ä¸æ•æ„Ÿ ä»£ç è§£æè¿™é‡Œé¢ä¸»è¦åšçš„å°±æ˜¯ ï¼Œåˆå§‹å¸§çš„è¾“å…¥ä¸è¾“å‡ºæ¥æ±‚å‡ºAiä¸Biï¼Œä»è€Œæ±‚å‡ºåˆå§‹çš„æ¨¡æ¿Hiï¼Œä¸‹é¢å°†åˆå§‹çš„Hiä¸å½“å‰å¸§æ‰€åœ¨çš„ä¸Šä¸ªä½ç½®è¿›è¡Œå·ç§¯ï¼Œé¢‘åŸŸä¹Ÿå°±æ˜¯è¿›è¡Œç›¸ä¹˜ã€‚ç„¶åæ‰¾åˆ°æœ€å¤§å€¼çš„ä½ç½®ä¹Ÿå°±æ˜¯å½“å‰ç›®æ ‡çš„ä¸­å¿ƒï¼Œç”±äºå®½é«˜ä¸å˜ï¼Œæ‰€ä»¥åœ¨æ­¤åŸºç¡€ä¸Šæ›´æ–°å®½é«˜å°±å¯ä»¥äº†ï¼Œå®ç°ç›®æ ‡è·Ÿè¸ªã€‚ import numpy as npimport cv2import osfrom utils import linear_mapping, pre_process, random_warp\"\"\"This module implements the basic correlation filter based tracking algorithm -- MOSSEDate: 2018-05-28\"\"\"class mosse: def __init__(self, args, img_path): # get arguments.. self.args = args self.img_path = img_path # get the img lists... self.frame_lists = self._get_img_lists(self.img_path) self.frame_lists.sort() # start to do the object tracking... def start_tracking(self): # get the image of the first frame... (read as gray scale image...) init_img = cv2.imread(self.frame_lists[0]) init_frame = cv2.cvtColor(init_img, cv2.COLOR_BGR2GRAY) init_frame = init_frame.astype(np.float32) # get the init ground truth.. [x, y, width, height] init_gt = cv2.selectROI('demo', init_img, False, False) init_gt = np.array(init_gt).astype(np.int64) # start to draw the gaussian response... response_map = self._get_gauss_response(init_frame, init_gt) # start to create the training set ... # get the goal.. print(init_gt) g = response_map[init_gt[1]:init_gt[1]+init_gt[3], init_gt[0]:init_gt[0]+init_gt[2]] print(g) fi = init_frame[init_gt[1]:init_gt[1]+init_gt[3], init_gt[0]:init_gt[0]+init_gt[2]] G = np.fft.fft2(g) # start to do the pre-training... Ai, Bi = self._pre_training(fi, G) # start the tracking... for idx in range(len(self.frame_lists)): current_frame = cv2.imread(self.frame_lists[idx]) frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY) frame_gray = frame_gray.astype(np.float32) if idx == 0: Ai = self.args.lr * Ai Bi = self.args.lr * Bi pos = init_gt.copy() clip_pos = np.array([pos[0], pos[1], pos[0]+pos[2], pos[1]+pos[3]]).astype(np.int64) else: Hi = Ai / Bi fi = frame_gray[clip_pos[1]:clip_pos[3], clip_pos[0]:clip_pos[2]] fi = pre_process(cv2.resize(fi, (init_gt[2], init_gt[3]))) Gi = Hi * np.fft.fft2(fi) gi = linear_mapping(np.fft.ifft2(Gi)) # find the max pos... max_value = np.max(gi) max_pos = np.where(gi == max_value) dy = int(np.mean(max_pos[0]) - gi.shape[0] / 2) dx = int(np.mean(max_pos[1]) - gi.shape[1] / 2) # update the position... pos[0] = pos[0] + dx pos[1] = pos[1] + dy # trying to get the clipped position [xmin, ymin, xmax, ymax] clip_pos[0] = np.clip(pos[0], 0, current_frame.shape[1]) clip_pos[1] = np.clip(pos[1], 0, current_frame.shape[0]) clip_pos[2] = np.clip(pos[0]+pos[2], 0, current_frame.shape[1]) clip_pos[3] = np.clip(pos[1]+pos[3], 0, current_frame.shape[0]) clip_pos = clip_pos.astype(np.int64) # get the current fi.. fi = frame_gray[clip_pos[1]:clip_pos[3], clip_pos[0]:clip_pos[2]] fi = pre_process(cv2.resize(fi, (init_gt[2], init_gt[3]))) # online update... Ai = self.args.lr * (G * np.conjugate(np.fft.fft2(fi))) + (1 - self.args.lr) * Ai Bi = self.args.lr * (np.fft.fft2(fi) * np.conjugate(np.fft.fft2(fi))) + (1 - self.args.lr) * Bi # visualize the tracking process... cv2.rectangle(current_frame, (pos[0], pos[1]), (pos[0]+pos[2], pos[1]+pos[3]), (255, 0, 0), 2) cv2.imshow('demo', current_frame) cv2.waitKey(100) # if record... save the frames.. if self.args.record: frame_path = 'record_frames/' + self.img_path.split('/')[1] + '/' if not os.path.exists(frame_path): os.mkdir(frame_path) cv2.imwrite(frame_path + str(idx).zfill(5) + '.png', current_frame) # pre train the filter on the first frame... def _pre_training(self, init_frame, G): height, width = G.shape fi = cv2.resize(init_frame, (width, height)) # pre-process img.. fi = pre_process(fi) Ai = G * np.conjugate(np.fft.fft2(fi)) Bi = np.fft.fft2(init_frame) * np.conjugate(np.fft.fft2(init_frame)) for _ in range(self.args.num_pretrain): if self.args.rotate: fi = pre_process(random_warp(init_frame)) else: fi = pre_process(init_frame) Ai = Ai + G * np.conjugate(np.fft.fft2(fi)) Bi = Bi + np.fft.fft2(fi) * np.conjugate(np.fft.fft2(fi)) return Ai, Bi # get the ground-truth gaussian reponse... def _get_gauss_response(self, img, gt): # get the shape of the image.. height, width = img.shape # get the mesh grid... xx, yy = np.meshgrid(np.arange(width), np.arange(height)) # get the center of the object... center_x = gt[0] + 0.5 * gt[2] center_y = gt[1] + 0.5 * gt[3] # cal the distance... dist = (np.square(xx - center_x) + np.square(yy - center_y)) / (2 * self.args.sigma) # get the response map... response = np.exp(-dist) # normalize... response = linear_mapping(response) return response # it will extract the image list def _get_img_lists(self, img_path): frame_list = [] for frame in os.listdir(img_path): if os.path.splitext(frame)[1] == '.jpg': frame_list.append(os.path.join(img_path, frame)) return frame_list # it will get the first ground truth of the video.. def _get_init_ground_truth(self, img_path): gt_path = os.path.join(img_path, 'groundtruth.txt') with open(gt_path, 'r') as f: # just read the first frame... line = f.readline() gt_pos = line.split(',') return [float(element) for element in gt_pos] å‚è€ƒé“¾æ¥ï¼šhttp://simtalk.cn/2017/07/03/Object-Tracking/https://github.com/TianhongDai/mosse-object-tracking","tags":["ç›®æ ‡è·Ÿè¸ª"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"æ ‘çš„å­ç»“æ„","path":"/2019/09/14/20190914-æ ‘çš„å­ç»“æ„/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š829537æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é“¾è¡¨ é¢˜ç›®æè¿°è¾“å…¥ä¸€ä¸ªé“¾è¡¨ï¼Œè¾“å‡ºè¯¥é“¾è¡¨ä¸­å€’æ•°ç¬¬kä¸ªç»“ç‚¹ã€‚ # -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def HasSubtree(self, pRoot1, pRoot2): # write code here if pRoot1 is None or pRoot2 is None: return False result = False if pRoot1.val == pRoot2.val: result = self.isSubtree(pRoot1,pRoot2) if result == False: result = self.HasSubtree(pRoot1.left, pRoot2) | self.HasSubtree(pRoot1.right, pRoot2) return result def isSubtree(self,root1,root2): if root2 is None: return True if root1 is None: return False if root1.val == root2.val: return self.isSubtree(root1.left,root2.left) self.isSubtree(root1.right,root2.right) return False è¿è¡Œæ—¶é—´ï¼š24 mså ç”¨å†…å­˜ï¼š5860K","tags":["æŠ€æœ¯"],"categories":["æŠ€æœ¯"]},{"title":"åˆå¹¶ä¸¤ä¸ªæ’åºçš„é“¾è¡¨","path":"/2019/09/14/20190914-åˆå¹¶ä¸¤ä¸ªæ’åºçš„é“¾è¡¨/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š587032æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é“¾è¡¨ é¢˜ç›®æè¿°è¾“å…¥ä¸¤ä¸ªå•è°ƒé€’å¢çš„é“¾è¡¨ï¼Œè¾“å‡ºä¸¤ä¸ªé“¾è¡¨åˆæˆåçš„é“¾è¡¨ï¼Œå½“ç„¶æˆ‘ä»¬éœ€è¦åˆæˆåçš„é“¾è¡¨æ»¡è¶³å•è°ƒä¸å‡è§„åˆ™ã€‚ /*struct ListNode {\tint val;\tstruct ListNode *next;\tListNode(int x) : val(x), next(NULL) {\t}};*/class Solution {public: ListNode* Merge(ListNode* pHead1, ListNode* pHead2) { if(!pHead1)return pHead2; if(!pHead2)return pHead1; if(pHead1-val=pHead2-val){ pHead1-next= Merge(pHead1-next,pHead2); return pHead1; }else{ pHead2-next= Merge(pHead1,pHead2-next); return pHead2; } }}; # -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # è¿”å›åˆå¹¶ååˆ—è¡¨ def Merge(self, pHead1, pHead2): # write code here if pHead1 is None: return pHead2 if pHead2 is None: return pHead1 if pHead1.val pHead2.val: pHead1.next = self.Merge(pHead1.next,pHead2) return pHead1 else: pHead2.next = self.Merge(pHead1,pHead2.next) return pHead2 è¿è¡Œæ—¶é—´ï¼š3mså ç”¨å†…å­˜ï¼š492k","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"mosse","path":"/2019/09/14/mosse/","content":"MOSSEMOSSE(Minimum Output Sum of Squared Error) æ˜¯2010å¹´ çš„CVPRï¼Œå®ƒçš„å…¨åå«åšVisual Object Tracking using Adaptive Correlation Filtersã€‚ MOSSE æ˜¯ç¬¬ä¸€ç¯‡å°†correlation filter(CF) å¼•å…¥object tracking çš„è®ºæ–‡ï¼Œå®ƒä¹Ÿæ˜¯CSKå’ŒKCF/DCFç­‰ç®—æ³•çš„åŸºç¡€ã€‚ CF(ç›¸å…³æ»¤æ³¢)ç›¸å…³ä¸€èˆ¬åˆ†ä¸ºè‡ªç›¸å…³å’Œäº’ç›¸å…³ï¼Œè¿™é‡Œæˆ‘ä»¬ä¸€èˆ¬æŒ‡çš„æ˜¯äº’ç›¸å…³ï¼Œå‡è®¾æˆ‘ä»¬æœ‰ä¸¤ä¸ªä¿¡å·få’Œg fâˆ—è¡¨ç¤ºfçš„å…±è½­ï¼Œäº’ç›¸å…³çš„ç›´æ¥è§£é‡Šå°±æ˜¯è¡¡é‡ä¸¤ä¸ªä¿¡å·åœ¨æŸä¸ªæ—¶åˆ»Ï„æ—¶çš„ç›¸ä¼¼ç¨‹åº¦ã€‚å‡è®¾få’Œgçš„å½¢çŠ¶ä¸€æ ·ï¼Œé‚£ä¹ˆä¸€å®šæ˜¯få’Œgå¯¹é½çš„æ—¶å€™äºŒè€…çš„ç›¸ä¼¼ç¨‹åº¦æœ€å¤§ï¼Œæ­¤æ—¶è¾¾åˆ°æœ€å¤§çš„è¾“å‡ºå“åº”ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼šå·ç§¯è®¡ç®—å’Œç›¸å…³è®¡ç®—çš„å…³ç³» Two-dimensional correlation is equivalent to two-dimensional convolution with the filter matrix rotated 180 degrees. è®ºæ–‡è§£è¯» å°†CFåº”ç”¨åœ¨trackingæ–¹é¢æœ€åŸºæœ¬çš„æ€æƒ³å°±æ˜¯ï¼Œè®¾è®¡ä¸€ä¸ªæ»¤æ³¢æ¨¡æ¿ï¼Œä½¿å¾—è¯¥æ¨¡æ¿ä¸è·Ÿè¸ªç›®æ ‡çš„ROIåšå·ç§¯è¿ç®—ï¼Œå¾—åˆ°æœ€å¤§çš„è¾“å‡ºå“åº”ã€‚ gè¡¨ç¤ºè¾“å‡ºå“åº” fè¡¨ç¤ºè¾“å…¥åŸå§‹å›¾ç‰‡çš„ç°åº¦å›¾åƒ hè¡¨ç¤ºæ»¤æ³¢æ¨¡æ¿ä¸ºäº†ç®€åŒ–è®¡ç®—ï¼Œå°†æ—¶åŸŸçš„å·ç§¯è½¬åŒ–ä¸ºé¢‘åŸŸçš„ç‚¹ä¹˜ç§¯ã€‚æ—¶åŸŸå…¬å¼è¡¨ç¤ºï¼šé¢‘åŸŸå…¬å¼è¡¨ç¤ºï¼šæ‰€ä»¥ç›®æ ‡Hçš„è®¡ç®—ä¸ºï¼šåœ¨è·Ÿè¸ªçš„å…‰ç…§ç­‰å…¶ä»–å› ç´ çš„å½±å“ä¸‹ï¼Œä¸ºäº†æé«˜æ»¤æ³¢æ¨¡æ¿çš„é²æ£’æ€§ï¼Œåœ¨æ–‡ç« ä¸­ä½œè€…å¯¹GroundTruthè¿›è¡Œéšæœºä»¿å°„å˜æ¢å¾—åˆ°ä¸€ç³»åˆ—çš„è®­ç»ƒæ ·æœ¬fiï¼Œgiæ˜¯ç”±é«˜æ–¯å‡½æ•°äº§ç”Ÿçš„å¹¶ä¸”å…¶å³°å€¼ä½ç½®æ˜¯åœ¨fiçš„ä¸­å¿ƒ,æˆ‘ä»¬åŒæ—¶è€ƒè™‘må¸§ä½œä¸ºå‚è€ƒï¼Œè¿™å°±æ˜¯MOSSEæ¨¡å‹çš„æ€æƒ³ï¼Œæœ€ç»ˆè¯¥æ¨¡å‹çš„ç›®æ ‡å‡½æ•°è¡¨ç¤ºä¸ºï¼šå°†ç›®æ ‡å‡½æ•°æœ€å°åŒ–ï¼Œå¯¹ä¸Šå¼åœ¨é¢‘åŸŸè¿›è¡Œæ±‚å¯¼ï¼ˆå¤æ•°åŸŸä¸åŒäºå®æ•°åŸŸï¼‰ï¼Œå¾—åˆ°ï¼šåœ¨è·Ÿè¸ªè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬åªéœ€è¦å°†ä»¥ä¸Šæ¨¡æ¿ä¸å½“å‰å¸§ä¸æ»¤æ³¢æ¨¡æ¿åšç›¸å…³æ“ä½œï¼Œåœ¨è¾“å‡ºå“åº”ä¸­æ‰¾åˆ°æœ€å¤§å€¼çš„ä½ç½®ï¼Œè¯¥ä½ç½®å°±æ˜¯ç›®æ ‡åœ¨å½“å‰å¸§ä¸­çš„ä½ç½®ã€‚æœ¬æ–‡çš„å‚æ•°æ›´æ–°çš„ç­–ç•¥ä¸ºï¼šå…¶ä¸­ï¼ŒÎ·æ˜¯ä¸€ä¸ªè¶…å‚æ•°ï¼Œä¸ºç»éªŒå€¼ã€‚ ç¼ºç‚¹ï¼š è¾“å…¥çš„ç‰¹å¾ä¸ºå•é€šé“ç°åº¦å›¾åƒï¼Œç‰¹å¾è¡¨è¾¾èƒ½åŠ›æœ‰é™ æ²¡æœ‰å°ºåº¦æ›´æ–°ï¼Œå¯¹äºå°ºåº¦å˜åŒ–çš„è·Ÿè¸ªç›®æ ‡ä¸æ•æ„Ÿ ä»£ç è§£æè¿™é‡Œé¢ä¸»è¦åšçš„å°±æ˜¯ ï¼Œåˆå§‹å¸§çš„è¾“å…¥ä¸è¾“å‡ºæ¥æ±‚å‡ºAiä¸Biï¼Œä»è€Œæ±‚å‡ºåˆå§‹çš„æ¨¡æ¿Hiï¼Œä¸‹é¢å°†åˆå§‹çš„Hiä¸å½“å‰å¸§æ‰€åœ¨çš„ä¸Šä¸ªä½ç½®è¿›è¡Œå·ç§¯ï¼Œé¢‘åŸŸä¹Ÿå°±æ˜¯è¿›è¡Œç›¸ä¹˜ã€‚ç„¶åæ‰¾åˆ°æœ€å¤§å€¼çš„ä½ç½®ä¹Ÿå°±æ˜¯å½“å‰ç›®æ ‡çš„ä¸­å¿ƒï¼Œç”±äºå®½é«˜ä¸å˜ï¼Œæ‰€ä»¥åœ¨æ­¤åŸºç¡€ä¸Šæ›´æ–°å®½é«˜å°±å¯ä»¥äº†ï¼Œå®ç°ç›®æ ‡è·Ÿè¸ªã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 | import numpy as np import cv2 import os from utils import linear_mapping, pre_process, random_warp \"\"\" This module implements the basic correlation filter based tracking algorithm -- MOSSE Date: 2018-05-28 \"\"\" class mosse: def __init__(self, args, img_path): # get arguments.. self.args = args self.img_path = img_path # get the img lists... self.frame_lists = self._get_img_lists(self.img_path) self.frame_lists.sort() # start to do the object tracking... def start_tracking(self): # get the image of the first frame... (read as gray scale image...) init_img = cv2.imread(self.frame_lists[0]) init_frame = cv2.cvtColor(init_img, cv2.COLOR_BGR2GRAY) init_frame = init_frame.astype(np.float32) # get the init ground truth.. [x, y, width, height] init_gt = cv2.selectROI('demo', init_img, False, False) init_gt = np.array(init_gt).astype(np.int64) # start to draw the gaussian response... response_map = self._get_gauss_response(init_frame, init_gt) # start to create the training set ... # get the goal.. print(init_gt) g = response_map[init_gt[1]:init_gt[1]+init_gt[3], init_gt[0]:init_gt[0]+init_gt[2]] print(g) fi = init_frame[init_gt[1]:init_gt[1]+init_gt[3], init_gt[0]:init_gt[0]+init_gt[2]] G = np.fft.fft2(g) # start to do the pre-training... Ai, Bi = self._pre_training(fi, G) # start the tracking... for idx in range(len(self.frame_lists)): current_frame = cv2.imread(self.frame_lists[idx]) frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY) frame_gray = frame_gray.astype(np.float32) if idx == 0: Ai = self.args.lr * Ai Bi = self.args.lr * Bi pos = init_gt.copy() clip_pos = np.array([pos[0], pos[1], pos[0]+pos[2], pos[1]+pos[3]]).astype(np.int64) else: Hi = Ai / Bi fi = frame_gray[clip_pos[1]:clip_pos[3], clip_pos[0]:clip_pos[2]] fi = pre_process(cv2.resize(fi, (init_gt[2], init_gt[3]))) Gi = Hi * np.fft.fft2(fi) gi = linear_mapping(np.fft.ifft2(Gi)) # find the max pos... max_value = np.max(gi) max_pos = np.where(gi == max_value) dy = int(np.mean(max_pos[0]) - gi.shape[0] / 2) dx = int(np.mean(max_pos[1]) - gi.shape[1] / 2) # update the position... pos[0] = pos[0] + dx pos[1] = pos[1] + dy # trying to get the clipped position [xmin, ymin, xmax, ymax] clip_pos[0] = np.clip(pos[0], 0, current_frame.shape[1]) clip_pos[1] = np.clip(pos[1], 0, current_frame.shape[0]) clip_pos[2] = np.clip(pos[0]+pos[2], 0, current_frame.shape[1]) clip_pos[3] = np.clip(pos[1]+pos[3], 0, current_frame.shape[0]) clip_pos = clip_pos.astype(np.int64) # get the current fi.. fi = frame_gray[clip_pos[1]:clip_pos[3], clip_pos[0]:clip_pos[2]] fi = pre_process(cv2.resize(fi, (init_gt[2], init_gt[3]))) # online update... Ai = self.args.lr * (G * np.conjugate(np.fft.fft2(fi))) + (1 - self.args.lr) * Ai Bi = self.args.lr * (np.fft.fft2(fi) * np.conjugate(np.fft.fft2(fi))) + (1 - self.args.lr) * Bi # visualize the tracking process... cv2.rectangle(current_frame, (pos[0], pos[1]), (pos[0]+pos[2], pos[1]+pos[3]), (255, 0, 0), 2) cv2.imshow('demo', current_frame) cv2.waitKey(100) # if record... save the frames.. if self.args.record: frame_path = 'record_frames/' + self.img_path.split('/')[1] + '/' if not os.path.exists(frame_path): os.mkdir(frame_path) cv2.imwrite(frame_path + str(idx).zfill(5) + '.png', current_frame) # pre train the filter on the first frame... def _pre_training(self, init_frame, G): height, width = G.shape fi = cv2.resize(init_frame, (width, height)) # pre-process img.. fi = pre_process(fi) Ai = G * np.conjugate(np.fft.fft2(fi)) Bi = np.fft.fft2(init_frame) * np.conjugate(np.fft.fft2(init_frame)) for _ in range(self.args.num_pretrain): if self.args.rotate: fi = pre_process(random_warp(init_frame)) else: fi = pre_process(init_frame) Ai = Ai + G * np.conjugate(np.fft.fft2(fi)) Bi = Bi + np.fft.fft2(fi) * np.conjugate(np.fft.fft2(fi)) return Ai, Bi # get the ground-truth gaussian reponse... def _get_gauss_response(self, img, gt): # get the shape of the image.. height, width = img.shape # get the mesh grid... xx, yy = np.meshgrid(np.arange(width), np.arange(height)) # get the center of the object... center_x = gt[0] + 0.5 * gt[2] center_y = gt[1] + 0.5 * gt[3] # cal the distance... dist = (np.square(xx - center_x) + np.square(yy - center_y)) / (2 * self.args.sigma) # get the response map... response = np.exp(-dist) # normalize... response = linear_mapping(response) return response # it will extract the image list def _get_img_lists(self, img_path): frame_list = [] for frame in os.listdir(img_path): if os.path.splitext(frame)[1] == '.jpg': frame_list.append(os.path.join(img_path, frame)) return frame_list # it will get the first ground truth of the video.. def _get_init_ground_truth(self, img_path): gt_path = os.path.join(img_path, 'groundtruth.txt') with open(gt_path, 'r') as f: # just read the first frame... line = f.readline() gt_pos = line.split(',') return [float(element) for element in gt_pos] â€”|â€” å‚è€ƒé“¾æ¥ï¼šhttp://simtalk.cn/2017/07/03/Object-Tracking/https://github.com/TianhongDai/mosse-object-tracking","tags":["Python"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"åˆå¹¶ä¸¤ä¸ªæ’åºçš„é“¾è¡¨","path":"/2019/09/14/åˆå¹¶ä¸¤ä¸ªæ’åºçš„é“¾è¡¨/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š587032æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é“¾è¡¨ é¢˜ç›®æè¿°è¾“å…¥ä¸¤ä¸ªå•è°ƒé€’å¢çš„é“¾è¡¨ï¼Œè¾“å‡ºä¸¤ä¸ªé“¾è¡¨åˆæˆåçš„é“¾è¡¨ï¼Œå½“ç„¶æˆ‘ä»¬éœ€è¦åˆæˆåçš„é“¾è¡¨æ»¡è¶³å•è°ƒä¸å‡è§„åˆ™ã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 | /* struct ListNode { int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) { } };*/ class Solution { public: ListNode* Merge(ListNode* pHead1, ListNode* pHead2) { if(!pHead1)return pHead2; if(!pHead2)return pHead1; if(pHead1-val=pHead2-val){ pHead1-next= Merge(pHead1-next,pHead2); return pHead1; }else{ pHead2-next= Merge(pHead1,pHead2-next); return pHead2; } } }; â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 | # -*- coding:utf-8 -*- # class ListNode: # def __init__(self, x): # self.val = x # self.next = None class Solution: # è¿”å›åˆå¹¶ååˆ—è¡¨ def Merge(self, pHead1, pHead2): # write code here if pHead1 is None: return pHead2 if pHead2 is None: return pHead1 if pHead1.val pHead2.val: pHead1.next = self.Merge(pHead1.next,pHead2) return pHead1 else: pHead2.next = self.Merge(pHead1,pHead2.next) return pHead2 â€”|â€” è¿è¡Œæ—¶é—´ï¼š3mså ç”¨å†…å­˜ï¼š492k","tags":["ç¼–ç¨‹"],"categories":["ç®—æ³•"]},{"title":"æ ‘çš„å­ç»“æ„","path":"/2019/09/14/æ ‘çš„å­ç»“æ„/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š829537æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é“¾è¡¨ é¢˜ç›®æè¿°è¾“å…¥ä¸€ä¸ªé“¾è¡¨ï¼Œè¾“å‡ºè¯¥é“¾è¡¨ä¸­å€’æ•°ç¬¬kä¸ªç»“ç‚¹ã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 | # -*- coding:utf-8 -*- # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: def HasSubtree(self, pRoot1, pRoot2): # write code here if pRoot1 is None or pRoot2 is None: return False result = False if pRoot1.val == pRoot2.val: result = self.isSubtree(pRoot1,pRoot2) if result == False: result = self.HasSubtree(pRoot1.left, pRoot2) | self.HasSubtree(pRoot1.right, pRoot2) return result def isSubtree(self,root1,root2): if root2 is None: return True if root1 is None: return False if root1.val == root2.val: return self.isSubtree(root1.left,root2.left) self.isSubtree(root1.right,root2.right) return False â€”|â€” è¿è¡Œæ—¶é—´ï¼š24 mså ç”¨å†…å­˜ï¼š5860K","tags":["ç¼–ç¨‹"],"categories":["ç®—æ³•"]},{"title":"åè½¬é“¾è¡¨","path":"/2019/09/12/20190912-åè½¬é“¾è¡¨/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š606485æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é“¾è¡¨ é¢˜ç›®æè¿°è¾“å…¥ä¸€ä¸ªé“¾è¡¨ï¼Œåè½¬é“¾è¡¨åï¼Œè¾“å‡ºæ–°é“¾è¡¨çš„è¡¨å¤´ã€‚ /*struct ListNode {\tint val;\tstruct ListNode *next;\tListNode(int x) : val(x), next(NULL) {\t}};*/class Solution {public: ListNode* ReverseList(ListNode* pHead) { ListNode *prev = NULL; ListNode *curr = pHead; ListNode *next = NULL; while(curr!=NULL) { next = curr-next; curr-next = prev; prev = curr; curr = next; } return prev; }}; è¿è¡Œæ—¶é—´ï¼š4mså ç”¨å†…å­˜ï¼š488","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"å››è¾¹å½¢æŒ‰ç…§é¡ºæ—¶é’ˆæ’åº","path":"/2019/09/12/20190912-å››è¾¹å½¢æŒ‰ç…§é¡ºæ—¶é’ˆæ’åº/","content":"åšå›¾åƒæ£€æµ‹çš„æ—¶å€™å¤„ç†æ•°æ®ç»å¸¸é‡åˆ°ç»™å‡ºçŸ©å½¢çš„å››ä¸ªåæ ‡ç‚¹ï¼Œè¦æ±‚æ‰¾å‡ºå·¦ä¸Šè§’åæ ‡å¹¶å¯¹ä¹±åºçš„åæ ‡æŒ‰é¡ºæ—¶é’ˆæˆ–è€…é€†æ—¶é’ˆè¿›è¡Œæ’åº from scipy.spatial import distance as distimport numpy as npimport mathdef cos_dist(a, b): if len(a) != len(b): return None part_up = 0.0 a_sq = 0.0 b_sq = 0.0 print(a, b) print(zip(a, b)) for a1, b1 in zip(a, b): part_up += a1*b1 a_sq += a1**2 b_sq += b1**2 part_down = math.sqrt(a_sq*b_sq) if part_down == 0.0: return None else: return part_up / part_down# this function is confined to rectangledef order_points(pts): # sort the points based on their x-coordinates xSorted = pts[np.argsort(pts[:, 0]), :] # grab the left-most and right-most points from the sorted # x-roodinate points leftMost = xSorted[:2, :] rightMost = xSorted[2:, :] # now, sort the left-most coordinates according to their # y-coordinates so we can grab the top-left and bottom-left # points, respectively leftMost = leftMost[np.argsort(leftMost[:, 1]), :] (tl, bl) = leftMost # now that we have the top-left coordinate, use it as an # anchor to calculate the Euclidean distance between the # top-left and right-most points; by the Pythagorean # theorem, the point with the largest distance will be # our bottom-right point D = dist.cdist(tl[np.newaxis], rightMost, \"euclidean\")[0] (br, tr) = rightMost[np.argsort(D)[::-1], :] # return the coordinates in top-left, top-right, # bottom-right, and bottom-left order return np.array([tl, tr, br, bl], dtype=\"float32\")def order_points_quadrangle(pts): # sort the points based on their x-coordinates xSorted = pts[np.argsort(pts[:, 0]), :] # grab the left-most and right-most points from the sorted # x-roodinate points leftMost = xSorted[:2, :] rightMost = xSorted[2:, :] # now, sort the left-most coordinates according to their # y-coordinates so we can grab the top-left and bottom-left # points, respectively leftMost = leftMost[np.argsort(leftMost[:, 1]), :] (tl, bl) = leftMost # now that we have the top-left and bottom-left coordinate, use it as an # base vector to calculate the angles between the other two vectors vector_0 = np.array(bl-tl) vector_1 = np.array(rightMost[0]-tl) vector_2 = np.array(rightMost[1]-tl) angle = [np.arccos(cos_dist(vector_0, vector_1)), np.arccos(cos_dist(vector_0, vector_2))] (br, tr) = rightMost[np.argsort(angle), :] # return the coordinates in top-left, top-right, # bottom-right, and bottom-left order return np.array([tl, tr, br, bl], dtype=\"float32\")testdata =[1074,439,1078,424,991,427,991,411]points = numpy.array(array).reshape(4,2)poit = order_points_quadrangle(points)poitarray([[ 991., 411.], [1078., 424.], [1074., 439.], [ 991., 427.]], dtype=float32) å‚è€ƒé“¾æ¥ï¼šhttp://www.bnee.net/article/821708.html","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"åè½¬é“¾è¡¨","path":"/2019/09/12/åè½¬é“¾è¡¨/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š606485æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é“¾è¡¨ é¢˜ç›®æè¿°è¾“å…¥ä¸€ä¸ªé“¾è¡¨ï¼Œåè½¬é“¾è¡¨åï¼Œè¾“å‡ºæ–°é“¾è¡¨çš„è¡¨å¤´ã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 | /* struct ListNode { int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) { } };*/ class Solution { public: ListNode* ReverseList(ListNode* pHead) { ListNode *prev = NULL; ListNode *curr = pHead; ListNode *next = NULL; while(curr!=NULL) { next = curr-next; curr-next = prev; prev = curr; curr = next; } return prev; } }; â€”|â€” è¿è¡Œæ—¶é—´ï¼š4mså ç”¨å†…å­˜ï¼š488","tags":["ç¼–ç¨‹"],"categories":["ç®—æ³•"]},{"title":"å››è¾¹å½¢æŒ‰ç…§é¡ºæ—¶é’ˆæ’åº","path":"/2019/09/12/å››è¾¹å½¢æŒ‰ç…§é¡ºæ—¶é’ˆæ’åº/","content":"åšå›¾åƒæ£€æµ‹çš„æ—¶å€™å¤„ç†æ•°æ®ç»å¸¸é‡åˆ°ç»™å‡ºçŸ©å½¢çš„å››ä¸ªåæ ‡ç‚¹ï¼Œè¦æ±‚æ‰¾å‡ºå·¦ä¸Šè§’åæ ‡å¹¶å¯¹ä¹±åºçš„åæ ‡æŒ‰é¡ºæ—¶é’ˆæˆ–è€…é€†æ—¶é’ˆè¿›è¡Œæ’åº 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 | from scipy.spatial import distance as dist import numpy as np import math def cos_dist(a, b): if len(a) != len(b): return None part_up = 0.0 a_sq = 0.0 b_sq = 0.0 print(a, b) print(zip(a, b)) for a1, b1 in zip(a, b): part_up += a1*b1 a_sq += a1**2 b_sq += b1**2 part_down = math.sqrt(a_sq*b_sq) if part_down == 0.0: return None else: return part_up / part_down # this function is confined to rectangle def order_points(pts): # sort the points based on their x-coordinates xSorted = pts[np.argsort(pts[:, 0]), :] # grab the left-most and right-most points from the sorted # x-roodinate points leftMost = xSorted[:2, :] rightMost = xSorted[2:, :] # now, sort the left-most coordinates according to their # y-coordinates so we can grab the top-left and bottom-left # points, respectively leftMost = leftMost[np.argsort(leftMost[:, 1]), :] (tl, bl) = leftMost # now that we have the top-left coordinate, use it as an # anchor to calculate the Euclidean distance between the # top-left and right-most points; by the Pythagorean # theorem, the point with the largest distance will be # our bottom-right point D = dist.cdist(tl[np.newaxis], rightMost, \"euclidean\")[0] (br, tr) = rightMost[np.argsort(D)[::-1], :] # return the coordinates in top-left, top-right, # bottom-right, and bottom-left order return np.array([tl, tr, br, bl], dtype=\"float32\") def order_points_quadrangle(pts): # sort the points based on their x-coordinates xSorted = pts[np.argsort(pts[:, 0]), :] # grab the left-most and right-most points from the sorted # x-roodinate points leftMost = xSorted[:2, :] rightMost = xSorted[2:, :] # now, sort the left-most coordinates according to their # y-coordinates so we can grab the top-left and bottom-left # points, respectively leftMost = leftMost[np.argsort(leftMost[:, 1]), :] (tl, bl) = leftMost # now that we have the top-left and bottom-left coordinate, use it as an # base vector to calculate the angles between the other two vectors vector_0 = np.array(bl-tl) vector_1 = np.array(rightMost[0]-tl) vector_2 = np.array(rightMost[1]-tl) angle = [np.arccos(cos_dist(vector_0, vector_1)), np.arccos(cos_dist(vector_0, vector_2))] (br, tr) = rightMost[np.argsort(angle), :] # return the coordinates in top-left, top-right, # bottom-right, and bottom-left order return np.array([tl, tr, br, bl], dtype=\"float32\") testdata =[1074,439,1078,424,991,427,991,411] points = numpy.array(array).reshape(4,2) poit = order_points_quadrangle(points) poit array([[ 991., 411.], [1078., 424.], [1074., 439.], [ 991., 427.]], dtype=float32) â€”|â€” å‚è€ƒé“¾æ¥ï¼šhttp://www.bnee.net/article/821708.html","tags":["Python"],"categories":["ç®—æ³•"]},{"title":"é“¾è¡¨ä¸­å€’æ•°ç¬¬kä¸ªç»“ç‚¹","path":"/2019/09/10/20190910-é“¾è¡¨ä¸­å€’æ•°ç¬¬kä¸ªç»“ç‚¹/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š829537æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é“¾è¡¨ é¢˜ç›®æè¿°è¾“å…¥ä¸€ä¸ªé“¾è¡¨ï¼Œè¾“å‡ºè¯¥é“¾è¡¨ä¸­å€’æ•°ç¬¬kä¸ªç»“ç‚¹ã€‚ /*struct ListNode {\tint val;\tstruct ListNode *next;\tListNode(int x) : val(x), next(NULL) {\t}};*/class Solution {public: ListNode* FindKthToTail(ListNode* pListHead, unsigned int k) { if (k == 0) return NULL;//å¦‚æœKä¸º0ï¼Œè¿”å›NULL queueListNode* que; ListNode *node = pListHead; while (node != NULL) { if (que.size() == k) { que.pop(); } que.push(node); node = node-next; } if (que.size() == k) return que.front(); else return NULL;//å¦‚æœkå¤§äºé“¾è¡¨çš„æœ€å¤§é•¿åº¦ï¼Œè¿”å›NULL }}; è¿è¡Œæ—¶é—´ï¼š3mså ç”¨å†…å­˜ï¼š472K","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"é“¾è¡¨ä¸­å€’æ•°ç¬¬kä¸ªç»“ç‚¹","path":"/2019/09/10/é“¾è¡¨ä¸­å€’æ•°ç¬¬kä¸ªç»“ç‚¹/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š829537æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é“¾è¡¨ é¢˜ç›®æè¿°è¾“å…¥ä¸€ä¸ªé“¾è¡¨ï¼Œè¾“å‡ºè¯¥é“¾è¡¨ä¸­å€’æ•°ç¬¬kä¸ªç»“ç‚¹ã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 | /* struct ListNode { int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) { } };*/ class Solution { public: ListNode* FindKthToTail(ListNode* pListHead, unsigned int k) { if (k == 0) return NULL;//å¦‚æœKä¸º0ï¼Œè¿”å›NULL queueListNode* que; ListNode *node = pListHead; while (node != NULL) { if (que.size() == k) { que.pop(); } que.push(node); node = node-next; } if (que.size() == k) return que.front(); else return NULL;//å¦‚æœkå¤§äºé“¾è¡¨çš„æœ€å¤§é•¿åº¦ï¼Œè¿”å›NULL } }; â€”|â€” è¿è¡Œæ—¶é—´ï¼š3mså ç”¨å†…å­˜ï¼š472K","tags":["ç¼–ç¨‹"],"categories":["ç®—æ³•"]},{"title":"ä»csvä¸­è¯»å–æ•°æ®","path":"/2019/09/08/20190908-ä»csvä¸­è¯»å–æ•°æ®/","content":"import csvimport numpy as npdef loadDataSet(filename): # è¯»å–æ•°æ® with open(filename) as f: dataMat = [] labelMat = [] f_csv = csv.reader(f) ## ç”¨csvè¯»å–ç›´æ¥æ˜¯ä¸ªlist headers = next(f_csv) for row in f_csv: dataMat.append([float(row[0]), float(row[1])]) labelMat.append(float(row[2])) f.close() return dataMat, labelMatdef loadData(filename): # è¯»å–æ•°æ® dataMat=[] labelMat=[] fr=open(filename) next(fr) # å¿½ç•¥ç¬¬ä¸€è¡Œ for line in fr.readlines(): lineArr=line.strip().split(',') dataMat.append([lineArr[0],lineArr[1]]) labelMat.append(lineArr[2]) return dataMat,labelMat # è¿”å›æ•°æ®ç‰¹å¾å’Œæ•°æ®ç±»åˆ«if __name__ == '__main__': dataMat,labelMat = loadData('test_data.csv') print(dataMat,labelMat)","tags":["Python"],"categories":["Python"]},{"title":"äºŒè¿›åˆ¶ä¸­1çš„ä¸ªæ•°","path":"/2019/09/08/20190908-äºŒè¿›åˆ¶ä¸­1çš„ä¸ªæ•°/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š489798æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š è¿›åˆ¶è½¬åŒ– è¡¥ç  åç  åŸç  é¢˜ç›®æè¿°è¾“å…¥ä¸€ä¸ªæ•´æ•°ï¼Œè¾“å‡ºè¯¥æ•°äºŒè¿›åˆ¶è¡¨ç¤ºä¸­1çš„ä¸ªæ•°ã€‚å…¶ä¸­è´Ÿæ•°ç”¨è¡¥ç è¡¨ç¤ºã€‚ class Solution {public: int NumberOf1(int n) { int count=0; while(n!=0) { count++; n = (n-1)n; } return count; }}; è¿è¡Œæ—¶é—´ï¼š3mså ç”¨å†…å­˜ï¼š356k","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"æ”¯æŒå‘é‡æœº","path":"/2019/09/08/20190908-æ”¯æŒå‘é‡æœº/","content":"SVMçš„æœ¬è´¨ï¼šå¯»æ‰¾æœ€å¤§çš„é—´éš”æ”¯æŒå‘é‡ï¼šè·ç¦»è¶…å¹³é¢æœ€è¿‘çš„é‚£äº›ç‚¹SMOç®—æ³•çš„åŸç†ï¼š æ¯æ¬¡å¾ªç¯ä¸­é€‰æ‹©ä¸¤ä¸ªalphaè¿›è¡Œä¼˜åŒ–å¤„ç†ã€‚ä¸€æ—¦æ‰¾åˆ°ä¸€å¯¹åˆé€‚çš„alphaï¼Œé‚£ä¹ˆå°±å¢å¤§å…¶ä¸­ä¸€ä¸ªåŒæ—¶å‡å°å¦ä¸€ä¸ªã€‚åˆé€‚ï¼šæ¡ä»¶ä¸€ï¼Œä¸¤ä¸ªalphaè¦åœ¨é—´éš”è¾¹ç•Œä¹‹å¤–ï¼›æ¡ä»¶äºŒï¼Œè¿™ä¸¤ä¸ªalphaè¿˜æ²¡æœ‰è¿›è¡Œè¿‡åŒºé—´åŒ–å¤„ç†æˆ–ä¸åœ¨è¾¹ç•Œä¸Š from numpy import *import csvdef loadDataSet(filename): # è¯»å–æ•°æ® with open(filename) as f: dataMat = [] labelMat = [] f_csv = csv.reader(f) ## ç”¨csvè¯»å–ç›´æ¥æ˜¯ä¸ªlist headers = next(f_csv) for row in f_csv: dataMat.append([float(row[0]), float(row[1])]) labelMat.append(float(row[2])) f.close() return dataMat, labelMatdef selectJrand(i, m): # åœ¨0-mä¸­éšæœºé€‰æ‹©ä¸€ä¸ªä¸æ˜¯içš„æ•´æ•° j = i while (j == i): j = int(random.uniform(0, m)) return jdef clipAlpha(aj, H, L): # ä¿è¯aåœ¨Lå’ŒHèŒƒå›´å†…ï¼ˆL = a = Hï¼‰ if aj H: aj = H if L aj: aj = L return ajdef kernelTrans(X, A, kTup): # æ ¸å‡½æ•°ï¼Œè¾“å…¥å‚æ•°,X:æ”¯æŒå‘é‡çš„ç‰¹å¾æ ‘ï¼›Aï¼šæŸä¸€è¡Œç‰¹å¾æ•°æ®ï¼›kTupï¼š('lin',k1)æ ¸å‡½æ•°çš„ç±»å‹å’Œå‚æ•° m, n = shape(X) K = mat(zeros((m, 1))) print(\"A shape : \",A.shape) print(\"X shape : \",X.shape) if kTup[0] == 'lin': # çº¿æ€§å‡½æ•° K = X * A.T elif kTup[0] == 'rbf': # å¾„å‘åŸºå‡½æ•°(radial bias function) for j in range(m): deltaRow = X[j, :] - A K[j] = deltaRow * deltaRow.T K = exp(K / (-1 * kTup[1] ** 2)) # è¿”å›ç”Ÿæˆçš„ç»“æœ else: raise NameError('Houston We Have a Problem -- That Kernel is not recognized') return K# å®šä¹‰ç±»ï¼Œæ–¹ä¾¿å­˜å‚¨æ•°æ®class optStruct: def __init__(self, dataMatIn, classLabels, C, toler, kTup): # å­˜å‚¨å„ç±»å‚æ•° self.X = dataMatIn # æ•°æ®ç‰¹å¾ self.labelMat = classLabels # æ•°æ®ç±»åˆ« self.C = C # è½¯é—´éš”å‚æ•°Cï¼Œå‚æ•°è¶Šå¤§ï¼Œéçº¿æ€§æ‹Ÿåˆèƒ½åŠ›è¶Šå¼º self.tol = toler # åœæ­¢é˜€å€¼ self.m = shape(dataMatIn)[0] # æ•°æ® bè¡Œæ•° self.alphas = mat(zeros((self.m, 1))) self.b = 0 # åˆå§‹è®¾ä¸º0 self.eCache = mat(zeros((self.m, 2))) # ç¼“å­˜ self.K = mat(zeros((self.m, self.m))) # æ ¸å‡½æ•°çš„è®¡ç®—ç»“æœ for i in range(self.m): self.K[:, i] = kernelTrans(self.X, self.X[i, :], kTup)def calcEk(oS, k): # è®¡ç®—Ekï¼ˆå‚è€ƒã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹p127å…¬å¼7.105ï¼‰ fXk = float(multiply(oS.alphas, oS.labelMat).T * oS.K[:, k] + oS.b) Ek = fXk - float(oS.labelMat[k]) return Ek# éšæœºé€‰å–ajï¼Œå¹¶è¿”å›å…¶Eå€¼def selectJ(i, oS, Ei): maxK = -1 maxDeltaE = 0 Ej = 0 oS.eCache[i] = [1, Ei] validEcacheList = nonzero(oS.eCache[:, 0].A)[0] # è¿”å›çŸ©é˜µä¸­çš„éé›¶ä½ç½®çš„è¡Œæ•° if (len(validEcacheList)) 1: for k in validEcacheList: if k == i: continue Ek = calcEk(oS, k) deltaE = abs(Ei - Ek) if (deltaE maxDeltaE): # è¿”å›æ­¥é•¿æœ€å¤§çš„aj maxK = k maxDeltaE = deltaE Ej = Ek return maxK, Ej else: j = selectJrand(i, oS.m) Ej = calcEk(oS, j) return j, Ejdef updateEk(oS, k): # æ›´æ–°osæ•°æ® Ek = calcEk(oS, k) oS.eCache[k] = [1, Ek]# é¦–å…ˆæ£€éªŒaiæ˜¯å¦æ»¡è¶³KKTæ¡ä»¶ï¼Œå¦‚æœä¸æ»¡è¶³ï¼Œéšæœºé€‰æ‹©ajè¿›è¡Œä¼˜åŒ–ï¼Œæ›´æ–°ai,aj,bå€¼def innerL(i, oS): # è¾“å…¥å‚æ•°iå’Œæ‰€æœ‰å‚æ•°æ•°æ® Ei = calcEk(oS, i) # è®¡ç®—Eå€¼ if ((oS.labelMat[i] * Ei -oS.tol) and (oS.alphas[i] oS.C)) or ( (oS.labelMat[i] * Ei oS.tol) and (oS.alphas[i] 0)): # æ£€éªŒè¿™è¡Œæ•°æ®æ˜¯å¦ç¬¦åˆKKTæ¡ä»¶ å‚è€ƒã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹p128å…¬å¼7.111-113 j, Ej = selectJ(i, oS, Ei) # éšæœºé€‰å–ajï¼Œå¹¶è¿”å›å…¶Eå€¼ alphaIold = oS.alphas[i].copy() alphaJold = oS.alphas[j].copy() if (oS.labelMat[i] != oS.labelMat[j]): # ä»¥ä¸‹ä»£ç çš„å…¬å¼å‚è€ƒã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹p126 L = max(0, oS.alphas[j] - oS.alphas[i]) H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i]) else: L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C) H = min(oS.C, oS.alphas[j] + oS.alphas[i]) if L == H: print(\"L==H\") return 0 eta = 2.0 * oS.K[i, j] - oS.K[i, i] - oS.K[j, j] # å‚è€ƒã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹p127å…¬å¼7.107 if eta = 0: print(\"eta=0\") return 0 oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej) / eta # å‚è€ƒã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹p127å…¬å¼7.106 oS.alphas[j] = clipAlpha(oS.alphas[j], H, L) # å‚è€ƒã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹p127å…¬å¼7.108 updateEk(oS, j) if (abs(oS.alphas[j] - alphaJold) oS.tol): # alphaå˜åŒ–å¤§å°é˜€å€¼ï¼ˆè‡ªå·±è®¾å®šï¼‰ print(\"j not moving enough\") return 0 oS.alphas[i] += oS.labelMat[j] * oS.labelMat[i] * (alphaJold - oS.alphas[j]) # å‚è€ƒã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹p127å…¬å¼7.109 updateEk(oS, i) # æ›´æ–°æ•°æ® # ä»¥ä¸‹æ±‚è§£bçš„è¿‡ç¨‹ï¼Œå‚è€ƒã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹p129å…¬å¼7.114-7.116 b1 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, i] - oS.labelMat[j] * ( oS.alphas[j] - alphaJold) * oS.K[i, j] b2 = oS.b - Ej - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, j] - oS.labelMat[j] * ( oS.alphas[j] - alphaJold) * oS.K[j, j] if (0 oS.alphas[i] oS.C): oS.b = b1 elif (0 oS.alphas[j] oS.C): oS.b = b2 else: oS.b = (b1 + b2) / 2.0 return 1 else: return 0# SMOå‡½æ•°ï¼Œç”¨äºå¿«é€Ÿæ±‚è§£å‡ºalphadef smoP(dataMatIn, classLabels, C, toler, maxIter, kTup=('lin', 0)): # è¾“å…¥å‚æ•°ï¼šæ•°æ®ç‰¹å¾ï¼Œæ•°æ®ç±»åˆ«ï¼Œå‚æ•°Cï¼Œé˜€å€¼tolerï¼Œæœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œæ ¸å‡½æ•°ï¼ˆé»˜è®¤çº¿æ€§æ ¸ï¼‰ oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler, kTup) # dataArr, labelArr, 200, 0.0001, 10000, ('rbf', 1.3)) # é€šè¿‡SMOç®—æ³•å¾—åˆ°bå’Œalpha iter = 0 entireSet = True alphaPairsChanged = 0 while (iter maxIter) and ((alphaPairsChanged 0) or (entireSet)): alphaPairsChanged = 0 if entireSet: for i in range(oS.m): # éå†æ‰€æœ‰æ•°æ® alphaPairsChanged += innerL(i, oS) print(\"fullSet, iter: %d i:%d, pairs changed %d\" % ( iter, i, alphaPairsChanged)) # æ˜¾ç¤ºç¬¬å¤šå°‘æ¬¡è¿­ä»£ï¼Œé‚£è¡Œç‰¹å¾æ•°æ®ä½¿alphaå‘ç”Ÿäº†æ”¹å˜ï¼Œè¿™æ¬¡æ”¹å˜äº†å¤šå°‘æ¬¡alpha iter += 1 else: nonBoundIs = nonzero((oS.alphas.A 0) * (oS.alphas.A C))[0] for i in nonBoundIs: # éå†éè¾¹ç•Œçš„æ•°æ® alphaPairsChanged += innerL(i, oS) print(\"non-bound, iter: %d i:%d, pairs changed %d\" % (iter, i, alphaPairsChanged)) iter += 1 if entireSet: entireSet = False elif (alphaPairsChanged == 0): entireSet = True print(\"iteration number: %d\" % iter) return oS.b, oS.alphasdef train(data_train, data_test): dataArr, labelArr = loadDataSet(data_train) # è¯»å–è®­ç»ƒæ•°æ® b, alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, ('rbf', 1.3)) # é€šè¿‡SMOç®—æ³•å¾—åˆ°bå’Œalpha datMat = mat(dataArr) labelMat = mat(labelArr).transpose() svInd = nonzero(alphas)[0] # é€‰å–ä¸ä¸º0æ•°æ®çš„è¡Œæ•°ï¼ˆä¹Ÿå°±æ˜¯æ”¯æŒå‘é‡ï¼‰ sVs = datMat[svInd] # æ”¯æŒå‘é‡çš„ç‰¹å¾æ•°æ® labelSV = labelMat[svInd] # æ”¯æŒå‘é‡çš„ç±»åˆ«ï¼ˆ1æˆ–-1ï¼‰ print(\"there are %d Support Vectors\" % shape(sVs)[0]) # æ‰“å°å‡ºå…±æœ‰å¤šå°‘çš„æ”¯æŒå‘é‡ m, n = shape(datMat) # è®­ç»ƒæ•°æ®çš„è¡Œåˆ—æ•° errorCount = 0 for i in range(m): kernelEval = kernelTrans(sVs, datMat[i, :], ('rbf', 1.3)) # å°†æ”¯æŒå‘é‡è½¬åŒ–ä¸ºæ ¸å‡½æ•° predict = kernelEval.T * multiply(labelSV, alphas[ svInd]) + b # è¿™ä¸€è¡Œçš„é¢„æµ‹ç»“æœï¼ˆä»£ç æ¥æºäºã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹p133é‡Œé¢æœ€åç”¨äºé¢„æµ‹çš„å…¬å¼ï¼‰æ³¨æ„æœ€åç¡®å®šçš„åˆ†ç¦»å¹³é¢åªæœ‰é‚£äº›æ”¯æŒå‘é‡å†³å®šã€‚ if sign(predict) != sign(labelArr[i]): # signå‡½æ•° -1 if x 0, 0 if x==0, 1 if x 0 errorCount += 1 print(\"the training error rate is: %f\" % (float(errorCount) / m)) # æ‰“å°å‡ºé”™è¯¯ç‡ dataArr_test, labelArr_test = loadDataSet(data_test) # è¯»å–æµ‹è¯•æ•°æ® errorCount_test = 0 datMat_test = mat(dataArr_test) labelMat = mat(labelArr_test).transpose() m, n = shape(datMat_test) for i in range(m): # åœ¨æµ‹è¯•æ•°æ®ä¸Šæ£€éªŒé”™è¯¯ç‡ kernelEval = kernelTrans(sVs, datMat_test[i, :], ('rbf', 1.3)) predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b if sign(predict) != sign(labelArr_test[i]): errorCount_test += 1 print(\"the test error rate is: %f\" % (float(errorCount_test) / m))# ä¸»ç¨‹åºdef main(): filename_traindata = './train_data.csv' filename_testdata = './test_data.csv' train(filename_traindata, filename_testdata)if __name__ == '__main__': main() feature1,feature2,label-0.214824,0.662756,-1.000000-0.061569,-0.091875,1.0000000.406933,0.648055,-1.0000000.223650,0.130142,1.0000000.231317,0.766906,-1.000000-0.748800,-0.531637,-1.000000-0.557789,0.375797,-1.0000000.207123,-0.019463,1.0000000.286462,0.719470,-1.0000000.195300,-0.179039,1.000000-0.152696,-0.153030,1.0000000.384471,0.653336,-1.000000-0.117280,-0.153217,1.000000-0.238076,0.000583,1.000000-0.413576,0.145681,1.0000000.490767,-0.680029,-1.0000000.199894,-0.199381,1.000000-0.356048,0.537960,-1.000000-0.392868,-0.125261,1.0000000.353588,-0.070617,1.0000000.020984,0.925720,-1.000000-0.475167,-0.346247,-1.0000000.074952,0.042783,1.0000000.394164,-0.058217,1.0000000.663418,0.436525,-1.0000000.402158,0.577744,-1.000000-0.449349,-0.038074,1.0000000.619080,-0.088188,-1.0000000.268066,-0.071621,1.000000-0.015165,0.359326,1.0000000.539368,-0.374972,-1.000000-0.319153,0.629673,-1.0000000.694424,0.641180,-1.0000000.079522,0.193198,1.0000000.253289,-0.285861,1.000000-0.035558,-0.010086,1.000000-0.403483,0.474466,-1.000000-0.034312,0.995685,-1.000000-0.590657,0.438051,-1.000000-0.098871,-0.023953,1.000000-0.250001,0.141621,1.000000-0.012998,0.525985,-1.0000000.153738,0.491531,-1.0000000.388215,-0.656567,-1.0000000.049008,0.013499,1.0000000.068286,0.392741,1.0000000.747800,-0.066630,-1.0000000.004621,-0.042932,1.000000-0.701600,0.190983,-1.0000000.055413,-0.024380,1.0000000.035398,-0.333682,1.0000000.211795,0.024689,1.000000-0.045677,0.172907,1.0000000.595222,0.209570,-1.0000000.229465,0.250409,1.000000-0.089293,0.068198,1.0000000.384300,-0.176570,1.0000000.834912,-0.110321,-1.000000-0.307768,0.503038,-1.000000-0.777063,-0.348066,-1.0000000.017390,0.152441,1.000000-0.293382,-0.139778,1.000000-0.203272,0.286855,1.0000000.957812,-0.152444,-1.0000000.004609,-0.070617,1.000000-0.755431,0.096711,-1.000000-0.526487,0.547282,-1.000000-0.246873,0.833713,-1.0000000.185639,-0.066162,1.0000000.851934,0.456603,-1.000000-0.827912,0.117122,-1.0000000.233512,-0.106274,1.0000000.583671,-0.709033,-1.000000-0.487023,0.625140,-1.000000-0.448939,0.176725,1.0000000.155907,-0.166371,1.0000000.334204,0.381237,-1.0000000.081536,-0.106212,1.0000000.227222,0.527437,-1.0000000.759290,0.330720,-1.0000000.204177,-0.023516,1.0000000.577939,0.403784,-1.000000-0.568534,0.442948,-1.000000-0.011520,0.021165,1.0000000.875720,0.422476,-1.0000000.297885,-0.632874,-1.000000-0.015821,0.031226,1.0000000.541359,-0.205969,-1.000000-0.689946,-0.508674,-1.000000-0.343049,0.841653,-1.0000000.523902,-0.436156,-1.0000000.249281,-0.711840,-1.0000000.193449,0.574598,-1.000000-0.257542,-0.753885,-1.000000-0.021605,0.158080,1.0000000.601559,-0.727041,-1.000000-0.791603,0.095651,-1.000000-0.908298,-0.053376,-1.0000000.122020,0.850966,-1.000000-0.725568,-0.292022,-1.000000 test datafeature1,feature2,label0.676771,-0.486687,-1.0000000.008473,0.186070,1.000000-0.727789,0.594062,-1.0000000.112367,0.287852,1.0000000.383633,-0.038068,1.000000-0.927138,-0.032633,-1.000000-0.842803,-0.423115,-1.000000-0.003677,-0.367338,1.0000000.443211,-0.698469,-1.000000-0.473835,0.005233,1.0000000.616741,0.590841,-1.0000000.557463,-0.373461,-1.000000-0.498535,-0.223231,-1.000000-0.246744,0.276413,1.000000-0.761980,-0.244188,-1.0000000.641594,-0.479861,-1.000000-0.659140,0.529830,-1.000000-0.054873,-0.238900,1.000000-0.089644,-0.244683,1.000000-0.431576,-0.481538,-1.000000-0.099535,0.728679,-1.000000-0.188428,0.156443,1.0000000.267051,0.318101,1.0000000.222114,-0.528887,-1.0000000.030369,0.113317,1.0000000.392321,0.026089,1.0000000.298871,-0.915427,-1.000000-0.034581,-0.133887,1.0000000.405956,0.206980,1.0000000.144902,-0.605762,-1.0000000.274362,-0.401338,1.0000000.397998,-0.780144,-1.0000000.037863,0.155137,1.000000-0.010363,-0.004170,1.0000000.506519,0.486619,-1.0000000.000082,-0.020625,1.0000000.057761,-0.155140,1.0000000.027748,-0.553763,-1.000000-0.413363,-0.746830,-1.0000000.081500,-0.014264,1.0000000.047137,-0.491271,1.000000-0.267459,0.024770,1.000000-0.148288,-0.532471,-1.000000-0.225559,-0.201622,1.0000000.772360,-0.518986,-1.000000-0.440670,0.688739,-1.0000000.329064,-0.095349,1.0000000.970170,-0.010671,-1.000000-0.689447,-0.318722,-1.000000-0.465493,-0.227468,-1.000000-0.049370,0.405711,1.000000-0.166117,0.274807,1.0000000.054483,0.012643,1.0000000.021389,0.076125,1.000000-0.104404,-0.914042,-1.0000000.294487,0.440886,-1.0000000.107915,-0.493703,-1.0000000.076311,0.438860,1.0000000.370593,-0.728737,-1.0000000.409890,0.306851,-1.0000000.285445,0.474399,-1.000000-0.870134,-0.161685,-1.000000-0.654144,-0.675129,-1.0000000.285278,-0.767310,-1.0000000.049548,-0.000907,1.0000000.030014,-0.093265,1.000000-0.128859,0.278865,1.0000000.307463,0.085667,1.0000000.023440,0.298638,1.0000000.053920,0.235344,1.0000000.059675,0.533339,-1.0000000.817125,0.016536,-1.000000-0.108771,0.477254,1.000000-0.118106,0.017284,1.0000000.288339,0.195457,1.0000000.567309,-0.200203,-1.000000-0.202446,0.409387,1.000000-0.330769,-0.240797,1.000000-0.422377,0.480683,-1.000000-0.295269,0.326017,1.0000000.261132,0.046478,1.000000-0.492244,-0.319998,-1.000000-0.384419,0.099170,1.0000000.101882,-0.781145,-1.0000000.234592,-0.383446,1.000000-0.020478,-0.901833,-1.0000000.328449,0.186633,1.000000-0.150059,-0.409158,1.000000-0.155876,-0.843413,-1.000000-0.098134,-0.136786,1.0000000.110575,-0.197205,1.0000000.219021,0.054347,1.0000000.030152,0.251682,1.0000000.033447,-0.122824,1.000000-0.686225,-0.020779,-1.000000-0.911211,-0.262011,-1.0000000.572557,0.377526,-1.000000-0.073647,-0.519163,-1.000000-0.281830,-0.797236,-1.000000-0.555263,0.126232,-1.000000 å‚è€ƒé“¾æ¥ï¼šhttps://blog.csdn.net/csqazwsxedc/article/details/71513197https://blog.csdn.net/zouxy09/article/details/17291543","tags":["Python"],"categories":["Python"]},{"title":"ä»csvä¸­è¯»å–æ•°æ®","path":"/2019/09/08/ä»csvä¸­è¯»å–æ•°æ®/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 | import csv import numpy as np def loadDataSet(filename): # è¯»å–æ•°æ® with open(filename) as f: dataMat = [] labelMat = [] f_csv = csv.reader(f) ## ç”¨csvè¯»å–ç›´æ¥æ˜¯ä¸ªlist headers = next(f_csv) for row in f_csv: dataMat.append([float(row[0]), float(row[1])]) labelMat.append(float(row[2])) f.close() return dataMat, labelMat def loadData(filename): # è¯»å–æ•°æ® dataMat=[] labelMat=[] fr=open(filename) next(fr) # å¿½ç•¥ç¬¬ä¸€è¡Œ for line in fr.readlines(): lineArr=line.strip().split(',') dataMat.append([lineArr[0],lineArr[1]]) labelMat.append(lineArr[2]) return dataMat,labelMat # è¿”å›æ•°æ®ç‰¹å¾å’Œæ•°æ®ç±»åˆ« if __name__ == '__main__': dataMat,labelMat = loadData('test_data.csv') print(dataMat,labelMat) â€”|â€”","tags":["Python"],"categories":["å…¶ä»–"]},{"title":"äºŒè¿›åˆ¶ä¸­1çš„ä¸ªæ•°","path":"/2019/09/08/äºŒè¿›åˆ¶ä¸­1çš„ä¸ªæ•°/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š489798æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š è¿›åˆ¶è½¬åŒ– è¡¥ç  åç  åŸç  é¢˜ç›®æè¿°è¾“å…¥ä¸€ä¸ªæ•´æ•°ï¼Œè¾“å‡ºè¯¥æ•°äºŒè¿›åˆ¶è¡¨ç¤ºä¸­1çš„ä¸ªæ•°ã€‚å…¶ä¸­è´Ÿæ•°ç”¨è¡¥ç è¡¨ç¤ºã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 | class Solution { public: int NumberOf1(int n) { int count=0; while(n!=0) { count++; n = (n-1)n; } return count; } }; â€”|â€” è¿è¡Œæ—¶é—´ï¼š3mså ç”¨å†…å­˜ï¼š356k","tags":["ç¼–ç¨‹"],"categories":["å…¶ä»–"]},{"title":"æ”¯æŒå‘é‡æœº","path":"/2019/09/08/æ”¯æŒå‘é‡æœº/","content":"SVMçš„æœ¬è´¨ï¼šå¯»æ‰¾æœ€å¤§çš„é—´éš”æ”¯æŒå‘é‡ï¼šè·ç¦»è¶…å¹³é¢æœ€è¿‘çš„é‚£äº›ç‚¹SMOç®—æ³•çš„åŸç†ï¼š æ¯æ¬¡å¾ªç¯ä¸­é€‰æ‹©ä¸¤ä¸ªalphaè¿›è¡Œä¼˜åŒ–å¤„ç†ã€‚ä¸€æ—¦æ‰¾åˆ°ä¸€å¯¹åˆé€‚çš„alphaï¼Œé‚£ä¹ˆå°±å¢å¤§å…¶ä¸­ä¸€ä¸ªåŒæ—¶å‡å°å¦ä¸€ä¸ªã€‚åˆé€‚ï¼šæ¡ä»¶ä¸€ï¼Œä¸¤ä¸ªalphaè¦åœ¨é—´éš”è¾¹ç•Œä¹‹å¤–ï¼›æ¡ä»¶äºŒï¼Œè¿™ä¸¤ä¸ªalphaè¿˜æ²¡æœ‰è¿›è¡Œè¿‡åŒºé—´åŒ–å¤„ç†æˆ–ä¸åœ¨è¾¹ç•Œä¸Š 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 | from numpy import * import csv def loadDataSet(filename): # è¯»å–æ•°æ® with open(filename) as f: dataMat = [] labelMat = [] f_csv = csv.reader(f) ## ç”¨csvè¯»å–ç›´æ¥æ˜¯ä¸ªlist headers = next(f_csv) for row in f_csv: dataMat.append([float(row[0]), float(row[1])]) labelMat.append(float(row[2])) f.close() return dataMat, labelMat def selectJrand(i, m): # åœ¨0-mä¸­éšæœºé€‰æ‹©ä¸€ä¸ªä¸æ˜¯içš„æ•´æ•° j = i while (j == i): j = int(random.uniform(0, m)) return j def clipAlpha(aj, H, L): # ä¿è¯aåœ¨Lå’ŒHèŒƒå›´å†…ï¼ˆL = a = Hï¼‰ if aj H: aj = H if L aj: aj = L return aj def kernelTrans(X, A, kTup): # æ ¸å‡½æ•°ï¼Œè¾“å…¥å‚æ•°,X:æ”¯æŒå‘é‡çš„ç‰¹å¾æ ‘ï¼›Aï¼šæŸä¸€è¡Œç‰¹å¾æ•°æ®ï¼›kTupï¼š('lin',k1)æ ¸å‡½æ•°çš„ç±»å‹å’Œå‚æ•° m, n = shape(X) K = mat(zeros((m, 1))) print(\"A shape : \",A.shape) print(\"X shape : \",X.shape) if kTup[0] == 'lin': # çº¿æ€§å‡½æ•° K = X * A.T elif kTup[0] == 'rbf': # å¾„å‘åŸºå‡½æ•°(radial bias function) for j in range(m): deltaRow = X[j, :] - A K[j] = deltaRow * deltaRow.T K = exp(K / (-1 * kTup[1] ** 2)) # è¿”å›ç”Ÿæˆçš„ç»“æœ else: raise NameError('Houston We Have a Problem -- That Kernel is not recognized') return K # å®šä¹‰ç±»ï¼Œæ–¹ä¾¿å­˜å‚¨æ•°æ® class optStruct: def __init__(self, dataMatIn, classLabels, C, toler, kTup): # å­˜å‚¨å„ç±»å‚æ•° self.X = dataMatIn # æ•°æ®ç‰¹å¾ self.labelMat = classLabels # æ•°æ®ç±»åˆ« self.C = C # è½¯é—´éš”å‚æ•°Cï¼Œå‚æ•°è¶Šå¤§ï¼Œéçº¿æ€§æ‹Ÿåˆèƒ½åŠ›è¶Šå¼º self.tol = toler # åœæ­¢é˜€å€¼ self.m = shape(dataMatIn)[0] # æ•°æ® bè¡Œæ•° self.alphas = mat(zeros((self.m, 1))) self.b = 0 # åˆå§‹è®¾ä¸º0 self.eCache = mat(zeros((self.m, 2))) # ç¼“å­˜ self.K = mat(zeros((self.m, self.m))) # æ ¸å‡½æ•°çš„è®¡ç®—ç»“æœ for i in range(self.m): self.K[:, i] = kernelTrans(self.X, self.X[i, :], kTup) def calcEk(oS, k): # è®¡ç®—Ekï¼ˆå‚è€ƒã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹p127å…¬å¼7.105ï¼‰ fXk = float(multiply(oS.alphas, oS.labelMat).T * oS.K[:, k] + oS.b) Ek = fXk - float(oS.labelMat[k]) return Ek # éšæœºé€‰å–ajï¼Œå¹¶è¿”å›å…¶Eå€¼ def selectJ(i, oS, Ei): maxK = -1 maxDeltaE = 0 Ej = 0 oS.eCache[i] = [1, Ei] validEcacheList = nonzero(oS.eCache[:, 0].A)[0] # è¿”å›çŸ©é˜µä¸­çš„éé›¶ä½ç½®çš„è¡Œæ•° if (len(validEcacheList)) 1: for k in validEcacheList: if k == i: continue Ek = calcEk(oS, k) deltaE = abs(Ei - Ek) if (deltaE maxDeltaE): # è¿”å›æ­¥é•¿æœ€å¤§çš„aj maxK = k maxDeltaE = deltaE Ej = Ek return maxK, Ej else: j = selectJrand(i, oS.m) Ej = calcEk(oS, j) return j, Ej def updateEk(oS, k): # æ›´æ–°osæ•°æ® Ek = calcEk(oS, k) oS.eCache[k] = [1, Ek] # é¦–å…ˆæ£€éªŒaiæ˜¯å¦æ»¡è¶³KKTæ¡ä»¶ï¼Œå¦‚æœä¸æ»¡è¶³ï¼Œéšæœºé€‰æ‹©ajè¿›è¡Œä¼˜åŒ–ï¼Œæ›´æ–°ai,aj,bå€¼ def innerL(i, oS): # è¾“å…¥å‚æ•°iå’Œæ‰€æœ‰å‚æ•°æ•°æ® Ei = calcEk(oS, i) # è®¡ç®—Eå€¼ if ((oS.labelMat[i] * Ei -oS.tol) and (oS.alphas[i] oS.C)) or ( (oS.labelMat[i] * Ei oS.tol) and (oS.alphas[i] 0)): # æ£€éªŒè¿™è¡Œæ•°æ®æ˜¯å¦ç¬¦åˆKKTæ¡ä»¶ å‚è€ƒã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹p128å…¬å¼7.111-113 j, Ej = selectJ(i, oS, Ei) # éšæœºé€‰å–ajï¼Œå¹¶è¿”å›å…¶Eå€¼ alphaIold = oS.alphas[i].copy() alphaJold = oS.alphas[j].copy() if (oS.labelMat[i] != oS.labelMat[j]): # ä»¥ä¸‹ä»£ç çš„å…¬å¼å‚è€ƒã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹p126 L = max(0, oS.alphas[j] - oS.alphas[i]) H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i]) else: L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C) H = min(oS.C, oS.alphas[j] + oS.alphas[i]) if L == H: print(\"L==H\") return 0 eta = 2.0 * oS.K[i, j] - oS.K[i, i] - oS.K[j, j] # å‚è€ƒã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹p127å…¬å¼7.107 if eta = 0: print(\"eta=0\") return 0 oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej) / eta # å‚è€ƒã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹p127å…¬å¼7.106 oS.alphas[j] = clipAlpha(oS.alphas[j], H, L) # å‚è€ƒã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹p127å…¬å¼7.108 updateEk(oS, j) if (abs(oS.alphas[j] - alphaJold) oS.tol): # alphaå˜åŒ–å¤§å°é˜€å€¼ï¼ˆè‡ªå·±è®¾å®šï¼‰ print(\"j not moving enough\") return 0 oS.alphas[i] += oS.labelMat[j] * oS.labelMat[i] * (alphaJold - oS.alphas[j]) # å‚è€ƒã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹p127å…¬å¼7.109 updateEk(oS, i) # æ›´æ–°æ•°æ® # ä»¥ä¸‹æ±‚è§£bçš„è¿‡ç¨‹ï¼Œå‚è€ƒã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹p129å…¬å¼7.114-7.116 b1 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, i] - oS.labelMat[j] * ( oS.alphas[j] - alphaJold) * oS.K[i, j] b2 = oS.b - Ej - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, j] - oS.labelMat[j] * ( oS.alphas[j] - alphaJold) * oS.K[j, j] if (0 oS.alphas[i] oS.C): oS.b = b1 elif (0 oS.alphas[j] oS.C): oS.b = b2 else: oS.b = (b1 + b2) / 2.0 return 1 else: return 0 # SMOå‡½æ•°ï¼Œç”¨äºå¿«é€Ÿæ±‚è§£å‡ºalpha def smoP(dataMatIn, classLabels, C, toler, maxIter, kTup=('lin', 0)): # è¾“å…¥å‚æ•°ï¼šæ•°æ®ç‰¹å¾ï¼Œæ•°æ®ç±»åˆ«ï¼Œå‚æ•°Cï¼Œé˜€å€¼tolerï¼Œæœ€å¤§è¿­ä»£æ¬¡æ•°ï¼Œæ ¸å‡½æ•°ï¼ˆé»˜è®¤çº¿æ€§æ ¸ï¼‰ oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler, kTup) # dataArr, labelArr, 200, 0.0001, 10000, ('rbf', 1.3)) # é€šè¿‡SMOç®—æ³•å¾—åˆ°bå’Œalpha iter = 0 entireSet = True alphaPairsChanged = 0 while (iter maxIter) and ((alphaPairsChanged 0) or (entireSet)): alphaPairsChanged = 0 if entireSet: for i in range(oS.m): # éå†æ‰€æœ‰æ•°æ® alphaPairsChanged += innerL(i, oS) print(\"fullSet, iter: %d i:%d, pairs changed %d\" % ( iter, i, alphaPairsChanged)) # æ˜¾ç¤ºç¬¬å¤šå°‘æ¬¡è¿­ä»£ï¼Œé‚£è¡Œç‰¹å¾æ•°æ®ä½¿alphaå‘ç”Ÿäº†æ”¹å˜ï¼Œè¿™æ¬¡æ”¹å˜äº†å¤šå°‘æ¬¡alpha iter += 1 else: nonBoundIs = nonzero((oS.alphas.A 0) * (oS.alphas.A C))[0] for i in nonBoundIs: # éå†éè¾¹ç•Œçš„æ•°æ® alphaPairsChanged += innerL(i, oS) print(\"non-bound, iter: %d i:%d, pairs changed %d\" % (iter, i, alphaPairsChanged)) iter += 1 if entireSet: entireSet = False elif (alphaPairsChanged == 0): entireSet = True print(\"iteration number: %d\" % iter) return oS.b, oS.alphas def train(data_train, data_test): dataArr, labelArr = loadDataSet(data_train) # è¯»å–è®­ç»ƒæ•°æ® b, alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, ('rbf', 1.3)) # é€šè¿‡SMOç®—æ³•å¾—åˆ°bå’Œalpha datMat = mat(dataArr) labelMat = mat(labelArr).transpose() svInd = nonzero(alphas)[0] # é€‰å–ä¸ä¸º0æ•°æ®çš„è¡Œæ•°ï¼ˆä¹Ÿå°±æ˜¯æ”¯æŒå‘é‡ï¼‰ sVs = datMat[svInd] # æ”¯æŒå‘é‡çš„ç‰¹å¾æ•°æ® labelSV = labelMat[svInd] # æ”¯æŒå‘é‡çš„ç±»åˆ«ï¼ˆ1æˆ–-1ï¼‰ print(\"there are %d Support Vectors\" % shape(sVs)[0]) # æ‰“å°å‡ºå…±æœ‰å¤šå°‘çš„æ”¯æŒå‘é‡ m, n = shape(datMat) # è®­ç»ƒæ•°æ®çš„è¡Œåˆ—æ•° errorCount = 0 for i in range(m): kernelEval = kernelTrans(sVs, datMat[i, :], ('rbf', 1.3)) # å°†æ”¯æŒå‘é‡è½¬åŒ–ä¸ºæ ¸å‡½æ•° predict = kernelEval.T * multiply(labelSV, alphas[ svInd]) + b # è¿™ä¸€è¡Œçš„é¢„æµ‹ç»“æœï¼ˆä»£ç æ¥æºäºã€Šç»Ÿè®¡å­¦ä¹ æ–¹æ³•ã€‹p133é‡Œé¢æœ€åç”¨äºé¢„æµ‹çš„å…¬å¼ï¼‰æ³¨æ„æœ€åç¡®å®šçš„åˆ†ç¦»å¹³é¢åªæœ‰é‚£äº›æ”¯æŒå‘é‡å†³å®šã€‚ if sign(predict) != sign(labelArr[i]): # signå‡½æ•° -1 if x 0, 0 if x==0, 1 if x 0 errorCount += 1 print(\"the training error rate is: %f\" % (float(errorCount) / m)) # æ‰“å°å‡ºé”™è¯¯ç‡ dataArr_test, labelArr_test = loadDataSet(data_test) # è¯»å–æµ‹è¯•æ•°æ® errorCount_test = 0 datMat_test = mat(dataArr_test) labelMat = mat(labelArr_test).transpose() m, n = shape(datMat_test) for i in range(m): # åœ¨æµ‹è¯•æ•°æ®ä¸Šæ£€éªŒé”™è¯¯ç‡ kernelEval = kernelTrans(sVs, datMat_test[i, :], ('rbf', 1.3)) predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b if sign(predict) != sign(labelArr_test[i]): errorCount_test += 1 print(\"the test error rate is: %f\" % (float(errorCount_test) / m)) # ä¸»ç¨‹åº def main(): filename_traindata = './train_data.csv' filename_testdata = './test_data.csv' train(filename_traindata, filename_testdata) if __name__ == '__main__': main() â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 | feature1,feature2,label -0.214824,0.662756,-1.000000 -0.061569,-0.091875,1.000000 0.406933,0.648055,-1.000000 0.223650,0.130142,1.000000 0.231317,0.766906,-1.000000 -0.748800,-0.531637,-1.000000 -0.557789,0.375797,-1.000000 0.207123,-0.019463,1.000000 0.286462,0.719470,-1.000000 0.195300,-0.179039,1.000000 -0.152696,-0.153030,1.000000 0.384471,0.653336,-1.000000 -0.117280,-0.153217,1.000000 -0.238076,0.000583,1.000000 -0.413576,0.145681,1.000000 0.490767,-0.680029,-1.000000 0.199894,-0.199381,1.000000 -0.356048,0.537960,-1.000000 -0.392868,-0.125261,1.000000 0.353588,-0.070617,1.000000 0.020984,0.925720,-1.000000 -0.475167,-0.346247,-1.000000 0.074952,0.042783,1.000000 0.394164,-0.058217,1.000000 0.663418,0.436525,-1.000000 0.402158,0.577744,-1.000000 -0.449349,-0.038074,1.000000 0.619080,-0.088188,-1.000000 0.268066,-0.071621,1.000000 -0.015165,0.359326,1.000000 0.539368,-0.374972,-1.000000 -0.319153,0.629673,-1.000000 0.694424,0.641180,-1.000000 0.079522,0.193198,1.000000 0.253289,-0.285861,1.000000 -0.035558,-0.010086,1.000000 -0.403483,0.474466,-1.000000 -0.034312,0.995685,-1.000000 -0.590657,0.438051,-1.000000 -0.098871,-0.023953,1.000000 -0.250001,0.141621,1.000000 -0.012998,0.525985,-1.000000 0.153738,0.491531,-1.000000 0.388215,-0.656567,-1.000000 0.049008,0.013499,1.000000 0.068286,0.392741,1.000000 0.747800,-0.066630,-1.000000 0.004621,-0.042932,1.000000 -0.701600,0.190983,-1.000000 0.055413,-0.024380,1.000000 0.035398,-0.333682,1.000000 0.211795,0.024689,1.000000 -0.045677,0.172907,1.000000 0.595222,0.209570,-1.000000 0.229465,0.250409,1.000000 -0.089293,0.068198,1.000000 0.384300,-0.176570,1.000000 0.834912,-0.110321,-1.000000 -0.307768,0.503038,-1.000000 -0.777063,-0.348066,-1.000000 0.017390,0.152441,1.000000 -0.293382,-0.139778,1.000000 -0.203272,0.286855,1.000000 0.957812,-0.152444,-1.000000 0.004609,-0.070617,1.000000 -0.755431,0.096711,-1.000000 -0.526487,0.547282,-1.000000 -0.246873,0.833713,-1.000000 0.185639,-0.066162,1.000000 0.851934,0.456603,-1.000000 -0.827912,0.117122,-1.000000 0.233512,-0.106274,1.000000 0.583671,-0.709033,-1.000000 -0.487023,0.625140,-1.000000 -0.448939,0.176725,1.000000 0.155907,-0.166371,1.000000 0.334204,0.381237,-1.000000 0.081536,-0.106212,1.000000 0.227222,0.527437,-1.000000 0.759290,0.330720,-1.000000 0.204177,-0.023516,1.000000 0.577939,0.403784,-1.000000 -0.568534,0.442948,-1.000000 -0.011520,0.021165,1.000000 0.875720,0.422476,-1.000000 0.297885,-0.632874,-1.000000 -0.015821,0.031226,1.000000 0.541359,-0.205969,-1.000000 -0.689946,-0.508674,-1.000000 -0.343049,0.841653,-1.000000 0.523902,-0.436156,-1.000000 0.249281,-0.711840,-1.000000 0.193449,0.574598,-1.000000 -0.257542,-0.753885,-1.000000 -0.021605,0.158080,1.000000 0.601559,-0.727041,-1.000000 -0.791603,0.095651,-1.000000 -0.908298,-0.053376,-1.000000 0.122020,0.850966,-1.000000 -0.725568,-0.292022,-1.000000 test data feature1,feature2,label 0.676771,-0.486687,-1.000000 0.008473,0.186070,1.000000 -0.727789,0.594062,-1.000000 0.112367,0.287852,1.000000 0.383633,-0.038068,1.000000 -0.927138,-0.032633,-1.000000 -0.842803,-0.423115,-1.000000 -0.003677,-0.367338,1.000000 0.443211,-0.698469,-1.000000 -0.473835,0.005233,1.000000 0.616741,0.590841,-1.000000 0.557463,-0.373461,-1.000000 -0.498535,-0.223231,-1.000000 -0.246744,0.276413,1.000000 -0.761980,-0.244188,-1.000000 0.641594,-0.479861,-1.000000 -0.659140,0.529830,-1.000000 -0.054873,-0.238900,1.000000 -0.089644,-0.244683,1.000000 -0.431576,-0.481538,-1.000000 -0.099535,0.728679,-1.000000 -0.188428,0.156443,1.000000 0.267051,0.318101,1.000000 0.222114,-0.528887,-1.000000 0.030369,0.113317,1.000000 0.392321,0.026089,1.000000 0.298871,-0.915427,-1.000000 -0.034581,-0.133887,1.000000 0.405956,0.206980,1.000000 0.144902,-0.605762,-1.000000 0.274362,-0.401338,1.000000 0.397998,-0.780144,-1.000000 0.037863,0.155137,1.000000 -0.010363,-0.004170,1.000000 0.506519,0.486619,-1.000000 0.000082,-0.020625,1.000000 0.057761,-0.155140,1.000000 0.027748,-0.553763,-1.000000 -0.413363,-0.746830,-1.000000 0.081500,-0.014264,1.000000 0.047137,-0.491271,1.000000 -0.267459,0.024770,1.000000 -0.148288,-0.532471,-1.000000 -0.225559,-0.201622,1.000000 0.772360,-0.518986,-1.000000 -0.440670,0.688739,-1.000000 0.329064,-0.095349,1.000000 0.970170,-0.010671,-1.000000 -0.689447,-0.318722,-1.000000 -0.465493,-0.227468,-1.000000 -0.049370,0.405711,1.000000 -0.166117,0.274807,1.000000 0.054483,0.012643,1.000000 0.021389,0.076125,1.000000 -0.104404,-0.914042,-1.000000 0.294487,0.440886,-1.000000 0.107915,-0.493703,-1.000000 0.076311,0.438860,1.000000 0.370593,-0.728737,-1.000000 0.409890,0.306851,-1.000000 0.285445,0.474399,-1.000000 -0.870134,-0.161685,-1.000000 -0.654144,-0.675129,-1.000000 0.285278,-0.767310,-1.000000 0.049548,-0.000907,1.000000 0.030014,-0.093265,1.000000 -0.128859,0.278865,1.000000 0.307463,0.085667,1.000000 0.023440,0.298638,1.000000 0.053920,0.235344,1.000000 0.059675,0.533339,-1.000000 0.817125,0.016536,-1.000000 -0.108771,0.477254,1.000000 -0.118106,0.017284,1.000000 0.288339,0.195457,1.000000 0.567309,-0.200203,-1.000000 -0.202446,0.409387,1.000000 -0.330769,-0.240797,1.000000 -0.422377,0.480683,-1.000000 -0.295269,0.326017,1.000000 0.261132,0.046478,1.000000 -0.492244,-0.319998,-1.000000 -0.384419,0.099170,1.000000 0.101882,-0.781145,-1.000000 0.234592,-0.383446,1.000000 -0.020478,-0.901833,-1.000000 0.328449,0.186633,1.000000 -0.150059,-0.409158,1.000000 -0.155876,-0.843413,-1.000000 -0.098134,-0.136786,1.000000 0.110575,-0.197205,1.000000 0.219021,0.054347,1.000000 0.030152,0.251682,1.000000 0.033447,-0.122824,1.000000 -0.686225,-0.020779,-1.000000 -0.911211,-0.262011,-1.000000 0.572557,0.377526,-1.000000 -0.073647,-0.519163,-1.000000 -0.281830,-0.797236,-1.000000 -0.555263,0.126232,-1.000000 â€”|â€” å‚è€ƒé“¾æ¥ï¼šhttps://blog.csdn.net/csqazwsxedc/article/details/71513197https://blog.csdn.net/zouxy09/article/details/17291543","tags":["Python"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"PCAé™ç»´","path":"/2019/09/07/20190907-PCAé™ç»´/","content":"PCAï¼ˆprincipal Component Analysisï¼‰ï¼Œä¸»æˆåˆ†åˆ†ææ³•ã€‚é¡¾åæ€ä¹‰ï¼Œå°±æ˜¯æå–å‡ºæ•°æ®ä¸­ä¸»è¦çš„æˆåˆ†ï¼Œæ˜¯ä¸€ç§æ•°æ®å‹ç¼©æ–¹æ³•ï¼Œå¸¸ç”¨äºå»é™¤å™ªå£°ã€æ•°æ®é¢„å¤„ç†ï¼Œä¹Ÿæ˜¯æœºå™¨å­¦ä¹ ä¸­å¸¸è§çš„é™ç»´æ–¹æ³•ã€‚ æ­¥éª¤ å¯¹æ‰€æœ‰æ ·æœ¬è¿›è¡Œä¸­å¿ƒåŒ–å¤„ç†å³å°†æ¯ä¸ªå…ƒç´ å‡å»å®ƒçš„å¹³å‡å€¼ï¼Œè¿™æ ·å¯ä»¥å¢åŠ åŸºå‘é‡çš„æ­£äº¤æ€§ã€‚ è®¡ç®—åæ–¹å·®çŸ©é˜µåŠç‰¹å¾å€¼ã€ç‰¹å¾å‘é‡ å¯¹ç‰¹å¾å€¼è¿›è¡Œæ’åº # coding:utf-8from matplotlib import pylab as pltimport numpy as npdef read_file(filename):f = open(filename, 'r')d = f.readlines()f.close()return dclass PCA(object):\"\"\"ä½¿ç”¨PCAå¯¹é«˜ç»´æ•°æ®è¿›è¡Œé™ç»´å¤„ç†\"\"\"def __init__(self):data = read_file('data.txt')self.n = len(data) # æ•°æ®çš„ä¸ªæ•°self.dim = 4 # åŸå§‹æ•°æ®çš„ç»´åº¦self.x = np.zeros((self.n, self.dim), dtype='float64')for i in range(self.n):data_ = data[i].split(',')self.x[i][0] = data_[0]self.x[i][1] = data_[1]self.x[i][2] = data_[2]self.x[i][3] = data_[3]self.k = 2 # é™åˆ°äºŒç»´self.mean_x = np.zeros((self.n, self.dim), dtype='float64') # åŸå§‹æ•°æ®å‡å»å‡å€¼ä»¥åçš„xself.mean = np.zeros((self.dim, 1), dtype='float64') # xçš„å‡å€¼self.cov = np.zeros((self.dim, self.dim), dtype='float64') # åæ–¹å·®çŸ©é˜µself.pre_x = np.zeros((self.n, self.dim), dtype='float64') # é¢„å¤„ç†ä¹‹åçš„æ•°æ®self.eig_val = np.zeros((1, self.dim), dtype='float64') # ç‰¹å¾å€¼self.eig_vec = np.zeros((self.dim, self.dim), dtype='float64') # ç‰¹å¾å‘é‡self.final_x = np.zeros((self.n, self.k), dtype='float64') # æŠ•å½±åçš„æ•°æ®self.pretreatment()self.pca()def pretreatment(self):\"\"\"é¢„å¤„ç†\"\"\"# æ±‚å‡å€¼for i in range(self.dim):self.mean[i] = np.mean(self.x[:, i])for i in range(self.n):self.mean_x[i] = self.x[i] - self.mean.T# æ±‚åæ–¹å·®# self.cov = np.cov(self.mean_x, rowvar=0)# mean_xå·²ç»æ˜¯xå‡å»å‡å€¼äº†ï¼Œæ‰€ä»¥ç›´æ¥ç›¸ä¹˜å°±æ˜¯æ–¹å·®self.cov = self.mean_x.T.dot(self.mean_x) / self.nfor i in range(self.dim):self.pre_x[:, i] = self.mean_x[:, i] / np.sqrt(self.cov[i][i]) # xçš„æ¯ä¸ªç»´åº¦éƒ½å¤„ç†ä¸€æ¬¡def pca(self):\"\"\"pcaçš„å®ç°\"\"\"# éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨è¿™é‡Œéœ€è¦å¯¹é¢„å¤„ç†ä¹‹åçš„æ•°æ®é‡æ–°è®¡ç®—åæ–¹å·®# è®¡ç®—å‡å€¼for i in range(self.dim):self.mean[i] = np.mean(self.pre_x[:, i])# è®¡ç®—åæ–¹å·®for i in range(self.n):self.mean_x[i] = self.pre_x[i] - self.mean.Tself.cov = (self.mean_x.T.dot(self.mean_x)) / self.n# æ±‚ç‰¹å¾å€¼self.eig_val, self.eig_vec = np.linalg.eig(np.mat(self.cov))# eig_vecçš„åˆ—å‘é‡æ˜¯ç‰¹å¾å‘é‡ï¼Œæ‰€ä»¥ç”¨eig_vå­˜å‚¨ç‰¹å¾å‘é‡ï¼Œä»¥ä¾¿åé¢æ’åºeig_v = np.zeros((self.dim, self.dim), dtype='float64')for i in range(self.dim):eig_v[i] = self.eig_vec[:, i].T# æ’åºeig_list = zip(self.eig_val, eig_v)sorted(eig_list,key=lambda g: g[0], reverse=True)# å¤„ç†æ’åºçš„ç»“æœfor i in range(len(list(eig_list))):self.eig_val[i] = eig_list[i][0]self.eig_vec[i] = eig_list[i][1]# æœ€å¤§çš„kä¸ªç‰¹å¾å‘é‡ï¼Œé™ç»´f_vec = self.eig_vec[0:self.k, :]self.final_x = self.pre_x * f_vec.T# æ˜¾ç¤ºé™ç»´åçš„æ•°æ®ç‚¹plt.plot(self.final_x[0:50, 0], self.final_x[0:50, 1], 'bo')plt.plot(self.final_x[50:100, 0], self.final_x[50:100, 1], 'go')plt.plot(self.final_x[100:150, 0], self.final_x[100:150, 1], 'ro')plt.show()if __name__ == '__main__':a = PCA() 5.1,3.5,1.4,0.2,Iris-setosa4.9,3.0,1.4,0.2,Iris-setosa4.7,3.2,1.3,0.2,Iris-setosa4.6,3.1,1.5,0.2,Iris-setosa5.0,3.6,1.4,0.2,Iris-setosa5.4,3.9,1.7,0.4,Iris-setosa4.6,3.4,1.4,0.3,Iris-setosa5.0,3.4,1.5,0.2,Iris-setosa4.4,2.9,1.4,0.2,Iris-setosa4.9,3.1,1.5,0.1,Iris-setosa5.4,3.7,1.5,0.2,Iris-setosa4.8,3.4,1.6,0.2,Iris-setosa4.8,3.0,1.4,0.1,Iris-setosa4.3,3.0,1.1,0.1,Iris-setosa5.8,4.0,1.2,0.2,Iris-setosa5.7,4.4,1.5,0.4,Iris-setosa5.4,3.9,1.3,0.4,Iris-setosa5.1,3.5,1.4,0.3,Iris-setosa5.7,3.8,1.7,0.3,Iris-setosa5.1,3.8,1.5,0.3,Iris-setosa5.4,3.4,1.7,0.2,Iris-setosa5.1,3.7,1.5,0.4,Iris-setosa4.6,3.6,1.0,0.2,Iris-setosa5.1,3.3,1.7,0.5,Iris-setosa4.8,3.4,1.9,0.2,Iris-setosa5.0,3.0,1.6,0.2,Iris-setosa5.0,3.4,1.6,0.4,Iris-setosa5.2,3.5,1.5,0.2,Iris-setosa5.2,3.4,1.4,0.2,Iris-setosa4.7,3.2,1.6,0.2,Iris-setosa4.8,3.1,1.6,0.2,Iris-setosa5.4,3.4,1.5,0.4,Iris-setosa5.2,4.1,1.5,0.1,Iris-setosa5.5,4.2,1.4,0.2,Iris-setosa4.9,3.1,1.5,0.1,Iris-setosa5.0,3.2,1.2,0.2,Iris-setosa5.5,3.5,1.3,0.2,Iris-setosa4.9,3.1,1.5,0.1,Iris-setosa4.4,3.0,1.3,0.2,Iris-setosa5.1,3.4,1.5,0.2,Iris-setosa5.0,3.5,1.3,0.3,Iris-setosa4.5,2.3,1.3,0.3,Iris-setosa4.4,3.2,1.3,0.2,Iris-setosa5.0,3.5,1.6,0.6,Iris-setosa5.1,3.8,1.9,0.4,Iris-setosa4.8,3.0,1.4,0.3,Iris-setosa5.1,3.8,1.6,0.2,Iris-setosa4.6,3.2,1.4,0.2,Iris-setosa5.3,3.7,1.5,0.2,Iris-setosa5.0,3.3,1.4,0.2,Iris-setosa7.0,3.2,4.7,1.4,Iris-versicolor6.4,3.2,4.5,1.5,Iris-versicolor6.9,3.1,4.9,1.5,Iris-versicolor5.5,2.3,4.0,1.3,Iris-versicolor6.5,2.8,4.6,1.5,Iris-versicolor5.7,2.8,4.5,1.3,Iris-versicolor6.3,3.3,4.7,1.6,Iris-versicolor4.9,2.4,3.3,1.0,Iris-versicolor6.6,2.9,4.6,1.3,Iris-versicolor5.2,2.7,3.9,1.4,Iris-versicolor5.0,2.0,3.5,1.0,Iris-versicolor5.9,3.0,4.2,1.5,Iris-versicolor6.0,2.2,4.0,1.0,Iris-versicolor6.1,2.9,4.7,1.4,Iris-versicolor5.6,2.9,3.6,1.3,Iris-versicolor6.7,3.1,4.4,1.4,Iris-versicolor5.6,3.0,4.5,1.5,Iris-versicolor5.8,2.7,4.1,1.0,Iris-versicolor6.2,2.2,4.5,1.5,Iris-versicolor5.6,2.5,3.9,1.1,Iris-versicolor5.9,3.2,4.8,1.8,Iris-versicolor6.1,2.8,4.0,1.3,Iris-versicolor6.3,2.5,4.9,1.5,Iris-versicolor6.1,2.8,4.7,1.2,Iris-versicolor6.4,2.9,4.3,1.3,Iris-versicolor6.6,3.0,4.4,1.4,Iris-versicolor6.8,2.8,4.8,1.4,Iris-versicolor6.7,3.0,5.0,1.7,Iris-versicolor6.0,2.9,4.5,1.5,Iris-versicolor5.7,2.6,3.5,1.0,Iris-versicolor5.5,2.4,3.8,1.1,Iris-versicolor5.5,2.4,3.7,1.0,Iris-versicolor5.8,2.7,3.9,1.2,Iris-versicolor6.0,2.7,5.1,1.6,Iris-versicolor5.4,3.0,4.5,1.5,Iris-versicolor6.0,3.4,4.5,1.6,Iris-versicolor6.7,3.1,4.7,1.5,Iris-versicolor6.3,2.3,4.4,1.3,Iris-versicolor5.6,3.0,4.1,1.3,Iris-versicolor5.5,2.5,4.0,1.3,Iris-versicolor5.5,2.6,4.4,1.2,Iris-versicolor6.1,3.0,4.6,1.4,Iris-versicolor5.8,2.6,4.0,1.2,Iris-versicolor5.0,2.3,3.3,1.0,Iris-versicolor5.6,2.7,4.2,1.3,Iris-versicolor5.7,3.0,4.2,1.2,Iris-versicolor5.7,2.9,4.2,1.3,Iris-versicolor6.2,2.9,4.3,1.3,Iris-versicolor5.1,2.5,3.0,1.1,Iris-versicolor5.7,2.8,4.1,1.3,Iris-versicolor6.3,3.3,6.0,2.5,Iris-virginica5.8,2.7,5.1,1.9,Iris-virginica7.1,3.0,5.9,2.1,Iris-virginica6.3,2.9,5.6,1.8,Iris-virginica6.5,3.0,5.8,2.2,Iris-virginica7.6,3.0,6.6,2.1,Iris-virginica4.9,2.5,4.5,1.7,Iris-virginica7.3,2.9,6.3,1.8,Iris-virginica6.7,2.5,5.8,1.8,Iris-virginica7.2,3.6,6.1,2.5,Iris-virginica6.5,3.2,5.1,2.0,Iris-virginica6.4,2.7,5.3,1.9,Iris-virginica6.8,3.0,5.5,2.1,Iris-virginica5.7,2.5,5.0,2.0,Iris-virginica5.8,2.8,5.1,2.4,Iris-virginica6.4,3.2,5.3,2.3,Iris-virginica6.5,3.0,5.5,1.8,Iris-virginica7.7,3.8,6.7,2.2,Iris-virginica7.7,2.6,6.9,2.3,Iris-virginica6.0,2.2,5.0,1.5,Iris-virginica6.9,3.2,5.7,2.3,Iris-virginica5.6,2.8,4.9,2.0,Iris-virginica7.7,2.8,6.7,2.0,Iris-virginica6.3,2.7,4.9,1.8,Iris-virginica6.7,3.3,5.7,2.1,Iris-virginica7.2,3.2,6.0,1.8,Iris-virginica6.2,2.8,4.8,1.8,Iris-virginica6.1,3.0,4.9,1.8,Iris-virginica6.4,2.8,5.6,2.1,Iris-virginica7.2,3.0,5.8,1.6,Iris-virginica7.4,2.8,6.1,1.9,Iris-virginica7.9,3.8,6.4,2.0,Iris-virginica6.4,2.8,5.6,2.2,Iris-virginica6.3,2.8,5.1,1.5,Iris-virginica6.1,2.6,5.6,1.4,Iris-virginica7.7,3.0,6.1,2.3,Iris-virginica6.3,3.4,5.6,2.4,Iris-virginica6.4,3.1,5.5,1.8,Iris-virginica6.0,3.0,4.8,1.8,Iris-virginica6.9,3.1,5.4,2.1,Iris-virginica6.7,3.1,5.6,2.4,Iris-virginica6.9,3.1,5.1,2.3,Iris-virginica5.8,2.7,5.1,1.9,Iris-virginica6.8,3.2,5.9,2.3,Iris-virginica6.7,3.3,5.7,2.5,Iris-virginica6.7,3.0,5.2,2.3,Iris-virginica6.3,2.5,5.0,1.9,Iris-virginica6.5,3.0,5.2,2.0,Iris-virginica6.2,3.4,5.4,2.3,Iris-virginica5.9,3.0,5.1,1.8,Iris-virginica å‚è€ƒé“¾æ¥ï¼šhttps://github.com/eva-n27/PCA","tags":["æœºå™¨å­¦ä¹ "],"categories":["Python"]},{"title":"xmlç”Ÿæˆ","path":"/2019/09/07/20190907-xmlç”Ÿæˆ/","content":"å…ˆåˆ›å»º root Element, ç„¶ååˆ›å»º SubElement, æœ€åå°† root ä¼ å…¥ ElementTree(element), åˆ›å»º tree, è°ƒç”¨ tree.write() æ–¹æ³•å†™å…¥æ–‡ä»¶,åˆ›å»º XML ç±»å‹çš„æ•°æ®æ–‡ä»¶ import xml.etree.ElementTree as ETdef subElement(root, tag, text): ele = ET.SubElement(root, tag) ele.text = text ele.tail = ' 'root = ET.Element(\"note\")to = root.makeelement(\"to\", {})to.text = \"peter\"to.tail = ' 'root.append(to)subElement(root, \"from\", \"marry\")subElement(root, \"heading\", \"Reminder\")subElement(root, \"body\", \"Don't forget the meeting!\")tree = ET.ElementTree(root)tree.write(\"note.xml\", encoding=\"utf-8\", xml_declaration=True) ?xml version='1.0' encoding='utf-8'?notetopeter/tofrommarry/fromheadingReminder/headingbodyDon't forget the meeting!/body/note","tags":["Python"],"categories":["Python"]},{"title":"kmeansç®—æ³•","path":"/2019/09/07/20190907-kmeansç®—æ³•/","content":"K-Meansçš„æ€æƒ³ååˆ†ç®€å•ï¼Œé¦–å…ˆéšæœºæŒ‡å®šç±»ä¸­å¿ƒï¼Œæ ¹æ®æ ·æœ¬ä¸ç±»ä¸­å¿ƒçš„è¿œè¿‘åˆ’åˆ†ç±»ç°‡ï¼Œæ¥ç€é‡æ–°è®¡ç®—ç±»ä¸­å¿ƒï¼Œè¿­ä»£ç›´è‡³æ”¶æ•›ã€‚ä½†æ˜¯å…¶ä¸­è¿­ä»£çš„è¿‡ç¨‹å¹¶ä¸æ˜¯ä¸»è§‚åœ°æƒ³è±¡å¾—å‡ºï¼Œäº‹å®ä¸Šï¼Œè‹¥å°†æ ·æœ¬çš„ç±»åˆ«çœ‹åšä¸ºâ€œéšå˜é‡â€ï¼ˆlatent variableï¼‰ï¼Œç±»ä¸­å¿ƒçœ‹ä½œæ ·æœ¬çš„åˆ†å¸ƒå‚æ•°ï¼Œè¿™ä¸€è¿‡ç¨‹æ­£æ˜¯é€šè¿‡EMç®—æ³•çš„ä¸¤æ­¥èµ°ç­–ç•¥è€Œè®¡ç®—å‡ºï¼Œå…¶æ ¹æœ¬çš„ç›®çš„æ˜¯ä¸ºäº†æœ€å°åŒ–å¹³æ–¹è¯¯å·®å‡½æ•°Eã€‚ kmeansç®—æ³•çš„æœ€å¤§å¼±ç‚¹ï¼šåªèƒ½å¤„ç†çƒå½¢çš„ç°‡ï¼ˆç†è®ºï¼‰kmeans è®¡ç®—æ­¥éª¤ 1.éšæœºé€‰å–Kä¸ªèšç±»ä¸­å¿ƒï¼Œè¿™é‡Œçš„kå€¼å¯ä»¥è‡ªå·±è®¾å®š 2.å…ˆè®¾ç½®ä¸€ä¸ªèšç±»æ ‡å¿—ï¼Œç”¨æ¥ä¿å­˜å½“å‰çš„ æ ·æœ¬ä¸ç¬¬å‡ ä¸ªèšç±»ä¸­å¿ƒæœ€è¿‘ 3.è®¡ç®—æ¯ä¸ªæ ·ä¾‹ä¸æ¯ä¸ªèšç±»ä¸­å¿ƒçš„è·ç¦»ï¼Œä¿å­˜æœ€å°è·ç¦»çš„kä»¥åŠè·ç¦» 4.æ›´æ–°èšç±»ä¸­å¿ƒï¼Œä¸ºå½“å‰ç±»åˆ«æ‰€æœ‰æ ·æœ¬çš„å‡å€¼å¤§å° from numpy import *import timeimport matplotlib.pyplot as plt# calculate Euclidean distancedef euclDistance(vector1, vector2):return sqrt(sum(power(vector2 - vector1, 2))) # æ±‚è¿™ä¸¤ä¸ªçŸ©é˜µçš„è·ç¦»ï¼Œ vector1, vector2 å‡ä¸ºçŸ©é˜µ# init centroids with random samples# åœ¨æ ·æœ¬é›†ä¸­éšæœºé€‰å–kä¸ªæ ·æœ¬ç‚¹ä½œä¸ºåˆå§‹è´¨å¿ƒdef initCentroids(dataSet, k):numSamples, dim = dataSet.shape # çŸ©é˜µçš„è¡Œæ•°ã€åˆ—æ•°centroids = zeros((k, dim)) # æ„Ÿè§‰è¦ä¸è¦ä½ éƒ½å¯ä»¥for i in range(k):index = int(random.uniform(0, numSamples)) # éšæœºäº§ç”Ÿä¸€ä¸ªæµ®ç‚¹æ•°ï¼Œç„¶åå°†å…¶è½¬åŒ–ä¸ºintå‹centroids[i, :] = dataSet[index, :]return centroids# k-means cluster# dataSetä¸ºä¸€ä¸ªçŸ©é˜µ# kä¸ºå°†dataSetçŸ©é˜µä¸­çš„æ ·æœ¬åˆ†æˆkä¸ªç±»def kmeans(dataSet, k):numSamples = dataSet.shape[0] # è¯»å–çŸ©é˜µdataSetçš„ç¬¬ä¸€ç»´åº¦çš„é•¿åº¦,å³è·å¾—æœ‰å¤šå°‘ä¸ªæ ·æœ¬æ•°æ®# first column stores which cluster this sample belongs to,# second column stores the error between this sample and its centroidclusterAssment = mat(zeros((numSamples, 2))) # å¾—åˆ°ä¸€ä¸ªN*2çš„é›¶çŸ©é˜µclusterChanged = True## step 1: init centroidscentroids = initCentroids(dataSet, k) # åœ¨æ ·æœ¬é›†ä¸­éšæœºé€‰å–kä¸ªæ ·æœ¬ç‚¹ä½œä¸ºåˆå§‹è´¨å¿ƒwhile clusterChanged:clusterChanged = False## for each samplefor i in range(numSamples): # rangeminDist = 100000.0minIndex = 0## for each centroid## step 2: find the centroid who is closest# è®¡ç®—æ¯ä¸ªæ ·æœ¬ç‚¹ä¸è´¨ç‚¹ä¹‹é—´çš„è·ç¦»ï¼Œå°†å…¶å½’å†…åˆ°è·ç¦»æœ€å°çš„é‚£ä¸€ç°‡for j in range(k):distance = euclDistance(centroids[j, :], dataSet[i, :])if distance minDist:minDist = distanceminIndex = j## step 3: update its cluster# kä¸ªç°‡é‡Œé¢ä¸ç¬¬iä¸ªæ ·æœ¬è·ç¦»æœ€å°çš„çš„æ ‡å·å’Œè·ç¦»ä¿å­˜åœ¨clusterAssmentä¸­# è‹¥æ‰€æœ‰çš„æ ·æœ¬ä¸åœ¨å˜åŒ–ï¼Œåˆ™é€€å‡ºwhileå¾ªç¯if clusterAssment[i, 0] != minIndex:clusterChanged = TrueclusterAssment[i, :] = minIndex, minDist ** 2 # ä¸¤ä¸ª**è¡¨ç¤ºçš„æ˜¯minDistçš„å¹³æ–¹## step 4: update centroidsfor j in range(k):# clusterAssment[:,0].A==jæ˜¯æ‰¾å‡ºçŸ©é˜µclusterAssmentä¸­ç¬¬ä¸€åˆ—å…ƒç´ ä¸­ç­‰äºjçš„è¡Œçš„ä¸‹æ ‡ï¼Œè¿”å›çš„æ˜¯ä¸€ä¸ªä»¥arrayçš„åˆ—è¡¨ï¼Œç¬¬ä¸€ä¸ªarrayä¸ºç­‰äºjçš„ä¸‹æ ‡pointsInCluster = dataSet[nonzero(clusterAssment[:, 0].A == j)[0]] # å°†dataSetçŸ©é˜µä¸­ç›¸å¯¹åº”çš„æ ·æœ¬æå–å‡ºæ¥centroids[j, :] = mean(pointsInCluster, axis=0) # è®¡ç®—æ ‡æ³¨ä¸ºjçš„æ‰€æœ‰æ ·æœ¬çš„å¹³å‡å€¼print('Congratulations, cluster complete!')return centroids, clusterAssment# show your cluster only available with 2-D data# centroidsä¸ºkä¸ªç±»åˆ«ï¼Œå…¶ä¸­ä¿å­˜ç€æ¯ä¸ªç±»åˆ«çš„è´¨å¿ƒ# clusterAssmentä¸ºæ ·æœ¬çš„æ ‡è®°ï¼Œç¬¬ä¸€åˆ—ä¸ºæ­¤æ ·æœ¬çš„ç±»åˆ«å·ï¼Œç¬¬äºŒåˆ—ä¸ºåˆ°æ­¤ç±»åˆ«è´¨å¿ƒçš„è·ç¦»def showCluster(dataSet, k, centroids, clusterAssment):numSamples, dim = dataSet.shapeif dim != 2:print(\"Sorry! I can not draw because the dimension of your data is not 2!\")return 1mark = ['or', 'ob', 'og', 'ok', '^r', '+r', 'sr', 'dr', 'r', 'pr']if k len(mark):print(\"Sorry! Your k is too large! please contact wojiushimogui\")return 1# draw all samplesfor i in range(numSamples):markIndex = int(clusterAssment[i, 0]) # ä¸ºæ ·æœ¬æŒ‡å®šé¢œè‰²plt.plot(dataSet[i, 0], dataSet[i, 1], mark[markIndex])mark = ['Dr', 'Db', 'Dg', 'Dk', '^b', '+b', 'sb', 'db', 'b', 'pb']# draw the centroidsfor i in range(k):plt.plot(centroids[i, 0], centroids[i, 1], mark[i], markersize=12)plt.show() from numpy import *import timeimport matplotlib.pyplot as pltimport kmeans## step 1: load dataprint(\"step 1: load data...\")dataSet = [] # åˆ—è¡¨ï¼Œç”¨æ¥è¡¨ç¤ºï¼Œåˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ ä¹Ÿæ˜¯ä¸€ä¸ªäºŒç»´çš„åˆ—è¡¨ï¼›è¿™ä¸ªäºŒç»´åˆ—è¡¨å°±æ˜¯ä¸€ä¸ªæ ·æœ¬ï¼Œæ ·æœ¬ä¸­åŒ…å«æœ‰æˆ‘ä»¬çš„å±æ€§å€¼å’Œç±»åˆ«å·ã€‚# ä¸æˆ‘ä»¬æ‰€ç†Ÿæ‚‰çš„çŸ©é˜µç±»ä¼¼ï¼Œæœ€ç»ˆæˆ‘ä»¬å°†è·å¾—N*2çš„çŸ©é˜µï¼Œæ¯è¡Œå…ƒç´ æ„æˆäº†æˆ‘ä»¬çš„è®­ç»ƒæ ·æœ¬çš„å±æ€§å€¼å’Œç±»åˆ«å·fileIn = open(\"./testSet.txt\") # æ˜¯æ­£æ–œæ for line in fileIn.readlines(): temp = [] lineArr = line.strip().split('\\t') # line.strip()æŠŠæœ«å°¾çš„' 'å»æ‰ temp.append(float(lineArr[0])) temp.append(float(lineArr[1])) dataSet.append(temp)# dataSet.append([float(lineArr[0]), float(lineArr[1])])fileIn.close()## step 2: clustering...print(\"step 2: clustering...\")dataSet = mat(dataSet) # mat()å‡½æ•°æ˜¯Numpyä¸­çš„åº“å‡½æ•°ï¼Œå°†æ•°ç»„è½¬åŒ–ä¸ºçŸ©é˜µk = 4centroids, clusterAssment = kmeans.kmeans(dataSet, k) # è°ƒç”¨KMeansæ–‡ä»¶ä¸­å®šä¹‰çš„kmeansæ–¹æ³•ã€‚## step 3: show the resultprint(\"step 3: show the result...\")kmeans.showCluster(dataSet, k, centroids, clusterAssment) 1.658985\t4.285136-3.453687\t3.4243214.838138\t1.151539-5.379713\t-3.3621040.972564\t2.924086-3.567919\t1.5316110.450614\t-3.302219-3.487105\t-1.7244322.668759\t1.594842-3.156485\t3.1911373.165506\t-3.999838-2.786837\t-3.0993544.208187\t2.984927-2.123337\t2.9433660.704199\t-0.479481-0.392370\t-3.9637042.831667\t1.574018-0.790153\t3.3431442.943496\t-3.357075-3.195883\t-2.2839262.336445\t2.875106-1.786345\t2.5542482.190101\t-1.906020-3.403367\t-2.7782881.778124\t3.880832-1.688346\t2.2302672.592976\t-2.054368-4.007257\t-3.2070662.257734\t3.387564-2.679011\t0.7851190.939512\t-4.023563-3.674424\t-2.2610842.046259\t2.735279-3.189470\t1.7802694.372646\t-0.822248-2.579316\t-3.4975761.889034\t5.190400-0.798747\t2.1855882.836520\t-2.658556-3.837877\t-3.2538152.096701\t3.886007-2.709034\t2.9238873.367037\t-3.184789-2.121479\t-4.2325862.329546\t3.179764-3.284816\t3.2730993.091414\t-3.815232-3.762093\t-2.4321913.542056\t2.778832-1.736822\t4.2410412.127073\t-2.983680-4.323818\t-3.9381163.792121\t5.135768-4.786473\t3.3585472.624081\t-3.260715-4.009299\t-2.9781152.493525\t1.963710-2.513661\t2.6421621.864375\t-3.176309-3.171184\t-3.5724522.894220\t2.489128-2.562539\t2.8844383.491078\t-3.947487-2.565729\t-2.0121143.332948\t3.983102-1.616805\t3.5731882.280615\t-2.559444-2.651229\t-3.1031982.321395\t3.154987-1.685703\t2.9396973.031012\t-3.620252-4.599622\t-2.1858294.196223\t1.126677-2.133863\t3.0936864.668892\t-2.562705-2.793241\t-2.1497062.884105\t3.043438-2.967647\t2.8486964.479332\t-1.764772-4.905566\t-2.911070 å‚è€ƒé“¾æ¥ï¼šhttps://github.com/wojiushimogui/kmeans","tags":["æœºå™¨å­¦ä¹ "],"categories":["ç®—æ³•"]},{"title":"xmlè§£æ","path":"/2019/09/07/20190907-xmlè§£æ/","content":"annotation\tfolderILSVRC2015_VID_train_0002/ILSVRC2015_train_00555002/folder\tfilename000000/filename\tsource databaseILSVRC_2015/database\t/source\tsize width1280/width height720/height\t/size\tobject trackid0/trackid namen02691156/name bndbox xmax659/xmax xmin592/xmin ymax375/ymax ymin334/ymin /bndbox occluded0/occluded generated0/generated\t/object/annotation ElementTreeç”Ÿæ¥å°±æ˜¯ä¸ºäº†å¤„ç†XML, å®ƒåœ¨Pythonæ ‡å‡†åº“ä¸­æœ‰ä¸¤ç§å®ç°ï¼šä¸€ç§æ˜¯çº¯Pythonå®ç°çš„, å¦‚xml.etree.ElementTree, å¦ä¸€ç§æ˜¯é€Ÿåº¦å¿«ä¸€ç‚¹çš„xml.etree.cElementTree. æ³¨æ„ï¼šå°½é‡ä½¿ç”¨Cè¯­è¨€å®ç°çš„é‚£ç§, å› ä¸ºå®ƒé€Ÿåº¦æ›´å¿«, è€Œä¸”æ¶ˆè€—çš„å†…å­˜æ›´å°‘. a. éå†æ ¹èŠ‚ç‚¹çš„ä¸‹ä¸€å±‚ b. ä¸‹æ ‡è®¿é—®å„ä¸ªæ ‡ç­¾ã€å±æ€§ã€æ–‡æœ¬ c. æŸ¥æ‰¾rootä¸‹çš„æŒ‡å®šæ ‡ç­¾ d. éå†XMLæ–‡ä»¶ e. ä¿®æ”¹XMLæ–‡ä»¶ ```pythonimport os, systry:import xml.etree.cElementTree as ETexcept:import xml.etree.ElementTree as ET è§£æxmlæ–‡ä»¶xmlFilePath = os.path.abspath('000000.xml')try: tree = ET.parse(xmlFilePath) # æˆ–è€… tree = ET.ElementTree(xmlFilePath) root = tree.getroot() # è·å–æ ¹èŠ‚ç‚¹except Exception as e: print('parse xml failed!') sys.exit() é€å±‚éå†print(root.tag, root.attrib, root.text)for child in root: print(child.tag, child.attrib, child.text) é€’å½’éå†å…¨éƒ¨:def traverseXml(element): if len(element) 0: # å¶èŠ‚ç‚¹çš„lenä¸º0 for child in element: print(child.tag, child.attrib) traverseXml(child)traverseXml(root) æ ¹æ®ç­¾åæŸ¥æ‰¾éœ€è¦çš„æ ‡ç­¾item_lists = root.findall('item') # åªèƒ½æ‰¾åˆ°å„¿å­, ä¸èƒ½æ‰¾åˆ°å­™å­, è¿”å›çš„æ˜¯å„¿å­ä»¬ç»„æˆçš„åˆ—è¡¨item = root.find('item') # è¿”å›çš„æ˜¯å•ä¸ªçš„å„¿å­print(root)print(item_lists)print(item) è·å–å¶å­èŠ‚ç‚¹çš„å€¼å½“è®¿é—®åˆ°å¶å­èŠ‚ç‚¹æ—¶, å°±å¯ä»¥åˆ©ç”¨ text æ¥å¾—åˆ°ç›¸åº”çš„æ ‡ç­¾äº†obj_bbox_set =[]objects = root.findall('object')for obj in objects: obj_name = obj.find('name').text bbox = obj.find('bndbox') x1 = int(bbox.find('xmin').text) y1 = int(bbox.find('ymin').text) x2 = int(bbox.find('xmax').text) y2 = int(bbox.find('ymax').text) obj_bbox_set.append([x1, x2, y1, y2, obj_name])print(obj_bbox_set)","tags":["Python"],"categories":["Python"]},{"title":"çŸ©å½¢è¦†ç›–","path":"/2019/09/07/20190907-çŸ©å½¢è¦†ç›–/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š363840æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é€’å½’ é¢˜ç›®æè¿°æˆ‘ä»¬å¯ä»¥ç”¨21çš„å°çŸ©å½¢æ¨ªç€æˆ–è€…ç«–ç€å»è¦†ç›–æ›´å¤§çš„çŸ©å½¢ã€‚è¯·é—®ç”¨nä¸ª21çš„å°çŸ©å½¢æ— é‡å åœ°è¦†ç›–ä¸€ä¸ª2*nçš„å¤§çŸ©å½¢ï¼Œæ€»å…±æœ‰å¤šå°‘ç§æ–¹æ³•ï¼Ÿ class Solution {public: int rectCover(int number) { if(number = 0) return 0; if(number == 1 || number == 2) return number; return rectCover(number - 1) + rectCover(number - 2); }}; è¿è¡Œæ—¶é—´ï¼š538mså ç”¨å†…å­˜ï¼š484k","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"PCAé™ç»´","path":"/2019/09/07/PCAé™ç»´/","content":"PCAï¼ˆprincipal Component Analysisï¼‰ï¼Œä¸»æˆåˆ†åˆ†ææ³•ã€‚é¡¾åæ€ä¹‰ï¼Œå°±æ˜¯æå–å‡ºæ•°æ®ä¸­ä¸»è¦çš„æˆåˆ†ï¼Œæ˜¯ä¸€ç§æ•°æ®å‹ç¼©æ–¹æ³•ï¼Œå¸¸ç”¨äºå»é™¤å™ªå£°ã€æ•°æ®é¢„å¤„ç†ï¼Œä¹Ÿæ˜¯æœºå™¨å­¦ä¹ ä¸­å¸¸è§çš„é™ç»´æ–¹æ³•ã€‚ æ­¥éª¤ å¯¹æ‰€æœ‰æ ·æœ¬è¿›è¡Œä¸­å¿ƒåŒ–å¤„ç†å³å°†æ¯ä¸ªå…ƒç´ å‡å»å®ƒçš„å¹³å‡å€¼ï¼Œè¿™æ ·å¯ä»¥å¢åŠ åŸºå‘é‡çš„æ­£äº¤æ€§ã€‚ è®¡ç®—åæ–¹å·®çŸ©é˜µåŠç‰¹å¾å€¼ã€ç‰¹å¾å‘é‡ å¯¹ç‰¹å¾å€¼è¿›è¡Œæ’åº 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 | # coding:utf-8 from matplotlib import pylab as plt import numpy as np def read_file(filename): f = open(filename, 'r') d = f.readlines() f.close() return d class PCA(object): \"\"\" ä½¿ç”¨PCAå¯¹é«˜ç»´æ•°æ®è¿›è¡Œé™ç»´å¤„ç† \"\"\" def __init__(self): data = read_file('data.txt') self.n = len(data) # æ•°æ®çš„ä¸ªæ•° self.dim = 4 # åŸå§‹æ•°æ®çš„ç»´åº¦ self.x = np.zeros((self.n, self.dim), dtype='float64') for i in range(self.n): data_ = data[i].split(',') self.x[i][0] = data_[0] self.x[i][1] = data_[1] self.x[i][2] = data_[2] self.x[i][3] = data_[3] self.k = 2 # é™åˆ°äºŒç»´ self.mean_x = np.zeros((self.n, self.dim), dtype='float64') # åŸå§‹æ•°æ®å‡å»å‡å€¼ä»¥åçš„x self.mean = np.zeros((self.dim, 1), dtype='float64') # xçš„å‡å€¼ self.cov = np.zeros((self.dim, self.dim), dtype='float64') # åæ–¹å·®çŸ©é˜µ self.pre_x = np.zeros((self.n, self.dim), dtype='float64') # é¢„å¤„ç†ä¹‹åçš„æ•°æ® self.eig_val = np.zeros((1, self.dim), dtype='float64') # ç‰¹å¾å€¼ self.eig_vec = np.zeros((self.dim, self.dim), dtype='float64') # ç‰¹å¾å‘é‡ self.final_x = np.zeros((self.n, self.k), dtype='float64') # æŠ•å½±åçš„æ•°æ® self.pretreatment() self.pca() def pretreatment(self): \"\"\" é¢„å¤„ç† \"\"\" # æ±‚å‡å€¼ for i in range(self.dim): self.mean[i] = np.mean(self.x[:, i]) for i in range(self.n): self.mean_x[i] = self.x[i] - self.mean.T # æ±‚åæ–¹å·® # self.cov = np.cov(self.mean_x, rowvar=0) # mean_xå·²ç»æ˜¯xå‡å»å‡å€¼äº†ï¼Œæ‰€ä»¥ç›´æ¥ç›¸ä¹˜å°±æ˜¯æ–¹å·® self.cov = self.mean_x.T.dot(self.mean_x) / self.n for i in range(self.dim): self.pre_x[:, i] = self.mean_x[:, i] / np.sqrt(self.cov[i][i]) # xçš„æ¯ä¸ªç»´åº¦éƒ½å¤„ç†ä¸€æ¬¡ def pca(self): \"\"\" pcaçš„å®ç° \"\"\" # éœ€è¦æ³¨æ„çš„æ˜¯ï¼Œåœ¨è¿™é‡Œéœ€è¦å¯¹é¢„å¤„ç†ä¹‹åçš„æ•°æ®é‡æ–°è®¡ç®—åæ–¹å·® # è®¡ç®—å‡å€¼ for i in range(self.dim): self.mean[i] = np.mean(self.pre_x[:, i]) # è®¡ç®—åæ–¹å·® for i in range(self.n): self.mean_x[i] = self.pre_x[i] - self.mean.T self.cov = (self.mean_x.T.dot(self.mean_x)) / self.n # æ±‚ç‰¹å¾å€¼ self.eig_val, self.eig_vec = np.linalg.eig(np.mat(self.cov)) # eig_vecçš„åˆ—å‘é‡æ˜¯ç‰¹å¾å‘é‡ï¼Œæ‰€ä»¥ç”¨eig_vå­˜å‚¨ç‰¹å¾å‘é‡ï¼Œä»¥ä¾¿åé¢æ’åº eig_v = np.zeros((self.dim, self.dim), dtype='float64') for i in range(self.dim): eig_v[i] = self.eig_vec[:, i].T # æ’åº eig_list = zip(self.eig_val, eig_v) sorted(eig_list,key=lambda g: g[0], reverse=True) # å¤„ç†æ’åºçš„ç»“æœ for i in range(len(list(eig_list))): self.eig_val[i] = eig_list[i][0] self.eig_vec[i] = eig_list[i][1] # æœ€å¤§çš„kä¸ªç‰¹å¾å‘é‡ï¼Œé™ç»´ f_vec = self.eig_vec[0:self.k, :] self.final_x = self.pre_x * f_vec.T # æ˜¾ç¤ºé™ç»´åçš„æ•°æ®ç‚¹ plt.plot(self.final_x[0:50, 0], self.final_x[0:50, 1], 'bo') plt.plot(self.final_x[50:100, 0], self.final_x[50:100, 1], 'go') plt.plot(self.final_x[100:150, 0], self.final_x[100:150, 1], 'ro') plt.show() if __name__ == '__main__': a = PCA() â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 | 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa 5.4,3.9,1.7,0.4,Iris-setosa 4.6,3.4,1.4,0.3,Iris-setosa 5.0,3.4,1.5,0.2,Iris-setosa 4.4,2.9,1.4,0.2,Iris-setosa 4.9,3.1,1.5,0.1,Iris-setosa 5.4,3.7,1.5,0.2,Iris-setosa 4.8,3.4,1.6,0.2,Iris-setosa 4.8,3.0,1.4,0.1,Iris-setosa 4.3,3.0,1.1,0.1,Iris-setosa 5.8,4.0,1.2,0.2,Iris-setosa 5.7,4.4,1.5,0.4,Iris-setosa 5.4,3.9,1.3,0.4,Iris-setosa 5.1,3.5,1.4,0.3,Iris-setosa 5.7,3.8,1.7,0.3,Iris-setosa 5.1,3.8,1.5,0.3,Iris-setosa 5.4,3.4,1.7,0.2,Iris-setosa 5.1,3.7,1.5,0.4,Iris-setosa 4.6,3.6,1.0,0.2,Iris-setosa 5.1,3.3,1.7,0.5,Iris-setosa 4.8,3.4,1.9,0.2,Iris-setosa 5.0,3.0,1.6,0.2,Iris-setosa 5.0,3.4,1.6,0.4,Iris-setosa 5.2,3.5,1.5,0.2,Iris-setosa 5.2,3.4,1.4,0.2,Iris-setosa 4.7,3.2,1.6,0.2,Iris-setosa 4.8,3.1,1.6,0.2,Iris-setosa 5.4,3.4,1.5,0.4,Iris-setosa 5.2,4.1,1.5,0.1,Iris-setosa 5.5,4.2,1.4,0.2,Iris-setosa 4.9,3.1,1.5,0.1,Iris-setosa 5.0,3.2,1.2,0.2,Iris-setosa 5.5,3.5,1.3,0.2,Iris-setosa 4.9,3.1,1.5,0.1,Iris-setosa 4.4,3.0,1.3,0.2,Iris-setosa 5.1,3.4,1.5,0.2,Iris-setosa 5.0,3.5,1.3,0.3,Iris-setosa 4.5,2.3,1.3,0.3,Iris-setosa 4.4,3.2,1.3,0.2,Iris-setosa 5.0,3.5,1.6,0.6,Iris-setosa 5.1,3.8,1.9,0.4,Iris-setosa 4.8,3.0,1.4,0.3,Iris-setosa 5.1,3.8,1.6,0.2,Iris-setosa 4.6,3.2,1.4,0.2,Iris-setosa 5.3,3.7,1.5,0.2,Iris-setosa 5.0,3.3,1.4,0.2,Iris-setosa 7.0,3.2,4.7,1.4,Iris-versicolor 6.4,3.2,4.5,1.5,Iris-versicolor 6.9,3.1,4.9,1.5,Iris-versicolor 5.5,2.3,4.0,1.3,Iris-versicolor 6.5,2.8,4.6,1.5,Iris-versicolor 5.7,2.8,4.5,1.3,Iris-versicolor 6.3,3.3,4.7,1.6,Iris-versicolor 4.9,2.4,3.3,1.0,Iris-versicolor 6.6,2.9,4.6,1.3,Iris-versicolor 5.2,2.7,3.9,1.4,Iris-versicolor 5.0,2.0,3.5,1.0,Iris-versicolor 5.9,3.0,4.2,1.5,Iris-versicolor 6.0,2.2,4.0,1.0,Iris-versicolor 6.1,2.9,4.7,1.4,Iris-versicolor 5.6,2.9,3.6,1.3,Iris-versicolor 6.7,3.1,4.4,1.4,Iris-versicolor 5.6,3.0,4.5,1.5,Iris-versicolor 5.8,2.7,4.1,1.0,Iris-versicolor 6.2,2.2,4.5,1.5,Iris-versicolor 5.6,2.5,3.9,1.1,Iris-versicolor 5.9,3.2,4.8,1.8,Iris-versicolor 6.1,2.8,4.0,1.3,Iris-versicolor 6.3,2.5,4.9,1.5,Iris-versicolor 6.1,2.8,4.7,1.2,Iris-versicolor 6.4,2.9,4.3,1.3,Iris-versicolor 6.6,3.0,4.4,1.4,Iris-versicolor 6.8,2.8,4.8,1.4,Iris-versicolor 6.7,3.0,5.0,1.7,Iris-versicolor 6.0,2.9,4.5,1.5,Iris-versicolor 5.7,2.6,3.5,1.0,Iris-versicolor 5.5,2.4,3.8,1.1,Iris-versicolor 5.5,2.4,3.7,1.0,Iris-versicolor 5.8,2.7,3.9,1.2,Iris-versicolor 6.0,2.7,5.1,1.6,Iris-versicolor 5.4,3.0,4.5,1.5,Iris-versicolor 6.0,3.4,4.5,1.6,Iris-versicolor 6.7,3.1,4.7,1.5,Iris-versicolor 6.3,2.3,4.4,1.3,Iris-versicolor 5.6,3.0,4.1,1.3,Iris-versicolor 5.5,2.5,4.0,1.3,Iris-versicolor 5.5,2.6,4.4,1.2,Iris-versicolor 6.1,3.0,4.6,1.4,Iris-versicolor 5.8,2.6,4.0,1.2,Iris-versicolor 5.0,2.3,3.3,1.0,Iris-versicolor 5.6,2.7,4.2,1.3,Iris-versicolor 5.7,3.0,4.2,1.2,Iris-versicolor 5.7,2.9,4.2,1.3,Iris-versicolor 6.2,2.9,4.3,1.3,Iris-versicolor 5.1,2.5,3.0,1.1,Iris-versicolor 5.7,2.8,4.1,1.3,Iris-versicolor 6.3,3.3,6.0,2.5,Iris-virginica 5.8,2.7,5.1,1.9,Iris-virginica 7.1,3.0,5.9,2.1,Iris-virginica 6.3,2.9,5.6,1.8,Iris-virginica 6.5,3.0,5.8,2.2,Iris-virginica 7.6,3.0,6.6,2.1,Iris-virginica 4.9,2.5,4.5,1.7,Iris-virginica 7.3,2.9,6.3,1.8,Iris-virginica 6.7,2.5,5.8,1.8,Iris-virginica 7.2,3.6,6.1,2.5,Iris-virginica 6.5,3.2,5.1,2.0,Iris-virginica 6.4,2.7,5.3,1.9,Iris-virginica 6.8,3.0,5.5,2.1,Iris-virginica 5.7,2.5,5.0,2.0,Iris-virginica 5.8,2.8,5.1,2.4,Iris-virginica 6.4,3.2,5.3,2.3,Iris-virginica 6.5,3.0,5.5,1.8,Iris-virginica 7.7,3.8,6.7,2.2,Iris-virginica 7.7,2.6,6.9,2.3,Iris-virginica 6.0,2.2,5.0,1.5,Iris-virginica 6.9,3.2,5.7,2.3,Iris-virginica 5.6,2.8,4.9,2.0,Iris-virginica 7.7,2.8,6.7,2.0,Iris-virginica 6.3,2.7,4.9,1.8,Iris-virginica 6.7,3.3,5.7,2.1,Iris-virginica 7.2,3.2,6.0,1.8,Iris-virginica 6.2,2.8,4.8,1.8,Iris-virginica 6.1,3.0,4.9,1.8,Iris-virginica 6.4,2.8,5.6,2.1,Iris-virginica 7.2,3.0,5.8,1.6,Iris-virginica 7.4,2.8,6.1,1.9,Iris-virginica 7.9,3.8,6.4,2.0,Iris-virginica 6.4,2.8,5.6,2.2,Iris-virginica 6.3,2.8,5.1,1.5,Iris-virginica 6.1,2.6,5.6,1.4,Iris-virginica 7.7,3.0,6.1,2.3,Iris-virginica 6.3,3.4,5.6,2.4,Iris-virginica 6.4,3.1,5.5,1.8,Iris-virginica 6.0,3.0,4.8,1.8,Iris-virginica 6.9,3.1,5.4,2.1,Iris-virginica 6.7,3.1,5.6,2.4,Iris-virginica 6.9,3.1,5.1,2.3,Iris-virginica 5.8,2.7,5.1,1.9,Iris-virginica 6.8,3.2,5.9,2.3,Iris-virginica 6.7,3.3,5.7,2.5,Iris-virginica 6.7,3.0,5.2,2.3,Iris-virginica 6.3,2.5,5.0,1.9,Iris-virginica 6.5,3.0,5.2,2.0,Iris-virginica 6.2,3.4,5.4,2.3,Iris-virginica 5.9,3.0,5.1,1.8,Iris-virginica â€”|â€” å‚è€ƒé“¾æ¥ï¼šhttps://github.com/eva-n27/PCA","tags":["Python"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"kmeansç®—æ³•","path":"/2019/09/07/kmeansç®—æ³•/","content":"K-Meansçš„æ€æƒ³ååˆ†ç®€å•ï¼Œé¦–å…ˆéšæœºæŒ‡å®šç±»ä¸­å¿ƒï¼Œæ ¹æ®æ ·æœ¬ä¸ç±»ä¸­å¿ƒçš„è¿œè¿‘åˆ’åˆ†ç±»ç°‡ï¼Œæ¥ç€é‡æ–°è®¡ç®—ç±»ä¸­å¿ƒï¼Œè¿­ä»£ç›´è‡³æ”¶æ•›ã€‚ä½†æ˜¯å…¶ä¸­è¿­ä»£çš„è¿‡ç¨‹å¹¶ä¸æ˜¯ä¸»è§‚åœ°æƒ³è±¡å¾—å‡ºï¼Œäº‹å®ä¸Šï¼Œè‹¥å°†æ ·æœ¬çš„ç±»åˆ«çœ‹åšä¸ºâ€œéšå˜é‡â€ï¼ˆlatent variableï¼‰ï¼Œç±»ä¸­å¿ƒçœ‹ä½œæ ·æœ¬çš„åˆ†å¸ƒå‚æ•°ï¼Œè¿™ä¸€è¿‡ç¨‹æ­£æ˜¯é€šè¿‡EMç®—æ³•çš„ä¸¤æ­¥èµ°ç­–ç•¥è€Œè®¡ç®—å‡ºï¼Œå…¶æ ¹æœ¬çš„ç›®çš„æ˜¯ä¸ºäº†æœ€å°åŒ–å¹³æ–¹è¯¯å·®å‡½æ•°Eã€‚ kmeansç®—æ³•çš„æœ€å¤§å¼±ç‚¹ï¼šåªèƒ½å¤„ç†çƒå½¢çš„ç°‡ï¼ˆç†è®ºï¼‰kmeans è®¡ç®—æ­¥éª¤ 1.éšæœºé€‰å–Kä¸ªèšç±»ä¸­å¿ƒï¼Œè¿™é‡Œçš„kå€¼å¯ä»¥è‡ªå·±è®¾å®š 2.å…ˆè®¾ç½®ä¸€ä¸ªèšç±»æ ‡å¿—ï¼Œç”¨æ¥ä¿å­˜å½“å‰çš„ æ ·æœ¬ä¸ç¬¬å‡ ä¸ªèšç±»ä¸­å¿ƒæœ€è¿‘ 3.è®¡ç®—æ¯ä¸ªæ ·ä¾‹ä¸æ¯ä¸ªèšç±»ä¸­å¿ƒçš„è·ç¦»ï¼Œä¿å­˜æœ€å°è·ç¦»çš„kä»¥åŠè·ç¦» 4.æ›´æ–°èšç±»ä¸­å¿ƒï¼Œä¸ºå½“å‰ç±»åˆ«æ‰€æœ‰æ ·æœ¬çš„å‡å€¼å¤§å° 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 | from numpy import * import time import matplotlib.pyplot as plt # calculate Euclidean distance def euclDistance(vector1, vector2): return sqrt(sum(power(vector2 - vector1, 2))) # æ±‚è¿™ä¸¤ä¸ªçŸ©é˜µçš„è·ç¦»ï¼Œ vector1, vector2 å‡ä¸ºçŸ©é˜µ # init centroids with random samples # åœ¨æ ·æœ¬é›†ä¸­éšæœºé€‰å–kä¸ªæ ·æœ¬ç‚¹ä½œä¸ºåˆå§‹è´¨å¿ƒ def initCentroids(dataSet, k): numSamples, dim = dataSet.shape # çŸ©é˜µçš„è¡Œæ•°ã€åˆ—æ•° centroids = zeros((k, dim)) # æ„Ÿè§‰è¦ä¸è¦ä½ éƒ½å¯ä»¥ for i in range(k): index = int(random.uniform(0, numSamples)) # éšæœºäº§ç”Ÿä¸€ä¸ªæµ®ç‚¹æ•°ï¼Œç„¶åå°†å…¶è½¬åŒ–ä¸ºintå‹ centroids[i, :] = dataSet[index, :] return centroids # k-means cluster # dataSetä¸ºä¸€ä¸ªçŸ©é˜µ # kä¸ºå°†dataSetçŸ©é˜µä¸­çš„æ ·æœ¬åˆ†æˆkä¸ªç±» def kmeans(dataSet, k): numSamples = dataSet.shape[0] # è¯»å–çŸ©é˜µdataSetçš„ç¬¬ä¸€ç»´åº¦çš„é•¿åº¦,å³è·å¾—æœ‰å¤šå°‘ä¸ªæ ·æœ¬æ•°æ® # first column stores which cluster this sample belongs to, # second column stores the error between this sample and its centroid clusterAssment = mat(zeros((numSamples, 2))) # å¾—åˆ°ä¸€ä¸ªN*2çš„é›¶çŸ©é˜µ clusterChanged = True ## step 1: init centroids centroids = initCentroids(dataSet, k) # åœ¨æ ·æœ¬é›†ä¸­éšæœºé€‰å–kä¸ªæ ·æœ¬ç‚¹ä½œä¸ºåˆå§‹è´¨å¿ƒ while clusterChanged: clusterChanged = False ## for each sample for i in range(numSamples): # range minDist = 100000.0 minIndex = 0 ## for each centroid ## step 2: find the centroid who is closest # è®¡ç®—æ¯ä¸ªæ ·æœ¬ç‚¹ä¸è´¨ç‚¹ä¹‹é—´çš„è·ç¦»ï¼Œå°†å…¶å½’å†…åˆ°è·ç¦»æœ€å°çš„é‚£ä¸€ç°‡ for j in range(k): distance = euclDistance(centroids[j, :], dataSet[i, :]) if distance minDist: minDist = distance minIndex = j ## step 3: update its cluster # kä¸ªç°‡é‡Œé¢ä¸ç¬¬iä¸ªæ ·æœ¬è·ç¦»æœ€å°çš„çš„æ ‡å·å’Œè·ç¦»ä¿å­˜åœ¨clusterAssmentä¸­ # è‹¥æ‰€æœ‰çš„æ ·æœ¬ä¸åœ¨å˜åŒ–ï¼Œåˆ™é€€å‡ºwhileå¾ªç¯ if clusterAssment[i, 0] != minIndex: clusterChanged = True clusterAssment[i, :] = minIndex, minDist ** 2 # ä¸¤ä¸ª**è¡¨ç¤ºçš„æ˜¯minDistçš„å¹³æ–¹ ## step 4: update centroids for j in range(k): # clusterAssment[:,0].A==jæ˜¯æ‰¾å‡ºçŸ©é˜µclusterAssmentä¸­ç¬¬ä¸€åˆ—å…ƒç´ ä¸­ç­‰äºjçš„è¡Œçš„ä¸‹æ ‡ï¼Œè¿”å›çš„æ˜¯ä¸€ä¸ªä»¥arrayçš„åˆ—è¡¨ï¼Œç¬¬ä¸€ä¸ªarrayä¸ºç­‰äºjçš„ä¸‹æ ‡ pointsInCluster = dataSet[nonzero(clusterAssment[:, 0].A == j)[0]] # å°†dataSetçŸ©é˜µä¸­ç›¸å¯¹åº”çš„æ ·æœ¬æå–å‡ºæ¥ centroids[j, :] = mean(pointsInCluster, axis=0) # è®¡ç®—æ ‡æ³¨ä¸ºjçš„æ‰€æœ‰æ ·æœ¬çš„å¹³å‡å€¼ print('Congratulations, cluster complete!') return centroids, clusterAssment # show your cluster only available with 2-D data # centroidsä¸ºkä¸ªç±»åˆ«ï¼Œå…¶ä¸­ä¿å­˜ç€æ¯ä¸ªç±»åˆ«çš„è´¨å¿ƒ # clusterAssmentä¸ºæ ·æœ¬çš„æ ‡è®°ï¼Œç¬¬ä¸€åˆ—ä¸ºæ­¤æ ·æœ¬çš„ç±»åˆ«å·ï¼Œç¬¬äºŒåˆ—ä¸ºåˆ°æ­¤ç±»åˆ«è´¨å¿ƒçš„è·ç¦» def showCluster(dataSet, k, centroids, clusterAssment): numSamples, dim = dataSet.shape if dim != 2: print(\"Sorry! I can not draw because the dimension of your data is not 2!\") return 1 mark = ['or', 'ob', 'og', 'ok', '^r', '+r', 'sr', 'dr', 'r', 'pr'] if k len(mark): print(\"Sorry! Your k is too large! please contact wojiushimogui\") return 1 # draw all samples for i in range(numSamples): markIndex = int(clusterAssment[i, 0]) # ä¸ºæ ·æœ¬æŒ‡å®šé¢œè‰² plt.plot(dataSet[i, 0], dataSet[i, 1], mark[markIndex]) mark = ['Dr', 'Db', 'Dg', 'Dk', '^b', '+b', 'sb', 'db', 'b', 'pb'] # draw the centroids for i in range(k): plt.plot(centroids[i, 0], centroids[i, 1], mark[i], markersize=12) plt.show() â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 | from numpy import * import time import matplotlib.pyplot as plt import kmeans ## step 1: load data print(\"step 1: load data...\") dataSet = [] # åˆ—è¡¨ï¼Œç”¨æ¥è¡¨ç¤ºï¼Œåˆ—è¡¨ä¸­çš„æ¯ä¸ªå…ƒç´ ä¹Ÿæ˜¯ä¸€ä¸ªäºŒç»´çš„åˆ—è¡¨ï¼›è¿™ä¸ªäºŒç»´åˆ—è¡¨å°±æ˜¯ä¸€ä¸ªæ ·æœ¬ï¼Œæ ·æœ¬ä¸­åŒ…å«æœ‰æˆ‘ä»¬çš„å±æ€§å€¼å’Œç±»åˆ«å·ã€‚ # ä¸æˆ‘ä»¬æ‰€ç†Ÿæ‚‰çš„çŸ©é˜µç±»ä¼¼ï¼Œæœ€ç»ˆæˆ‘ä»¬å°†è·å¾—N*2çš„çŸ©é˜µï¼Œæ¯è¡Œå…ƒç´ æ„æˆäº†æˆ‘ä»¬çš„è®­ç»ƒæ ·æœ¬çš„å±æ€§å€¼å’Œç±»åˆ«å· fileIn = open(\"./testSet.txt\") # æ˜¯æ­£æ–œæ  for line in fileIn.readlines(): temp = [] lineArr = line.strip().split('\\t') # line.strip()æŠŠæœ«å°¾çš„' 'å»æ‰ temp.append(float(lineArr[0])) temp.append(float(lineArr[1])) dataSet.append(temp) # dataSet.append([float(lineArr[0]), float(lineArr[1])]) fileIn.close() ## step 2: clustering... print(\"step 2: clustering...\") dataSet = mat(dataSet) # mat()å‡½æ•°æ˜¯Numpyä¸­çš„åº“å‡½æ•°ï¼Œå°†æ•°ç»„è½¬åŒ–ä¸ºçŸ©é˜µ k = 4 centroids, clusterAssment = kmeans.kmeans(dataSet, k) # è°ƒç”¨KMeansæ–‡ä»¶ä¸­å®šä¹‰çš„kmeansæ–¹æ³•ã€‚ ## step 3: show the result print(\"step 3: show the result...\") kmeans.showCluster(dataSet, k, centroids, clusterAssment) â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 | 1.658985\t4.285136 -3.453687\t3.424321 4.838138\t1.151539 -5.379713\t-3.362104 0.972564\t2.924086 -3.567919\t1.531611 0.450614\t-3.302219 -3.487105\t-1.724432 2.668759\t1.594842 -3.156485\t3.191137 3.165506\t-3.999838 -2.786837\t-3.099354 4.208187\t2.984927 -2.123337\t2.943366 0.704199\t-0.479481 -0.392370\t-3.963704 2.831667\t1.574018 -0.790153\t3.343144 2.943496\t-3.357075 -3.195883\t-2.283926 2.336445\t2.875106 -1.786345\t2.554248 2.190101\t-1.906020 -3.403367\t-2.778288 1.778124\t3.880832 -1.688346\t2.230267 2.592976\t-2.054368 -4.007257\t-3.207066 2.257734\t3.387564 -2.679011\t0.785119 0.939512\t-4.023563 -3.674424\t-2.261084 2.046259\t2.735279 -3.189470\t1.780269 4.372646\t-0.822248 -2.579316\t-3.497576 1.889034\t5.190400 -0.798747\t2.185588 2.836520\t-2.658556 -3.837877\t-3.253815 2.096701\t3.886007 -2.709034\t2.923887 3.367037\t-3.184789 -2.121479\t-4.232586 2.329546\t3.179764 -3.284816\t3.273099 3.091414\t-3.815232 -3.762093\t-2.432191 3.542056\t2.778832 -1.736822\t4.241041 2.127073\t-2.983680 -4.323818\t-3.938116 3.792121\t5.135768 -4.786473\t3.358547 2.624081\t-3.260715 -4.009299\t-2.978115 2.493525\t1.963710 -2.513661\t2.642162 1.864375\t-3.176309 -3.171184\t-3.572452 2.894220\t2.489128 -2.562539\t2.884438 3.491078\t-3.947487 -2.565729\t-2.012114 3.332948\t3.983102 -1.616805\t3.573188 2.280615\t-2.559444 -2.651229\t-3.103198 2.321395\t3.154987 -1.685703\t2.939697 3.031012\t-3.620252 -4.599622\t-2.185829 4.196223\t1.126677 -2.133863\t3.093686 4.668892\t-2.562705 -2.793241\t-2.149706 2.884105\t3.043438 -2.967647\t2.848696 4.479332\t-1.764772 -4.905566\t-2.911070 â€”|â€” å‚è€ƒé“¾æ¥ï¼šhttps://github.com/wojiushimogui/kmeans","tags":["Python"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"xmlç”Ÿæˆ","path":"/2019/09/07/xmlç”Ÿæˆ/","content":"å…ˆåˆ›å»º root Element, ç„¶ååˆ›å»º SubElement, æœ€åå°† root ä¼ å…¥ ElementTree(element), åˆ›å»º tree, è°ƒç”¨ tree.write() æ–¹æ³•å†™å…¥æ–‡ä»¶,åˆ›å»º XML ç±»å‹çš„æ•°æ®æ–‡ä»¶ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | import xml.etree.ElementTree as ET def subElement(root, tag, text): ele = ET.SubElement(root, tag) ele.text = text ele.tail = ' ' root = ET.Element(\"note\") to = root.makeelement(\"to\", {}) to.text = \"peter\" to.tail = ' ' root.append(to) subElement(root, \"from\", \"marry\") subElement(root, \"heading\", \"Reminder\") subElement(root, \"body\", \"Don't forget the meeting!\") tree = ET.ElementTree(root) tree.write(\"note.xml\", encoding=\"utf-8\", xml_declaration=True) â€”|â€” 1 2 3 4 5 6 7 | ?xml version='1.0' encoding='utf-8'? note topeter/to frommarry/from headingReminder/heading bodyDon't forget the meeting!/body /note â€”|â€”","tags":["XML"],"categories":["å·¥å…·"]},{"title":"xmlè§£æ","path":"/2019/09/07/xmlè§£æ/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 | annotation folderILSVRC2015_VID_train_0002/ILSVRC2015_train_00555002/folder filename000000/filename source databaseILSVRC_2015/database /source size width1280/width height720/height /size object trackid0/trackid namen02691156/name bndbox xmax659/xmax xmin592/xmin ymax375/ymax ymin334/ymin /bndbox occluded0/occluded generated0/generated /object /annotation â€”|â€” ElementTreeç”Ÿæ¥å°±æ˜¯ä¸ºäº†å¤„ç†XML, å®ƒåœ¨Pythonæ ‡å‡†åº“ä¸­æœ‰ä¸¤ç§å®ç°ï¼šä¸€ç§æ˜¯çº¯Pythonå®ç°çš„, å¦‚xml.etree.ElementTree, å¦ä¸€ç§æ˜¯é€Ÿåº¦å¿«ä¸€ç‚¹çš„xml.etree.cElementTree. æ³¨æ„ï¼šå°½é‡ä½¿ç”¨Cè¯­è¨€å®ç°çš„é‚£ç§, å› ä¸ºå®ƒé€Ÿåº¦æ›´å¿«, è€Œä¸”æ¶ˆè€—çš„å†…å­˜æ›´å°‘. a. éå†æ ¹èŠ‚ç‚¹çš„ä¸‹ä¸€å±‚ b. ä¸‹æ ‡è®¿é—®å„ä¸ªæ ‡ç­¾ã€å±æ€§ã€æ–‡æœ¬ c. æŸ¥æ‰¾rootä¸‹çš„æŒ‡å®šæ ‡ç­¾ d. éå†XMLæ–‡ä»¶ e. ä¿®æ”¹XMLæ–‡ä»¶ 1 2 3 4 5 | import os, sys try: import xml.etree.cElementTree as ET except: import xml.etree.ElementTree as ET â€”|â€” è§£æxmlæ–‡ä»¶1 2 3 4 5 6 7 8 | xmlFilePath = os.path.abspath('000000.xml') try: tree = ET.parse(xmlFilePath) # æˆ–è€… tree = ET.ElementTree(xmlFilePath) root = tree.getroot() # è·å–æ ¹èŠ‚ç‚¹ except Exception as e: print('parse xml failed!') sys.exit() â€”|â€” é€å±‚éå†1 2 3 | print(root.tag, root.attrib, root.text) for child in root: print(child.tag, child.attrib, child.text) â€”|â€” é€’å½’éå†å…¨éƒ¨:1 2 3 4 5 6 7 | def traverseXml(element): if len(element) 0: # å¶èŠ‚ç‚¹çš„lenä¸º0 for child in element: print(child.tag, child.attrib) traverseXml(child) traverseXml(root) â€”|â€” æ ¹æ®ç­¾åæŸ¥æ‰¾éœ€è¦çš„æ ‡ç­¾1 2 3 4 5 | item_lists = root.findall('item') # åªèƒ½æ‰¾åˆ°å„¿å­, ä¸èƒ½æ‰¾åˆ°å­™å­, è¿”å›çš„æ˜¯å„¿å­ä»¬ç»„æˆçš„åˆ—è¡¨ item = root.find('item') # è¿”å›çš„æ˜¯å•ä¸ªçš„å„¿å­ print(root) print(item_lists) print(item) â€”|â€” è·å–å¶å­èŠ‚ç‚¹çš„å€¼å½“è®¿é—®åˆ°å¶å­èŠ‚ç‚¹æ—¶, å°±å¯ä»¥åˆ©ç”¨ text æ¥å¾—åˆ°ç›¸åº”çš„æ ‡ç­¾äº†1 2 3 4 5 6 7 8 9 10 11 | obj_bbox_set =[] objects = root.findall('object') for obj in objects: obj_name = obj.find('name').text bbox = obj.find('bndbox') x1 = int(bbox.find('xmin').text) y1 = int(bbox.find('ymin').text) x2 = int(bbox.find('xmax').text) y2 = int(bbox.find('ymax').text) obj_bbox_set.append([x1, x2, y1, y2, obj_name]) print(obj_bbox_set) â€”|â€”","tags":["Python"],"categories":["ç®—æ³•"]},{"title":"çŸ©å½¢è¦†ç›–","path":"/2019/09/07/çŸ©å½¢è¦†ç›–/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š363840æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é€’å½’ é¢˜ç›®æè¿°æˆ‘ä»¬å¯ä»¥ç”¨21çš„å°çŸ©å½¢æ¨ªç€æˆ–è€…ç«–ç€å»è¦†ç›–æ›´å¤§çš„çŸ©å½¢ã€‚è¯·é—®ç”¨nä¸ª21çš„å°çŸ©å½¢æ— é‡å åœ°è¦†ç›–ä¸€ä¸ª2*nçš„å¤§çŸ©å½¢ï¼Œæ€»å…±æœ‰å¤šå°‘ç§æ–¹æ³•ï¼Ÿ 1 2 3 4 5 6 7 8 | class Solution { public: int rectCover(int number) { if(number = 0) return 0; if(number == 1 || number == 2) return number; return rectCover(number - 1) + rectCover(number - 2); } }; â€”|â€” è¿è¡Œæ—¶é—´ï¼š538mså ç”¨å†…å­˜ï¼š484k","tags":["ç¼–ç¨‹"],"categories":["ç®—æ³•"]},{"title":"mx-maskrcnnç¯å¢ƒæ­å»º","path":"/2019/09/06/20190906-mx-maskrcnnç¯å¢ƒæ­å»º/","content":"ä»https://hub.docker.com/ é€‰å–æ‰€éœ€çš„é•œåƒ ä¸‹è½½caffe2 é•œåƒdocker pull caffe2/caffe2 :snapshot-py2-cuda8.0-cudnn7-ubuntu16.04Cannot open your terminal â€˜/dev/pts/0â€™é”™è¯¯åŸå› è§£å†³å¯ä»¥ä½¿ç”¨scriptå‘½ä»¤æ¥è®°å½•è¿™ä¸ªç»ˆç«¯ä¼šè¯,æ‰§è¡Œscript /dev/nullscreen -S caiusdocker åˆ†é…http://www.cnblogs.com/codeaaa/p/9041533.htmlhttps://blog.csdn.net/u013948858/article/details/78429954ï¼ˆæœ‰æ•ˆï¼‰docker run -it -v /media/:/media/ â€”name=mxcaius â€”runtime=nvidia 89f57a4ade86 /bin/bashdocker ubuntuæºå¡ä¸»ï¼Œè§£å†³æªæ–½ï¼šmv sourceæ”¹å®Œä¹‹åæ”¹å›å»mv sources.list.d.odd sources.list.déœ€è¦BLASåº“ï¼Œå¯ä»¥å®‰è£…ATLASã€OpenBLASã€MKLï¼Œæˆ‘å®‰è£…çš„æ˜¯atlassudo apt-get install libatlas-base-devå®‰è£…opencvåº“pip install opencv-pythonsudo apt-get install libopencv-devå®‰è£…PythonåŒ…cd python;python setup.py installapt-get install python-numpyodules/imgproc/src/resize.cpp:3596: error: (-215:Assertion failed) func != 0 in function â€˜resizeâ€™ numpy 1.14setuptoolså’Œnumpy(sudo apt-get install python-numpy) git clone â€”recursive https://github.com/apache/incubator-mxnet.git incubator-mxnet â€”branch 0.11.0cp rcnn/CXX_OP/* incubator-mxnet/src/operator/cd incubator-mxnetmake -j USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1cd ..makebash scripts/train_alternate.shmakecaius@sugon:~$ echo -n â€œcvlab1205â€ |md5sumd0599e86d6134fee87bcf017ddca1990 ç„¶åæˆ‘ä»¬ä½¿ç”¨docker psæŸ¥çœ‹åˆ°è¯¥å®¹å™¨ä¿¡æ¯ï¼Œæ¥ä¸‹æ¥å°±ä½¿ç”¨docker attachè¿›å…¥è¯¥å®¹å™¨å¯å½¢å˜å·ç§¯ IndexError: list index out of range self.class_id = [0, 1] imdb = eval(dataset)(image_set, root_path, dataset_path) [â€˜trainâ€™]imagesetIcdar2015/media/data1/caius/dataset[â€˜trainâ€™]imagesetIcdar2015/media/data1/caius/dataset icdar2015_train gt roidb loaded from model/res50-fpn/icdar2015/alternate/cache/icdar2015_train_gt_roidb.pklOpenCV Error: Assertion failed (func != 0) in resize, file /io/opencv/modules/imgproc/src/imgwarp.cpp, line 3370Traceback (most recent call last):File â€œtrain_alternate_mask_fpn.pyâ€, line 118, inmain()File â€œtrain_alternate_mask_fpn.pyâ€, line 115, in mainargs.rcnn_epoch, args.rcnn_lr, args.rcnn_lr_step)File â€œtrain_alternate_mask_fpn.pyâ€, line 61, in alternate_trainvis=False, shuffle=False, thresh=0)File â€œ/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/tools/test_rpn.pyâ€, line 63, in test_rpnimdb_boxes = generate_proposals(predictor, test_data, imdb, vis=vis, thresh=thresh)File â€œ/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/core/tester.pyâ€, line 61, in generate_proposalsfor im_info, data_batch in test_data:File â€œ/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/core/loader.pyâ€, line 60, in nextself.get_batch()File â€œ/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/core/loader.pyâ€, line 83, in get_batchdata, label, im_info = get_rpn_testbatch(roidb)File â€œ/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/io/rpn.pyâ€, line 32, in get_rpn_testbatchimgs, roidb,masks = get_image(roidb)File â€œ/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/io/image.pyâ€, line 99, in get_imagemask, _ = resize(mask, target_size, max_size)File â€œ/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/io/image.pyâ€, line 138, in resizeim = cv2.resize(im, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR)cv2.error: /io/opencv/modules/imgproc/src/imgwarp.cpp:3370: error: (-215) func != 0 in function resize root@d59236d7a683:/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss#","tags":["å·¥å…·"],"categories":["å·¥å…·"]},{"title":"dockerä½¿ç”¨ç¬”è®°","path":"/2019/09/06/20190906-dockerä½¿ç”¨ç¬”è®°/","content":"0. æŸ¥çœ‹éœ€è¦çš„Dockeråˆ—è¡¨https://hub.docker.com/r/nvidia/cuda/åˆ—å‡ºå·²ç»å­˜åœ¨çš„é•œåƒåˆ—è¡¨ 1. åˆ›å»ºå®¹å™¨(æ²¡æœ‰é•œåƒ, ä¼šè‡ªåŠ¨ä¸‹è½½)docker run -it -v /media/data2/dh:/media/data2/dh â€”name=cloud â€”runtime=nvidia nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04 /bin/bash(å·²æœ‰é•œåƒ)docker run -it -v /media/data2/dh:/media/data2/dh â€”name=cloud â€”runtime=nvidia afc5ab1e9a0d /bin/bash 2. å¯åŠ¨å®¹å™¨dockerstart cloudï¼ˆå®¹å™¨åï¼‰ 3. è¿›å…¥å®¹å™¨docker exec -it cloud /bin/bash 4. ç¦»å¼€å®¹å™¨exit 5. åˆ é™¤å®¹å™¨A. åœæ­¢å®¹å™¨docker stop CONTAINER_ID B. åˆ é™¤å®¹å™¨docker rm CONTAINER_ID å¯åŠ¨ systemctl start docker å®ˆæŠ¤è¿›ç¨‹é‡å¯ sudo systemctl daemon-reload é‡å¯dockeræœåŠ¡ systemctl restart docker é‡å¯dockeræœåŠ¡ sudo service docker restart å…³é—­docker service docker stop å…³é—­docker systemctl stop docker","tags":["å·¥å…·"],"categories":["å·¥å…·"]},{"title":"å˜æ€è·³å°é˜¶","path":"/2019/09/06/20190906-å˜æ€è·³å°é˜¶/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š396047æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é€’å½’ é¢˜ç›®æè¿°ä¸€åªé’è›™ä¸€æ¬¡å¯ä»¥è·³ä¸Š1çº§å°é˜¶ï¼Œä¹Ÿå¯ä»¥è·³ä¸Š2çº§â€¦â€¦å®ƒä¹Ÿå¯ä»¥è·³ä¸Šnçº§ã€‚æ±‚è¯¥é’è›™è·³ä¸Šä¸€ä¸ªnçº§çš„å°é˜¶æ€»å…±æœ‰å¤šå°‘ç§è·³æ³•ã€‚ è§£æf(n) = f(n-1)+f(n-2)+â€¦+f(1)f(n-1) = f(n-2)+f(n-3)+â€¦f(1) f(n) = 2*f(n-1) class Solution {public: int jumpFloorII(int number) { if(number == 1) return 1; else return 2*jumpFloorII(number-1); }}; è¿è¡Œæ—¶é—´ï¼š4mså ç”¨å†…å­˜ï¼š480k","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"dockerä½¿ç”¨ç¬”è®°","path":"/2019/09/06/dockerä½¿ç”¨ç¬”è®°/","content":"0. æŸ¥çœ‹éœ€è¦çš„Dockeråˆ—è¡¨https://hub.docker.com/r/nvidia/cuda/åˆ—å‡ºå·²ç»å­˜åœ¨çš„é•œåƒåˆ—è¡¨ 1. åˆ›å»ºå®¹å™¨(æ²¡æœ‰é•œåƒ, ä¼šè‡ªåŠ¨ä¸‹è½½)docker run -it -v /media/data2/dh:/media/data2/dh â€”name=cloud â€”runtime=nvidia nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04 /bin/bash(å·²æœ‰é•œåƒ)docker run -it -v /media/data2/dh:/media/data2/dh â€”name=cloud â€”runtime=nvidia afc5ab1e9a0d /bin/bash 2. å¯åŠ¨å®¹å™¨dockerstart cloudï¼ˆå®¹å™¨åï¼‰ 3. è¿›å…¥å®¹å™¨docker exec -it cloud /bin/bash 4. ç¦»å¼€å®¹å™¨exit 5. åˆ é™¤å®¹å™¨A. åœæ­¢å®¹å™¨docker stop CONTAINER_ID B. åˆ é™¤å®¹å™¨docker rm CONTAINER_ID å¯åŠ¨ systemctl start docker å®ˆæŠ¤è¿›ç¨‹é‡å¯ sudo systemctl daemon-reload é‡å¯dockeræœåŠ¡ systemctl restart docker é‡å¯dockeræœåŠ¡ sudo service docker restart å…³é—­docker service docker stop å…³é—­docker systemctl stop docker","tags":["Docker"],"categories":["C++"]},{"title":"mx-maskrcnnç¯å¢ƒæ­å»º","path":"/2019/09/06/mx-maskrcnnç¯å¢ƒæ­å»º/","content":"ä»https://hub.docker.com/ é€‰å–æ‰€éœ€çš„é•œåƒ ä¸‹è½½caffe2 é•œåƒdocker pull caffe2/caffe2 :snapshot-py2-cuda8.0-cudnn7-ubuntu16.04Cannot open your terminal â€˜/dev/pts/0â€™é”™è¯¯åŸå› è§£å†³å¯ä»¥ä½¿ç”¨scriptå‘½ä»¤æ¥è®°å½•è¿™ä¸ªç»ˆç«¯ä¼šè¯,æ‰§è¡Œscript /dev/nullscreen -S caiusdocker åˆ†é…http://www.cnblogs.com/codeaaa/p/9041533.htmlhttps://blog.csdn.net/u013948858/article/details/78429954ï¼ˆæœ‰æ•ˆï¼‰docker run -it -v /media/:/media/ â€”name=mxcaius â€”runtime=nvidia 89f57a4ade86 /bin/bashdocker ubuntuæºå¡ä¸»ï¼Œè§£å†³æªæ–½ï¼šmv sourceæ”¹å®Œä¹‹åæ”¹å›å»mv sources.list.d.odd sources.list.déœ€è¦BLASåº“ï¼Œå¯ä»¥å®‰è£…ATLASã€OpenBLASã€MKLï¼Œæˆ‘å®‰è£…çš„æ˜¯atlassudo apt-get install libatlas-base-devå®‰è£…opencvåº“pip install opencv-pythonsudo apt-get install libopencv-devå®‰è£…PythonåŒ…cd python;python setup.py installapt-get install python-numpyodules/imgproc/src/resize.cpp:3596: error: (-215:Assertion failed) func != 0 in function â€˜resizeâ€™ numpy 1.14setuptoolså’Œnumpy(sudo apt-get install python-numpy) git clone â€”recursive https://github.com/apache/incubator-mxnet.git incubator-mxnet â€”branch 0.11.0cp rcnn/CXX_OP/* incubator-mxnet/src/operator/cd incubator-mxnetmake -j USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1cd ..makebash scripts/train_alternate.shmakecaius@sugon:~$ echo -n â€œcvlab1205â€ |md5sumd0599e86d6134fee87bcf017ddca1990 ç„¶åæˆ‘ä»¬ä½¿ç”¨docker psæŸ¥çœ‹åˆ°è¯¥å®¹å™¨ä¿¡æ¯ï¼Œæ¥ä¸‹æ¥å°±ä½¿ç”¨docker attachè¿›å…¥è¯¥å®¹å™¨å¯å½¢å˜å·ç§¯ IndexError: list index out of range self.class_id = [0, 1] imdb = eval(dataset)(image_set, root_path, dataset_path) [â€˜trainâ€™]imagesetIcdar2015/media/data1/caius/dataset[â€˜trainâ€™]imagesetIcdar2015/media/data1/caius/dataset icdar2015_train gt roidb loaded from model/res50-fpn/icdar2015/alternate/cache/icdar2015_train_gt_roidb.pklOpenCV Error: Assertion failed (func != 0) in resize, file /io/opencv/modules/imgproc/src/imgwarp.cpp, line 3370Traceback (most recent call last):File â€œtrain_alternate_mask_fpn.pyâ€, line 118, inmain()File â€œtrain_alternate_mask_fpn.pyâ€, line 115, in mainargs.rcnn_epoch, args.rcnn_lr, args.rcnn_lr_step)File â€œtrain_alternate_mask_fpn.pyâ€, line 61, in alternate_trainvis=False, shuffle=False, thresh=0)File â€œ/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/tools/test_rpn.pyâ€, line 63, in test_rpnimdb_boxes = generate_proposals(predictor, test_data, imdb, vis=vis, thresh=thresh)File â€œ/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/core/tester.pyâ€, line 61, in generate_proposalsfor im_info, data_batch in test_data:File â€œ/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/core/loader.pyâ€, line 60, in nextself.get_batch()File â€œ/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/core/loader.pyâ€, line 83, in get_batchdata, label, im_info = get_rpn_testbatch(roidb)File â€œ/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/io/rpn.pyâ€, line 32, in get_rpn_testbatchimgs, roidb,masks = get_image(roidb)File â€œ/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/io/image.pyâ€, line 99, in get_imagemask, _ = resize(mask, target_size, max_size)File â€œ/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/io/image.pyâ€, line 138, in resizeim = cv2.resize(im, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR)cv2.error: /io/opencv/modules/imgproc/src/imgwarp.cpp:3370: error: (-215) func != 0 in function resize root@d59236d7a683:/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss#","tags":["Python"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"å˜æ€è·³å°é˜¶","path":"/2019/09/06/å˜æ€è·³å°é˜¶/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š396047æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é€’å½’ é¢˜ç›®æè¿°ä¸€åªé’è›™ä¸€æ¬¡å¯ä»¥è·³ä¸Š1çº§å°é˜¶ï¼Œä¹Ÿå¯ä»¥è·³ä¸Š2çº§â€¦â€¦å®ƒä¹Ÿå¯ä»¥è·³ä¸Šnçº§ã€‚æ±‚è¯¥é’è›™è·³ä¸Šä¸€ä¸ªnçº§çš„å°é˜¶æ€»å…±æœ‰å¤šå°‘ç§è·³æ³•ã€‚ è§£æf(n) = f(n-1)+f(n-2)+â€¦+f(1)f(n-1) = f(n-2)+f(n-3)+â€¦f(1) f(n) = 2*f(n-1) 1 2 3 4 5 6 7 8 9 | class Solution { public: int jumpFloorII(int number) { if(number == 1) return 1; else return 2*jumpFloorII(number-1); } }; â€”|â€” è¿è¡Œæ—¶é—´ï¼š4mså ç”¨å†…å­˜ï¼š480k","tags":["ç¼–ç¨‹"],"categories":["ç®—æ³•"]},{"title":"Terminal æ‰‹å†Œ","path":"/2019/09/05/20190905-cmdæ‰‹å†Œ/","content":"æŸ¥çœ‹ç£ç›˜ç©ºé—´df -hl æŸ¥çœ‹ç£ç›˜å‰©ä½™ç©ºé—´df -h æŸ¥çœ‹æ¯ä¸ªæ ¹è·¯å¾„çš„åˆ†åŒºå¤§å° æŸ¥çœ‹æ–‡ä»¶/æ–‡ä»¶å¤¹å¤§å°æŸ¥çœ‹æŒ‡å®šæ–‡ä»¶/æ–‡ä»¶å¤¹å¤§å°ï¼šdu -hs æ–‡ä»¶åæˆ–æ–‡ä»¶å¤¹åæŸ¥çœ‹å½“å‰æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡ä»¶å¤§å°ï¼ˆåŒ…æ‹¬å­æ–‡ä»¶å¤¹ï¼‰ï¼šdu -sh æŸ¥çœ‹æ–‡ä»¶æ•°é‡ç»Ÿè®¡å½“å‰ç›®å½•ä¸‹æ–‡ä»¶çš„ä¸ªæ•°ï¼ˆä¸åŒ…æ‹¬ç›®å½•ï¼‰ls -l | grep â€œ^-â€œ | wc -l ls å‘½ä»¤ls -a æ˜¾ç¤ºå…¨éƒ¨çš„æ–‡ä»¶åŠæ–‡ä»¶å¤¹ï¼ŒåŒ…æ‹¬éšè—çš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹ls -l æ˜¾ç¤ºè¾ƒå…¨çš„æ–‡ä»¶ä¿¡æ¯ï¼ŒåŒ…æ‹¬æƒé™ã€ç”¨æˆ·ã€ç”¨æˆ·ç»„ç­‰ã€‚ls â€”color æ˜¾ç¤ºæ–‡ä»¶åŠæ–‡ä»¶å¤¹ï¼Œå¹¶æ ‡æœ‰ä¸åŒçš„é¢œè‰²ã€‚ tabé”®tab command ç”¨äºå½“ä½ çš„å‘½ä»¤è®°ä¸å…¨æ—¶ï¼Œè¾“å…¥ä¸€éƒ¨å†æŒ‰ä¸€ä¸‹è¿›è¡Œè¡¥å…¨ï¼Œå¦‚æœæœ‰å¤šä¸ªå‰é¢éƒ¨åˆ†ç›¸åŒå‘½ä»¤ï¼Œåˆ™æŒ‰ä¸¤æ¬¡tabé”® cmpcmp /bin/ls /bin/dir ç”¨äºæ¯”è¾ƒä¸¤ä¸ªæ–‡ä»¶æ˜¯å¦æ˜¯å®Œå…¨ç›¸åŒçš„ã€‚ cpcp /bin/ls /bin/a ç”¨äºå¤åˆ¶æ–‡ä»¶çš„å‘½ä»¤ã€‚è¿™æ—¶å°±å¤åˆ¶äº†ä¸€ä¸ªå‘½ä»¤æ–‡ä»¶ï¼Œå°±å¯ä»¥è¿è¡Œaå‘½ä»¤ï¼Œä¸lsç”¨æ³•ç›¸åŒcpå‘½ä»¤ä¸æ“ä½œæ–‡ä»¶ä¸€æ ·æ˜¯ç”¨æ¥å¤åˆ¶çš„ï¼Œå¸¦rè¡¨ç¤ºå°†å…¶å­ç›®å½•ä¸€èµ·å¤åˆ¶ã€‚ mvæ ¼å¼ï¼šmv /home/user1/æ¡Œé¢/ruijie/xrgsu /usr/share/local/bin/xrgsu chmodç”¨äºæ”¹ä¸ºç”¨æˆ·å¯¹äºæ–‡ä»¶çš„æ“ä½œæƒé™ã€‚chmod 0+r æ·»åŠ è¯»çš„æƒé™ã€‚sudo chmod 0-r filenameå–æ¶ˆè¯»çš„æƒé™ã€‚ cat cat å‘½ä»¤å°±æ˜¯ç”¨äºæŸ¥çœ‹ubuntuä¸­æ–‡æœ¬æ–‡ä»¶çš„å†…å®¹çš„å‘½ä»¤ã€‚ cat /proc/cpuinfo ç”¨äºæŸ¥çœ‹è®¡ç®—æœºçš„cpuä¿¡æ¯ã€‚ cat /proc/meminfo ç”¨äºæŸ¥çœ‹è®¡ç®—æœºçš„å†…åœ¨ä¿¡æ¯ã€‚ cat /etc/issue æŸ¥çœ‹ubuntuçš„ç‰ˆæœ¬ä¿¡æ¯ã€‚ æŸ¥çœ‹cuda ç‰ˆæœ¬ cat /usr/local/cuda/version.txt æŸ¥çœ‹cudnn ç‰ˆæœ¬ cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 touchæ ¼å¼ï¼štouch test1 test2 test3touchå‘½ä»¤ç”¨äºåˆ›å»ºæ–‡ä»¶ï¼Œå¯ä»¥åŒä¸€æ—¶é—´åˆ›å»ºå¤šä¸ªæ–‡ä»¶ã€‚ pwdpwdå‘½ä»¤æ˜¯ç”¨æ¥æŒ‡å‡ºå½“å‰æ‰€åœ¨çš„è·¯å¾„ã€‚æ˜¯print working directoryçš„ç¼©å†™","tags":["å·¥å…·"],"categories":["å·¥å…·"]},{"title":"è·³å°é˜¶","path":"/2019/09/05/20190905-è·³å°é˜¶/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š489122æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é€’å½’ é¢˜ç›®æè¿°ä¸€åªé’è›™ä¸€æ¬¡å¯ä»¥è·³ä¸Š1çº§å°é˜¶ï¼Œä¹Ÿå¯ä»¥è·³ä¸Š2çº§ã€‚æ±‚è¯¥é’è›™è·³ä¸Šä¸€ä¸ªnçº§çš„å°é˜¶æ€»å…±æœ‰å¤šå°‘ç§è·³æ³•ï¼ˆå…ˆåæ¬¡åºä¸åŒç®—ä¸åŒçš„ç»“æœï¼‰ã€‚ class Solution {public: int jumpFloor(int number) { if(number == 1) return 1; else if(number==2) return 2; else return jumpFloor(number-1)+jumpFloor(number-2); }}; è¿è¡Œæ—¶é—´ï¼š519 mså ç”¨å†…å­˜ï¼š456K","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"Terminal æ‰‹å†Œ","path":"/2019/09/05/cmdæ‰‹å†Œ/","content":"æŸ¥çœ‹ç£ç›˜ç©ºé—´df -hl æŸ¥çœ‹ç£ç›˜å‰©ä½™ç©ºé—´df -h æŸ¥çœ‹æ¯ä¸ªæ ¹è·¯å¾„çš„åˆ†åŒºå¤§å° æŸ¥çœ‹æ–‡ä»¶/æ–‡ä»¶å¤¹å¤§å°æŸ¥çœ‹æŒ‡å®šæ–‡ä»¶/æ–‡ä»¶å¤¹å¤§å°ï¼šdu -hs æ–‡ä»¶åæˆ–æ–‡ä»¶å¤¹åæŸ¥çœ‹å½“å‰æ–‡ä»¶å¤¹ä¸‹æ‰€æœ‰æ–‡ä»¶å¤§å°ï¼ˆåŒ…æ‹¬å­æ–‡ä»¶å¤¹ï¼‰ï¼šdu -sh æŸ¥çœ‹æ–‡ä»¶æ•°é‡ç»Ÿè®¡å½“å‰ç›®å½•ä¸‹æ–‡ä»¶çš„ä¸ªæ•°ï¼ˆä¸åŒ…æ‹¬ç›®å½•ï¼‰ls -l | grep â€œ^-â€œ | wc -l ls å‘½ä»¤ls -a æ˜¾ç¤ºå…¨éƒ¨çš„æ–‡ä»¶åŠæ–‡ä»¶å¤¹ï¼ŒåŒ…æ‹¬éšè—çš„æ–‡ä»¶æˆ–æ–‡ä»¶å¤¹ls -l æ˜¾ç¤ºè¾ƒå…¨çš„æ–‡ä»¶ä¿¡æ¯ï¼ŒåŒ…æ‹¬æƒé™ã€ç”¨æˆ·ã€ç”¨æˆ·ç»„ç­‰ã€‚ls â€”color æ˜¾ç¤ºæ–‡ä»¶åŠæ–‡ä»¶å¤¹ï¼Œå¹¶æ ‡æœ‰ä¸åŒçš„é¢œè‰²ã€‚ tabé”®tab command ç”¨äºå½“ä½ çš„å‘½ä»¤è®°ä¸å…¨æ—¶ï¼Œè¾“å…¥ä¸€éƒ¨å†æŒ‰ä¸€ä¸‹è¿›è¡Œè¡¥å…¨ï¼Œå¦‚æœæœ‰å¤šä¸ªå‰é¢éƒ¨åˆ†ç›¸åŒå‘½ä»¤ï¼Œåˆ™æŒ‰ä¸¤æ¬¡tabé”® cmpcmp /bin/ls /bin/dir ç”¨äºæ¯”è¾ƒä¸¤ä¸ªæ–‡ä»¶æ˜¯å¦æ˜¯å®Œå…¨ç›¸åŒçš„ã€‚ cpcp /bin/ls /bin/a ç”¨äºå¤åˆ¶æ–‡ä»¶çš„å‘½ä»¤ã€‚è¿™æ—¶å°±å¤åˆ¶äº†ä¸€ä¸ªå‘½ä»¤æ–‡ä»¶ï¼Œå°±å¯ä»¥è¿è¡Œaå‘½ä»¤ï¼Œä¸lsç”¨æ³•ç›¸åŒcpå‘½ä»¤ä¸æ“ä½œæ–‡ä»¶ä¸€æ ·æ˜¯ç”¨æ¥å¤åˆ¶çš„ï¼Œå¸¦rè¡¨ç¤ºå°†å…¶å­ç›®å½•ä¸€èµ·å¤åˆ¶ã€‚ mvæ ¼å¼ï¼šmv /home/user1/æ¡Œé¢/ruijie/xrgsu /usr/share/local/bin/xrgsu chmodç”¨äºæ”¹ä¸ºç”¨æˆ·å¯¹äºæ–‡ä»¶çš„æ“ä½œæƒé™ã€‚chmod 0+r æ·»åŠ è¯»çš„æƒé™ã€‚sudo chmod 0-r filenameå–æ¶ˆè¯»çš„æƒé™ã€‚ cat cat å‘½ä»¤å°±æ˜¯ç”¨äºæŸ¥çœ‹ubuntuä¸­æ–‡æœ¬æ–‡ä»¶çš„å†…å®¹çš„å‘½ä»¤ã€‚ cat /proc/cpuinfo ç”¨äºæŸ¥çœ‹è®¡ç®—æœºçš„cpuä¿¡æ¯ã€‚ cat /proc/meminfo ç”¨äºæŸ¥çœ‹è®¡ç®—æœºçš„å†…åœ¨ä¿¡æ¯ã€‚ cat /etc/issue æŸ¥çœ‹ubuntuçš„ç‰ˆæœ¬ä¿¡æ¯ã€‚ æŸ¥çœ‹cuda ç‰ˆæœ¬ cat /usr/local/cuda/version.txt æŸ¥çœ‹cudnn ç‰ˆæœ¬ cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 touchæ ¼å¼ï¼štouch test1 test2 test3touchå‘½ä»¤ç”¨äºåˆ›å»ºæ–‡ä»¶ï¼Œå¯ä»¥åŒä¸€æ—¶é—´åˆ›å»ºå¤šä¸ªæ–‡ä»¶ã€‚ pwdpwdå‘½ä»¤æ˜¯ç”¨æ¥æŒ‡å‡ºå½“å‰æ‰€åœ¨çš„è·¯å¾„ã€‚æ˜¯print working directoryçš„ç¼©å†™","tags":["ç¼–ç¨‹"],"categories":["å…¶ä»–"]},{"title":"è·³å°é˜¶","path":"/2019/09/05/è·³å°é˜¶/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š489122æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é€’å½’ é¢˜ç›®æè¿°ä¸€åªé’è›™ä¸€æ¬¡å¯ä»¥è·³ä¸Š1çº§å°é˜¶ï¼Œä¹Ÿå¯ä»¥è·³ä¸Š2çº§ã€‚æ±‚è¯¥é’è›™è·³ä¸Šä¸€ä¸ªnçº§çš„å°é˜¶æ€»å…±æœ‰å¤šå°‘ç§è·³æ³•ï¼ˆå…ˆåæ¬¡åºä¸åŒç®—ä¸åŒçš„ç»“æœï¼‰ã€‚ 1 2 3 4 5 6 7 8 9 10 11 | class Solution { public: int jumpFloor(int number) { if(number == 1) return 1; else if(number==2) return 2; else return jumpFloor(number-1)+jumpFloor(number-2); } }; â€”|â€” è¿è¡Œæ—¶é—´ï¼š519 mså ç”¨å†…å­˜ï¼š456K","tags":["ç¼–ç¨‹"],"categories":["ç®—æ³•"]},{"title":"æ–æ³¢é‚£å¥‘æ•°åˆ—","path":"/2019/09/04/20190904-æ–æ³¢é‚£å¥‘æ•°åˆ—/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š640784æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é€’å½’ é¢˜ç›®æè¿°å¤§å®¶éƒ½çŸ¥é“æ–æ³¢é‚£å¥‘æ•°åˆ—ï¼Œç°åœ¨è¦æ±‚è¾“å…¥ä¸€ä¸ªæ•´æ•°nï¼Œè¯·ä½ è¾“å‡ºæ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹ï¼ˆä»0å¼€å§‹ï¼Œç¬¬0é¡¹ä¸º0ï¼‰ã€‚n=39 class Solution {public: int Fibonacci(int n) { if(n = 0) return 0; if(n==1 || n==2) return 1; return Fibonacci(n-1)+Fibonacci(n-2); }}; è¿è¡Œæ—¶é—´ï¼š694 mså ç”¨å†…å­˜ï¼š484K","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"é‡‡å‘è®°å½•","path":"/2019/09/04/20190904-é‡‡å‘è®°å½•/","content":"os.getcwd()è¯¥å‡½æ•°ä¸éœ€è¦ä¼ é€’å‚æ•°ï¼Œå®ƒè¿”å›å½“å‰çš„ç›®å½•ã€‚éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œå½“å‰ç›®å½•å¹¶ä¸æ˜¯æŒ‡è„šæœ¬æ‰€åœ¨çš„ç›®å½•ï¼Œè€Œæ˜¯æ‰€è¿è¡Œè„šæœ¬çš„ç›®å½•ã€‚å°±æ˜¯è¯´å¦‚æœä½ åœ¨homeçš„ç»ˆç«¯è¿è¡Œ os.listdir(path)å…¶å‚æ•°path ä¸ºè¦è·å¾—å†…å®¹ç›®å½•çš„è·¯å¾„","tags":["å…¶ä»–"],"categories":["å…¶ä»–"]},{"title":"æ–æ³¢é‚£å¥‘æ•°åˆ—","path":"/2019/09/04/æ–æ³¢é‚£å¥‘æ•°åˆ—/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š640784æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é€’å½’ é¢˜ç›®æè¿°å¤§å®¶éƒ½çŸ¥é“æ–æ³¢é‚£å¥‘æ•°åˆ—ï¼Œç°åœ¨è¦æ±‚è¾“å…¥ä¸€ä¸ªæ•´æ•°nï¼Œè¯·ä½ è¾“å‡ºæ–æ³¢é‚£å¥‘æ•°åˆ—çš„ç¬¬né¡¹ï¼ˆä»0å¼€å§‹ï¼Œç¬¬0é¡¹ä¸º0ï¼‰ã€‚n=39 1 2 3 4 5 6 7 8 | class Solution { public: int Fibonacci(int n) { if(n = 0) return 0; if(n==1 || n==2) return 1; return Fibonacci(n-1)+Fibonacci(n-2); } }; â€”|â€” è¿è¡Œæ—¶é—´ï¼š694 mså ç”¨å†…å­˜ï¼š484K","tags":["ç¼–ç¨‹"],"categories":["ç®—æ³•"]},{"title":"é‡‡å‘è®°å½•","path":"/2019/09/04/é‡‡å‘è®°å½•/","content":"os.getcwd()è¯¥å‡½æ•°ä¸éœ€è¦ä¼ é€’å‚æ•°ï¼Œå®ƒè¿”å›å½“å‰çš„ç›®å½•ã€‚éœ€è¦è¯´æ˜çš„æ˜¯ï¼Œå½“å‰ç›®å½•å¹¶ä¸æ˜¯æŒ‡è„šæœ¬æ‰€åœ¨çš„ç›®å½•ï¼Œè€Œæ˜¯æ‰€è¿è¡Œè„šæœ¬çš„ç›®å½•ã€‚å°±æ˜¯è¯´å¦‚æœä½ åœ¨homeçš„ç»ˆç«¯è¿è¡Œ os.listdir(path)å…¶å‚æ•°path ä¸ºè¦è·å¾—å†…å®¹ç›®å½•çš„è·¯å¾„","tags":["ç¼–ç¨‹"],"categories":["å…¶ä»–"]},{"title":"éå†æ–‡ä»¶å¤¹å›¾ç‰‡","path":"/2019/09/03/20190903-éå†æ–‡ä»¶å¤¹å›¾ç‰‡/","content":"é¢˜ç›®è¦æ±‚è¦æ±‚ä»ä¸‹å›¾æ‰€ç¤ºçš„æ–‡ä»¶å¤¹éå†å­æ–‡ä»¶å¤¹ï¼Œå¹¶å°†æ‰€åœ¨çš„å›¾ç‰‡è·¯å¾„ä¿å­˜åˆ°txtæ–‡ä»¶ä¸­ ä»£ç å¦‚ä¸‹import os, globimport randomroot = '/media/data1/lesson/pokemon/pokeman'f_w = open(os.path.join('../', 'train2.txt'), 'w', encoding='utf8')images = []for name in sorted(os.listdir(os.path.join((root)))): # è·å–å­æ–‡ä»¶å¤¹ if not os.path.isdir(os.path.join(root,name)): continue images += glob.glob(os.path.join(root, name, '*.png')) images += glob.glob(os.path.join(root, name, '*.jpg')) images += glob.glob(os.path.join(root, name, '*.jpeg'))random.shuffle(images)for line in images: f_w.write(line + ' ') # f_w.write(line)f_w.close()print('Write Done!') # å’–å–±çš„ä»£ç import osimport globimport os.path as osp dataset_dir = os.getcwd() bulbasaur_dir = osp.join(dataset_dir, 'pokeman/bulbasaur') charmander_dir = osp.join(dataset_dir, 'pokeman/charmander') mewtwo_dir = osp.join(dataset_dir, 'pokeman/mewtwo') pikachu_dir = osp.join(dataset_dir, 'pokeman/pikachu') squirtle_dir = osp.join(dataset_dir, 'pokeman/squirtle')def process(dir_path, save_path): img_paths = glob.glob(osp.join(dir_path, '*.jpg')) f= open(save_path, 'w') for img_path in img_paths: f.write(img_path) f.write(' ') f.close()dataset_dir = os.getcwd()bulbasaur_dir = osp.join(dataset_dir, 'pokeman/bulbasaur')charmander_dir = osp.join(dataset_dir, 'pokeman/charmander')mewtwo_dir = osp.join(dataset_dir, 'pokeman/mewtwo')pikachu_dir = osp.join(dataset_dir, 'pokeman/pikachu')squirtle_dir = osp.join(dataset_dir, 'pokeman/squirtle')save_dir = osp.join(dataset_dir, 'imgpath.txt')process(test_dir, save_dir)","tags":["æŠ€æœ¯"],"categories":["æŠ€æœ¯"]},{"title":"éå†æ–‡ä»¶å¤¹å›¾ç‰‡","path":"/2019/09/03/éå†æ–‡ä»¶å¤¹å›¾ç‰‡/","content":"é¢˜ç›®è¦æ±‚è¦æ±‚ä»ä¸‹å›¾æ‰€ç¤ºçš„æ–‡ä»¶å¤¹éå†å­æ–‡ä»¶å¤¹ï¼Œå¹¶å°†æ‰€åœ¨çš„å›¾ç‰‡è·¯å¾„ä¿å­˜åˆ°txtæ–‡ä»¶ä¸­ ä»£ç å¦‚ä¸‹1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | import os, glob import random root = '/media/data1/lesson/pokemon/pokeman' f_w = open(os.path.join('../', 'train2.txt'), 'w', encoding='utf8') images = [] for name in sorted(os.listdir(os.path.join((root)))): # è·å–å­æ–‡ä»¶å¤¹ if not os.path.isdir(os.path.join(root,name)): continue images += glob.glob(os.path.join(root, name, '*.png')) images += glob.glob(os.path.join(root, name, '*.jpg')) images += glob.glob(os.path.join(root, name, '*.jpeg')) random.shuffle(images) for line in images: f_w.write(line + ' ') # f_w.write(line) f_w.close() print('Write Done!') â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 | # å’–å–±çš„ä»£ç  import os import glob import os.path as osp dataset_dir = os.getcwd() bulbasaur_dir = osp.join(dataset_dir, 'pokeman/bulbasaur') charmander_dir = osp.join(dataset_dir, 'pokeman/charmander') mewtwo_dir = osp.join(dataset_dir, 'pokeman/mewtwo') pikachu_dir = osp.join(dataset_dir, 'pokeman/pikachu') squirtle_dir = osp.join(dataset_dir, 'pokeman/squirtle') def process(dir_path, save_path): img_paths = glob.glob(osp.join(dir_path, '*.jpg')) f= open(save_path, 'w') for img_path in img_paths: f.write(img_path) f.write(' ') f.close() dataset_dir = os.getcwd() bulbasaur_dir = osp.join(dataset_dir, 'pokeman/bulbasaur') charmander_dir = osp.join(dataset_dir, 'pokeman/charmander') mewtwo_dir = osp.join(dataset_dir, 'pokeman/mewtwo') pikachu_dir = osp.join(dataset_dir, 'pokeman/pikachu') squirtle_dir = osp.join(dataset_dir, 'pokeman/squirtle') save_dir = osp.join(dataset_dir, 'imgpath.txt') process(test_dir, save_dir) â€”|â€”","tags":["ç¼–ç¨‹"],"categories":["å…¶ä»–"]},{"title":"ç”¨ä¸¤ä¸ªæ ˆå®ç°é˜Ÿåˆ—","path":"/2019/09/02/20190902-ç”¨ä¸¤ä¸ªæ ˆå®ç°é˜Ÿåˆ—/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š499320æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é˜Ÿåˆ— æ ˆ é¢˜ç›®æè¿°ç”¨ä¸¤ä¸ªæ ˆæ¥å®ç°ä¸€ä¸ªé˜Ÿåˆ—ï¼Œå®Œæˆé˜Ÿåˆ—çš„Pushå’ŒPopæ“ä½œã€‚ é˜Ÿåˆ—ä¸­çš„å…ƒç´ ä¸ºintç±»å‹ã€‚ class Solution{public: void push(int node) { while(!stack2.empty())//å…¥é˜Ÿæ—¶è¦ä¿è¯ Stack2ä¸ºç©º { stack1.push(stack2.top()); stack2.pop(); } stack1.push(node); } int pop() { while(!stack1.empty())//å…¥é˜Ÿæ—¶è¦ä¿è¯ Stack1ä¸ºç©º { stack2.push(stack1.top()); stack1.pop(); } int temp = stack2.top(); stack2.pop(); return temp; }private: stackint stack1; stackint stack2;}; è¿è¡Œæ—¶é—´ï¼š3mså ç”¨å†…å­˜ï¼š460K","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"ç”¨ä¸¤ä¸ªæ ˆå®ç°é˜Ÿåˆ—","path":"/2019/09/02/ç”¨ä¸¤ä¸ªæ ˆå®ç°é˜Ÿåˆ—/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š499320æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é˜Ÿåˆ— æ ˆ é¢˜ç›®æè¿°ç”¨ä¸¤ä¸ªæ ˆæ¥å®ç°ä¸€ä¸ªé˜Ÿåˆ—ï¼Œå®Œæˆé˜Ÿåˆ—çš„Pushå’ŒPopæ“ä½œã€‚ é˜Ÿåˆ—ä¸­çš„å…ƒç´ ä¸ºintç±»å‹ã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 | class Solution { public: void push(int node) { while(!stack2.empty())//å…¥é˜Ÿæ—¶è¦ä¿è¯ Stack2ä¸ºç©º { stack1.push(stack2.top()); stack2.pop(); } stack1.push(node); } int pop() { while(!stack1.empty())//å…¥é˜Ÿæ—¶è¦ä¿è¯ Stack1ä¸ºç©º { stack2.push(stack1.top()); stack1.pop(); } int temp = stack2.top(); stack2.pop(); return temp; } private: stackint stack1; stackint stack2; }; â€”|â€” è¿è¡Œæ—¶é—´ï¼š3mså ç”¨å†…å­˜ï¼š460K","tags":["ç¼–ç¨‹"],"categories":["C++"]},{"title":"ä»å°¾åˆ°å¤´æ‰“å°é“¾è¡¨","path":"/2019/08/29/20190829-ä»å°¾åˆ°å¤´æ‰“å°é“¾è¡¨/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š1022710æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é“¾è¡¨ é¢˜ç›®æè¿°è¾“å…¥ä¸€ä¸ªé“¾è¡¨ï¼ŒæŒ‰é“¾è¡¨ä»å°¾åˆ°å¤´çš„é¡ºåºè¿”å›ä¸€ä¸ªArrayListã€‚ /*** struct ListNode {* int val;* struct ListNode *next;* ListNode(int x) :* val(x), next(NULL) {* }* };*/class Solution {public: vectorint printListFromTailToHead(ListNode* head) { vectorint value; ListNode *p=NULL; p=head; stackint stk; while(p!=NULL){ stk.push(p-val); p=p-next; } while(!stk.empty()){ value.push_back(stk.top()); stk.pop(); } return value; }}; è¿è¡Œæ—¶é—´ï¼š3mså ç”¨å†…å­˜ï¼š480K","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"æ›¿æ¢ç©ºæ ¼","path":"/2019/08/29/20190829-æ›¿æ¢ç©ºæ ¼/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š1142927æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š å­—ç¬¦ä¸² é¢˜ç›®æè¿°è¯·å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œå°†ä¸€ä¸ªå­—ç¬¦ä¸²ä¸­çš„æ¯ä¸ªç©ºæ ¼æ›¿æ¢æˆâ€œ%20â€ã€‚ä¾‹å¦‚ï¼Œå½“å­—ç¬¦ä¸²ä¸ºWe Are Happy.åˆ™ç»è¿‡æ›¿æ¢ä¹‹åçš„å­—ç¬¦ä¸²ä¸ºWe%20Are%20Happyã€‚ ä»£ç class Solution {public:\tvoid replaceSpace(char *str,int length) { if(str==NULL) return ; int CountOfBlanks=0; int Originallength=0; for(int i=0;str[i]!='\\0';i++) { Originallength++; if(str[i]==' ') ++CountOfBlanks; } int len =Originallength+2*CountOfBlanks; if(len+1length) return ; /* int pOrignallength=orignallength; int pNewlength=newlength; while(pOrignallength=0 pNewlengthpOrignallength) { if(str[pOrignallength]==' ') { str[pNewlength--]='0'; str[pNewlength--]='2'; str[pNewlength--]='%'; } else { str[pNewlength--]=str[pOrignallength]; } pOrignallength--; } */ char*pStr1=str+Originallength;//å¤åˆ¶ç»“æŸç¬¦â€˜\\0â€™ char*pStr2=str+len; while(pStr1pStr2) { if(*pStr1==' ') { *pStr2--='0'; *pStr2--='2'; *pStr2--='%'; } else { *pStr2--=*pStr1; } --pStr1; } }};","tags":["æŠ€æœ¯"],"categories":["æŠ€æœ¯"]},{"title":"ä»å°¾åˆ°å¤´æ‰“å°é“¾è¡¨","path":"/2019/08/29/ä»å°¾åˆ°å¤´æ‰“å°é“¾è¡¨/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š1022710æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é“¾è¡¨ é¢˜ç›®æè¿°è¾“å…¥ä¸€ä¸ªé“¾è¡¨ï¼ŒæŒ‰é“¾è¡¨ä»å°¾åˆ°å¤´çš„é¡ºåºè¿”å›ä¸€ä¸ªArrayListã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 | /** * struct ListNode { * int val; * struct ListNode *next; * ListNode(int x) : * val(x), next(NULL) { * } * }; */ class Solution { public: vectorint printListFromTailToHead(ListNode* head) { vectorint value; ListNode *p=NULL; p=head; stackint stk; while(p!=NULL){ stk.push(p-val); p=p-next; } while(!stk.empty()){ value.push_back(stk.top()); stk.pop(); } return value; } }; â€”|â€” è¿è¡Œæ—¶é—´ï¼š3mså ç”¨å†…å­˜ï¼š480K","tags":["ç¼–ç¨‹"],"categories":["ç®—æ³•"]},{"title":"æ›¿æ¢ç©ºæ ¼","path":"/2019/08/29/æ›¿æ¢ç©ºæ ¼/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š1142927æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š å­—ç¬¦ä¸² é¢˜ç›®æè¿°è¯·å®ç°ä¸€ä¸ªå‡½æ•°ï¼Œå°†ä¸€ä¸ªå­—ç¬¦ä¸²ä¸­çš„æ¯ä¸ªç©ºæ ¼æ›¿æ¢æˆâ€œ%20â€ã€‚ä¾‹å¦‚ï¼Œå½“å­—ç¬¦ä¸²ä¸ºWe Are Happy.åˆ™ç»è¿‡æ›¿æ¢ä¹‹åçš„å­—ç¬¦ä¸²ä¸ºWe%20Are%20Happyã€‚ ä»£ç 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 | class Solution { public: void replaceSpace(char *str,int length) { if(str==NULL) return ; int CountOfBlanks=0; int Originallength=0; for(int i=0;str[i]!='\\0';i++) { Originallength++; if(str[i]==' ') ++CountOfBlanks; } int len =Originallength+2*CountOfBlanks; if(len+1length) return ; /* int pOrignallength=orignallength; int pNewlength=newlength; while(pOrignallength=0 pNewlengthpOrignallength) { if(str[pOrignallength]==' ') { str[pNewlength--]='0'; str[pNewlength--]='2'; str[pNewlength--]='%'; } else { str[pNewlength--]=str[pOrignallength]; } pOrignallength--; } */ char*pStr1=str+Originallength;//å¤åˆ¶ç»“æŸç¬¦â€˜\\0â€™ char*pStr2=str+len; while(pStr1pStr2) { if(*pStr1==' ') { *pStr2--='0'; *pStr2--='2'; *pStr2--='%'; } else { *pStr2--=*pStr1; } --pStr1; } } }; â€”|â€”","tags":["ç¼–ç¨‹"],"categories":["å…¶ä»–"]},{"title":"PILæ¨¡å—","path":"/2019/08/28/20190828-PILæ¨¡å—/","content":"å¯¼å…¥from PIL import Image è¯»å–img = Image.open(filepath) æ˜¾ç¤ºimg.show() ä¸ numpy æ•°ç»„çš„äº’ç›¸è½¬æ¢PIL Image è½¬ numpy æ•°ç»„ img_to_array = np.array(img) numpy æ•°ç»„è½¬ PIL Image (æ³¨æ„è¦ç¡®ä¿æ•°ç»„å†…çš„å€¼ç¬¦åˆ PIL çš„è¦æ±‚) array_to_img = Image.fromarray(img_to_array) PIL ä¸ cv2 æ ¼å¼äº’ç›¸è½¬æ¢PIL.Imageè¯»å…¥çš„å›¾ç‰‡æ•°æ®ç±»å‹ä¸æ˜¯ numpy æ•°ç»„, å®ƒçš„sizeå±æ€§ä¸º (w, h), åˆ©ç”¨np.arrayè½¬æ¢æˆ numpy æ•°ç»„å, å®ƒçš„é€šé“é¡ºåºä¸º (r, g, b) from PIL import Imageimport numpy as np# PIL to cv2pil_img = Image.open(img_path)print(pil_img.size) # (w, h)np_img = np.array(pil_img)cv2_img = np_img[:, :, ::-1] # äº¤æ¢é€šé“# cv2 to PILpil_img = Image.fromarray(cv2_img[:, :, ::-1])","tags":["è®¡ç®—æœºè§†è§‰"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"Pytorch trick","path":"/2019/08/28/20190828-Pytorch-trick/","content":"ç›®å½•ï¼š æŒ‡å®šGPUç¼–å· æŸ¥çœ‹æ¨¡å‹æ¯å±‚è¾“å‡ºè¯¦æƒ… æ¢¯åº¦è£å‰ª æ‰©å±•å•å¼ å›¾ç‰‡ç»´åº¦ ç‹¬çƒ­ç¼–ç  é˜²æ­¢éªŒè¯æ¨¡å‹æ—¶çˆ†æ˜¾å­˜ å­¦ä¹ ç‡è¡°å‡ å†»ç»“æŸäº›å±‚çš„å‚æ•° å¯¹ä¸åŒå±‚ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡ 1. æŒ‡å®šGPUç¼–å· è®¾ç½®å½“å‰ä½¿ç”¨çš„GPUè®¾å¤‡ä»…ä¸º0å·è®¾å¤‡ï¼Œè®¾å¤‡åç§°ä¸º /gpu:0ï¼šos.environ[â€œCUDA_VISIBLE_DEVICESâ€] = â€œ0â€ è®¾ç½®å½“å‰ä½¿ç”¨çš„GPUè®¾å¤‡ä¸º0,1å·ä¸¤ä¸ªè®¾å¤‡ï¼Œåç§°ä¾æ¬¡ä¸º /gpu:0ã€/gpu:1ï¼š os.environ[â€œCUDA_VISIBLE_DEVICESâ€] = â€œ0,1â€ ï¼Œæ ¹æ®é¡ºåºè¡¨ç¤ºä¼˜å…ˆä½¿ç”¨0å·è®¾å¤‡,ç„¶åä½¿ç”¨1å·è®¾å¤‡ã€‚æŒ‡å®šGPUçš„å‘½ä»¤éœ€è¦æ”¾åœ¨å’Œç¥ç»ç½‘ç»œç›¸å…³çš„ä¸€ç³»åˆ—æ“ä½œçš„å‰é¢ã€‚ 2.æŸ¥çœ‹æ¨¡å‹æ¯å±‚è¾“å‡ºè¯¦æƒ…Kerasæœ‰ä¸€ä¸ªç®€æ´çš„APIæ¥æŸ¥çœ‹æ¨¡å‹çš„æ¯ä¸€å±‚è¾“å‡ºå°ºå¯¸ï¼Œè¿™åœ¨è°ƒè¯•ç½‘ç»œæ—¶éå¸¸æœ‰ç”¨ã€‚ç°åœ¨åœ¨PyTorchä¸­ä¹Ÿå¯ä»¥å®ç°è¿™ä¸ªåŠŸèƒ½ã€‚ ä½¿ç”¨å¾ˆç®€å•ï¼Œå¦‚ä¸‹ç”¨æ³•ï¼š from torchsummary import summarysummary(your_model, input_size=(channels, H, W)) input_size æ˜¯æ ¹æ®ä½ è‡ªå·±çš„ç½‘ç»œæ¨¡å‹çš„è¾“å…¥å°ºå¯¸è¿›è¡Œè®¾ç½®ã€‚ 3.æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰import torch.nn as nnoutputs = model(data)loss= loss_fn(outputs, target)optimizer.zero_grad()loss.backward()nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)optimizer.step() nn.utils.clip_grad_norm_ çš„å‚æ•°ï¼š parameters â€“ ä¸€ä¸ªåŸºäºå˜é‡çš„è¿­ä»£å™¨ï¼Œä¼šè¿›è¡Œæ¢¯åº¦å½’ä¸€åŒ– max_norm â€“ æ¢¯åº¦çš„æœ€å¤§èŒƒæ•° max_norm â€“ æ¢¯åº¦çš„æœ€å¤§èŒƒæ•°çŸ¥ä¹ç”¨æˆ· ä¸æ¤­çš„æ¤­åœ† æå‡ºï¼šæ¢¯åº¦è£å‰ªåœ¨æŸäº›ä»»åŠ¡ä¸Šä¼šé¢å¤–æ¶ˆè€—å¤§é‡çš„è®¡ç®—æ—¶é—´ï¼Œå¯ç§»æ­¥è¯„è®ºåŒºæŸ¥çœ‹è¯¦æƒ…ã€‚ 4ã€æ‰©å±•å•å¼ å›¾ç‰‡ç»´åº¦å› ä¸ºåœ¨è®­ç»ƒæ—¶çš„æ•°æ®ç»´åº¦ä¸€èˆ¬éƒ½æ˜¯ (batch_size, c, h, w)ï¼Œè€Œåœ¨æµ‹è¯•æ—¶åªè¾“å…¥ä¸€å¼ å›¾ç‰‡ï¼Œæ‰€ä»¥éœ€è¦æ‰©å±•ç»´åº¦ï¼Œæ‰©å±•ç»´åº¦æœ‰å¤šä¸ªæ–¹æ³•ï¼š import cv2import torchimage = cv2.imread(img_path)image = torch.tensor(image)print(image.size())img = image.view(1, *image.size())print(img.size())# output:# torch.Size([h, w, c])# torch.Size([1, h, w, c]) æˆ–è€… import cv2import numpy as npimage = cv2.imread(img_path)print(image.shape)img = image[np.newaxis, :, :, :]print(img.shape)# output:# (h, w, c)# (1, h, w, c) æˆ–è€… import cv2import torchimage = cv2.imread(img_path)image = torch.tensor(image)print(image.size())img = image.unsqueeze(dim=0)print(img.size())img = img.squeeze(dim=0)print(img.size())# output:# torch.Size([(h, w, c)])# torch.Size([1, h, w, c])# torch.Size([h, w, c]) tensor.unsqueeze(dim)ï¼šæ‰©å±•ç»´åº¦ï¼ŒdimæŒ‡å®šæ‰©å±•å“ªä¸ªç»´åº¦ã€‚ tensor.squeeze(dim)ï¼šå»é™¤dimæŒ‡å®šçš„ä¸”sizeä¸º1çš„ç»´åº¦ï¼Œç»´åº¦å¤§äº1æ—¶ï¼Œsqueeze()ä¸èµ·ä½œç”¨ï¼Œä¸æŒ‡å®šdimæ—¶ï¼Œå»é™¤æ‰€æœ‰sizeä¸º1çš„ç»´åº¦ã€‚ 5.ç‹¬çƒ­ç¼–ç åœ¨PyTorchä¸­ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°çš„æ—¶å€™ä¼šè‡ªåŠ¨æŠŠlabelè½¬åŒ–æˆonehotï¼Œæ‰€ä»¥ä¸ç”¨æ‰‹åŠ¨è½¬åŒ–ï¼Œè€Œä½¿ç”¨MSEéœ€è¦æ‰‹åŠ¨è½¬åŒ–æˆonehotç¼–ç ã€‚ import torchclass_num = 8batch_size = 4def one_hot(label): \"\"\" å°†ä¸€ç»´åˆ—è¡¨è½¬æ¢ä¸ºç‹¬çƒ­ç¼–ç  \"\"\" label = label.resize_(batch_size, 1) m_zeros = torch.zeros(batch_size, class_num) # ä» value ä¸­å–å€¼ï¼Œç„¶åæ ¹æ® dim å’Œ index ç»™ç›¸åº”ä½ç½®èµ‹å€¼ onehot = m_zeros.scatter_(1, label, 1) # (dim,index,value) return onehot.numpy() # Tensor - Numpylabel = torch.LongTensor(batch_size).random_() % class_num # å¯¹éšæœºæ•°å–ä½™print(one_hot(label))# output:[[0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0.]] 6. é˜²æ­¢éªŒè¯æ¨¡å‹æ—¶çˆ†æ˜¾å­˜éªŒè¯æ¨¡å‹æ—¶ä¸éœ€è¦æ±‚å¯¼ï¼Œå³ä¸éœ€è¦æ¢¯åº¦è®¡ç®—ï¼Œå…³é—­autogradï¼Œå¯ä»¥æé«˜é€Ÿåº¦ï¼ŒèŠ‚çº¦å†…å­˜ã€‚å¦‚æœä¸å…³é—­å¯èƒ½ä¼šçˆ†æ˜¾å­˜ã€‚ with torch.no_grad(): # ä½¿ç”¨modelè¿›è¡Œé¢„æµ‹çš„ä»£ç  pass æ„Ÿè°¢çŸ¥ä¹ç”¨æˆ·zhaz çš„æé†’ï¼Œæˆ‘æŠŠ torch.cuda.empty_cache() çš„ä½¿ç”¨åŸå› æ›´æ–°ä¸€ä¸‹ã€‚ è¿™æ˜¯åŸå›ç­”ï¼š Pytorch è®­ç»ƒæ—¶æ— ç”¨çš„ä¸´æ—¶å˜é‡å¯èƒ½ä¼šè¶Šæ¥è¶Šå¤šï¼Œå¯¼è‡´ out of memory ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢è¯­å¥æ¥æ¸…ç†è¿™äº›ä¸éœ€è¦çš„å˜é‡ã€‚ å®˜ç½‘ ä¸Šçš„è§£é‡Šä¸ºï¼š Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.torch.cuda.empty_cache() æ„æ€å°±æ˜¯PyTorchçš„ç¼“å­˜åˆ†é…å™¨ä¼šäº‹å…ˆåˆ†é…ä¸€äº›å›ºå®šçš„æ˜¾å­˜ï¼Œå³ä½¿å®é™…ä¸Štensorså¹¶æ²¡æœ‰ä½¿ç”¨å®Œè¿™äº›æ˜¾å­˜ï¼Œè¿™äº›æ˜¾å­˜ä¹Ÿä¸èƒ½è¢«å…¶ä»–åº”ç”¨ä½¿ç”¨ã€‚è¿™ä¸ªåˆ†é…è¿‡ç¨‹ç”±ç¬¬ä¸€æ¬¡CUDAå†…å­˜è®¿é—®è§¦å‘çš„ã€‚ è€Œ torch.cuda.empty_cache() çš„ä½œç”¨å°±æ˜¯é‡Šæ”¾ç¼“å­˜åˆ†é…å™¨å½“å‰æŒæœ‰çš„ä¸”æœªå ç”¨çš„ç¼“å­˜æ˜¾å­˜ï¼Œä»¥ä¾¿è¿™äº›æ˜¾å­˜å¯ä»¥è¢«å…¶ä»–GPUåº”ç”¨ç¨‹åºä¸­ä½¿ç”¨ï¼Œå¹¶ä¸”é€šè¿‡ nvidia-smiå‘½ä»¤å¯è§ã€‚æ³¨æ„ä½¿ç”¨æ­¤å‘½ä»¤ä¸ä¼šé‡Šæ”¾tensorså ç”¨çš„æ˜¾å­˜ã€‚ å¯¹äºä¸ç”¨çš„æ•°æ®å˜é‡ï¼ŒPytorch å¯ä»¥è‡ªåŠ¨è¿›è¡Œå›æ”¶ä»è€Œé‡Šæ”¾ç›¸åº”çš„æ˜¾å­˜ã€‚ 7. å­¦ä¹ ç‡è¡°å‡import torch.optim as optimfrom torch.optim import lr_scheduler# è®­ç»ƒå‰çš„åˆå§‹åŒ–optimizer = optim.Adam(net.parameters(), lr=0.001)scheduler = lr_scheduler.StepLR(optimizer, 10, 0.1) # # æ¯è¿‡10ä¸ªepochï¼Œå­¦ä¹ ç‡ä¹˜ä»¥0.1# è®­ç»ƒè¿‡ç¨‹ä¸­for n in n_epoch: scheduler.step() ... 8. å†»ç»“æŸäº›å±‚çš„å‚æ•°åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„æ—¶å€™ï¼Œæˆ‘ä»¬æœ‰æ—¶æƒ³å†»ç»“å‰é¢å‡ å±‚ï¼Œä½¿å…¶å‚æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸å‘ç”Ÿå˜åŒ–ã€‚ æˆ‘ä»¬éœ€è¦å…ˆçŸ¥é“æ¯ä¸€å±‚çš„åå­—ï¼Œé€šè¿‡å¦‚ä¸‹ä»£ç æ‰“å°ï¼š net = Network() # è·å–è‡ªå®šä¹‰ç½‘ç»œç»“æ„for name, value in net.named_parameters(): print('name: {0}, grad: {1}'.format(name, value.requires_grad)) å‡è®¾å‰å‡ å±‚ä¿¡æ¯å¦‚ä¸‹ï¼š name: cnn.VGG_16.convolution1_1.weight, grad: Truename: cnn.VGG_16.convolution1_1.bias, grad: Truename: cnn.VGG_16.convolution1_2.weight, grad: Truename: cnn.VGG_16.convolution1_2.bias, grad: Truename: cnn.VGG_16.convolution2_1.weight, grad: Truename: cnn.VGG_16.convolution2_1.bias, grad: Truename: cnn.VGG_16.convolution2_2.weight, grad: Truename: cnn.VGG_16.convolution2_2.bias, grad: True åé¢çš„Trueè¡¨ç¤ºè¯¥å±‚çš„å‚æ•°å¯è®­ç»ƒï¼Œç„¶åæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªè¦å†»ç»“çš„å±‚çš„åˆ—è¡¨ï¼š no_grad = [ 'cnn.VGG_16.convolution1_1.weight', 'cnn.VGG_16.convolution1_1.bias', 'cnn.VGG_16.convolution1_2.weight', 'cnn.VGG_16.convolution1_2.bias'] å†»ç»“æ–¹æ³•å¦‚ä¸‹ï¼š net = Net.CTPN() # è·å–ç½‘ç»œç»“æ„for name, value in net.named_parameters(): if name in no_grad: value.requires_grad = False else: value.requires_grad = True å†»ç»“åæˆ‘ä»¬å†æ‰“å°æ¯å±‚çš„ä¿¡æ¯ï¼š name: cnn.VGG_16.convolution1_1.weight, grad: Falsename: cnn.VGG_16.convolution1_1.bias, grad: Falsename: cnn.VGG_16.convolution1_2.weight, grad: Falsename: cnn.VGG_16.convolution1_2.bias, grad: Falsename: cnn.VGG_16.convolution2_1.weight, grad: Truename: cnn.VGG_16.convolution2_1.bias, grad: Truename: cnn.VGG_16.convolution2_2.weight, grad: Truename: cnn.VGG_16.convolution2_2.bias, grad: True å¯ä»¥çœ‹åˆ°å‰ä¸¤å±‚çš„weightå’Œbiasçš„requires_gradéƒ½ä¸ºFalseï¼Œè¡¨ç¤ºå®ƒä»¬ä¸å¯è®­ç»ƒã€‚æœ€ååœ¨å®šä¹‰ä¼˜åŒ–å™¨æ—¶ï¼Œåªå¯¹requires_gradä¸ºTrueçš„å±‚çš„å‚æ•°è¿›è¡Œæ›´æ–°ã€‚ optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.01) 9. å¯¹ä¸åŒå±‚ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡æˆ‘ä»¬å¯¹æ¨¡å‹çš„ä¸åŒå±‚ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡ã€‚ è¿˜æ˜¯ä½¿ç”¨è¿™ä¸ªæ¨¡å‹ä½œä¸ºä¾‹å­ï¼š net = Network() # è·å–è‡ªå®šä¹‰ç½‘ç»œç»“æ„for name, value in net.named_parameters(): print('name: {}'.format(name))# è¾“å‡ºï¼š# name: cnn.VGG_16.convolution1_1.weight# name: cnn.VGG_16.convolution1_1.bias# name: cnn.VGG_16.convolution1_2.weight# name: cnn.VGG_16.convolution1_2.bias# name: cnn.VGG_16.convolution2_1.weight# name: cnn.VGG_16.convolution2_1.bias# name: cnn.VGG_16.convolution2_2.weight# name: cnn.VGG_16.convolution2_2.bias å¯¹ convolution1 å’Œ convolution2 è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡ï¼Œé¦–å…ˆå°†å®ƒä»¬åˆ†å¼€ï¼Œå³æ”¾åˆ°ä¸åŒçš„åˆ—è¡¨é‡Œï¼š conv1_params = []conv2_params = []for name, parms in net.named_parameters(): if \"convolution1\" in name: conv1_params += [parms] else: conv2_params += [parms]# ç„¶ååœ¨ä¼˜åŒ–å™¨ä¸­è¿›è¡Œå¦‚ä¸‹æ“ä½œï¼šoptimizer = optim.Adam( [ {\"params\": conv1_params, 'lr': 0.01}, {\"params\": conv2_params, 'lr': 0.001}, ], weight_decay=1e-3,) æˆ‘ä»¬å°†æ¨¡å‹åˆ’åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œå­˜æ”¾åˆ°ä¸€ä¸ªåˆ—è¡¨é‡Œï¼Œæ¯éƒ¨åˆ†å°±å¯¹åº”ä¸Šé¢çš„ä¸€ä¸ªå­—å…¸ï¼Œåœ¨å­—å…¸é‡Œè®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡ã€‚å½“è¿™ä¸¤éƒ¨åˆ†æœ‰ç›¸åŒçš„å…¶ä»–å‚æ•°æ—¶ï¼Œå°±å°†è¯¥å‚æ•°æ”¾åˆ°åˆ—è¡¨å¤–é¢ä½œä¸ºå…¨å±€å‚æ•°ï¼Œå¦‚ä¸Šé¢çš„weight_decayã€‚ ä¹Ÿå¯ä»¥åœ¨åˆ—è¡¨å¤–è®¾ç½®ä¸€ä¸ªå…¨å±€å­¦ä¹ ç‡ï¼Œå½“å„éƒ¨åˆ†å­—å…¸é‡Œè®¾ç½®äº†å±€éƒ¨å­¦ä¹ ç‡æ—¶ï¼Œå°±ä½¿ç”¨è¯¥å­¦ä¹ ç‡ï¼Œå¦åˆ™å°±ä½¿ç”¨åˆ—è¡¨å¤–çš„å…¨å±€å­¦ä¹ ç‡ã€‚ æ˜¾ç¤ºè®­ç»ƒæ—¶é—´for epoch in range(start_epoch, config.epochs): start = time.time() train_loss, lr = train_epoch(model, optimizer, scheduler, train_loader, device, criterion, epoch, all_step, writer, logger) logger.info('[{}/{}], train_loss: {:.4f}, time: {:.4f}, lr: {}'.format( epoch, config.epochs, train_loss, time.time() - start, lr)) å‚è€ƒï¼šhttps://mp.weixin.qq.com/s?__biz=MzU3NjE4NjQ4MA==mid=2247485953idx=2sn=3ae788b7d643541254ba311f7a7facedchksm=fd16fb1eca61720870bc58c1a465a346cf2c6a7e8bea39e4b3d582474b595021f3a5b635086dmpshare=1scene=1srcid=sharer_sharetime=1566885137387sharer_shareid=285785c5623899db73795495779fe8be#rd","tags":["æ·±åº¦å­¦ä¹ "],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"å¹¿åº¦/å®½åº¦ä¼˜å…ˆæœç´¢(BFS)","path":"/2019/08/28/20190828-BFS/","content":"å‰è¨€å¹¿åº¦ä¼˜å…ˆæœç´¢ æ˜¯æœ€ç®€å•çš„å›¾æœç´¢ç®—æ³•ä¹‹ä¸€ï¼Œ ä¹Ÿæ˜¯è®¸å¤šé‡è¦çš„å›¾ç®—æ³•çš„åŸå‹ã€‚Primeçš„æœ€å°ç”Ÿæˆæ ‘ç®—æ³•å’ŒDijkstraçš„å•æºæœ€çŸ­è·¯å¾„ç®—æ³•éƒ½ä½¿ç”¨äº†ç±»ä¼¼å¹¿åº¦ä¼˜å…ˆæœç´¢çš„æ€æƒ³ã€‚ç»™å®šå›¾G=(V,E) å’Œä¸€ä¸ªå¯ä»¥è¯†åˆ«çš„æº èŠ‚ç‚¹ sï¼Œå¹¿åº¦ä¼˜å…ˆæœç´¢å¯¹å›¾Gä¸­çš„è¾¹è¿›è¡Œç³»ç»Ÿæ€§çš„æ¢ç´¢æ¥å‘ç°å¯ä»¥ä»æºèŠ‚ç‚¹ï¼Œåˆ°è¾¾æ‰€æœ‰çš„èŠ‚ç‚¹ã€‚è¯¥ç®—æ³•èƒ½å¤Ÿè®¡ç®—ä»æºç»“ç‚¹såˆ°æ¯ä¸ªå¯åˆ°è¾¾çš„èŠ‚ç‚¹çš„è·ç¦»(æœ€å°çš„è¾¹æ•°)ï¼ŒåŒæ—¶ç”Ÿæˆä¸€æ£µâ€œå¹¿åº¦ä¼˜å…ˆæœç´¢æ ‘â€ã€‚è¯¥æ ‘ä»¥æºç»“ç‚¹sä¸ºæ ¹èŠ‚ç‚¹ï¼ŒåŒ…æ‹¬æ‰€æœ‰å¯ä»¥ä»såˆ°è¾¾çš„ç»“ç‚¹ã€‚å¯¹äºæ¯ä¸ªä»æºç»“ç‚¹så¯ä»¥åˆ°è¾¾çš„ç»“ç‚¹vï¼Œåœ¨å¹¿åº¦ä¼˜å…ˆæœç´¢æ ‘é‡Œä»ç»“ç‚¹såˆ°ç»“ç‚¹vçš„ç®€å•è·¯å¾„æ‰€å¯¹åº”çš„çš„å°±æ˜¯å›¾Gä¸­ä»ç»“ç‚¹såˆ°ç»“ç‚¹vçš„â€œæœ€çŸ­è·¯å¾„â€ï¼Œå³åŒ…å«æœ€å°‘è¾¹æ•°çš„è·¯å¾„ï¼Œè¯¥ç®—æ³•æ—¢å¯ä»¥ç”¨äºæœ‰å‘å›¾ä¹Ÿå¯ä»¥ç”¨äºæ— å‘å›¾ã€‚å¹¿åº¦ä¼˜å…ˆç®—æ³•ä¹‹æ‰€ä»¥å¦‚æ­¤å¾—åæ˜¯å› ä¸ºè¯¥ç®—æ³•å§‹ç»ˆæ˜¯å°†å·²ç»å‘ç°çš„ç»“ç‚¹å’Œæœªå‘ç°ç»“ç‚¹ä¹‹é—´çš„è¾¹ç•Œï¼Œæ²¿å…¶å¹¿åº¦æ–¹å‘å‘å¤–æ‰©å±•ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç®—æ³•éœ€è¦åœ¨å‘ç°æ‰€æœ‰è·ç¦»æºç»“ç‚¹sä¸ºkçš„æ‰€æœ‰ç»“ç‚¹ä¹‹åï¼Œæ‰ä¼šå‘ç°è·ç¦»æºç»“ç‚¹sä¸ºk+1çš„ å…¶ä»–ç»“ç‚¹ã€‚ å›¾çš„æ¦‚å¿µ å›¾(graph) æ˜¯ä¸€ç§$\\textcolor{Blue}{ç½‘çŠ¶æ•°æ®} $ç»“æ„ï¼Œ å›¾æ˜¯ç”±éç©ºçš„é¡¶ç‚¹é›†åˆå’Œä¸€ä¸ªæè¿°é¡¶ç‚¹ä¹‹é—´çš„å…³ç³»çš„é›†åˆç»„æˆã€‚ å›¾ç”±é¡¶ç‚¹å’Œè¾¹ç»„æˆï¼Œé¡¶ç‚¹è¡¨ç¤ºå¯¹è±¡ï¼Œè¾¹è¡¨ç¤ºå¯¹è±¡ä¹‹é—´çš„è¿æ¥å…³ç³»ã€‚ è¾¹ä¹Ÿå¯ä»¥å¸¦æƒå€¼ï¼Œç§°ä¸ºå¸¦æƒå€¼å›¾ã€‚ æ— å‘å›¾æœ¯è¯­ ä¸¤ä¸ªé¡¶ç‚¹ä¹‹é—´å¦‚æœæœ‰è¾¹è¿æ¥ï¼Œè§†ä¸ºä¸¤ä¸ªé¡¶ç‚¹ç›¸é‚» ç›¸é‚»é¡¶ç‚¹é—´çš„åºåˆ—ç§°ä¸ºè·¯å¾„ èµ·ç‚¹å’Œç»ˆç‚¹é‡åˆçš„è·¯å¾„ç§°ä¸ºåœˆ é¡¶ç‚¹è¿æ¥çš„è¾¹æ•°å«åšè¿™ä¸ªé¡¶ç‚¹çš„åº¦ æ²¡æœ‰åœˆçš„è¿é€šå›¾ï¼Œå°±æ˜¯æ ‘ æ²¡æœ‰åœˆçš„éè¿é€šå›¾ï¼Œå°±æ˜¯æ£®æ— ä¸€æ£µæ ‘çš„è¾¹æ•°ç­‰äºé¡¶ç‚¹æ•°-1 è¾¹æ•°ç­‰äºé¡¶ç‚¹æ•°-1 çš„è¿é€šå›¾ï¼Œå°±æ˜¯æ ‘ BFS(G,s) \\\\for each vertex u \\in G.V -{s}\\\\\tu.color = WHITE\\\\\tu.d = \\infty\\\\\tu.\\pi = NIL\\\\s.color = GRAY\\\\s.d = 0\\\\s.\\pi = NIL\\\\Q = \\emptyset\\\\ENQUEUE(Q,s)\\\\while Q eq = \\emptyset\\\\\tu = DEQUEUE(Q)\\\\\tfor each v \\in G.Adj[u]\\\\ if v.color == WHITE\\\\ v.color = GRAY\\\\ v.d = u.d+1\\\\ v.\\pi = u\\\\ ENQUEUE(Q,v)\\\\\tu.color = BLACK\\\\ å¹¿åº¦ä¼˜å…ˆæœç´¢çš„æµç¨‹å›¾ å®ä¾‹POJ3984ã€Šè¿·å®«é—®é¢˜ã€‹å®šä¹‰ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼šint maze[5][5] = {0, 1, 0, 0, 0,0, 1, 0, 1, 0,0, 0, 0, 0, 0,0, 1, 1, 1, 0,0, 0, 0, 1, 0,};å®ƒè¡¨ç¤ºä¸€ä¸ªè¿·å®«ï¼Œå…¶ä¸­çš„1è¡¨ç¤ºå¢™å£ï¼Œ0è¡¨ç¤ºå¯ä»¥èµ°çš„è·¯ï¼Œåªèƒ½æ¨ªç€èµ°æˆ–ç«–ç€èµ°ï¼Œä¸èƒ½æ–œç€èµ°ï¼Œè¦æ±‚ç¼–ç¨‹åºæ‰¾å‡ºä»å·¦ä¸Šè§’åˆ°å³ä¸‹è§’çš„æœ€çŸ­è·¯çº¿ã€‚ è§£é¢˜æ€è·¯é˜Ÿåˆ—æ˜¯å…ˆè¿›åå‡ºï¼Œåè¿›å…ˆå‡ºã€‚ å¯¹åº”äºé¢˜ç›®çš„è¾“å…¥æ•°ç»„ï¼š0, 1, 0, 0, 0,0, 1, 0, 1, 0,0, 0, 0, 0, 0,0, 1, 1, 1, 0,0, 0, 0, 1, 0,æŠŠèŠ‚ç‚¹å®šä¹‰ä¸º(x,y)ï¼Œ(x,y)è¡¨ç¤ºæ•°ç»„mazeçš„é¡¹maze[x][y]ã€‚äºæ˜¯èµ·ç‚¹å°±æ˜¯(0,0)ï¼Œç»ˆç‚¹æ˜¯(4,4)ã€‚æŒ‰ç…§åˆšåˆšçš„æ€è·¯ï¼Œæ‰‹å·¥æ¢³ç†ä¸€éï¼šåˆå§‹æ¡ä»¶ï¼šèµ·ç‚¹Vsä¸º(0,0)ç»ˆç‚¹Vdä¸º(4,4)ç°è‰²èŠ‚ç‚¹é›†åˆQ={}åˆå§‹åŒ–æ‰€æœ‰èŠ‚ç‚¹ä¸ºç™½è‰²èŠ‚ç‚¹ä¸‹é¢å¼€å§‹å¹¿åº¦ä¼˜å…ˆæœç´¢ï¼š1.èµ·å§‹èŠ‚ç‚¹Vså˜æˆç°è‰²ï¼ŒåŠ å…¥é˜Ÿåˆ—Qï¼ŒQ={(0,0)}2.å–å‡ºé˜Ÿåˆ—Qçš„å¤´ä¸€ä¸ªèŠ‚ç‚¹Vnï¼ŒVn={0,0}ï¼ŒQ={}3.æŠŠVn={0,0}æŸ“æˆé»‘è‰²ï¼Œå–å‡ºVnæ‰€æœ‰ç›¸é‚»çš„ç™½è‰²èŠ‚ç‚¹{(1,0)}4.ä¸åŒ…å«ç»ˆç‚¹(4,4)ï¼ŒæŸ“æˆç°è‰²ï¼ŒåŠ å…¥é˜Ÿåˆ—Qï¼ŒQ={(1,0)}5.å–å‡ºé˜Ÿåˆ—Qçš„å¤´ä¸€ä¸ªèŠ‚ç‚¹Vnï¼ŒVn={1,0}ï¼ŒQ={}6.æŠŠVn={1,0}æŸ“æˆé»‘è‰²ï¼Œå–å‡ºVnæ‰€æœ‰ç›¸é‚»çš„ç™½è‰²èŠ‚ç‚¹{(2,0)}7.ä¸åŒ…å«ç»ˆç‚¹(4,4)ï¼ŒæŸ“æˆç°è‰²ï¼ŒåŠ å…¥é˜Ÿåˆ—Qï¼ŒQ={(2,0)}8.å–å‡ºé˜Ÿåˆ—Qçš„å¤´ä¸€ä¸ªèŠ‚ç‚¹Vnï¼ŒVn={2,0}ï¼ŒQ={}9.æŠŠVn={2,0}æŸ“æˆé»‘è‰²ï¼Œå–å‡ºVnæ‰€æœ‰ç›¸é‚»çš„ç™½è‰²èŠ‚ç‚¹{(2,1), (3,0)}10.ä¸åŒ…å«ç»ˆç‚¹(4,4)ï¼ŒæŸ“æˆç°è‰²ï¼ŒåŠ å…¥é˜Ÿåˆ—Qï¼ŒQ={(2,1), (3,0)}11.å–å‡ºé˜Ÿåˆ—Qçš„å¤´ä¸€ä¸ªèŠ‚ç‚¹Vnï¼ŒVn={2,1}ï¼ŒQ={(3,0)}12.æŠŠVn={2,1}æŸ“æˆé»‘è‰²ï¼Œå–å‡ºVnæ‰€æœ‰ç›¸é‚»çš„ç™½è‰²èŠ‚ç‚¹{(2,2)}13.ä¸åŒ…å«ç»ˆç‚¹(4,4)ï¼ŒæŸ“æˆç°è‰²ï¼ŒåŠ å…¥é˜Ÿåˆ—Qï¼ŒQ={(3,0), (2,2)}14.æŒç»­ä¸‹å»ï¼ŒçŸ¥é“Vnçš„æ‰€æœ‰ç›¸é‚»çš„ç™½è‰²èŠ‚ç‚¹ä¸­åŒ…å«äº†(4,4)â€¦â€¦15.æ­¤æ—¶è·å¾—äº†ç­”æ¡ˆ // BFS.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚//#include \"stdafx.h\"#include iostreamusing namespace std;int map[5][5];//ç›¸é‚»å››ä¸ªèŠ‚ç‚¹int borderUponX[4] = { 0, 0, 1, -1 };int borderUponY[4] = { 1, -1, 0, 0 };int front = 0, rear = 1;struct node {\tint pre;\tint x;\tint y;} path[100];void print(int i) {//å½“å‰èŠ‚ç‚¹\tif (path[i].pre != -1) {//æ‰¾åˆ°å‰é¢é‚£ä¸ªèŠ‚ç‚¹ print(path[i].pre); cout \"(\" path[i].x \",\" path[i].y \")\" endl;\t}\telse {//æœ€å‰é¢çš„é‚£ä¸ªèŠ‚ç‚¹ cout \"(\" path[i].x \",\" path[i].y \")\" endl;\t}}void bfsSearch(int x, int y) {\t//å¼€å§‹èŠ‚ç‚¹ï¼ˆå‡ºå‘ï¼‰ï¼Œå‰é¢æ²¡æœ‰èŠ‚ç‚¹äº†\tpath[front].x = x;\tpath[front].y = y;\tpath[front].pre = -1;\t//å½“front == rearçš„æ—¶å€™è¯´æ˜å·²ç»èµ°å®Œäº†æ‰€ä»¥â€œç›¸é‚»â€èŠ‚ç‚¹\t//ä¸”éƒ½ä¸é€š\twhile (front rear) { for (int i = 0; i != 4; i++) { //ç›¸é‚»èŠ‚ç‚¹åæ ‡ int pathX = path[front].x + borderUponX[i]; int pathY = path[front].y + borderUponY[i]; //ä¸ç¬¦åˆçš„èŠ‚ç‚¹ï¼ˆé‡åˆ°è¾¹ç•Œæˆ–å·²ç»èµ°è¿‡äº†ï¼‰ if (pathY 0 || pathX 0 || pathX 4 || pathY 4 || map[pathX][pathY]) continue; else {//å°†frontçš„ç›¸é‚»çš„å¯ä»¥è¿‡å»çš„å¹¶ä¸”æ˜¯è¿˜æ²¡æœ‰èµ°è¿‡çš„èŠ‚ç‚¹åŠ åˆ°è·¯å¾„é‡Œé¢ map[pathX][pathY] = 1; path[rear].x = pathX; path[rear].y = pathY; path[rear].pre = front; rear++; } if (pathX == 4 pathY == 4) { //æ‰¾åˆ°äº†ä¸€æ¡è·¯å¾„ï¼Œåˆæ˜¯ç¬¬ä¸€æ¬¡æ‰¾åˆ° //é‚£ä¹ˆå°±æ˜¯æœ€çŸ­è·¯å¾„äº† print(rear - 1); break; } } front++;\t}}int main(int argc, char const *argv[]){\tfor (int i = 0; i 5; i++) for (int j = 0; j 5; j++) cin map[i][j];\tbfsSearch(0, 0);\tsystem(\"pause\");\treturn 0;}","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"Pytorchæ‰‹å†Œ","path":"/2019/08/28/20190828-Pytorchæ‰‹å†Œ/","content":"PyTorch ä¸»è¦æä¾›ä»¥ä¸‹ä¸¤å¤§ç‰¹è‰²ï¼š æ”¯æŒå¼ºåŠ›GPUåŠ é€Ÿçš„Tensorè®¡ç®—èƒ½åŠ› åŸºäºtapeçš„å…·æœ‰è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼èƒ½åŠ›çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¡†æ¶ PyTorch ä¸»è¦åŒ…å«ä»¥ä¸‹ç»„æˆè¦ç´ : ç»„æˆè¦ç´  æè¿°è¯´æ˜ torch ä¸€ä¸ªç±»ä¼¼äºnumpyçš„tensorå“­, æä¾›å¼ºåŠ›çš„GPUæ”¯æŒ torch.autograd ä¸€ä¸ªåŸºäºtapeçš„å…·æœ‰è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼èƒ½åŠ›çš„åº“, å¯ä»¥æ”¯æŒå‡ ä¹æ‰€æœ‰çš„tesnor operatioin torch.nn ä¸€ä¸ªç¥ç»ç½‘ç»œåº“, ä¸autogradæ·±åº¦æ•´åˆ, å¯ä»¥æä¾›æœ€å¤§é™åº¦çš„çµæ´»æ€§ torch.multiprocessing Pythonçš„å¤šçº¿ç¨‹å¤„ç†, å¯ä»¥æä¾›torch Tensorsä¹‹é—´çš„å†…å­˜å…±äº«, å¯¹äºåŠ è½½æ•°æ®å’ŒHogwild trainingæ¥è¯´ååˆ†æœ‰ç”¨ torch.utils ä¸€äº›åŠŸèƒ½ç±»å’Œå‡½æ•°, å¦‚DataLoader, Trainerç­‰ç­‰ torch.legacy(.nn/.optim) ä¸ºäº†å…¼å®¹æ€§è€Œå­˜åœ¨çš„ä¸€äº›ä»£ç å’Œå®ç° Pytorché€šå¸¸å¯ä»¥ä½œä¸ºä»¥ä¸‹ç”¨é€”ä½¿ç”¨: ä¸ºäº†ä½¿ç”¨GPUsæ€§èƒ½çš„numpyæ›¿ä»£å“ å¯ä»¥æä¾›å¼ºå¤§çµæ´»åŠ›å’Œé€Ÿåº¦ä¼˜åŠ¿çš„æ·±åº¦å­¦ä¹ å¹³å°. torchbackends.cudnntorch.backends.cudnn.benchmark = True ä¸Šè¿°è®¾ç½®å¯ä»¥è®©å†…ç½®çš„cudnnçš„auto-tunerè‡ªåŠ¨å¯»æ‰¾æœ€åˆé€‚å½“å‰é…ç½®çš„æç¬‘ç®—æ³•, æ¥è¾¾åˆ°ä¼˜åŒ–è¿è¡Œæ•ˆç‡çš„ç›®æ ‡, åœ¨ä½¿ç”¨æ—¶, åº”è¯¥éµå¾ªä»¥ä¸‹ä¸¤ä¸ªå‡†åˆ™: å¦‚æœç½‘ç»œçš„è¾“å…¥æ•°æ®ç»´åº¦æˆ–ç±»å‹ä¸Šå˜åŒ–ä¸å¤§, åˆ™è¯¥è®¾ç½®å¯ä»¥å¢åŠ è¿è¡Œæ•ˆç‡ å¦‚æœç½‘ç»œçš„è¾“å…¥æ•°æ®åœ¨æ¯æ¬¡çš„iterationä¸­éƒ½å˜åŒ–çš„è¯, ä¼šå¯¼è‡´cudnnæ¯æ¬¡éƒ½å¯»æ‰¾ä¸€éæœ€ä¼˜é…ç½®, è¿™æ ·åè€Œ ä¼šé™ä½ è¿è¡Œæ•ˆç‡. torch.cat()torch.cat(seq, dim=0, out=None) # è¿”å›è¿æ¥åçš„tensor å°†ç»™å®šçš„ tensor åºåˆ— seq æŒ‰ç…§ç»´åº¦è¿æ¥èµ·æ¥. é»˜è®¤ç»´åº¦ä¸º0, è¯´æ˜ä¼šå°†å…¶åœ¨ç¬¬ 0 ä¸ªç»´åº¦ä¸Šè¿›è¡Œæ‹¼æ¥.(æœ€åçš„ç»“æœæ˜¯ç¬¬ 0 ç»´åº¦å¢å¤§, ä¾‹å¦‚ä¸‰ä¸ª2è¡Œ3åˆ—çš„ tensor æŒ‰ç…§ç¬¬0ç»´åº¦æ‹¼æ¥, æœ€åå¾—åˆ°çš„ tensor ç»´åº¦ä¸º6è¡Œ3åˆ—) clamp()/clamp_()torch.clamp(input, min, max, out=None) - Tensor å°†inputé‡Œé¢å…ƒç´ å…¨éƒ¨åˆ’åˆ†åˆ°[min,max]åŒºé—´å†…, å°äºminçš„ç½®ä¸ºmin, å¤§äºmaxçš„ç½®ä¸ºmax. å¦‚æœä¸æŒ‡å®šminæˆ–è€…max,åˆ™è®¤ä¸ºæ— ä¸‹ç•Œæˆ–ä¸Šç•Œå…¶ä»–è°ƒç”¨å½¢å¼: torch.Tensor(min, max) # è°ƒç”¨tensorä¸ºinput, è¿”å›å€¼ä¸ºout device()device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") gather()torch.gather(input, dim, index, out=None) - Tensor æ²¿ç€dimæŒ‡å®šçš„è½´æŒ‰ç€indexæŒ‡å®šçš„å€¼é‡æ–°ç»„åˆæˆä¸€ä¸ªæ–°çš„tensor. out[i][j][k] = input[index[i][j][k]][j][k] # if dim == 0out[i][j][k] = input[i][index[i][j][k]][k] # if dim == 1out[i][j][k] = input[i][j][index[i][j][k]] # if dim == 2 å³å‡è®¾inputæ˜¯ä¸€ä¸ª n ç»´çš„tensor, å…¶ size ä¸º (x0,x1,â€¦,xiâˆ’1,xi,xi+1,â€¦,xnâˆ’1), è‹¥dim=i, åˆ™ index å¿…é¡»ä¹Ÿæ˜¯ä¸€ä¸ª n ç»´çš„tensor, å…¶ size ä¸º (x0,x1,â€¦,xiâˆ’1,y,xi+1,â€¦,xnâˆ’1), å…¶ä¸­ yâ‰¥1, è€Œè¿”å›çš„ tensor out çš„ size å’Œ index çš„ size ç›¸åŒ.ä¸€å¥æ¥è¯´ gather çš„ä½œç”¨å°±æ˜¯, åœ¨æŒ‡å®šçš„ç»´åº¦ä¸Šç­›é€‰ç»™ç»™å®šä¸‹æ ‡indexæŒ‡ç¤ºçš„å€¼, å…¶ä»–å€¼èˆå¼ƒ.ä¸€ä¸ªä¾‹å­è¯´æ˜:scoresæ˜¯ä¸€ä¸ªè®¡ç®—å‡ºæ¥çš„åˆ†æ•°ï¼Œç±»å‹ä¸º[torch.FloatTensor of size 5x1000]è€Œy_varæ˜¯æ­£ç¡®åˆ†æ•°çš„ç´¢å¼•ï¼Œç±»å‹ä¸º[torch.LongTensor of size 5]å®¹æ˜“çŸ¥é“ï¼Œè¿™é‡Œæœ‰1000ä¸ªç±»åˆ«ï¼Œæœ‰5ä¸ªè¾“å…¥å›¾åƒï¼Œæ¯ä¸ªå›¾åƒå¾—å‡ºçš„åˆ†æ•°ä¸­åªæœ‰ä¸€ä¸ªæ˜¯æ­£ç¡®çš„ï¼Œæ­£ç¡®çš„ç´¢å¼•å°±åœ¨y_varä¸­ï¼Œè¿™é‡Œè¦åšçš„æ˜¯å°†æ­£ç¡®åˆ†æ•°æ ¹æ®ç´¢å¼•æ ‡å·æå–å‡ºæ¥ã€‚ scores = model(X_var) # åˆ†æ•°scores = scores.gather(1, y_var.view(-1, 1)).squeeze() #è¿›è¡Œæå– æå–åçš„scoresæ ¼å¼ä¹Ÿä¸º[torch.FloatTensor of size 5]è¿™é‡Œè®²ä¸€ä¸‹å˜åŒ–è¿‡ç¨‹ï¼š é¦–å…ˆè¦çŸ¥é“ä¹‹å‰çš„scoresçš„sizeä¸º[5,1000]ï¼Œè€Œy_varçš„sizeä¸º[5]ï¼Œscoresä¸º2ç»´ï¼Œy_varä¸º1ç»´ä¸åŒ¹é…ï¼Œæ‰€ä»¥å…ˆç”¨viewå°†å…¶å±•å¼€ä¸º[5,1]çš„sizeï¼Œè¿™æ ·ç»´æ•°nå°±ä¸scroesåŒ¹é…äº†ã€‚ æ¥ä¸‹æ¥è¿›è¡Œgatherï¼Œgatherå‡½æ•°ä¸­ç¬¬ä¸€ä¸ªå‚æ•°ä¸º1ï¼Œæ„æ€æ˜¯åœ¨ç¬¬äºŒç»´è¿›è¡Œæ±‡èšï¼Œä¹Ÿå°±æ˜¯è¯´é€šè¿‡y_varä¸­çš„äº”ä¸ªå€¼æ¥åœ¨scroesä¸­ç¬¬äºŒç»´çš„5ä¸ª1000ä¸­è¿›è¡Œä¸€ä¸€æŒ‘é€‰ï¼ŒæŒ‘é€‰å‡ºæ¥åçš„sizeä¹Ÿä¸º[5,1]ï¼Œç„¶åå†é€šè¿‡squeezeå°†é‚£ä¸ªä¸€ç»´å»æ‰ï¼Œæœ€åç»“æœä¸º[5].Tensorå½¢å¼ torch.Tensor.gather(dim, index) - Tensor torch.ge()torch.gt()torch.gt(input, other, out=None) # - Tensor æ ¹æ® input å’Œ other çš„å€¼è¿”å›ä¸€ä¸ªäºŒå€¼ tensor, å¦‚æœæ»¡è¶³å¤§äºæ¡ä»¶åˆ™ä¸º1, ä¸æ»¡è¶³åˆ™ä¸º0.other å¯ä»¥æ˜¯èƒ½å¤Ÿè½¬æ¢æˆ input size çš„tensor, ä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ª float æ ‡é‡. torch.index_select()torch.index_select(input, dim, index, out=None) # - Tensor è¿”å›åœ¨ dim ç»´åº¦ä¸Šçš„ index æŒ‡æ˜çš„ä¸‹æ ‡ç»„æˆçš„ tensor.è¿”å›çš„ tensor çš„ç»´åº¦çš„æ•°é‡å’Œ input æ˜¯ç›¸åŒçš„, ä½†æ˜¯ç¬¬ dim ç»´åº¦çš„ size ä¼šå’Œ index sizeå¤§å°ç›¸åŒ. å…¶ä»–ç»´åº¦çš„ size ä¿æŒä¸å˜. torch.le()torch.le(input, other, out=None) # -Tensor æŒ‰å…ƒç´ è®¡ç®— inputâ‰¤other. max()torch.max(input) # è¿”å›ä¸€ä¸ªTensor, ä»£è¡¨æ‰€æœ‰å…ƒç´ ä¸­çš„æœ€å¤§å€¼torch.max(input,dim,keepdim=False,out=None) # è¿”å›ä¸€ä¸ªå…ƒç»„:(Tensor, LongTensor) ç¬¬äºŒç§å½¢å¼ä¼šè¿”å›ä¸€ä¸ªå…ƒç»„, å…ƒç»„å†…å…ƒç´ ç±»å‹ä¸º: (Tensor, LongTensor), å…¶ä¸­, å‰è€…ä»£è¡¨å¯¹åº” dim ä¸Š reduce åçš„æœ€å¤§å€¼, åè€…ä»£è¡¨æœ€å¤§å€¼åœ¨ç»´åº¦ dim ä¸­å¯¹åº”çš„ä¸‹æ ‡.å¦‚æœkeepdim=True, åˆ™è¾“å‡ºçš„ tensor çš„ size ä¼šå’Œè¾“å…¥çš„ç›¸åŒ, åªä¸è¿‡å¯¹åº” dim ç»´åº¦ä¸Šçš„sizeä¸º1. å¦åˆ™, å¯¹åº” dim ç»´åº¦ä¼šè¢« squeeze/reduce, ä½¿å¾—è¾“å‡ºçš„ç»´åº¦æ¯”è¾“å…¥çš„ç»´åº¦å°‘1. a = torch.randn(4, 4) atensor([[-1.2360, -0.2942, -0.1222, 0.8475], [ 1.1949, -1.1127, -2.2379, -0.6702], [ 1.5717, -0.9207, 0.1297, -1.8768], [-0.6172, 1.0036, -0.6060, -0.2432]]) torch.max(a, 1)(tensor([ 0.8475, 1.1949, 1.5717, 1.0036]), tensor([ 3, 0, 0, 1])) mm()æ³¨æ„, æ²¡æœ‰torch.mm_ç‰ˆæœ¬ torch.mm(mat1, mat2, out=None) # è¿”å›å€¼ä¸ºTensor, ä¹Ÿå¯ä»¥ä½¿ç”¨outè®°å½•è¿”å›å€¼ ä¸¤çŸ©é˜µç›¸ä¹˜, çŸ©é˜µçš„sizeéœ€è¦æ»¡è¶³ä¹˜æ³•è§„åˆ™å…¶ä»–è°ƒç”¨å½¢å¼: torch.Tensor(mat2) # è°ƒç”¨è€…ä¸ºmat1 norm()è¿”å›è¾“å…¥tensorçš„p-normæ ‡é‡ torch.norm(input, p=2) # è¿”å›ä¸€ä¸ªæ ‡é‡tensor numel()torch.numel(input) #è¿”å›ä¸€ä¸ªintå€¼ è¿”å› inpput tensor ä¸­çš„å…ƒç´ çš„æ€»ä¸ªæ•° a = torch.randn(1,2,3,4,5)print(torch.numel(a)) # 120 ones()randn()æ ‡å‡†æ­£å¤ªåˆ†å¸ƒéšæœºåŸºç¡€, ä¼ å…¥å‚æ•°ä¸ºç»´åº¦ä¿¡æ¯ torch.sort()torch.sort(input, dim=None, descending=False, out=None) # è¿”å› (Tensor, LongTensor) sum()torch.sum(input, dtype=None) # è¿”å›æ±‚å’Œåçš„Tensor(åªæœ‰ä¸€ä¸ªå…ƒç´ )torch.sum(input, dim, keepdim=False, dtype=None) # è¿”å›åœ¨dimä¸Šreduceçš„sumå’Œ, å¦‚æœdimåŒ…å«å¤šä¸ªç»´åº¦, åˆ™éƒ½è¿›è¡Œreduceæ±‚å’Œ.# reduceè¿™ä¸ªè¯å¾ˆå½¢è±¡, å› ä¸ºè¿”å›çš„Tensorçš„ç»´åº¦åˆšå¥½æ²¡æœ‰äº†dimæŒ‡ç¤ºçš„é‚£äº›ç»´åº¦ å…¶ä»–å½¢å¼: torch.Tensor.sum() torch.t()torch.t(input) # è¿”å›è½¬ç½®åçš„Tensor å…¶ä»–å½¢å¼: torch.Tensor.t() unsqueeze()åœ¨æŒ‡å®šç»´åº¦ä¸Šæ’å…¥ä¸€ä¸ª singleton ç»´åº¦(ä¸€èˆ¬ç”¨äºå°†å•ä¸€æ•°æ®å¤„ç†ç”¨ batch çš„å½¢å¼) torch.unsqueeze(input, dim, out=None) # - Tensor è¿”å›çš„tensorä¸input tensor å…±äº«æ•°æ® dim çš„å–å€¼èŒƒå›´åœ¨ [-input.dim()-1, input.dim()+1] ä¹‹é—´, å¦‚æœä¸ºè´Ÿå€¼, åˆ™ç›¸å½“äº dim = dim + input.dim() + 1. zeros()torch.cudatorch.cuda.empty_cache()é‡Šæ”¾æ‰€æœ‰æœªä½¿ç”¨çš„ GPU å†…å­˜, ä½¿ç”¨è¿™äº›å†…å­˜å¯ä»¥è¢«å…¶ä»– GPU åº”ç”¨ä½¿ç”¨, å¹¶ä¸”å¯ä»¥è¢« nvidia-smi æŸ¥åˆ°.empty_cache() å¹¶ä¸ä¼šå¼ºåˆ¶æå‡ä¾› PyTorch ä½¿ç”¨çš„æ˜¾å¡å†…å­˜çš„å¤§å°, æŸ¥çœ‹Memory management torch.Tensortorch.Tensor æ˜¯é»˜è®¤ç±»å‹ torch.FloatTensor çš„åˆ«å, ä½¿ç”¨ torch.Tenosr çš„æ„é€ å‡½æ•°åˆ›å»º tensor å˜é‡æ—¶, ä¼ å…¥çš„æ˜¯ç»´åº¦ä¿¡æ¯(æ³¨æ„ä¸ torch.tensor() çš„åŒºåˆ«): t = torch.Tensor(2,3,4) # é‡Œé¢çš„æ•°å€¼æœªåˆå§‹åŒ–, æ˜¯éšæœºçš„print(t.size()) # torch.Size([2,3,4]) torch.LongTesnor ä½¿ç”¨æ–¹æ³•ç›¸ä¼¼, åªä¸è¿‡æ•°æ®ç±»å‹æ˜¯é•¿æ•´å‹. troch.tensor()åˆ›å»ºtensor torch.tensor(data, dtype=None, device=None, requires_grad=False) å¯ä»¥åˆ©ç”¨torch.tensorä»pythonçš„listæ•°æ®æˆ–è€…å…¶ä»–åºåˆ—æ•°æ®ä¸­åˆ›å»ºtensorå¯¹è±¡ torch.tensor([[1,-1],[1,-1]])torch.tensor(np.array([[1,2,3],[4,5,6]])) æ³¨æ„, torch.tensor()å‡½æ•°æ€»æ˜¯ä¼šå¯¹æ•°æ®è¿›è¡Œå¤åˆ¶æ“ä½œ, å› æ­¤, å¦‚æœä½ ä»…ä»…æ˜¯æƒ³å°†æ•°æ®çš„requires_gradæ ‡å¿—æ”¹å˜, é‚£ä¹ˆå°±åº”è¯¥ä½¿ç”¨required_grad_()æˆ–è€…detach()å‡½æ•°æ¥é¿å…å¤åˆ¶. åŒæ—¶, å¯¹numpyæ•°ç»„ä½¿ç”¨torch.as_tensor()å°†å…¶è½¬æ¢æˆtensorè€Œæ— éœ€å¤åˆ¶ torch.Tensor.cpu()torch.Tensor.cpu()z = x.cpu() å°†tensorç§»åŠ¨åˆ°cpuä¸Š, æ³¨æ„è¿”å›å€¼zæ˜¯cpuä¸Šçš„æ•°æ®, tensor x æœ¬èº«çš„deviceå±æ€§ä¸å˜ torch.Tensor.cuda()torch.Tensor.cuda()z = x.cuda() torch.Tensor.dim()torch.Tensor.dim() - int è¿”å› tensor çš„ç»´åº¦çš„ä¸ªæ•°. torch.Tensor.max()torch.Tensor.max(dim=None, keepdim=False) - Tensor or (Tensor, Tensor) è¯¦æƒ…è§ torch.max() torch.Tensor.numel()torch.Tensor.numel() è¯¦è§ torch.numel() torch.Tensor.to()torch.Tensor.to(*args, *kwargs) è¿”å›ä¸€ä¸ªè½¬ç§»åçš„tensor, è€Œè‡ªèº«ç»´æŒä¸å˜ t = torch.randn(2,3)t.to(torch.float64)t.to(device)t.to(\"cuda:0\") å°†tensorç§»åŠ¨åˆ°gpuä¸Š, æ³¨æ„è¿”å›å€¼ z æ˜¯gpu ä¸Šçš„æ•°æ®, tensor x æœ¬èº«çš„ device å±æ€§ä¸å˜ torch.Tensor.numpy()tensorä¸numpyæ•°ç»„çš„è½¬æ¢ torch.Tensor.numpy() # è¿”å›tensorå¯¹åº”çš„numpyæ•°ç»„torch.from_numpy(ndarray) # å°†numpyæ•°ç»„ndarrayè½¬æ¢æˆå¯¹åº”çš„tensorå¹¶è¿”å›. torch.Tensor å®é™…ä¸Šæ˜¯ torch.FloatFensor çš„åˆ«å torch.Tensor.permute()é‡æ–°æ’åˆ—tensorçš„ç»´åº¦ torch.Tensor.permute(*dims) # è¿”å›ä¸€ä¸ªé‡æ–°æ’åˆ—ç»´åº¦åçš„ tensor torch.Tensor.unsqueeze()è¯¦ç»†å¯è§torch.unsqueeze torch.Tensor.expand()torch.Tensor.expand(*sizes) # è¿”å› tensor å°† tensor ä¸­çš„ singleton ç»´åº¦æ‰©å±•åˆ°ä¸€ä¸ªæ›´å¤§çš„ size.å‚æ•° -1 æ„å‘³ç€ä¸æ”¹å˜åŸå§‹çš„ç»´åº¦æ–°å¢çš„ç»´åº¦çš„å…ƒç´ è¢«è¢«æ·»åŠ åˆ°å‰å¤´, sizeä¸èƒ½è®¾ç½®ä¸º-1.expand å¹¶æ²¡æœ‰ç”³è¯·æ–°çš„å†…å­˜, è€Œä»…ä»…æ˜¯åœ¨å½“å‰å·²ç»å­˜åœ¨çš„ tensor ä¸Šé¢åˆ›å»ºäº†æ–°çš„è§†å›¾(view), ä½¿å¾— singleton ç»´åº¦è¢«æ‰©å±•æˆäº†ä¸€ä¸ªæ›´å¤§çš„å°ºå¯¸.Any dimension of size 1 can be expanded to an arbitrary value without new memory. x = torch.tensor([1],[2],[3])print(x.size()) # torch.Size([3,1])print(x.expand(3,4)) # torch.Size([3,4]) # å°†ç»´åº¦ä¸º1çš„æ‰©å±•åˆ°ä»»æ„å°ºå¯¸print(x.expand(-1,4)) # torch.Size([3,4]) # -1 ä»£è¡¨ä¸æ”¹å˜ç»´åº¦ æ³¨æ„, åªèƒ½å¯¹ singleton çš„ç»´åº¦è¿›è¡Œæ‰©å±•, å¦‚æœå¼ºè¡Œå¯¹å…¶ä»–ç»´åº¦æ‰©å±•, åˆ™ä¼šæŠ¥é”™. torch.Tensor.expand_as()torch.Tensor.expand_as(other) # è¿”å› tensor å°†å½“å‰ tensor æ‰©å±•åˆ°å’Œ other ä¸€æ ·çš„size.self.expand_as(other) ä¸ self.expand(other.size()) ç­‰ä»·. torch.Tensor.index_fill_()torch.Tensor.index_fill_(dim, index, val) # è¿”å›tensor åœ¨ç»™å®šçš„ç»´åº¦ dim ä¸Š, ç”¨ val å°†è¯¥ç»´åº¦ä¸Šçš„ index åæ ‡çš„å€¼å¡«å……. x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)index = torch.tensor([0, 2])x.index_fill_(1, index, -1)print(x)#tensor([[-1., 2., -1.],# [-1., 5., -1.],# [-1., 8., -1.]]) torch.Tensor.contiguous()è¿”å›ä¸€ä¸ªè¿ç»­çš„tensor, æ•°æ®å†…å®¹ä¸å˜ torch.Tensor.contiguous() # å¦‚æœtensoræœ¬èº«å°±æ˜¯è¿ç»­çš„, é‚£ä¹ˆå°±ä¼šè¿”å›tensoræœ¬èº« è¿™é‡Œçš„ contiguous æŒ‡çš„æ˜¯å†…å­˜ä¸Šçš„è¿ç»­, ç”±äºåœ¨ PyTorch ä¸­, view åªèƒ½ç”¨åœ¨ contiguous çš„ tensor ä¸Šé¢, è€Œå¦‚æœåœ¨ view ä¹‹å‰ä½¿ç”¨äº† transpose, permute ç­‰æ“ä½œå, å°±éœ€è¦ä½¿ç”¨ contiguous æ¥è¿”å›ä¸€ä¸ª contiguous tensor.åœ¨ PyTorch 0.4 ç‰ˆæœ¬ä»¥å, å¢åŠ äº† torch.reshape(), è¿™ä¸ numpy.reshape() çš„åŠŸèƒ½ç±»ä¼¼, å®ƒå¤§è‡´ç›¸å½“äº tensor.contiguous().view() ? torch.Tensor.item()å½“Tensorä¸­åªåŒ…å«ä¸€ä¸ªå…ƒç´ æ—¶, å¯ä»¥åˆ©ç”¨è¯¥å‡½æ•°è¿”å›è¿™ä¸ªå…ƒç´ çš„æ ‡é‡ torch.Tensor.tolist()å¯ä»¥å°†Tensorè½¬æ¢æˆåˆ—è¡¨ torch.Tensor.zero_()torch.Tensor.zero_() å°†å½“å‰çš„ tensor å˜é‡å…¨éƒ¨ç½®ä¸º0(åŸåœ°) torch.autogradset_grad_enabled()class torch.autograd.set_grad_enabled(mode) ç”¨æ¥æ§åˆ¶æ¢¯åº¦è®¡ç®—çš„å¼€å…³(ä¾æ®boolç±»å‹å‚æ•°modeå†³å®š), å¯ä»¥å½“åšä¸Šä¸‹æ–‡ç®¡ç†å™¨ä½¿ç”¨, ä¹Ÿå¯ä»¥å½“åšå‡½æ•°ä½¿ç”¨ # å½“åšä¸Šä¸‹æ–‡ç®¡ç†å™¨with torch.set_grad_enabled(is_train): # æ³¨æ„, è¿™é‡Œçœç•¥äº†autograd loss.backward() optimizer.step()# å½“åšå‡½æ•°ä½¿ç”¨w1 = torch.Tensor([1], requires=True)torch.set_grad_enabled(True)print(w1.requires_grad) # Truetorch.set_grad_enabled(False)print(w1.requires_grad) # False no_grad()class torch.autograd.no_grad ç”¨äºç¦ç”¨æ¢¯åº¦è®¡ç®—çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨.åœ¨æµ‹è¯•é˜¶æ®µ, å½“ä½ ç¡®ä¿¡ä½ ä¸ä¼šè°ƒç”¨Tensor.backward()æ—¶,ç¦ç”¨æ¢¯åº¦è®¡ç®—ååˆ†æœ‰ç”¨. è¿™ä¼šé™ä½è®¡ç®—ä½¿ç”¨å†…å­˜æ¶ˆè€—. x = torch.tensor([1.0], requires_grad=True)with torch.no_grad(): # çœç•¥äº†autograd print(x.requires_grad) # True, è™½ç„¶ä¸ºTrue, ä½†åœ¨è¯¥ä¸Šä¸‹æ–‡ä¸­, ä¼šæ— è§†æ‰requires_gradå‚æ•°, ä¸€å¾‹åšFalseå¤„ç† y = x*2 print(y.requires_grad) # False, åœ¨å½“å‰ä¸Šä¸‹æ–‡äº§ç”Ÿçš„tensorçš„requires_gradå±æ€§ä¸ºFalseprint(x.requires_grad) # True torch.autograd.Functionclass torch.autograd.Function ä¸ºå¯å¾®åˆ†çš„ ops è®°å½• operation history, åŒæ—¶å®šä¹‰è®¡ç®—å…¬å¼. æ¯ä¸€ä¸ªä½œç”¨åœ¨ tensor ä¸Šçš„ operatin éƒ½ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„ function å¯¹è±¡, å®ƒä¼šæ‰§è¡Œè®¡ç®—è¿‡ç¨‹å¹¶è®°å½•ç›¸å…³ä¿¡æ¯. è¿™äº›ä¿¡æ¯å¯ä»¥ä»ä¸€ä¸ªç”± functions ç»„æˆçš„æœ‰å‘å›¾ä¸­è·å¾—. å½“ backward() æ–¹æ³•è¢«è°ƒç”¨æ—¶, å°±ä¼šåˆ©ç”¨è¿™äº›ä¿¡æ¯åœ¨ function ä¸Šè¿›è¡Œåå‘ä¼ æ’­, å¹¶å°†æ¢¯åº¦ä¼ ç»™ä¸‹ä¸€ä¸ª Funtion.é€šå¸¸æƒ…å†µä¸‹, å½“ç”¨äºéœ€è¦è‡ªå®šä¹‰å¯è‡ªåŠ¨æ±‚å¯¼çš„ ops æ—¶, å¯ä»¥å®ç°ä¸€ä¸ª Function çš„å­ç±». # Exampleclass Exp(Function): @staticmethod def forward(ctx, i): result = i.exp() ctx.save_for_backward(result) @staticmethod def backward(ctx, grad_output): result, = ctx.saved_tensors return grad_output*result static forward(ctx, args, kwargs):*å®šä¹‰å‰å‘è®¡ç®—çš„é€»è¾‘. static backward(ctx, *grad_outputs):å®šä¹‰åå‘ä¼ å¯¼çš„é€»è¾‘, å¦‚æœç¡®å®šä¸ä¼šä½¿ç”¨åˆ°åå‘ä¼ æ’­, åˆ™å¯ä»¥ä¸å®ç°è¯¥å‡½æ•°. torch.nnModuleclass torch.nn.Module æ‰€æœ‰ç¥ç»ç½‘ç»œModuleçš„åŸºç±», è‡ªå®šä¹‰çš„æ¨¡å‹ä¹Ÿåº”è¯¥æ˜¯å®ƒçš„å­ç±».Moduleså¯ä»¥åŒ…å«å…¶ä»–Module(å¦‚Linear, Conv2dç­‰ç­‰). parameters()for param in model.parameters(): print(param.data, param.size()) state_dict: torch.nn.Module.state_dict(destination=None,prefix=\"\",keep_vars=False) ä»¥å­—å…¸å½¢å¼è¿”å›æ•´ä¸ªmoduleçš„çŠ¶æ€ traintorch.nn.Module.train(mode=True) å°†moduleçš„æ¨¡å¼è®¾ç½®ä¸ºtrain, è¿™åªå¯¹éƒ¨åˆ†moduleæœ‰æ•ˆ, å¦‚Dropout, BatchNormç­‰, è¯¦ç»†è¯·æŸ¥çœ‹å®˜ç½‘.è¿”å›å€¼: torch.nn.Module trainingtorch.nn.Module.training # å±æ€§, è¿”å›ä¸€ä¸ªboolå€¼, æŒ‡ç¤ºå½“å‰çš„æ¨¡å¼æ˜¯å¦ä¸ºtrain evaltorch.nn.Module.eval() # æ³¨æ„, å’Œtrainä¸åŒ, evalä¸ºæ— å‚å‡½æ•° å°†moduleçš„modeè®¾ç½®ä¸ºevaluation, åŒæ ·, åªå¯¹éƒ¨åˆ†moduleèµ·æ•ˆ. Lineartorch.nn.Linear(in_features, out_features, bias=True) å…¨è¿æ¥å±‚çš„å®ç°. è¾“å…¥çš„shapeä¸º (N,â€¦,infeatures), è¾“å‡ºçš„shapeä¸º (N,â€¦,outfeatures), å¯ä»¥çœ‹å‡º, é™¤äº†æœ€åä¸€ç»´ä¸åŒå¤–, å…¶ä»–ç»´åº¦éƒ½ç›¸åŒ. (é€šå¸¸åœ¨ä½¿ç”¨Linearä¹‹å‰, ä¼šå°†è¾“å…¥å˜æˆäºŒç»´çš„çŸ©é˜µ, å…¶ä¸­ç¬¬ä¸€ç»´ä¸ºbatch size, ç¬¬äºŒç»´ä¸ºç‰¹å¾å‘é‡).in_features å’Œ out_features å¯ä»¥å½“åšå±æ€§ç”¨.æ¥è·å–. Conv2dclass torch.nn.Conv2çš„(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) in_channels(int): out_channels(int): kernel_size(intortuple): stride(intortuple, optional): MaxPool2dSoftmax()```pythonclass torch.nn.Softmax(dim=None) dimæŒ‡æ˜äº†éœ€è¦è¿›è¡Œ softmax çš„ç»´åº¦, åœ¨è¿™ä¸ªç»´åº¦ä¸Šçš„å€¼, åŠ èµ·æ¥å’Œä¸º1. ReLUtorch.nn.ReLU(inplace=False) è¾“å…¥è¾“å‡ºçš„shapeæ˜¯ç›¸åŒçš„, æ‰§è¡Œreluå‡½æ•° torch.nn.Sequentialclass torch.nn.Sequential(*args) torch.nn.MSELossclass torch.nn.MSELoss(size_average=None, reduce=None, reduction=\"elementwise_mean\") size_average(bool, optional): å¼ƒç”¨(è§reductionå‚æ•°). é»˜è®¤æƒ…å†µä¸‹, lossä¼šè®¡ç®—åœ¨æ¯ä¸ªæ ·æœ¬ä¸Šçš„å¹³å‡è¯¯å·®. å¦‚æœå°†size_averageç½®ä¸ºFalse, åˆ™è®¡ç®—å¹³æ–¹è¯¯å·®æ€»å’Œ. å½“reduceå‚æ•°ä¸ºFalseæ—¶, å¿½è§†è¯¥å‚æ•° reduce(bool, optional): å¼ƒç”¨(è§reductionå‚æ•°). reduceå‚æ•°é¡¾åæ€ä¹‰, å°±æ˜¯æ˜¯å¦è®©MSELosså‡½æ•°è¿”å›å€¼çš„ç»´åº¦å‡å°‘, é»˜è®¤ä¸ºTrue, å³ä¼šå°†ä»»æ„ç»´åº¦çš„è¾“å…¥è®¡ç®—losså, è¿”å›ä¸€ä¸ªæ ‡é‡(å¹³å‡oræ€»å’Œå–å†³äºsize_average), å¦‚æœä¸ºFalse, åˆ™è¯´æ˜è¿”å›å€¼ç»´åº¦ä¸åº”è¯¥å‘ç”Ÿå˜åŒ–, æ•…è€Œè¿”å›å€¼å°±æ˜¯å¯¹æ¯ä¸ªå…ƒç´ å•ç‹¬è¿›è¡Œå¹³æ–¹æŸå¤±è®¡ç®—.12345678910y = torch.tensor([1,2,3,4], dtype=torch.float)pred_y = torch.tensor([1,1,1,1], dtype=torch.float)loss_fn1 = torch.nn.MSELoss()loss1 = loss_fn1(y, pred_y)loss_fn2 = torch.nn.MSELoss(size_average=False)loss2 = loss_fn2(y, pred_y)loss_fn3 = torch.nn.MSELoss(reduce=False)loss3 = loss_fn3(y, pred_y)print(loss1,loss2,loss3)# tensor(3.5000) tensor(14.) tensor([0., 1., 4., 9.]) 3. reduction(string, optional): ç”¨å­—ç¬¦ä¸²æ¥æ›¿ä»£ä¸Šé¢ä¸¤ä¸ªå‚æ•°çš„ä½œç”¨: â€œelementwise_meanâ€(é»˜è®¤) | â€œsumâ€ | â€œnoneâ€ (ä¸è¿›è¡Œreduce).### torch.nn.functional#### conv1d()#### conv2d()#### relu() ```python torch.nn.functional.relu(input, inplace=True) # è¿”å› ä¸€ä¸ª Tenosr #### relu_()```python torch.nn.functional.relu_(input) # relu() çš„åŸåœ°ç‰ˆæœ¬ torch.optimlr_schedulerStepLRclass torch.optim.lr_schedulr.StepLR(optimizer,step_size,gamma=0.1,last_epoch=-1) æ¯ç»è¿‡step_sizeæ¬¡epochä¹‹å, lrå°±ä¼šè¡°å‡gammaå€(new_lr=lrÃ—gamma), åˆå§‹çš„lræ¥è‡ªäºoptimizerä¸­çš„lrå‚æ•°. # Observe that all parameters are being optimizedoptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)# Decay LR by a factor of 0.1 every 7 epochsexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) ExponentialLRclass torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma,last_epoch=-1) CosineAnnealingLR## Adamclass torch.optim.Adam(params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False) conv2dtorch.utils.dataDataLoaderclass torch.utils.data.DataLoader(dataset,batch_size=1,shuffle=False,sampler=None,batch_sampler=None,num_workers=0,collate_fn=function default_collate,pin_memory=False,drop_last=False,timeout=0,worker_init_fn=None) æ•°æ®åŠ è½½å™¨, å°†æ•°æ®é›†å’Œé‡‡æ ·å™¨ç»“åˆèµ·æ¥, å¹¶ä¸”æä¾›å•/å¤šçº¿ç¨‹çš„è¿­ä»£å™¨. dataset(utils.data.Dataset): batch_size(int,optional): batchä¸­çš„æ ·æœ¬ä¸ªæ•° shuffle(bool,optional) num_worker(int,optional): åŠ è½½æ•°æ®çš„çº¿ç¨‹ä¸ªæ•°, 0æ„å‘³ç€åªæœ‰ä¸€ä¸ªä¸»çº¿ç¨‹.æ–¹æ³•ï¼š iter(self): å¯ä»¥å½“åšè¿­ä»£å™¨ä½¿ç”¨, å¦‚inputs,class_ids=next(iter(dataloaders)), å…¶ä¸­, inputçš„shapeä¸º (N,C,H,W), class_idsçš„shapeä¸º (N). len(self): è¿”å›æ•°æ®é›†çš„ç±»åˆ«æ•°ç›® torchvisiontorchvision.utilsmake_gridtorchvision.utils.make_grid(tensor,nrow=8,padding=2,normalize=False,range=None,scale_each=False,pad_value=0) åˆ¶ä½œä¸€ä¸ªå…³äºimageçš„grid, è¿”å›å€¼ä¾ç„¶æ˜¯ä¸€ä¸ªtensor, åªä¸è¿‡å°ºåº¦å˜æˆäº†3D, ç›¸å½“äºæŠŠå¤šä¸ªå›¾ç‰‡æ‹¼æ¥åœ¨ä¸€èµ·äº†, ç›´æ¥é€šè¿‡plt.imshow(grid)å³å¯è¾“å‡ºç½‘æ ¼åŒ–ä»¥åçš„å›¾ç‰‡. tensor(Tensor/list): 4Dçš„ mini-batch Tensor, Shapeä¸º (NÃ—CÃ—HÃ—W), æˆ–è€…æ˜¯åŒç»´åº¦çš„list. torchvision.transformstorchvision.transforms.Compose 1 2 3 4 5 6 7 class torchvision.transforms.Compose(transforms)# ä½¿ç”¨trans.Compose([transforms.CenterCrop(10),transforms.ToTensor(),]) å°†å¤šä¸ªtransformsæ“ä½œç»„åˆèµ·æ¥, æ³¨æ„å‚æ•°æ˜¯åˆ—è¡¨å½¢å¼ Transforms on PIL Image# cv2 image to PIL Image# skimage to PIL Image æ³¨æ„, ä»¥ä¸‹æ“ä½œä½œç”¨åœ¨PIL Imageä¸Šçš„ CenterCropclass torchvision.transform.CenterCrop(size) sizeå‚æ•°è¡¨ç¤ºè¾“å‡ºçš„å›¾è°±çš„å¤§å°, å¦‚æœåªä¼ å…¥äº†ä¸€ä¸ªæ•°å­—, åˆ™è¯¥æ•°å­—æ—¢è¡¨ç¤ºé«˜åº¦, åˆè¡¨ç¤ºå®½åº¦. Resizeclass torchvision.transforms.Resize(size, interpolation=2) size: æœŸæœ›çš„è¾“å‡ºsize. interpolation: æ’å€¼æ–¹æ³•, é»˜è®¤ä¸ºåŒçº¿æ€§æ’å€¼ ToTensor 1 class torchvision.transforms.ToTensor å°†ä¸€ä¸ªPIL Imageæˆ–è€…numpy.ndarray (HÃ—WÃ—C,[0, 255])è½¬æ¢æˆtorch.FloatTensor (CÃ—HÃ—W, [0.0, 1.0]). RandomHorizontalFliptransforms.RandomHorizontalFlip(p=0.5) åœ¨ç»™å®šæ¦‚ç‡ä¸‹å¯¹PIL Imageéšæœºæ‰§è¡Œæ°´å¹³ç¿»è½¬æ“ä½œ RandomResizedCroptorch.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333), interpolation=2) å¯¹PIL Imageéšæœºæ‰§è¡Œå‰ªè£æ“ä½œ(æŒ‰ç…§scaleå’Œratioçš„åŒºé—´å‰ªè£), ç„¶åå°†å‰ªè£åçš„å›¾ç‰‡æ”¾ç¼©éƒ½æœŸæœ›çš„å°ºå¯¸(é»˜è®¤æ’å€¼ä¸ºåŒçº¿æ€§æ’å€¼) size: æœŸæœ›å¾—åˆ°çš„å°ºå¯¸ scale: å‰ªè£çš„é¢ç§¯æ¯”ä¾‹(ç›¸å¯¹äºåŸå§‹å›¾) ratio: å‰ªè£çš„å®½é«˜æ¯” interpolation: é»˜è®¤ä¸º:PIL.Image.BILINEAR Transforms on torch.*Tensoræ³¨æ„, ä»¥ä¸‹æ“ä½œæ˜¯ä½œç”¨åœ¨tensorä¸Šçš„ Normalize 1 class torchvision.transforms.Normalize(mean, std) å°†å›¾ç‰‡tensoræŒ‰ç…§å‡å€¼meanå’Œæ ‡å‡†å·®stdè¿›è¡Œå½’ä¸€åŒ–, å¯¹äºnä¸ªchannels, æœ‰ mean=(M1, â€¦, Mn), std=(S1,â€¦,Sn).æ³¨æ„, è¿™ä¸ªå½’ä¸€åŒ–æ“ä½œæ˜¯åŸåœ°è¿›è¡Œçš„ torchvision.datasetsImageFolderclass torchvision.datasets.ImageFolder(root, transform=None, target_transform=None, loader=function default_loader) ä¸€ä¸ªä¸€èˆ¬åŒ–çš„æ•°æ®åŠ è½½å™¨, ä¸»è¦é’ˆå¯¹å¦‚ä¸‹æ•°æ®æ’åˆ—æ ¼å¼: root/dog/x.pngroot/dog/y.pngroot/dog/z.png...root/cat/123.pngroot/cat/nsdf3.pngroot/cat/asd932_.png root: æ ¹ç›®å½•è·¯å¾„ transform(callable,optional): å¯¹å›¾ç‰‡è¦åšçš„å˜æ¢æ“ä½œ target_transform(callable,optional): å¯¹targetè¦åšçš„å˜æ¢æ“ä½œ loader: ç”¨äºåŠ è½½ç»™å®šè·¯å¾„å›¾ç‰‡çš„å‡½æ•°å±æ€§ï¼š classes(list): è¿”å›ç±»åˆ«çš„åå­—åˆ—è¡¨ class_names class_to_idx(dict): ä»¥å­—å…¸çš„å½¢å¼è¿”å›(class_name, class_index) imgs(list): è¿”å›å…ƒç»„åˆ—è¡¨: (image path, class_index)æ–¹æ³•ï¼š getitem(index): æ ¹æ®indexè¿”å›(sample,target)å…ƒç»„. å¯ä»¥ä½¿ç”¨ len(imagefolder) è¿”å›ç±»åˆ«æ•°é‡ sort()sort(dim=None, descending=False) # é»˜è®¤ä¸ºå‡åº, è¿”å›(Tensor, LongTensor) è¯¦è§ torch.sort() torch.distributedtorch.distributed.reduce()inspect æ¨¡å—inspect.signature() # æŸ¥çœ‹å‡½æ•°ç­¾å, python3.6ä»¥ä¸Šinspect.getargspec() # æŸ¥çœ‹å‡½æ•°ç­¾å, python3.6ä»¥ä¸Šinspect.getsource() # è·å–æ¨¡å‹çš„codeinspect.getabsfile() # è·å–æ¨¡å—çš„è·¯å¾„ un normalizemean = torch.tensor([1, 2, 3], dtype=torch.float32)std = torch.tensor([2, 2, 2], dtype=torch.float32)normalize = T.Normalize(mean.tolist(), std.tolist())unnormalize = T.Normalize((-mean / std).tolist(), (1.0 / std).tolist())â€‹~~~s","tags":["æ·±åº¦å­¦ä¹ "],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"globæ¨¡å—","path":"/2019/08/28/20190828-globæ¨¡å—/","content":"globæ¨¡å—æ˜¯Pythonæœ€ç®€å•çš„æ¨¡å—ä¹‹ä¸€, å†…å®¹éå¸¸å°‘, ç”¨å®ƒå¯ä»¥æŸ¥æ‰¾ç¬¦åˆç‰¹å®šè§„åˆ™çš„æ–‡ä»¶è·¯å¾„å, æŸ¥æ‰¾æ–‡ä»¶æ—¶åªä¼šç”¨åˆ°ä¸‰ä¸ªåŒ¹é…ç¬¦: : åŒ¹é…0ä¸ªæˆ–å¤šä¸ªå­—ç¬¦ ? : åŒ¹é…å•ä¸ªå­—ç¬¦ [] : åŒ¹é…æŒ‡å®šèŒƒå›´å†…çš„å­—ç¬¦, å¦‚[0-9]åŒ¹é…æ•°å­— glob.glob()å‚æ•°:_(str): æ–‡ä»¶è·¯å¾„çš„æ­£åˆ™è¡¨è¾¾å¼ è¿”å›å€¼:_(list): ç¬¦åˆæ­£åˆ™è¡¨è¾¾å¼çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ å¤‡æ³¨:è¿”å›æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨, å®ƒåªæœ‰ä¸€ä¸ªå‚æ•°pathname, å®šä¹‰äº†æ–‡ä»¶è·¯å¾„åŒ¹é…çš„è§„åˆ™, è¿™é‡Œå¯ä»¥æ˜¯ç»å¯¹è·¯å¾„æˆ–è€…ç›¸å¯¹è·¯å¾„: import globpathes_list = glob.glob(\"~/Pictures/*.jpg\")# è·å–Picturesä¸‹çš„æ‰€æœ‰å›¾ç‰‡relative_pathes_list = glob.glob(\"../*.py\")# è·å–ä¸Šçº§ç›®å½•ä¸­çš„æ‰€æœ‰.pyæ–‡ä»¶ åœ¨ linux, osx ç³»ç»Ÿä¸­, é€šé…ç¬¦çš„åŒ¹é…æ˜¯å¤§å°å†™åŒºåˆ†çš„, ä¹Ÿå°±æ˜¯éœ€è¦ç‰¹åˆ«æŒ‡å®šå¤§å°å†™: extensions = ['jpg', 'JPG', 'jpeg', 'JPEG'] ä½†æ˜¯åœ¨ windows å½“ä¸­, é€šé…ç¬¦çš„åŒ¹é…æ˜¯ä¸åŒºåˆ†å¤§å°å†™çš„, å› æ­¤åªéœ€è¦æŒ‡å®šå¤§å°å†™ä¸­çš„ä¸€ä¸ªå³å¯, ä¸¤ä¸ªéƒ½æŒ‡å®šçš„è¯, ä¼šå‡ºç°é‡å¤çš„æƒ…å†µ extensions = ['jpg', 'jpeg'] glob.iglobè·å–ä¸€ä¸ªå¯éå†çš„å¯¹è±¡, ä½¿ç”¨å®ƒå¯ä»¥é€ä¸ªè·å–åŒ¹é…çš„æ–‡ä»¶è·¯å¾„å. ä¸glob.glob()çš„åŒºåˆ«æ˜¯: glob.glob()ä¼šåŒæ—¶è·å–åˆ°æ‰€æœ‰çš„åŒ¹é…è·¯å¾„, è€Œglob.iglob()ä¸€æ¬¡åªè·å–ä¸€ä¸ªåŒ¹é…è·¯å¾„. f = glob.iglob(\"../*.py\")print f # generator object iglob at 0x00B9FF80for py in f: print(py)","tags":["Python"],"categories":["Python"]},{"title":"loggingæ¨¡å—","path":"/2019/08/28/20190828-loggingæ¨¡å—/","content":"ç®€å•ä½¿ç”¨import logginglogging.debug(\"debug msg\")logging.info(\"info msg\")logging.warn(\"warn msg\")logging.error(\"error msg\")logging.critical(\"critical msg\") é»˜è®¤æƒ…å†µä¸‹, loggingæ¨¡å—å°†æ—¥å¿—æ‰“å°åˆ°å±å¹•ä¸Š, åªæœ‰æ—¥å¿—çº§åˆ«é«˜äºWARNINGçš„æ—¥å¿—ä¿¡æ¯æ‰å›è¾“å‡º","tags":["Python"],"categories":["Python"]},{"title":"opencvæ¨¡å—","path":"/2019/08/28/20190828-opencvæ¨¡å—/","content":"opencv åŸºç¡€çŸ¥è¯†cv2.imread è¯»å…¥çš„å›¾ç‰‡, å…¶shapeä¸º(h, w, c), é¢œè‰²é€šé“é¡ºåºä¸º (b, g, r) å¸¸ç”¨é¢œè‰²è¯»å–å›¾ç‰‡img = cv2.imread(img_path) ä¿å­˜å›¾ç‰‡cv2.imwrite(save_path, img) æ–‡æœ¬(startX, startY) ä¸ºå·¦ä¸Šè§’åæ ‡ cv2.putText(img, \"text test\", (startX, startY), cv2.FONT_HERSHEY_SIMPLEX, font_size, (B,G,R), thickness) ç”»æ¡†(x,y) ä¸ºå·¦ä¸Šè§’åæ ‡(x+h,y+w) ä¸ºå³ä¸‹è§’åæ ‡ cv2.rectangle(img,(x,y), (x+h,y+w), (0,255,0), thickness) waitKey()keypress = cv2.waitKey(200) # 200ä¸ºå½“å‰å›¾ç‰‡çš„æ˜¾ç¤ºæŒç»­æ—¶é—´if keypress == ord('c') # keypressä¸ºæŒ‰é”®çš„æ•´æ•°å½¢å¼, æ‰€ä»¥éœ€è¦ç”¨ordå°†å­—ç¬¦ç±»å‹è½¬æ¢if cv2.waitKey(200) == 27: # Decimal 27 = Esc opencvä¸numpyopencvçš„åŸºç¡€ç±»å‹ä¸ºnumpy.ndarray, å› æ­¤å¯ä»¥ç›´æ¥ä½¿ç”¨ ndarray çš„ä¸€äº›å±æ€§çš„æ–¹æ³• import cv2img = cv2.imread('./test.jpg')print(type(img)) # class 'numpy.ndarray'print(img.shape) # (500, 1069, 3) (é«˜, å®½, é€šé“) åˆ©ç”¨ cv2.merge æ–¹æ³•å°† numpy.ndarray æ•°æ®è½¬æ¢æˆopencvçš„å›¾ç‰‡æ•°æ®: # å›¾ç‰‡çš„åˆ†è¾¨ç‡ä¸º300*200(å®½*é«˜)ï¼Œè¿™é‡Œb, g, rè®¾ä¸ºéšæœºå€¼ï¼Œæ³¨æ„dtypeå±æ€§b = np.random.randint(0, 255, (200, 300), dtype=np.uint8)g = np.random.randint(0, 255, (200, 300), dtype=np.uint8)r = np.random.randint(0, 255, (200, 300), dtype=np.uint8)# åˆå¹¶é€šé“ï¼Œå½¢æˆå›¾ç‰‡img = cv2.merge([b, g, r]) # opencvçš„é€šé“æ˜¯båœ¨æœ€å‰,råœ¨æœ€å# æ˜¾ç¤ºå›¾ç‰‡cv2.imshow('test', img)cv2.waitKey(0)cv2.destroyWindow('test') é€šé“çš„æ‹†åˆ†ä¸åˆå¹¶æ‹†åˆ†: cv2.splitåˆå¹¶: cv2.merge # å›¾ç‰‡çš„åˆ†è¾¨ç‡ä¸º800*200(å®½*é«˜)ï¼Œè¿™é‡Œb, g, rè®¾ä¸ºéšæœºå€¼ï¼Œæ³¨æ„dtypeå±æ€§b = np.random.randint(0, 255, (200, 800), dtype=np.uint8)g = np.random.randint(0, 255, (200, 800), dtype=np.uint8)r = np.random.randint(0, 255, (200, 800), dtype=np.uint8)# åˆå¹¶é€šé“ï¼Œå½¢æˆå›¾ç‰‡img = cv2.merge([b, g, r]) # opencvçš„é€šé“æ˜¯båœ¨æœ€å‰,råœ¨æœ€å# æ˜¾ç¤ºå›¾ç‰‡cv2.imshow('test', img)cv2.waitKey(0)cv2.destroyWindow('test')# æ‹†åˆ†é€šé“, æ¯ä¸ªé€šé“éƒ½å˜æˆäº†å•é€šé“æ•°ç»„[blue, green, red] = cv2.split(img) å°† BGR è½¬æ¢æˆ RGB é€šé“é¡ºåº# æ–¹æ³•ä¸€:rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)# æ–¹æ³•äºŒ:rgb_img = img[:, :, [2, 1, 0]] # img[h,w,v]rgb_img = img[:, :, ::-1] PIL ä¸ cv2 æ ¼å¼äº’ç›¸è½¬æ¢PIL.Imageè¯»å…¥çš„å›¾ç‰‡æ•°æ®ç±»å‹ä¸æ˜¯ numpy æ•°ç»„, å®ƒçš„sizeå±æ€§ä¸º (w, h), åˆ©ç”¨np.arrayè½¬æ¢æˆ numpy æ•°ç»„å, å®ƒçš„é€šé“é¡ºåºä¸º (r, g, b) from PIL import Imageimport numpy as np# PIL to cv2pil_img = Image.open(img_path)print(pil_img.size) # (w, h)np_img = np.array(pil_img)cv2_img = np_img[:, :, ::-1] # äº¤æ¢é€šé“# cv2 to PILpil_img = Image.fromarray(cv2_img[:, :, ::-1]) ç”¨matplotlibæ˜¾ç¤ºå›¾åƒb,g,r=cv2.split(img)img2=cv2.merge([r,g,b])plt.imshow(img2)plt.show() æˆªå–å­å›¾# å·²çŸ¥å­å›¾å·¦ä¸Šè§’åæ ‡ (x1, y1), å³ä¸‹è§’åæ ‡(x2, y2)crop_img = img[y1:y2, x1:x2, :] opencv æ ¸å¿ƒç®—æ³•cv2import cv2image_path = './test.jpg'src_image = cv2.imread(image_path) # è¯»å–å›¾ç‰‡size = src_image.shape # è·å–å›¾ç‰‡çš„å°ºå¯¸, è¿”å›ä¸€ä¸ªå…ƒç»„: (height, width, depth)copy_image = src_image.copy() # å¤åˆ¶å›¾ç‰‡cv2.imwrite('./dst_test.jpg', copy_image) # ä¿å­˜å›¾ç‰‡cv2.imshow('image', src_image) # æ˜¾ç¤ºå›¾ç‰‡# åˆ©ç”¨ä¸‹æ ‡è®¿é—®æŒ‡å®šåƒç´ for x in range(src_image.shape[0]): # ä»¥è¡Œä¸ºä¸», è¡Œæ•°=å›¾ç‰‡height for y in range(src_image.shape[1]): # åˆ—æ•° = å›¾ç‰‡width src_image[x,y] = (255,0,255) # (blue, green, red) å€¼è¶Šé«˜è¡¨ç¤ºå¯¹åº”é¢œè‰²è¶Šæ˜¾è‘—, å…¨0ä¸ºé»‘, å…¨255ä¸ºç™½","tags":["è®¡ç®—æœºè§†è§‰"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"pythonæ‰‹å†Œ","path":"/2019/08/28/20190828-pythonæ‰‹å†Œ/","content":"æœ€è¿‘åœ¨å­¦ä¹ æŸ¥æ‰¾èµ„æ–™çš„è¿‡ç¨‹ä¹‹ä¸­ï¼Œçœ‹åˆ°äº†å¤§ä½¬çš„åšå®¢ï¼Œè§‰å¾—å†™å¾—å¾ˆå¥½ï¼Œä¹Ÿæ˜¯æˆ‘ç°åœ¨æ‰€æ¬ ç¼ºçš„ï¼Œæ‰€ä»¥ä¸‹é¢å…ˆå¯¹å¤§ä½¬çš„åšå®¢è¿›è¡Œå¤ç°ã€‚ä»¥ä¾›è‡ªå·±æ—¥åå¤ä¹ ï¼ŒæŸ¥æ‰¾ï¼Œå®Œå–„æˆè‡ªå·±çš„ä¸œè¥¿ã€‚ å­—ç¬¦ä¸²å›ºå®šå­—æ•°ï¼Œä¸è¶³çš„ç©ºæ ¼è¡¥é½str.ljust(10) # å·¦å¯¹é½ å­—ç¬¦ä¸²é•¿10ä½rjustï¼Œljustå’Œcenterä¸‰ä¸ªæ–¹æ³•æ¥ç»™å­—ç¬¦ä¸²è¡¥å…¨ç©ºæ ¼rjustï¼Œå‘å³å¯¹å…¶ï¼Œåœ¨å·¦è¾¹è¡¥ç©ºæ ¼ljustï¼Œå‘å·¦å¯¹å…¶ï¼Œåœ¨å³è¾¹è¡¥ç©ºæ ¼centerï¼Œè®©å­—ç¬¦ä¸²å±…ä¸­ï¼Œåœ¨å·¦å³è¡¥ç©ºæ ¼ æ’åºsorted: è¿”å›ä¸€ä¸ªæ–°çš„ listlist.sort(): æ”¹å˜ list è‡ªèº«çš„å€¼reverse å‚æ•°: é»˜è®¤ä¸º False, å‡åº, True æ—¶å˜ä¸ºé™åº åˆ—è¡¨å¾ªç¯åˆ é™¤åˆ—è¡¨å…ƒç´ å¸¸è§é”™è¯¯: ç›´æ¥åˆ é™¤, æˆ–è€…æ­£åºåˆ é™¤ æ­£ç¡®åšæ³•:1.ä½¿ç”¨ pop, å€’åºåˆ é™¤ for i in range(len(list)): list.pop() 2.ä½¿ç”¨åˆ‡ç‰‡, éå†æ‹·è´åˆ—è¡¨, æ“ä½œåŸå§‹åˆ—è¡¨, ç”¨ remove åˆ é™¤, remove ä¼šæ“ä½œé¦–ä¸ªé‡åˆ°çš„åŒ¹é…å…ƒç´ , ç›¸ç­‰å…ƒç´ åˆ é™¤, åˆ é™¤å“ªä¸ªéƒ½ä¸€æ · for x in enumerate(a[::]): a.remove(x)for x in enumerate(a[::-1]): a.remove(x) éå†åˆ—è¡¨:zz_list = ['a', 'b', 'c', 'd']for index in list: print(index) # 0 # 1 # 2 # 3for index in range(len(list)): print(index) # 0 # 1 # 2 # 3for index, val in enumerate(list): print(index, val) # 0 a # 1 b # 2 c # 3 d# è®¾ç½®éå†çš„å¼€å§‹åºå·, valçš„è¾“å‡ºä¸å˜for i, val in enumerate(list, 2): print(index, val) # 2 a # 3 b # 4 c # 5 d append() æ–¹æ³•è¿½åŠ å•ä¸ªå…ƒç´  extend() æ–¹æ³•extend()å‡½æ•°ç”¨äºåœ¨åˆ—è¡¨æœ«å°¾ä¸€æ¬¡æ€§è¿½åŠ å¦ä¸€ä¸ªåºåˆ—ä¸­çš„å¤šä¸ªå€¼(ç”¨æ–°åˆ—è¡¨æ‰©å±•åŸæ¥çš„åˆ—è¡¨).è¯¥æ–¹æ³•æ²¡æœ‰è¿”å›å€¼, ä¼šç›´æ¥åœ¨å·²ç»å­˜åœ¨çš„åˆ—è¡¨ä¸­æ·»åŠ æ–°çš„åˆ—è¡¨å†…å®¹, extendå’Œ+=çš„ä½œç”¨å·®ä¸å¤š a= [[1,2,3],[4,5,6]]b= [['a','b','c'],['d','e','f']]a.extend(b)print(a)# [[1, 2, 3], [4, 5, 6], ['a', 'b', 'c'], ['d', 'e', 'f']] åºåˆ—åˆ‡ç‰‡(åŒå†’å·)Pythonåºåˆ—åˆ‡ç‰‡åœ°å€å¯ä»¥å†™ä¸º [å¼€å§‹(åŒ…å«) : ç»“æŸ(ä¸åŒ…å«) : æ­¥é•¿]. å½“å¼€å§‹çœç•¥çš„æ—¶å€™, é»˜è®¤ä»ç¬¬0é¡¹å¼€å§‹, å½“ç»“å°¾çœç•¥çš„æ—¶å€™, é»˜è®¤åˆ°æ•°ç»„æœ€å, å½“æ­¥é•¿çœç•¥çš„æ—¶å€™, é»˜è®¤ä¸º1. æ­¥é•¿å¯ä»¥ä¸ºè´Ÿæ•°, ä»£è¡¨ä»å³å‘å·¦å–æ•°. a = range(10) # a = [0, 1, 2, 3, 4, 5, 6, 7, 8 ,9]a[0:9:1] # [0, 1, 2, 3, 4, 5, 6, 7, 8] åŒ…å«å¼€å§‹ä¸‹æ ‡, ä¸åŒ…å«ç»“æŸä¸‹æ ‡a[1::2] # [1, 3, 5, 7, 9]a[::3] # [0, 3, 6, 9]a[::-1] # [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]a[::-2] # [9, 7, 5, 3, 1] update() æ–¹æ³•dict.update(dict2) å°† dict2 ä¸­çš„é”®å€¼æ›´æ–°åˆ° dict ä¸­, å¯¹äºå­˜åœ¨çš„åˆ™è¦†ç›–åŸå€¼, å¯¹äºä¸å­˜åœ¨çš„åˆ™æ·»åŠ æ–°çš„é”®å€¼. å®ä¾‹#!/usr/bin/pythondict = {'Name': 'Zara', 'Age': 7}dict2 = {'Sex': 'female' }dict.update(dict2)print \"Value : %s\" % dict ä»¥ä¸Šå®ä¾‹è¾“å‡ºç»“æœä¸ºï¼š Value : {'Age': 7, 'Name': 'Zara', 'Sex': 'female'} å­—å…¸éå†å­—å…¸: zz_dict = {'x': 1, 'y':2, 'z':3} éå†keys: # è¾“å‡ºå‡ä¸º: x y zfor key in zz_dict: print(key)for key in zz_dict.iterkeys(): print(key)for key in zz_dict.keys(): print(key) éå†values: # è¾“å‡ºå‡ä¸º 1 2 3for value in zz_dict.itervalues(): print(value)for value in zz_dict.values(): print(value) éå†keyså’Œvalues # è¾“å‡ºä¸º: x corresponds to 1 (å…¶ä½™ä¸¤ä¸ªä¹Ÿä¸€æ ·)for key, value in zz_dict.iteritems(): # python3 æ²¡æœ‰iteritems print(key, \"corresponds to\", value)for key, value in zz_dict.items(): print(key, \"corresponds to\", value) å­—ç¬¦ä¸²åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦ä¸ºå­—æ¯æˆ–è€…æ•°å­—str.isalnum() å­—æ¯æˆ–æ•°å­—str.isalpha() å­—æ¯str.isdigit() æ•°å­—str.isspace() ç©ºç™½ç¬¦, \\t, , \\r isdigit() å’Œ isnumeric() çš„åŒºåˆ« num = \"1\" #unicodenum.isdigit() # Truenum.isdecimal() # Truenum.isnumeric() # Truenum = \"1\" # å…¨è§’num.isdigit() # Truenum.isdecimal() # Truenum.isnumeric() # Truenum = b\"1\" # bytenum.isdigit() # Truenum.isdecimal() # AttributeError 'bytes' object has no attribute 'isdecimal'num.isnumeric() # AttributeError 'bytes' object has no attribute 'isnumeric'num = \"IV\" # ç½—é©¬æ•°å­—num.isdigit() # Truenum.isdecimal() # Falsenum.isnumeric() # Truenum = \"å››\" # æ±‰å­—num.isdigit() # Falsenum.isdecimal() # Falsenum.isnumeric() # True isdigit()True: Unicodeæ•°å­—ï¼Œbyteæ•°å­—ï¼ˆå•å­—èŠ‚ï¼‰ï¼Œå…¨è§’æ•°å­—ï¼ˆåŒå­—èŠ‚ï¼‰ï¼Œç½—é©¬æ•°å­—False: æ±‰å­—æ•°å­—Error: æ—  isdecimal()True: Unicodeæ•°å­—ï¼Œï¼Œå…¨è§’æ•°å­—ï¼ˆåŒå­—èŠ‚ï¼‰False: ç½—é©¬æ•°å­—ï¼Œæ±‰å­—æ•°å­—Error: byteæ•°å­—ï¼ˆå•å­—èŠ‚ï¼‰ isnumeric()True: Unicodeæ•°å­—ï¼Œå…¨è§’æ•°å­—ï¼ˆåŒå­—èŠ‚ï¼‰ï¼Œç½—é©¬æ•°å­—ï¼Œæ±‰å­—æ•°å­—False: æ— Error: byteæ•°å­—ï¼ˆå•å­—èŠ‚ï¼‰ str.rstrip()å‚æ•°:chars: æŒ‡å®šåˆ é™¤çš„å­—ç¬¦(é»˜è®¤ä¸ºç©ºæ ¼æˆ–æ¢è¡Œç¬¦) è¿”å›å€¼:è¿”å›åˆ é™¤æŒ‡å®šå­—ç¬¦åçš„æ–°å­—ç¬¦ä¸² å¤‡æ³¨:åˆ é™¤å­—ç¬¦ä¸²æœ«å°¾çš„æŒ‡å®šå­—ç¬¦(é»˜è®¤ä¸ºç©ºæ ¼æˆ–æ¢è¡Œç¬¦) str.rstrip([chars]) str.strip()å‚æ•°chars â€” ç§»é™¤å­—ç¬¦ä¸²å¤´å°¾æŒ‡å®šçš„å­—ç¬¦åºåˆ—ã€‚è¿”å›å€¼è¿”å›ç§»é™¤å­—ç¬¦ä¸²å¤´å°¾æŒ‡å®šçš„å­—ç¬¦ç”Ÿæˆçš„æ–°å­—ç¬¦ä¸²ã€‚å¤‡æ³¨: str.strip([chars]) str.split()å‚æ•° str â€” åˆ†éš”ç¬¦ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰çš„ç©ºå­—ç¬¦ï¼ŒåŒ…æ‹¬ç©ºæ ¼ã€æ¢è¡Œ( )ã€åˆ¶è¡¨ç¬¦(\\t)ç­‰ã€‚ num â€” åˆ†å‰²æ¬¡æ•°ã€‚é»˜è®¤ä¸º -1, å³åˆ†éš”æ‰€æœ‰ã€‚è¿”å›å€¼ è¿”å›åˆ†å‰²åçš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚ str.split(str=\"\", num=string.count(str)). æ–‡ä»¶reduce() å‡½æ•°reduce() å‡½æ•°ä¼šå¯¹å‚æ•°åºåˆ—ä¸­å…ƒç´ è¿›è¡Œç´¯ç§¯ã€‚å‡½æ•°å°†ä¸€ä¸ªæ•°æ®é›†åˆï¼ˆé“¾è¡¨ï¼Œå…ƒç»„ç­‰ï¼‰ä¸­çš„æ‰€æœ‰æ•°æ®è¿›è¡Œä¸‹åˆ—æ“ä½œï¼šç”¨ä¼ ç»™ reduce ä¸­çš„å‡½æ•° functionï¼ˆæœ‰ä¸¤ä¸ªå‚æ•°ï¼‰å…ˆå¯¹é›†åˆä¸­çš„ç¬¬ 1ã€2 ä¸ªå…ƒç´ è¿›è¡Œæ“ä½œï¼Œå¾—åˆ°çš„ç»“æœå†ä¸ç¬¬ä¸‰ä¸ªæ•°æ®ç”¨ function å‡½æ•°è¿ç®—ï¼Œæœ€åå¾—åˆ°ä¸€ä¸ªç»“æœã€‚reduce() å‡½æ•°è¯­æ³•ï¼š reduce(function, iterable[, initializer]) å‚æ•° function â€” å‡½æ•°ï¼Œæœ‰ä¸¤ä¸ªå‚æ•° iterable â€” å¯è¿­ä»£å¯¹è±¡ initializer â€” å¯é€‰ï¼Œåˆå§‹å‚æ•°è¿”å›å€¼ è¿”å›å‡½æ•°è®¡ç®—ç»“æœå®ä¾‹ ```pythondef add(x, y) : # ä¸¤æ•°ç›¸åŠ ... return x + y... reduce(add, [1,2,3,4,5]) # è®¡ç®—åˆ—è¡¨å’Œï¼š1+2+3+4+515 reduce(lambda x, y: x+y, [1,2,3,4,5]) # ä½¿ç”¨ lambda åŒ¿åå‡½æ•°15 zip() å‡½æ•°zip() å‡½æ•°ç”¨äºå°†å¯è¿­ä»£çš„å¯¹è±¡ä½œä¸ºå‚æ•°, å°†å¯¹è±¡ä¸­å¯¹åº”çš„å…ƒç´ æ‰“åŒ…æˆä¸€ä¸ªä¸ª å…ƒç»„ ,ç„¶åè¿”å›æœ‰è¿™äº›å…ƒç»„ç»„æˆçš„ å¯¹è±¡. ( ç›¸æ¯”äºpython2ä¸­è¿”å›åˆ—è¡¨çš„æ–¹å¼, è¿™æ ·åšçš„å¥½å¤„æ˜¯èŠ‚çº¦äº†ä¸å°‘çš„å†…å­˜ )å¯ä»¥ç”¨list()è½¬æ¢æˆ–è€…dict()è½¬æ¢å°†å¯¹è±¡è½¬æ¢æˆç›¸åº”çš„æ•°æ®ç±»å‹å¦‚æœå„ä¸ªè¿­ä»£å™¨çš„å…ƒç´ ä¸ªæ•°ä¸ä¸€è‡´, åˆ™è¿”å›åˆ—è¡¨é•¿åº¦ä¸æœ€çŸ­çš„å¯¹è±¡ç›¸åŒ, å¤šå‡ºæ¥çš„éƒ¨åˆ†ä¼šè¢«èˆå¼ƒ, åˆ©ç”¨*å·æ“ä½œç¬¦, å¯ä»¥å°†å…ƒç»„è§£å‹æˆåˆ—è¡¨. a = [1,2,3]b = [4,5,6]c = ['a','b','c','d','e','f']zip_ab = zip(a,b)print(zip_ab) # zip object at 0x104605348print(dict(zip_ab)) # {1: 4, 2: 5, 3: 6}# !!!æ³¨æ„, ä¸€æ—¦å°†zip_abè½¬æ¢æˆdictä»¥å, zip_abå†…éƒ¨å°±ä¸ºç©ºäº†!! ä¾‹å¦‚, å†æ¬¡è°ƒç”¨ä¸Šé¢çš„è¯­å¥:print(dict(zip_ab)) # {}# ä½†æ˜¯zip_abå¯¹è±¡æœ¬èº«ä¸ä¼šæ¶ˆå¤±, åœ°å€ä»ç„¶ä¸å˜print(zip_ab) # zip object at 0x104605348zip_abc = zip(a,b,c) # æ³¨æ„, ä¸‰ä¸ªå…ƒç´ çš„zipæ˜¯ä¸èƒ½è½¬æ¢æˆdictç±»å‹çš„print(zip_abc) # zip object at 0x1046054c8print(list(zip_abc)) # [(1, 4, 'a'), (2, 5, 'b'), (3, 6, 'c')]zip_abc = zip(a,b,c)z_a, z_b, z_c = zip(*zip_abc) # åˆ©ç”¨zip(*)å¯ä»¥å°†zipå¯¹è±¡é‡æ–°è§£å‹, è¿”å›ç±»å‹æ˜¯å…ƒç»„print(z_a) # (1,2,3)print(z_b) # (4,5,6)print(z_c) # ('a','b','c') getattr() å‡½æ•°getattr()å‡½æ•°ç”¨äºè¿”å›ä¸€ä¸ªå¯¹è±¡çš„å±æ€§å€¼, è¯­æ³•å¦‚ä¸‹ getattr(object, name[, default]) å‚æ•°ï¼š object: å¯¹è±¡ name: å­—ç¬¦ä¸², å¯¹è±¡å±æ€§ default: é»˜è®¤è¿”å›å€¼, å¦‚æœä¸æä¾›è¯¥å‚æ•°, åœ¨æ²¡æœ‰å¯¹åº”å±æ€§æ—¶, å°†è§¦å‘Attributerrorå®ä¾‹class A(object):... bar = 1... a = A() getattr(a, 'bar') # è·å–å±æ€§ bar å€¼1 getattr(a, 'bar2') # å±æ€§ bar2 ä¸å­˜åœ¨ï¼Œè§¦å‘å¼‚å¸¸Traceback (most recent call last):File \"stdin\", line 1, in moduleAttributeError: 'A' object has no attribute 'bar2' getattr(a, 'bar2', 3) # å±æ€§ bar2 ä¸å­˜åœ¨ï¼Œä½†è®¾ç½®äº†é»˜è®¤å€¼3 dir() å‡½æ•°å¯ä»¥æŸ¥çœ‹æŸä¸ªç±»çš„æ‰€æœ‰æ–¹æ³•å’Œå±æ€§ members = [attr for attr in dir(classA)] _var: åœ¨ä¸€ä¸ªæ¨¡å—ä¸­ä»¥å•ä¸‹åˆ’çº¿å¼€å¤´çš„å˜é‡å’Œå‡½æ•°ä¼šè¢«é»˜è®¤å½“åšå†…éƒ¨å‡½æ•°, åœ¨ä½¿ç”¨from a_module import * å¯¼å…¥æ—¶, è¿™éƒ¨åˆ†å˜é‡å’Œå‡½æ•°ä¸ä¼šè¢«å¯¼å…¥. ä¸è¿‡å¦‚æœä½¿ç”¨import a_moduleå¯¼å…¥æ¨¡å—æ—¶, ä»ç„¶å¯ä»¥ç”¨a_module._varçš„å½¢å¼è®¿é—®è¯¥å˜é‡æˆ–å‡½æ•° var_: æœ‰æ—¶å€™, ä¸€ä¸ªå˜é‡çš„æœ€é€‚åˆçš„åç§°å·²ç»è¢«å¦ä¸€ä¸ªå…³é”®å­—æ‰€å ç”¨. åœ¨è¿™ç§æƒ…å†µä¸‹, å¯ä»¥åœ¨åç§°çš„æœ«å°¾é™„åŠ ä¸€ä¸ªä¸‹åˆ’çº¿æ¥è§£å†³å†²çª. __var: åŒä¸‹åˆ’çº¿å‰ç¼€ä¼šå¯¼è‡´Pythonè§£é‡Šå™¨é‡å†™å±æ€§åç§°, ä»¥é¿å…å­ç±»ä¸­çš„å‘½åå†²çª. ä¸¾ä¾‹æ¥è¯´, å¦‚æœåœ¨class Testä¸­æœ‰ä¸€ä¸ªæˆå‘˜__x, é‚£ä¹ˆå½“åˆ©ç”¨å†…ç½®å‡½æ•°dir(Test)æ¥æŸ¥çœ‹ç±»çš„å±æ€§æ—¶, ä¼šå‘ç°__xè¢«è§£é‡Šå™¨é‡å‘½åä¸º_Test__x. åŒä¸‹åˆ’çº¿çš„åç§°ä¿®é¥°åŒæ ·ä¹Ÿé€‚ç”¨äºæ–¹æ³•åç§°. var: åŒä¸‹åˆ’çº¿å¼€å¤´å’Œç»“å°¾çš„æ˜¯ä¸€äº› Python çš„ç‰¹æ®Šå¯¹è±¡, å¦‚ç±»æˆå‘˜çš„ init, del, name, call ç­‰. Python å®˜æ–¹æ¨èæ°¸è¿œä¸è¦è®²è¿™æ ·çš„å‘½åæ–¹å¼åº”ç”¨äºè‡ªå·±çš„å˜é‡æˆ–å‡½æ•°. æœ‰ä¸€ç§è¯´æ³•æ˜¯è¯´åŒä¸‹åˆ’çº¿å»ºè®®ä¸ºç±»çš„ç§æœ‰æˆå‘˜, ä½†æ˜¯ PEP8 å½“å‰çš„å®˜æ–¹ç‰ˆæœ¬ä¸­å¹¶æ²¡æœ‰æ˜è¯´. _: æœ‰æ—¶å€™æˆ‘ä»¬ä¼šç”¨ä¸€ä¸ªç‹¬ç«‹çš„ä¸‹åˆ’çº¿ä½œä¸ºä¸€ä¸ªåå­—, è¿™é€šå¸¸æ˜¯ç”¨æ¥æŒ‡ç¤ºæŸä¸ªå˜é‡æ—¶ä¸´æ—¶çš„æˆ–è€…æ— å…³ç´§è¦çš„. ç±»çš„ç‰¹æ®Šæ–¹æ³•call()åœ¨ Python ä¸­, å‡½æ•°å®é™…ä¸Šä¹Ÿæ˜¯ä¸€ä¸ªå¯¹è±¡: 1 2 3 f = abs print(f.name) # â€˜absâ€™ print(f(-123)) # 123 ä»ä¸Šé¢å¯ä»¥çœ‹å‡º, å‡½æ•°æ˜¯ä¸€ä¸ªå¯¹è±¡, å½“å®ƒèµ‹ç»™å¦ä¸€ä¸ªå˜é‡æ—¶, è¯¥å˜é‡ä¹Ÿæ˜¯ä¸€ä¸ªå‡½æ•°å¯¹è±¡, å¯ä»¥èµ·åˆ°ä¸åŸå‡½æ•°ç›¸åŒçš„æ•ˆæœ. åœ¨ Python ä¸­, ä¸€ä¸ªç±»å®ä¾‹ä¹Ÿå¯ä»¥å˜æˆä¸€ä¸ªå¯è°ƒç”¨å¯¹è±¡, åªéœ€è¦å®ç°ä¸€ä¸ªç‰¹æ®Šæ–¹æ³• __call__() å³å¯. ä¸‹é¢æˆ‘ä»¬ä¸¾ä¾‹æŠŠ Person ç±»å˜æˆä¸€ä¸ªå¯è°ƒç”¨å¯¹è±¡: ```python class Person(object): def __init__(self, name, gender): self.name = name self.gender = gender def __call__(self, friend): print(\"name:\", self.name) print(\"friend:\", friend) æ¥ä¸‹æ¥æˆ‘ä»¬å°±å¯ä»¥å°† Person ç±»çš„å®ä¾‹å¯¹è±¡å½“åšä¸€ä¸ªå‡½æ•°æ¥ä½¿ç”¨, å¦‚ä¸‹æ‰€ç¤º: p = Person('Bob', 'male')p('Tim')# name: Bob# friend: Tim getitem()å‡¡æ˜¯åœ¨ç±»ä¸­å®šä¹‰äº† getitem() æ–¹æ³•, é‚£ä¹ˆå®ƒçš„å®ä¾‹å¯¹è±¡å°±æ˜¯å¯ä»¥é€šè¿‡ [] æ“ä½œç¬¦æ¥è®¿é—®æŒ‡å®šçš„æˆå‘˜æˆ–è¿›è¡Œç‰¹å®šçš„è¡Œä¸º, å¤§å¤šæ•°æƒ…å†µä¸‹ä¼šå°†è¯¥æ–¹æ³•å®ç°æˆé€šè¿‡ç´¢å¼•æ¥æ–¹æ³•å…ƒç´ çš„å½¢å¼. class DataBase(object): def __init__(self): super(DataBase, self).__init__() self.vals = [1,2,3,4,5] def __getitem__(self, key): return self.vals[key] setitem()ä½¿å¾—å¯ä»¥é€šè¿‡ A[3] = 4, B[â€œaâ€] = 5 ç­‰æ–¹å¼æ¥å¯¹ç±»ä¸­çš„å…ƒç´ è¿›è¡Œèµ‹å€¼ file()æŸ¥çœ‹æ¨¡å—çš„è·¯å¾„ len()ä½¿å¾—ç±»å¯¹è±¡å¯ä»¥ä½¿ç”¨ Python çš„å†…å»ºæ–¹æ³• len(), è¿”å›ä½ è‡ªå®šä¹‰çš„æ•°å€¼. class DictDemo: def __init__(self,key,value): self.dict = {} self.dict[key] = value def __getitem__(self,key): return self.dict[key] def __setitem__(self,key,value): self.dict[key] = value def __len__(self): return len(self.dict)dictDemo = DictDemo('key0','value0')print(dictDemo['key0']) #value0dictDemo['key1'] = 'value1'print(dictDemo['key1']) #value1print(len(dictDemo)) #2 repr()class Test(object): def __init__(self, value='hello, world!'): self.data = value t = Test() t__main__.Test at 0x7fa91c307190 print t__main__.Test object at 0x7fa91c307190# çœ‹åˆ°äº†ä¹ˆï¼Ÿä¸Šé¢æ‰“å°ç±»å¯¹è±¡å¹¶ä¸æ˜¯å¾ˆå‹å¥½ï¼Œæ˜¾ç¤ºçš„æ˜¯å¯¹è±¡çš„å†…å­˜åœ°å€# ä¸‹é¢æˆ‘ä»¬é‡æ„ä¸‹è¯¥ç±»çš„__repr__ä»¥åŠ__str__ï¼Œçœ‹çœ‹å®ƒä»¬ä¿©æœ‰å•¥åŒºåˆ«# é‡æ„__repr__class TestRepr(Test): def __repr__(self): return 'TestRepr(%s)' % self.data tr = TestRepr() trTestRepr(hello, world!) print trTestRepr(hello, world!)# é‡æ„__repr__æ–¹æ³•åï¼Œä¸ç®¡ç›´æ¥è¾“å‡ºå¯¹è±¡è¿˜æ˜¯é€šè¿‡printæ‰“å°çš„ä¿¡æ¯éƒ½æŒ‰æˆ‘ä»¬__repr__æ–¹æ³•ä¸­å®šä¹‰çš„æ ¼å¼è¿›è¡Œæ˜¾ç¤ºäº†# é‡æ„__str__calss TestStr(Test): def __str__(self): return '[Value: %s]' % self.data ts = TestStr() ts__main__.TestStr at 0x7fa91c314e50 print ts[Value: hello, world!]# ä½ ä¼šå‘ç°ï¼Œç›´æ¥è¾“å‡ºå¯¹è±¡tsæ—¶å¹¶æ²¡æœ‰æŒ‰æˆ‘ä»¬__str__æ–¹æ³•ä¸­å®šä¹‰çš„æ ¼å¼è¿›è¡Œè¾“å‡ºï¼Œè€Œç”¨printè¾“å‡ºçš„ä¿¡æ¯å´æ”¹å˜äº† str()å‚è§ repr() ä»£ç ç¤ºä¾‹ æ˜Ÿå· **: ä¹˜æ³•**: ä¹˜å¹‚ ç”¨äºå‡½æ•°å‚æ•°å•æ˜Ÿå·: å°†æ‰€æœ‰å‚æ•°ä»¥ å…ƒç»„(tuple) çš„å½¢å¼å¯¼å…¥ def foo(param1, *param2): print(param1) print(param2)foo(1,2,3,4,5)# 1# (2,3,4,5) åŒæ˜Ÿå·: å°†æ‰€æœ‰å‚æ•°ä»¥ å­—å…¸ çš„å½¢å¼å¯¼å…¥ def bar(param1, **param2): print(param1) print(param2)bar(1, a=2, b=3)# 1# {'a': 2, 'b': 3} å½“ç„¶è¿™ä¸¤ä¸ªç”¨æ³•å¯ä»¥åŒæ—¶å‡ºç°åœ¨ä¸€ä¸ªå‡½æ•°ä¸­: def fun(a, b=10, *args, **kwargs): print(a) print(b) print(args) print(kwargs)fun(1,2,3,4,e=5,f=6)# 1# 2# (3,4)# {'e': 5, 'f': 6} globals() å‡½æ•°è¯¥å‡½æ•°ä¼šä»¥å­—å…¸ç±»å‹è¿”å›å½“å‰ä½ç½®çš„å…¨éƒ¨å…¨å±€å˜é‡ stripe()readlines()lambda å‡½æ•°3.6æ–°åŠŸèƒ½ f stringåŒ…çš„å¯¼å…¥æœºåˆ¶æ¨¡å—å’ŒåŒ…çš„å®šä¹‰æ¨¡å—(module): ç”¨æ¥ä»é€»è¾‘ä¸Šç»„ç»‡ Python ä»£ç (å˜é‡, å‡½æ•°, ç±»), é€šå¸¸æ˜¯ä¸€ä¸ª.pyæ–‡ä»¶.åŒ…(package): å®šä¹‰äº†ä¸€ä¸ªç”±æ¨¡å—å’Œå­åŒ…ç»„æˆçš„ Python åº”ç”¨ç¨‹åºæ‰§è¡Œç¯å¢ƒ, æœ¬è´¨ä¸Šå°±æ˜¯ä¸€ä¸ªæœ‰å±‚æ¬¡çš„æ–‡ä»¶ç›®å½•ç»“æœ(å¿…é¡»å¸¦æœ‰ä¸€ä¸ª__init__.pyæ–‡ä»¶) import çš„æœç´¢è·¯å¾„ åœ¨å½“å‰ç›®å½•ä¸‹æœç´¢ åœ¨ç¯å¢ƒå˜é‡PYTHONPATHä¸­æŒ‡å®šçš„è·¯å¾„åˆ—è¡¨ä¸­æœç´¢ åœ¨ Python å®‰è£…è·¯å¾„çš„libåº“ä¸­æœç´¢Python æ‰€æœ‰åŠ è½½çš„æ¨¡å‹ä¿¡æ¯éƒ½å­˜æ”¾åœ¨sys.modulesç»“æ„ä¸­, å½“importä¸€ä¸ªæ¨¡å—æ—¶, ä¼šæŒ‰å¦‚ä¸‹æ­¥éª¤æ¥è¿›è¡Œ: å¦‚æœimport A, æ£€æŸ¥sys.modulesä¸­æ˜¯å¦å·²ç»æœ‰A, å¦‚æœæœ‰åˆ™ä¸åŠ è½½, å¦‚æœæ²¡æœ‰åˆ™ä¸ºAåˆ›å»ºmoduleå¯¹è±¡, å¹¶åŠ è½½A; å¦‚æœæ˜¯from A import B, å…ˆä¸ºAåˆ›å»ºmoduleå¯¹è±¡, å†è§£æA(æ­¤æ—¶ä¼šåŠ è½½å¹¶æ‰§è¡ŒAä¸­çš„æ‰€æœ‰ä»£ç ), ä»ä¸­å¯»æ‰¾Bå¹¶å¡«å……åˆ°Açš„__dict__ä¸­.åœ¨å¯¼å…¥æ¨¡å—çš„æ—¶å€™, æ¨¡å—æ‰€åœ¨æ–‡ä»¶å¤¹ä¼šè‡ªåŠ¨ç”Ÿæˆä¸€ä¸ª__pycache__/module_name.cpython-35.pycçš„æ–‡ä»¶. ```pythonimport module_nameçš„æœ¬è´¨æ˜¯å°†module_name.pyä¸­çš„å…¨éƒ¨ä»£ç åŠ è½½åˆ°å†…å­˜ä¸­, å¹¶å°†å…¶èµ‹å€¼ç»™ä¸æ¨¡å—åŒåçš„å˜é‡, è¿™ä¸ªå˜é‡çš„ç±»å‹æ˜¯classmodule.from module_name import nameçš„æœ¬è´¨æ˜¯å°†æŒ‡å®šçš„å˜é‡æˆ–è€…æ–¹æ³•å¯¼å…¥åˆ°å½“å‰çš„æ–‡ä»¶ä¸­import package_nameçš„æœ¬è´¨æ˜¯æ‰§è¡Œè¯¥åŒ…ä¸‹çš„__init__.pyæ–‡ä»¶, åœ¨æ‰§è¡Œæ–‡ä»¶å, ä¼šåœ¨package_nameç›®å½•ä¸‹ç”Ÿæˆä¸€ä¸ª__pycache__/__init__cpython-35.pycæ–‡ä»¶.from package_name import *çš„æœ¬è´¨æ˜¯å¯¼å…¥__init__.pyæ–‡ä»¶ä¸­çš„__all__åˆ—è¡¨(eg. __all__ = ['L2Norm', 'MultiBoxLoss']). ç›¸å¯¹å¯¼å…¥å’Œç»å¯¹å¯¼å…¥ç»å¯¹å¯¼å…¥: import A.Bfrom A import B ç›¸å¯¹å¯¼å…¥: from . import B # . ä»£è¡¨å½“å‰è·¯å¾„from ..A import B # .. ä»£è¡¨ä¸Šå±‚è·¯å¾„, ... ä»£è¡¨ä¸Šä¸Šå±‚è·¯å¾„. åœ¨æ²¡æœ‰æ˜ç¡®æŒ‡å®šåŒ…ç»“æ„çš„æƒ…å†µä¸‹, Python æ˜¯æ ¹æ®__name__æ¥å†³å®šä¸€ä¸ªæ¨¡å—åœ¨åŒ…ä¸­çš„ç»“æ„çš„, å¦‚æœæ˜¯__main__, åˆ™å®ƒæœ¬èº«å°±æ˜¯é¡¶å±‚æ¨¡å—, æ²¡æœ‰åŒ…ç»“æ„, å¦‚æœæ˜¯A.B.Cç»“æ„, åˆ™Aæ˜¯é¡¶å±‚æ¨¡å—. Python çš„å¯¼å…¥æ–¹å¼çš„ä¸åŒå…·æœ‰ä¸åŒçš„è§„åˆ™:1.å¦‚æœæ˜¯ç»å¯¹å¯¼å…¥, ä¸€ä¸ªæ¨¡å—åªèƒ½å¯¼å…¥è‡ªèº«çš„å­æ¨¡å—æˆ–è€…å’Œå®ƒçš„é¡¶å±‚æ¨¡å—åŒçº§åˆ«çš„æ¨¡å—åŠå…¶å­æ¨¡å—.2.å¦‚æœæ˜¯ç›¸å¯¹å¯¼å…¥, ä¸€ä¸ªæ¨¡å—å¿…é¡»æœ‰åŒ…ç»“æ„ä¸”åªèƒ½å¯¼å…¥å®ƒçš„é¡¶å±‚æ¨¡å—å†…éƒ¨çš„æ¨¡å—.å¦‚æœä¸€ä¸ªæ¨¡å—è¢«ç›´æ¥è¿è¡Œ, åˆ™å®ƒè‡ªå·±ä¸ºé¡¶å±‚æ¨¡å—, ä¸å­˜åœ¨å±‚æ¬¡ç»“æ„, æ‰€ä»¥ä¹Ÿæ‰¾ä¸åˆ°ä¸Šå±‚(..)çš„ç›¸å¯¹è·¯å¾„Python2.x é»˜è®¤ä¸ºç›¸å¯¹è·¯å¾„å¯¼å…¥, è€Œ Python3.x é»˜è®¤ä¸ºç»å¯¹è·¯å¾„å¯¼å…¥, è¿™æ ·å¯ä»¥é¿å…å¯¼å…¥çš„å­åŒ…è¦†ç›–æ‰æ ‡å‡†åº“æ¨¡å—. é€šå¸¸, åœ¨ Python2.x ä¸­, æˆ‘ä»¬åˆ©ç”¨ä¸‹é¢çš„è¯­å¥æ¥ä½¿å…¶å¯¼å…¥è§„åˆ™éµå¾ª Python3.x from __future__ import absolute_import absolute_importçš„æ„æ€å¹¶ä¸æ˜¯å°†æ‰€æœ‰çš„å¯¼å…¥éƒ½è§†ä¸ºç»å¯¹å¯¼å…¥, è€Œæ˜¯æŒ‡ç¦ç”¨éšå¼ç›¸å¯¹å¯¼å…¥(implicit relative import), å…³äºéšå¼çš„æ˜¾ç¤ºçš„å…·ä½“åŒºåˆ«, å¯ä»¥çœ‹ä¸‹é¢çš„ä¾‹å­, å‡è®¾æœ‰å¦‚ä¸‹çš„åŒ…ç»“æ„: thingâ””â”€â”€ __init__.pyâ”œâ”€â”€ booksâ”‚ â”œâ”€â”€ __init__.pyâ”‚ â”œâ”€â”€ adventure.pyâ”‚ â”œâ”€â”€ history.pyâ”‚ â”œâ”€â”€ horror.pyâ”‚ â””â”€â”€ lovestory.pyâ”œâ”€â”€ furnitureâ”‚ â”œâ”€â”€ __init__.pyâ”‚ â”œâ”€â”€ armchair.pyâ”‚ â”œâ”€â”€ bench.pyâ”‚ â”œâ”€â”€ screen.pyâ”‚ â””â”€â”€ stool.py é‚£ä¹ˆå¦‚æœæƒ³åœ¨stool.pyä¸­å¯¼å…¥benchæ¨¡å—, åˆ™æœ‰å¦‚ä¸‹å‡ ç§æ–¹å¼: import bench # éšå¼ç›¸å¯¹å¯¼å…¥from . import bench # æ˜¾å¼ç›¸å¯¹å¯¼å…¥from furniture import bench # ç»å¯¹å¯¼å…¥ éšå¼ç›¸å¯¹å¯¼å…¥æ²¡æœ‰å‘Šè¯‰è§£é‡Šå™¨ç›¸å¯¹äºè°è¿›è¡Œå¯¼å…¥, é»˜è®¤ç›¸å¯¹äºå½“å‰æ¨¡å—; è€Œæ˜¾å¼ç›¸å¯¹å¯¼å…¥åˆ™æ˜ç¡®å‘Šè¯‰äº†è§£é‡Šå™¨ç›¸å¯¹äºè°æ¥å¯¼å…¥. ä»¥ä¸Šå¯¼å…¥æ–¹å¼çš„ç¬¬ä¸‰ç§æ˜¯å®˜æ–¹æ¨èçš„, ç¬¬ä¸€ç§æ˜¯å®˜æ–¹å¼ºçƒˆä¸æ¨èçš„, Python3 ä¸­ç¬¬ä¸€ç§å¯¼å…¥æ–¹å¼åªèƒ½ç”¨äºå¯¼å…¥sys.pathä¸­çš„æ¨¡å—.**æ³¨æ„, è¿˜æœ‰ç›¸å¯¹å¯¼å…¥çš„æ¨¡å—ä¸èƒ½è¢«ç›´æ¥è¿è¡Œ, ä¼šæç¤ºå¦‚ä¸‹é”™è¯¯: Traceback (most recent call last): File \"test.py\", line 8, in module from .ssd import SSDModuleNotFoundError: No module named '__main__.ssd'; '__main__' is not a package å¦å¤–å­˜åœ¨ä¸€ç§æƒ…å†µå°±æ˜¯: å‡å¦‚æœ‰ä¸¤ä¸ªæ¨¡å—a.pyå’Œb.pyæ”¾åœ¨åŒä¸€ä¸ªç›®å½•ä¸‹, åˆ™å¯ä»¥ç›´æ¥åœ¨a.pyä¸­ä½¿ç”¨import bæ¥å¯¼å…¥æ¨¡å—b. è¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢? æˆ‘ä»¬ä¸Šé¢è¯´äº†åœ¨ Python3.x ä¸­ä¸èƒ½ä½¿ç”¨è¿™ç§éšå¼ç›¸å¯¹å¯¼å…¥, ä½†æ˜¯è¿™é‡Œå´å¯ä»¥æˆåŠŸå¯¼å…¥, è¿™æ˜¯å› ä¸ºæ­¤æ—¶æˆ‘ä»¬æ˜¯ç›´æ¥è¿è¡Œa.py, æ‰€ä»¥a.pyå’Œb.pyçš„ç›®å½•æ²¡æœ‰è¢«å½“åšä¸€ä¸ªåŒ…æ¥å¤„ç†, å› æ­¤ä¸æ¶‰åŠç›¸å¯¹å¯¼å…¥å’Œç»å¯¹å¯¼å…¥çš„æ¦‚å¿µ. å› æ­¤ç›¸å¯¹å¯¼å…¥å’Œç»å¯¹å¯¼å…¥ä»…ä»…æ˜¯é’ˆå¯¹äºåŒ…è€Œè¨€çš„. ç»¼åˆè·ç¦»å­˜åœ¨ç›®å½•ç»“æ„å¦‚ä¸‹æ‰€ç¤º: dirRootâ””â”€â”€ __init__.pyâ”œâ”€â”€ file1.pyâ”œâ”€â”€ file2.pyâ”œâ”€â”€ dirAâ”‚ â”œâ”€â”€ __init__.pyâ”‚ â”œâ”€â”€ a1.pyâ”‚ â””â”€â”€ a2.pyâ”œâ”€â”€ dirBâ”‚ â”œâ”€â”€ __init__.pyâ”‚ â”œâ”€â”€ b1.pyâ”‚ â””â”€â”€ b2.py ç›´æ¥è¿è¡Œa1.py, å¹¶å¸Œæœ›å¯¼å…¥a2æ¨¡å—: # a1.pyimport a2 # æ­£ç¡®, æ­¤æ—¶å¹¶æœªå°† dirA å½“åšåŒ…æ¥å¤„ç†, a1.py å’Œ a2.py ç›¸å½“äºä¸¤ä¸ªç‹¬ç«‹çš„æ¨¡å—from a2 import func_a2 # æ­£ç¡®from .a2 import func_a2 # é”™è¯¯, å½“è¿›è¡Œç›¸å¯¹å¯¼å…¥æ—¶, ä¸èƒ½ç›´æ¥è¿è¡Œ ç›´æ¥è¿è¡Œfile1.py, å¹¶å¸Œæœ›å¯¼å…¥a1æ¨¡å—, åŒæ—¶a1æ¨¡å—ä¸­éœ€è¦å¯¼å…¥a2æ¨¡å—: # file1.pyfrom dirA import a1a1.func_a1() # a1.py ä¸­çš„å‡½æ•°a1.func_a2() # a1.py ä¸­å¯¼å…¥äº† a2.py çš„å‡½æ•°, å¯ä»¥ç›´æ¥ä½¿ç”¨# a1.pyimport a2 # é”™è¯¯, æ­¤æ—¶ç”±äº dirA ä¸­æœ‰ __init__.py æ–‡ä»¶, å› æ­¤ä¼šå°† dirA å½“åšåŒ…æ¥å¤„ç†,# ç”±äº Python3.x ä¸å…è®¸ä½¿ç”¨éšå¼çš„ç›¸å¯¹å¯¼å…¥, å› æ­¤è¯¥è¯­å¥éæ³•from a2 import func_a2 # é”™è¯¯, åŸå› åŒä¸Šfrom .a2 import func_a2 # æ­£ç¡®, å½“è¿›è¡Œç›¸å¯¹å¯¼å…¥æ—¶, éœ€è¦ä½¿ç”¨æ˜¾å¼çš„ç›¸å¯¹å¯¼å…¥ ç›´æ¥è¿è¡Œfile1.py, å¹¶å¸Œæœ›å¯¼å…¥a1æ¨¡å—, åŒæ—¶a1æ¨¡å—ä¸­éœ€è¦å¯¼å…¥dirB/b1æ¨¡å—(è·¨æ–‡ä»¶å¤¹å¯¼å…¥): # file1.pyfrom dirA import a1a1.func_a1() # a1.py ä¸­çš„å‡½æ•°a1.func_a2() # a2.py ä¸­çš„å‡½æ•°a1.func_b1() # b1.py ä¸­çš„å‡½æ•°# a1.pyfrom .a2 import func_a2 # æ¨èä½¿ç”¨ç»å¯¹å¯¼å…¥ from dirA.a1 import func_a2from dirB import b1 # ç”±äºè¿è¡Œçš„æ˜¯ file1.py æ–‡ä»¶, å› æ­¤é¡¶å±‚ç›®å½•æ˜¯ dirRootfrom dirB.b1 import func_b1 # æ‰€ä»¥å¯ä»¥ç›´æ¥ä½¿ç”¨ dirB åŒ… ç›´æ¥è¿è¡Œa1.py, å¹¶å¸Œæœ›è·¨ç›®å½•çš„å¯¼å…¥dirB/b1æ¨¡å—. ç”±äºè¿™ç§è·¨ç›®å½•çš„å¯¼å…¥è¶…è¶Šäº†é¡¶å±‚è·¯å¾„çš„é™åˆ¶, å› æ­¤å¿…é¡»ä½¿ç”¨sys.path.append()æ–¹æ³•æ¥é¢å¤–æ·»åŠ æœç´¢è·¯å¾„, å¦åˆ™æ— æ³•æ­£å¸¸å¯¼ # a1.pyimport syssys.path.append(\"../\") # å°† dirA çš„ä¸Šä¸€æ¬¡ç›®å½•æ·»åŠ åˆ°æœç´¢è·¯å¾„ä¸­from dirB import b1 # æ­£ç¡®, æ³¨æ„å¿…é¡»å…ˆæ·»åŠ  path, ç„¶åå†å¯¼å…¥from dirB.b1 import func_b1 # æ­£ç¡®from .a2 import func_a2 # è¿™é‡Œæ˜¯é”™è¯¯çš„, å½“ç›´æ¥æ‰§è¡Œ a1.py æ—¶, a1.py ä¸­ä¸èƒ½åŒ…å«æ˜¾å¼ç›¸å¯¹å¯¼å…¥ è·å– python ç‰ˆæœ¬:print(sys.version_info) è·å–åŒ…çš„å®‰è£…ä½ç½®print(cv2) è§£æ xml æ–‡ä»¶å¯¼å…¥: import sysif sys.version_info[0] == 2: import xml.etree.cElementTree as ETelse: import xml.etree.ElementTree as ET è§£æ: xmlfile = ET.parse(xmlfile_path)root = xmlfile.getroot() # è·å–æ ¹èŠ‚ç‚¹root.tag # æ ‡ç­¾root.attrib # å±æ€§å­—å…¸for child in root: # è¿­ä»£è®¿é—®å­èŠ‚ç‚¹ print(child.tag, child.attrib)# å¯ä»¥é€šè¿‡ç´¢å¼•è®¿é—®åµŒå¥—èŠ‚ç‚¹çš„å†…å®¹root[0][1].textElement.findall() #Element.find() # python ä¸­ == å’Œ is çš„åŒºåˆ«== åªç”¨äºåˆ¤æ–­å€¼æ˜¯å¦ç›¸ç­‰is ç”¨äºåˆ¤æ–­ä¸¤ä¸ªå¯¹è±¡æ˜¯å¦ä¸ºåŒä¸€ä¸ªå®ä¾‹å°æ•´æ•°å¯¹è±¡æ± : Python ä¸ºäº†ä¼˜åŒ–é€Ÿåº¦ï¼Œä½¿ç”¨äº†å°æ•´æ•°å¯¹è±¡æ± ï¼Œé¿å…ä¸ºæ•´æ•°é¢‘ç¹ç”³è¯·å’Œé”€æ¯å†…å­˜ç©ºé—´ã€‚è€ŒPython å¯¹å°æ•´æ•°çš„å®šä¹‰æ˜¯ [-5, 257)ï¼Œåªæœ‰æ•°å­—åœ¨-5åˆ°256ä¹‹é—´å®ƒä»¬çš„idæ‰ä¼šç›¸ç­‰ï¼Œè¶…è¿‡äº†è¿™ä¸ªèŒƒå›´å°±ä¸è¡Œäº†ï¼ŒåŒæ ·çš„é“ç†ï¼Œå­—ç¬¦ä¸²å¯¹è±¡ä¹Ÿæœ‰ä¸€ä¸ªç±»ä¼¼çš„ç¼“å†²æ± ï¼Œè¶…è¿‡åŒºé—´èŒƒå›´å†…è‡ªç„¶ä¸ä¼šç›¸ç­‰äº† é˜Ÿåˆ— queueåœ¨ Python3 ä¸­, åŸæ¥çš„Queueæ¨¡å—è¢«é‡å‘½åä¸ºqueue, è¯¥æ¨¡å—åŒ…å«ä»¥ä¸‹ä¸‰ç±»æ•°æ®ç»“æ„: queue.Queue(maxsize=0): FIFO queue, å…ˆè¿›å…ˆå‡ºé˜Ÿåˆ—, ä»£è¡¨æ™®é€šé˜Ÿåˆ— queue.LifoQueue(maxsize=0): LIFO queue, åè¿›å…ˆå‡ºé˜Ÿåˆ—, ç±»ä¼¼æ ˆçš„ä½œç”¨ queue.PriorityQueue(maxsize=0): ä¼˜å…ˆçº§é˜Ÿåˆ—, ç±»ä¼¼å †çš„ä½œç”¨. é»˜è®¤ä¸ºå°é¡¶å †, å¸¸ç”¨å½¢å¼ä¸ºå…ƒç»„:(priority_number, data)ä¸Šé¢çš„ maxsize è¡¨æ˜äº†é˜Ÿåˆ—ä¸­æœ€å¤§å¯ä»¥å®¹çº³çš„å…ƒç´ æ•°é‡, å¦‚æœè¶…è¿‡, åˆ™æ— æ³•æ’å…¥. å½“ maxsize = 0 æ—¶, ä»£è¡¨å…ƒç´ æ•°é‡æ— é™åˆ¶.å…¬æœ‰æ–¹æ³•(ä»¥ä¸Šä¸‰ä¸ªé€šç”¨): qsize(): è¿”å› approximate size, qsize() 0 ä¸ä¿è¯get()ä¸€å®š work, åŒç†, qsize() maxsize ä¸ä¿è¯put()ä¸€å®š work. empty(): å¦‚æœé˜Ÿåˆ—ä¸ºç©º, è¿”å› True. å’Œqsize()ä¸€æ ·, ä¸æä¾›ä¿è¯æ€§. full(): å¦‚æœé˜Ÿåˆ—æ»¡, è¿”å› True. ä¸æä¾›ä¿è¯æ€§ put(item[, block[, timeout]]) put_nowait(item): ç­‰ä»·äºput(item, False) get([block[, timeout]]) get_nowait(): ç­‰ä»·äºget(False) task_done(): join(): å † heapqheapq æ¨¡å—åªæœ‰æœ€å°å †çš„åŠŸèƒ½, è¦å®ç°æœ€å¤§å †, éœ€è¦åœ¨å…¥å †å’Œå‡ºå †çš„æ—¶å€™å–å, å¹¶ä¸” heapq æ¨¡å—åªèƒ½ä½œç”¨äºæ•°å€¼å‹ç±»å‹.æœ€å¤§å †: _heapify_max(), _heappop_max()ç»™å®šä¸€ç»„æ•°æ®, åˆ›å»ºå †, ä¸¤ç§æ–¹å¼(äºŒè€…ç­‰ä»·): import heapqdata = [1,3,6,2,8,5]heap = []for d in data: heapq.heappush(heap, n) # æ–¹æ³•ä¸€ é€ä¸ªæ„å»ºheapq.heapify(data) # æ–¹æ³•äºŒ åŸåœ°æ„å»º, æ•ˆç‡æ›´é«˜ å°é¡¶å †: heap = [1,3,6,2,8,5]heapq.heapify(heap)heapq.heappop(heap) # è¿”å›å¹¶åˆ é™¤å †é¡¶heapq.heapreplace(heap, 10) # åˆ é™¤å †é¡¶å¹¶æ·»åŠ æ–°å€¼heapq.heappushpop(heap, 10) # å…ˆå°†æ–°å€¼åŠ å…¥å †ä¸­, ç„¶åç«‹åˆ»å¼¹å‡ºå †é¡¶print(heap[0]) # æŸ¥çœ‹å †é¡¶ å¤§é¡¶å †: # æ–¹æ³•ä¸€: å–è´Ÿå€¼heap = [-1,-3,-6,-2,-8,-5]# æ–¹æ³•äºŒ: å†…ç½®æ–¹æ³•heap = [1,3,6,2,8,5]heapq._heapify_max(heap) # max_heapprint(heap[0]) # æŸ¥çœ‹å †é¡¶, 8heapq._heappop_max(heap) # po from maxheapprint(heap[0]) # 6heapq._heapreplace_max(heap, 10)print(heap[0]) # 10# heapq._heappushpop_max(heap, 10) # æ³¨æ„, æ²¡æœ‰ _heappushpop_max å‡½æ•° Python åˆ·é¢˜å¸¸ç”¨é˜Ÿåˆ—: import Queuebase_queue = Queue.Queue() # åŸºæœ¬é˜Ÿåˆ—, å…ˆè¿›å…ˆå‡ºbase_queue.put(x)base_queue.get()lifo_queue = Queue.LifoQueue() # å…ˆè¿›åå‡º, ç±»ä¼¼æ ˆlifo_queue.put(x)lifo_queue.get()prio_queue = Queue.PriorityQueue() # ä¼˜å…ˆé˜Ÿåˆ—, ä¸C++ä¸­priority_queueç±»ä¼¼, å¯å®ç°å †çš„åŠŸèƒ½prio_queue.put(x)prio_queue.get() numpy ä¸­vstack, hstack, concatenate å’Œ stack ä¹‹é—´çš„åŒºåˆ«å’Œè”ç³»concatenatenumpy.concatenate((a1, a2, ...), axis=0, out=None) concatenate çš„ä½œç”¨å°±æ˜¯å°†å¤šä¸ªæ•°ç»„åºåˆ—æŒ‰ç…§axisæŒ‡å®šçš„ç»´åº¦è¿æ¥èµ·æ¥, è¿™äº›æ•°ç»„åºåˆ— a1, a2, â€¦ å¿…é¡»ä¿è¯ é™¤äº† axis æŒ‡å®šç»´åº¦ä¹‹å¤–çš„å…¶ä»–ç»´åº¦å…·æœ‰ç›¸åŒçš„ shape.æ³¨æ„: è¿™é‡Œçš„ç»´åº¦æŒ‡çš„æ˜¯a1, a2çš„ç»´åº¦, è€Œä¸æ˜¯(a1, a2)çš„ç»´åº¦ä»ç»´åº¦è§’åº¦æ¥æ›´å¥½ç†è§£ concatenate çš„ä½œç”¨concatenate æ‰§è¡Œåçš„ shape ç‰¹å®šæ˜¯: axis æŒ‡å®šçš„ç»´åº¦æ˜¯å¤šä¸ªæ•°ç»„åºåˆ—å¯¹åº”ç»´åº¦çš„æ•°å€¼å’Œ, è€Œå…¶ä»–ç»´åº¦ä¿æŒä¸å˜. ä¹Ÿå°±æ˜¯è¯´ä¸ä¼šå¢åŠ æ–°çš„ç»´åº¦, è¿™æ˜¯ concatenate ä¸ stack ä¹‹é—´çš„ä¸€ä¸ªé‡è¦çš„åŒºåˆ«.å¦‚ä¸‹æ‰€ç¤º: import numpy as npa1 = np.array([[1, 1], [2, 2], [3, 3]]) # shape = 3x2a2 = np.array([[1, 1], [2, 2]]) # shape = 2 x 2print(a1.shape, a2.shape)concat1 = np.concatenate((a1, a2), axis=0)print(concat1.shape) # shape ä¸º [5, 2], åœ¨ 0 ç»´åº¦ä¸Šä¸º 3+2, å…¶ä»–ç»´åº¦ä¿æŒä¸å˜print(concat1) # a1, a2 ç»´åº¦ 0 ä¸åŒ, ä¸€ä¸ªä¸º 3, ä¸€ä¸ªä¸º 2, å…¶ä»–ç»´åº¦ç›¸åŒ, å‡ä¸º 2#[[1 1]# [2 2]# [3 3]# [1 1]# [2 2]]#print(np.concatenate((a1, a2), axis=1)) # ç”±äºç»´åº¦ 0 äºŒè€…ä¸åŒ, æ— æ³•ä¿æŒä¸å˜, å› æ­¤æŠ¥é”™a1 = np.array([[1, 2, 3]]) # shape = 1x3a2 = np.array([[1, 2]]) # shape = 1x2print(a1.shape, a2.shape)concat2 = np.concatenate((a1, a2), axis=1)print(concat2.shape) # shape ä¸º [1, 5]åœ¨ 1 ç»´åº¦ä¸Šä¸º 3 + 2, 0 ç»´åº¦ä¸Šä¿æŒ 1 ä¸å˜print(concat2)# [[1 2 3 1 2]]# print(np.concatenate((a1, a2), axis=0)) # ç»´åº¦ 1 ä¸åŒ, æŠ¥é”™ æœ‰æ—¶å€™, concatenateçš„ç¬¬ä¸€ä¸ªå‚æ•°åªä¼šä¼ é€ä¸€ä¸ªä¸€ä¸ªæ•°ç»„åºåˆ—, è¿™æ—¶å€™, ç­‰ä»·äºå°†è¿™ä¸ªæ•°ç»„åºåˆ—çš„ç¬¬ä¸€ç»´çš„å…ƒç´ çœ‹åšæ˜¯å¤šä¸ªæ•°ç»„åºåˆ—ä½œä¸ºconcatenateçš„å‚æ•°è¿›è¡Œä¼ é€’. å¦‚ä¸‹æ‰€ç¤º: a = [[1, 2, 3], [1, 2, 3]]print(np.concatenate(a, axis=0)) # è¯¥è¡Œä¸ä¸‹ä¸€è¡Œç­‰ä»·print(np.concatenate((a[0], a[1]), axis=0))a = [[1, 2, 3], [1, 2]]print(np.concatenate(a, axis=0)) # å¯ä»¥çœ‹å‡º, è™½ç„¶ a çš„ç¬¬ä¸€ç»´åº¦ä¸º 2, ç¬¬äºŒç»´åº¦ä¸º 3 å’Œ 2# ä½†æ˜¯, æˆ‘ä»¬è¦å°†å…¶æ‹†åˆ†, æ‹†åˆ†å, a[0], a[1] çš„ç¬¬ä¸€ç»´åº¦3å’Œ2, å…¶ä»–ç»´åº¦ç›¸åŒ, å› æ­¤å¯ä»¥åœ¨ç¬¬ä¸€ç»´åº¦ä¸Šè¿›è¡Œè¿æ¥print(np.concatenate((a[0], a[1]), axis=0)) stacknumpy.stack(arrays, axis=0, out=None)numpy.stack((a1, a2, ...), axis=0, out=None) stack çš„ä½œç”¨å°±æ˜¯å°†å¤šä¸ªæ•°ç»„åºåˆ—æŒ‰ç…§axisæŒ‡å®šçš„ç»´åº¦ å †å  èµ·æ¥, è¿™äº›æ•°ç»„åºåˆ— a1, a2, â€¦ å¿…é¡»ä¿è¯ æ‰€æœ‰ç»´åº¦éƒ½ç›¸åŒ, æ³¨æ„è¿™é‡Œä¸ concatenate çš„åŒºåˆ«.è¦æ›´å¥½çš„ç†è§£stack, å¯ä»¥å€ŸåŠ© ç»´åº¦ çš„æ¦‚å¿µè¿›è¡Œç†è§£, å¯¹äº shape ç›¸åŒçš„ k ä¸ªæ•°ç»„åºåˆ—æ¥è¯´, stack çš„ä½œç”¨ç›¸å½“äºæ–°æ’å…¥ä¸€ä¸ªç»´åº¦, ç»´åº¦çš„å¤§å°ä¸º k, æ’å…¥çš„ä½ç½®ä¸ºaxisæŒ‡å®šçš„ä½ç½®. å¦‚ä¸‹æ‰€ç¤º: a1 = [[1, 1], [2, 2], [3, 3]] # shape = 3x2a2 = [[4, 4], [5, 5], [6, 6]] # shape = 3x2a3 = [[7, 7], [8, 8], [9, 9]] # shape = 3x2a4 = [[0, 0], [0, 0], [0, 0]] # shape = 3x2stack1 = np.stack((a1, a2, a3, a4), axis=0) # æ–°æ’å…¥ç»´åº¦å¤§å°ä¸º 4, ä½ç½®ä¸ºç¬¬ 0 ç»´print(stack1.shape) # shape ä¸º (4, 3, 2)print('### ', stack1) # å…ˆå°† shape ç”»å¥½, ç„¶åè¿›è¡Œå¡«å……, åœ¨ç¬¬ 0 ç»´ä¸Šè¿›è¡Œå †å , å› æ­¤ stack1[*][*] = a1[0], a1[1], ..., a4[2]stack2 = np.stack((a1, a2, a3, a4), axis=1) # æ–°æ’å…¥ç»´åº¦å¤§å°ä¸º 4, ä½ç½®ä¸ºç¬¬ 1 ç»´print(stack2.shape) # shape ä¸º (3, 4, 2)print('### ', stack2) # åœ¨ç¬¬ 1 ç»´ä¸Šè¿›è¡Œå †å , å› æ­¤ stack2[*][*] = a1[0], a2[0], a3[0], a1[1], ...stack3 = np.stack((a1, a2, a3, a4), axis=2) # æ–°æ’å…¥ç»´åº¦å¤§å°ä¸º 4, ä½ç½®ä¸ºç¬¬ 2 ç»´print(stack3.shape) # shape ä¸º (3, 2, 4)print('### ', stack3) # åœ¨ç¬¬ 2 ç»´ä¸Šè¿›è¡Œå †å , å› æ­¤ stack2[*][*] = [1 4 7 0], [1 4 7 0], [2 5 8 0], ... hstack å’Œ vstackhstack å’Œ vstack è™½ç„¶åå­—ä¸­éƒ½å¸¦æœ‰ stack, ä½†æ˜¯å®é™…ä¸Š, å®ƒä»¬å’Œnp.stackçš„å…³ç³»å¹¶ä¸å¤§, ä¸€ä¸ªæ˜æ˜¾çš„åŒºåˆ«å°±æ˜¯np.stackè¦æ±‚è¿›è¡Œå †å çš„å¤šä¸ªæ•°ç»„åºåˆ—éœ€è¦ä¿è¯ shape å®Œå…¨ç›¸åŒ, å¹¶ä¸”å †å åä¼šæ–°å¢åŠ ä¸€ä¸ªç”±axisæŒ‡å®šçš„ç»´åº¦. å®é™…ä¸Š, hstack å’Œ vstack å¯ä»¥çœ‹åšæ˜¯ç‰¹æ®Šçš„ concatenate, å®ƒä»¬åœ¨æŸäº›æƒ…å†µä¸‹å¯ä»¥ç”¨ concatenate æ¥ä»£æ›¿æ—¢ç„¶ hstack å’Œ vstack æ˜¯ç‰¹æ®Šçš„ concatenate, ä¹Ÿå°±æ˜¯è¯´, å®ƒä»¬æ‰€æ¥å—çš„å¤šä¸ªæ•°ç»„åºåˆ—åœ¨axisæŒ‡å®šçš„ç»´åº¦ä¸Šå¯ä»¥ä¸åŒ, è€Œåœ¨å…¶ä»–ç»´åº¦ä¸Šå¿…é¡»ç›¸åŒ.vstack: åœ¨å‚ç›´æ–¹å‘ä¸Šå°†å¤šä¸ªæ•°ç»„åºåˆ—è¿›è¡Œå †å , ç›¸å½“äºåœ¨axis=0ç»´åº¦ä¸Šæ‰§è¡Œconcatenatehstack: åœ¨æ°´å¹³æ–¹å‘ä¸Šå°†å¤šä¸ªæ•°ç»„åºåˆ—è¿›è¡Œå †å , ç›¸å½“äºåœ¨axis=1ç»´åº¦ä¸Šæ‰§è¡Œconcatenate a = [[1, 1], [2, 2], [3, 3]] # shape = 3x2b = [[4, 4], [5, 5], [6, 6]] # shape = 3x2c = [[7, 7], [8, 8], [9, 9]] # shape = 3x2d = [[0, 0], [0, 0], [0, 0]] # shape = 3x2v = np.vstack((a, b, c, d))print(v.shape) # (12, 2)print(v)x = np.concatenate((a, b, c, d), axis = 0) # ç­‰ä»·äº vstackprint(x.shape) # 12, 2print(x)h = np.hstack((a, b, c, d))print(h.shape) # (3, 8)print(h)x = np.concatenate((a, b, c, d), axis = 1) # ç­‰ä»·äº hstackprint(x.shape) # 3, 8print(x) éœ€è¦ç‰¹åˆ«æ³¨æ„, å½“å¤šä¸ªæ•°ç»„åºåˆ—æ˜¯ä¸€ç»´æ•°ç»„æ—¶, åº”è¯¥å…ˆå°†ä¸€ç»´æ•°ç»„è½¬æ¢æˆäºŒç»´æ•°ç»„, ç„¶åæ‰èƒ½ä¸ç›¸åº”çš„ concatenate è¿›è¡Œç­‰ä»·. è¿™æ˜¯å› ä¸º, åœ¨æ•°ç»„åºåˆ—æ˜¯ä¸€ç»´æ•°ç»„æ—¶, concatenate æ˜¯æ— æ³•ä½¿ç”¨axis=1çš„, å› æ­¤æ­¤æ—¶çš„ hstack ç›¸å½“äºæ˜¯åœ¨axis=0ä¸Šè¿›è¡Œ concatenate, è€Œ vstack åˆ™éœ€è¦å…ˆå°†æ•°ç»„çš„ shape ä» (N,) è½¬æ¢æˆ (1, N) åæ‰ç›¸å½“äºæ˜¯åœ¨axis=1ä¸Šè¿›è¡Œ concatenate a = np.array([1, 2, 3, 4, 5]) # å½“é¢å¯¹çš„æ˜¯ä¸€ç»´æ•°ç»„æ—¶,b = np.array([6, 7, 8, 9, 10])h = np.hstack((a, b))print(h.shape)print(h)con = np.concatenate((a, b), axis=0) # å½“ a, b æ˜¯ä¸€ç»´æ•°ç»„æ—¶, hstack ç›¸å½“äºåœ¨ axis=0 ä¸Šè¿›è¡Œè¿æ¥print(con.shape)print(con)v = np.vstack((a, b))print(v.shape)print(v)con = np.concatenate(([a], [b]), axis=0) # å½“ a, b æ˜¯ä¸€ç»´æ•°ç»„æ—¶, vstack ç›¸å½“äºå°† a, b å…ˆè½¬æ¢æˆäºŒç»´ (1, N), ç„¶ååœ¨ axis=0 ä¸Šè¿›è¡Œè¿æ¥print(con.shape)print(con) set å»é‡å¯¹äºäºŒç»´åˆ—è¡¨, ç”±äº list çš„å…ƒç´ ä¹Ÿæ˜¯ list, åœ¨å†…å­˜ä¸­å­˜å‚¨çš„æ˜¯é¦–å…ƒç´ åœ°å€, æ— æ³•ç›´æ¥ä½¿ç”¨ set, å› æ­¤éœ€è¦å…ˆå°†å†…éƒ¨çš„å…ƒç´ å…¨éƒ¨å…¨æ¢æˆ tuple å, æ‰èƒ½ä½¿ç”¨ list å»é‡. å¦‚ä¸‹æ‰€ç¤º a = list()a.append([1,2,3])a.append([1,2,3])a.append([1,2,3])a.append([4, 5, 6])# b = set(a) # æŠ¥é”™b = set(map(tuple, a))print(b) # {(4, 5, 6), (1, 2, 3)} os.sepç”¨æ³•ythonæ˜¯è·¨å¹³å°çš„ã€‚åœ¨Windowsä¸Šï¼Œæ–‡ä»¶çš„è·¯å¾„åˆ†éš”ç¬¦æ˜¯â€™\\â€™ï¼Œåœ¨Linuxä¸Šæ˜¯â€™/â€˜ã€‚ä¸ºäº†è®©ä»£ç åœ¨ä¸åŒçš„å¹³å°ä¸Šéƒ½èƒ½è¿è¡Œï¼Œé‚£ä¹ˆè·¯å¾„åº”è¯¥å†™â€™\\â€™è¿˜æ˜¯â€™/â€˜å‘¢ï¼Ÿä½¿ç”¨os.sepçš„è¯ï¼Œå°±ä¸ç”¨è€ƒè™‘è¿™ä¸ªäº†ï¼Œos.sepæ ¹æ®ä½ æ‰€å¤„çš„å¹³å°ï¼Œè‡ªåŠ¨é‡‡ç”¨ç›¸åº”çš„åˆ†éš”ç¬¦å·ã€‚ä¸¾ä¾‹Linuxä¸‹ä¸€ä¸ªè·¯å¾„ï¼Œ/usr/share/python,é‚£ä¹ˆä¸Šé¢çš„os.sepå°±æ˜¯â€˜/â€™windowsä¸‹ä¸€ä¸ªè·¯å¾„ï¼ŒCï¼š\\Users\\Public\\Desktop,é‚£ä¹ˆä¸Šé¢çš„os.sepå°±æ˜¯â€˜\\â€™. data_dir = os.sep.join(['hello', 'world']) Python3 å…ƒç»„Pythonå…ƒç»„åŒ…å«äº†ä»¥ä¸‹å†…ç½®å‡½æ•° len(tuple) è®¡ç®—å…ƒç»„å…ƒç´ ä¸ªæ•°ã€‚1234 tuple1 = (â€˜Googleâ€™, â€˜Runoobâ€™, â€˜Taobaoâ€™) len(tuple1)3 * max(tuple) è¿”å›å…ƒç»„ä¸­å…ƒç´ æœ€å¤§å€¼ã€‚ tuple2 = ('5', '4', '8') max(tuple2) '8' * min(tuple) è¿”å›å…ƒç»„ä¸­å…ƒç´ æœ€å°å€¼ã€‚ tuple2 = ('5', '4', '8') min(tuple2) '4' * tuple(seq) å°†åˆ—è¡¨è½¬æ¢ä¸ºå…ƒç»„ã€‚ list1= ['Google', 'Taobao', 'Runoob', 'Baidu'] tuple1=tuple(list1) tuple1 ('Google', 'Taobao', 'Runoob', 'Baidu') ## åºåˆ—åŒ–Pythonå¯¹è±¡ä½ éœ€è¦å°†ä¸€ä¸ªPythonå¯¹è±¡åºåˆ—åŒ–ä¸ºä¸€ä¸ªå­—èŠ‚æµï¼Œä»¥ä¾¿å°†å®ƒä¿å­˜åˆ°ä¸€ä¸ªæ–‡ä»¶ã€å­˜å‚¨åˆ°æ•°æ®åº“æˆ–è€…é€šè¿‡ç½‘ç»œä¼ è¾“å®ƒã€‚ å¯¹äºåºåˆ—åŒ–æœ€æ™®éçš„åšæ³•å°±æ˜¯ä½¿ç”¨ pickle æ¨¡å—ã€‚ä¸ºäº†å°†ä¸€ä¸ªå¯¹è±¡ä¿å­˜åˆ°ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œå¯ä»¥è¿™æ ·åšpickle å¯¹äºå¤§å‹çš„æ•°æ®ç»“æ„æ¯”å¦‚ä½¿ç”¨ array æˆ– numpy æ¨¡å—åˆ›å»ºçš„äºŒè¿›åˆ¶æ•°ç»„æ•ˆç‡å¹¶ä¸æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„ç¼–ç æ–¹å¼ã€‚ å¦‚æœä½ éœ€è¦ç§»åŠ¨å¤§é‡çš„æ•°ç»„æ•°æ®ï¼Œä½ æœ€å¥½æ˜¯å…ˆåœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­å°†å…¶ä¿å­˜ä¸ºæ•°ç»„æ•°æ®å—æˆ–ä½¿ç”¨æ›´é«˜çº§çš„æ ‡å‡†ç¼–ç æ–¹å¼å¦‚HDF5 (éœ€è¦ç¬¬ä¸‰æ–¹åº“çš„æ”¯æŒ)ã€‚ ```python In [1]: import pickle In [2]: obj = 123,\"abcdef\", [\"ac\", 123], {\"key\": \"value\", \"key1\": \"value1\"} In [3]: print(obj) (123, 'abcdef', ['ac', 123], {'key': 'value', 'key1': 'value1'}) In [4]: # åºåˆ—åŒ–åˆ°æ–‡ä»¶ In [5]: with open(r'./a.pickle','wb') as f: ...: pickle.dump(obj,f) ...: In [6]: with open(r'./a.pickle','rb') as f: ...: aa= pickle.load(f) ...: print(aa) ...: ...: (123, 'abcdef', ['ac', 123], {'key': 'value', 'key1': 'value1'}) å‚è€ƒé“¾æ¥ï¼šhttps://hellozhaozheng.github.iohttps://www.runoob.com/python/python-tutorial.html","tags":["Python"],"categories":["Python"]},{"title":"shutilæ¨¡å—","path":"/2019/08/28/20190828-shutilæ¨¡å—/","content":"shutil.copyfile(\"old\",\"new\") # å¤åˆ¶æ–‡ä»¶ï¼Œéƒ½åªèƒ½æ˜¯æ–‡ä»¶shutil.copytree(\"old\",\"new\") # å¤åˆ¶æ–‡ä»¶å¤¹ï¼Œéƒ½åªèƒ½æ˜¯ç›®å½•ï¼Œä¸”newå¿…é¡»ä¸å­˜åœ¨shutil.copy(\"old\",\"new\") # å¤åˆ¶æ–‡ä»¶/æ–‡ä»¶å¤¹ï¼Œå¤åˆ¶ old ä¸º newï¼ˆnewæ˜¯æ–‡ä»¶ï¼Œè‹¥ä¸å­˜åœ¨ï¼Œå³æ–°å»ºï¼‰ï¼Œå¤åˆ¶ old ä¸ºè‡³ new æ–‡ä»¶å¤¹ï¼ˆæ–‡ä»¶å¤¹å·²å­˜åœ¨ï¼‰shutil.move(\"old\",\"new\") # ç§»åŠ¨æ–‡ä»¶/æ–‡ä»¶å¤¹è‡³ new æ–‡ä»¶å¤¹ä¸­","tags":["Python"],"categories":["Python"]},{"title":"skimageæ¨¡å—","path":"/2019/08/28/20190828-skimageæ¨¡å—/","content":"æ¯”opencvçš„é€Ÿåº¦è¦æ…¢å¾ˆå¤š, ä½†æ˜¯ä½¿ç”¨èµ·æ¥æ›´åŠ ç®€å•, çœŸçš„å¯¹é€Ÿåº¦è¦æ±‚å¾ˆé«˜çš„è¯, ä¸€èˆ¬éƒ½ä¼šC++å’Œopecvä½¿ç”¨. æ‰€ä»¥ä¸€èˆ¬æƒ…å†µä¸‹, é¦–å…ˆçœ‹skimageèƒ½å¦å®ç°, ä¸è¡Œçš„è¯å†è½¬ç”¨opencv import skimagefrom skimage import io # IO is a submodule. Submodules need to be imported from the parent module explicitly.img = io.imread(\"1.jpg\")","tags":["è®¡ç®—æœºè§†è§‰"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"ä»ä¸Šå¾€ä¸‹æ‰“å°äºŒå‰æ ‘","path":"/2019/08/28/20190828-ä»ä¸Šå¾€ä¸‹æ‰“å°äºŒå‰æ ‘/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š420679æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é˜Ÿåˆ— æ ‘ é¢˜ç›®æè¿°ä»ä¸Šå¾€ä¸‹æ‰“å°å‡ºäºŒå‰æ ‘çš„æ¯ä¸ªèŠ‚ç‚¹ï¼ŒåŒå±‚èŠ‚ç‚¹ä»å·¦è‡³å³æ‰“å°ã€‚ /*struct TreeNode {\tint val;\tstruct TreeNode *left;\tstruct TreeNode *right;\tTreeNode(int x) : val(x), left(NULL), right(NULL) {\t}};*/class Solution {public: vectorint PrintFromTopToBottom(TreeNode* root) { //é˜Ÿåˆ—æ˜¯å…ˆè¿›å…ˆå‡º queueTreeNode* que; vectorint vec; que.push(root);//å…ˆå°†æ•´ä¸ªäºŒå‰æ ‘æ”¾å…¥é˜Ÿåˆ— while(!que.empty()) //å½“é˜Ÿåˆ—éç©ºè¿›è¡Œå¾ªç¯ { TreeNode* p; p = que.front();//å…ˆè¯»å–é˜Ÿåˆ—çš„é¦–å…ƒç´  que.pop();//å¼¹å‡ºé˜Ÿåˆ—çš„é¦–å…ƒç´  if(p == NULL) continue;//æ‰€æœ‰å…ƒç´ å­˜å…¥vecåï¼Œç”±äºé˜Ÿåˆ—ä¸­å­˜æ”¾ç€ç©ºæŒ‡é’ˆï¼Œä¾ç„¶è¿›å…¥å¾ªç¯ï¼Œä½†æ­¤æ—¶pçš„å€¼ä¸ºNULLï¼Œä¸æ‰§è¡Œä¸‹é¢çš„æ“ä½œï¼Œè·³å‡ºå¾ªç¯ç»“æŸ que.push(p-left); que.push(p-right); vec.push_back(p-val); } return vec; }}; è¿è¡Œæ—¶é—´ï¼š3mså ç”¨å†…å­˜ï¼š464k","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"numpyå®ç°ç¥ç»ç½‘ç»œ","path":"/2019/08/28/20190828-numpyå®ç°ç¥ç»ç½‘ç»œ/","content":"import numpy as np# N ä¸ºbatch size, D_in ä¸ºè¾“å…¥ç»´åº¦# H ä¸ºéšè—å±‚çš„ç»´åº¦, D_out ä¸ºè¾“å‡ºçš„ç»´åº¦N, D_in, H, D_out = 64, 1000, 100, 10# åˆ›å»ºéšæœºçš„è¾“å…¥å’Œè¾“å‡ºæ•°æ®x = np.random.randn(N, D_in) # N Ã— D_in çš„çŸ©é˜µy = np.random.randn(N, D_out) # N Ã— D_out çš„çŸ©é˜µ# å¯¹ä¸¤ä¸ªéšè—å±‚w1,w2è¿›è¡Œåˆå§‹åŒ–w1 = np.random.randn(D_in, H)w2 = np.random.randn(H, D_out)# è®¾ç½®å­¦ä¹ ç‡learning_rate = 1e-6for t in range(500): # å‰å‘ä¼ æ’­: è®¡ç®—é¢„æµ‹ç»“æœ y_pred h = x.dot(w1) # xç»´åº¦ä¸º64 Ã— 1000, w1ç»´åº¦ä¸º 1000 Ã— 100, è®¡ç®—å®Œä»¥å, hç»´åº¦ä¸º 64 Ã— 100 h_relu = np.maximum(h,0) y_pred = h_relu.dot(w2) # h_reluç»´åº¦ä¸º 64Ã—100, w2ç»´åº¦ä¸º100Ã—10, yçš„ç»´åº¦ä¸º64Ã—10 # è®¡ç®—æŸå¤± loss = np.square(y_pred - y).sum() print(t, loss) # åå‘ä¼ æ’­æ ¹æ®lossæ›´æ–°w1å’Œw2çš„å€¼ grad_y_pred = 2.0*(y_pred - y) # å¯¹y_predæ±‚å¯¼ grad_w2 = h_relu.T.dot(grad_y_pred) # å¯¹w2æ±‚å¯¼, å¾®åˆ†çŸ©é˜µåº”è¯¥ä¸w2çš„sizeç›¸åŒ grad_h_relu = grad_y_pred.dot(w2.T) # å¯¹h_reluæ±‚å¯¼ grad_h = grad_h_relu.copy() grad_h[h 0] = 0 # ç»è¿‡relu, å°†å°äº0çš„æ¢¯åº¦å½’0 grad_w1 = x.T.dot(grad_h) # Update weights w1 = w1 - learning_rate * grad_w1 w2 = w2 - learning_rate * grad_w2","tags":["æ·±åº¦å­¦ä¹ "],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"å¹¿åº¦/å®½åº¦ä¼˜å…ˆæœç´¢(BFS)","path":"/2019/08/28/BFS/","content":"å‰è¨€å¹¿åº¦ä¼˜å…ˆæœç´¢ æ˜¯æœ€ç®€å•çš„å›¾æœç´¢ç®—æ³•ä¹‹ä¸€ï¼Œ ä¹Ÿæ˜¯è®¸å¤šé‡è¦çš„å›¾ç®—æ³•çš„åŸå‹ã€‚Primeçš„æœ€å°ç”Ÿæˆæ ‘ç®—æ³•å’ŒDijkstraçš„å•æºæœ€çŸ­è·¯å¾„ç®—æ³•éƒ½ä½¿ç”¨äº†ç±»ä¼¼å¹¿åº¦ä¼˜å…ˆæœç´¢çš„æ€æƒ³ã€‚ç»™å®šå›¾G=(V,E) å’Œä¸€ä¸ªå¯ä»¥è¯†åˆ«çš„æº èŠ‚ç‚¹ sï¼Œå¹¿åº¦ä¼˜å…ˆæœç´¢å¯¹å›¾Gä¸­çš„è¾¹è¿›è¡Œç³»ç»Ÿæ€§çš„æ¢ç´¢æ¥å‘ç°å¯ä»¥ä»æºèŠ‚ç‚¹ï¼Œåˆ°è¾¾æ‰€æœ‰çš„èŠ‚ç‚¹ã€‚è¯¥ç®—æ³•èƒ½å¤Ÿè®¡ç®—ä»æºç»“ç‚¹såˆ°æ¯ä¸ªå¯åˆ°è¾¾çš„èŠ‚ç‚¹çš„è·ç¦»(æœ€å°çš„è¾¹æ•°)ï¼ŒåŒæ—¶ç”Ÿæˆä¸€æ£µâ€œå¹¿åº¦ä¼˜å…ˆæœç´¢æ ‘â€ã€‚è¯¥æ ‘ä»¥æºç»“ç‚¹sä¸ºæ ¹èŠ‚ç‚¹ï¼ŒåŒ…æ‹¬æ‰€æœ‰å¯ä»¥ä»såˆ°è¾¾çš„ç»“ç‚¹ã€‚å¯¹äºæ¯ä¸ªä»æºç»“ç‚¹så¯ä»¥åˆ°è¾¾çš„ç»“ç‚¹vï¼Œåœ¨å¹¿åº¦ä¼˜å…ˆæœç´¢æ ‘é‡Œä»ç»“ç‚¹såˆ°ç»“ç‚¹vçš„ç®€å•è·¯å¾„æ‰€å¯¹åº”çš„çš„å°±æ˜¯å›¾Gä¸­ä»ç»“ç‚¹såˆ°ç»“ç‚¹vçš„â€œæœ€çŸ­è·¯å¾„â€ï¼Œå³åŒ…å«æœ€å°‘è¾¹æ•°çš„è·¯å¾„ï¼Œè¯¥ç®—æ³•æ—¢å¯ä»¥ç”¨äºæœ‰å‘å›¾ä¹Ÿå¯ä»¥ç”¨äºæ— å‘å›¾ã€‚å¹¿åº¦ä¼˜å…ˆç®—æ³•ä¹‹æ‰€ä»¥å¦‚æ­¤å¾—åæ˜¯å› ä¸ºè¯¥ç®—æ³•å§‹ç»ˆæ˜¯å°†å·²ç»å‘ç°çš„ç»“ç‚¹å’Œæœªå‘ç°ç»“ç‚¹ä¹‹é—´çš„è¾¹ç•Œï¼Œæ²¿å…¶å¹¿åº¦æ–¹å‘å‘å¤–æ‰©å±•ã€‚ä¹Ÿå°±æ˜¯è¯´ï¼Œç®—æ³•éœ€è¦åœ¨å‘ç°æ‰€æœ‰è·ç¦»æºç»“ç‚¹sä¸ºkçš„æ‰€æœ‰ç»“ç‚¹ä¹‹åï¼Œæ‰ä¼šå‘ç°è·ç¦»æºç»“ç‚¹sä¸ºk+1çš„ å…¶ä»–ç»“ç‚¹ã€‚ å›¾çš„æ¦‚å¿µ å›¾(graph) æ˜¯ä¸€ç§$\\textcolor{Blue}{ç½‘çŠ¶æ•°æ®} $ç»“æ„ï¼Œ å›¾æ˜¯ç”±éç©ºçš„é¡¶ç‚¹é›†åˆå’Œä¸€ä¸ªæè¿°é¡¶ç‚¹ä¹‹é—´çš„å…³ç³»çš„é›†åˆç»„æˆã€‚ å›¾ç”±é¡¶ç‚¹å’Œè¾¹ç»„æˆï¼Œé¡¶ç‚¹è¡¨ç¤ºå¯¹è±¡ï¼Œè¾¹è¡¨ç¤ºå¯¹è±¡ä¹‹é—´çš„è¿æ¥å…³ç³»ã€‚ è¾¹ä¹Ÿå¯ä»¥å¸¦æƒå€¼ï¼Œç§°ä¸ºå¸¦æƒå€¼å›¾ã€‚ æ— å‘å›¾æœ¯è¯­ ä¸¤ä¸ªé¡¶ç‚¹ä¹‹é—´å¦‚æœæœ‰è¾¹è¿æ¥ï¼Œè§†ä¸ºä¸¤ä¸ªé¡¶ç‚¹ç›¸é‚» ç›¸é‚»é¡¶ç‚¹é—´çš„åºåˆ—ç§°ä¸ºè·¯å¾„ èµ·ç‚¹å’Œç»ˆç‚¹é‡åˆçš„è·¯å¾„ç§°ä¸ºåœˆ é¡¶ç‚¹è¿æ¥çš„è¾¹æ•°å«åšè¿™ä¸ªé¡¶ç‚¹çš„åº¦ æ²¡æœ‰åœˆçš„è¿é€šå›¾ï¼Œå°±æ˜¯æ ‘ æ²¡æœ‰åœˆçš„éè¿é€šå›¾ï¼Œå°±æ˜¯æ£®æ— ä¸€æ£µæ ‘çš„è¾¹æ•°ç­‰äºé¡¶ç‚¹æ•°-1 è¾¹æ•°ç­‰äºé¡¶ç‚¹æ•°-1 çš„è¿é€šå›¾ï¼Œå°±æ˜¯æ ‘ 12345678910111213141516171819 | BFS(G,s) \\\\ for each vertex u \\in G.V -{s}\\\\ u.color = WHITE\\\\ u.d = \\infty\\\\ u.\\pi = NIL\\\\ s.color = GRAY\\\\ s.d = 0\\\\ s.\\pi = NIL\\\\ Q = \\emptyset\\\\ ENQUEUE(Q,s)\\\\ while Q eq = \\emptyset\\\\ u = DEQUEUE(Q)\\\\ for each v \\in G.Adj[u]\\\\ if v.color == WHITE\\\\ v.color = GRAY\\\\ v.d = u.d+1\\\\ v.\\pi = u\\\\ ENQUEUE(Q,v)\\\\ u.color = BLACK\\\\ â€”|â€” å¹¿åº¦ä¼˜å…ˆæœç´¢çš„æµç¨‹å›¾ å®ä¾‹POJ3984ã€Šè¿·å®«é—®é¢˜ã€‹å®šä¹‰ä¸€ä¸ªäºŒç»´æ•°ç»„ï¼šint maze[5][5] = {0, 1, 0, 0, 0,0, 1, 0, 1, 0,0, 0, 0, 0, 0,0, 1, 1, 1, 0,0, 0, 0, 1, 0,};å®ƒè¡¨ç¤ºä¸€ä¸ªè¿·å®«ï¼Œå…¶ä¸­çš„1è¡¨ç¤ºå¢™å£ï¼Œ0è¡¨ç¤ºå¯ä»¥èµ°çš„è·¯ï¼Œåªèƒ½æ¨ªç€èµ°æˆ–ç«–ç€èµ°ï¼Œä¸èƒ½æ–œç€èµ°ï¼Œè¦æ±‚ç¼–ç¨‹åºæ‰¾å‡ºä»å·¦ä¸Šè§’åˆ°å³ä¸‹è§’çš„æœ€çŸ­è·¯çº¿ã€‚ è§£é¢˜æ€è·¯é˜Ÿåˆ—æ˜¯å…ˆè¿›åå‡ºï¼Œåè¿›å…ˆå‡ºã€‚ å¯¹åº”äºé¢˜ç›®çš„è¾“å…¥æ•°ç»„ï¼š0, 1, 0, 0, 0,0, 1, 0, 1, 0,0, 0, 0, 0, 0,0, 1, 1, 1, 0,0, 0, 0, 1, 0,æŠŠèŠ‚ç‚¹å®šä¹‰ä¸º(x,y)ï¼Œ(x,y)è¡¨ç¤ºæ•°ç»„mazeçš„é¡¹maze[x][y]ã€‚äºæ˜¯èµ·ç‚¹å°±æ˜¯(0,0)ï¼Œç»ˆç‚¹æ˜¯(4,4)ã€‚æŒ‰ç…§åˆšåˆšçš„æ€è·¯ï¼Œæ‰‹å·¥æ¢³ç†ä¸€éï¼šåˆå§‹æ¡ä»¶ï¼šèµ·ç‚¹Vsä¸º(0,0)ç»ˆç‚¹Vdä¸º(4,4)ç°è‰²èŠ‚ç‚¹é›†åˆQ={}åˆå§‹åŒ–æ‰€æœ‰èŠ‚ç‚¹ä¸ºç™½è‰²èŠ‚ç‚¹ä¸‹é¢å¼€å§‹å¹¿åº¦ä¼˜å…ˆæœç´¢ï¼š1.èµ·å§‹èŠ‚ç‚¹Vså˜æˆç°è‰²ï¼ŒåŠ å…¥é˜Ÿåˆ—Qï¼ŒQ={(0,0)}2.å–å‡ºé˜Ÿåˆ—Qçš„å¤´ä¸€ä¸ªèŠ‚ç‚¹Vnï¼ŒVn={0,0}ï¼ŒQ={}3.æŠŠVn={0,0}æŸ“æˆé»‘è‰²ï¼Œå–å‡ºVnæ‰€æœ‰ç›¸é‚»çš„ç™½è‰²èŠ‚ç‚¹{(1,0)}4.ä¸åŒ…å«ç»ˆç‚¹(4,4)ï¼ŒæŸ“æˆç°è‰²ï¼ŒåŠ å…¥é˜Ÿåˆ—Qï¼ŒQ={(1,0)}5.å–å‡ºé˜Ÿåˆ—Qçš„å¤´ä¸€ä¸ªèŠ‚ç‚¹Vnï¼ŒVn={1,0}ï¼ŒQ={}6.æŠŠVn={1,0}æŸ“æˆé»‘è‰²ï¼Œå–å‡ºVnæ‰€æœ‰ç›¸é‚»çš„ç™½è‰²èŠ‚ç‚¹{(2,0)}7.ä¸åŒ…å«ç»ˆç‚¹(4,4)ï¼ŒæŸ“æˆç°è‰²ï¼ŒåŠ å…¥é˜Ÿåˆ—Qï¼ŒQ={(2,0)}8.å–å‡ºé˜Ÿåˆ—Qçš„å¤´ä¸€ä¸ªèŠ‚ç‚¹Vnï¼ŒVn={2,0}ï¼ŒQ={}9.æŠŠVn={2,0}æŸ“æˆé»‘è‰²ï¼Œå–å‡ºVnæ‰€æœ‰ç›¸é‚»çš„ç™½è‰²èŠ‚ç‚¹{(2,1), (3,0)}10.ä¸åŒ…å«ç»ˆç‚¹(4,4)ï¼ŒæŸ“æˆç°è‰²ï¼ŒåŠ å…¥é˜Ÿåˆ—Qï¼ŒQ={(2,1), (3,0)}11.å–å‡ºé˜Ÿåˆ—Qçš„å¤´ä¸€ä¸ªèŠ‚ç‚¹Vnï¼ŒVn={2,1}ï¼ŒQ={(3,0)}12.æŠŠVn={2,1}æŸ“æˆé»‘è‰²ï¼Œå–å‡ºVnæ‰€æœ‰ç›¸é‚»çš„ç™½è‰²èŠ‚ç‚¹{(2,2)}13.ä¸åŒ…å«ç»ˆç‚¹(4,4)ï¼ŒæŸ“æˆç°è‰²ï¼ŒåŠ å…¥é˜Ÿåˆ—Qï¼ŒQ={(3,0), (2,2)}14.æŒç»­ä¸‹å»ï¼ŒçŸ¥é“Vnçš„æ‰€æœ‰ç›¸é‚»çš„ç™½è‰²èŠ‚ç‚¹ä¸­åŒ…å«äº†(4,4)â€¦â€¦15.æ­¤æ—¶è·å¾—äº†ç­”æ¡ˆ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 | // BFS.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚ // #include \"stdafx.h\" #include iostream using namespace std; int map[5][5]; //ç›¸é‚»å››ä¸ªèŠ‚ç‚¹ int borderUponX[4] = { 0, 0, 1, -1 }; int borderUponY[4] = { 1, -1, 0, 0 }; int front = 0, rear = 1; struct node { int pre; int x; int y; } path[100]; void print(int i) {//å½“å‰èŠ‚ç‚¹ if (path[i].pre != -1) {//æ‰¾åˆ°å‰é¢é‚£ä¸ªèŠ‚ç‚¹ print(path[i].pre); cout \"(\" path[i].x \",\" path[i].y \")\" endl; } else {//æœ€å‰é¢çš„é‚£ä¸ªèŠ‚ç‚¹ cout \"(\" path[i].x \",\" path[i].y \")\" endl; } } void bfsSearch(int x, int y) { //å¼€å§‹èŠ‚ç‚¹ï¼ˆå‡ºå‘ï¼‰ï¼Œå‰é¢æ²¡æœ‰èŠ‚ç‚¹äº† path[front].x = x; path[front].y = y; path[front].pre = -1; //å½“front == rearçš„æ—¶å€™è¯´æ˜å·²ç»èµ°å®Œäº†æ‰€ä»¥â€œç›¸é‚»â€èŠ‚ç‚¹ //ä¸”éƒ½ä¸é€š while (front rear) { for (int i = 0; i != 4; i++) { //ç›¸é‚»èŠ‚ç‚¹åæ ‡ int pathX = path[front].x + borderUponX[i]; int pathY = path[front].y + borderUponY[i]; //ä¸ç¬¦åˆçš„èŠ‚ç‚¹ï¼ˆé‡åˆ°è¾¹ç•Œæˆ–å·²ç»èµ°è¿‡äº†ï¼‰ if (pathY 0 || pathX 0 || pathX 4 || pathY 4 || map[pathX][pathY]) continue; else {//å°†frontçš„ç›¸é‚»çš„å¯ä»¥è¿‡å»çš„å¹¶ä¸”æ˜¯è¿˜æ²¡æœ‰èµ°è¿‡çš„èŠ‚ç‚¹åŠ åˆ°è·¯å¾„é‡Œé¢ map[pathX][pathY] = 1; path[rear].x = pathX; path[rear].y = pathY; path[rear].pre = front; rear++; } if (pathX == 4 pathY == 4) { //æ‰¾åˆ°äº†ä¸€æ¡è·¯å¾„ï¼Œåˆæ˜¯ç¬¬ä¸€æ¬¡æ‰¾åˆ° //é‚£ä¹ˆå°±æ˜¯æœ€çŸ­è·¯å¾„äº† print(rear - 1); break; } } front++; } } int main(int argc, char const *argv[]) { for (int i = 0; i 5; i++) for (int j = 0; j 5; j++) cin map[i][j]; bfsSearch(0, 0); system(\"pause\"); return 0; } â€”|â€”","tags":["BFS"],"categories":["ç®—æ³•"]},{"title":"PILæ¨¡å—","path":"/2019/08/28/PILæ¨¡å—/","content":"å¯¼å…¥1 | from PIL import Image â€”|â€” è¯»å–1 | img = Image.open(filepath) â€”|â€” æ˜¾ç¤º1 | img.show() â€”|â€” ä¸ numpy æ•°ç»„çš„äº’ç›¸è½¬æ¢PIL Image è½¬ numpy æ•°ç»„ 1 | img_to_array = np.array(img) â€”|â€” numpy æ•°ç»„è½¬ PIL Image (æ³¨æ„è¦ç¡®ä¿æ•°ç»„å†…çš„å€¼ç¬¦åˆ PIL çš„è¦æ±‚) 1 | array_to_img = Image.fromarray(img_to_array) â€”|â€” PIL ä¸ cv2 æ ¼å¼äº’ç›¸è½¬æ¢PIL.Imageè¯»å…¥çš„å›¾ç‰‡æ•°æ®ç±»å‹ä¸æ˜¯ numpy æ•°ç»„, å®ƒçš„sizeå±æ€§ä¸º (w, h), åˆ©ç”¨np.arrayè½¬æ¢æˆ numpy æ•°ç»„å, å®ƒçš„é€šé“é¡ºåºä¸º (r, g, b) 1 2 3 4 5 6 7 8 9 10 11 | from PIL import Image import numpy as np # PIL to cv2 pil_img = Image.open(img_path) print(pil_img.size) # (w, h) np_img = np.array(pil_img) cv2_img = np_img[:, :, ::-1] # äº¤æ¢é€šé“ # cv2 to PIL pil_img = Image.fromarray(cv2_img[:, :, ::-1]) â€”|â€”","tags":["Python"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"Pytorch trick","path":"/2019/08/28/Pytorch-trick/","content":"ç›®å½•ï¼š æŒ‡å®šGPUç¼–å· æŸ¥çœ‹æ¨¡å‹æ¯å±‚è¾“å‡ºè¯¦æƒ… æ¢¯åº¦è£å‰ª æ‰©å±•å•å¼ å›¾ç‰‡ç»´åº¦ ç‹¬çƒ­ç¼–ç  é˜²æ­¢éªŒè¯æ¨¡å‹æ—¶çˆ†æ˜¾å­˜ å­¦ä¹ ç‡è¡°å‡ å†»ç»“æŸäº›å±‚çš„å‚æ•° å¯¹ä¸åŒå±‚ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡ 1. æŒ‡å®šGPUç¼–å· è®¾ç½®å½“å‰ä½¿ç”¨çš„GPUè®¾å¤‡ä»…ä¸º0å·è®¾å¤‡ï¼Œè®¾å¤‡åç§°ä¸º /gpu:0ï¼šos.environ[â€œCUDA_VISIBLE_DEVICESâ€] = â€œ0â€ è®¾ç½®å½“å‰ä½¿ç”¨çš„GPUè®¾å¤‡ä¸º0,1å·ä¸¤ä¸ªè®¾å¤‡ï¼Œåç§°ä¾æ¬¡ä¸º /gpu:0ã€/gpu:1ï¼š os.environ[â€œCUDA_VISIBLE_DEVICESâ€] = â€œ0,1â€ ï¼Œæ ¹æ®é¡ºåºè¡¨ç¤ºä¼˜å…ˆä½¿ç”¨0å·è®¾å¤‡,ç„¶åä½¿ç”¨1å·è®¾å¤‡ã€‚æŒ‡å®šGPUçš„å‘½ä»¤éœ€è¦æ”¾åœ¨å’Œç¥ç»ç½‘ç»œç›¸å…³çš„ä¸€ç³»åˆ—æ“ä½œçš„å‰é¢ã€‚ 2.æŸ¥çœ‹æ¨¡å‹æ¯å±‚è¾“å‡ºè¯¦æƒ…Kerasæœ‰ä¸€ä¸ªç®€æ´çš„APIæ¥æŸ¥çœ‹æ¨¡å‹çš„æ¯ä¸€å±‚è¾“å‡ºå°ºå¯¸ï¼Œè¿™åœ¨è°ƒè¯•ç½‘ç»œæ—¶éå¸¸æœ‰ç”¨ã€‚ç°åœ¨åœ¨PyTorchä¸­ä¹Ÿå¯ä»¥å®ç°è¿™ä¸ªåŠŸèƒ½ã€‚ ä½¿ç”¨å¾ˆç®€å•ï¼Œå¦‚ä¸‹ç”¨æ³•ï¼š 1 2 | from torchsummary import summary summary(your_model, input_size=(channels, H, W)) â€”|â€” input_size æ˜¯æ ¹æ®ä½ è‡ªå·±çš„ç½‘ç»œæ¨¡å‹çš„è¾“å…¥å°ºå¯¸è¿›è¡Œè®¾ç½®ã€‚ 3.æ¢¯åº¦è£å‰ªï¼ˆGradient Clippingï¼‰1 2 3 4 5 6 7 8 | import torch.nn as nn outputs = model(data) loss= loss_fn(outputs, target) optimizer.zero_grad() loss.backward() nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2) optimizer.step() â€”|â€” nn.utils.clip_grad_norm_ çš„å‚æ•°ï¼š parameters â€“ ä¸€ä¸ªåŸºäºå˜é‡çš„è¿­ä»£å™¨ï¼Œä¼šè¿›è¡Œæ¢¯åº¦å½’ä¸€åŒ– max_norm â€“ æ¢¯åº¦çš„æœ€å¤§èŒƒæ•° max_norm â€“ æ¢¯åº¦çš„æœ€å¤§èŒƒæ•°çŸ¥ä¹ç”¨æˆ· ä¸æ¤­çš„æ¤­åœ† æå‡ºï¼šæ¢¯åº¦è£å‰ªåœ¨æŸäº›ä»»åŠ¡ä¸Šä¼šé¢å¤–æ¶ˆè€—å¤§é‡çš„è®¡ç®—æ—¶é—´ï¼Œå¯ç§»æ­¥è¯„è®ºåŒºæŸ¥çœ‹è¯¦æƒ…ã€‚ 4ã€æ‰©å±•å•å¼ å›¾ç‰‡ç»´åº¦å› ä¸ºåœ¨è®­ç»ƒæ—¶çš„æ•°æ®ç»´åº¦ä¸€èˆ¬éƒ½æ˜¯ (batch_size, c, h, w)ï¼Œè€Œåœ¨æµ‹è¯•æ—¶åªè¾“å…¥ä¸€å¼ å›¾ç‰‡ï¼Œæ‰€ä»¥éœ€è¦æ‰©å±•ç»´åº¦ï¼Œæ‰©å±•ç»´åº¦æœ‰å¤šä¸ªæ–¹æ³•ï¼š 1 2 3 4 5 6 7 8 9 10 11 12 13 | import cv2 import torch image = cv2.imread(img_path) image = torch.tensor(image) print(image.size()) img = image.view(1, *image.size()) print(img.size()) # output: # torch.Size([h, w, c]) # torch.Size([1, h, w, c]) â€”|â€” æˆ–è€… 1 2 3 4 5 6 7 8 9 10 11 | import cv2 import numpy as np image = cv2.imread(img_path) print(image.shape) img = image[np.newaxis, :, :, :] print(img.shape) # output: # (h, w, c) # (1, h, w, c) â€”|â€” æˆ–è€… 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | import cv2 import torch image = cv2.imread(img_path) image = torch.tensor(image) print(image.size()) img = image.unsqueeze(dim=0) print(img.size()) img = img.squeeze(dim=0) print(img.size()) # output: # torch.Size([(h, w, c)]) # torch.Size([1, h, w, c]) # torch.Size([h, w, c]) â€”|â€” tensor.unsqueeze(dim)ï¼šæ‰©å±•ç»´åº¦ï¼ŒdimæŒ‡å®šæ‰©å±•å“ªä¸ªç»´åº¦ã€‚ tensor.squeeze(dim)ï¼šå»é™¤dimæŒ‡å®šçš„ä¸”sizeä¸º1çš„ç»´åº¦ï¼Œç»´åº¦å¤§äº1æ—¶ï¼Œsqueeze()ä¸èµ·ä½œç”¨ï¼Œä¸æŒ‡å®šdimæ—¶ï¼Œå»é™¤æ‰€æœ‰sizeä¸º1çš„ç»´åº¦ã€‚ 5.ç‹¬çƒ­ç¼–ç åœ¨PyTorchä¸­ä½¿ç”¨äº¤å‰ç†µæŸå¤±å‡½æ•°çš„æ—¶å€™ä¼šè‡ªåŠ¨æŠŠlabelè½¬åŒ–æˆonehotï¼Œæ‰€ä»¥ä¸ç”¨æ‰‹åŠ¨è½¬åŒ–ï¼Œè€Œä½¿ç”¨MSEéœ€è¦æ‰‹åŠ¨è½¬åŒ–æˆonehotç¼–ç ã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 | import torch class_num = 8 batch_size = 4 def one_hot(label): \"\"\" å°†ä¸€ç»´åˆ—è¡¨è½¬æ¢ä¸ºç‹¬çƒ­ç¼–ç  \"\"\" label = label.resize_(batch_size, 1) m_zeros = torch.zeros(batch_size, class_num) # ä» value ä¸­å–å€¼ï¼Œç„¶åæ ¹æ® dim å’Œ index ç»™ç›¸åº”ä½ç½®èµ‹å€¼ onehot = m_zeros.scatter_(1, label, 1) # (dim,index,value) return onehot.numpy() # Tensor - Numpy label = torch.LongTensor(batch_size).random_() % class_num # å¯¹éšæœºæ•°å–ä½™ print(one_hot(label)) # output: [[0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0.]] â€”|â€” 6. é˜²æ­¢éªŒè¯æ¨¡å‹æ—¶çˆ†æ˜¾å­˜éªŒè¯æ¨¡å‹æ—¶ä¸éœ€è¦æ±‚å¯¼ï¼Œå³ä¸éœ€è¦æ¢¯åº¦è®¡ç®—ï¼Œå…³é—­autogradï¼Œå¯ä»¥æé«˜é€Ÿåº¦ï¼ŒèŠ‚çº¦å†…å­˜ã€‚å¦‚æœä¸å…³é—­å¯èƒ½ä¼šçˆ†æ˜¾å­˜ã€‚ 1 2 3 | with torch.no_grad(): # ä½¿ç”¨modelè¿›è¡Œé¢„æµ‹çš„ä»£ç  pass â€”|â€” æ„Ÿè°¢çŸ¥ä¹ç”¨æˆ·zhaz çš„æé†’ï¼Œæˆ‘æŠŠ torch.cuda.empty_cache() çš„ä½¿ç”¨åŸå› æ›´æ–°ä¸€ä¸‹ã€‚ è¿™æ˜¯åŸå›ç­”ï¼š Pytorch è®­ç»ƒæ—¶æ— ç”¨çš„ä¸´æ—¶å˜é‡å¯èƒ½ä¼šè¶Šæ¥è¶Šå¤šï¼Œå¯¼è‡´ out of memory ï¼Œå¯ä»¥ä½¿ç”¨ä¸‹é¢è¯­å¥æ¥æ¸…ç†è¿™äº›ä¸éœ€è¦çš„å˜é‡ã€‚ å®˜ç½‘ ä¸Šçš„è§£é‡Šä¸ºï¼š Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.torch.cuda.empty_cache() æ„æ€å°±æ˜¯PyTorchçš„ç¼“å­˜åˆ†é…å™¨ä¼šäº‹å…ˆåˆ†é…ä¸€äº›å›ºå®šçš„æ˜¾å­˜ï¼Œå³ä½¿å®é™…ä¸Štensorså¹¶æ²¡æœ‰ä½¿ç”¨å®Œè¿™äº›æ˜¾å­˜ï¼Œè¿™äº›æ˜¾å­˜ä¹Ÿä¸èƒ½è¢«å…¶ä»–åº”ç”¨ä½¿ç”¨ã€‚è¿™ä¸ªåˆ†é…è¿‡ç¨‹ç”±ç¬¬ä¸€æ¬¡CUDAå†…å­˜è®¿é—®è§¦å‘çš„ã€‚ è€Œ torch.cuda.empty_cache() çš„ä½œç”¨å°±æ˜¯é‡Šæ”¾ç¼“å­˜åˆ†é…å™¨å½“å‰æŒæœ‰çš„ä¸”æœªå ç”¨çš„ç¼“å­˜æ˜¾å­˜ï¼Œä»¥ä¾¿è¿™äº›æ˜¾å­˜å¯ä»¥è¢«å…¶ä»–GPUåº”ç”¨ç¨‹åºä¸­ä½¿ç”¨ï¼Œå¹¶ä¸”é€šè¿‡ nvidia-smiå‘½ä»¤å¯è§ã€‚æ³¨æ„ä½¿ç”¨æ­¤å‘½ä»¤ä¸ä¼šé‡Šæ”¾tensorså ç”¨çš„æ˜¾å­˜ã€‚ å¯¹äºä¸ç”¨çš„æ•°æ®å˜é‡ï¼ŒPytorch å¯ä»¥è‡ªåŠ¨è¿›è¡Œå›æ”¶ä»è€Œé‡Šæ”¾ç›¸åº”çš„æ˜¾å­˜ã€‚ 7. å­¦ä¹ ç‡è¡°å‡1 2 3 4 5 6 7 8 9 10 11 | import torch.optim as optim from torch.optim import lr_scheduler # è®­ç»ƒå‰çš„åˆå§‹åŒ– optimizer = optim.Adam(net.parameters(), lr=0.001) scheduler = lr_scheduler.StepLR(optimizer, 10, 0.1) # # æ¯è¿‡10ä¸ªepochï¼Œå­¦ä¹ ç‡ä¹˜ä»¥0.1 # è®­ç»ƒè¿‡ç¨‹ä¸­ for n in n_epoch: scheduler.step() ... â€”|â€” 8. å†»ç»“æŸäº›å±‚çš„å‚æ•°åœ¨åŠ è½½é¢„è®­ç»ƒæ¨¡å‹çš„æ—¶å€™ï¼Œæˆ‘ä»¬æœ‰æ—¶æƒ³å†»ç»“å‰é¢å‡ å±‚ï¼Œä½¿å…¶å‚æ•°åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ä¸å‘ç”Ÿå˜åŒ–ã€‚ æˆ‘ä»¬éœ€è¦å…ˆçŸ¥é“æ¯ä¸€å±‚çš„åå­—ï¼Œé€šè¿‡å¦‚ä¸‹ä»£ç æ‰“å°ï¼š 1 2 3 | net = Network() # è·å–è‡ªå®šä¹‰ç½‘ç»œç»“æ„ for name, value in net.named_parameters(): print('name: {0}, grad: {1}'.format(name, value.requires_grad)) â€”|â€” å‡è®¾å‰å‡ å±‚ä¿¡æ¯å¦‚ä¸‹ï¼š 1 2 3 4 5 6 7 8 | name: cnn.VGG_16.convolution1_1.weight, grad: True name: cnn.VGG_16.convolution1_1.bias, grad: True name: cnn.VGG_16.convolution1_2.weight, grad: True name: cnn.VGG_16.convolution1_2.bias, grad: True name: cnn.VGG_16.convolution2_1.weight, grad: True name: cnn.VGG_16.convolution2_1.bias, grad: True name: cnn.VGG_16.convolution2_2.weight, grad: True name: cnn.VGG_16.convolution2_2.bias, grad: True â€”|â€” åé¢çš„Trueè¡¨ç¤ºè¯¥å±‚çš„å‚æ•°å¯è®­ç»ƒï¼Œç„¶åæˆ‘ä»¬å®šä¹‰ä¸€ä¸ªè¦å†»ç»“çš„å±‚çš„åˆ—è¡¨ï¼š 1 2 3 4 5 6 | no_grad = [ 'cnn.VGG_16.convolution1_1.weight', 'cnn.VGG_16.convolution1_1.bias', 'cnn.VGG_16.convolution1_2.weight', 'cnn.VGG_16.convolution1_2.bias' ] â€”|â€” å†»ç»“æ–¹æ³•å¦‚ä¸‹ï¼š 1 2 3 4 5 6 | net = Net.CTPN() # è·å–ç½‘ç»œç»“æ„ for name, value in net.named_parameters(): if name in no_grad: value.requires_grad = False else: value.requires_grad = True â€”|â€” å†»ç»“åæˆ‘ä»¬å†æ‰“å°æ¯å±‚çš„ä¿¡æ¯ï¼š 1 2 3 4 5 6 7 8 | name: cnn.VGG_16.convolution1_1.weight, grad: False name: cnn.VGG_16.convolution1_1.bias, grad: False name: cnn.VGG_16.convolution1_2.weight, grad: False name: cnn.VGG_16.convolution1_2.bias, grad: False name: cnn.VGG_16.convolution2_1.weight, grad: True name: cnn.VGG_16.convolution2_1.bias, grad: True name: cnn.VGG_16.convolution2_2.weight, grad: True name: cnn.VGG_16.convolution2_2.bias, grad: True â€”|â€” å¯ä»¥çœ‹åˆ°å‰ä¸¤å±‚çš„weightå’Œbiasçš„requires_gradéƒ½ä¸ºFalseï¼Œè¡¨ç¤ºå®ƒä»¬ä¸å¯è®­ç»ƒã€‚æœ€ååœ¨å®šä¹‰ä¼˜åŒ–å™¨æ—¶ï¼Œåªå¯¹requires_gradä¸ºTrueçš„å±‚çš„å‚æ•°è¿›è¡Œæ›´æ–°ã€‚ 1 | optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.01) â€”|â€” 9. å¯¹ä¸åŒå±‚ä½¿ç”¨ä¸åŒå­¦ä¹ ç‡æˆ‘ä»¬å¯¹æ¨¡å‹çš„ä¸åŒå±‚ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡ã€‚ è¿˜æ˜¯ä½¿ç”¨è¿™ä¸ªæ¨¡å‹ä½œä¸ºä¾‹å­ï¼š 1 2 3 4 5 6 7 8 9 10 11 12 13 | net = Network() # è·å–è‡ªå®šä¹‰ç½‘ç»œç»“æ„ for name, value in net.named_parameters(): print('name: {}'.format(name)) # è¾“å‡ºï¼š # name: cnn.VGG_16.convolution1_1.weight # name: cnn.VGG_16.convolution1_1.bias # name: cnn.VGG_16.convolution1_2.weight # name: cnn.VGG_16.convolution1_2.bias # name: cnn.VGG_16.convolution2_1.weight # name: cnn.VGG_16.convolution2_1.bias # name: cnn.VGG_16.convolution2_2.weight # name: cnn.VGG_16.convolution2_2.bias â€”|â€” å¯¹ convolution1 å’Œ convolution2 è®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡ï¼Œé¦–å…ˆå°†å®ƒä»¬åˆ†å¼€ï¼Œå³æ”¾åˆ°ä¸åŒçš„åˆ—è¡¨é‡Œï¼š 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | conv1_params = [] conv2_params = [] for name, parms in net.named_parameters(): if \"convolution1\" in name: conv1_params += [parms] else: conv2_params += [parms] # ç„¶ååœ¨ä¼˜åŒ–å™¨ä¸­è¿›è¡Œå¦‚ä¸‹æ“ä½œï¼š optimizer = optim.Adam( [ {\"params\": conv1_params, 'lr': 0.01}, {\"params\": conv2_params, 'lr': 0.001}, ], weight_decay=1e-3, ) â€”|â€” æˆ‘ä»¬å°†æ¨¡å‹åˆ’åˆ†ä¸ºä¸¤éƒ¨åˆ†ï¼Œå­˜æ”¾åˆ°ä¸€ä¸ªåˆ—è¡¨é‡Œï¼Œæ¯éƒ¨åˆ†å°±å¯¹åº”ä¸Šé¢çš„ä¸€ä¸ªå­—å…¸ï¼Œåœ¨å­—å…¸é‡Œè®¾ç½®ä¸åŒçš„å­¦ä¹ ç‡ã€‚å½“è¿™ä¸¤éƒ¨åˆ†æœ‰ç›¸åŒçš„å…¶ä»–å‚æ•°æ—¶ï¼Œå°±å°†è¯¥å‚æ•°æ”¾åˆ°åˆ—è¡¨å¤–é¢ä½œä¸ºå…¨å±€å‚æ•°ï¼Œå¦‚ä¸Šé¢çš„weight_decayã€‚ ä¹Ÿå¯ä»¥åœ¨åˆ—è¡¨å¤–è®¾ç½®ä¸€ä¸ªå…¨å±€å­¦ä¹ ç‡ï¼Œå½“å„éƒ¨åˆ†å­—å…¸é‡Œè®¾ç½®äº†å±€éƒ¨å­¦ä¹ ç‡æ—¶ï¼Œå°±ä½¿ç”¨è¯¥å­¦ä¹ ç‡ï¼Œå¦åˆ™å°±ä½¿ç”¨åˆ—è¡¨å¤–çš„å…¨å±€å­¦ä¹ ç‡ã€‚ æ˜¾ç¤ºè®­ç»ƒæ—¶é—´1 2 3 4 5 6 | for epoch in range(start_epoch, config.epochs): start = time.time() train_loss, lr = train_epoch(model, optimizer, scheduler, train_loader, device, criterion, epoch, all_step, writer, logger) logger.info('[{}/{}], train_loss: {:.4f}, time: {:.4f}, lr: {}'.format( epoch, config.epochs, train_loss, time.time() - start, lr)) â€”|â€” å‚è€ƒï¼šhttps://mp.weixin.qq.com/s?__biz=MzU3NjE4NjQ4MA==mid=2247485953idx=2sn=3ae788b7d643541254ba311f7a7facedchksm=fd16fb1eca61720870bc58c1a465a346cf2c6a7e8bea39e4b3d582474b595021f3a5b635086dmpshare=1scene=1srcid=sharer_sharetime=1566885137387sharer_shareid=285785c5623899db73795495779fe8be#rd","tags":["Python"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"Pytorchæ‰‹å†Œ","path":"/2019/08/28/Pytorchæ‰‹å†Œ/","content":"PyTorch ä¸»è¦æä¾›ä»¥ä¸‹ä¸¤å¤§ç‰¹è‰²ï¼š æ”¯æŒå¼ºåŠ›GPUåŠ é€Ÿçš„Tensorè®¡ç®—èƒ½åŠ› åŸºäºtapeçš„å…·æœ‰è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼èƒ½åŠ›çš„æ·±åº¦ç¥ç»ç½‘ç»œæ¡†æ¶ PyTorch ä¸»è¦åŒ…å«ä»¥ä¸‹ç»„æˆè¦ç´ : ç»„æˆè¦ç´  æè¿°è¯´æ˜ torch ä¸€ä¸ªç±»ä¼¼äºnumpyçš„tensorå“­, æä¾›å¼ºåŠ›çš„GPUæ”¯æŒ torch.autograd ä¸€ä¸ªåŸºäºtapeçš„å…·æœ‰è‡ªåŠ¨å¾®åˆ†æ±‚å¯¼èƒ½åŠ›çš„åº“, å¯ä»¥æ”¯æŒå‡ ä¹æ‰€æœ‰çš„tesnor operatioin torch.nn ä¸€ä¸ªç¥ç»ç½‘ç»œåº“, ä¸autogradæ·±åº¦æ•´åˆ, å¯ä»¥æä¾›æœ€å¤§é™åº¦çš„çµæ´»æ€§ torch.multiprocessing Pythonçš„å¤šçº¿ç¨‹å¤„ç†, å¯ä»¥æä¾›torch Tensorsä¹‹é—´çš„å†…å­˜å…±äº«, å¯¹äºåŠ è½½æ•°æ®å’ŒHogwild trainingæ¥è¯´ååˆ†æœ‰ç”¨ torch.utils ä¸€äº›åŠŸèƒ½ç±»å’Œå‡½æ•°, å¦‚DataLoader, Trainerç­‰ç­‰ torch.legacy(.nn/.optim) ä¸ºäº†å…¼å®¹æ€§è€Œå­˜åœ¨çš„ä¸€äº›ä»£ç å’Œå®ç° Pytorché€šå¸¸å¯ä»¥ä½œä¸ºä»¥ä¸‹ç”¨é€”ä½¿ç”¨: ä¸ºäº†ä½¿ç”¨GPUsæ€§èƒ½çš„numpyæ›¿ä»£å“ å¯ä»¥æä¾›å¼ºå¤§çµæ´»åŠ›å’Œé€Ÿåº¦ä¼˜åŠ¿çš„æ·±åº¦å­¦ä¹ å¹³å°. torchbackends.cudnn 1 | torch.backends.cudnn.benchmark = True â€”|â€” ä¸Šè¿°è®¾ç½®å¯ä»¥è®©å†…ç½®çš„cudnnçš„auto-tunerè‡ªåŠ¨å¯»æ‰¾æœ€åˆé€‚å½“å‰é…ç½®çš„æç¬‘ç®—æ³•, æ¥è¾¾åˆ°ä¼˜åŒ–è¿è¡Œæ•ˆç‡çš„ç›®æ ‡, åœ¨ä½¿ç”¨æ—¶, åº”è¯¥éµå¾ªä»¥ä¸‹ä¸¤ä¸ªå‡†åˆ™: å¦‚æœç½‘ç»œçš„è¾“å…¥æ•°æ®ç»´åº¦æˆ–ç±»å‹ä¸Šå˜åŒ–ä¸å¤§, åˆ™è¯¥è®¾ç½®å¯ä»¥å¢åŠ è¿è¡Œæ•ˆç‡ å¦‚æœç½‘ç»œçš„è¾“å…¥æ•°æ®åœ¨æ¯æ¬¡çš„iterationä¸­éƒ½å˜åŒ–çš„è¯, ä¼šå¯¼è‡´cudnnæ¯æ¬¡éƒ½å¯»æ‰¾ä¸€éæœ€ä¼˜é…ç½®, è¿™æ ·åè€Œ ä¼šé™ä½ è¿è¡Œæ•ˆç‡. torch.cat() 1 | torch.cat(seq, dim=0, out=None) # è¿”å›è¿æ¥åçš„tensor â€”|â€” å°†ç»™å®šçš„ tensor åºåˆ— seq æŒ‰ç…§ç»´åº¦è¿æ¥èµ·æ¥. é»˜è®¤ç»´åº¦ä¸º0, è¯´æ˜ä¼šå°†å…¶åœ¨ç¬¬ 0 ä¸ªç»´åº¦ä¸Šè¿›è¡Œæ‹¼æ¥.(æœ€åçš„ç»“æœæ˜¯ç¬¬ 0 ç»´åº¦å¢å¤§, ä¾‹å¦‚ä¸‰ä¸ª2è¡Œ3åˆ—çš„ tensor æŒ‰ç…§ç¬¬0ç»´åº¦æ‹¼æ¥, æœ€åå¾—åˆ°çš„ tensor ç»´åº¦ä¸º6è¡Œ3åˆ—) clamp()/clamp_()1 | torch.clamp(input, min, max, out=None) - Tensor â€”|â€” å°†inputé‡Œé¢å…ƒç´ å…¨éƒ¨åˆ’åˆ†åˆ°[min,max]åŒºé—´å†…, å°äºminçš„ç½®ä¸ºmin, å¤§äºmaxçš„ç½®ä¸ºmax. å¦‚æœä¸æŒ‡å®šminæˆ–è€…max,åˆ™è®¤ä¸ºæ— ä¸‹ç•Œæˆ–ä¸Šç•Œå…¶ä»–è°ƒç”¨å½¢å¼: 1 | torch.Tensor(min, max) # è°ƒç”¨tensorä¸ºinput, è¿”å›å€¼ä¸ºout â€”|â€” device()1 | device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") â€”|â€” gather()1 | torch.gather(input, dim, index, out=None) - Tensor â€”|â€” æ²¿ç€dimæŒ‡å®šçš„è½´æŒ‰ç€indexæŒ‡å®šçš„å€¼é‡æ–°ç»„åˆæˆä¸€ä¸ªæ–°çš„tensor. 1 2 3 | out[i][j][k] = input[index[i][j][k]][j][k] # if dim == 0 out[i][j][k] = input[i][index[i][j][k]][k] # if dim == 1 out[i][j][k] = input[i][j][index[i][j][k]] # if dim == 2 â€”|â€” å³å‡è®¾inputæ˜¯ä¸€ä¸ª n ç»´çš„tensor, å…¶ size ä¸º (x0,x1,â€¦,xiâˆ’1,xi,xi+1,â€¦,xnâˆ’1), è‹¥dim=i, åˆ™ index å¿…é¡»ä¹Ÿæ˜¯ä¸€ä¸ª n ç»´çš„tensor, å…¶ size ä¸º (x0,x1,â€¦,xiâˆ’1,y,xi+1,â€¦,xnâˆ’1), å…¶ä¸­ yâ‰¥1, è€Œè¿”å›çš„ tensor out çš„ size å’Œ index çš„ size ç›¸åŒ.ä¸€å¥æ¥è¯´ gather çš„ä½œç”¨å°±æ˜¯, åœ¨æŒ‡å®šçš„ç»´åº¦ä¸Šç­›é€‰ç»™ç»™å®šä¸‹æ ‡indexæŒ‡ç¤ºçš„å€¼, å…¶ä»–å€¼èˆå¼ƒ.ä¸€ä¸ªä¾‹å­è¯´æ˜:scoresæ˜¯ä¸€ä¸ªè®¡ç®—å‡ºæ¥çš„åˆ†æ•°ï¼Œç±»å‹ä¸º[torch.FloatTensor of size 5x1000]è€Œy_varæ˜¯æ­£ç¡®åˆ†æ•°çš„ç´¢å¼•ï¼Œç±»å‹ä¸º[torch.LongTensor of size 5]å®¹æ˜“çŸ¥é“ï¼Œè¿™é‡Œæœ‰1000ä¸ªç±»åˆ«ï¼Œæœ‰5ä¸ªè¾“å…¥å›¾åƒï¼Œæ¯ä¸ªå›¾åƒå¾—å‡ºçš„åˆ†æ•°ä¸­åªæœ‰ä¸€ä¸ªæ˜¯æ­£ç¡®çš„ï¼Œæ­£ç¡®çš„ç´¢å¼•å°±åœ¨y_varä¸­ï¼Œè¿™é‡Œè¦åšçš„æ˜¯å°†æ­£ç¡®åˆ†æ•°æ ¹æ®ç´¢å¼•æ ‡å·æå–å‡ºæ¥ã€‚ 1 2 | scores = model(X_var) # åˆ†æ•° scores = scores.gather(1, y_var.view(-1, 1)).squeeze() #è¿›è¡Œæå– â€”|â€” æå–åçš„scoresæ ¼å¼ä¹Ÿä¸º[torch.FloatTensor of size 5]è¿™é‡Œè®²ä¸€ä¸‹å˜åŒ–è¿‡ç¨‹ï¼š é¦–å…ˆè¦çŸ¥é“ä¹‹å‰çš„scoresçš„sizeä¸º[5,1000]ï¼Œè€Œy_varçš„sizeä¸º[5]ï¼Œscoresä¸º2ç»´ï¼Œy_varä¸º1ç»´ä¸åŒ¹é…ï¼Œæ‰€ä»¥å…ˆç”¨viewå°†å…¶å±•å¼€ä¸º[5,1]çš„sizeï¼Œè¿™æ ·ç»´æ•°nå°±ä¸scroesåŒ¹é…äº†ã€‚ æ¥ä¸‹æ¥è¿›è¡Œgatherï¼Œgatherå‡½æ•°ä¸­ç¬¬ä¸€ä¸ªå‚æ•°ä¸º1ï¼Œæ„æ€æ˜¯åœ¨ç¬¬äºŒç»´è¿›è¡Œæ±‡èšï¼Œä¹Ÿå°±æ˜¯è¯´é€šè¿‡y_varä¸­çš„äº”ä¸ªå€¼æ¥åœ¨scroesä¸­ç¬¬äºŒç»´çš„5ä¸ª1000ä¸­è¿›è¡Œä¸€ä¸€æŒ‘é€‰ï¼ŒæŒ‘é€‰å‡ºæ¥åçš„sizeä¹Ÿä¸º[5,1]ï¼Œç„¶åå†é€šè¿‡squeezeå°†é‚£ä¸ªä¸€ç»´å»æ‰ï¼Œæœ€åç»“æœä¸º[5].Tensorå½¢å¼ 1 | torch.Tensor.gather(dim, index) - Tensor â€”|â€” torch.ge()torch.gt()1 | torch.gt(input, other, out=None) # - Tensor â€”|â€” æ ¹æ® input å’Œ other çš„å€¼è¿”å›ä¸€ä¸ªäºŒå€¼ tensor, å¦‚æœæ»¡è¶³å¤§äºæ¡ä»¶åˆ™ä¸º1, ä¸æ»¡è¶³åˆ™ä¸º0.other å¯ä»¥æ˜¯èƒ½å¤Ÿè½¬æ¢æˆ input size çš„tensor, ä¹Ÿå¯ä»¥æ˜¯ä¸€ä¸ª float æ ‡é‡. torch.index_select()1 | torch.index_select(input, dim, index, out=None) # - Tensor â€”|â€” è¿”å›åœ¨ dim ç»´åº¦ä¸Šçš„ index æŒ‡æ˜çš„ä¸‹æ ‡ç»„æˆçš„ tensor.è¿”å›çš„ tensor çš„ç»´åº¦çš„æ•°é‡å’Œ input æ˜¯ç›¸åŒçš„, ä½†æ˜¯ç¬¬ dim ç»´åº¦çš„ size ä¼šå’Œ index sizeå¤§å°ç›¸åŒ. å…¶ä»–ç»´åº¦çš„ size ä¿æŒä¸å˜. torch.le()1 | torch.le(input, other, out=None) # -Tensor â€”|â€” æŒ‰å…ƒç´ è®¡ç®— inputâ‰¤other. max()1 2 3 | torch.max(input) # è¿”å›ä¸€ä¸ªTensor, ä»£è¡¨æ‰€æœ‰å…ƒç´ ä¸­çš„æœ€å¤§å€¼ torch.max(input,dim,keepdim=False,out=None) # è¿”å›ä¸€ä¸ªå…ƒç»„:(Tensor, LongTensor) â€”|â€” ç¬¬äºŒç§å½¢å¼ä¼šè¿”å›ä¸€ä¸ªå…ƒç»„, å…ƒç»„å†…å…ƒç´ ç±»å‹ä¸º: (Tensor, LongTensor), å…¶ä¸­, å‰è€…ä»£è¡¨å¯¹åº” dim ä¸Š reduce åçš„æœ€å¤§å€¼, åè€…ä»£è¡¨æœ€å¤§å€¼åœ¨ç»´åº¦ dim ä¸­å¯¹åº”çš„ä¸‹æ ‡.å¦‚æœkeepdim=True, åˆ™è¾“å‡ºçš„ tensor çš„ size ä¼šå’Œè¾“å…¥çš„ç›¸åŒ, åªä¸è¿‡å¯¹åº” dim ç»´åº¦ä¸Šçš„sizeä¸º1. å¦åˆ™, å¯¹åº” dim ç»´åº¦ä¼šè¢« squeeze/reduce, ä½¿å¾—è¾“å‡ºçš„ç»´åº¦æ¯”è¾“å…¥çš„ç»´åº¦å°‘1. 1 2 3 4 5 6 7 8 | a = torch.randn(4, 4) a tensor([[-1.2360, -0.2942, -0.1222, 0.8475], [ 1.1949, -1.1127, -2.2379, -0.6702], [ 1.5717, -0.9207, 0.1297, -1.8768], [-0.6172, 1.0036, -0.6060, -0.2432]]) torch.max(a, 1) (tensor([ 0.8475, 1.1949, 1.5717, 1.0036]), tensor([ 3, 0, 0, 1])) â€”|â€” mm()æ³¨æ„, æ²¡æœ‰torch.mm_ç‰ˆæœ¬ 1 | torch.mm(mat1, mat2, out=None) # è¿”å›å€¼ä¸ºTensor, ä¹Ÿå¯ä»¥ä½¿ç”¨outè®°å½•è¿”å›å€¼ â€”|â€” ä¸¤çŸ©é˜µç›¸ä¹˜, çŸ©é˜µçš„sizeéœ€è¦æ»¡è¶³ä¹˜æ³•è§„åˆ™å…¶ä»–è°ƒç”¨å½¢å¼: 1 | torch.Tensor(mat2) # è°ƒç”¨è€…ä¸ºmat1 â€”|â€” norm()è¿”å›è¾“å…¥tensorçš„p-normæ ‡é‡ 1 | torch.norm(input, p=2) # è¿”å›ä¸€ä¸ªæ ‡é‡tensor â€”|â€” numel()1 | torch.numel(input) #è¿”å›ä¸€ä¸ªintå€¼ â€”|â€” è¿”å› inpput tensor ä¸­çš„å…ƒç´ çš„æ€»ä¸ªæ•° 1 2 | a = torch.randn(1,2,3,4,5) print(torch.numel(a)) # 120 â€”|â€” ones()randn()æ ‡å‡†æ­£å¤ªåˆ†å¸ƒéšæœºåŸºç¡€, ä¼ å…¥å‚æ•°ä¸ºç»´åº¦ä¿¡æ¯ torch.sort()1 | torch.sort(input, dim=None, descending=False, out=None) # è¿”å› (Tensor, LongTensor) â€”|â€” sum()1 2 3 4 | torch.sum(input, dtype=None) # è¿”å›æ±‚å’Œåçš„Tensor(åªæœ‰ä¸€ä¸ªå…ƒç´ ) torch.sum(input, dim, keepdim=False, dtype=None) # è¿”å›åœ¨dimä¸Šreduceçš„sumå’Œ, å¦‚æœdimåŒ…å«å¤šä¸ªç»´åº¦, åˆ™éƒ½è¿›è¡Œreduceæ±‚å’Œ. # reduceè¿™ä¸ªè¯å¾ˆå½¢è±¡, å› ä¸ºè¿”å›çš„Tensorçš„ç»´åº¦åˆšå¥½æ²¡æœ‰äº†dimæŒ‡ç¤ºçš„é‚£äº›ç»´åº¦ â€”|â€” å…¶ä»–å½¢å¼: 1 | torch.Tensor.sum() â€”|â€” torch.t()1 | torch.t(input) # è¿”å›è½¬ç½®åçš„Tensor â€”|â€” å…¶ä»–å½¢å¼: 1 | torch.Tensor.t() â€”|â€” unsqueeze()åœ¨æŒ‡å®šç»´åº¦ä¸Šæ’å…¥ä¸€ä¸ª singleton ç»´åº¦(ä¸€èˆ¬ç”¨äºå°†å•ä¸€æ•°æ®å¤„ç†ç”¨ batch çš„å½¢å¼) 1 | torch.unsqueeze(input, dim, out=None) # - Tensor â€”|â€” è¿”å›çš„tensorä¸input tensor å…±äº«æ•°æ® dim çš„å–å€¼èŒƒå›´åœ¨ [-input.dim()-1, input.dim()+1] ä¹‹é—´, å¦‚æœä¸ºè´Ÿå€¼, åˆ™ç›¸å½“äº dim = dim + input.dim() + 1. zeros()torch.cudatorch.cuda.empty_cache()é‡Šæ”¾æ‰€æœ‰æœªä½¿ç”¨çš„ GPU å†…å­˜, ä½¿ç”¨è¿™äº›å†…å­˜å¯ä»¥è¢«å…¶ä»– GPU åº”ç”¨ä½¿ç”¨, å¹¶ä¸”å¯ä»¥è¢« nvidia-smi æŸ¥åˆ°.empty_cache() å¹¶ä¸ä¼šå¼ºåˆ¶æå‡ä¾› PyTorch ä½¿ç”¨çš„æ˜¾å¡å†…å­˜çš„å¤§å°, æŸ¥çœ‹Memory management torch.Tensortorch.Tensor æ˜¯é»˜è®¤ç±»å‹ torch.FloatTensor çš„åˆ«å, ä½¿ç”¨ torch.Tenosr çš„æ„é€ å‡½æ•°åˆ›å»º tensor å˜é‡æ—¶, ä¼ å…¥çš„æ˜¯ç»´åº¦ä¿¡æ¯(æ³¨æ„ä¸ torch.tensor() çš„åŒºåˆ«): 1 2 | t = torch.Tensor(2,3,4) # é‡Œé¢çš„æ•°å€¼æœªåˆå§‹åŒ–, æ˜¯éšæœºçš„ print(t.size()) # torch.Size([2,3,4]) â€”|â€” torch.LongTesnor ä½¿ç”¨æ–¹æ³•ç›¸ä¼¼, åªä¸è¿‡æ•°æ®ç±»å‹æ˜¯é•¿æ•´å‹. troch.tensor()åˆ›å»ºtensor 1 | torch.tensor(data, dtype=None, device=None, requires_grad=False) â€”|â€” å¯ä»¥åˆ©ç”¨torch.tensorä»pythonçš„listæ•°æ®æˆ–è€…å…¶ä»–åºåˆ—æ•°æ®ä¸­åˆ›å»ºtensorå¯¹è±¡ 1 2 | torch.tensor([[1,-1],[1,-1]]) torch.tensor(np.array([[1,2,3],[4,5,6]])) â€”|â€” æ³¨æ„, torch.tensor()å‡½æ•°æ€»æ˜¯ä¼šå¯¹æ•°æ®è¿›è¡Œå¤åˆ¶æ“ä½œ, å› æ­¤, å¦‚æœä½ ä»…ä»…æ˜¯æƒ³å°†æ•°æ®çš„requires_gradæ ‡å¿—æ”¹å˜, é‚£ä¹ˆå°±åº”è¯¥ä½¿ç”¨required_grad_()æˆ–è€…detach()å‡½æ•°æ¥é¿å…å¤åˆ¶. åŒæ—¶, å¯¹numpyæ•°ç»„ä½¿ç”¨torch.as_tensor()å°†å…¶è½¬æ¢æˆtensorè€Œæ— éœ€å¤åˆ¶ torch.Tensor.cpu()1 2 | torch.Tensor.cpu() z = x.cpu() â€”|â€” å°†tensorç§»åŠ¨åˆ°cpuä¸Š, æ³¨æ„è¿”å›å€¼zæ˜¯cpuä¸Šçš„æ•°æ®, tensor x æœ¬èº«çš„deviceå±æ€§ä¸å˜ torch.Tensor.cuda()1 2 | torch.Tensor.cuda() z = x.cuda() â€”|â€” torch.Tensor.dim()1 | torch.Tensor.dim() - int â€”|â€” è¿”å› tensor çš„ç»´åº¦çš„ä¸ªæ•°. torch.Tensor.max()1 | torch.Tensor.max(dim=None, keepdim=False) - Tensor or (Tensor, Tensor) â€”|â€” è¯¦æƒ…è§ torch.max() torch.Tensor.numel()1 | torch.Tensor.numel() â€”|â€” è¯¦è§ torch.numel() torch.Tensor.to()1 | torch.Tensor.to(*args, *kwargs) â€”|â€” è¿”å›ä¸€ä¸ªè½¬ç§»åçš„tensor, è€Œè‡ªèº«ç»´æŒä¸å˜ 1 2 3 4 | t = torch.randn(2,3) t.to(torch.float64) t.to(device) t.to(\"cuda:0\") â€”|â€” å°†tensorç§»åŠ¨åˆ°gpuä¸Š, æ³¨æ„è¿”å›å€¼ z æ˜¯gpu ä¸Šçš„æ•°æ®, tensor x æœ¬èº«çš„ device å±æ€§ä¸å˜ torch.Tensor.numpy()tensorä¸numpyæ•°ç»„çš„è½¬æ¢ 1 2 3 | torch.Tensor.numpy() # è¿”å›tensorå¯¹åº”çš„numpyæ•°ç»„ torch.from_numpy(ndarray) # å°†numpyæ•°ç»„ndarrayè½¬æ¢æˆå¯¹åº”çš„tensorå¹¶è¿”å›. â€”|â€” torch.Tensor å®é™…ä¸Šæ˜¯ torch.FloatFensor çš„åˆ«å torch.Tensor.permute()é‡æ–°æ’åˆ—tensorçš„ç»´åº¦ 1 | torch.Tensor.permute(*dims) # è¿”å›ä¸€ä¸ªé‡æ–°æ’åˆ—ç»´åº¦åçš„ tensor â€”|â€” torch.Tensor.unsqueeze()è¯¦ç»†å¯è§torch.unsqueeze torch.Tensor.expand()1 | torch.Tensor.expand(*sizes) # è¿”å› tensor â€”|â€” å°† tensor ä¸­çš„ singleton ç»´åº¦æ‰©å±•åˆ°ä¸€ä¸ªæ›´å¤§çš„ size.å‚æ•° -1 æ„å‘³ç€ä¸æ”¹å˜åŸå§‹çš„ç»´åº¦æ–°å¢çš„ç»´åº¦çš„å…ƒç´ è¢«è¢«æ·»åŠ åˆ°å‰å¤´, sizeä¸èƒ½è®¾ç½®ä¸º-1.expand å¹¶æ²¡æœ‰ç”³è¯·æ–°çš„å†…å­˜, è€Œä»…ä»…æ˜¯åœ¨å½“å‰å·²ç»å­˜åœ¨çš„ tensor ä¸Šé¢åˆ›å»ºäº†æ–°çš„è§†å›¾(view), ä½¿å¾— singleton ç»´åº¦è¢«æ‰©å±•æˆäº†ä¸€ä¸ªæ›´å¤§çš„å°ºå¯¸.Any dimension of size 1 can be expanded to an arbitrary value without new memory. 1 2 3 4 | x = torch.tensor([1],[2],[3]) print(x.size()) # torch.Size([3,1]) print(x.expand(3,4)) # torch.Size([3,4]) # å°†ç»´åº¦ä¸º1çš„æ‰©å±•åˆ°ä»»æ„å°ºå¯¸ print(x.expand(-1,4)) # torch.Size([3,4]) # -1 ä»£è¡¨ä¸æ”¹å˜ç»´åº¦ â€”|â€” æ³¨æ„, åªèƒ½å¯¹ singleton çš„ç»´åº¦è¿›è¡Œæ‰©å±•, å¦‚æœå¼ºè¡Œå¯¹å…¶ä»–ç»´åº¦æ‰©å±•, åˆ™ä¼šæŠ¥é”™. torch.Tensor.expand_as()1 | torch.Tensor.expand_as(other) # è¿”å› tensor â€”|â€” å°†å½“å‰ tensor æ‰©å±•åˆ°å’Œ other ä¸€æ ·çš„size.self.expand_as(other) ä¸ self.expand(other.size()) ç­‰ä»·. torch.Tensor.index_fill_()1 | torch.Tensor.index_fill_(dim, index, val) # è¿”å›tensor â€”|â€” åœ¨ç»™å®šçš„ç»´åº¦ dim ä¸Š, ç”¨ val å°†è¯¥ç»´åº¦ä¸Šçš„ index åæ ‡çš„å€¼å¡«å……. 1 2 3 4 5 6 7 | x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) index = torch.tensor([0, 2]) x.index_fill_(1, index, -1) print(x) #tensor([[-1., 2., -1.], # [-1., 5., -1.], # [-1., 8., -1.]]) â€”|â€” torch.Tensor.contiguous()è¿”å›ä¸€ä¸ªè¿ç»­çš„tensor, æ•°æ®å†…å®¹ä¸å˜ 1 | torch.Tensor.contiguous() # å¦‚æœtensoræœ¬èº«å°±æ˜¯è¿ç»­çš„, é‚£ä¹ˆå°±ä¼šè¿”å›tensoræœ¬èº« â€”|â€” è¿™é‡Œçš„ contiguous æŒ‡çš„æ˜¯å†…å­˜ä¸Šçš„è¿ç»­, ç”±äºåœ¨ PyTorch ä¸­, view åªèƒ½ç”¨åœ¨ contiguous çš„ tensor ä¸Šé¢, è€Œå¦‚æœåœ¨ view ä¹‹å‰ä½¿ç”¨äº† transpose, permute ç­‰æ“ä½œå, å°±éœ€è¦ä½¿ç”¨ contiguous æ¥è¿”å›ä¸€ä¸ª contiguous tensor.åœ¨ PyTorch 0.4 ç‰ˆæœ¬ä»¥å, å¢åŠ äº† torch.reshape(), è¿™ä¸ numpy.reshape() çš„åŠŸèƒ½ç±»ä¼¼, å®ƒå¤§è‡´ç›¸å½“äº tensor.contiguous().view() ? torch.Tensor.item()å½“Tensorä¸­åªåŒ…å«ä¸€ä¸ªå…ƒç´ æ—¶, å¯ä»¥åˆ©ç”¨è¯¥å‡½æ•°è¿”å›è¿™ä¸ªå…ƒç´ çš„æ ‡é‡ torch.Tensor.tolist()å¯ä»¥å°†Tensorè½¬æ¢æˆåˆ—è¡¨ torch.Tensor.zero_()1 | torch.Tensor.zero_() â€”|â€” å°†å½“å‰çš„ tensor å˜é‡å…¨éƒ¨ç½®ä¸º0(åŸåœ°) torch.autogradset_grad_enabled()1 | class torch.autograd.set_grad_enabled(mode) â€”|â€” ç”¨æ¥æ§åˆ¶æ¢¯åº¦è®¡ç®—çš„å¼€å…³(ä¾æ®boolç±»å‹å‚æ•°modeå†³å®š), å¯ä»¥å½“åšä¸Šä¸‹æ–‡ç®¡ç†å™¨ä½¿ç”¨, ä¹Ÿå¯ä»¥å½“åšå‡½æ•°ä½¿ç”¨ 1 2 3 4 5 6 7 8 9 10 11 | # å½“åšä¸Šä¸‹æ–‡ç®¡ç†å™¨ with torch.set_grad_enabled(is_train): # æ³¨æ„, è¿™é‡Œçœç•¥äº†autograd loss.backward() optimizer.step() # å½“åšå‡½æ•°ä½¿ç”¨ w1 = torch.Tensor([1], requires=True) torch.set_grad_enabled(True) print(w1.requires_grad) # True torch.set_grad_enabled(False) print(w1.requires_grad) # False â€”|â€” no_grad()1 | class torch.autograd.no_grad â€”|â€” ç”¨äºç¦ç”¨æ¢¯åº¦è®¡ç®—çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨.åœ¨æµ‹è¯•é˜¶æ®µ, å½“ä½ ç¡®ä¿¡ä½ ä¸ä¼šè°ƒç”¨Tensor.backward()æ—¶,ç¦ç”¨æ¢¯åº¦è®¡ç®—ååˆ†æœ‰ç”¨. è¿™ä¼šé™ä½è®¡ç®—ä½¿ç”¨å†…å­˜æ¶ˆè€—. 1 2 3 4 5 6 | x = torch.tensor([1.0], requires_grad=True) with torch.no_grad(): # çœç•¥äº†autograd print(x.requires_grad) # True, è™½ç„¶ä¸ºTrue, ä½†åœ¨è¯¥ä¸Šä¸‹æ–‡ä¸­, ä¼šæ— è§†æ‰requires_gradå‚æ•°, ä¸€å¾‹åšFalseå¤„ç† y = x*2 print(y.requires_grad) # False, åœ¨å½“å‰ä¸Šä¸‹æ–‡äº§ç”Ÿçš„tensorçš„requires_gradå±æ€§ä¸ºFalse print(x.requires_grad) # True â€”|â€” torch.autograd.Function1 | class torch.autograd.Function â€”|â€” ä¸ºå¯å¾®åˆ†çš„ ops è®°å½• operation history, åŒæ—¶å®šä¹‰è®¡ç®—å…¬å¼. æ¯ä¸€ä¸ªä½œç”¨åœ¨ tensor ä¸Šçš„ operatin éƒ½ä¼šåˆ›å»ºä¸€ä¸ªæ–°çš„ function å¯¹è±¡, å®ƒä¼šæ‰§è¡Œè®¡ç®—è¿‡ç¨‹å¹¶è®°å½•ç›¸å…³ä¿¡æ¯. è¿™äº›ä¿¡æ¯å¯ä»¥ä»ä¸€ä¸ªç”± functions ç»„æˆçš„æœ‰å‘å›¾ä¸­è·å¾—. å½“ backward() æ–¹æ³•è¢«è°ƒç”¨æ—¶, å°±ä¼šåˆ©ç”¨è¿™äº›ä¿¡æ¯åœ¨ function ä¸Šè¿›è¡Œåå‘ä¼ æ’­, å¹¶å°†æ¢¯åº¦ä¼ ç»™ä¸‹ä¸€ä¸ª Funtion.é€šå¸¸æƒ…å†µä¸‹, å½“ç”¨äºéœ€è¦è‡ªå®šä¹‰å¯è‡ªåŠ¨æ±‚å¯¼çš„ ops æ—¶, å¯ä»¥å®ç°ä¸€ä¸ª Function çš„å­ç±». 1 2 3 4 5 6 7 8 9 10 11 12 | # Example class Exp(Function): @staticmethod def forward(ctx, i): result = i.exp() ctx.save_for_backward(result) @staticmethod def backward(ctx, grad_output): result, = ctx.saved_tensors return grad_output*result â€”|â€” static forward(ctx, args, kwargs):*å®šä¹‰å‰å‘è®¡ç®—çš„é€»è¾‘. static backward(ctx, *grad_outputs):å®šä¹‰åå‘ä¼ å¯¼çš„é€»è¾‘, å¦‚æœç¡®å®šä¸ä¼šä½¿ç”¨åˆ°åå‘ä¼ æ’­, åˆ™å¯ä»¥ä¸å®ç°è¯¥å‡½æ•°. torch.nnModule1 | class torch.nn.Module â€”|â€” æ‰€æœ‰ç¥ç»ç½‘ç»œModuleçš„åŸºç±», è‡ªå®šä¹‰çš„æ¨¡å‹ä¹Ÿåº”è¯¥æ˜¯å®ƒçš„å­ç±».Moduleså¯ä»¥åŒ…å«å…¶ä»–Module(å¦‚Linear, Conv2dç­‰ç­‰). parameters()1 2 | for param in model.parameters(): print(param.data, param.size()) â€”|â€” state_dict: 1 | torch.nn.Module.state_dict(destination=None,prefix=\"\",keep_vars=False) â€”|â€” ä»¥å­—å…¸å½¢å¼è¿”å›æ•´ä¸ªmoduleçš„çŠ¶æ€ train1 | torch.nn.Module.train(mode=True) â€”|â€” å°†moduleçš„æ¨¡å¼è®¾ç½®ä¸ºtrain, è¿™åªå¯¹éƒ¨åˆ†moduleæœ‰æ•ˆ, å¦‚Dropout, BatchNormç­‰, è¯¦ç»†è¯·æŸ¥çœ‹å®˜ç½‘.è¿”å›å€¼: torch.nn.Module training1 | torch.nn.Module.training # å±æ€§, è¿”å›ä¸€ä¸ªboolå€¼, æŒ‡ç¤ºå½“å‰çš„æ¨¡å¼æ˜¯å¦ä¸ºtrain â€”|â€” eval1 | torch.nn.Module.eval() # æ³¨æ„, å’Œtrainä¸åŒ, evalä¸ºæ— å‚å‡½æ•° â€”|â€” å°†moduleçš„modeè®¾ç½®ä¸ºevaluation, åŒæ ·, åªå¯¹éƒ¨åˆ†moduleèµ·æ•ˆ. Linear1 | torch.nn.Linear(in_features, out_features, bias=True) â€”|â€” å…¨è¿æ¥å±‚çš„å®ç°. è¾“å…¥çš„shapeä¸º (N,â€¦,infeatures), è¾“å‡ºçš„shapeä¸º (N,â€¦,outfeatures), å¯ä»¥çœ‹å‡º, é™¤äº†æœ€åä¸€ç»´ä¸åŒå¤–, å…¶ä»–ç»´åº¦éƒ½ç›¸åŒ. (é€šå¸¸åœ¨ä½¿ç”¨Linearä¹‹å‰, ä¼šå°†è¾“å…¥å˜æˆäºŒç»´çš„çŸ©é˜µ, å…¶ä¸­ç¬¬ä¸€ç»´ä¸ºbatch size, ç¬¬äºŒç»´ä¸ºç‰¹å¾å‘é‡).in_features å’Œ out_features å¯ä»¥å½“åšå±æ€§ç”¨.æ¥è·å–. Conv2d1 | class torch.nn.Conv2çš„(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) â€”|â€” in_channels(int): out_channels(int): kernel_size(intortuple): stride(intortuple, optional): MaxPool2dSoftmax() 1 | class torch.nn.Softmax(dim=None) â€”|â€” dimæŒ‡æ˜äº†éœ€è¦è¿›è¡Œ softmax çš„ç»´åº¦, åœ¨è¿™ä¸ªç»´åº¦ä¸Šçš„å€¼, åŠ èµ·æ¥å’Œä¸º1. ReLU1 | torch.nn.ReLU(inplace=False) â€”|â€” è¾“å…¥è¾“å‡ºçš„shapeæ˜¯ç›¸åŒçš„, æ‰§è¡Œreluå‡½æ•° torch.nn.Sequential1 | class torch.nn.Sequential(*args) â€”|â€” torch.nn.MSELoss1 | class torch.nn.MSELoss(size_average=None, reduce=None, reduction=\"elementwise_mean\") â€”|â€” size_average(bool, optional): å¼ƒç”¨(è§reductionå‚æ•°). é»˜è®¤æƒ…å†µä¸‹, lossä¼šè®¡ç®—åœ¨æ¯ä¸ªæ ·æœ¬ä¸Šçš„å¹³å‡è¯¯å·®. å¦‚æœå°†size_averageç½®ä¸ºFalse, åˆ™è®¡ç®—å¹³æ–¹è¯¯å·®æ€»å’Œ. å½“reduceå‚æ•°ä¸ºFalseæ—¶, å¿½è§†è¯¥å‚æ•° reduce(bool, optional): å¼ƒç”¨(è§reductionå‚æ•°). reduceå‚æ•°é¡¾åæ€ä¹‰, å°±æ˜¯æ˜¯å¦è®©MSELosså‡½æ•°è¿”å›å€¼çš„ç»´åº¦å‡å°‘, é»˜è®¤ä¸ºTrue, å³ä¼šå°†ä»»æ„ç»´åº¦çš„è¾“å…¥è®¡ç®—losså, è¿”å›ä¸€ä¸ªæ ‡é‡(å¹³å‡oræ€»å’Œå–å†³äºsize_average), å¦‚æœä¸ºFalse, åˆ™è¯´æ˜è¿”å›å€¼ç»´åº¦ä¸åº”è¯¥å‘ç”Ÿå˜åŒ–, æ•…è€Œè¿”å›å€¼å°±æ˜¯å¯¹æ¯ä¸ªå…ƒç´ å•ç‹¬è¿›è¡Œå¹³æ–¹æŸå¤±è®¡ç®—. 1 2 3 4 5 6 7 8 9 10 | y = torch.tensor([1,2,3,4], dtype=torch.float) pred_y = torch.tensor([1,1,1,1], dtype=torch.float) loss_fn1 = torch.nn.MSELoss() loss1 = loss_fn1(y, pred_y) loss_fn2 = torch.nn.MSELoss(size_average=False) loss2 = loss_fn2(y, pred_y) loss_fn3 = torch.nn.MSELoss(reduce=False) loss3 = loss_fn3(y, pred_y) print(loss1,loss2,loss3) # tensor(3.5000) tensor(14.) tensor([0., 1., 4., 9.]) â€”|â€” 3. reduction(string, optional): ç”¨å­—ç¬¦ä¸²æ¥æ›¿ä»£ä¸Šé¢ä¸¤ä¸ªå‚æ•°çš„ä½œç”¨: â€œelementwise_meanâ€(é»˜è®¤) | â€œsumâ€ | â€œnoneâ€ (ä¸è¿›è¡Œreduce). torch.nn.functionalconv1d()conv2d()relu() 1 | torch.nn.functional.relu(input, inplace=True) # è¿”å› ä¸€ä¸ª Tenosr â€”|â€” relu_()1 | torch.nn.functional.relu_(input) # relu() çš„åŸåœ°ç‰ˆæœ¬ â€”|â€” torch.optimlr_schedulerStepLR1 | class torch.optim.lr_schedulr.StepLR(optimizer,step_size,gamma=0.1,last_epoch=-1) â€”|â€” æ¯ç»è¿‡step_sizeæ¬¡epochä¹‹å, lrå°±ä¼šè¡°å‡gammaå€(new_lr=lrÃ—gamma), åˆå§‹çš„lræ¥è‡ªäºoptimizerä¸­çš„lrå‚æ•°. 1 2 3 4 5 | # Observe that all parameters are being optimized optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) â€”|â€” ExponentialLR1 | class torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma,last_epoch=-1) â€”|â€” CosineAnnealingLR1 2 | ## Adam class torch.optim.Adam(params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False) â€”|â€” conv2dtorch.utils.dataDataLoader1 | class torch.utils.data.DataLoader(dataset,batch_size=1,shuffle=False,sampler=None,batch_sampler=None,num_workers=0,collate_fn=function default_collate,pin_memory=False,drop_last=False,timeout=0,worker_init_fn=None) â€”|â€” æ•°æ®åŠ è½½å™¨, å°†æ•°æ®é›†å’Œé‡‡æ ·å™¨ç»“åˆèµ·æ¥, å¹¶ä¸”æä¾›å•/å¤šçº¿ç¨‹çš„è¿­ä»£å™¨. dataset(utils.data.Dataset): batch_size(int,optional): batchä¸­çš„æ ·æœ¬ä¸ªæ•° shuffle(bool,optional) num_worker(int,optional): åŠ è½½æ•°æ®çš„çº¿ç¨‹ä¸ªæ•°, 0æ„å‘³ç€åªæœ‰ä¸€ä¸ªä¸»çº¿ç¨‹.æ–¹æ³•ï¼š iter(self): å¯ä»¥å½“åšè¿­ä»£å™¨ä½¿ç”¨, å¦‚inputs,class_ids=next(iter(dataloaders)), å…¶ä¸­, inputçš„shapeä¸º (N,C,H,W), class_idsçš„shapeä¸º (N). len(self): è¿”å›æ•°æ®é›†çš„ç±»åˆ«æ•°ç›® torchvisiontorchvision.utilsmake_grid1 | torchvision.utils.make_grid(tensor,nrow=8,padding=2,normalize=False,range=None,scale_each=False,pad_value=0) â€”|â€” åˆ¶ä½œä¸€ä¸ªå…³äºimageçš„grid, è¿”å›å€¼ä¾ç„¶æ˜¯ä¸€ä¸ªtensor, åªä¸è¿‡å°ºåº¦å˜æˆäº†3D, ç›¸å½“äºæŠŠå¤šä¸ªå›¾ç‰‡æ‹¼æ¥åœ¨ä¸€èµ·äº†, ç›´æ¥é€šè¿‡plt.imshow(grid)å³å¯è¾“å‡ºç½‘æ ¼åŒ–ä»¥åçš„å›¾ç‰‡. tensor(Tensor/list): 4Dçš„ mini-batch Tensor, Shapeä¸º (NÃ—CÃ—HÃ—W), æˆ–è€…æ˜¯åŒç»´åº¦çš„list. torchvision.transformstorchvision.transforms.Compose 1 2 3 4 5 6 7 | class torchvision.transforms.Compose(transforms) # ä½¿ç”¨ trans.Compose([ transforms.CenterCrop(10), transforms.ToTensor(), ]) â€”|â€” å°†å¤šä¸ªtransformsæ“ä½œç»„åˆèµ·æ¥, æ³¨æ„å‚æ•°æ˜¯åˆ—è¡¨å½¢å¼ Transforms on PIL Image1 2 3 | # cv2 image to PIL Image # skimage to PIL Image â€”|â€” æ³¨æ„, ä»¥ä¸‹æ“ä½œä½œç”¨åœ¨PIL Imageä¸Šçš„ CenterCrop1 | class torchvision.transform.CenterCrop(size) â€”|â€” sizeå‚æ•°è¡¨ç¤ºè¾“å‡ºçš„å›¾è°±çš„å¤§å°, å¦‚æœåªä¼ å…¥äº†ä¸€ä¸ªæ•°å­—, åˆ™è¯¥æ•°å­—æ—¢è¡¨ç¤ºé«˜åº¦, åˆè¡¨ç¤ºå®½åº¦. Resize1 | class torchvision.transforms.Resize(size, interpolation=2) â€”|â€” size: æœŸæœ›çš„è¾“å‡ºsize. interpolation: æ’å€¼æ–¹æ³•, é»˜è®¤ä¸ºåŒçº¿æ€§æ’å€¼ ToTensor 1 | class torchvision.transforms.ToTensor â€”|â€” å°†ä¸€ä¸ªPIL Imageæˆ–è€…numpy.ndarray (HÃ—WÃ—C,[0, 255])è½¬æ¢æˆtorch.FloatTensor (CÃ—HÃ—W, [0.0, 1.0]). RandomHorizontalFlip1 | transforms.RandomHorizontalFlip(p=0.5) â€”|â€” åœ¨ç»™å®šæ¦‚ç‡ä¸‹å¯¹PIL Imageéšæœºæ‰§è¡Œæ°´å¹³ç¿»è½¬æ“ä½œ RandomResizedCrop1 | torch.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333), interpolation=2) â€”|â€” å¯¹PIL Imageéšæœºæ‰§è¡Œå‰ªè£æ“ä½œ(æŒ‰ç…§scaleå’Œratioçš„åŒºé—´å‰ªè£), ç„¶åå°†å‰ªè£åçš„å›¾ç‰‡æ”¾ç¼©éƒ½æœŸæœ›çš„å°ºå¯¸(é»˜è®¤æ’å€¼ä¸ºåŒçº¿æ€§æ’å€¼) size: æœŸæœ›å¾—åˆ°çš„å°ºå¯¸ scale: å‰ªè£çš„é¢ç§¯æ¯”ä¾‹(ç›¸å¯¹äºåŸå§‹å›¾) ratio: å‰ªè£çš„å®½é«˜æ¯” interpolation: é»˜è®¤ä¸º:PIL.Image.BILINEAR Transforms on torch.*Tensoræ³¨æ„, ä»¥ä¸‹æ“ä½œæ˜¯ä½œç”¨åœ¨tensorä¸Šçš„ Normalize 1 | class torchvision.transforms.Normalize(mean, std) â€”|â€” å°†å›¾ç‰‡tensoræŒ‰ç…§å‡å€¼meanå’Œæ ‡å‡†å·®stdè¿›è¡Œå½’ä¸€åŒ–, å¯¹äºnä¸ªchannels, æœ‰ mean=(M1, â€¦, Mn), std=(S1,â€¦,Sn).æ³¨æ„, è¿™ä¸ªå½’ä¸€åŒ–æ“ä½œæ˜¯åŸåœ°è¿›è¡Œçš„ torchvision.datasetsImageFolder1 | class torchvision.datasets.ImageFolder(root, transform=None, target_transform=None, loader=function default_loader) â€”|â€” ä¸€ä¸ªä¸€èˆ¬åŒ–çš„æ•°æ®åŠ è½½å™¨, ä¸»è¦é’ˆå¯¹å¦‚ä¸‹æ•°æ®æ’åˆ—æ ¼å¼: 1 2 3 4 5 6 7 | root/dog/x.png root/dog/y.png root/dog/z.png ... root/cat/123.png root/cat/nsdf3.png root/cat/asd932_.png â€”|â€” root: æ ¹ç›®å½•è·¯å¾„ transform(callable,optional): å¯¹å›¾ç‰‡è¦åšçš„å˜æ¢æ“ä½œ target_transform(callable,optional): å¯¹targetè¦åšçš„å˜æ¢æ“ä½œ loader: ç”¨äºåŠ è½½ç»™å®šè·¯å¾„å›¾ç‰‡çš„å‡½æ•°å±æ€§ï¼š classes(list): è¿”å›ç±»åˆ«çš„åå­—åˆ—è¡¨ class_names class_to_idx(dict): ä»¥å­—å…¸çš„å½¢å¼è¿”å›(class_name, class_index) imgs(list): è¿”å›å…ƒç»„åˆ—è¡¨: (image path, class_index)æ–¹æ³•ï¼š getitem(index): æ ¹æ®indexè¿”å›(sample,target)å…ƒç»„. å¯ä»¥ä½¿ç”¨ len(imagefolder) è¿”å›ç±»åˆ«æ•°é‡ sort()1 | sort(dim=None, descending=False) # é»˜è®¤ä¸ºå‡åº, è¿”å›(Tensor, LongTensor) â€”|â€” è¯¦è§ torch.sort() torch.distributedtorch.distributed.reduce()inspect æ¨¡å—1 2 3 4 | inspect.signature() # æŸ¥çœ‹å‡½æ•°ç­¾å, python3.6ä»¥ä¸Š inspect.getargspec() # æŸ¥çœ‹å‡½æ•°ç­¾å, python3.6ä»¥ä¸Š inspect.getsource() # è·å–æ¨¡å‹çš„code inspect.getabsfile() # è·å–æ¨¡å—çš„è·¯å¾„ â€”|â€” un normalize1 2 3 4 5 6 7 | mean = torch.tensor([1, 2, 3], dtype=torch.float32) std = torch.tensor([2, 2, 2], dtype=torch.float32) normalize = T.Normalize(mean.tolist(), std.tolist()) unnormalize = T.Normalize((-mean / std).tolist(), (1.0 / std).tolist()) â€‹~~~s â€”|â€”","tags":["Python"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"globæ¨¡å—","path":"/2019/08/28/globæ¨¡å—/","content":"globæ¨¡å—æ˜¯Pythonæœ€ç®€å•çš„æ¨¡å—ä¹‹ä¸€, å†…å®¹éå¸¸å°‘, ç”¨å®ƒå¯ä»¥æŸ¥æ‰¾ç¬¦åˆç‰¹å®šè§„åˆ™çš„æ–‡ä»¶è·¯å¾„å, æŸ¥æ‰¾æ–‡ä»¶æ—¶åªä¼šç”¨åˆ°ä¸‰ä¸ªåŒ¹é…ç¬¦: : åŒ¹é…0ä¸ªæˆ–å¤šä¸ªå­—ç¬¦ ? : åŒ¹é…å•ä¸ªå­—ç¬¦ [] : åŒ¹é…æŒ‡å®šèŒƒå›´å†…çš„å­—ç¬¦, å¦‚[0-9]åŒ¹é…æ•°å­— glob.glob()å‚æ•°:_(str): æ–‡ä»¶è·¯å¾„çš„æ­£åˆ™è¡¨è¾¾å¼ è¿”å›å€¼:_(list): ç¬¦åˆæ­£åˆ™è¡¨è¾¾å¼çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨ å¤‡æ³¨:è¿”å›æ‰€æœ‰åŒ¹é…çš„æ–‡ä»¶è·¯å¾„åˆ—è¡¨, å®ƒåªæœ‰ä¸€ä¸ªå‚æ•°pathname, å®šä¹‰äº†æ–‡ä»¶è·¯å¾„åŒ¹é…çš„è§„åˆ™, è¿™é‡Œå¯ä»¥æ˜¯ç»å¯¹è·¯å¾„æˆ–è€…ç›¸å¯¹è·¯å¾„: 1 2 3 4 5 6 | import glob pathes_list = glob.glob(\"~/Pictures/*.jpg\") # è·å–Picturesä¸‹çš„æ‰€æœ‰å›¾ç‰‡ relative_pathes_list = glob.glob(\"../*.py\") # è·å–ä¸Šçº§ç›®å½•ä¸­çš„æ‰€æœ‰.pyæ–‡ä»¶ â€”|â€” åœ¨ linux, osx ç³»ç»Ÿä¸­, é€šé…ç¬¦çš„åŒ¹é…æ˜¯å¤§å°å†™åŒºåˆ†çš„, ä¹Ÿå°±æ˜¯éœ€è¦ç‰¹åˆ«æŒ‡å®šå¤§å°å†™: 1 | extensions = ['jpg', 'JPG', 'jpeg', 'JPEG'] â€”|â€” ä½†æ˜¯åœ¨ windows å½“ä¸­, é€šé…ç¬¦çš„åŒ¹é…æ˜¯ä¸åŒºåˆ†å¤§å°å†™çš„, å› æ­¤åªéœ€è¦æŒ‡å®šå¤§å°å†™ä¸­çš„ä¸€ä¸ªå³å¯, ä¸¤ä¸ªéƒ½æŒ‡å®šçš„è¯, ä¼šå‡ºç°é‡å¤çš„æƒ…å†µ 1 | extensions = ['jpg', 'jpeg'] â€”|â€” glob.iglobè·å–ä¸€ä¸ªå¯éå†çš„å¯¹è±¡, ä½¿ç”¨å®ƒå¯ä»¥é€ä¸ªè·å–åŒ¹é…çš„æ–‡ä»¶è·¯å¾„å. ä¸glob.glob()çš„åŒºåˆ«æ˜¯: glob.glob()ä¼šåŒæ—¶è·å–åˆ°æ‰€æœ‰çš„åŒ¹é…è·¯å¾„, è€Œglob.iglob()ä¸€æ¬¡åªè·å–ä¸€ä¸ªåŒ¹é…è·¯å¾„. 1 2 3 4 | f = glob.iglob(\"../*.py\") print f # generator object iglob at 0x00B9FF80 for py in f: print(py) â€”|â€”","tags":["Python"],"categories":["Python"]},{"title":"loggingæ¨¡å—","path":"/2019/08/28/loggingæ¨¡å—/","content":"ç®€å•ä½¿ç”¨1 2 3 4 5 6 7 | import logging logging.debug(\"debug msg\") logging.info(\"info msg\") logging.warn(\"warn msg\") logging.error(\"error msg\") logging.critical(\"critical msg\") â€”|â€” é»˜è®¤æƒ…å†µä¸‹, loggingæ¨¡å—å°†æ—¥å¿—æ‰“å°åˆ°å±å¹•ä¸Š, åªæœ‰æ—¥å¿—çº§åˆ«é«˜äºWARNINGçš„æ—¥å¿—ä¿¡æ¯æ‰å›è¾“å‡º","tags":["ç¼–ç¨‹"],"categories":["Python"]},{"title":"numpyå®ç°ç¥ç»ç½‘ç»œ","path":"/2019/08/28/numpyå®ç°ç¥ç»ç½‘ç»œ/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 | import numpy as np # N ä¸ºbatch size, D_in ä¸ºè¾“å…¥ç»´åº¦ # H ä¸ºéšè—å±‚çš„ç»´åº¦, D_out ä¸ºè¾“å‡ºçš„ç»´åº¦ N, D_in, H, D_out = 64, 1000, 100, 10 # åˆ›å»ºéšæœºçš„è¾“å…¥å’Œè¾“å‡ºæ•°æ® x = np.random.randn(N, D_in) # N Ã— D_in çš„çŸ©é˜µ y = np.random.randn(N, D_out) # N Ã— D_out çš„çŸ©é˜µ # å¯¹ä¸¤ä¸ªéšè—å±‚w1,w2è¿›è¡Œåˆå§‹åŒ– w1 = np.random.randn(D_in, H) w2 = np.random.randn(H, D_out) # è®¾ç½®å­¦ä¹ ç‡ learning_rate = 1e-6 for t in range(500): # å‰å‘ä¼ æ’­: è®¡ç®—é¢„æµ‹ç»“æœ y_pred h = x.dot(w1) # xç»´åº¦ä¸º64 Ã— 1000, w1ç»´åº¦ä¸º 1000 Ã— 100, è®¡ç®—å®Œä»¥å, hç»´åº¦ä¸º 64 Ã— 100 h_relu = np.maximum(h,0) y_pred = h_relu.dot(w2) # h_reluç»´åº¦ä¸º 64Ã—100, w2ç»´åº¦ä¸º100Ã—10, yçš„ç»´åº¦ä¸º64Ã—10 # è®¡ç®—æŸå¤± loss = np.square(y_pred - y).sum() print(t, loss) # åå‘ä¼ æ’­æ ¹æ®lossæ›´æ–°w1å’Œw2çš„å€¼ grad_y_pred = 2.0*(y_pred - y) # å¯¹y_predæ±‚å¯¼ grad_w2 = h_relu.T.dot(grad_y_pred) # å¯¹w2æ±‚å¯¼, å¾®åˆ†çŸ©é˜µåº”è¯¥ä¸w2çš„sizeç›¸åŒ grad_h_relu = grad_y_pred.dot(w2.T) # å¯¹h_reluæ±‚å¯¼ grad_h = grad_h_relu.copy() grad_h[h 0] = 0 # ç»è¿‡relu, å°†å°äº0çš„æ¢¯åº¦å½’0 grad_w1 = x.T.dot(grad_h) # Update weights w1 = w1 - learning_rate * grad_w1 w2 = w2 - learning_rate * grad_w2 â€”|â€”","tags":["Python"],"categories":["Python"]},{"title":"opencvæ¨¡å—","path":"/2019/08/28/opencvæ¨¡å—/","content":"opencv åŸºç¡€çŸ¥è¯†cv2.imread è¯»å…¥çš„å›¾ç‰‡, å…¶shapeä¸º(h, w, c), é¢œè‰²é€šé“é¡ºåºä¸º (b, g, r) å¸¸ç”¨é¢œè‰²è¯»å–å›¾ç‰‡1 | img = cv2.imread(img_path) â€”|â€” ä¿å­˜å›¾ç‰‡1 | cv2.imwrite(save_path, img) â€”|â€” æ–‡æœ¬(startX, startY) ä¸ºå·¦ä¸Šè§’åæ ‡ 1 | cv2.putText(img, \"text test\", (startX, startY), cv2.FONT_HERSHEY_SIMPLEX, font_size, (B,G,R), thickness) â€”|â€” ç”»æ¡†(x,y) ä¸ºå·¦ä¸Šè§’åæ ‡(x+h,y+w) ä¸ºå³ä¸‹è§’åæ ‡ 1 | cv2.rectangle(img,(x,y), (x+h,y+w), (0,255,0), thickness) â€”|â€” waitKey()1 2 3 4 5 | keypress = cv2.waitKey(200) # 200ä¸ºå½“å‰å›¾ç‰‡çš„æ˜¾ç¤ºæŒç»­æ—¶é—´ if keypress == ord('c') # keypressä¸ºæŒ‰é”®çš„æ•´æ•°å½¢å¼, æ‰€ä»¥éœ€è¦ç”¨ordå°†å­—ç¬¦ç±»å‹è½¬æ¢ if cv2.waitKey(200) == 27: # Decimal 27 = Esc â€”|â€” opencvä¸numpyopencvçš„åŸºç¡€ç±»å‹ä¸ºnumpy.ndarray, å› æ­¤å¯ä»¥ç›´æ¥ä½¿ç”¨ ndarray çš„ä¸€äº›å±æ€§çš„æ–¹æ³• 1 2 3 4 | import cv2 img = cv2.imread('./test.jpg') print(type(img)) # class 'numpy.ndarray' print(img.shape) # (500, 1069, 3) (é«˜, å®½, é€šé“) â€”|â€” åˆ©ç”¨ cv2.merge æ–¹æ³•å°† numpy.ndarray æ•°æ®è½¬æ¢æˆopencvçš„å›¾ç‰‡æ•°æ®: 1 2 3 4 5 6 7 8 9 10 | # å›¾ç‰‡çš„åˆ†è¾¨ç‡ä¸º300*200(å®½*é«˜)ï¼Œè¿™é‡Œb, g, rè®¾ä¸ºéšæœºå€¼ï¼Œæ³¨æ„dtypeå±æ€§ b = np.random.randint(0, 255, (200, 300), dtype=np.uint8) g = np.random.randint(0, 255, (200, 300), dtype=np.uint8) r = np.random.randint(0, 255, (200, 300), dtype=np.uint8) # åˆå¹¶é€šé“ï¼Œå½¢æˆå›¾ç‰‡ img = cv2.merge([b, g, r]) # opencvçš„é€šé“æ˜¯båœ¨æœ€å‰,råœ¨æœ€å # æ˜¾ç¤ºå›¾ç‰‡ cv2.imshow('test', img) cv2.waitKey(0) cv2.destroyWindow('test') â€”|â€” é€šé“çš„æ‹†åˆ†ä¸åˆå¹¶æ‹†åˆ†: cv2.splitåˆå¹¶: cv2.merge 1 2 3 4 5 6 7 8 9 10 11 12 | # å›¾ç‰‡çš„åˆ†è¾¨ç‡ä¸º800*200(å®½*é«˜)ï¼Œè¿™é‡Œb, g, rè®¾ä¸ºéšæœºå€¼ï¼Œæ³¨æ„dtypeå±æ€§ b = np.random.randint(0, 255, (200, 800), dtype=np.uint8) g = np.random.randint(0, 255, (200, 800), dtype=np.uint8) r = np.random.randint(0, 255, (200, 800), dtype=np.uint8) # åˆå¹¶é€šé“ï¼Œå½¢æˆå›¾ç‰‡ img = cv2.merge([b, g, r]) # opencvçš„é€šé“æ˜¯båœ¨æœ€å‰,råœ¨æœ€å # æ˜¾ç¤ºå›¾ç‰‡ cv2.imshow('test', img) cv2.waitKey(0) cv2.destroyWindow('test') # æ‹†åˆ†é€šé“, æ¯ä¸ªé€šé“éƒ½å˜æˆäº†å•é€šé“æ•°ç»„ [blue, green, red] = cv2.split(img) â€”|â€” å°† BGR è½¬æ¢æˆ RGB é€šé“é¡ºåº1 2 3 4 5 | # æ–¹æ³•ä¸€: rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # æ–¹æ³•äºŒ: rgb_img = img[:, :, [2, 1, 0]] # img[h,w,v] rgb_img = img[:, :, ::-1] â€”|â€” PIL ä¸ cv2 æ ¼å¼äº’ç›¸è½¬æ¢PIL.Imageè¯»å…¥çš„å›¾ç‰‡æ•°æ®ç±»å‹ä¸æ˜¯ numpy æ•°ç»„, å®ƒçš„sizeå±æ€§ä¸º (w, h), åˆ©ç”¨np.arrayè½¬æ¢æˆ numpy æ•°ç»„å, å®ƒçš„é€šé“é¡ºåºä¸º (r, g, b) 1 2 3 4 5 6 7 8 9 10 11 | from PIL import Image import numpy as np # PIL to cv2 pil_img = Image.open(img_path) print(pil_img.size) # (w, h) np_img = np.array(pil_img) cv2_img = np_img[:, :, ::-1] # äº¤æ¢é€šé“ # cv2 to PIL pil_img = Image.fromarray(cv2_img[:, :, ::-1]) â€”|â€” ç”¨matplotlibæ˜¾ç¤ºå›¾åƒ1 2 3 4 | b,g,r=cv2.split(img) img2=cv2.merge([r,g,b]) plt.imshow(img2) plt.show() â€”|â€” æˆªå–å­å›¾1 2 | # å·²çŸ¥å­å›¾å·¦ä¸Šè§’åæ ‡ (x1, y1), å³ä¸‹è§’åæ ‡(x2, y2) crop_img = img[y1:y2, x1:x2, :] â€”|â€” opencv æ ¸å¿ƒç®—æ³•cv21 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 | import cv2 image_path = './test.jpg' src_image = cv2.imread(image_path) # è¯»å–å›¾ç‰‡ size = src_image.shape # è·å–å›¾ç‰‡çš„å°ºå¯¸, è¿”å›ä¸€ä¸ªå…ƒç»„: (height, width, depth) copy_image = src_image.copy() # å¤åˆ¶å›¾ç‰‡ cv2.imwrite('./dst_test.jpg', copy_image) # ä¿å­˜å›¾ç‰‡ cv2.imshow('image', src_image) # æ˜¾ç¤ºå›¾ç‰‡ # åˆ©ç”¨ä¸‹æ ‡è®¿é—®æŒ‡å®šåƒç´  for x in range(src_image.shape[0]): # ä»¥è¡Œä¸ºä¸», è¡Œæ•°=å›¾ç‰‡height for y in range(src_image.shape[1]): # åˆ—æ•° = å›¾ç‰‡width src_image[x,y] = (255,0,255) # (blue, green, red) å€¼è¶Šé«˜è¡¨ç¤ºå¯¹åº”é¢œè‰²è¶Šæ˜¾è‘—, å…¨0ä¸ºé»‘, å…¨255ä¸ºç™½ â€”|â€”","tags":["Python"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"pythonæ‰‹å†Œ","path":"/2019/08/28/pythonæ‰‹å†Œ/","content":"æœ€è¿‘åœ¨å­¦ä¹ æŸ¥æ‰¾èµ„æ–™çš„è¿‡ç¨‹ä¹‹ä¸­ï¼Œçœ‹åˆ°äº†å¤§ä½¬çš„åšå®¢ï¼Œè§‰å¾—å†™å¾—å¾ˆå¥½ï¼Œä¹Ÿæ˜¯æˆ‘ç°åœ¨æ‰€æ¬ ç¼ºçš„ï¼Œæ‰€ä»¥ä¸‹é¢å…ˆå¯¹å¤§ä½¬çš„åšå®¢è¿›è¡Œå¤ç°ã€‚ä»¥ä¾›è‡ªå·±æ—¥åå¤ä¹ ï¼ŒæŸ¥æ‰¾ï¼Œå®Œå–„æˆè‡ªå·±çš„ä¸œè¥¿ã€‚ å­—ç¬¦ä¸²å›ºå®šå­—æ•°ï¼Œä¸è¶³çš„ç©ºæ ¼è¡¥é½1 2 3 4 5 | str.ljust(10) # å·¦å¯¹é½ å­—ç¬¦ä¸²é•¿10ä½ rjustï¼Œljustå’Œcenterä¸‰ä¸ªæ–¹æ³•æ¥ç»™å­—ç¬¦ä¸²è¡¥å…¨ç©ºæ ¼ rjustï¼Œå‘å³å¯¹å…¶ï¼Œåœ¨å·¦è¾¹è¡¥ç©ºæ ¼ ljustï¼Œå‘å·¦å¯¹å…¶ï¼Œåœ¨å³è¾¹è¡¥ç©ºæ ¼ centerï¼Œè®©å­—ç¬¦ä¸²å±…ä¸­ï¼Œåœ¨å·¦å³è¡¥ç©ºæ ¼ â€”|â€” æ’åºsorted: è¿”å›ä¸€ä¸ªæ–°çš„ listlist.sort(): æ”¹å˜ list è‡ªèº«çš„å€¼reverse å‚æ•°: é»˜è®¤ä¸º False, å‡åº, True æ—¶å˜ä¸ºé™åº åˆ—è¡¨å¾ªç¯åˆ é™¤åˆ—è¡¨å…ƒç´ å¸¸è§é”™è¯¯: ç›´æ¥åˆ é™¤, æˆ–è€…æ­£åºåˆ é™¤ æ­£ç¡®åšæ³•:1.ä½¿ç”¨ pop, å€’åºåˆ é™¤ 1 2 | for i in range(len(list)): list.pop() â€”|â€” 2.ä½¿ç”¨åˆ‡ç‰‡, éå†æ‹·è´åˆ—è¡¨, æ“ä½œåŸå§‹åˆ—è¡¨, ç”¨ remove åˆ é™¤, remove ä¼šæ“ä½œé¦–ä¸ªé‡åˆ°çš„åŒ¹é…å…ƒç´ , ç›¸ç­‰å…ƒç´ åˆ é™¤, åˆ é™¤å“ªä¸ªéƒ½ä¸€æ · 1 2 3 4 5 | for x in enumerate(a[::]): a.remove(x) for x in enumerate(a[::-1]): a.remove(x) â€”|â€” éå†åˆ—è¡¨:1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 | zz_list = ['a', 'b', 'c', 'd'] for index in list: print(index) # 0 # 1 # 2 # 3 for index in range(len(list)): print(index) # 0 # 1 # 2 # 3 for index, val in enumerate(list): print(index, val) # 0 a # 1 b # 2 c # 3 d # è®¾ç½®éå†çš„å¼€å§‹åºå·, valçš„è¾“å‡ºä¸å˜ for i, val in enumerate(list, 2): print(index, val) # 2 a # 3 b # 4 c # 5 d â€”|â€” append() æ–¹æ³•è¿½åŠ å•ä¸ªå…ƒç´  extend() æ–¹æ³•extend()å‡½æ•°ç”¨äºåœ¨åˆ—è¡¨æœ«å°¾ä¸€æ¬¡æ€§è¿½åŠ å¦ä¸€ä¸ªåºåˆ—ä¸­çš„å¤šä¸ªå€¼(ç”¨æ–°åˆ—è¡¨æ‰©å±•åŸæ¥çš„åˆ—è¡¨).è¯¥æ–¹æ³•æ²¡æœ‰è¿”å›å€¼, ä¼šç›´æ¥åœ¨å·²ç»å­˜åœ¨çš„åˆ—è¡¨ä¸­æ·»åŠ æ–°çš„åˆ—è¡¨å†…å®¹, extendå’Œ+=çš„ä½œç”¨å·®ä¸å¤š 1 2 3 4 5 | a= [[1,2,3],[4,5,6]] b= [['a','b','c'],['d','e','f']] a.extend(b) print(a) # [[1, 2, 3], [4, 5, 6], ['a', 'b', 'c'], ['d', 'e', 'f']] â€”|â€” åºåˆ—åˆ‡ç‰‡(åŒå†’å·)Pythonåºåˆ—åˆ‡ç‰‡åœ°å€å¯ä»¥å†™ä¸º [å¼€å§‹(åŒ…å«) : ç»“æŸ(ä¸åŒ…å«) : æ­¥é•¿]. å½“å¼€å§‹çœç•¥çš„æ—¶å€™, é»˜è®¤ä»ç¬¬0é¡¹å¼€å§‹, å½“ç»“å°¾çœç•¥çš„æ—¶å€™, é»˜è®¤åˆ°æ•°ç»„æœ€å, å½“æ­¥é•¿çœç•¥çš„æ—¶å€™, é»˜è®¤ä¸º1. æ­¥é•¿å¯ä»¥ä¸ºè´Ÿæ•°, ä»£è¡¨ä»å³å‘å·¦å–æ•°. 1 2 3 4 5 6 | a = range(10) # a = [0, 1, 2, 3, 4, 5, 6, 7, 8 ,9] a[0:9:1] # [0, 1, 2, 3, 4, 5, 6, 7, 8] åŒ…å«å¼€å§‹ä¸‹æ ‡, ä¸åŒ…å«ç»“æŸä¸‹æ ‡ a[1::2] # [1, 3, 5, 7, 9] a[::3] # [0, 3, 6, 9] a[::-1] # [9, 8, 7, 6, 5, 4, 3, 2, 1, 0] a[::-2] # [9, 7, 5, 3, 1] â€”|â€” update() æ–¹æ³•1 | dict.update(dict2) â€”|â€” å°† dict2 ä¸­çš„é”®å€¼æ›´æ–°åˆ° dict ä¸­, å¯¹äºå­˜åœ¨çš„åˆ™è¦†ç›–åŸå€¼, å¯¹äºä¸å­˜åœ¨çš„åˆ™æ·»åŠ æ–°çš„é”®å€¼. å®ä¾‹1 2 3 4 5 6 7 | #!/usr/bin/python dict = {'Name': 'Zara', 'Age': 7} dict2 = {'Sex': 'female' } dict.update(dict2) print \"Value : %s\" % dict â€”|â€” ä»¥ä¸Šå®ä¾‹è¾“å‡ºç»“æœä¸ºï¼š 1 | Value : {'Age': 7, 'Name': 'Zara', 'Sex': 'female'} â€”|â€” å­—å…¸éå†å­—å…¸: 1 | zz_dict = {'x': 1, 'y':2, 'z':3} â€”|â€” éå†keys: 1 2 3 4 5 6 7 8 9 | # è¾“å‡ºå‡ä¸º: x y z for key in zz_dict: print(key) for key in zz_dict.iterkeys(): print(key) for key in zz_dict.keys(): print(key) â€”|â€” éå†values: 1 2 3 4 5 6 | # è¾“å‡ºå‡ä¸º 1 2 3 for value in zz_dict.itervalues(): print(value) for value in zz_dict.values(): print(value) â€”|â€” éå†keyså’Œvalues 1 2 3 4 5 6 | # è¾“å‡ºä¸º: x corresponds to 1 (å…¶ä½™ä¸¤ä¸ªä¹Ÿä¸€æ ·) for key, value in zz_dict.iteritems(): # python3 æ²¡æœ‰iteritems print(key, \"corresponds to\", value) for key, value in zz_dict.items(): print(key, \"corresponds to\", value) â€”|â€” å­—ç¬¦ä¸²åˆ¤æ–­å­—ç¬¦ä¸²æ˜¯å¦ä¸ºå­—æ¯æˆ–è€…æ•°å­—str.isalnum() å­—æ¯æˆ–æ•°å­—str.isalpha() å­—æ¯str.isdigit() æ•°å­—str.isspace() ç©ºç™½ç¬¦, \\t, , \\r isdigit() å’Œ isnumeric() çš„åŒºåˆ« 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 | num = \"1\" #unicode num.isdigit() # True num.isdecimal() # True num.isnumeric() # True num = \"1\" # å…¨è§’ num.isdigit() # True num.isdecimal() # True num.isnumeric() # True num = b\"1\" # byte num.isdigit() # True num.isdecimal() # AttributeError 'bytes' object has no attribute 'isdecimal' num.isnumeric() # AttributeError 'bytes' object has no attribute 'isnumeric' num = \"IV\" # ç½—é©¬æ•°å­— num.isdigit() # True num.isdecimal() # False num.isnumeric() # True num = \"å››\" # æ±‰å­— num.isdigit() # False num.isdecimal() # False num.isnumeric() # True â€”|â€” isdigit()True: Unicodeæ•°å­—ï¼Œbyteæ•°å­—ï¼ˆå•å­—èŠ‚ï¼‰ï¼Œå…¨è§’æ•°å­—ï¼ˆåŒå­—èŠ‚ï¼‰ï¼Œç½—é©¬æ•°å­—False: æ±‰å­—æ•°å­—Error: æ—  isdecimal()True: Unicodeæ•°å­—ï¼Œï¼Œå…¨è§’æ•°å­—ï¼ˆåŒå­—èŠ‚ï¼‰False: ç½—é©¬æ•°å­—ï¼Œæ±‰å­—æ•°å­—Error: byteæ•°å­—ï¼ˆå•å­—èŠ‚ï¼‰ isnumeric()True: Unicodeæ•°å­—ï¼Œå…¨è§’æ•°å­—ï¼ˆåŒå­—èŠ‚ï¼‰ï¼Œç½—é©¬æ•°å­—ï¼Œæ±‰å­—æ•°å­—False: æ— Error: byteæ•°å­—ï¼ˆå•å­—èŠ‚ï¼‰ str.rstrip()å‚æ•°:chars: æŒ‡å®šåˆ é™¤çš„å­—ç¬¦(é»˜è®¤ä¸ºç©ºæ ¼æˆ–æ¢è¡Œç¬¦) è¿”å›å€¼:è¿”å›åˆ é™¤æŒ‡å®šå­—ç¬¦åçš„æ–°å­—ç¬¦ä¸² å¤‡æ³¨:åˆ é™¤å­—ç¬¦ä¸²æœ«å°¾çš„æŒ‡å®šå­—ç¬¦(é»˜è®¤ä¸ºç©ºæ ¼æˆ–æ¢è¡Œç¬¦) 1 | str.rstrip([chars]) â€”|â€” str.strip()å‚æ•°chars â€” ç§»é™¤å­—ç¬¦ä¸²å¤´å°¾æŒ‡å®šçš„å­—ç¬¦åºåˆ—ã€‚è¿”å›å€¼è¿”å›ç§»é™¤å­—ç¬¦ä¸²å¤´å°¾æŒ‡å®šçš„å­—ç¬¦ç”Ÿæˆçš„æ–°å­—ç¬¦ä¸²ã€‚å¤‡æ³¨: 1 | str.strip([chars]) â€”|â€” str.split()å‚æ•° str â€” åˆ†éš”ç¬¦ï¼Œé»˜è®¤ä¸ºæ‰€æœ‰çš„ç©ºå­—ç¬¦ï¼ŒåŒ…æ‹¬ç©ºæ ¼ã€æ¢è¡Œ( )ã€åˆ¶è¡¨ç¬¦(\\t)ç­‰ã€‚ num â€” åˆ†å‰²æ¬¡æ•°ã€‚é»˜è®¤ä¸º -1, å³åˆ†éš”æ‰€æœ‰ã€‚è¿”å›å€¼ è¿”å›åˆ†å‰²åçš„å­—ç¬¦ä¸²åˆ—è¡¨ã€‚ 1 | str.split(str=\"\", num=string.count(str)). â€”|â€” æ–‡ä»¶reduce() å‡½æ•°reduce() å‡½æ•°ä¼šå¯¹å‚æ•°åºåˆ—ä¸­å…ƒç´ è¿›è¡Œç´¯ç§¯ã€‚å‡½æ•°å°†ä¸€ä¸ªæ•°æ®é›†åˆï¼ˆé“¾è¡¨ï¼Œå…ƒç»„ç­‰ï¼‰ä¸­çš„æ‰€æœ‰æ•°æ®è¿›è¡Œä¸‹åˆ—æ“ä½œï¼šç”¨ä¼ ç»™ reduce ä¸­çš„å‡½æ•° functionï¼ˆæœ‰ä¸¤ä¸ªå‚æ•°ï¼‰å…ˆå¯¹é›†åˆä¸­çš„ç¬¬ 1ã€2 ä¸ªå…ƒç´ è¿›è¡Œæ“ä½œï¼Œå¾—åˆ°çš„ç»“æœå†ä¸ç¬¬ä¸‰ä¸ªæ•°æ®ç”¨ function å‡½æ•°è¿ç®—ï¼Œæœ€åå¾—åˆ°ä¸€ä¸ªç»“æœã€‚reduce() å‡½æ•°è¯­æ³•ï¼š 1 | reduce(function, iterable[, initializer]) â€”|â€” å‚æ•° function â€” å‡½æ•°ï¼Œæœ‰ä¸¤ä¸ªå‚æ•° iterable â€” å¯è¿­ä»£å¯¹è±¡ initializer â€” å¯é€‰ï¼Œåˆå§‹å‚æ•°è¿”å›å€¼ è¿”å›å‡½æ•°è®¡ç®—ç»“æœå®ä¾‹ 1 2 3 4 5 6 7 | def add(x, y) : # ä¸¤æ•°ç›¸åŠ  ... return x + y ... reduce(add, [1,2,3,4,5]) # è®¡ç®—åˆ—è¡¨å’Œï¼š1+2+3+4+5 15 reduce(lambda x, y: x+y, [1,2,3,4,5]) # ä½¿ç”¨ lambda åŒ¿åå‡½æ•° 15 â€”|â€” zip() å‡½æ•°zip() å‡½æ•°ç”¨äºå°†å¯è¿­ä»£çš„å¯¹è±¡ä½œä¸ºå‚æ•°, å°†å¯¹è±¡ä¸­å¯¹åº”çš„å…ƒç´ æ‰“åŒ…æˆä¸€ä¸ªä¸ª å…ƒç»„ ,ç„¶åè¿”å›æœ‰è¿™äº›å…ƒç»„ç»„æˆçš„ å¯¹è±¡. ( ç›¸æ¯”äºpython2ä¸­è¿”å›åˆ—è¡¨çš„æ–¹å¼, è¿™æ ·åšçš„å¥½å¤„æ˜¯èŠ‚çº¦äº†ä¸å°‘çš„å†…å­˜ )å¯ä»¥ç”¨list()è½¬æ¢æˆ–è€…dict()è½¬æ¢å°†å¯¹è±¡è½¬æ¢æˆç›¸åº”çš„æ•°æ®ç±»å‹å¦‚æœå„ä¸ªè¿­ä»£å™¨çš„å…ƒç´ ä¸ªæ•°ä¸ä¸€è‡´, åˆ™è¿”å›åˆ—è¡¨é•¿åº¦ä¸æœ€çŸ­çš„å¯¹è±¡ç›¸åŒ, å¤šå‡ºæ¥çš„éƒ¨åˆ†ä¼šè¢«èˆå¼ƒ, åˆ©ç”¨*å·æ“ä½œç¬¦, å¯ä»¥å°†å…ƒç»„è§£å‹æˆåˆ—è¡¨. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 | a = [1,2,3] b = [4,5,6] c = ['a','b','c','d','e','f'] zip_ab = zip(a,b) print(zip_ab) # zip object at 0x104605348 print(dict(zip_ab)) # {1: 4, 2: 5, 3: 6} # !!!æ³¨æ„, ä¸€æ—¦å°†zip_abè½¬æ¢æˆdictä»¥å, zip_abå†…éƒ¨å°±ä¸ºç©ºäº†!! ä¾‹å¦‚, å†æ¬¡è°ƒç”¨ä¸Šé¢çš„è¯­å¥: print(dict(zip_ab)) # {} # ä½†æ˜¯zip_abå¯¹è±¡æœ¬èº«ä¸ä¼šæ¶ˆå¤±, åœ°å€ä»ç„¶ä¸å˜ print(zip_ab) # zip object at 0x104605348 zip_abc = zip(a,b,c) # æ³¨æ„, ä¸‰ä¸ªå…ƒç´ çš„zipæ˜¯ä¸èƒ½è½¬æ¢æˆdictç±»å‹çš„ print(zip_abc) # zip object at 0x1046054c8 print(list(zip_abc)) # [(1, 4, 'a'), (2, 5, 'b'), (3, 6, 'c')] zip_abc = zip(a,b,c) z_a, z_b, z_c = zip(*zip_abc) # åˆ©ç”¨zip(*)å¯ä»¥å°†zipå¯¹è±¡é‡æ–°è§£å‹, è¿”å›ç±»å‹æ˜¯å…ƒç»„ print(z_a) # (1,2,3) print(z_b) # (4,5,6) print(z_c) # ('a','b','c') â€”|â€” getattr() å‡½æ•°getattr()å‡½æ•°ç”¨äºè¿”å›ä¸€ä¸ªå¯¹è±¡çš„å±æ€§å€¼, è¯­æ³•å¦‚ä¸‹ 1 | getattr(object, name[, default]) â€”|â€” å‚æ•°ï¼š object: å¯¹è±¡ name: å­—ç¬¦ä¸², å¯¹è±¡å±æ€§ default: é»˜è®¤è¿”å›å€¼, å¦‚æœä¸æä¾›è¯¥å‚æ•°, åœ¨æ²¡æœ‰å¯¹åº”å±æ€§æ—¶, å°†è§¦å‘Attributerrorå®ä¾‹ 1 2 3 4 5 6 7 8 9 10 11 12 13 | class A(object): ... bar = 1 ... a = A() getattr(a, 'bar') # è·å–å±æ€§ bar å€¼ 1 getattr(a, 'bar2') # å±æ€§ bar2 ä¸å­˜åœ¨ï¼Œè§¦å‘å¼‚å¸¸ Traceback (most recent call last): File \"stdin\", line 1, in module AttributeError: 'A' object has no attribute 'bar2' getattr(a, 'bar2', 3) # å±æ€§ bar2 ä¸å­˜åœ¨ï¼Œä½†è®¾ç½®äº†é»˜è®¤å€¼ 3 â€”|â€” dir() å‡½æ•°å¯ä»¥æŸ¥çœ‹æŸä¸ªç±»çš„æ‰€æœ‰æ–¹æ³•å’Œå±æ€§ 1 | members = [attr for attr in dir(classA)] â€”|â€” _var: åœ¨ä¸€ä¸ªæ¨¡å—ä¸­ä»¥å•ä¸‹åˆ’çº¿å¼€å¤´çš„å˜é‡å’Œå‡½æ•°ä¼šè¢«é»˜è®¤å½“åšå†…éƒ¨å‡½æ•°, åœ¨ä½¿ç”¨from a_module import * å¯¼å…¥æ—¶, è¿™éƒ¨åˆ†å˜é‡å’Œå‡½æ•°ä¸ä¼šè¢«å¯¼å…¥. ä¸è¿‡å¦‚æœä½¿ç”¨import a_moduleå¯¼å…¥æ¨¡å—æ—¶, ä»ç„¶å¯ä»¥ç”¨a_module._varçš„å½¢å¼è®¿é—®è¯¥å˜é‡æˆ–å‡½æ•° var_: æœ‰æ—¶å€™, ä¸€ä¸ªå˜é‡çš„æœ€é€‚åˆçš„åç§°å·²ç»è¢«å¦ä¸€ä¸ªå…³é”®å­—æ‰€å ç”¨. åœ¨è¿™ç§æƒ…å†µä¸‹, å¯ä»¥åœ¨åç§°çš„æœ«å°¾é™„åŠ ä¸€ä¸ªä¸‹åˆ’çº¿æ¥è§£å†³å†²çª. __var: åŒä¸‹åˆ’çº¿å‰ç¼€ä¼šå¯¼è‡´Pythonè§£é‡Šå™¨é‡å†™å±æ€§åç§°, ä»¥é¿å…å­ç±»ä¸­çš„å‘½åå†²çª. ä¸¾ä¾‹æ¥è¯´, å¦‚æœåœ¨class Testä¸­æœ‰ä¸€ä¸ªæˆå‘˜__x, é‚£ä¹ˆå½“åˆ©ç”¨å†…ç½®å‡½æ•°dir(Test)æ¥æŸ¥çœ‹ç±»çš„å±æ€§æ—¶, ä¼šå‘ç°__xè¢«è§£é‡Šå™¨é‡å‘½åä¸º_Test__x. åŒä¸‹åˆ’çº¿çš„åç§°ä¿®é¥°åŒæ ·ä¹Ÿé€‚ç”¨äºæ–¹æ³•åç§°. var: åŒä¸‹åˆ’çº¿å¼€å¤´å’Œç»“å°¾çš„æ˜¯ä¸€äº› Python çš„ç‰¹æ®Šå¯¹è±¡, å¦‚ç±»æˆå‘˜çš„ init, del, name, call ç­‰. Python å®˜æ–¹æ¨èæ°¸è¿œä¸è¦è®²è¿™æ ·çš„å‘½åæ–¹å¼åº”ç”¨äºè‡ªå·±çš„å˜é‡æˆ–å‡½æ•°. æœ‰ä¸€ç§è¯´æ³•æ˜¯è¯´åŒä¸‹åˆ’çº¿å»ºè®®ä¸ºç±»çš„ç§æœ‰æˆå‘˜, ä½†æ˜¯ PEP8 å½“å‰çš„å®˜æ–¹ç‰ˆæœ¬ä¸­å¹¶æ²¡æœ‰æ˜è¯´. _: æœ‰æ—¶å€™æˆ‘ä»¬ä¼šç”¨ä¸€ä¸ªç‹¬ç«‹çš„ä¸‹åˆ’çº¿ä½œä¸ºä¸€ä¸ªåå­—, è¿™é€šå¸¸æ˜¯ç”¨æ¥æŒ‡ç¤ºæŸä¸ªå˜é‡æ—¶ä¸´æ—¶çš„æˆ–è€…æ— å…³ç´§è¦çš„. ç±»çš„ç‰¹æ®Šæ–¹æ³•call()åœ¨ Python ä¸­, å‡½æ•°å®é™…ä¸Šä¹Ÿæ˜¯ä¸€ä¸ªå¯¹è±¡: 1 2 3 | f = abs print(f.__name__) # 'abs' print(f(-123)) # 123 â€”|â€” ä»ä¸Šé¢å¯ä»¥çœ‹å‡º, å‡½æ•°æ˜¯ä¸€ä¸ªå¯¹è±¡, å½“å®ƒèµ‹ç»™å¦ä¸€ä¸ªå˜é‡æ—¶, è¯¥å˜é‡ä¹Ÿæ˜¯ä¸€ä¸ªå‡½æ•°å¯¹è±¡, å¯ä»¥èµ·åˆ°ä¸åŸå‡½æ•°ç›¸åŒçš„æ•ˆæœ. åœ¨ Python ä¸­, ä¸€ä¸ªç±»å®ä¾‹ä¹Ÿå¯ä»¥å˜æˆä¸€ä¸ªå¯è°ƒç”¨å¯¹è±¡, åªéœ€è¦å®ç°ä¸€ä¸ªç‰¹æ®Šæ–¹æ³• call() å³å¯. ä¸‹é¢æˆ‘ä»¬ä¸¾ä¾‹æŠŠ Person ç±»å˜æˆä¸€ä¸ªå¯è°ƒç”¨å¯¹è±¡: 1 2 3 4 5 6 7 8 9 | class Person(object): def __init__(self, name, gender): self.name = name self.gender = gender def __call__(self, friend): print(\"name:\", self.name) print(\"friend:\", friend) â€”|â€” æ¥ä¸‹æ¥æˆ‘ä»¬å°±å¯ä»¥å°† Person ç±»çš„å®ä¾‹å¯¹è±¡å½“åšä¸€ä¸ªå‡½æ•°æ¥ä½¿ç”¨, å¦‚ä¸‹æ‰€ç¤º: 1 2 3 4 | p = Person('Bob', 'male') p('Tim') # name: Bob # friend: Tim â€”|â€” getitem()å‡¡æ˜¯åœ¨ç±»ä¸­å®šä¹‰äº† getitem() æ–¹æ³•, é‚£ä¹ˆå®ƒçš„å®ä¾‹å¯¹è±¡å°±æ˜¯å¯ä»¥é€šè¿‡ [] æ“ä½œç¬¦æ¥è®¿é—®æŒ‡å®šçš„æˆå‘˜æˆ–è¿›è¡Œç‰¹å®šçš„è¡Œä¸º, å¤§å¤šæ•°æƒ…å†µä¸‹ä¼šå°†è¯¥æ–¹æ³•å®ç°æˆé€šè¿‡ç´¢å¼•æ¥æ–¹æ³•å…ƒç´ çš„å½¢å¼. 1 2 3 4 5 6 7 | class DataBase(object): def __init__(self): super(DataBase, self).__init__() self.vals = [1,2,3,4,5] def __getitem__(self, key): return self.vals[key] â€”|â€” setitem()ä½¿å¾—å¯ä»¥é€šè¿‡ A[3] = 4, B[â€œaâ€] = 5 ç­‰æ–¹å¼æ¥å¯¹ç±»ä¸­çš„å…ƒç´ è¿›è¡Œèµ‹å€¼ file()æŸ¥çœ‹æ¨¡å—çš„è·¯å¾„ len()ä½¿å¾—ç±»å¯¹è±¡å¯ä»¥ä½¿ç”¨ Python çš„å†…å»ºæ–¹æ³• len(), è¿”å›ä½ è‡ªå®šä¹‰çš„æ•°å€¼. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | class DictDemo: def __init__(self,key,value): self.dict = {} self.dict[key] = value def __getitem__(self,key): return self.dict[key] def __setitem__(self,key,value): self.dict[key] = value def __len__(self): return len(self.dict) dictDemo = DictDemo('key0','value0') print(dictDemo['key0']) #value0 dictDemo['key1'] = 'value1' print(dictDemo['key1']) #value1 print(len(dictDemo)) #2 â€”|â€” repr()1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 | class Test(object): def __init__(self, value='hello, world!'): self.data = value t = Test() t __main__.Test at 0x7fa91c307190 print t __main__.Test object at 0x7fa91c307190 # çœ‹åˆ°äº†ä¹ˆï¼Ÿä¸Šé¢æ‰“å°ç±»å¯¹è±¡å¹¶ä¸æ˜¯å¾ˆå‹å¥½ï¼Œæ˜¾ç¤ºçš„æ˜¯å¯¹è±¡çš„å†…å­˜åœ°å€ # ä¸‹é¢æˆ‘ä»¬é‡æ„ä¸‹è¯¥ç±»çš„__repr__ä»¥åŠ__str__ï¼Œçœ‹çœ‹å®ƒä»¬ä¿©æœ‰å•¥åŒºåˆ« # é‡æ„__repr__ class TestRepr(Test): def __repr__(self): return 'TestRepr(%s)' % self.data tr = TestRepr() tr TestRepr(hello, world!) print tr TestRepr(hello, world!) # é‡æ„__repr__æ–¹æ³•åï¼Œä¸ç®¡ç›´æ¥è¾“å‡ºå¯¹è±¡è¿˜æ˜¯é€šè¿‡printæ‰“å°çš„ä¿¡æ¯éƒ½æŒ‰æˆ‘ä»¬__repr__æ–¹æ³•ä¸­å®šä¹‰çš„æ ¼å¼è¿›è¡Œæ˜¾ç¤ºäº† # é‡æ„__str__ calss TestStr(Test): def __str__(self): return '[Value: %s]' % self.data ts = TestStr() ts __main__.TestStr at 0x7fa91c314e50 print ts [Value: hello, world!] # ä½ ä¼šå‘ç°ï¼Œç›´æ¥è¾“å‡ºå¯¹è±¡tsæ—¶å¹¶æ²¡æœ‰æŒ‰æˆ‘ä»¬__str__æ–¹æ³•ä¸­å®šä¹‰çš„æ ¼å¼è¿›è¡Œè¾“å‡ºï¼Œè€Œç”¨printè¾“å‡ºçš„ä¿¡æ¯å´æ”¹å˜äº† â€”|â€” str()å‚è§ repr() ä»£ç ç¤ºä¾‹ æ˜Ÿå· **: ä¹˜æ³•**: ä¹˜å¹‚ ç”¨äºå‡½æ•°å‚æ•°å•æ˜Ÿå·: å°†æ‰€æœ‰å‚æ•°ä»¥ å…ƒç»„(tuple) çš„å½¢å¼å¯¼å…¥ 1 2 3 4 5 6 | def foo(param1, *param2): print(param1) print(param2) foo(1,2,3,4,5) # 1 # (2,3,4,5) â€”|â€” åŒæ˜Ÿå·: å°†æ‰€æœ‰å‚æ•°ä»¥ å­—å…¸ çš„å½¢å¼å¯¼å…¥ 1 2 3 4 5 6 | def bar(param1, **param2): print(param1) print(param2) bar(1, a=2, b=3) # 1 # {'a': 2, 'b': 3} â€”|â€” å½“ç„¶è¿™ä¸¤ä¸ªç”¨æ³•å¯ä»¥åŒæ—¶å‡ºç°åœ¨ä¸€ä¸ªå‡½æ•°ä¸­: 1 2 3 4 5 6 7 8 9 10 | def fun(a, b=10, *args, **kwargs): print(a) print(b) print(args) print(kwargs) fun(1,2,3,4,e=5,f=6) # 1 # 2 # (3,4) # {'e': 5, 'f': 6} â€”|â€” globals() å‡½æ•°è¯¥å‡½æ•°ä¼šä»¥å­—å…¸ç±»å‹è¿”å›å½“å‰ä½ç½®çš„å…¨éƒ¨å…¨å±€å˜é‡ stripe()readlines()lambda å‡½æ•°3.6æ–°åŠŸèƒ½ f stringåŒ…çš„å¯¼å…¥æœºåˆ¶æ¨¡å—å’ŒåŒ…çš„å®šä¹‰æ¨¡å—(module): ç”¨æ¥ä»é€»è¾‘ä¸Šç»„ç»‡ Python ä»£ç (å˜é‡, å‡½æ•°, ç±»), é€šå¸¸æ˜¯ä¸€ä¸ª.pyæ–‡ä»¶.åŒ…(package): å®šä¹‰äº†ä¸€ä¸ªç”±æ¨¡å—å’Œå­åŒ…ç»„æˆçš„ Python åº”ç”¨ç¨‹åºæ‰§è¡Œç¯å¢ƒ, æœ¬è´¨ä¸Šå°±æ˜¯ä¸€ä¸ªæœ‰å±‚æ¬¡çš„æ–‡ä»¶ç›®å½•ç»“æœ(å¿…é¡»å¸¦æœ‰ä¸€ä¸ª__init__.pyæ–‡ä»¶) import çš„æœç´¢è·¯å¾„ åœ¨å½“å‰ç›®å½•ä¸‹æœç´¢ åœ¨ç¯å¢ƒå˜é‡PYTHONPATHä¸­æŒ‡å®šçš„è·¯å¾„åˆ—è¡¨ä¸­æœç´¢ åœ¨ Python å®‰è£…è·¯å¾„çš„libåº“ä¸­æœç´¢Python æ‰€æœ‰åŠ è½½çš„æ¨¡å‹ä¿¡æ¯éƒ½å­˜æ”¾åœ¨sys.modulesç»“æ„ä¸­, å½“importä¸€ä¸ªæ¨¡å—æ—¶, ä¼šæŒ‰å¦‚ä¸‹æ­¥éª¤æ¥è¿›è¡Œ: å¦‚æœimport A, æ£€æŸ¥sys.modulesä¸­æ˜¯å¦å·²ç»æœ‰A, å¦‚æœæœ‰åˆ™ä¸åŠ è½½, å¦‚æœæ²¡æœ‰åˆ™ä¸ºAåˆ›å»ºmoduleå¯¹è±¡, å¹¶åŠ è½½A; å¦‚æœæ˜¯from A import B, å…ˆä¸ºAåˆ›å»ºmoduleå¯¹è±¡, å†è§£æA(æ­¤æ—¶ä¼šåŠ è½½å¹¶æ‰§è¡ŒAä¸­çš„æ‰€æœ‰ä»£ç ), ä»ä¸­å¯»æ‰¾Bå¹¶å¡«å……åˆ°Açš„__dict__ä¸­.åœ¨å¯¼å…¥æ¨¡å—çš„æ—¶å€™, æ¨¡å—æ‰€åœ¨æ–‡ä»¶å¤¹ä¼šè‡ªåŠ¨ç”Ÿæˆä¸€ä¸ª__pycache__/module_name.cpython-35.pycçš„æ–‡ä»¶. 1 2 3 4 | import module_nameçš„æœ¬è´¨æ˜¯å°†module_name.pyä¸­çš„å…¨éƒ¨ä»£ç åŠ è½½åˆ°å†…å­˜ä¸­, å¹¶å°†å…¶èµ‹å€¼ç»™ä¸æ¨¡å—åŒåçš„å˜é‡, è¿™ä¸ªå˜é‡çš„ç±»å‹æ˜¯classmodule. from module_name import nameçš„æœ¬è´¨æ˜¯å°†æŒ‡å®šçš„å˜é‡æˆ–è€…æ–¹æ³•å¯¼å…¥åˆ°å½“å‰çš„æ–‡ä»¶ä¸­ import package_nameçš„æœ¬è´¨æ˜¯æ‰§è¡Œè¯¥åŒ…ä¸‹çš„__init__.pyæ–‡ä»¶, åœ¨æ‰§è¡Œæ–‡ä»¶å, ä¼šåœ¨package_nameç›®å½•ä¸‹ç”Ÿæˆä¸€ä¸ª__pycache__/__init__cpython-35.pycæ–‡ä»¶. from package_name import *çš„æœ¬è´¨æ˜¯å¯¼å…¥__init__.pyæ–‡ä»¶ä¸­çš„__all__åˆ—è¡¨(eg. __all__ = ['L2Norm', 'MultiBoxLoss']). â€”|â€” ç›¸å¯¹å¯¼å…¥å’Œç»å¯¹å¯¼å…¥ç»å¯¹å¯¼å…¥: 1 2 | import A.B from A import B â€”|â€” ç›¸å¯¹å¯¼å…¥: 1 2 | from . import B # . ä»£è¡¨å½“å‰è·¯å¾„ from ..A import B # .. ä»£è¡¨ä¸Šå±‚è·¯å¾„, ... ä»£è¡¨ä¸Šä¸Šå±‚è·¯å¾„. â€”|â€” åœ¨æ²¡æœ‰æ˜ç¡®æŒ‡å®šåŒ…ç»“æ„çš„æƒ…å†µä¸‹, Python æ˜¯æ ¹æ®__name__æ¥å†³å®šä¸€ä¸ªæ¨¡å—åœ¨åŒ…ä¸­çš„ç»“æ„çš„, å¦‚æœæ˜¯__main__, åˆ™å®ƒæœ¬èº«å°±æ˜¯é¡¶å±‚æ¨¡å—, æ²¡æœ‰åŒ…ç»“æ„, å¦‚æœæ˜¯A.B.Cç»“æ„, åˆ™Aæ˜¯é¡¶å±‚æ¨¡å—. Python çš„å¯¼å…¥æ–¹å¼çš„ä¸åŒå…·æœ‰ä¸åŒçš„è§„åˆ™: 1.å¦‚æœæ˜¯ç»å¯¹å¯¼å…¥, ä¸€ä¸ªæ¨¡å—åªèƒ½å¯¼å…¥è‡ªèº«çš„å­æ¨¡å—æˆ–è€…å’Œå®ƒçš„é¡¶å±‚æ¨¡å—åŒçº§åˆ«çš„æ¨¡å—åŠå…¶å­æ¨¡å—.2.å¦‚æœæ˜¯ç›¸å¯¹å¯¼å…¥, ä¸€ä¸ªæ¨¡å—å¿…é¡»æœ‰åŒ…ç»“æ„ä¸”åªèƒ½å¯¼å…¥å®ƒçš„é¡¶å±‚æ¨¡å—å†…éƒ¨çš„æ¨¡å—. å¦‚æœä¸€ä¸ªæ¨¡å—è¢«ç›´æ¥è¿è¡Œ, åˆ™å®ƒè‡ªå·±ä¸ºé¡¶å±‚æ¨¡å—, ä¸å­˜åœ¨å±‚æ¬¡ç»“æ„, æ‰€ä»¥ä¹Ÿæ‰¾ä¸åˆ°ä¸Šå±‚(..)çš„ç›¸å¯¹è·¯å¾„Python2.x é»˜è®¤ä¸ºç›¸å¯¹è·¯å¾„å¯¼å…¥, è€Œ Python3.x é»˜è®¤ä¸ºç»å¯¹è·¯å¾„å¯¼å…¥, è¿™æ ·å¯ä»¥é¿å…å¯¼å…¥çš„å­åŒ…è¦†ç›–æ‰æ ‡å‡†åº“æ¨¡å—. é€šå¸¸, åœ¨ Python2.x ä¸­, æˆ‘ä»¬åˆ©ç”¨ä¸‹é¢çš„è¯­å¥æ¥ä½¿å…¶å¯¼å…¥è§„åˆ™éµå¾ª Python3.x 1 | from __future__ import absolute_import â€”|â€” absolute_importçš„æ„æ€å¹¶ä¸æ˜¯å°†æ‰€æœ‰çš„å¯¼å…¥éƒ½è§†ä¸ºç»å¯¹å¯¼å…¥, è€Œæ˜¯æŒ‡ç¦ç”¨éšå¼ç›¸å¯¹å¯¼å…¥(implicit relative import), å…³äºéšå¼çš„æ˜¾ç¤ºçš„å…·ä½“åŒºåˆ«, å¯ä»¥çœ‹ä¸‹é¢çš„ä¾‹å­, å‡è®¾æœ‰å¦‚ä¸‹çš„åŒ…ç»“æ„: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 | thing â””â”€â”€ __init__.py â”œâ”€â”€ books â”‚ â”œâ”€â”€ __init__.py â”‚ â”œâ”€â”€ adventure.py â”‚ â”œâ”€â”€ history.py â”‚ â”œâ”€â”€ horror.py â”‚ â””â”€â”€ lovestory.py â”œâ”€â”€ furniture â”‚ â”œâ”€â”€ __init__.py â”‚ â”œâ”€â”€ armchair.py â”‚ â”œâ”€â”€ bench.py â”‚ â”œâ”€â”€ screen.py â”‚ â””â”€â”€ stool.py â€”|â€” é‚£ä¹ˆå¦‚æœæƒ³åœ¨stool.pyä¸­å¯¼å…¥benchæ¨¡å—, åˆ™æœ‰å¦‚ä¸‹å‡ ç§æ–¹å¼: 1 2 3 | import bench # éšå¼ç›¸å¯¹å¯¼å…¥ from . import bench # æ˜¾å¼ç›¸å¯¹å¯¼å…¥ from furniture import bench # ç»å¯¹å¯¼å…¥ â€”|â€” éšå¼ç›¸å¯¹å¯¼å…¥æ²¡æœ‰å‘Šè¯‰è§£é‡Šå™¨ç›¸å¯¹äºè°è¿›è¡Œå¯¼å…¥, é»˜è®¤ç›¸å¯¹äºå½“å‰æ¨¡å—; è€Œæ˜¾å¼ç›¸å¯¹å¯¼å…¥åˆ™æ˜ç¡®å‘Šè¯‰äº†è§£é‡Šå™¨ç›¸å¯¹äºè°æ¥å¯¼å…¥. ä»¥ä¸Šå¯¼å…¥æ–¹å¼çš„ç¬¬ä¸‰ç§æ˜¯å®˜æ–¹æ¨èçš„, ç¬¬ä¸€ç§æ˜¯å®˜æ–¹å¼ºçƒˆä¸æ¨èçš„, Python3 ä¸­ç¬¬ä¸€ç§å¯¼å…¥æ–¹å¼åªèƒ½ç”¨äºå¯¼å…¥sys.pathä¸­çš„æ¨¡å—.**æ³¨æ„, è¿˜æœ‰ç›¸å¯¹å¯¼å…¥çš„æ¨¡å—ä¸èƒ½è¢«ç›´æ¥è¿è¡Œ, ä¼šæç¤ºå¦‚ä¸‹é”™è¯¯: 1 2 3 4 | Traceback (most recent call last): File \"test.py\", line 8, in module from .ssd import SSD ModuleNotFoundError: No module named '__main__.ssd'; '__main__' is not a package â€”|â€” å¦å¤–å­˜åœ¨ä¸€ç§æƒ…å†µå°±æ˜¯: å‡å¦‚æœ‰ä¸¤ä¸ªæ¨¡å—a.pyå’Œb.pyæ”¾åœ¨åŒä¸€ä¸ªç›®å½•ä¸‹, åˆ™å¯ä»¥ç›´æ¥åœ¨a.pyä¸­ä½¿ç”¨import bæ¥å¯¼å…¥æ¨¡å—b. è¿™æ˜¯ä¸ºä»€ä¹ˆå‘¢? æˆ‘ä»¬ä¸Šé¢è¯´äº†åœ¨ Python3.x ä¸­ä¸èƒ½ä½¿ç”¨è¿™ç§éšå¼ç›¸å¯¹å¯¼å…¥, ä½†æ˜¯è¿™é‡Œå´å¯ä»¥æˆåŠŸå¯¼å…¥, è¿™æ˜¯å› ä¸ºæ­¤æ—¶æˆ‘ä»¬æ˜¯ç›´æ¥è¿è¡Œa.py, æ‰€ä»¥a.pyå’Œb.pyçš„ç›®å½•æ²¡æœ‰è¢«å½“åšä¸€ä¸ªåŒ…æ¥å¤„ç†, å› æ­¤ä¸æ¶‰åŠç›¸å¯¹å¯¼å…¥å’Œç»å¯¹å¯¼å…¥çš„æ¦‚å¿µ. å› æ­¤ç›¸å¯¹å¯¼å…¥å’Œç»å¯¹å¯¼å…¥ä»…ä»…æ˜¯é’ˆå¯¹äºåŒ…è€Œè¨€çš„. ç»¼åˆè·ç¦»å­˜åœ¨ç›®å½•ç»“æ„å¦‚ä¸‹æ‰€ç¤º: 1 2 3 4 5 6 7 8 9 10 11 12 | dirRoot â””â”€â”€ __init__.py â”œâ”€â”€ file1.py â”œâ”€â”€ file2.py â”œâ”€â”€ dirA â”‚ â”œâ”€â”€ __init__.py â”‚ â”œâ”€â”€ a1.py â”‚ â””â”€â”€ a2.py â”œâ”€â”€ dirB â”‚ â”œâ”€â”€ __init__.py â”‚ â”œâ”€â”€ b1.py â”‚ â””â”€â”€ b2.py â€”|â€” ç›´æ¥è¿è¡Œa1.py, å¹¶å¸Œæœ›å¯¼å…¥a2æ¨¡å—: 1 2 3 4 | # a1.py import a2 # æ­£ç¡®, æ­¤æ—¶å¹¶æœªå°† dirA å½“åšåŒ…æ¥å¤„ç†, a1.py å’Œ a2.py ç›¸å½“äºä¸¤ä¸ªç‹¬ç«‹çš„æ¨¡å— from a2 import func_a2 # æ­£ç¡® from .a2 import func_a2 # é”™è¯¯, å½“è¿›è¡Œç›¸å¯¹å¯¼å…¥æ—¶, ä¸èƒ½ç›´æ¥è¿è¡Œ â€”|â€” ç›´æ¥è¿è¡Œfile1.py, å¹¶å¸Œæœ›å¯¼å…¥a1æ¨¡å—, åŒæ—¶a1æ¨¡å—ä¸­éœ€è¦å¯¼å…¥a2æ¨¡å—: 1 2 3 4 5 6 7 8 9 10 | # file1.py from dirA import a1 a1.func_a1() # a1.py ä¸­çš„å‡½æ•° a1.func_a2() # a1.py ä¸­å¯¼å…¥äº† a2.py çš„å‡½æ•°, å¯ä»¥ç›´æ¥ä½¿ç”¨ # a1.py import a2 # é”™è¯¯, æ­¤æ—¶ç”±äº dirA ä¸­æœ‰ __init__.py æ–‡ä»¶, å› æ­¤ä¼šå°† dirA å½“åšåŒ…æ¥å¤„ç†, # ç”±äº Python3.x ä¸å…è®¸ä½¿ç”¨éšå¼çš„ç›¸å¯¹å¯¼å…¥, å› æ­¤è¯¥è¯­å¥éæ³• from a2 import func_a2 # é”™è¯¯, åŸå› åŒä¸Š from .a2 import func_a2 # æ­£ç¡®, å½“è¿›è¡Œç›¸å¯¹å¯¼å…¥æ—¶, éœ€è¦ä½¿ç”¨æ˜¾å¼çš„ç›¸å¯¹å¯¼å…¥ â€”|â€” ç›´æ¥è¿è¡Œfile1.py, å¹¶å¸Œæœ›å¯¼å…¥a1æ¨¡å—, åŒæ—¶a1æ¨¡å—ä¸­éœ€è¦å¯¼å…¥dirB/b1æ¨¡å—(è·¨æ–‡ä»¶å¤¹å¯¼å…¥): 1 2 3 4 5 6 7 8 9 10 | # file1.py from dirA import a1 a1.func_a1() # a1.py ä¸­çš„å‡½æ•° a1.func_a2() # a2.py ä¸­çš„å‡½æ•° a1.func_b1() # b1.py ä¸­çš„å‡½æ•° # a1.py from .a2 import func_a2 # æ¨èä½¿ç”¨ç»å¯¹å¯¼å…¥ from dirA.a1 import func_a2 from dirB import b1 # ç”±äºè¿è¡Œçš„æ˜¯ file1.py æ–‡ä»¶, å› æ­¤é¡¶å±‚ç›®å½•æ˜¯ dirRoot from dirB.b1 import func_b1 # æ‰€ä»¥å¯ä»¥ç›´æ¥ä½¿ç”¨ dirB åŒ… â€”|â€” ç›´æ¥è¿è¡Œa1.py, å¹¶å¸Œæœ›è·¨ç›®å½•çš„å¯¼å…¥dirB/b1æ¨¡å—. ç”±äºè¿™ç§è·¨ç›®å½•çš„å¯¼å…¥è¶…è¶Šäº†é¡¶å±‚è·¯å¾„çš„é™åˆ¶, å› æ­¤å¿…é¡»ä½¿ç”¨sys.path.append()æ–¹æ³•æ¥é¢å¤–æ·»åŠ æœç´¢è·¯å¾„, å¦åˆ™æ— æ³•æ­£å¸¸å¯¼ 1 2 3 4 5 6 | # a1.py import sys sys.path.append(\"../\") # å°† dirA çš„ä¸Šä¸€æ¬¡ç›®å½•æ·»åŠ åˆ°æœç´¢è·¯å¾„ä¸­ from dirB import b1 # æ­£ç¡®, æ³¨æ„å¿…é¡»å…ˆæ·»åŠ  path, ç„¶åå†å¯¼å…¥ from dirB.b1 import func_b1 # æ­£ç¡® from .a2 import func_a2 # è¿™é‡Œæ˜¯é”™è¯¯çš„, å½“ç›´æ¥æ‰§è¡Œ a1.py æ—¶, a1.py ä¸­ä¸èƒ½åŒ…å«æ˜¾å¼ç›¸å¯¹å¯¼å…¥ â€”|â€” è·å– python ç‰ˆæœ¬:1 | print(sys.version_info) â€”|â€” è·å–åŒ…çš„å®‰è£…ä½ç½®1 | print(cv2) â€”|â€” è§£æ xml æ–‡ä»¶å¯¼å…¥: 1 2 3 4 5 | import sys if sys.version_info[0] == 2: import xml.etree.cElementTree as ET else: import xml.etree.ElementTree as ET â€”|â€” è§£æ: 1 2 3 4 5 6 7 8 9 10 11 12 13 | xmlfile = ET.parse(xmlfile_path) root = xmlfile.getroot() # è·å–æ ¹èŠ‚ç‚¹ root.tag # æ ‡ç­¾ root.attrib # å±æ€§å­—å…¸ for child in root: # è¿­ä»£è®¿é—®å­èŠ‚ç‚¹ print(child.tag, child.attrib) # å¯ä»¥é€šè¿‡ç´¢å¼•è®¿é—®åµŒå¥—èŠ‚ç‚¹çš„å†…å®¹ root[0][1].text Element.findall() # Element.find() # â€”|â€” python ä¸­ == å’Œ is çš„åŒºåˆ«== åªç”¨äºåˆ¤æ–­å€¼æ˜¯å¦ç›¸ç­‰is ç”¨äºåˆ¤æ–­ä¸¤ä¸ªå¯¹è±¡æ˜¯å¦ä¸ºåŒä¸€ä¸ªå®ä¾‹å°æ•´æ•°å¯¹è±¡æ± : Python ä¸ºäº†ä¼˜åŒ–é€Ÿåº¦ï¼Œä½¿ç”¨äº†å°æ•´æ•°å¯¹è±¡æ± ï¼Œé¿å…ä¸ºæ•´æ•°é¢‘ç¹ç”³è¯·å’Œé”€æ¯å†…å­˜ç©ºé—´ã€‚è€ŒPython å¯¹å°æ•´æ•°çš„å®šä¹‰æ˜¯ [-5, 257)ï¼Œåªæœ‰æ•°å­—åœ¨-5åˆ°256ä¹‹é—´å®ƒä»¬çš„idæ‰ä¼šç›¸ç­‰ï¼Œè¶…è¿‡äº†è¿™ä¸ªèŒƒå›´å°±ä¸è¡Œäº†ï¼ŒåŒæ ·çš„é“ç†ï¼Œå­—ç¬¦ä¸²å¯¹è±¡ä¹Ÿæœ‰ä¸€ä¸ªç±»ä¼¼çš„ç¼“å†²æ± ï¼Œè¶…è¿‡åŒºé—´èŒƒå›´å†…è‡ªç„¶ä¸ä¼šç›¸ç­‰äº† é˜Ÿåˆ— queueåœ¨ Python3 ä¸­, åŸæ¥çš„Queueæ¨¡å—è¢«é‡å‘½åä¸ºqueue, è¯¥æ¨¡å—åŒ…å«ä»¥ä¸‹ä¸‰ç±»æ•°æ®ç»“æ„: queue.Queue(maxsize=0): FIFO queue, å…ˆè¿›å…ˆå‡ºé˜Ÿåˆ—, ä»£è¡¨æ™®é€šé˜Ÿåˆ— queue.LifoQueue(maxsize=0): LIFO queue, åè¿›å…ˆå‡ºé˜Ÿåˆ—, ç±»ä¼¼æ ˆçš„ä½œç”¨ queue.PriorityQueue(maxsize=0): ä¼˜å…ˆçº§é˜Ÿåˆ—, ç±»ä¼¼å †çš„ä½œç”¨. é»˜è®¤ä¸ºå°é¡¶å †, å¸¸ç”¨å½¢å¼ä¸ºå…ƒç»„:(priority_number, data)ä¸Šé¢çš„ maxsize è¡¨æ˜äº†é˜Ÿåˆ—ä¸­æœ€å¤§å¯ä»¥å®¹çº³çš„å…ƒç´ æ•°é‡, å¦‚æœè¶…è¿‡, åˆ™æ— æ³•æ’å…¥. å½“ maxsize = 0 æ—¶, ä»£è¡¨å…ƒç´ æ•°é‡æ— é™åˆ¶.å…¬æœ‰æ–¹æ³•(ä»¥ä¸Šä¸‰ä¸ªé€šç”¨): qsize(): è¿”å› approximate size, qsize() 0 ä¸ä¿è¯get()ä¸€å®š work, åŒç†, qsize() maxsize ä¸ä¿è¯put()ä¸€å®š work. empty(): å¦‚æœé˜Ÿåˆ—ä¸ºç©º, è¿”å› True. å’Œqsize()ä¸€æ ·, ä¸æä¾›ä¿è¯æ€§. full(): å¦‚æœé˜Ÿåˆ—æ»¡, è¿”å› True. ä¸æä¾›ä¿è¯æ€§ put(item[, block[, timeout]]) put_nowait(item): ç­‰ä»·äºput(item, False) get([block[, timeout]]) get_nowait(): ç­‰ä»·äºget(False) task_done(): join(): å † heapqheapq æ¨¡å—åªæœ‰æœ€å°å †çš„åŠŸèƒ½, è¦å®ç°æœ€å¤§å †, éœ€è¦åœ¨å…¥å †å’Œå‡ºå †çš„æ—¶å€™å–å, å¹¶ä¸” heapq æ¨¡å—åªèƒ½ä½œç”¨äºæ•°å€¼å‹ç±»å‹.æœ€å¤§å †: _heapify_max(), _heappop_max() ç»™å®šä¸€ç»„æ•°æ®, åˆ›å»ºå †, ä¸¤ç§æ–¹å¼(äºŒè€…ç­‰ä»·): 1 2 3 4 5 6 7 | import heapq data = [1,3,6,2,8,5] heap = [] for d in data: heapq.heappush(heap, n) # æ–¹æ³•ä¸€ é€ä¸ªæ„å»º heapq.heapify(data) # æ–¹æ³•äºŒ åŸåœ°æ„å»º, æ•ˆç‡æ›´é«˜ â€”|â€” å°é¡¶å †: 1 2 3 4 5 6 | heap = [1,3,6,2,8,5] heapq.heapify(heap) heapq.heappop(heap) # è¿”å›å¹¶åˆ é™¤å †é¡¶ heapq.heapreplace(heap, 10) # åˆ é™¤å †é¡¶å¹¶æ·»åŠ æ–°å€¼ heapq.heappushpop(heap, 10) # å…ˆå°†æ–°å€¼åŠ å…¥å †ä¸­, ç„¶åç«‹åˆ»å¼¹å‡ºå †é¡¶ print(heap[0]) # æŸ¥çœ‹å †é¡¶ â€”|â€” å¤§é¡¶å †: 1 2 3 4 5 6 7 8 9 10 11 12 | # æ–¹æ³•ä¸€: å–è´Ÿå€¼ heap = [-1,-3,-6,-2,-8,-5] # æ–¹æ³•äºŒ: å†…ç½®æ–¹æ³• heap = [1,3,6,2,8,5] heapq._heapify_max(heap) # max_heap print(heap[0]) # æŸ¥çœ‹å †é¡¶, 8 heapq._heappop_max(heap) # po from maxheap print(heap[0]) # 6 heapq._heapreplace_max(heap, 10) print(heap[0]) # 10 # heapq._heappushpop_max(heap, 10) # æ³¨æ„, æ²¡æœ‰ _heappushpop_max å‡½æ•° â€”|â€” Python åˆ·é¢˜å¸¸ç”¨é˜Ÿåˆ—: 1 2 3 4 5 6 7 8 9 10 11 12 | import Queue base_queue = Queue.Queue() # åŸºæœ¬é˜Ÿåˆ—, å…ˆè¿›å…ˆå‡º base_queue.put(x) base_queue.get() lifo_queue = Queue.LifoQueue() # å…ˆè¿›åå‡º, ç±»ä¼¼æ ˆ lifo_queue.put(x) lifo_queue.get() prio_queue = Queue.PriorityQueue() # ä¼˜å…ˆé˜Ÿåˆ—, ä¸C++ä¸­priority_queueç±»ä¼¼, å¯å®ç°å †çš„åŠŸèƒ½ prio_queue.put(x) prio_queue.get() â€”|â€” numpy ä¸­vstack, hstack, concatenate å’Œ stack ä¹‹é—´çš„åŒºåˆ«å’Œè”ç³»concatenate1 | numpy.concatenate((a1, a2, ...), axis=0, out=None) â€”|â€” concatenate çš„ä½œç”¨å°±æ˜¯å°†å¤šä¸ªæ•°ç»„åºåˆ—æŒ‰ç…§axisæŒ‡å®šçš„ç»´åº¦è¿æ¥èµ·æ¥, è¿™äº›æ•°ç»„åºåˆ— a1, a2, â€¦ å¿…é¡»ä¿è¯ é™¤äº† axis æŒ‡å®šç»´åº¦ä¹‹å¤–çš„å…¶ä»–ç»´åº¦å…·æœ‰ç›¸åŒçš„ shape. æ³¨æ„: è¿™é‡Œçš„ç»´åº¦æŒ‡çš„æ˜¯a1, a2çš„ç»´åº¦, è€Œä¸æ˜¯(a1, a2)çš„ç»´åº¦ ä»ç»´åº¦è§’åº¦æ¥æ›´å¥½ç†è§£ concatenate çš„ä½œç”¨concatenate æ‰§è¡Œåçš„ shape ç‰¹å®šæ˜¯: axis æŒ‡å®šçš„ç»´åº¦æ˜¯å¤šä¸ªæ•°ç»„åºåˆ—å¯¹åº”ç»´åº¦çš„æ•°å€¼å’Œ, è€Œå…¶ä»–ç»´åº¦ä¿æŒä¸å˜. ä¹Ÿå°±æ˜¯è¯´ä¸ä¼šå¢åŠ æ–°çš„ç»´åº¦, è¿™æ˜¯ concatenate ä¸ stack ä¹‹é—´çš„ä¸€ä¸ªé‡è¦çš„åŒºåˆ«. å¦‚ä¸‹æ‰€ç¤º: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 | import numpy as np a1 = np.array([[1, 1], [2, 2], [3, 3]]) # shape = 3x2 a2 = np.array([[1, 1], [2, 2]]) # shape = 2 x 2 print(a1.shape, a2.shape) concat1 = np.concatenate((a1, a2), axis=0) print(concat1.shape) # shape ä¸º [5, 2], åœ¨ 0 ç»´åº¦ä¸Šä¸º 3+2, å…¶ä»–ç»´åº¦ä¿æŒä¸å˜ print(concat1) # a1, a2 ç»´åº¦ 0 ä¸åŒ, ä¸€ä¸ªä¸º 3, ä¸€ä¸ªä¸º 2, å…¶ä»–ç»´åº¦ç›¸åŒ, å‡ä¸º 2 #[[1 1] # [2 2] # [3 3] # [1 1] # [2 2]] #print(np.concatenate((a1, a2), axis=1)) # ç”±äºç»´åº¦ 0 äºŒè€…ä¸åŒ, æ— æ³•ä¿æŒä¸å˜, å› æ­¤æŠ¥é”™ a1 = np.array([[1, 2, 3]]) # shape = 1x3 a2 = np.array([[1, 2]]) # shape = 1x2 print(a1.shape, a2.shape) concat2 = np.concatenate((a1, a2), axis=1) print(concat2.shape) # shape ä¸º [1, 5]åœ¨ 1 ç»´åº¦ä¸Šä¸º 3 + 2, 0 ç»´åº¦ä¸Šä¿æŒ 1 ä¸å˜ print(concat2) # [[1 2 3 1 2]] # print(np.concatenate((a1, a2), axis=0)) # ç»´åº¦ 1 ä¸åŒ, æŠ¥é”™ â€”|â€” æœ‰æ—¶å€™, concatenateçš„ç¬¬ä¸€ä¸ªå‚æ•°åªä¼šä¼ é€ä¸€ä¸ªä¸€ä¸ªæ•°ç»„åºåˆ—, è¿™æ—¶å€™, ç­‰ä»·äºå°†è¿™ä¸ªæ•°ç»„åºåˆ—çš„ç¬¬ä¸€ç»´çš„å…ƒç´ çœ‹åšæ˜¯å¤šä¸ªæ•°ç»„åºåˆ—ä½œä¸ºconcatenateçš„å‚æ•°è¿›è¡Œä¼ é€’. å¦‚ä¸‹æ‰€ç¤º: 1 2 3 4 5 6 7 8 9 | a = [[1, 2, 3], [1, 2, 3]] print(np.concatenate(a, axis=0)) # è¯¥è¡Œä¸ä¸‹ä¸€è¡Œç­‰ä»· print(np.concatenate((a[0], a[1]), axis=0)) a = [[1, 2, 3], [1, 2]] print(np.concatenate(a, axis=0)) # å¯ä»¥çœ‹å‡º, è™½ç„¶ a çš„ç¬¬ä¸€ç»´åº¦ä¸º 2, ç¬¬äºŒç»´åº¦ä¸º 3 å’Œ 2 # ä½†æ˜¯, æˆ‘ä»¬è¦å°†å…¶æ‹†åˆ†, æ‹†åˆ†å, a[0], a[1] çš„ç¬¬ä¸€ç»´åº¦3å’Œ2, å…¶ä»–ç»´åº¦ç›¸åŒ, å› æ­¤å¯ä»¥åœ¨ç¬¬ä¸€ç»´åº¦ä¸Šè¿›è¡Œè¿æ¥ print(np.concatenate((a[0], a[1]), axis=0)) â€”|â€” stack1 2 | numpy.stack(arrays, axis=0, out=None) numpy.stack((a1, a2, ...), axis=0, out=None) â€”|â€” stack çš„ä½œç”¨å°±æ˜¯å°†å¤šä¸ªæ•°ç»„åºåˆ—æŒ‰ç…§axisæŒ‡å®šçš„ç»´åº¦ å †å  èµ·æ¥, è¿™äº›æ•°ç»„åºåˆ— a1, a2, â€¦ å¿…é¡»ä¿è¯ æ‰€æœ‰ç»´åº¦éƒ½ç›¸åŒ, æ³¨æ„è¿™é‡Œä¸ concatenate çš„åŒºåˆ«. è¦æ›´å¥½çš„ç†è§£stack, å¯ä»¥å€ŸåŠ© ç»´åº¦ çš„æ¦‚å¿µè¿›è¡Œç†è§£, å¯¹äº shape ç›¸åŒçš„ k ä¸ªæ•°ç»„åºåˆ—æ¥è¯´, stack çš„ä½œç”¨ç›¸å½“äºæ–°æ’å…¥ä¸€ä¸ªç»´åº¦, ç»´åº¦çš„å¤§å°ä¸º k, æ’å…¥çš„ä½ç½®ä¸ºaxisæŒ‡å®šçš„ä½ç½®. å¦‚ä¸‹æ‰€ç¤º: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | a1 = [[1, 1], [2, 2], [3, 3]] # shape = 3x2 a2 = [[4, 4], [5, 5], [6, 6]] # shape = 3x2 a3 = [[7, 7], [8, 8], [9, 9]] # shape = 3x2 a4 = [[0, 0], [0, 0], [0, 0]] # shape = 3x2 stack1 = np.stack((a1, a2, a3, a4), axis=0) # æ–°æ’å…¥ç»´åº¦å¤§å°ä¸º 4, ä½ç½®ä¸ºç¬¬ 0 ç»´ print(stack1.shape) # shape ä¸º (4, 3, 2) print('### ', stack1) # å…ˆå°† shape ç”»å¥½, ç„¶åè¿›è¡Œå¡«å……, åœ¨ç¬¬ 0 ç»´ä¸Šè¿›è¡Œå †å , å› æ­¤ stack1[*][*] = a1[0], a1[1], ..., a4[2] stack2 = np.stack((a1, a2, a3, a4), axis=1) # æ–°æ’å…¥ç»´åº¦å¤§å°ä¸º 4, ä½ç½®ä¸ºç¬¬ 1 ç»´ print(stack2.shape) # shape ä¸º (3, 4, 2) print('### ', stack2) # åœ¨ç¬¬ 1 ç»´ä¸Šè¿›è¡Œå †å , å› æ­¤ stack2[*][*] = a1[0], a2[0], a3[0], a1[1], ... stack3 = np.stack((a1, a2, a3, a4), axis=2) # æ–°æ’å…¥ç»´åº¦å¤§å°ä¸º 4, ä½ç½®ä¸ºç¬¬ 2 ç»´ print(stack3.shape) # shape ä¸º (3, 2, 4) print('### ', stack3) # åœ¨ç¬¬ 2 ç»´ä¸Šè¿›è¡Œå †å , å› æ­¤ stack2[*][*] = [1 4 7 0], [1 4 7 0], [2 5 8 0], ... â€”|â€” hstack å’Œ vstackhstack å’Œ vstack è™½ç„¶åå­—ä¸­éƒ½å¸¦æœ‰ stack, ä½†æ˜¯å®é™…ä¸Š, å®ƒä»¬å’Œnp.stackçš„å…³ç³»å¹¶ä¸å¤§, ä¸€ä¸ªæ˜æ˜¾çš„åŒºåˆ«å°±æ˜¯np.stackè¦æ±‚è¿›è¡Œå †å çš„å¤šä¸ªæ•°ç»„åºåˆ—éœ€è¦ä¿è¯ shape å®Œå…¨ç›¸åŒ, å¹¶ä¸”å †å åä¼šæ–°å¢åŠ ä¸€ä¸ªç”±axisæŒ‡å®šçš„ç»´åº¦. å®é™…ä¸Š, hstack å’Œ vstack å¯ä»¥çœ‹åšæ˜¯ç‰¹æ®Šçš„ concatenate, å®ƒä»¬åœ¨æŸäº›æƒ…å†µä¸‹å¯ä»¥ç”¨ concatenate æ¥ä»£æ›¿ æ—¢ç„¶ hstack å’Œ vstack æ˜¯ç‰¹æ®Šçš„ concatenate, ä¹Ÿå°±æ˜¯è¯´, å®ƒä»¬æ‰€æ¥å—çš„å¤šä¸ªæ•°ç»„åºåˆ—åœ¨axisæŒ‡å®šçš„ç»´åº¦ä¸Šå¯ä»¥ä¸åŒ, è€Œåœ¨å…¶ä»–ç»´åº¦ä¸Šå¿…é¡»ç›¸åŒ. vstack: åœ¨å‚ç›´æ–¹å‘ä¸Šå°†å¤šä¸ªæ•°ç»„åºåˆ—è¿›è¡Œå †å , ç›¸å½“äºåœ¨axis=0ç»´åº¦ä¸Šæ‰§è¡Œconcatenatehstack: åœ¨æ°´å¹³æ–¹å‘ä¸Šå°†å¤šä¸ªæ•°ç»„åºåˆ—è¿›è¡Œå †å , ç›¸å½“äºåœ¨axis=1ç»´åº¦ä¸Šæ‰§è¡Œconcatenate 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 | a = [[1, 1], [2, 2], [3, 3]] # shape = 3x2 b = [[4, 4], [5, 5], [6, 6]] # shape = 3x2 c = [[7, 7], [8, 8], [9, 9]] # shape = 3x2 d = [[0, 0], [0, 0], [0, 0]] # shape = 3x2 v = np.vstack((a, b, c, d)) print(v.shape) # (12, 2) print(v) x = np.concatenate((a, b, c, d), axis = 0) # ç­‰ä»·äº vstack print(x.shape) # 12, 2 print(x) h = np.hstack((a, b, c, d)) print(h.shape) # (3, 8) print(h) x = np.concatenate((a, b, c, d), axis = 1) # ç­‰ä»·äº hstack print(x.shape) # 3, 8 print(x) â€”|â€” éœ€è¦ç‰¹åˆ«æ³¨æ„, å½“å¤šä¸ªæ•°ç»„åºåˆ—æ˜¯ä¸€ç»´æ•°ç»„æ—¶, åº”è¯¥å…ˆå°†ä¸€ç»´æ•°ç»„è½¬æ¢æˆäºŒç»´æ•°ç»„, ç„¶åæ‰èƒ½ä¸ç›¸åº”çš„ concatenate è¿›è¡Œç­‰ä»·. è¿™æ˜¯å› ä¸º, åœ¨æ•°ç»„åºåˆ—æ˜¯ä¸€ç»´æ•°ç»„æ—¶, concatenate æ˜¯æ— æ³•ä½¿ç”¨axis=1çš„, å› æ­¤æ­¤æ—¶çš„ hstack ç›¸å½“äºæ˜¯åœ¨axis=0ä¸Šè¿›è¡Œ concatenate, è€Œ vstack åˆ™éœ€è¦å…ˆå°†æ•°ç»„çš„ shape ä» (N,) è½¬æ¢æˆ (1, N) åæ‰ç›¸å½“äºæ˜¯åœ¨axis=1ä¸Šè¿›è¡Œ concatenate 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 | a = np.array([1, 2, 3, 4, 5]) # å½“é¢å¯¹çš„æ˜¯ä¸€ç»´æ•°ç»„æ—¶, b = np.array([6, 7, 8, 9, 10]) h = np.hstack((a, b)) print(h.shape) print(h) con = np.concatenate((a, b), axis=0) # å½“ a, b æ˜¯ä¸€ç»´æ•°ç»„æ—¶, hstack ç›¸å½“äºåœ¨ axis=0 ä¸Šè¿›è¡Œè¿æ¥ print(con.shape) print(con) v = np.vstack((a, b)) print(v.shape) print(v) con = np.concatenate(([a], [b]), axis=0) # å½“ a, b æ˜¯ä¸€ç»´æ•°ç»„æ—¶, vstack ç›¸å½“äºå°† a, b å…ˆè½¬æ¢æˆäºŒç»´ (1, N), ç„¶ååœ¨ axis=0 ä¸Šè¿›è¡Œè¿æ¥ print(con.shape) print(con) â€”|â€” set å»é‡å¯¹äºäºŒç»´åˆ—è¡¨, ç”±äº list çš„å…ƒç´ ä¹Ÿæ˜¯ list, åœ¨å†…å­˜ä¸­å­˜å‚¨çš„æ˜¯é¦–å…ƒç´ åœ°å€, æ— æ³•ç›´æ¥ä½¿ç”¨ set, å› æ­¤éœ€è¦å…ˆå°†å†…éƒ¨çš„å…ƒç´ å…¨éƒ¨å…¨æ¢æˆ tuple å, æ‰èƒ½ä½¿ç”¨ list å»é‡. å¦‚ä¸‹æ‰€ç¤º 1 2 3 4 5 6 7 8 | a = list() a.append([1,2,3]) a.append([1,2,3]) a.append([1,2,3]) a.append([4, 5, 6]) # b = set(a) # æŠ¥é”™ b = set(map(tuple, a)) print(b) # {(4, 5, 6), (1, 2, 3)} â€”|â€” os.sepç”¨æ³•ythonæ˜¯è·¨å¹³å°çš„ã€‚åœ¨Windowsä¸Šï¼Œæ–‡ä»¶çš„è·¯å¾„åˆ†éš”ç¬¦æ˜¯â€™\\â€™ï¼Œåœ¨Linuxä¸Šæ˜¯â€™/â€˜ã€‚ ä¸ºäº†è®©ä»£ç åœ¨ä¸åŒçš„å¹³å°ä¸Šéƒ½èƒ½è¿è¡Œï¼Œé‚£ä¹ˆè·¯å¾„åº”è¯¥å†™â€™\\â€™è¿˜æ˜¯â€™/â€˜å‘¢ï¼Ÿ ä½¿ç”¨os.sepçš„è¯ï¼Œå°±ä¸ç”¨è€ƒè™‘è¿™ä¸ªäº†ï¼Œos.sepæ ¹æ®ä½ æ‰€å¤„çš„å¹³å°ï¼Œè‡ªåŠ¨é‡‡ç”¨ç›¸åº”çš„åˆ†éš”ç¬¦å·ã€‚ ä¸¾ä¾‹ Linuxä¸‹ä¸€ä¸ªè·¯å¾„ï¼Œ/usr/share/python,é‚£ä¹ˆä¸Šé¢çš„os.sepå°±æ˜¯â€˜/â€™windowsä¸‹ä¸€ä¸ªè·¯å¾„ï¼ŒCï¼š\\Users\\Public\\Desktop,é‚£ä¹ˆä¸Šé¢çš„os.sepå°±æ˜¯â€˜\\â€™. 1 | data_dir = os.sep.join(['hello', 'world']) â€”|â€” Python3 å…ƒç»„Pythonå…ƒç»„åŒ…å«äº†ä»¥ä¸‹å†…ç½®å‡½æ•° len(tuple) è®¡ç®—å…ƒç»„å…ƒç´ ä¸ªæ•°ã€‚ 1 2 3 4 | tuple1 = ('Google', 'Runoob', 'Taobao') len(tuple1) 3 â€”|â€” max(tuple) è¿”å›å…ƒç»„ä¸­å…ƒç´ æœ€å¤§å€¼ã€‚ 1 2 3 4 | tuple2 = ('5', '4', '8') max(tuple2) '8' â€”|â€” min(tuple) è¿”å›å…ƒç»„ä¸­å…ƒç´ æœ€å°å€¼ã€‚ 1 2 3 4 | tuple2 = ('5', '4', '8') min(tuple2) '4' â€”|â€” tuple(seq) å°†åˆ—è¡¨è½¬æ¢ä¸ºå…ƒç»„ã€‚ 1 2 3 4 | list1= ['Google', 'Taobao', 'Runoob', 'Baidu'] tuple1=tuple(list1) tuple1 ('Google', 'Taobao', 'Runoob', 'Baidu') â€”|â€” åºåˆ—åŒ–Pythonå¯¹è±¡ä½ éœ€è¦å°†ä¸€ä¸ªPythonå¯¹è±¡åºåˆ—åŒ–ä¸ºä¸€ä¸ªå­—èŠ‚æµï¼Œä»¥ä¾¿å°†å®ƒä¿å­˜åˆ°ä¸€ä¸ªæ–‡ä»¶ã€å­˜å‚¨åˆ°æ•°æ®åº“æˆ–è€…é€šè¿‡ç½‘ç»œä¼ è¾“å®ƒã€‚å¯¹äºåºåˆ—åŒ–æœ€æ™®éçš„åšæ³•å°±æ˜¯ä½¿ç”¨ pickle æ¨¡å—ã€‚ä¸ºäº†å°†ä¸€ä¸ªå¯¹è±¡ä¿å­˜åˆ°ä¸€ä¸ªæ–‡ä»¶ä¸­ï¼Œå¯ä»¥è¿™æ ·åš pickle å¯¹äºå¤§å‹çš„æ•°æ®ç»“æ„æ¯”å¦‚ä½¿ç”¨ array æˆ– numpy æ¨¡å—åˆ›å»ºçš„äºŒè¿›åˆ¶æ•°ç»„æ•ˆç‡å¹¶ä¸æ˜¯ä¸€ä¸ªé«˜æ•ˆçš„ç¼–ç æ–¹å¼ã€‚ å¦‚æœä½ éœ€è¦ç§»åŠ¨å¤§é‡çš„æ•°ç»„æ•°æ®ï¼Œä½ æœ€å¥½æ˜¯å…ˆåœ¨ä¸€ä¸ªæ–‡ä»¶ä¸­å°†å…¶ä¿å­˜ä¸ºæ•°ç»„æ•°æ®å—æˆ–ä½¿ç”¨æ›´é«˜çº§çš„æ ‡å‡†ç¼–ç æ–¹å¼å¦‚HDF5 (éœ€è¦ç¬¬ä¸‰æ–¹åº“çš„æ”¯æŒ)ã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 | In [1]: import pickle In [2]: obj = 123,\"abcdef\", [\"ac\", 123], {\"key\": \"value\", \"key1\": \"value1\"} In [3]: print(obj) (123, 'abcdef', ['ac', 123], {'key': 'value', 'key1': 'value1'}) In [4]: # åºåˆ—åŒ–åˆ°æ–‡ä»¶ In [5]: with open(r'./a.pickle','wb') as f: ...: pickle.dump(obj,f) ...: In [6]: with open(r'./a.pickle','rb') as f: ...: aa= pickle.load(f) ...: print(aa) ...: ...: (123, 'abcdef', ['ac', 123], {'key': 'value', 'key1': 'value1'}) â€”|â€” å‚è€ƒé“¾æ¥ï¼šhttps://hellozhaozheng.github.iohttps://www.runoob.com/python/python-tutorial.html","tags":["Python"],"categories":["ç®—æ³•"]},{"title":"shutilæ¨¡å—","path":"/2019/08/28/shutilæ¨¡å—/","content":"1 2 3 4 5 6 7 | shutil.copyfile(\"old\",\"new\") # å¤åˆ¶æ–‡ä»¶ï¼Œéƒ½åªèƒ½æ˜¯æ–‡ä»¶ shutil.copytree(\"old\",\"new\") # å¤åˆ¶æ–‡ä»¶å¤¹ï¼Œéƒ½åªèƒ½æ˜¯ç›®å½•ï¼Œä¸”newå¿…é¡»ä¸å­˜åœ¨ shutil.copy(\"old\",\"new\") # å¤åˆ¶æ–‡ä»¶/æ–‡ä»¶å¤¹ï¼Œå¤åˆ¶ old ä¸º newï¼ˆnewæ˜¯æ–‡ä»¶ï¼Œè‹¥ä¸å­˜åœ¨ï¼Œå³æ–°å»ºï¼‰ï¼Œå¤åˆ¶ old ä¸ºè‡³ new æ–‡ä»¶å¤¹ï¼ˆæ–‡ä»¶å¤¹å·²å­˜åœ¨ï¼‰ shutil.move(\"old\",\"new\") # ç§»åŠ¨æ–‡ä»¶/æ–‡ä»¶å¤¹è‡³ new æ–‡ä»¶å¤¹ä¸­ â€”|â€”","tags":["ç¼–ç¨‹"],"categories":["Python"]},{"title":"skimageæ¨¡å—","path":"/2019/08/28/skimageæ¨¡å—/","content":"æ¯”opencvçš„é€Ÿåº¦è¦æ…¢å¾ˆå¤š, ä½†æ˜¯ä½¿ç”¨èµ·æ¥æ›´åŠ ç®€å•, çœŸçš„å¯¹é€Ÿåº¦è¦æ±‚å¾ˆé«˜çš„è¯, ä¸€èˆ¬éƒ½ä¼šC++å’Œopecvä½¿ç”¨. æ‰€ä»¥ä¸€èˆ¬æƒ…å†µä¸‹, é¦–å…ˆçœ‹skimageèƒ½å¦å®ç°, ä¸è¡Œçš„è¯å†è½¬ç”¨opencv 1 2 3 | import skimage from skimage import io # IO is a submodule. Submodules need to be imported from the parent module explicitly. img = io.imread(\"1.jpg\") â€”|â€”","tags":["OpenCV"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"ä»ä¸Šå¾€ä¸‹æ‰“å°äºŒå‰æ ‘","path":"/2019/08/28/ä»ä¸Šå¾€ä¸‹æ‰“å°äºŒå‰æ ‘/","content":"æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š420679æœ¬é¢˜çŸ¥è¯†ç‚¹ï¼š é˜Ÿåˆ— æ ‘ é¢˜ç›®æè¿°ä»ä¸Šå¾€ä¸‹æ‰“å°å‡ºäºŒå‰æ ‘çš„æ¯ä¸ªèŠ‚ç‚¹ï¼ŒåŒå±‚èŠ‚ç‚¹ä»å·¦è‡³å³æ‰“å°ã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 | /* struct TreeNode { int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) { } };*/ class Solution { public: vectorint PrintFromTopToBottom(TreeNode* root) { //é˜Ÿåˆ—æ˜¯å…ˆè¿›å…ˆå‡º queueTreeNode* que; vectorint vec; que.push(root);//å…ˆå°†æ•´ä¸ªäºŒå‰æ ‘æ”¾å…¥é˜Ÿåˆ— while(!que.empty()) //å½“é˜Ÿåˆ—éç©ºè¿›è¡Œå¾ªç¯ { TreeNode* p; p = que.front();//å…ˆè¯»å–é˜Ÿåˆ—çš„é¦–å…ƒç´  que.pop();//å¼¹å‡ºé˜Ÿåˆ—çš„é¦–å…ƒç´  if(p == NULL) continue;//æ‰€æœ‰å…ƒç´ å­˜å…¥vecåï¼Œç”±äºé˜Ÿåˆ—ä¸­å­˜æ”¾ç€ç©ºæŒ‡é’ˆï¼Œä¾ç„¶è¿›å…¥å¾ªç¯ï¼Œä½†æ­¤æ—¶pçš„å€¼ä¸ºNULLï¼Œä¸æ‰§è¡Œä¸‹é¢çš„æ“ä½œï¼Œè·³å‡ºå¾ªç¯ç»“æŸ que.push(p-left); que.push(p-right); vec.push_back(p-val); } return vec; } }; â€”|â€” è¿è¡Œæ—¶é—´ï¼š3mså ç”¨å†…å­˜ï¼š464k","tags":["ç¼–ç¨‹"],"categories":["ç®—æ³•"]},{"title":"äºŒç»´æ•°ç»„ä¸­çš„æŸ¥æ‰¾","path":"/2019/08/27/20190827-äºŒç»´æ•°ç»„ä¸­çš„æŸ¥æ‰¾/","content":"é¢˜ç›®æè¿°åœ¨ä¸€ä¸ªäºŒç»´æ•°ç»„ä¸­ï¼ˆæ¯ä¸ªä¸€ç»´æ•°ç»„çš„é•¿åº¦ç›¸åŒï¼‰ï¼Œæ¯ä¸€è¡Œéƒ½æŒ‰ç…§ä»å·¦åˆ°å³é€’å¢çš„é¡ºåºæ’åºï¼Œæ¯ä¸€åˆ—éƒ½æŒ‰ç…§ä»ä¸Šåˆ°ä¸‹é€’å¢çš„é¡ºåºæ’åºã€‚è¯·å®Œæˆä¸€ä¸ªå‡½æ•°ï¼Œè¾“å…¥è¿™æ ·çš„ä¸€ä¸ªäºŒç»´æ•°ç»„å’Œä¸€ä¸ªæ•´æ•°ï¼Œåˆ¤æ–­æ•°ç»„ä¸­æ˜¯å¦å«æœ‰è¯¥æ•´æ•°ã€‚ æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š1323359 é¢˜ç›®è§£æå·²çŸ¥ä»å·¦åˆ°å³ã€ä»ä¸Šå¯¼çº¿éƒ½å‘ˆé€’å¢å…³ç³»ï¼Œä»¤è¡Œrowä¸º0ï¼Œä»¤åˆ—colä¸ºç¬¬ä¸€è¡Œæœ€åä¸€ä½ï¼Œç„¶åå°†targetä¸ç¬¬ä¸€è¡Œæœ€åä¸€ä¸ªæ•°è¿›è¡Œæ¯”è¾ƒï¼Œè‹¥å¤§äºè¿™ä¸ªæ•°åˆ™ï¼Œè¡Œæ•°++ï¼Œè‹¥å°äºè¿™ä¸ªæ•°ï¼Œåˆ™åˆ—æ•°å‘å‰é€’å‡ã€‚é¢˜ç›®çš„è¦æ±‚æ˜¯ å½“ç›®æ ‡æ•°targetæ—¢ä¸å¤§äºä¹Ÿä¸å°äºarray[row][col]ï¼Œè®¤ä¸ºtarget==array[row][col]ï¼Œè¿”å›trueã€‚å…¶ä»–çš„æƒ…å†µåˆ™è¿”å›falseã€‚æ³¨ï¼š åœ¨ c++ä¸­è·å–vectoræ•°ç»„çš„è¡Œæ•°å’Œåˆ—æ•°çš„ä»£ç ä¸javaä¸ä¸€æ · c++:int col = array[0].size()-1;int row = array.size();java:int col = array[0].length - 1;int array_len = array.length; c++ä»£ç  å¦‚ä¸‹ï¼š class Solution {public: //è¡Œæ•°ï¼š //example.length // åˆ—æ•°ï¼š //example[0].length //ç¬¬0è¡Œçš„åˆ—æ•° bool Find(int target, vectorvectorint array) { if(array.size()==0) { return false; } int row = 0; int col = array[0].size()-1; int array_len = array.size(); while(row array_len col=0 ) { if(target array[row][col]) row++; else if(target array[row][col]) col--; else return true; } return false; }}; è¿è¡Œæ—¶é—´ï¼š11mså ç”¨å†…å­˜ï¼š1500k","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"äºŒç»´æ•°ç»„ä¸­çš„æŸ¥æ‰¾","path":"/2019/08/27/äºŒç»´æ•°ç»„ä¸­çš„æŸ¥æ‰¾/","content":"é¢˜ç›®æè¿°åœ¨ä¸€ä¸ªäºŒç»´æ•°ç»„ä¸­ï¼ˆæ¯ä¸ªä¸€ç»´æ•°ç»„çš„é•¿åº¦ç›¸åŒï¼‰ï¼Œæ¯ä¸€è¡Œéƒ½æŒ‰ç…§ä»å·¦åˆ°å³é€’å¢çš„é¡ºåºæ’åºï¼Œæ¯ä¸€åˆ—éƒ½æŒ‰ç…§ä»ä¸Šåˆ°ä¸‹é€’å¢çš„é¡ºåºæ’åºã€‚è¯·å®Œæˆä¸€ä¸ªå‡½æ•°ï¼Œè¾“å…¥è¿™æ ·çš„ä¸€ä¸ªäºŒç»´æ•°ç»„å’Œä¸€ä¸ªæ•´æ•°ï¼Œåˆ¤æ–­æ•°ç»„ä¸­æ˜¯å¦å«æœ‰è¯¥æ•´æ•°ã€‚ æ—¶é—´é™åˆ¶ï¼š1ç§’ ç©ºé—´é™åˆ¶ï¼š32768K çƒ­åº¦æŒ‡æ•°ï¼š1323359 é¢˜ç›®è§£æå·²çŸ¥ä»å·¦åˆ°å³ã€ä»ä¸Šå¯¼çº¿éƒ½å‘ˆé€’å¢å…³ç³»ï¼Œä»¤è¡Œrowä¸º0ï¼Œä»¤åˆ—colä¸ºç¬¬ä¸€è¡Œæœ€åä¸€ä½ï¼Œç„¶åå°†targetä¸ç¬¬ä¸€è¡Œæœ€åä¸€ä¸ªæ•°è¿›è¡Œæ¯”è¾ƒï¼Œè‹¥å¤§äºè¿™ä¸ªæ•°åˆ™ï¼Œè¡Œæ•°++ï¼Œè‹¥å°äºè¿™ä¸ªæ•°ï¼Œåˆ™åˆ—æ•°å‘å‰é€’å‡ã€‚é¢˜ç›®çš„è¦æ±‚æ˜¯ å½“ç›®æ ‡æ•°targetæ—¢ä¸å¤§äºä¹Ÿä¸å°äºarray[row][col]ï¼Œè®¤ä¸ºtarget==array[row][col]ï¼Œè¿”å›trueã€‚å…¶ä»–çš„æƒ…å†µåˆ™è¿”å›falseã€‚æ³¨ï¼š åœ¨ c++ä¸­è·å–vectoræ•°ç»„çš„è¡Œæ•°å’Œåˆ—æ•°çš„ä»£ç ä¸javaä¸ä¸€æ · 1 2 3 4 5 6 | c++: int col = array[0].size()-1; int row = array.size(); java: int col = array[0].length - 1; int array_len = array.length; â€”|â€” c++ä»£ç  å¦‚ä¸‹ï¼š 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 | class Solution { public: //è¡Œæ•°ï¼š //example.length // åˆ—æ•°ï¼š //example[0].length //ç¬¬0è¡Œçš„åˆ—æ•° bool Find(int target, vectorvectorint array) { if(array.size()==0) { return false; } int row = 0; int col = array[0].size()-1; int array_len = array.size(); while(row array_len col=0 ) { if(target array[row][col]) row++; else if(target array[row][col]) col--; else return true; } return false; } }; â€”|â€” è¿è¡Œæ—¶é—´ï¼š11mså ç”¨å†…å­˜ï¼š1500k","tags":["C++"],"categories":["ç®—æ³•"]},{"title":"ä»¿å°„å˜æ¢","path":"/2019/08/26/20190826-ä»¿å°„å˜æ¢/","content":"æ¬§å¼å˜æ¢åŒ…æ‹¬å¹³ç§»å’Œæ—‹è½¬å˜æ¢ ä»€ä¹ˆæ˜¯ä»¿å°„å˜æ¢ï¼Ÿ ç®€å•çš„æ¥è¯´â€œä»¿å°„å˜æ¢â€ å°±æ˜¯ï¼šâ€œçº¿æ€§å˜æ¢â€+å¹³ç§»å°ºåº¦å˜æ¢åŒ…æ‹¬ç›¸ä¼¼å˜æ¢ï¼Œå½“xä¸yå˜æ¢çš„å°ºåº¦ç›¸ç­‰çš„æ—¶å€™ï¼Œå«åšç›¸ä¼¼å˜æ¢ä»€ä¹ˆæ˜¯çº¿æ€§å˜æ¢ï¼Ÿ 1 çº¿æ€§å˜æ¢ çº¿æ€§å˜æ¢ä»å‡ ä½•ç›´è§‚æœ‰ä¸‰ä¸ª è¦ç‚¹ï¼š å˜æ¢å‰æ˜¯ç›´çº¿çš„ï¼Œå˜æ¢åä¾æ—§æ˜¯ç›´çº¿ ç›´çº¿æ¯”ä¾‹ä¿æŒä¸å˜ å˜æ¢å‰æ˜¯åŸç‚¹ï¼Œå˜æ¢åä¾ç„¶æ˜¯åŸç‚¹ 2 ä»¿å°„å˜æ¢ä»¿å°„å˜æ¢ä»å‡ ä½•ç›´è§‚åªæœ‰ä¸¤ä¸ªè¦ç‚¹ï¼š å˜æ¢å‰æ˜¯ç›´çº¿çš„ï¼Œå˜æ¢åä¾ç„¶æ˜¯ç›´çº¿ ç›´çº¿æ¯”ä¾‹ä¿æŒä¸å˜ ä¸çº¿æ€§å˜æ¢ç›¸æ¯”å°‘äº†åŸç‚¹ä¿æŒä¸å˜è¿™ä¸€æ¡","tags":["è®¡ç®—æœºè§†è§‰"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"ä»¿å°„å˜æ¢","path":"/2019/08/26/ä»¿å°„å˜æ¢/","content":"æ¬§å¼å˜æ¢åŒ…æ‹¬å¹³ç§»å’Œæ—‹è½¬å˜æ¢ ä»€ä¹ˆæ˜¯ä»¿å°„å˜æ¢ï¼Ÿ ç®€å•çš„æ¥è¯´â€œä»¿å°„å˜æ¢â€ å°±æ˜¯ï¼šâ€œçº¿æ€§å˜æ¢â€+å¹³ç§»å°ºåº¦å˜æ¢åŒ…æ‹¬ç›¸ä¼¼å˜æ¢ï¼Œå½“xä¸yå˜æ¢çš„å°ºåº¦ç›¸ç­‰çš„æ—¶å€™ï¼Œå«åšç›¸ä¼¼å˜æ¢ä»€ä¹ˆæ˜¯çº¿æ€§å˜æ¢ï¼Ÿ 1 çº¿æ€§å˜æ¢ çº¿æ€§å˜æ¢ä»å‡ ä½•ç›´è§‚æœ‰ä¸‰ä¸ª è¦ç‚¹ï¼š å˜æ¢å‰æ˜¯ç›´çº¿çš„ï¼Œå˜æ¢åä¾æ—§æ˜¯ç›´çº¿ ç›´çº¿æ¯”ä¾‹ä¿æŒä¸å˜ å˜æ¢å‰æ˜¯åŸç‚¹ï¼Œå˜æ¢åä¾ç„¶æ˜¯åŸç‚¹ 2 ä»¿å°„å˜æ¢ä»¿å°„å˜æ¢ä»å‡ ä½•ç›´è§‚åªæœ‰ä¸¤ä¸ªè¦ç‚¹ï¼š å˜æ¢å‰æ˜¯ç›´çº¿çš„ï¼Œå˜æ¢åä¾ç„¶æ˜¯ç›´çº¿ ç›´çº¿æ¯”ä¾‹ä¿æŒä¸å˜ ä¸çº¿æ€§å˜æ¢ç›¸æ¯”å°‘äº†åŸç‚¹ä¿æŒä¸å˜è¿™ä¸€æ¡","tags":["ç¼–ç¨‹"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"Matplotlibç»˜åˆ¶å…­ç§å¯è§†åŒ–å›¾è¡¨","path":"/2019/08/24/20190824-pythonplot/","content":"01 æŠ˜çº¿å›¾import numpy as npimport matplotlib.pyplot as pltx = np.linspace(0,2,100)plt.plot(x, x, label='linear')plt.plot(x, x**2, label='quadratic')plt.plot(x, x**3, label='cubic')plt.xlabel('x label')plt.ylabel('y label')plt.title(\"Simple Plot\")plt.legend()plt.show() 02 æ•£ç‚¹å›¾import numpy as npimport matplotlib.pyplot as pltx = np.arange(0., 5., 0.2)# çº¢è‰²ç ´æŠ˜å·, è“è‰²æ–¹å— ï¼Œç»¿è‰²ä¸‰è§’å—plt.plot(x, x, 'r--', x, x**2, 'bs', x, x**3, 'g^')plt.show() 03 ç›´æ–¹å›¾import numpy as npimport matplotlib.pyplot as pltnp.random.seed(19680801)mu1, sigma1 = 100, 15mu2, sigma2 = 80, 15x1 = mu1 + sigma1 * np.random.randn(10000)x2 = mu2 + sigma2 * np.random.randn(10000)# the histogram of the data# 50ï¼šå°†æ•°æ®åˆ†æˆ50ç»„# facecolorï¼šé¢œè‰²ï¼›alphaï¼šé€æ˜åº¦# densityï¼šæ˜¯å¯†åº¦è€Œä¸æ˜¯å…·ä½“æ•°å€¼n1, bins1, patches1 = plt.hist(x1, 50, density=True, facecolor='g', alpha=1)n2, bins2, patches2 = plt.hist(x2, 50, density=True, facecolor='r', alpha=0.2)# nï¼šæ¦‚ç‡å€¼ï¼›binsï¼šå…·ä½“æ•°å€¼ï¼›patchesï¼šç›´æ–¹å›¾å¯¹è±¡ã€‚plt.xlabel('Smarts')plt.ylabel('Probability')plt.title('Histogram of IQ')plt.text(110, .025, r'$\\mu=100,\\ \\sigma=15$')plt.text(50, .025, r'$\\mu=80,\\ \\sigma=15$')# è®¾ç½®xï¼Œyè½´çš„å…·ä½“èŒƒå›´plt.axis([40, 160, 0, 0.03])plt.grid(True)plt.show() 04 æŸ±çŠ¶å›¾4.1 å¹¶åˆ—æŸ±çŠ¶å›¾import numpy as npimport matplotlib.pyplot as pltsize = 5a = np.random.random(size)b = np.random.random(size)c = np.random.random(size)x = np.arange(size)# æœ‰å¤šå°‘ä¸ªç±»å‹ï¼Œåªéœ€æ›´æ”¹nå³å¯total_width, n = 0.8, 3width = total_width / n# é‡æ–°æ‹Ÿå®šxçš„åæ ‡x = x - (total_width - width) / 2# è¿™é‡Œä½¿ç”¨çš„æ˜¯åç§»plt.bar(x, a, width=width, label='a')plt.bar(x + width, b, width=width, label='b')plt.bar(x + 2 * width, c, width=width, label='c')plt.legend()plt.show() 4.2 å åŠ æŸ±çŠ¶å›¾import numpy as npimport matplotlib.pyplot as pltsize = 5a = np.random.random(size)b = np.random.random(size)c = np.random.random(size)x = np.arange(size)# è¿™é‡Œä½¿ç”¨çš„æ˜¯åç§»plt.bar(x, a, width=0.5, label='a',fc='r')plt.bar(x, b, bottom=a, width=0.5, label='b', fc='g')plt.bar(x, c, bottom=a+b, width=0.5, label='c', fc='b')plt.ylim(0, 2.5)plt.legend()plt.grid(True)plt.show() 05 é¥¼å›¾5.1 æ™®é€šé¥¼å›¾import matplotlib.pyplot as pltlabels = 'Frogs', 'Hogs', 'Dogs', 'Logs'sizes = [15, 30, 45, 10]# è®¾ç½®åˆ†ç¦»çš„è·ç¦»ï¼Œ0è¡¨ç¤ºä¸åˆ†ç¦»explode = (0, 0.1, 0, 0)plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)# Equal aspect ratio ä¿è¯ç”»å‡ºçš„å›¾æ˜¯æ­£åœ†å½¢plt.axis('equal')plt.show() 5.2 åµŒå¥—é¥¼å›¾import numpy as npimport matplotlib.pyplot as plt# è®¾ç½®æ¯ç¯çš„å®½åº¦size = 0.3vals = np.array([[60., 32.], [37., 40.], [29., 10.]])# é€šè¿‡get_cmapéšæœºè·å–é¢œè‰²cmap = plt.get_cmap(\"tab20c\")outer_colors = cmap(np.arange(3)*4)inner_colors = cmap(np.array([1, 2, 5, 6, 9, 10]))print(vals.sum(axis=1))# [92. 77. 39.]plt.pie(vals.sum(axis=1), radius=1, colors=outer_colors, wedgeprops=dict(width=size, edgecolor='w'))print(vals.flatten())# [60. 32. 37. 40. 29. 10.]plt.pie(vals.flatten(), radius=1-size, colors=inner_colors, wedgeprops=dict(width=size, edgecolor='w'))# equal ä½¿å¾—ä¸ºæ­£åœ†plt.axis('equal')plt.show() 5.3 æè½´é¥¼å›¾import numpy as npimport matplotlib.pyplot as pltnp.random.seed(19680801)N = 10theta = np.linspace(0.0, 2 * np.pi, N, endpoint=False)radii = 10 * np.random.rand(N)width = np.pi / 4 * np.random.rand(N)ax = plt.subplot(111, projection='polar')bars = ax.bar(theta, radii, width=width, bottom=0.0)# leftè¡¨ç¤ºä»å“ªå¼€å§‹ï¼Œ# radiiè¡¨ç¤ºä»ä¸­å¿ƒç‚¹å‘è¾¹ç¼˜ç»˜åˆ¶çš„é•¿åº¦ï¼ˆåŠå¾„ï¼‰# widthè¡¨ç¤ºæœ«ç«¯çš„å¼§é•¿# è‡ªå®šä¹‰é¢œè‰²å’Œä¸é€æ˜åº¦for r, bar in zip(radii, bars): bar.set_facecolor(plt.cm.viridis(r / 10.)) bar.set_alpha(0.5)plt.show() 06 ä¸‰ç»´å›¾6.1 ç»˜åˆ¶ä¸‰ç»´æ•£ç‚¹å›¾import numpy as npimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Ddata = np.random.randint(0, 255, size=[40, 40, 40])x, y, z = data[0], data[1], data[2]ax = plt.subplot(111, projection='3d') # åˆ›å»ºä¸€ä¸ªä¸‰ç»´çš„ç»˜å›¾å·¥ç¨‹# å°†æ•°æ®ç‚¹åˆ†æˆä¸‰éƒ¨åˆ†ç”»ï¼Œåœ¨é¢œè‰²ä¸Šæœ‰åŒºåˆ†åº¦ax.scatter(x[:10], y[:10], z[:10], c='y') # ç»˜åˆ¶æ•°æ®ç‚¹ax.scatter(x[10:20], y[10:20], z[10:20], c='r')ax.scatter(x[30:40], y[30:40], z[30:40], c='g')ax.set_zlabel('Z') # åæ ‡è½´ax.set_ylabel('Y')ax.set_xlabel('X')plt.show() 6.2 ç»˜åˆ¶ä¸‰ç»´å¹³é¢å›¾from matplotlib import pyplot as pltimport numpy as npfrom mpl_toolkits.mplot3d import Axes3Dfig = plt.figure()ax = Axes3D(fig)X = np.arange(-4, 4, 0.25)Y = np.arange(-4, 4, 0.25)X, Y = np.meshgrid(X, Y)R = np.sqrt(X**2 + Y**2)Z = np.sin(R)# å…·ä½“å‡½æ•°æ–¹æ³•å¯ç”¨ help(function) æŸ¥çœ‹ï¼Œå¦‚ï¼šhelp(ax.plot_surface)ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='rainbow')plt.show() å‚è€ƒé“¾æ¥ï¼š https://mp.weixin.qq.com/s/bMvrle-FRvli0pRNi83waQ","tags":["Python"],"categories":["Python"]},{"title":"SSDæºç è§£æ","path":"/2019/08/24/20190824-ssd/","content":"è®ºæ–‡åœ°å€: SSD: Single Shot MultiBox Detector éå®˜æ–¹ä»£ç : pytorch ä»‹ç»SSDï¼Œå…¨ç§°Single Shot MultiBox Detectorï¼Œæ˜¯ä¸€ç§One-Stageçš„æ–¹æ³•ï¼Œå®ƒç”±Wei Liuåœ¨ECCV 2016ä¸Šæå‡ºï¼ŒSSDå…·æœ‰å¦‚ä¸‹ä¸»è¦ç‰¹ç‚¹ï¼š ä»YOLOä¸­ç»§æ‰¿äº†å°†detectionè½¬åŒ–ä¸ºregressionçš„æ€è·¯ï¼ŒåŒæ—¶ä¸€æ¬¡å³å¯å®Œæˆç½‘ç»œè®­ç»ƒ åŸºäºFaster RCNNä¸­çš„anchorï¼Œæå‡ºäº†ç›¸ä¼¼çš„prior box åŠ å…¥åŸºäºç‰¹å¾é‡‘å­—å¡”ï¼ˆPyramidal Feature Hierarchyï¼‰çš„æ£€æµ‹æ–¹å¼ï¼Œç›¸å½“äºåŠä¸ªFPNæ€è·¯ SSDçš„ ç½‘ç»œç»“æ„ ç”±ä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼ŒSSDçš„åŸºç¡€ç½‘ç»œç»“æ„ç”±åŸºç¡€ç½‘ç»œVGG16ç»„æˆï¼Œåœ¨VGG16åŸºç¡€ç½‘ç»œä¹‹åæ¥äº†ä¸€ä¸ª3x3çš„å·ç§¯å’Œä¸€ä¸ª1x1çš„å·ç§¯åšç‰¹å¾èåˆï¼Œç„¶åå¢åŠ äº†ä¸€ä¸ªExtra Feature Layers å±‚ï¼Œè¿™ä¸ªå±‚ç”±å…«ä¸ªå·ç§¯å±‚æ„æˆã€‚SSDåœ¨å‰é¢çš„åŸºç¡€ç½‘ç»œå»conv_4_3ä¹‹åçš„reluå±‚è¾“å‡ºï¼Œä»¥åŠå€’æ•°ç¬¬äºŒå±‚çš„conv_7_1çš„reluå†åŠ ä¸ŠExtra Feature Layerså±‚çš„ç¬¬1,3,5,7å±‚ å…±æœ‰6ä¸ª featuremapå±‚ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šå¯¹boxè¿›è¡Œé¢„æµ‹ã€‚ä½†æ„Ÿè§‰è¦æå‡æ•ˆæœçš„è¯å¯ä»¥å¯¹åŸºç¡€ç½‘ç»œè¿›è¡Œæ›´æ”¹ï¼Œå¢åŠ ç‰¹å¾èåˆç­‰ç­‰ã€‚ ç©ºæ´å·ç§¯(Dilation Conv)ssdç½‘ç»œé‡Œè¿˜ä½¿ç”¨äº†ç©ºæ´å·ç§¯(Dilation Conv),é‡‡ç”¨VGG16åšåŸºç¡€æ¨¡å‹ï¼Œé¦–å…ˆVGG16æ˜¯åœ¨ILSVRC CLS-LOCæ•°æ®é›†é¢„è®­ç»ƒã€‚ç„¶åå€Ÿé‰´äº†DeepLab-LargeFOVï¼Œåˆ†åˆ«å°†VGG16çš„å…¨è¿æ¥å±‚fc6å’Œfc7è½¬æ¢æˆ 3Ã—3å·ç§¯å±‚ conv6å’Œ 1Ã—1 å·ç§¯å±‚conv7ï¼ŒåŒæ—¶å°†æ± åŒ–å±‚ pool5 ç”±åŸæ¥çš„ stride=2 çš„ 2Ã—2 å˜æˆ stride=1 çš„(çŒœæƒ³æ˜¯ä¸æƒ³reduceç‰¹å¾å›¾å¤§å°)ï¼Œä¸ºäº†é…åˆè¿™ç§å˜åŒ–ï¼Œé‡‡ç”¨äº†ä¸€ç§ Atrous Algorithmï¼Œå…¶å®å°±æ˜¯conv6é‡‡ç”¨æ‰©å±•å·ç§¯æˆ–å¸¦å­”å·ç§¯ï¼ˆDilation Convï¼‰ï¼Œå…¶åœ¨ä¸å¢åŠ å‚æ•°ä¸æ¨¡å‹å¤æ‚åº¦çš„æ¡ä»¶ä¸‹æŒ‡æ•°çº§æ‰©å¤§å·ç§¯çš„è§†é‡ï¼Œå…¶ä½¿ç”¨æ‰©å¼ ç‡(dilation rate)å‚æ•°ï¼Œæ¥è¡¨ç¤ºæ‰©å¼ çš„å¤§å°ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œ(a)æ˜¯æ™®é€šçš„ 3Ã—3 å·ç§¯ï¼Œå…¶è§†é‡å°±æ˜¯ 3Ã—3 ï¼Œ(b)æ˜¯æ‰©å¼ ç‡ä¸º 1ï¼Œæ­¤æ—¶è§†é‡å˜æˆ 7Ã—7 ï¼Œ(c)æ‰©å¼ ç‡ä¸º3æ—¶ï¼Œè§†é‡æ‰©å¤§ä¸º 15Ã—15 ï¼Œä½†æ˜¯è§†é‡çš„ç‰¹å¾æ›´ç¨€ç–äº†ã€‚Conv6é‡‡ç”¨ 3Ã—3 å¤§å°ä½†dilation rate=6çš„æ‰©å±•å·ç§¯ã€‚ Prior BoxSSDä¸­æœ‰ç€ç±»ä¼¼anchoræœºåˆ¶çš„Prior Boxæœºåˆ¶ï¼Œç”¨äºæ¥ç”Ÿæˆå…ˆéªŒæ¡†ï¼Œåé¢å°†è¿™äº›å…ˆéªŒæ¡†ä¸çœŸå®çš„gtè¿›è¡ŒåŒ¹é…ï¼Œç„¶åä¸é¢„æµ‹çš„è¿›è¡Œå›å½’ã€‚ä»è€Œå¾—åˆ°ç‰©ä½“çœŸå®çš„ ä½ç½®ã€‚SSDçš„prior Box æŒ‰ç…§å¦‚ä¸‹è§„åˆ™ç”Ÿæˆï¼š ä»¥feature mapä¸Šæ¯ä¸ªç‚¹çš„ä¸­ç‚¹ä¸ºä¸­å¿ƒï¼ˆoffset=0.5ï¼‰ï¼Œç”Ÿæˆä¸€äº›åˆ—åŒå¿ƒçš„prior boxï¼ˆç„¶åä¸­å¿ƒç‚¹çš„åæ ‡ä¼šä¹˜ä»¥stepï¼Œç›¸å½“äºä»feature mapä½ç½®æ˜ å°„å›åŸå›¾ä½ç½®ï¼‰ æ­£æ–¹å½¢prior boxæœ€å°è¾¹é•¿ä¸ºâ€™â€™â€™pash $min_size$â€™â€™â€™ï¼Œæœ€å¤§è¾¹é•¿ä¸ºï¼š\\sqrt{min_size*max_size}E=mc^2 æ ¹æ®ç›¸åº”çš„aspect ratioï¼Œä¼šç”Ÿæˆä¸åŒä¸ªæ•°çš„é•¿æ–¹å½¢ ï¼Œé•¿å®½ä¸ºï¼š$ \\sqrt{aspect_ratio}min_size$ å’Œ $1/ \\sqrt{aspect_ratio} min_size$ $f(x)=ax+b$ æœ€ç»ˆç½‘ç»œç”Ÿæˆå›ºå®šæ•°é‡çš„Prior Box æ¯ä¸ªfeature map å¯¹åº”prior boxçš„min_size å’Œmax_size ç”±ä»¥ä¸‹çš„å…¬å¼å†³å®šï¼Œå…¬å¼ä¸­çš„mæ˜¯ä½¿ç”¨feature mapçš„æ•°é‡(m=6)ç¬¬ä¸€å±‚feature mapå¯¹åº”çš„min_size=S1ï¼Œmax_size=S2ï¼›ç¬¬äºŒå±‚min_size=S2ï¼Œmax_size=S3ï¼›å…¶ä»–ç±»æ¨ã€‚åœ¨åŸæ–‡ä¸­ï¼ŒSmin=0.2ï¼ŒSmax=0.9 | min_size | max_size | def.boxes numâ€”|â€”|â€”|â€”conv4_3 | 30 | 60 | 4fc7 | 60 | 111 | 6conv6_2 | 111 | 162 | 6fc7 | 162 | 213 | 6conv4_3 | 213 | 264 | 4fc7 | 264 | 315 | 4 è®­ç»ƒç­–ç•¥æ­£è´Ÿæ ·æœ¬ç»™å®šè¾“å…¥å›¾åƒä»¥åŠæ¯ä¸ªç‰©ä½“çš„Ground Truth,é¦–å…ˆæ‰¾åˆ°æ¯ä¸ªGround True boxå¯¹åº”çš„default boxä¸­IOUæœ€å¤§çš„æœ€ä¸ºæ­£æ ·æœ¬ã€‚ç„¶å,åœ¨å‰©ä¸‹çš„default boxä¸­å¯»æ‰¾ä¸Ground Truth çš„IOUå¤§äº0.5çš„default boxä½œä¸ºæ­£æ ·æœ¬ã€‚ä¸€ä¸ªGround Truthå¯èƒ½å¯¹åº”å¤šä¸ªæ­£æ ·æœ¬default box.å…¶ä»–çš„default boxä½œä¸ºè´Ÿæ ·æœ¬ã€‚,ä¸ºäº†ä¿è¯æ ·æœ¬å°½é‡å¹³è¡¡,SSDé‡‡ç”¨äº†hard nagative mining,å³å¯¹è´Ÿæ ·æœ¬è¿›è¡ŒæŠ½æ ·,æŠ½æ ·æ—¶æŒ‰ç…§ç½®ä¿¡åº¦è¯¯å·®(é¢„æµ‹èƒŒæ™¯çš„ç½®ä¿¡åº¦è¶Šå°,è¯¯å·®è¶Šå¤§)è¿›è¡Œå¥–åºæ’åˆ—,é€‰å–è¯¯å·®è¾ƒå¤§çš„top-kä½œä¸ºè®­ç»ƒçš„è´Ÿæ ·æœ¬,ä¿è¯æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹æ¥è¿‘1:3ã€‚ ç›®æ ‡å‡½æ•°ç›®æ ‡å‡½æ•°ä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¼˜åŒ–æ ‡å‡†,ç›®æ ‡å‡½æ•°ä¹Ÿç§°æŸå¤±å‡½æ•°,ä¸»è¦åŒ…æ‹¬ä½ç½®è¯¯å·®(localization loss,loc) ä¸ç½®ä¿¡åº¦è¯¯å·®(confidence loss,conf,åˆ†ç±»æŸå¤±)çš„åŠ æƒå’Œ,å®šä¹‰ä¸ºï¼š ä»£ç è§£æåŸºç¡€æ¨¡å‹å®šä¹‰# vgg([64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',# 512, 512, 512], 3)# This function is derived from torchvision VGG make_layers()# https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.pydef vgg(cfg, i, batch_norm=False): layers = [] in_channels = i for v in cfg: if v == 'M': layers += [nn.MaxPool2d(kernel_size=2, stride=2)] elif v == 'C': layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)] else: conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1) if batch_norm: layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)] else: layers += [conv2d, nn.ReLU(inplace=True)] in_channels = v pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6) conv7 = nn.Conv2d(1024, 1024, kernel_size=1) layers += [pool5, conv6, nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)] return layers base = { '300': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M', 512, 512, 512], '512': [],}extras = { '300': [256, 'S', 512, 128, 'S', 256, 128, 256, 128, 256], '512': [],}mbox = { '300': [4, 6, 6, 6, 4, 4], # number of boxes per feature map location '512': [],}def add_extras(cfg, i, batch_norm=False): # Extra layers added to VGG for feature scaling layers =[] in_channel = i flag = False for k,v in enumerate(cfg): if in_channel !=\"S\": if v == \"S\": layers += [nn.Conv2d(in_channel,cfg[k+1], kernel_size=(1,3)[flag], stride=2,padding=1)] # flag æ¥æ§åˆ¶å·ç§¯æ ¸æ˜¯1 è¿˜æ˜¯3 else: layers += [nn.Conv2d(in_channel, v, kernel_size=(1,3)[flag])] flag = not flag in_channel = v return layers æ¨¡å‹Headéƒ¨åˆ†çš„ç”Ÿæˆ# cfg [4, 6, 6, 6, 4, 4], # number of boxes per feature map locationdef multibox(vgg, extra_layers, cfg, num_classes): loc_layers = [] conf_layers = [] vgg_source = [21, -2] for k, v in enumerate(vgg_source): loc_layers += [nn.Conv2d(vgg[v].out_channels, cfg[k] * 4, kernel_size=3, padding=1)] conf_layers += [nn.Conv2d(vgg[v].out_channels, cfg[k] * num_classes, kernel_size=3, padding=1)] # å¯¹extra_layersä¸­çš„ï¼ˆConv2d-2_1ã€Conv2d-4_1ã€Conv2d-6_1ã€Conv2d-8_1ï¼‰å±‚é€šè¿‡å·ç§¯æå–ç‰¹å¾ for k, v in enumerate(extra_layers[1::2], 2): loc_layers += [nn.Conv2d(v.out_channels, cfg[k] * 4, kernel_size=3, padding=1)] conf_layers += [nn.Conv2d(v.out_channels, cfg[k] * num_classes, kernel_size=3, padding=1)] return vgg, extra_layers, (loc_layers, conf_layers) æ¨¡å‹å…ˆéªŒæ¡†çš„ç”Ÿæˆclass PriorBox(object): \"\"\"Compute priorbox coordinates in center-offset form for each source feature map. \"\"\" def __init__(self, cfg): super(PriorBox, self).__init__() self.image_size = cfg['min_dim'] # number of priors for feature map location (either 4 or 6) self.num_priors = len(cfg['aspect_ratios']) self.variance = cfg['variance'] or [0.1] self.feature_maps = cfg['feature_maps'] self.min_sizes = cfg['min_sizes'] self.max_sizes = cfg['max_sizes'] self.steps = cfg['steps'] self.aspect_ratios = cfg['aspect_ratios'] self.clip = cfg['clip'] self.version = cfg['name'] for v in self.variance: if v = 0: raise ValueError('Variances must be greater than 0') def forward(self): mean = [] # 'steps': [8, 16, 32, 64, 100, 300], # 'feature_maps': [38, 19, 10, 5, 3, 1], # 'min_sizes': [21, 45, 99, 153, 207, 261], # 'max_sizes': [45, 99, 153, 207, 261, 315], for k, f in enumerate(self.feature_maps): for i, j in product(range(f), repeat=2): f_k = self.image_size / self.steps[k] # unit center x,y cx = (j + 0.5) / f_k cy = (i + 0.5) / f_k # aspect_ratio: 1 # rel size: min_size s_k = self.min_sizes[k]/self.image_size mean += [cx, cy, s_k, s_k] # aspect_ratio: 1 # rel size: sqrt(s_k * s_(k+1)) s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size)) mean += [cx, cy, s_k_prime, s_k_prime] # rest of aspect ratios for ar in self.aspect_ratios[k]: mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)] mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)] # back to torch land output = torch.Tensor(mean).view(-1, 4) if self.clip: output.clamp_(max=1, min=0) return output MultiBox æŸå¤±å‡½æ•°class MultiBoxLoss(nn.Module): \"\"\"SSD Weighted Loss Function Compute Targets: 1) Produce Confidence Target Indices by matching ground truth boxes with (default) 'priorboxes' that have jaccard index threshold parameter (default threshold: 0.5). 2) Produce localization target by 'encoding' variance into offsets of ground truth boxes and their matched 'priorboxes'. 3) Hard negative mining to filter the excessive number of negative examples that comes with using a large number of default bounding boxes. (default negative:positive ratio 3:1) Objective Loss: L(x,c,l,g) = (Lconf(x, c) + Î±Lloc(x,l,g)) / N Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss weighted by Î± which is set to 1 by cross val. Args: c: class confidences, l: predicted boxes, g: ground truth boxes N: number of matched default boxes See: https://arxiv.org/pdf/1512.02325.pdf for more details. # è®¡ç®—ç›®æ ‡: # è¾“å‡ºé‚£äº›ä¸çœŸå®æ¡†çš„iouå¤§äºä¸€å®šé˜ˆå€¼çš„æ¡†çš„ä¸‹æ ‡. # æ ¹æ®ä¸çœŸå®æ¡†çš„åç§»é‡è¾“å‡ºlocalizationç›®æ ‡ # ç”¨éš¾æ ·ä¾‹æŒ–æ˜ç®—æ³•å»é™¤å¤§é‡è´Ÿæ ·æœ¬(é»˜è®¤æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ä¸º1:3) # ç›®æ ‡æŸå¤±: # L(x,c,l,g) = (Lconf(x,c) + Î±Lloc(x,l,g)) / N # å‚æ•°: # c: ç±»åˆ«ç½®ä¿¡åº¦(class confidences) # l: é¢„æµ‹çš„æ¡†(predicted boxes) # g: çœŸå®æ¡†(ground truth boxes) # N: åŒ¹é…åˆ°çš„æ¡†çš„æ•°é‡(number of matched default boxes) \"\"\"# MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5, False, args.cuda) def __init__(self, num_classes, overlap_thresh, prior_for_matching, bkg_label, neg_mining, neg_pos, neg_overlap, encode_target, use_gpu=True): super(MultiBoxLoss, self).__init__() self.use_gpu = use_gpu self.num_classes = num_classes # åˆ—è¡¨æ•° 21 self.threshold = overlap_thresh # äº¤å¹¶æ¯”é˜ˆå€¼, 0.5 self.background_label = bkg_label # èƒŒæ™¯æ ‡ç­¾, 0 self.encode_target = encode_target # True æ²¡åµç”¨ self.use_prior_for_matching = prior_for_matching # True, æ²¡åµç”¨ self.do_neg_mining = neg_mining # è´Ÿæ ·æœ¬å’Œæ­£æ ·æœ¬çš„æ¯”ä¾‹, 3:1 self.negpos_ratio = neg_pos # 0.5 åˆ¤å®šè´Ÿæ ·æœ¬çš„é˜ˆå€¼. self.neg_overlap = neg_overlap # False æ²¡åµç”¨ self.variance = cfg['variance'] def forward(self, predictions, targets): \"\"\"Multibox Loss Args: predictions (tuple): A tuple containing loc preds, conf preds, and prior boxes from SSD net. conf shape: torch.size(batch_size,num_priors,num_classes) loc shape: torch.size(batch_size,num_priors,4) priors shape: torch.size(num_priors,4) targets (tensor): Ground truth boxes and labels for a batch, shape: [batch_size,num_objs,5] (last idx is the label). \"\"\" loc_data, conf_data, priors = predictions # loc_data: [batch_size, 8732, 4] # conf_data: [batch_size, 8732, 21] # priors: [8732, 4] default box å¯¹äºä»»æ„çš„å›¾ç‰‡, éƒ½æ˜¯ç›¸åŒçš„, å› æ­¤æ— éœ€å¸¦æœ‰ batch ç»´åº¦ num = loc_data.size(0) # num = batch_size priors = priors[:loc_data.size(1), :] # loc_data.size(1) = 8732, å› æ­¤ priors ç»´æŒä¸å˜ num_priors = (priors.size(0)) # num_priors = 8732 num_classes = self.num_classes # num_classes = 21 (é»˜è®¤ä¸ºvocæ•°æ®é›†) # match priors (default boxes) and ground truth boxes # å°†priors(default boxes)å’Œground truth boxesåŒ¹é… loc_t = torch.Tensor(num, num_priors, 4) # shape:[batch_size, 8732, 4] conf_t = torch.LongTensor(num, num_priors) # shape:[batch_size, 8732] for idx in range(num): # targetsæ˜¯åˆ—è¡¨, åˆ—è¡¨çš„é•¿åº¦ä¸ºbatch_size, åˆ—è¡¨ä¸­æ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ª tensor, # å…¶ shape ä¸º [num_objs, 5], å…¶ä¸­ num_objs ä¸ºå½“å‰å›¾ç‰‡ä¸­ç‰©ä½“çš„æ•°é‡, ç¬¬äºŒç»´å‰4ä¸ªå…ƒç´ ä¸ºè¾¹æ¡†åæ ‡, æœ€åä¸€ä¸ªå…ƒç´ ä¸ºç±»åˆ«ç¼–å·(1~20) truths = targets[idx][:, :-1].data # [num_objs, 4] labels = targets[idx][:, -1].data # [num_objs] ä½¿ç”¨çš„æ˜¯ -1, è€Œä¸æ˜¯ -1:, å› æ­¤, è¿”å›çš„ç»´åº¦å˜å°‘äº† defaults = priors.data # [8732, 4] # from ..box_utils import match # å…³é”®å‡½æ•°, å®ç°å€™é€‰æ¡†ä¸çœŸå®æ¡†ä¹‹é—´çš„åŒ¹é…, æ³¨æ„æ˜¯å€™é€‰æ¡†è€Œä¸æ˜¯é¢„æµ‹ç»“æœæ¡†! è¿™ä¸ªå‡½æ•°å®ç°è¾ƒä¸ºå¤æ‚, ä¼šåœ¨åé¢ç€é‡è®²è§£ match(self.threshold, truths, defaults, self.variance, labels, loc_t, conf_t, ) if self.use_gpu: loc_t = loc_t.cuda() conf_t = conf_t.cuda() # wrap targets # ç”¨Variableå°è£…loc_t, æ–°ç‰ˆæœ¬çš„ PyTorch æ— éœ€è¿™ä¹ˆåš, åªéœ€è¦å°† requires_grad å±æ€§è®¾ç½®ä¸º True å°±è¡Œäº† loc_t = Variable(loc_t, requires_grad=False) conf_t = Variable(conf_t, requires_grad=False) pos = conf_t 0 # ç­›é€‰å‡º 0 çš„boxä¸‹æ ‡(å¤§éƒ¨åˆ†éƒ½æ˜¯=0çš„) num_pos = pos.sum(dim=1, keepdim=True) # æ±‚å’Œ, å–å¾—æ»¡è¶³æ¡ä»¶çš„boxçš„æ•°é‡, [batch_size, num_gt_threshold] # Localization Loss (Smooth L1) # Shape: [batch,num_priors,4] # ä½ç½®(localization)æŸå¤±å‡½æ•°, ä½¿ç”¨ Smooth L1 å‡½æ•°æ±‚æŸå¤± # loc_data:[batch, num_priors, 4] # pos: [batch, num_priors] # pos_idx: [batch, num_priors, 4], å¤åˆ¶ä¸‹æ ‡æˆåæ ‡æ ¼å¼, ä»¥ä¾¿è·å–åæ ‡å€¼ pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data) loc_p = loc_data[pos_idx].view(-1, 4) # è·å–é¢„æµ‹ç»“æœå€¼ loc_t = loc_t[pos_idx].view(-1, 4) # è·å–gtå€¼ loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False) # è®¡ç®—æŸå¤± # Compute max conf across batch for hard negative mining # è®¡ç®—æœ€å¤§çš„ç½®ä¿¡åº¦, ä»¥è¿›è¡Œéš¾è´Ÿæ ·æœ¬æŒ–æ˜ # conf_data: [batch, num_priors, num_classes] # batch_conf: [batch, num_priors, num_classes] batch_conf = conf_data.view(-1, self.num_classes) # conf_t: [batch, num_priors] # loss_c: [batch*num_priors, 1], è®¡ç®—æ¯ä¸ªpriorboxé¢„æµ‹åçš„æŸå¤± loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1)) # éš¾è´Ÿæ ·æœ¬æŒ–æ˜, æŒ‰ç…§lossè¿›è¡Œæ’åº, å–lossæœ€å¤§çš„è´Ÿæ ·æœ¬å‚ä¸æ›´æ–° # Hard Negative Mining loss_c[pos] = 0 # filter out pos boxes for now # å°†æ‰€æœ‰çš„posä¸‹æ ‡çš„boxçš„lossç½®ä¸º0(posæŒ‡ç¤ºçš„æ˜¯æ­£æ ·æœ¬çš„ä¸‹æ ‡) # å°† loss_c çš„shape ä» [batch*num_priors, 1] è½¬æ¢æˆ [batch, num_priors] loss_c = loss_c.view(num, -1) # reshape # è¿›è¡Œé™åºæ’åº, å¹¶è·å–åˆ°æ’åºçš„ä¸‹æ ‡ _, loss_idx = loss_c.sort(1, descending=True) # å°†ä¸‹æ ‡è¿›è¡Œå‡åºæ’åº, å¹¶è·å–åˆ°ä¸‹æ ‡çš„ä¸‹æ ‡ _, idx_rank = loss_idx.sort(1) # num_pos: [batch, 1], ç»Ÿè®¡æ¯ä¸ªæ ·æœ¬ä¸­çš„objä¸ªæ•° num_pos = pos.long().sum(1, keepdim=True) # æ ¹æ®objçš„ä¸ªæ•°, ç¡®å®šè´Ÿæ ·æœ¬çš„ä¸ªæ•°(æ­£æ ·æœ¬çš„3å€) num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1) # è·å–åˆ°è´Ÿæ ·æœ¬çš„ä¸‹æ ‡ neg = idx_rank num_neg.expand_as(idx_rank) # è®¡ç®—åŒ…æ‹¬æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„ç½®ä¿¡åº¦æŸå¤± # pos: [batch, num_priors] # pos_idx: [batch, num_priors, num_classes] pos_idx = pos.unsqueeze(2).expand_as(conf_data) # neg: [batch, num_priors] # neg_idx: [batch, num_priors, num_classes] neg_idx = neg.unsqueeze(2).expand_as(conf_data) # æŒ‰ç…§pos_idxå’Œneg_idxæŒ‡ç¤ºçš„ä¸‹æ ‡ç­›é€‰å‚ä¸è®¡ç®—æŸå¤±çš„é¢„æµ‹æ•°æ® conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes) # æŒ‰ç…§pos_idxå’Œneg_idxç­›é€‰ç›®æ ‡æ•°æ® targets_weighted = conf_t[(pos+neg).gt(0)] # è®¡ç®—äºŒè€…çš„äº¤å‰ç†µ loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False) # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + Î±Lloc(x,l,g)) / N # å°†æŸå¤±å‡½æ•°å½’ä¸€åŒ–åè¿”å› N = num_pos.data.sum() loss_l /= N loss_c /= N return loss_l, loss_c matchå‡½æ•°çš„è§£ædef match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx): # threshold: (float) ç¡®å®šæ˜¯å¦åŒ¹é…çš„äº¤å¹¶æ¯”é˜ˆå€¼ # truths: (tensor: [num_obj, 4]) å­˜å‚¨çœŸå® box çš„è¾¹æ¡†åæ ‡ # priors: (tensor: [num_priors, 4], å³[8732, 4]), å­˜å‚¨æ¨èæ¡†çš„åæ ‡, æ³¨æ„, æ­¤æ—¶çš„æ¡†æ˜¯ default box, è€Œä¸æ˜¯ SSD ç½‘ç»œé¢„æµ‹å‡ºæ¥çš„æ¡†çš„åæ ‡, é¢„æµ‹çš„ç»“æœå­˜å‚¨åœ¨ loc_dataä¸­, å…¶ shape ä¸º[num_obj, 8732, 4]. # variances: cfg['variance'], [0.1, 0.2], ç”¨äºå°†åæ ‡è½¬æ¢æˆæ–¹ä¾¿è®­ç»ƒçš„å½¢å¼(å‚è€ƒRCNNç³»åˆ—å¯¹è¾¹æ¡†åæ ‡çš„å¤„ç†) # labels: (tensor: [num_obj]), ä»£è¡¨äº†æ¯ä¸ªçœŸå® box å¯¹åº”çš„ç±»åˆ«çš„ç¼–å· # loc_t: (tensor: [batches, 8732, 4]), # conf_t: (tensor: [batches, 8732]), # idx: batches ä¸­å›¾ç‰‡çš„åºå·, æ ‡è¯†å½“å‰æ­£åœ¨å¤„ç†çš„ image åœ¨ batches ä¸­çš„åºå· # jaccard index overlaps = jaccard( # [A, B], è¿”å›ä»»æ„ä¸¤ä¸ªboxä¹‹é—´çš„äº¤å¹¶æ¯”, overlaps[i][j] ä»£è¡¨box_aä¸­çš„ç¬¬iä¸ªboxä¸box_bä¸­çš„ç¬¬jä¸ªboxä¹‹é—´çš„äº¤å¹¶æ¯”. truths, point_form(priors) ) # äºŒéƒ¨å›¾åŒ¹é…(Bipartite Matching) # [num_objs,1], å¾—åˆ°å¯¹äºæ¯ä¸ª gt box æ¥è¯´çš„åŒ¹é…åº¦æœ€é«˜çš„ prior box, å‰è€…å­˜å‚¨äº¤å¹¶æ¯”, åè€…å­˜å‚¨prior boxåœ¨num_priorsä¸­çš„ä½ç½® best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True) # keepdim=True, å› æ­¤shapeä¸º[num_objs,1] # [1,num_priors] best ground truth for each prior # [1, num_priors], å³[1,8732], åŒç†, å¾—åˆ°å¯¹äºæ¯ä¸ª prior box æ¥è¯´çš„åŒ¹é…åº¦æœ€é«˜çš„ gt box best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True) best_truth_idx.squeeze_(0) # ä¸Šé¢ç‰¹æ„ä¿ç•™äº†ç»´åº¦(keepdim=True), è¿™é‡Œåˆéƒ½æŠŠç»´åº¦ squeeze/reduce äº†, å®é™…ä¸Šåªéœ€ç”¨é»˜è®¤çš„ keepdim=False å°±å¯ä»¥è‡ªåŠ¨ squeeze/reduce ç»´åº¦. best_truth_overlap.squeeze_(0) best_prior_idx.squeeze_(1) best_prior_overlap.squeeze_(1) best_truth_overlap.index_fill_(0, best_prior_idx, 2) # ensure best prior # ç»´åº¦å‹ç¼©åå˜ä¸º[num_priors], best_prior_idx ç»´åº¦ä¸º[num_objs], # è¯¥è¯­å¥ä¼šå°†ä¸gt boxåŒ¹é…åº¦æœ€å¥½çš„prior box çš„äº¤å¹¶æ¯”ç½®ä¸º 2, ç¡®ä¿å…¶æœ€å¤§, ä»¥å…é˜²æ­¢æŸäº› gtbox æ²¡æœ‰åŒ¹é…çš„ priorbox. # å‡æƒ³ä¸€ç§æç«¯æƒ…å†µ, æ‰€æœ‰çš„priorboxä¸æŸä¸ªgtbox(æ ‡è®°ä¸ºG)çš„äº¤å¹¶æ¯”ä¸º1, è€Œå…¶ä»–gtboxåˆ†åˆ«æœ‰ä¸€ä¸ªäº¤å¹¶æ¯” # æœ€é«˜çš„priorbox, ä½†æ˜¯è‚¯å®šå°äº1(å› ä¸ºå…¶ä»–çš„gtboxä¸Gçš„äº¤å¹¶æ¯”è‚¯å®šå°äº1), è¿™æ ·ä¸€æ¥, å°±ä¼šä½¿å¾—æ‰€æœ‰ # çš„priorboxéƒ½ä¸GåŒ¹é…, ä¸ºäº†é˜²æ­¢è¿™ç§æƒ…å†µ, æˆ‘ä»¬å°†é‚£äº›å¯¹gtboxæ¥è¯´, å…·æœ‰æœ€é«˜äº¤å¹¶æ¯”çš„priorbox, # å¼ºåˆ¶è¿›è¡Œäº’ç›¸åŒ¹é…, å³ä»¤best_truth_idx[best_prior_idx[j]] = j, è¯¦ç»†è§ä¸‹é¢çš„forå¾ªç¯ # TODO refactor: index best_prior_idx with long tensor # ensure every gt matches with its prior of max overlap # æ³¨æ„!!: å› ä¸º gt box çš„æ•°é‡è¦è¿œè¿œå°‘äº prior box çš„æ•°é‡, å› æ­¤, åŒä¸€ä¸ª gt box ä¼šä¸å¤šä¸ª prior box åŒ¹é…. for j in range(best_prior_idx.size(0)): best_truth_idx[best_prior_idx[j]] = j # best_prior_idx[j] ä»£è¡¨ä¸box_açš„ç¬¬jä¸ªboxäº¤å¹¶æ¯”æœ€é«˜çš„ prior box çš„ä¸‹æ ‡, å°†ä¸è¯¥ gtbox # åŒ¹é…åº¦æœ€å¥½çš„ prior box çš„ä¸‹æ ‡æ”¹ä¸ºj, ç”±æ­¤,å®Œæˆäº†è¯¥ gtbox ä¸ç¬¬jä¸ª prior box çš„åŒ¹é…. # è¿™é‡Œçš„å¾ªç¯åªä¼šè¿›è¡Œnum_objæ¬¡, å‰©ä½™çš„åŒ¹é…ä¸º best_truth_idx ä¸­åŸæœ¬çš„å€¼. # è¿™é‡Œå¤„ç†çš„æƒ…å†µæ˜¯, priorboxä¸­ç¬¬iä¸ªboxä¸gtboxä¸­ç¬¬kä¸ªboxçš„äº¤å¹¶æ¯”æœ€é«˜, # å³ best_truth_idx[i]= k # ä½†æ˜¯å¯¹äºbest_prior_idx[k]æ¥è¯´, å®ƒå´ä¸priorboxçš„ç¬¬lä¸ªboxæœ‰ç€æœ€é«˜çš„äº¤å¹¶æ¯”, # å³best_prior_idx[k]=l # è€Œå¯¹äºgtboxçš„å¦ä¸€ä¸ªè¾¹æ¡†gtbox[j]æ¥è¯´, å®ƒä¸priorbox[i]çš„äº¤å¹¶æ¯”æœ€å¤§, # å³ä½†æ˜¯å¯¹äºbest_prior_idx[j] = i. # é‚£ä¹ˆ, æ­¤æ—¶, æˆ‘ä»¬å°±åº”è¯¥å°†best_truth_idx[i]= k ä¿®æ”¹æˆ best_truth_idx[i]= j. # å³ä»¤ priorbox[i] ä¸ gtbox[j]å¯¹åº”. # è¿™æ ·åšçš„åŸå› : é˜²æ­¢æŸä¸ªgtboxæ²¡æœ‰åŒ¹é…çš„ prior box. matches = truths[best_truth_idx] # Shape: [num_priors,4] # truths çš„shape ä¸º[num_objs, 4], è€Œbest_truth_idxæ˜¯ä¸€ä¸ªæŒ‡ç¤ºä¸‹æ ‡çš„åˆ—è¡¨, åˆ—è¡¨é•¿åº¦ä¸º 8732, # åˆ—è¡¨ä¸­çš„ä¸‹æ ‡èŒƒå›´ä¸º0~num_objs-1, ä»£è¡¨çš„æ˜¯ä¸æ¯ä¸ªpriorboxåŒ¹é…çš„gtboxçš„ä¸‹æ ‡ # ä¸Šé¢çš„è¡¨è¾¾å¼ä¼šè¿”å›ä¸€ä¸ªshapeä¸º [num_priors, 4], å³ [8732, 4] çš„tensor, ä»£è¡¨çš„å°±æ˜¯ä¸æ¯ä¸ªpriorboxåŒ¹é…çš„gtboxçš„åæ ‡å€¼. conf = labels[best_truth_idx] + 1 # ä¸ä¸Šé¢çš„è¯­å¥é“ç†å·®ä¸å¤š, è¿™é‡Œå¾—åˆ°çš„æ˜¯æ¯ä¸ªprior boxåŒ¹é…çš„ç±»åˆ«ç¼–å·, shape ä¸º[8732] conf[best_truth_overlap threshold] = 0 # å°†ä¸gtboxçš„äº¤å¹¶æ¯”å°äºé˜ˆå€¼çš„ç½®ä¸º0 , å³è®¤ä¸ºæ˜¯éç‰©ä½“æ¡† loc = encode(matches, priors, variances) # è¿”å›ç¼–ç åçš„ä¸­å¿ƒåæ ‡å’Œå®½é«˜. loc_t[idx] = loc # [num_priors,4] encoded offsets to learn # è®¾ç½®ç¬¬idxå¼ å›¾ç‰‡çš„gtç¼–ç åæ ‡ä¿¡æ¯ conf_t[idx] = conf # [num_priors] top class label for each prior è®¾ç½®ç¬¬idxå¼ å›¾ç‰‡çš„ç¼–å·ä¿¡æ¯.(å¤§äº0å³ä¸ºç‰©ä½“ç¼–å·, è®¤ä¸ºæœ‰ç‰©ä½“, å°äº0è®¤ä¸ºæ˜¯èƒŒæ™¯) å‚è€ƒæ–‡ç« ï¼šhttps://blog.csdn.net/happyday_d/article/details/86021993https://hellozhaozheng.github.io/z_post/PyTorch-SSD","tags":["ç›®æ ‡æ£€æµ‹"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"Matplotlibç»˜åˆ¶å…­ç§å¯è§†åŒ–å›¾è¡¨","path":"/2019/08/24/pythonplot/","content":"01 æŠ˜çº¿å›¾1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | import numpy as np import matplotlib.pyplot as plt x = np.linspace(0,2,100) plt.plot(x, x, label='linear') plt.plot(x, x**2, label='quadratic') plt.plot(x, x**3, label='cubic') plt.xlabel('x label') plt.ylabel('y label') plt.title(\"Simple Plot\") plt.legend() plt.show() â€”|â€” 02 æ•£ç‚¹å›¾1 2 3 4 5 6 7 8 | import numpy as np import matplotlib.pyplot as plt x = np.arange(0., 5., 0.2) # çº¢è‰²ç ´æŠ˜å·, è“è‰²æ–¹å— ï¼Œç»¿è‰²ä¸‰è§’å— plt.plot(x, x, 'r--', x, x**2, 'bs', x, x**3, 'g^') plt.show() â€”|â€” 03 ç›´æ–¹å›¾1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 | import numpy as np import matplotlib.pyplot as plt np.random.seed(19680801) mu1, sigma1 = 100, 15 mu2, sigma2 = 80, 15 x1 = mu1 + sigma1 * np.random.randn(10000) x2 = mu2 + sigma2 * np.random.randn(10000) # the histogram of the data # 50ï¼šå°†æ•°æ®åˆ†æˆ50ç»„ # facecolorï¼šé¢œè‰²ï¼›alphaï¼šé€æ˜åº¦ # densityï¼šæ˜¯å¯†åº¦è€Œä¸æ˜¯å…·ä½“æ•°å€¼ n1, bins1, patches1 = plt.hist(x1, 50, density=True, facecolor='g', alpha=1) n2, bins2, patches2 = plt.hist(x2, 50, density=True, facecolor='r', alpha=0.2) # nï¼šæ¦‚ç‡å€¼ï¼›binsï¼šå…·ä½“æ•°å€¼ï¼›patchesï¼šç›´æ–¹å›¾å¯¹è±¡ã€‚ plt.xlabel('Smarts') plt.ylabel('Probability') plt.title('Histogram of IQ') plt.text(110, .025, r'$\\mu=100,\\ \\sigma=15$') plt.text(50, .025, r'$\\mu=80,\\ \\sigma=15$') # è®¾ç½®xï¼Œyè½´çš„å…·ä½“èŒƒå›´ plt.axis([40, 160, 0, 0.03]) plt.grid(True) plt.show() â€”|â€” 04 æŸ±çŠ¶å›¾4.1 å¹¶åˆ—æŸ±çŠ¶å›¾1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 | import numpy as np import matplotlib.pyplot as plt size = 5 a = np.random.random(size) b = np.random.random(size) c = np.random.random(size) x = np.arange(size) # æœ‰å¤šå°‘ä¸ªç±»å‹ï¼Œåªéœ€æ›´æ”¹nå³å¯ total_width, n = 0.8, 3 width = total_width / n # é‡æ–°æ‹Ÿå®šxçš„åæ ‡ x = x - (total_width - width) / 2 # è¿™é‡Œä½¿ç”¨çš„æ˜¯åç§» plt.bar(x, a, width=width, label='a') plt.bar(x + width, b, width=width, label='b') plt.bar(x + 2 * width, c, width=width, label='c') plt.legend() plt.show() â€”|â€” 4.2 å åŠ æŸ±çŠ¶å›¾1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 | import numpy as np import matplotlib.pyplot as plt size = 5 a = np.random.random(size) b = np.random.random(size) c = np.random.random(size) x = np.arange(size) # è¿™é‡Œä½¿ç”¨çš„æ˜¯åç§» plt.bar(x, a, width=0.5, label='a',fc='r') plt.bar(x, b, bottom=a, width=0.5, label='b', fc='g') plt.bar(x, c, bottom=a+b, width=0.5, label='c', fc='b') plt.ylim(0, 2.5) plt.legend() plt.grid(True) plt.show() â€”|â€” 05 é¥¼å›¾5.1 æ™®é€šé¥¼å›¾1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | import matplotlib.pyplot as plt labels = 'Frogs', 'Hogs', 'Dogs', 'Logs' sizes = [15, 30, 45, 10] # è®¾ç½®åˆ†ç¦»çš„è·ç¦»ï¼Œ0è¡¨ç¤ºä¸åˆ†ç¦» explode = (0, 0.1, 0, 0) plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90) # Equal aspect ratio ä¿è¯ç”»å‡ºçš„å›¾æ˜¯æ­£åœ†å½¢ plt.axis('equal') plt.show() â€”|â€” 5.2 åµŒå¥—é¥¼å›¾1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 | import numpy as np import matplotlib.pyplot as plt # è®¾ç½®æ¯ç¯çš„å®½åº¦ size = 0.3 vals = np.array([[60., 32.], [37., 40.], [29., 10.]]) # é€šè¿‡get_cmapéšæœºè·å–é¢œè‰² cmap = plt.get_cmap(\"tab20c\") outer_colors = cmap(np.arange(3)*4) inner_colors = cmap(np.array([1, 2, 5, 6, 9, 10])) print(vals.sum(axis=1)) # [92. 77. 39.] plt.pie(vals.sum(axis=1), radius=1, colors=outer_colors, wedgeprops=dict(width=size, edgecolor='w')) print(vals.flatten()) # [60. 32. 37. 40. 29. 10.] plt.pie(vals.flatten(), radius=1-size, colors=inner_colors, wedgeprops=dict(width=size, edgecolor='w')) # equal ä½¿å¾—ä¸ºæ­£åœ† plt.axis('equal') plt.show() â€”|â€” 5.3 æè½´é¥¼å›¾1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 | import numpy as np import matplotlib.pyplot as plt np.random.seed(19680801) N = 10 theta = np.linspace(0.0, 2 * np.pi, N, endpoint=False) radii = 10 * np.random.rand(N) width = np.pi / 4 * np.random.rand(N) ax = plt.subplot(111, projection='polar') bars = ax.bar(theta, radii, width=width, bottom=0.0) # leftè¡¨ç¤ºä»å“ªå¼€å§‹ï¼Œ # radiiè¡¨ç¤ºä»ä¸­å¿ƒç‚¹å‘è¾¹ç¼˜ç»˜åˆ¶çš„é•¿åº¦ï¼ˆåŠå¾„ï¼‰ # widthè¡¨ç¤ºæœ«ç«¯çš„å¼§é•¿ # è‡ªå®šä¹‰é¢œè‰²å’Œä¸é€æ˜åº¦ for r, bar in zip(radii, bars): bar.set_facecolor(plt.cm.viridis(r / 10.)) bar.set_alpha(0.5) plt.show() â€”|â€” 06 ä¸‰ç»´å›¾6.1 ç»˜åˆ¶ä¸‰ç»´æ•£ç‚¹å›¾1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D data = np.random.randint(0, 255, size=[40, 40, 40]) x, y, z = data[0], data[1], data[2] ax = plt.subplot(111, projection='3d') # åˆ›å»ºä¸€ä¸ªä¸‰ç»´çš„ç»˜å›¾å·¥ç¨‹ # å°†æ•°æ®ç‚¹åˆ†æˆä¸‰éƒ¨åˆ†ç”»ï¼Œåœ¨é¢œè‰²ä¸Šæœ‰åŒºåˆ†åº¦ ax.scatter(x[:10], y[:10], z[:10], c='y') # ç»˜åˆ¶æ•°æ®ç‚¹ ax.scatter(x[10:20], y[10:20], z[10:20], c='r') ax.scatter(x[30:40], y[30:40], z[30:40], c='g') ax.set_zlabel('Z') # åæ ‡è½´ ax.set_ylabel('Y') ax.set_xlabel('X') plt.show() â€”|â€” 6.2 ç»˜åˆ¶ä¸‰ç»´å¹³é¢å›¾1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 | from matplotlib import pyplot as plt import numpy as np from mpl_toolkits.mplot3d import Axes3D fig = plt.figure() ax = Axes3D(fig) X = np.arange(-4, 4, 0.25) Y = np.arange(-4, 4, 0.25) X, Y = np.meshgrid(X, Y) R = np.sqrt(X**2 + Y**2) Z = np.sin(R) # å…·ä½“å‡½æ•°æ–¹æ³•å¯ç”¨ help(function) æŸ¥çœ‹ï¼Œå¦‚ï¼šhelp(ax.plot_surface) ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='rainbow') plt.show() â€”|â€” å‚è€ƒé“¾æ¥ï¼š https://mp.weixin.qq.com/s/bMvrle-FRvli0pRNi83waQ","tags":["Python"],"categories":["Python"]},{"title":"SSDæºç è§£æ","path":"/2019/08/24/ssd/","content":"è®ºæ–‡åœ°å€: SSD: Single Shot MultiBox Detector éå®˜æ–¹ä»£ç : pytorch ä»‹ç»SSDï¼Œå…¨ç§°Single Shot MultiBox Detectorï¼Œæ˜¯ä¸€ç§One-Stageçš„æ–¹æ³•ï¼Œå®ƒç”±Wei Liuåœ¨ECCV 2016ä¸Šæå‡ºï¼ŒSSDå…·æœ‰å¦‚ä¸‹ä¸»è¦ç‰¹ç‚¹ï¼š ä»YOLOä¸­ç»§æ‰¿äº†å°†detectionè½¬åŒ–ä¸ºregressionçš„æ€è·¯ï¼ŒåŒæ—¶ä¸€æ¬¡å³å¯å®Œæˆç½‘ç»œè®­ç»ƒ åŸºäºFaster RCNNä¸­çš„anchorï¼Œæå‡ºäº†ç›¸ä¼¼çš„prior box åŠ å…¥åŸºäºç‰¹å¾é‡‘å­—å¡”ï¼ˆPyramidal Feature Hierarchyï¼‰çš„æ£€æµ‹æ–¹å¼ï¼Œç›¸å½“äºåŠä¸ªFPNæ€è·¯ SSDçš„ ç½‘ç»œç»“æ„ ç”±ä¸Šå›¾å¯ä»¥çœ‹å‡ºï¼ŒSSDçš„åŸºç¡€ç½‘ç»œç»“æ„ç”±åŸºç¡€ç½‘ç»œVGG16ç»„æˆï¼Œåœ¨VGG16åŸºç¡€ç½‘ç»œä¹‹åæ¥äº†ä¸€ä¸ª3x3çš„å·ç§¯å’Œä¸€ä¸ª1x1çš„å·ç§¯åšç‰¹å¾èåˆï¼Œç„¶åå¢åŠ äº†ä¸€ä¸ªExtra Feature Layers å±‚ï¼Œè¿™ä¸ªå±‚ç”±å…«ä¸ªå·ç§¯å±‚æ„æˆã€‚SSDåœ¨å‰é¢çš„åŸºç¡€ç½‘ç»œå»conv_4_3ä¹‹åçš„reluå±‚è¾“å‡ºï¼Œä»¥åŠå€’æ•°ç¬¬äºŒå±‚çš„conv_7_1çš„reluå†åŠ ä¸ŠExtra Feature Layerså±‚çš„ç¬¬1,3,5,7å±‚ å…±æœ‰6ä¸ª featuremapå±‚ï¼Œåœ¨æ­¤åŸºç¡€ä¸Šå¯¹boxè¿›è¡Œé¢„æµ‹ã€‚ä½†æ„Ÿè§‰è¦æå‡æ•ˆæœçš„è¯å¯ä»¥å¯¹åŸºç¡€ç½‘ç»œè¿›è¡Œæ›´æ”¹ï¼Œå¢åŠ ç‰¹å¾èåˆç­‰ç­‰ã€‚ ç©ºæ´å·ç§¯(Dilation Conv)ssdç½‘ç»œé‡Œè¿˜ä½¿ç”¨äº†ç©ºæ´å·ç§¯(Dilation Conv),é‡‡ç”¨VGG16åšåŸºç¡€æ¨¡å‹ï¼Œé¦–å…ˆVGG16æ˜¯åœ¨ILSVRC CLS-LOCæ•°æ®é›†é¢„è®­ç»ƒã€‚ç„¶åå€Ÿé‰´äº†DeepLab-LargeFOVï¼Œåˆ†åˆ«å°†VGG16çš„å…¨è¿æ¥å±‚fc6å’Œfc7è½¬æ¢æˆ 3Ã—3å·ç§¯å±‚ conv6å’Œ 1Ã—1 å·ç§¯å±‚conv7ï¼ŒåŒæ—¶å°†æ± åŒ–å±‚ pool5 ç”±åŸæ¥çš„ stride=2 çš„ 2Ã—2 å˜æˆ stride=1 çš„(çŒœæƒ³æ˜¯ä¸æƒ³reduceç‰¹å¾å›¾å¤§å°)ï¼Œä¸ºäº†é…åˆè¿™ç§å˜åŒ–ï¼Œé‡‡ç”¨äº†ä¸€ç§ Atrous Algorithmï¼Œå…¶å®å°±æ˜¯conv6é‡‡ç”¨æ‰©å±•å·ç§¯æˆ–å¸¦å­”å·ç§¯ï¼ˆDilation Convï¼‰ï¼Œå…¶åœ¨ä¸å¢åŠ å‚æ•°ä¸æ¨¡å‹å¤æ‚åº¦çš„æ¡ä»¶ä¸‹æŒ‡æ•°çº§æ‰©å¤§å·ç§¯çš„è§†é‡ï¼Œå…¶ä½¿ç”¨æ‰©å¼ ç‡(dilation rate)å‚æ•°ï¼Œæ¥è¡¨ç¤ºæ‰©å¼ çš„å¤§å°ï¼Œå¦‚ä¸‹å›¾æ‰€ç¤ºï¼Œ(a)æ˜¯æ™®é€šçš„ 3Ã—3 å·ç§¯ï¼Œå…¶è§†é‡å°±æ˜¯ 3Ã—3 ï¼Œ(b)æ˜¯æ‰©å¼ ç‡ä¸º 1ï¼Œæ­¤æ—¶è§†é‡å˜æˆ 7Ã—7 ï¼Œ(c)æ‰©å¼ ç‡ä¸º3æ—¶ï¼Œè§†é‡æ‰©å¤§ä¸º 15Ã—15 ï¼Œä½†æ˜¯è§†é‡çš„ç‰¹å¾æ›´ç¨€ç–äº†ã€‚Conv6é‡‡ç”¨ 3Ã—3 å¤§å°ä½†dilation rate=6çš„æ‰©å±•å·ç§¯ã€‚ Prior BoxSSDä¸­æœ‰ç€ç±»ä¼¼anchoræœºåˆ¶çš„Prior Boxæœºåˆ¶ï¼Œç”¨äºæ¥ç”Ÿæˆå…ˆéªŒæ¡†ï¼Œåé¢å°†è¿™äº›å…ˆéªŒæ¡†ä¸çœŸå®çš„gtè¿›è¡ŒåŒ¹é…ï¼Œç„¶åä¸é¢„æµ‹çš„è¿›è¡Œå›å½’ã€‚ä»è€Œå¾—åˆ°ç‰©ä½“çœŸå®çš„ ä½ç½®ã€‚SSDçš„prior Box æŒ‰ç…§å¦‚ä¸‹è§„åˆ™ç”Ÿæˆï¼š ä»¥feature mapä¸Šæ¯ä¸ªç‚¹çš„ä¸­ç‚¹ä¸ºä¸­å¿ƒï¼ˆoffset=0.5ï¼‰ï¼Œç”Ÿæˆä¸€äº›åˆ—åŒå¿ƒçš„prior boxï¼ˆç„¶åä¸­å¿ƒç‚¹çš„åæ ‡ä¼šä¹˜ä»¥stepï¼Œç›¸å½“äºä»feature mapä½ç½®æ˜ å°„å›åŸå›¾ä½ç½®ï¼‰ æ­£æ–¹å½¢prior boxæœ€å°è¾¹é•¿ä¸ºâ€™â€™â€™pash $min_size$â€™â€™â€™ï¼Œæœ€å¤§è¾¹é•¿ä¸ºï¼š\\sqrt{min_size*max_size}E=mc^2 æ ¹æ®ç›¸åº”çš„aspect ratioï¼Œä¼šç”Ÿæˆä¸åŒä¸ªæ•°çš„é•¿æ–¹å½¢ ï¼Œé•¿å®½ä¸ºï¼š$ \\sqrt{aspect_ratio}min_size$ å’Œ $1/ \\sqrt{aspect_ratio} min_size$ $f(x)=ax+b$ æœ€ç»ˆç½‘ç»œç”Ÿæˆå›ºå®šæ•°é‡çš„Prior Box æ¯ä¸ªfeature map å¯¹åº”prior boxçš„min_size å’Œmax_size ç”±ä»¥ä¸‹çš„å…¬å¼å†³å®šï¼Œå…¬å¼ä¸­çš„mæ˜¯ä½¿ç”¨feature mapçš„æ•°é‡(m=6)ç¬¬ä¸€å±‚feature mapå¯¹åº”çš„min_size=S1ï¼Œmax_size=S2ï¼›ç¬¬äºŒå±‚min_size=S2ï¼Œmax_size=S3ï¼›å…¶ä»–ç±»æ¨ã€‚åœ¨åŸæ–‡ä¸­ï¼ŒSmin=0.2ï¼ŒSmax=0.9 | min_size | max_size | def.boxes numâ€”|â€”|â€”|â€”conv4_3 | 30 | 60 | 4fc7 | 60 | 111 | 6conv6_2 | 111 | 162 | 6fc7 | 162 | 213 | 6conv4_3 | 213 | 264 | 4fc7 | 264 | 315 | 4 è®­ç»ƒç­–ç•¥æ­£è´Ÿæ ·æœ¬ç»™å®šè¾“å…¥å›¾åƒä»¥åŠæ¯ä¸ªç‰©ä½“çš„Ground Truth,é¦–å…ˆæ‰¾åˆ°æ¯ä¸ªGround True boxå¯¹åº”çš„default boxä¸­IOUæœ€å¤§çš„æœ€ä¸ºæ­£æ ·æœ¬ã€‚ç„¶å,åœ¨å‰©ä¸‹çš„default boxä¸­å¯»æ‰¾ä¸Ground Truth çš„IOUå¤§äº0.5çš„default boxä½œä¸ºæ­£æ ·æœ¬ã€‚ä¸€ä¸ªGround Truthå¯èƒ½å¯¹åº”å¤šä¸ªæ­£æ ·æœ¬default box.å…¶ä»–çš„default boxä½œä¸ºè´Ÿæ ·æœ¬ã€‚,ä¸ºäº†ä¿è¯æ ·æœ¬å°½é‡å¹³è¡¡,SSDé‡‡ç”¨äº†hard nagative mining,å³å¯¹è´Ÿæ ·æœ¬è¿›è¡ŒæŠ½æ ·,æŠ½æ ·æ—¶æŒ‰ç…§ç½®ä¿¡åº¦è¯¯å·®(é¢„æµ‹èƒŒæ™¯çš„ç½®ä¿¡åº¦è¶Šå°,è¯¯å·®è¶Šå¤§)è¿›è¡Œå¥–åºæ’åˆ—,é€‰å–è¯¯å·®è¾ƒå¤§çš„top-kä½œä¸ºè®­ç»ƒçš„è´Ÿæ ·æœ¬,ä¿è¯æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹æ¥è¿‘1:3ã€‚ ç›®æ ‡å‡½æ•°ç›®æ ‡å‡½æ•°ä¸ºè®­ç»ƒè¿‡ç¨‹ä¸­çš„ä¼˜åŒ–æ ‡å‡†,ç›®æ ‡å‡½æ•°ä¹Ÿç§°æŸå¤±å‡½æ•°,ä¸»è¦åŒ…æ‹¬ä½ç½®è¯¯å·®(localization loss,loc) ä¸ç½®ä¿¡åº¦è¯¯å·®(confidence loss,conf,åˆ†ç±»æŸå¤±)çš„åŠ æƒå’Œ,å®šä¹‰ä¸ºï¼š ä»£ç è§£æåŸºç¡€æ¨¡å‹å®šä¹‰1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 | # vgg([64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M', # 512, 512, 512], 3) # This function is derived from torchvision VGG make_layers() # https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py def vgg(cfg, i, batch_norm=False): layers = [] in_channels = i for v in cfg: if v == 'M': layers += [nn.MaxPool2d(kernel_size=2, stride=2)] elif v == 'C': layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)] else: conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1) if batch_norm: layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)] else: layers += [conv2d, nn.ReLU(inplace=True)] in_channels = v pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6) conv7 = nn.Conv2d(1024, 1024, kernel_size=1) layers += [pool5, conv6, nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)] return layers â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 | base = { '300': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M', 512, 512, 512], '512': [], } extras = { '300': [256, 'S', 512, 128, 'S', 256, 128, 256, 128, 256], '512': [], } mbox = { '300': [4, 6, 6, 6, 4, 4], # number of boxes per feature map location '512': [], } def add_extras(cfg, i, batch_norm=False): # Extra layers added to VGG for feature scaling layers =[] in_channel = i flag = False for k,v in enumerate(cfg): if in_channel !=\"S\": if v == \"S\": layers += [nn.Conv2d(in_channel,cfg[k+1], kernel_size=(1,3)[flag], stride=2,padding=1)] # flag æ¥æ§åˆ¶å·ç§¯æ ¸æ˜¯1 è¿˜æ˜¯3 else: layers += [nn.Conv2d(in_channel, v, kernel_size=(1,3)[flag])] flag = not flag in_channel = v return layers â€”|â€” æ¨¡å‹Headéƒ¨åˆ†çš„ç”Ÿæˆ1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 | # cfg [4, 6, 6, 6, 4, 4], # number of boxes per feature map location def multibox(vgg, extra_layers, cfg, num_classes): loc_layers = [] conf_layers = [] vgg_source = [21, -2] for k, v in enumerate(vgg_source): loc_layers += [nn.Conv2d(vgg[v].out_channels, cfg[k] * 4, kernel_size=3, padding=1)] conf_layers += [nn.Conv2d(vgg[v].out_channels, cfg[k] * num_classes, kernel_size=3, padding=1)] # å¯¹extra_layersä¸­çš„ï¼ˆConv2d-2_1ã€Conv2d-4_1ã€Conv2d-6_1ã€Conv2d-8_1ï¼‰å±‚é€šè¿‡å·ç§¯æå–ç‰¹å¾ for k, v in enumerate(extra_layers[1::2], 2): loc_layers += [nn.Conv2d(v.out_channels, cfg[k] * 4, kernel_size=3, padding=1)] conf_layers += [nn.Conv2d(v.out_channels, cfg[k] * num_classes, kernel_size=3, padding=1)] return vgg, extra_layers, (loc_layers, conf_layers) â€”|â€” æ¨¡å‹å…ˆéªŒæ¡†çš„ç”Ÿæˆ1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 | class PriorBox(object): \"\"\"Compute priorbox coordinates in center-offset form for each source feature map. \"\"\" def __init__(self, cfg): super(PriorBox, self).__init__() self.image_size = cfg['min_dim'] # number of priors for feature map location (either 4 or 6) self.num_priors = len(cfg['aspect_ratios']) self.variance = cfg['variance'] or [0.1] self.feature_maps = cfg['feature_maps'] self.min_sizes = cfg['min_sizes'] self.max_sizes = cfg['max_sizes'] self.steps = cfg['steps'] self.aspect_ratios = cfg['aspect_ratios'] self.clip = cfg['clip'] self.version = cfg['name'] for v in self.variance: if v = 0: raise ValueError('Variances must be greater than 0') def forward(self): mean = [] # 'steps': [8, 16, 32, 64, 100, 300], # 'feature_maps': [38, 19, 10, 5, 3, 1], # 'min_sizes': [21, 45, 99, 153, 207, 261], # 'max_sizes': [45, 99, 153, 207, 261, 315], for k, f in enumerate(self.feature_maps): for i, j in product(range(f), repeat=2): f_k = self.image_size / self.steps[k] # unit center x,y cx = (j + 0.5) / f_k cy = (i + 0.5) / f_k # aspect_ratio: 1 # rel size: min_size s_k = self.min_sizes[k]/self.image_size mean += [cx, cy, s_k, s_k] # aspect_ratio: 1 # rel size: sqrt(s_k * s_(k+1)) s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size)) mean += [cx, cy, s_k_prime, s_k_prime] # rest of aspect ratios for ar in self.aspect_ratios[k]: mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)] mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)] # back to torch land output = torch.Tensor(mean).view(-1, 4) if self.clip: output.clamp_(max=1, min=0) return output â€”|â€” MultiBox æŸå¤±å‡½æ•°1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 | class MultiBoxLoss(nn.Module): \"\"\"SSD Weighted Loss Function Compute Targets: 1) Produce Confidence Target Indices by matching ground truth boxes with (default) 'priorboxes' that have jaccard index threshold parameter (default threshold: 0.5). 2) Produce localization target by 'encoding' variance into offsets of ground truth boxes and their matched 'priorboxes'. 3) Hard negative mining to filter the excessive number of negative examples that comes with using a large number of default bounding boxes. (default negative:positive ratio 3:1) Objective Loss: L(x,c,l,g) = (Lconf(x, c) + Î±Lloc(x,l,g)) / N Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss weighted by Î± which is set to 1 by cross val. Args: c: class confidences, l: predicted boxes, g: ground truth boxes N: number of matched default boxes See: https://arxiv.org/pdf/1512.02325.pdf for more details. # è®¡ç®—ç›®æ ‡: # è¾“å‡ºé‚£äº›ä¸çœŸå®æ¡†çš„iouå¤§äºä¸€å®šé˜ˆå€¼çš„æ¡†çš„ä¸‹æ ‡. # æ ¹æ®ä¸çœŸå®æ¡†çš„åç§»é‡è¾“å‡ºlocalizationç›®æ ‡ # ç”¨éš¾æ ·ä¾‹æŒ–æ˜ç®—æ³•å»é™¤å¤§é‡è´Ÿæ ·æœ¬(é»˜è®¤æ­£è´Ÿæ ·æœ¬æ¯”ä¾‹ä¸º1:3) # ç›®æ ‡æŸå¤±: # L(x,c,l,g) = (Lconf(x,c) + Î±Lloc(x,l,g)) / N # å‚æ•°: # c: ç±»åˆ«ç½®ä¿¡åº¦(class confidences) # l: é¢„æµ‹çš„æ¡†(predicted boxes) # g: çœŸå®æ¡†(ground truth boxes) # N: åŒ¹é…åˆ°çš„æ¡†çš„æ•°é‡(number of matched default boxes) \"\"\" # MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5, False, args.cuda) def __init__(self, num_classes, overlap_thresh, prior_for_matching, bkg_label, neg_mining, neg_pos, neg_overlap, encode_target, use_gpu=True): super(MultiBoxLoss, self).__init__() self.use_gpu = use_gpu self.num_classes = num_classes # åˆ—è¡¨æ•° 21 self.threshold = overlap_thresh # äº¤å¹¶æ¯”é˜ˆå€¼, 0.5 self.background_label = bkg_label # èƒŒæ™¯æ ‡ç­¾, 0 self.encode_target = encode_target # True æ²¡åµç”¨ self.use_prior_for_matching = prior_for_matching # True, æ²¡åµç”¨ self.do_neg_mining = neg_mining # è´Ÿæ ·æœ¬å’Œæ­£æ ·æœ¬çš„æ¯”ä¾‹, 3:1 self.negpos_ratio = neg_pos # 0.5 åˆ¤å®šè´Ÿæ ·æœ¬çš„é˜ˆå€¼. self.neg_overlap = neg_overlap # False æ²¡åµç”¨ self.variance = cfg['variance'] def forward(self, predictions, targets): \"\"\"Multibox Loss Args: predictions (tuple): A tuple containing loc preds, conf preds, and prior boxes from SSD net. conf shape: torch.size(batch_size,num_priors,num_classes) loc shape: torch.size(batch_size,num_priors,4) priors shape: torch.size(num_priors,4) targets (tensor): Ground truth boxes and labels for a batch, shape: [batch_size,num_objs,5] (last idx is the label). \"\"\" loc_data, conf_data, priors = predictions # loc_data: [batch_size, 8732, 4] # conf_data: [batch_size, 8732, 21] # priors: [8732, 4] default box å¯¹äºä»»æ„çš„å›¾ç‰‡, éƒ½æ˜¯ç›¸åŒçš„, å› æ­¤æ— éœ€å¸¦æœ‰ batch ç»´åº¦ num = loc_data.size(0) # num = batch_size priors = priors[:loc_data.size(1), :] # loc_data.size(1) = 8732, å› æ­¤ priors ç»´æŒä¸å˜ num_priors = (priors.size(0)) # num_priors = 8732 num_classes = self.num_classes # num_classes = 21 (é»˜è®¤ä¸ºvocæ•°æ®é›†) # match priors (default boxes) and ground truth boxes # å°†priors(default boxes)å’Œground truth boxesåŒ¹é… loc_t = torch.Tensor(num, num_priors, 4) # shape:[batch_size, 8732, 4] conf_t = torch.LongTensor(num, num_priors) # shape:[batch_size, 8732] for idx in range(num): # targetsæ˜¯åˆ—è¡¨, åˆ—è¡¨çš„é•¿åº¦ä¸ºbatch_size, åˆ—è¡¨ä¸­æ¯ä¸ªå…ƒç´ ä¸ºä¸€ä¸ª tensor, # å…¶ shape ä¸º [num_objs, 5], å…¶ä¸­ num_objs ä¸ºå½“å‰å›¾ç‰‡ä¸­ç‰©ä½“çš„æ•°é‡, ç¬¬äºŒç»´å‰4ä¸ªå…ƒç´ ä¸ºè¾¹æ¡†åæ ‡, æœ€åä¸€ä¸ªå…ƒç´ ä¸ºç±»åˆ«ç¼–å·(1~20) truths = targets[idx][:, :-1].data # [num_objs, 4] labels = targets[idx][:, -1].data # [num_objs] ä½¿ç”¨çš„æ˜¯ -1, è€Œä¸æ˜¯ -1:, å› æ­¤, è¿”å›çš„ç»´åº¦å˜å°‘äº† defaults = priors.data # [8732, 4] # from ..box_utils import match # å…³é”®å‡½æ•°, å®ç°å€™é€‰æ¡†ä¸çœŸå®æ¡†ä¹‹é—´çš„åŒ¹é…, æ³¨æ„æ˜¯å€™é€‰æ¡†è€Œä¸æ˜¯é¢„æµ‹ç»“æœæ¡†! è¿™ä¸ªå‡½æ•°å®ç°è¾ƒä¸ºå¤æ‚, ä¼šåœ¨åé¢ç€é‡è®²è§£ match(self.threshold, truths, defaults, self.variance, labels, loc_t, conf_t, ) if self.use_gpu: loc_t = loc_t.cuda() conf_t = conf_t.cuda() # wrap targets # ç”¨Variableå°è£…loc_t, æ–°ç‰ˆæœ¬çš„ PyTorch æ— éœ€è¿™ä¹ˆåš, åªéœ€è¦å°† requires_grad å±æ€§è®¾ç½®ä¸º True å°±è¡Œäº† loc_t = Variable(loc_t, requires_grad=False) conf_t = Variable(conf_t, requires_grad=False) pos = conf_t 0 # ç­›é€‰å‡º 0 çš„boxä¸‹æ ‡(å¤§éƒ¨åˆ†éƒ½æ˜¯=0çš„) num_pos = pos.sum(dim=1, keepdim=True) # æ±‚å’Œ, å–å¾—æ»¡è¶³æ¡ä»¶çš„boxçš„æ•°é‡, [batch_size, num_gt_threshold] # Localization Loss (Smooth L1) # Shape: [batch,num_priors,4] # ä½ç½®(localization)æŸå¤±å‡½æ•°, ä½¿ç”¨ Smooth L1 å‡½æ•°æ±‚æŸå¤± # loc_data:[batch, num_priors, 4] # pos: [batch, num_priors] # pos_idx: [batch, num_priors, 4], å¤åˆ¶ä¸‹æ ‡æˆåæ ‡æ ¼å¼, ä»¥ä¾¿è·å–åæ ‡å€¼ pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data) loc_p = loc_data[pos_idx].view(-1, 4) # è·å–é¢„æµ‹ç»“æœå€¼ loc_t = loc_t[pos_idx].view(-1, 4) # è·å–gtå€¼ loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False) # è®¡ç®—æŸå¤± # Compute max conf across batch for hard negative mining # è®¡ç®—æœ€å¤§çš„ç½®ä¿¡åº¦, ä»¥è¿›è¡Œéš¾è´Ÿæ ·æœ¬æŒ–æ˜ # conf_data: [batch, num_priors, num_classes] # batch_conf: [batch, num_priors, num_classes] batch_conf = conf_data.view(-1, self.num_classes) # conf_t: [batch, num_priors] # loss_c: [batch*num_priors, 1], è®¡ç®—æ¯ä¸ªpriorboxé¢„æµ‹åçš„æŸå¤± loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1)) # éš¾è´Ÿæ ·æœ¬æŒ–æ˜, æŒ‰ç…§lossè¿›è¡Œæ’åº, å–lossæœ€å¤§çš„è´Ÿæ ·æœ¬å‚ä¸æ›´æ–° # Hard Negative Mining loss_c[pos] = 0 # filter out pos boxes for now # å°†æ‰€æœ‰çš„posä¸‹æ ‡çš„boxçš„lossç½®ä¸º0(posæŒ‡ç¤ºçš„æ˜¯æ­£æ ·æœ¬çš„ä¸‹æ ‡) # å°† loss_c çš„shape ä» [batch*num_priors, 1] è½¬æ¢æˆ [batch, num_priors] loss_c = loss_c.view(num, -1) # reshape # è¿›è¡Œé™åºæ’åº, å¹¶è·å–åˆ°æ’åºçš„ä¸‹æ ‡ _, loss_idx = loss_c.sort(1, descending=True) # å°†ä¸‹æ ‡è¿›è¡Œå‡åºæ’åº, å¹¶è·å–åˆ°ä¸‹æ ‡çš„ä¸‹æ ‡ _, idx_rank = loss_idx.sort(1) # num_pos: [batch, 1], ç»Ÿè®¡æ¯ä¸ªæ ·æœ¬ä¸­çš„objä¸ªæ•° num_pos = pos.long().sum(1, keepdim=True) # æ ¹æ®objçš„ä¸ªæ•°, ç¡®å®šè´Ÿæ ·æœ¬çš„ä¸ªæ•°(æ­£æ ·æœ¬çš„3å€) num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1) # è·å–åˆ°è´Ÿæ ·æœ¬çš„ä¸‹æ ‡ neg = idx_rank num_neg.expand_as(idx_rank) # è®¡ç®—åŒ…æ‹¬æ­£æ ·æœ¬å’Œè´Ÿæ ·æœ¬çš„ç½®ä¿¡åº¦æŸå¤± # pos: [batch, num_priors] # pos_idx: [batch, num_priors, num_classes] pos_idx = pos.unsqueeze(2).expand_as(conf_data) # neg: [batch, num_priors] # neg_idx: [batch, num_priors, num_classes] neg_idx = neg.unsqueeze(2).expand_as(conf_data) # æŒ‰ç…§pos_idxå’Œneg_idxæŒ‡ç¤ºçš„ä¸‹æ ‡ç­›é€‰å‚ä¸è®¡ç®—æŸå¤±çš„é¢„æµ‹æ•°æ® conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes) # æŒ‰ç…§pos_idxå’Œneg_idxç­›é€‰ç›®æ ‡æ•°æ® targets_weighted = conf_t[(pos+neg).gt(0)] # è®¡ç®—äºŒè€…çš„äº¤å‰ç†µ loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False) # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + Î±Lloc(x,l,g)) / N # å°†æŸå¤±å‡½æ•°å½’ä¸€åŒ–åè¿”å› N = num_pos.data.sum() loss_l /= N loss_c /= N return loss_l, loss_c â€”|â€” matchå‡½æ•°çš„è§£æ1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 | def match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx): # threshold: (float) ç¡®å®šæ˜¯å¦åŒ¹é…çš„äº¤å¹¶æ¯”é˜ˆå€¼ # truths: (tensor: [num_obj, 4]) å­˜å‚¨çœŸå® box çš„è¾¹æ¡†åæ ‡ # priors: (tensor: [num_priors, 4], å³[8732, 4]), å­˜å‚¨æ¨èæ¡†çš„åæ ‡, æ³¨æ„, æ­¤æ—¶çš„æ¡†æ˜¯ default box, è€Œä¸æ˜¯ SSD ç½‘ç»œé¢„æµ‹å‡ºæ¥çš„æ¡†çš„åæ ‡, é¢„æµ‹çš„ç»“æœå­˜å‚¨åœ¨ loc_dataä¸­, å…¶ shape ä¸º[num_obj, 8732, 4]. # variances: cfg['variance'], [0.1, 0.2], ç”¨äºå°†åæ ‡è½¬æ¢æˆæ–¹ä¾¿è®­ç»ƒçš„å½¢å¼(å‚è€ƒRCNNç³»åˆ—å¯¹è¾¹æ¡†åæ ‡çš„å¤„ç†) # labels: (tensor: [num_obj]), ä»£è¡¨äº†æ¯ä¸ªçœŸå® box å¯¹åº”çš„ç±»åˆ«çš„ç¼–å· # loc_t: (tensor: [batches, 8732, 4]), # conf_t: (tensor: [batches, 8732]), # idx: batches ä¸­å›¾ç‰‡çš„åºå·, æ ‡è¯†å½“å‰æ­£åœ¨å¤„ç†çš„ image åœ¨ batches ä¸­çš„åºå· # jaccard index overlaps = jaccard( # [A, B], è¿”å›ä»»æ„ä¸¤ä¸ªboxä¹‹é—´çš„äº¤å¹¶æ¯”, overlaps[i][j] ä»£è¡¨box_aä¸­çš„ç¬¬iä¸ªboxä¸box_bä¸­çš„ç¬¬jä¸ªboxä¹‹é—´çš„äº¤å¹¶æ¯”. truths, point_form(priors) ) # äºŒéƒ¨å›¾åŒ¹é…(Bipartite Matching) # [num_objs,1], å¾—åˆ°å¯¹äºæ¯ä¸ª gt box æ¥è¯´çš„åŒ¹é…åº¦æœ€é«˜çš„ prior box, å‰è€…å­˜å‚¨äº¤å¹¶æ¯”, åè€…å­˜å‚¨prior boxåœ¨num_priorsä¸­çš„ä½ç½® best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True) # keepdim=True, å› æ­¤shapeä¸º[num_objs,1] # [1,num_priors] best ground truth for each prior # [1, num_priors], å³[1,8732], åŒç†, å¾—åˆ°å¯¹äºæ¯ä¸ª prior box æ¥è¯´çš„åŒ¹é…åº¦æœ€é«˜çš„ gt box best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True) best_truth_idx.squeeze_(0) # ä¸Šé¢ç‰¹æ„ä¿ç•™äº†ç»´åº¦(keepdim=True), è¿™é‡Œåˆéƒ½æŠŠç»´åº¦ squeeze/reduce äº†, å®é™…ä¸Šåªéœ€ç”¨é»˜è®¤çš„ keepdim=False å°±å¯ä»¥è‡ªåŠ¨ squeeze/reduce ç»´åº¦. best_truth_overlap.squeeze_(0) best_prior_idx.squeeze_(1) best_prior_overlap.squeeze_(1) best_truth_overlap.index_fill_(0, best_prior_idx, 2) # ensure best prior # ç»´åº¦å‹ç¼©åå˜ä¸º[num_priors], best_prior_idx ç»´åº¦ä¸º[num_objs], # è¯¥è¯­å¥ä¼šå°†ä¸gt boxåŒ¹é…åº¦æœ€å¥½çš„prior box çš„äº¤å¹¶æ¯”ç½®ä¸º 2, ç¡®ä¿å…¶æœ€å¤§, ä»¥å…é˜²æ­¢æŸäº› gtbox æ²¡æœ‰åŒ¹é…çš„ priorbox. # å‡æƒ³ä¸€ç§æç«¯æƒ…å†µ, æ‰€æœ‰çš„priorboxä¸æŸä¸ªgtbox(æ ‡è®°ä¸ºG)çš„äº¤å¹¶æ¯”ä¸º1, è€Œå…¶ä»–gtboxåˆ†åˆ«æœ‰ä¸€ä¸ªäº¤å¹¶æ¯” # æœ€é«˜çš„priorbox, ä½†æ˜¯è‚¯å®šå°äº1(å› ä¸ºå…¶ä»–çš„gtboxä¸Gçš„äº¤å¹¶æ¯”è‚¯å®šå°äº1), è¿™æ ·ä¸€æ¥, å°±ä¼šä½¿å¾—æ‰€æœ‰ # çš„priorboxéƒ½ä¸GåŒ¹é…, ä¸ºäº†é˜²æ­¢è¿™ç§æƒ…å†µ, æˆ‘ä»¬å°†é‚£äº›å¯¹gtboxæ¥è¯´, å…·æœ‰æœ€é«˜äº¤å¹¶æ¯”çš„priorbox, # å¼ºåˆ¶è¿›è¡Œäº’ç›¸åŒ¹é…, å³ä»¤best_truth_idx[best_prior_idx[j]] = j, è¯¦ç»†è§ä¸‹é¢çš„forå¾ªç¯ # TODO refactor: index best_prior_idx with long tensor # ensure every gt matches with its prior of max overlap # æ³¨æ„!!: å› ä¸º gt box çš„æ•°é‡è¦è¿œè¿œå°‘äº prior box çš„æ•°é‡, å› æ­¤, åŒä¸€ä¸ª gt box ä¼šä¸å¤šä¸ª prior box åŒ¹é…. for j in range(best_prior_idx.size(0)): best_truth_idx[best_prior_idx[j]] = j # best_prior_idx[j] ä»£è¡¨ä¸box_açš„ç¬¬jä¸ªboxäº¤å¹¶æ¯”æœ€é«˜çš„ prior box çš„ä¸‹æ ‡, å°†ä¸è¯¥ gtbox # åŒ¹é…åº¦æœ€å¥½çš„ prior box çš„ä¸‹æ ‡æ”¹ä¸ºj, ç”±æ­¤,å®Œæˆäº†è¯¥ gtbox ä¸ç¬¬jä¸ª prior box çš„åŒ¹é…. # è¿™é‡Œçš„å¾ªç¯åªä¼šè¿›è¡Œnum_objæ¬¡, å‰©ä½™çš„åŒ¹é…ä¸º best_truth_idx ä¸­åŸæœ¬çš„å€¼. # è¿™é‡Œå¤„ç†çš„æƒ…å†µæ˜¯, priorboxä¸­ç¬¬iä¸ªboxä¸gtboxä¸­ç¬¬kä¸ªboxçš„äº¤å¹¶æ¯”æœ€é«˜, # å³ best_truth_idx[i]= k # ä½†æ˜¯å¯¹äºbest_prior_idx[k]æ¥è¯´, å®ƒå´ä¸priorboxçš„ç¬¬lä¸ªboxæœ‰ç€æœ€é«˜çš„äº¤å¹¶æ¯”, # å³best_prior_idx[k]=l # è€Œå¯¹äºgtboxçš„å¦ä¸€ä¸ªè¾¹æ¡†gtbox[j]æ¥è¯´, å®ƒä¸priorbox[i]çš„äº¤å¹¶æ¯”æœ€å¤§, # å³ä½†æ˜¯å¯¹äºbest_prior_idx[j] = i. # é‚£ä¹ˆ, æ­¤æ—¶, æˆ‘ä»¬å°±åº”è¯¥å°†best_truth_idx[i]= k ä¿®æ”¹æˆ best_truth_idx[i]= j. # å³ä»¤ priorbox[i] ä¸ gtbox[j]å¯¹åº”. # è¿™æ ·åšçš„åŸå› : é˜²æ­¢æŸä¸ªgtboxæ²¡æœ‰åŒ¹é…çš„ prior box. matches = truths[best_truth_idx] # Shape: [num_priors,4] # truths çš„shape ä¸º[num_objs, 4], è€Œbest_truth_idxæ˜¯ä¸€ä¸ªæŒ‡ç¤ºä¸‹æ ‡çš„åˆ—è¡¨, åˆ—è¡¨é•¿åº¦ä¸º 8732, # åˆ—è¡¨ä¸­çš„ä¸‹æ ‡èŒƒå›´ä¸º0~num_objs-1, ä»£è¡¨çš„æ˜¯ä¸æ¯ä¸ªpriorboxåŒ¹é…çš„gtboxçš„ä¸‹æ ‡ # ä¸Šé¢çš„è¡¨è¾¾å¼ä¼šè¿”å›ä¸€ä¸ªshapeä¸º [num_priors, 4], å³ [8732, 4] çš„tensor, ä»£è¡¨çš„å°±æ˜¯ä¸æ¯ä¸ªpriorboxåŒ¹é…çš„gtboxçš„åæ ‡å€¼. conf = labels[best_truth_idx] + 1 # ä¸ä¸Šé¢çš„è¯­å¥é“ç†å·®ä¸å¤š, è¿™é‡Œå¾—åˆ°çš„æ˜¯æ¯ä¸ªprior boxåŒ¹é…çš„ç±»åˆ«ç¼–å·, shape ä¸º[8732] conf[best_truth_overlap threshold] = 0 # å°†ä¸gtboxçš„äº¤å¹¶æ¯”å°äºé˜ˆå€¼çš„ç½®ä¸º0 , å³è®¤ä¸ºæ˜¯éç‰©ä½“æ¡† loc = encode(matches, priors, variances) # è¿”å›ç¼–ç åçš„ä¸­å¿ƒåæ ‡å’Œå®½é«˜. loc_t[idx] = loc # [num_priors,4] encoded offsets to learn # è®¾ç½®ç¬¬idxå¼ å›¾ç‰‡çš„gtç¼–ç åæ ‡ä¿¡æ¯ conf_t[idx] = conf # [num_priors] top class label for each prior è®¾ç½®ç¬¬idxå¼ å›¾ç‰‡çš„ç¼–å·ä¿¡æ¯.(å¤§äº0å³ä¸ºç‰©ä½“ç¼–å·, è®¤ä¸ºæœ‰ç‰©ä½“, å°äº0è®¤ä¸ºæ˜¯èƒŒæ™¯) â€”|â€” å‚è€ƒæ–‡ç« ï¼šhttps://blog.csdn.net/happyday_d/article/details/86021993https://hellozhaozheng.github.io/z_post/PyTorch-SSD","tags":["PyTorch"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"SqueezeNet","path":"/2019/08/20/20190820-squeezenet/","content":"è®ºæ–‡åœ°å€: SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and 0.5MB model sizeéå®˜æ–¹ä»£ç : pytorch ä»‹ç»è¿™ç¯‡æ–‡ç« æ˜¯DeepScalï¼ŒåŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡ï¼Œä»¥åŠæ–¯å¦ç¦å¤§å­¦åœ¨ICLR 2017å‘è¡¨çš„ä¸€ç¯‡æ–‡ç« ã€‚æ–‡ç« çš„ä¸»è¦ç›®çš„æ˜¯ä¸ºäº†å‹ç¼©æ¨¡å‹ï¼Œæé«˜è¿è¡Œé€Ÿåº¦ã€‚è¿™ç¯‡æ–‡ç« ä¸»è¦æå‡ºäº†SqueezeNet: ä½¿ç”¨å°‘é‡å‚æ•°ä¿æŒç²¾åº¦ã€‚ ç»“æ„è®¾è®¡ç­–ç•¥è¿™ç¯‡æ–‡ç« çš„é¦–è¦ç›®æ ‡æ˜¯åœ¨ä¿æŒå‡†ç¡®ç‡ çš„åŒæ—¶ï¼Œæœ‰å‡ ä¸ªå‚æ•°çš„CNNæ¶æ„ã€‚è¿™ç¯‡æ–‡ç« åœ¨è®¾è®¡CNNæ¶æ„çš„æ—¶å€™é‡‡å–äº†ä¸‰ä¸ªä¸»è¦ç­–ç•¥ã€‚è¿™ç¯‡æ–‡ç« çš„ä¸»è¦æ¨¡å— æ˜¯Fireæ¨¡å—ã€‚ ç”¨1x1çš„å·ç§¯æ ¸ä»£æ›¿3x3çš„å·ç§¯æ ¸ï¼Œä»è€Œå‡å°‘å‚æ•°é‡ã€‚1x1 å·ç§¯çš„å‚æ•°æ¯”3x3çš„å·ç§¯æ ¸å°‘äº† 9X. å‡å°‘3x3 å·ç§¯è¾“å…¥é€šé“çš„æ•°é‡ã€‚å‡è®¾æœ‰ä¸€ä¸ªå·ç§¯å±‚, å®ƒå®Œå…¨ç”±3x3 å·ç§¯ç»„æˆã€‚æ­¤å±‚ä¸­å‚æ•°çš„æ€»æ•°é‡ä¸ºï¼š(è¾“å…¥é€šé“æ•°) (è¿‡æ»¤å™¨æ•°) (3 * 3)ã€‚è¦åœ¨squeezeå±‚ä¸­å°†è¾“å…¥çš„é€šé“æ•°å‡å°‘ã€‚ åœ¨ç½‘ç»œä¸­å‡å°‘ä¸‹é‡‡æ ·(maxpooling)å®ç°, ä»¥ä¾¿å·ç§¯å±‚å…·æœ‰è¾ƒå¤§çš„ç‰¹å¾å›¾ã€‚ Fire ModuleFire Moduleæ˜¯å°†åŸæ¥ä¸€å±‚convå±‚å˜æˆä¸¤å±‚ï¼šsqueezeå±‚+expandå±‚ï¼Œå„è‡ªå¸¦ä¸ŠReluæ¿€æ´»å±‚ã€‚åœ¨squeezeå±‚é‡Œé¢å…¨æ˜¯1x1çš„å·ç§¯kernelï¼Œæ•°é‡è®°ä¸ºS11ï¼›åœ¨expandå±‚é‡Œé¢æœ‰1x1å’Œ3x3çš„å·ç§¯kernelï¼Œexpandå±‚ä¹‹åå°†1x1å’Œ3x3çš„å·ç§¯output feature mapsåœ¨channelç»´åº¦catã€‚ è‡ªå·±æ‰‹æ¨çš„ä¸€å¼ å›¾ï¼Œå­—æ¯”è¾ƒä¸‘ï¼Œä¹Ÿæ²¡æ—¶é—´é‡ç°å†™ä¸€ä¸‹ã€‚ fire moudleçš„pytorchä»£ç å¾ˆå¥‡æ€ªçš„æ˜¯è®ºæ–‡ä¸­ç”¨çš„æ˜¯3ä¸ª1x1ï¼Œä»¥åŠexpandç”¨çš„æ˜¯4ä¸ª1x1çš„å·ç§¯æ ¸å’Œ4ä¸ª 3x3çš„å·ç§¯æ ¸ï¼Œä½†æ˜¯pytrochç‰ˆæœ¬çš„ä»£ç å¹¶æ²¡æœ‰ä½“ç°å‡ºæ¥ã€‚ class fire(nn.Module): def __init__(self, inplanes,squeeze_planes, expand_planes): super(fire,self).__init__() self.conv1 = nn.Conv2d(inplanes,squeeze_planes, kernel_size=1, stride=1) self.bn1 = nn.BatchNorm2d(squeeze_planes) self.relu1 = nn.ReLU(inplace=True) self.conv2 = nn.Conv2d(squeeze_planes, expand_planes, kernel_size=1, stride=1) self.bn2 = nn.BatchNorm2d(expand_planes) self.conv3 = nn.Conv2d(squeeze_planes,expand_planes,kernel_size=3, stride=1,padding=1) self.bn3 = nn.BatchNorm2d(expand_planes) self.relu2 = nn.ReLU(inplace=True) # using MSR initialization for m in self.modules(): if isinstance(m,nn.Conv2d): n = m.kernel_size[0]*m.kernel_size[1]*m.in_channels m.weight.data.normal_(0,math.sqrt(2./n)) def forward(self,x): x = self.conv1(x) x = self.bn1(x) x = self.relu1(x) out1 = self.conv2(x) out1 = self.bn2(out1) out2 = self.conv3(x) out2 = self.bn3(out2) out = torch.cat([out1,out2],1) out = self.relu2(out) return out SqueezeNetçš„å…·ä½“ç½‘ç»œç»“æ„ å®éªŒç»“æœimagenetæ•°æ®ä¸Šæ¯”è¾ƒäº†alexnetï¼Œå¯ä»¥çœ‹åˆ°å‡†ç¡®ç‡å·®ä¸å¤šçš„æƒ…å†µä¸‹ï¼ŒsqueezeNetæ¨¡å‹å‚æ•°æ•°é‡æ˜¾è‘—é™ä½äº†ï¼ˆä¸‹è¡¨å€’æ•°ç¬¬ä¸‰è¡Œï¼‰ï¼Œå‚æ•°å‡å°‘50Xï¼›å¦‚æœå†åŠ ä¸Šdeep compressionæŠ€æœ¯ï¼Œå‹ç¼©æ¯”å¯ä»¥è¾¾åˆ°461Xï¼è¿˜æ˜¯ä¸é”™çš„ç»“æœã€‚ å‚è€ƒæ–‡ç« ï¼šhttps://blog.csdn.net/xbinworld/article/details/50897870","tags":["æ·±åº¦å­¦ä¹ "],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"SqueezeNet","path":"/2019/08/20/squeezenet/","content":"è®ºæ–‡åœ°å€: SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and 0.5MB model sizeéå®˜æ–¹ä»£ç : pytorch ä»‹ç»è¿™ç¯‡æ–‡ç« æ˜¯DeepScalï¼ŒåŠ å·å¤§å­¦ä¼¯å…‹åˆ©åˆ†æ ¡ï¼Œä»¥åŠæ–¯å¦ç¦å¤§å­¦åœ¨ICLR 2017å‘è¡¨çš„ä¸€ç¯‡æ–‡ç« ã€‚æ–‡ç« çš„ä¸»è¦ç›®çš„æ˜¯ä¸ºäº†å‹ç¼©æ¨¡å‹ï¼Œæé«˜è¿è¡Œé€Ÿåº¦ã€‚è¿™ç¯‡æ–‡ç« ä¸»è¦æå‡ºäº†SqueezeNet: ä½¿ç”¨å°‘é‡å‚æ•°ä¿æŒç²¾åº¦ã€‚ ç»“æ„è®¾è®¡ç­–ç•¥è¿™ç¯‡æ–‡ç« çš„é¦–è¦ç›®æ ‡æ˜¯åœ¨ä¿æŒå‡†ç¡®ç‡ çš„åŒæ—¶ï¼Œæœ‰å‡ ä¸ªå‚æ•°çš„CNNæ¶æ„ã€‚è¿™ç¯‡æ–‡ç« åœ¨è®¾è®¡CNNæ¶æ„çš„æ—¶å€™é‡‡å–äº†ä¸‰ä¸ªä¸»è¦ç­–ç•¥ã€‚è¿™ç¯‡æ–‡ç« çš„ä¸»è¦æ¨¡å— æ˜¯Fireæ¨¡å—ã€‚ ç”¨1x1çš„å·ç§¯æ ¸ä»£æ›¿3x3çš„å·ç§¯æ ¸ï¼Œä»è€Œå‡å°‘å‚æ•°é‡ã€‚1x1 å·ç§¯çš„å‚æ•°æ¯”3x3çš„å·ç§¯æ ¸å°‘äº† 9X. å‡å°‘3x3 å·ç§¯è¾“å…¥é€šé“çš„æ•°é‡ã€‚å‡è®¾æœ‰ä¸€ä¸ªå·ç§¯å±‚, å®ƒå®Œå…¨ç”±3x3 å·ç§¯ç»„æˆã€‚æ­¤å±‚ä¸­å‚æ•°çš„æ€»æ•°é‡ä¸ºï¼š(è¾“å…¥é€šé“æ•°) (è¿‡æ»¤å™¨æ•°) (3 * 3)ã€‚è¦åœ¨squeezeå±‚ä¸­å°†è¾“å…¥çš„é€šé“æ•°å‡å°‘ã€‚ åœ¨ç½‘ç»œä¸­å‡å°‘ä¸‹é‡‡æ ·(maxpooling)å®ç°, ä»¥ä¾¿å·ç§¯å±‚å…·æœ‰è¾ƒå¤§çš„ç‰¹å¾å›¾ã€‚ Fire ModuleFire Moduleæ˜¯å°†åŸæ¥ä¸€å±‚convå±‚å˜æˆä¸¤å±‚ï¼šsqueezeå±‚+expandå±‚ï¼Œå„è‡ªå¸¦ä¸ŠReluæ¿€æ´»å±‚ã€‚åœ¨squeezeå±‚é‡Œé¢å…¨æ˜¯1x1çš„å·ç§¯kernelï¼Œæ•°é‡è®°ä¸ºS11ï¼›åœ¨expandå±‚é‡Œé¢æœ‰1x1å’Œ3x3çš„å·ç§¯kernelï¼Œexpandå±‚ä¹‹åå°†1x1å’Œ3x3çš„å·ç§¯output feature mapsåœ¨channelç»´åº¦catã€‚ è‡ªå·±æ‰‹æ¨çš„ä¸€å¼ å›¾ï¼Œå­—æ¯”è¾ƒä¸‘ï¼Œä¹Ÿæ²¡æ—¶é—´é‡ç°å†™ä¸€ä¸‹ã€‚ fire moudleçš„pytorchä»£ç å¾ˆå¥‡æ€ªçš„æ˜¯è®ºæ–‡ä¸­ç”¨çš„æ˜¯3ä¸ª1x1ï¼Œä»¥åŠexpandç”¨çš„æ˜¯4ä¸ª1x1çš„å·ç§¯æ ¸å’Œ4ä¸ª 3x3çš„å·ç§¯æ ¸ï¼Œä½†æ˜¯pytrochç‰ˆæœ¬çš„ä»£ç å¹¶æ²¡æœ‰ä½“ç°å‡ºæ¥ã€‚ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 | class fire(nn.Module): def __init__(self, inplanes,squeeze_planes, expand_planes): super(fire,self).__init__() self.conv1 = nn.Conv2d(inplanes,squeeze_planes, kernel_size=1, stride=1) self.bn1 = nn.BatchNorm2d(squeeze_planes) self.relu1 = nn.ReLU(inplace=True) self.conv2 = nn.Conv2d(squeeze_planes, expand_planes, kernel_size=1, stride=1) self.bn2 = nn.BatchNorm2d(expand_planes) self.conv3 = nn.Conv2d(squeeze_planes,expand_planes,kernel_size=3, stride=1,padding=1) self.bn3 = nn.BatchNorm2d(expand_planes) self.relu2 = nn.ReLU(inplace=True) # using MSR initialization for m in self.modules(): if isinstance(m,nn.Conv2d): n = m.kernel_size[0]*m.kernel_size[1]*m.in_channels m.weight.data.normal_(0,math.sqrt(2./n)) def forward(self,x): x = self.conv1(x) x = self.bn1(x) x = self.relu1(x) out1 = self.conv2(x) out1 = self.bn2(out1) out2 = self.conv3(x) out2 = self.bn3(out2) out = torch.cat([out1,out2],1) out = self.relu2(out) return out â€”|â€” SqueezeNetçš„å…·ä½“ç½‘ç»œç»“æ„ å®éªŒç»“æœimagenetæ•°æ®ä¸Šæ¯”è¾ƒäº†alexnetï¼Œå¯ä»¥çœ‹åˆ°å‡†ç¡®ç‡å·®ä¸å¤šçš„æƒ…å†µä¸‹ï¼ŒsqueezeNetæ¨¡å‹å‚æ•°æ•°é‡æ˜¾è‘—é™ä½äº†ï¼ˆä¸‹è¡¨å€’æ•°ç¬¬ä¸‰è¡Œï¼‰ï¼Œå‚æ•°å‡å°‘50Xï¼›å¦‚æœå†åŠ ä¸Šdeep compressionæŠ€æœ¯ï¼Œå‹ç¼©æ¯”å¯ä»¥è¾¾åˆ°461Xï¼è¿˜æ˜¯ä¸é”™çš„ç»“æœã€‚ å‚è€ƒæ–‡ç« ï¼šhttps://blog.csdn.net/xbinworld/article/details/50897870","tags":["PyTorch"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"å¤šå±‚æ„ŸçŸ¥æœºçš„åå‘ä¼ æ’­","path":"/2019/08/19/20190819-MLP-Back-Propagation/","content":"å…¨è¿æ¥ç¥ç»ç½‘ç»œæ˜¯å½¢å¼ä¸Šæœ€ç®€å•çš„ç¥ç»ç½‘ç»œï¼Œåå‘ä¼ æ’­ç®—æ³•æ˜¯ä¸€ç§å¸¸ç”¨çš„è®­ç»ƒç¥ç»ç½‘ç»œçš„ç®—æ³•ï¼Œç†è§£å…¨è¿æ¥ç¥ç»ç½‘ç»œä¸­çš„åå‘ä¼ æ’­ç®—æ³•æ˜¯ç†è§£å…¶ä»–æ›´åŠ å¤æ‚ç½‘ç»œä¸­åå‘ä¼ æ’­ç®—æ³•çš„é‡è¦åŸºç¡€ã€‚ å‚è€ƒé“¾æ¥ï¼š https://zhuanlan.zhihu.com/p/61863634","tags":["æ·±åº¦å­¦ä¹ "],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"MobileNets","path":"/2019/08/19/20190819-MobileNets/","content":"è®ºæ–‡åœ°å€:MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications éå®˜æ–¹ä»£ç :pytorch/models å‰è¨€è¿™ç¯‡æ–‡ç« æ˜¯è°·æ­Œåœ¨2017é’ˆå¯¹æ‰‹æœºç­‰åµŒå…¥å¼è®¾å¤‡æå‡ºçš„ä¸€ç§è½»é‡çº§æ·±å±‚ç½‘ç»œï¼Œè¿™ç¯‡è®ºæ–‡ä¸»è¦çš„è´¡çŒ®ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§æ·±åº¦å¯åˆ†ç¦»å·ç§¯ã€‚ ä¸»è¦è§£å†³çš„é—®é¢˜æ˜¯æ³¨é‡ä¼˜åŒ–å»¶è¿Ÿï¼ŒåŒæ—¶ä¹Ÿå…¼é¡¾äº†æ¨¡å‹çš„å¤§å°ï¼Œä¸åƒæœ‰äº›æ¨¡å‹è™½ç„¶å‚æ•°é‡æ¯”è¾ƒå°ï¼Œä½†æ˜¯é€Ÿåº¦ä¹Ÿæ˜¯æ…¢çš„å¯ä»¥ã€‚ MobileNetsä½¿ç”¨äº†å¤§é‡çš„3 Ã— 3çš„å·ç§¯æ ¸ï¼Œæå¤§åœ°å‡å°‘äº†è®¡ç®—é‡ï¼ˆ1/8åˆ°1/9ä¹‹é—´ï¼‰ï¼ŒåŒæ—¶å‡†ç¡®ç‡ä¸‹é™çš„å¾ˆå°‘ï¼Œç›¸æ¯”å…¶ä»–çš„æ–¹æ³•ç¡®æœ‰ä¼˜åŠ¿ã€‚ æ·±åº¦å¯åˆ†ç¦»å·ç§¯ç¤ºä¾‹ æ¨¡å‹ç»“æ„å’Œè®­ç»ƒMobileNetsç»“æ„å»ºç«‹åœ¨ä¸Šè¿°æ·±åº¦å¯åˆ†è§£å·ç§¯ä¸­ï¼ˆåªæœ‰ç¬¬ä¸€å±‚æ˜¯æ ‡å‡†å·ç§¯ï¼‰ã€‚è¯¥ç½‘ç»œå…è®¸æˆ‘ä»¬æ¢ç´¢ç½‘ç»œæ‹“æ‰‘ï¼Œæ‰¾åˆ°ä¸€ä¸ªé€‚åˆçš„è‰¯å¥½ç½‘ç»œã€‚å…¶å…·ä½“æ¶æ„åœ¨è¡¨1è¯´æ˜ã€‚é™¤äº†æœ€åçš„å…¨è¿æ¥å±‚ï¼Œæ‰€æœ‰å±‚åé¢è·Ÿäº†batchnormå’ŒReLUï¼Œæœ€ç»ˆè¾“å…¥åˆ°softmaxè¿›è¡Œåˆ†ç±»ã€‚å›¾3å¯¹æ¯”äº†æ ‡å‡†å·ç§¯å’Œåˆ†è§£å·ç§¯çš„ç»“æ„ï¼ŒäºŒè€…éƒ½é™„å¸¦äº†BNå’ŒReLUå±‚ã€‚æŒ‰ç…§ä½œè€…çš„è®¡ç®—æ–¹æ³•ï¼ŒMobileNetsæ€»å…±28å±‚ï¼ˆ1 + 2 Ã— 13 + 1 = 28ï¼‰","tags":["æ·±åº¦å­¦ä¹ "],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"åˆ†æ²»ç®—æ³•","path":"/2019/08/19/20190819-åˆ†æ²»ç®—æ³•/","content":"é—®é¢˜ï¼š ç»™å®šä¸€ç»„æ•°ï¼Œè¦æ±‚ä»ä¸­æ‰¾å‡ºç¬¬kå°çš„å…ƒç´ ã€‚åˆ†æï¼šè¿™é‡Œé€šè¿‡å¿«é€Ÿæ’åºç®—æ³•æ¥è§£å†³æ¬¡é—®é¢˜ã€‚è®°ä¸€è¶Ÿå¿«é€Ÿæ’åºåï¼Œå·¦å­é›†ä¸­çš„å…ƒç´ ä¸ªæ•°ä¸ºnleftï¼Œåˆ™é€‰æ‹©é—®é¢˜ï¼Œå¯èƒ½æ˜¯ä¸€ä¸‹å‡ ç§æƒ…å†µä¹‹ä¸€ï¼š nleftç­‰äºk-1ï¼Œåˆ™æ¢çº½å€¼å³ä¸ºæ‰€æ±‚ï¼› nleftå¤§äºk-1,åˆ™ç»§ç»­åœ¨å·¦å­æ ‘ä¸­æ‰¾ï¼› nleftå°äºk-1,åˆ™ç»§ç»­åœ¨å³å­é›†ä¸­æ‰¾C++ä»£ç å®ç° // t5.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚//#include \"stdafx.h\"#include \"stdafx.h\"#includestdio.h#includestdlib.h#includetime.h#includemath.h#includestring.hint quickSelect(int a[], int l, int r, int k){int p = rand() % (r - l + 1) + l;int pivot = a[p];{int t = a[p]; a[p] = a[r]; a[r] = t; }int i = l,j = r;while (i j){while (i ja[i] pivot) i++;if (i j) {a[j] = a[i];j--;}while (i ja[i] pivot) j--;if (i j) {a[i] = a[j];i++;}}a[i] = pivot;p = i;if (i - l + 1 == k) return a[i];// j + 1, right, k - (j - left + 1)1)if (i - l + 1 k) return quickSelect(a, i+1, r, k-i+l-1);else return quickSelect(a, l, i-1, k);}int main(){int a[] = { 1,4,54,8,3,7,45,58,27,8,25,26,21,12 };printf_s(\"%d \", quickSelect(a, 0, 14, 6));system(\"pause\");return 0;}","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"MobileNetsV2","path":"/2019/08/19/20190819-MobileNetsV2/","content":"è®ºæ–‡åœ°å€:MobileNetV2: Inverted Residuals and Linear Bottlenecks éå®˜æ–¹ä»£ç :pytorch ä»‹ç»è¿™ç¯‡æ–‡ç« æ˜¯è°·æ­Œåœ¨2019æå‡ºæ¥çš„æ–‡ç« åœ¨MobileNets åŸºç¡€ä¸Šåšçš„æ”¹è¿›ã€‚ æ·±åº¦å¯åˆ†ç¦»å·ç§¯ç¤ºä¾‹ é¦–å…ˆåœ¨Xception ä¸­è¢«å¹¿æ³›ä½¿ç”¨ å¥½å¤„ï¼š ç†è®ºä¸Šå¯ä»¥æˆå€çš„å‡å°‘å·ç§¯å±‚çš„æ—¶é—´å¤æ‚åº¦å’Œç©ºé—´å¤æ‚åº¦ æ–‡ç« å†…å®¹ä¸MobileNets çš„å¯¹æ¯” ç›¸åŒç‚¹ éƒ½é‡‡ç”¨ Depth-wise (DW) å·ç§¯æ­é… Point-wise (PW) å·ç§¯çš„æ–¹å¼æ¥æç‰¹å¾ ä¸åŒç‚¹ Linear Bottleneck V2 åœ¨ DW å·ç§¯ä¹‹å‰æ–°åŠ äº†ä¸€ä¸ª PW å·ç§¯ã€‚ DWå·ç§¯ç”±äºæœ¬èº«çš„è®¡ç®—ç‰¹æ€§ä¸èƒ½æ”¹å˜é€šé“æ•°çš„èƒ½åŠ›ã€‚è‹¥é€šé“æ•°å¾ˆå°‘çš„è¯ï¼ŒDWåœ¨æå–åœ°ä½çº¬ç‰¹å¾ï¼Œæ•ˆæœå¯èƒ½å¹¶ä¸ä¼šå¥½ã€‚ åœ¨æ¯ä¸ªDWä¹‹å‰ï¼Œå¢åŠ äº†PWç”¨äºå‡ç»´ï¼Œè¿™æ ·DWå¯ä»¥æ›´å¥½çš„æå–ç‰¹å¾ V2 å»æ‰äº†ç¬¬äºŒä¸ª PW çš„æ¿€æ´»å‡½æ•° æ¿€æ´»å‡½æ•°åœ¨é«˜ç»´ç©ºé—´èƒ½å¤Ÿæœ‰æ•ˆçš„å¢åŠ éçº¿æ€§ï¼Œè€Œåœ¨ä½ç»´ç©ºé—´æ—¶åˆ™ä¼šç ´åç‰¹å¾ ç¬¬äºŒä¸ª PW çš„ä¸»è¦åŠŸèƒ½å°±æ˜¯é™ç»´ ä¸ResNetçš„å¯¹æ¯” ç›¸åŒç‚¹ MobileNet V2 å€Ÿé‰´ ResNetï¼Œéƒ½é‡‡ç”¨äº† 1x1-3x3-1x1çš„æ¨¡å¼ MobileNet V2 å€Ÿé‰´ ResNetï¼ŒåŒæ ·ä½¿ç”¨ Shortcut å°†è¾“å‡ºä¸è¾“å…¥ç›¸åŠ  ä¸åŒç‚¹ Inverted Residual Block ResNet ä½¿ç”¨ æ ‡å‡†å·ç§¯ æç‰¹å¾ï¼ŒMobileNet å§‹ç»ˆä½¿ç”¨ DWå·ç§¯ æç‰¹å¾ ResNet å…ˆé™ç»´ (0.25å€)ã€å·ç§¯ã€å†å‡ç»´ï¼Œè€Œ MobileNet V2 åˆ™æ˜¯ å…ˆå‡ç»´ (6å€)ã€å·ç§¯ã€å†é™ç»´ã€‚ç›´è§‚çš„å½¢è±¡ä¸Šæ¥çœ‹ï¼ŒResNet çš„å¾®ç»“æ„æ˜¯æ²™æ¼å½¢ï¼Œè€Œ MobileNet V2 åˆ™æ˜¯çººé”¤å½¢ï¼Œåˆšå¥½ç›¸åã€‚å› æ­¤è®ºæ–‡ä½œè€…å°† MobileNet V2 çš„ç»“æ„ç§°ä¸º Inverted Residual Blockã€‚ä½¿ç”¨DWå·ç§¯è€Œä½œçš„é€‚é…ï¼Œç‰¹å¾æå–èƒ½å¤Ÿåœ¨é«˜ç»´è¿›è¡Œ ç»“è®º MobileNets ä¸MobileNets V2åœ¨æ¨¡å‹ç»“æ„ä¸Šçš„å¯¹æ¯” MobileNetsV2 çš„å·ç§¯å±‚æ•°æ¯”V1è¦å¤šï¼Œä½†æ˜¯æ—¶é—´å¤æ‚åº¦ï¼Œä»¥åŠç©ºé—´å¤æ‚åº¦ï¼Œä»¥åŠåœ¨cpuä¸Šçš„æ¨ç†æ—¶é—´è¦è¿œè¿œä¼˜äºMobileNets å‚è€ƒæ–‡ç« ï¼šhttps://zhuanlan.zhihu.com/p/33075914","tags":["æ·±åº¦å­¦ä¹ "],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"å¤šå±‚æ„ŸçŸ¥æœºçš„åå‘ä¼ æ’­","path":"/2019/08/19/MLP-Back-Propagation/","content":"å…¨è¿æ¥ç¥ç»ç½‘ç»œæ˜¯å½¢å¼ä¸Šæœ€ç®€å•çš„ç¥ç»ç½‘ç»œï¼Œåå‘ä¼ æ’­ç®—æ³•æ˜¯ä¸€ç§å¸¸ç”¨çš„è®­ç»ƒç¥ç»ç½‘ç»œçš„ç®—æ³•ï¼Œç†è§£å…¨è¿æ¥ç¥ç»ç½‘ç»œä¸­çš„åå‘ä¼ æ’­ç®—æ³•æ˜¯ç†è§£å…¶ä»–æ›´åŠ å¤æ‚ç½‘ç»œä¸­åå‘ä¼ æ’­ç®—æ³•çš„é‡è¦åŸºç¡€ã€‚ å‚è€ƒé“¾æ¥ï¼š https://zhuanlan.zhihu.com/p/61863634","tags":["ç¼–ç¨‹"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"MobileNets","path":"/2019/08/19/MobileNets/","content":"è®ºæ–‡åœ°å€:MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications éå®˜æ–¹ä»£ç :pytorch/models å‰è¨€è¿™ç¯‡æ–‡ç« æ˜¯è°·æ­Œåœ¨2017é’ˆå¯¹æ‰‹æœºç­‰åµŒå…¥å¼è®¾å¤‡æå‡ºçš„ä¸€ç§è½»é‡çº§æ·±å±‚ç½‘ç»œï¼Œè¿™ç¯‡è®ºæ–‡ä¸»è¦çš„è´¡çŒ®ç‚¹åœ¨äºæå‡ºäº†ä¸€ç§æ·±åº¦å¯åˆ†ç¦»å·ç§¯ã€‚ ä¸»è¦è§£å†³çš„é—®é¢˜æ˜¯æ³¨é‡ä¼˜åŒ–å»¶è¿Ÿï¼ŒåŒæ—¶ä¹Ÿå…¼é¡¾äº†æ¨¡å‹çš„å¤§å°ï¼Œä¸åƒæœ‰äº›æ¨¡å‹è™½ç„¶å‚æ•°é‡æ¯”è¾ƒå°ï¼Œä½†æ˜¯é€Ÿåº¦ä¹Ÿæ˜¯æ…¢çš„å¯ä»¥ã€‚ MobileNetsä½¿ç”¨äº†å¤§é‡çš„3 Ã— 3çš„å·ç§¯æ ¸ï¼Œæå¤§åœ°å‡å°‘äº†è®¡ç®—é‡ï¼ˆ1/8åˆ°1/9ä¹‹é—´ï¼‰ï¼ŒåŒæ—¶å‡†ç¡®ç‡ä¸‹é™çš„å¾ˆå°‘ï¼Œç›¸æ¯”å…¶ä»–çš„æ–¹æ³•ç¡®æœ‰ä¼˜åŠ¿ã€‚ æ·±åº¦å¯åˆ†ç¦»å·ç§¯ç¤ºä¾‹ æ¨¡å‹ç»“æ„å’Œè®­ç»ƒMobileNetsç»“æ„å»ºç«‹åœ¨ä¸Šè¿°æ·±åº¦å¯åˆ†è§£å·ç§¯ä¸­ï¼ˆåªæœ‰ç¬¬ä¸€å±‚æ˜¯æ ‡å‡†å·ç§¯ï¼‰ã€‚è¯¥ç½‘ç»œå…è®¸æˆ‘ä»¬æ¢ç´¢ç½‘ç»œæ‹“æ‰‘ï¼Œæ‰¾åˆ°ä¸€ä¸ªé€‚åˆçš„è‰¯å¥½ç½‘ç»œã€‚å…¶å…·ä½“æ¶æ„åœ¨è¡¨1è¯´æ˜ã€‚é™¤äº†æœ€åçš„å…¨è¿æ¥å±‚ï¼Œæ‰€æœ‰å±‚åé¢è·Ÿäº†batchnormå’ŒReLUï¼Œæœ€ç»ˆè¾“å…¥åˆ°softmaxè¿›è¡Œåˆ†ç±»ã€‚å›¾3å¯¹æ¯”äº†æ ‡å‡†å·ç§¯å’Œåˆ†è§£å·ç§¯çš„ç»“æ„ï¼ŒäºŒè€…éƒ½é™„å¸¦äº†BNå’ŒReLUå±‚ã€‚æŒ‰ç…§ä½œè€…çš„è®¡ç®—æ–¹æ³•ï¼ŒMobileNetsæ€»å…±28å±‚ï¼ˆ1 + 2 Ã— 13 + 1 = 28ï¼‰","tags":["PyTorch"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"MobileNetsV2","path":"/2019/08/19/MobileNetsV2/","content":"è®ºæ–‡åœ°å€:MobileNetV2: Inverted Residuals and Linear Bottlenecks éå®˜æ–¹ä»£ç :pytorch ä»‹ç»è¿™ç¯‡æ–‡ç« æ˜¯è°·æ­Œåœ¨2019æå‡ºæ¥çš„æ–‡ç« åœ¨MobileNets åŸºç¡€ä¸Šåšçš„æ”¹è¿›ã€‚ æ·±åº¦å¯åˆ†ç¦»å·ç§¯ç¤ºä¾‹ é¦–å…ˆåœ¨Xception ä¸­è¢«å¹¿æ³›ä½¿ç”¨ å¥½å¤„ï¼š ç†è®ºä¸Šå¯ä»¥æˆå€çš„å‡å°‘å·ç§¯å±‚çš„æ—¶é—´å¤æ‚åº¦å’Œç©ºé—´å¤æ‚åº¦ æ–‡ç« å†…å®¹ä¸MobileNets çš„å¯¹æ¯” ç›¸åŒç‚¹ éƒ½é‡‡ç”¨ Depth-wise (DW) å·ç§¯æ­é… Point-wise (PW) å·ç§¯çš„æ–¹å¼æ¥æç‰¹å¾ ä¸åŒç‚¹ Linear Bottleneck V2 åœ¨ DW å·ç§¯ä¹‹å‰æ–°åŠ äº†ä¸€ä¸ª PW å·ç§¯ã€‚ DWå·ç§¯ç”±äºæœ¬èº«çš„è®¡ç®—ç‰¹æ€§ä¸èƒ½æ”¹å˜é€šé“æ•°çš„èƒ½åŠ›ã€‚è‹¥é€šé“æ•°å¾ˆå°‘çš„è¯ï¼ŒDWåœ¨æå–åœ°ä½çº¬ç‰¹å¾ï¼Œæ•ˆæœå¯èƒ½å¹¶ä¸ä¼šå¥½ã€‚ åœ¨æ¯ä¸ªDWä¹‹å‰ï¼Œå¢åŠ äº†PWç”¨äºå‡ç»´ï¼Œè¿™æ ·DWå¯ä»¥æ›´å¥½çš„æå–ç‰¹å¾ V2 å»æ‰äº†ç¬¬äºŒä¸ª PW çš„æ¿€æ´»å‡½æ•° æ¿€æ´»å‡½æ•°åœ¨é«˜ç»´ç©ºé—´èƒ½å¤Ÿæœ‰æ•ˆçš„å¢åŠ éçº¿æ€§ï¼Œè€Œåœ¨ä½ç»´ç©ºé—´æ—¶åˆ™ä¼šç ´åç‰¹å¾ ç¬¬äºŒä¸ª PW çš„ä¸»è¦åŠŸèƒ½å°±æ˜¯é™ç»´ ä¸ResNetçš„å¯¹æ¯” ç›¸åŒç‚¹ MobileNet V2 å€Ÿé‰´ ResNetï¼Œéƒ½é‡‡ç”¨äº† 1x1-3x3-1x1çš„æ¨¡å¼ MobileNet V2 å€Ÿé‰´ ResNetï¼ŒåŒæ ·ä½¿ç”¨ Shortcut å°†è¾“å‡ºä¸è¾“å…¥ç›¸åŠ  ä¸åŒç‚¹ Inverted Residual Block ResNet ä½¿ç”¨ æ ‡å‡†å·ç§¯ æç‰¹å¾ï¼ŒMobileNet å§‹ç»ˆä½¿ç”¨ DWå·ç§¯ æç‰¹å¾ ResNet å…ˆé™ç»´ (0.25å€)ã€å·ç§¯ã€å†å‡ç»´ï¼Œè€Œ MobileNet V2 åˆ™æ˜¯ å…ˆå‡ç»´ (6å€)ã€å·ç§¯ã€å†é™ç»´ã€‚ç›´è§‚çš„å½¢è±¡ä¸Šæ¥çœ‹ï¼ŒResNet çš„å¾®ç»“æ„æ˜¯æ²™æ¼å½¢ï¼Œè€Œ MobileNet V2 åˆ™æ˜¯çººé”¤å½¢ï¼Œåˆšå¥½ç›¸åã€‚å› æ­¤è®ºæ–‡ä½œè€…å°† MobileNet V2 çš„ç»“æ„ç§°ä¸º Inverted Residual Blockã€‚ä½¿ç”¨DWå·ç§¯è€Œä½œçš„é€‚é…ï¼Œç‰¹å¾æå–èƒ½å¤Ÿåœ¨é«˜ç»´è¿›è¡Œ ç»“è®º MobileNets ä¸MobileNets V2åœ¨æ¨¡å‹ç»“æ„ä¸Šçš„å¯¹æ¯” MobileNetsV2 çš„å·ç§¯å±‚æ•°æ¯”V1è¦å¤šï¼Œä½†æ˜¯æ—¶é—´å¤æ‚åº¦ï¼Œä»¥åŠç©ºé—´å¤æ‚åº¦ï¼Œä»¥åŠåœ¨cpuä¸Šçš„æ¨ç†æ—¶é—´è¦è¿œè¿œä¼˜äºMobileNets å‚è€ƒæ–‡ç« ï¼šhttps://zhuanlan.zhihu.com/p/33075914","tags":["PyTorch"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"åˆ†æ²»ç®—æ³•","path":"/2019/08/19/åˆ†æ²»ç®—æ³•/","content":"é—®é¢˜ï¼š ç»™å®šä¸€ç»„æ•°ï¼Œè¦æ±‚ä»ä¸­æ‰¾å‡ºç¬¬kå°çš„å…ƒç´ ã€‚åˆ†æï¼šè¿™é‡Œé€šè¿‡å¿«é€Ÿæ’åºç®—æ³•æ¥è§£å†³æ¬¡é—®é¢˜ã€‚è®°ä¸€è¶Ÿå¿«é€Ÿæ’åºåï¼Œå·¦å­é›†ä¸­çš„å…ƒç´ ä¸ªæ•°ä¸ºnleftï¼Œåˆ™é€‰æ‹©é—®é¢˜ï¼Œå¯èƒ½æ˜¯ä¸€ä¸‹å‡ ç§æƒ…å†µä¹‹ä¸€ï¼š nleftç­‰äºk-1ï¼Œåˆ™æ¢çº½å€¼å³ä¸ºæ‰€æ±‚ï¼› nleftå¤§äºk-1,åˆ™ç»§ç»­åœ¨å·¦å­æ ‘ä¸­æ‰¾ï¼› nleftå°äºk-1,åˆ™ç»§ç»­åœ¨å³å­é›†ä¸­æ‰¾C++ä»£ç å®ç° 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 | // t5.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚ // #include \"stdafx.h\" #include \"stdafx.h\" #includestdio.h #includestdlib.h #includetime.h #includemath.h #includestring.h int quickSelect(int a[], int l, int r, int k) { int p = rand() % (r - l + 1) + l; int pivot = a[p]; {int t = a[p]; a[p] = a[r]; a[r] = t; } int i = l,j = r; while (i j) { while (i ja[i] pivot) i++; if (i j) { a[j] = a[i]; j--; } while (i ja[i] pivot) j--; if (i j) { a[i] = a[j]; i++; } } a[i] = pivot; p = i; if (i - l + 1 == k) return a[i]; // j + 1, right, k - (j - left + 1)1) if (i - l + 1 k) return quickSelect(a, i+1, r, k-i+l-1); else return quickSelect(a, l, i-1, k); } int main() { int a[] = { 1,4,54,8,3,7,45,58,27,8,25,26,21,12 }; printf_s(\"%d \", quickSelect(a, 0, 14, 6)); system(\"pause\"); return 0; } â€”|â€”","tags":["C++"],"categories":["ç®—æ³•"]},{"title":"ç®—æ³•ç«èµ›å…¥é—¨ç»å…¸ç¬¬å››ç« ","path":"/2019/08/18/20190818-algorithm4/","content":"// t4.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚//#include \"stdafx.h\"//ç´ æ•°åˆ¤æ–­æ–¹æ³•2#includestdio.h#includestdlib.h#includetime.h#includemath.h#includestring.h#define Max 1000000#define maxn 100int left, chance; //è¿˜éœ€è¦leftä½ç½®ï¼Œé”™chanceä¹‹åå°±ä¼šè¾“char s[maxn], s2[maxn]; //ç­”æ¡ˆæ˜¯å­—ç¬¦ä¸²s, ç©å®¶çŒœçš„å­—æ¯åºåˆ—æ˜¯s2int win, lose; // win=1 è¡¨ç¤ºå·²ç»èµ¢äº†; lose=1 è¡¨ç¤ºå·²ç»è¾“äº†long fac(int n){\tif (0 == n || 1 == n)\t{ return 1;\t}\telse\t{ return fac(n - 1)*n;\t}}// éé€’å½’çš„é˜¶ä¹˜æ–¹æ³•long fact(int n){\tlong iRes = 1;\tfor (int i = 1; i = n; i++)\t{ iRes *= i;\t}\treturn iRes;}int combination(int m, int n){\tlong iRes = fac(n) / (fac(m)*fac(n - m));\t// long iRes = fact(n) /(fac(m)*fac(n-m));\tprintf_s(\"%ld %.2lf \", iRes, (double)clock() / CLOCKS_PER_SEC);\treturn iRes;}// åˆ½å­æ‰‹æ¸¸æˆ---guesså‡½æ•°void guess(char ch){\tint bad = 1;\tfor (int i = 0; i strlen(s); i++) if (s[i] == ch) { left--; s[i] = ' '; bad = 0; }\tif (bad) --chance;\tif (!chance) lose = 1;\tif (!left) win = 1;}int main(){\t/*\tå…³é”®:\t1 ç”¨ç´ æ•°ç­›é€‰æ³•å…ˆé¢„å¤„ç†ï¼Œé»˜è®¤åˆšå¼€å§‹å…¨ä¸ºç´ æ•°ï¼Œç„¶åå¯¹ç´ æ•°çš„å€æ•°æ ‡è®°ä¸ºéç´ æ•°ï¼Œ for(int j = i*i ; j = 10000 ; j += i){iPrimeArr[j] = 1;}\t2 é€šè¿‡å¼€æ ¹å·åˆ¤æ–­ç´ æ•°æ—¶ï¼Œå¯ä»¥ç”¨floor,æ³¨æ„æµ®ç‚¹æ•°åŠ ä¸Š0.5ï¼Œint iRadical = floor(sqrt(n*1.0) + 0.5);\t3 å¯ä»¥ç”¨assert()å¯¹è¾“å…¥çš„åˆæ³•æ€§è¿›è¡Œæ ¡éªŒï¼Œassert(n = 5 n = 10000);void assert(int exp),å¦‚æœè¡¨è¾¾å¼çš„å€¼ä¸º0åˆ™é€€å‡ºã€‚*/\t//int sum = 1;\t//for (int i = 3; i = Max; i += 2)\t//{\t//\t//å› ä¸ºå¶æ•°é™¤äº†2 éƒ½ä¸æ˜¯è´¨æ•°\t//\tint j;\t//\tfor (j = 2; j = (int)sqrt(i); j++)//åˆ©ç”¨ä¸Šè¿°ç»“è®ºåˆ¤æ–­\t// if (i%j == 0) break;\t//\tif (j (int)sqrt(i))\t// sum++;\t//}\t//printf_s(\"Time used = %0.2f s \", (double)clock() / CLOCKS_PER_SEC);\t//printf_s(\"%d \", sum);\t//int a, b;\t//scanf_s(\"%d %d\", a, b);\t//combination(a, b);\t//åˆ½å­æ‰‹æ¸¸æˆ\t/*\tæ¸¸æˆè§„åˆ™æ˜¯è¿™æ ·çš„ï¼šè®¡ç®—æœºæƒ³ä¸€ä¸ªå•è¯è®©ä½ çŒœï¼Œä½ æ¯æ¬¡å¯ä»¥çŒœä¸€ä¸ªå­—æ¯ã€‚ å¦‚æœå•è¯é‡Œæœ‰é‚£ä¸ªå­—æ¯ï¼Œæ‰€æœ‰è¯¥å­—æ¯ä¼šæ˜¾ç¤ºå‡ºæ¥ï¼›å¦‚æœæ²¡æœ‰é‚£ä¸ªå­—æ¯ï¼Œåˆ™è®¡ç®—æœºä¼šåœ¨ä¸€å¹…â€œåˆ½å­æ‰‹â€ç”»ä¸Šå¡«ä¸€ç¬”ã€‚ è¿™å¹…ç”»ä¸€å…±éœ€è¦7ç¬”å°±èƒ½å®Œæˆï¼Œå› æ­¤ä½ æœ€å¤šåªèƒ½é”™6æ¬¡ã€‚ æ³¨æ„ï¼ŒçŒœä¸€ä¸ªå·²ç»çŒœè¿‡çš„å­—æ¯ä¹Ÿç®—é”™ã€‚\tåœ¨æœ¬é¢˜ä¸­ï¼Œä½ çš„ä»»åŠ¡æ˜¯ç¼–å†™ä¸€ä¸ªâ€œè£åˆ¤â€ç¨‹åºï¼Œè¾“å…¥å•è¯å’Œç©å®¶çš„çŒœæµ‹ï¼Œåˆ¤æ–­ç©å®¶èµ¢äº†ï¼ˆYou win.ï¼‰ã€ è¾“äº†ï¼ˆYou lose.ï¼‰è¿˜æ˜¯æ”¾å¼ƒäº†ï¼ˆYou chickened out.ï¼‰ã€‚ æ¯ç»„æ•°æ®åŒ…å«3è¡Œï¼Œç¬¬1è¡Œæ˜¯æ¸¸æˆç¼–å·ï¼ˆ-1ä¸ºè¾“å…¥ç»“æŸæ ‡è®°ï¼‰ï¼Œç¬¬2è¡Œæ˜¯è®¡ç®—æœºæƒ³çš„å•è¯ï¼Œç¬¬3è¡Œæ˜¯ç©å®¶çš„çŒœæµ‹ã€‚ åä¸¤è¡Œä¿è¯åªå«å°å†™å­—æ¯ã€‚\t*/\t//char ans[21];\t//char gus[28];\t//int times, yes;\t//int chances = 7;\t//int win = 0;\t//int lose = 0;\t//scanf_s(\"%d\", times);\t//while (times != -1)\t//{\t//\tprintf_s(\"Round %d \", times);\t//\tscanf_s(\"%s %s\", ans, gus);\t//\tprintf_s(\"Round %s s \", ans, gus);\t//\tyes = 0;\t//\twin = 0, lose = 0, chances = 7;\t//\tfor (int i = 0; i strlen(gus); i++)\t//\t{\t// int flag = 0;\t// for (int j = 0; j strlen(ans); j++)\t// {\t// if (ans[j] == gus[i])\t// {\t// yes++;\t// flag = 1;//æ‰¾åˆ°ä¹‹åä¸é€€å‡ºï¼Œå› ä¸ºæœ‰ä¸€ä¸ªæœ‰ç›¸åŒçš„å­—æ¯\t// }\t// if (flag == 0)\t// {\t// chances--;\t// }\t// if (chances == 0)\t// {\t// lose = 1;\t// printf_s(\"You lose. \");\t// }\t// else if (yes == strlen(ans))\t// {\t// win = 1;\t// printf_s(\"You win. \");\t// break;\t// }\t// if (win != 1 lose != 1)\t// {\t// printf(\"You chickened out. \");\t// }\t// scanf_s(\"%d\", times);\t// }\t//\t}\t//}\tint rnd;\twhile (scanf_s(\"%d%s%s\", rnd, s, s2) == 3 rnd != -1)\t{ printf_s(\"%Round %d \", rnd); win = lose = 0; left = strlen(s); chance = 7; for (int i = 0; i strlen(s2); i++) { guess(s2[i]); //çŒœä¸€ä¸ªå­—æ¯ if (win || lose) break; //æ£€æŸ¥çŠ¶æ€ } //æ ¹æ®ç»“æœè¿›è¡Œè¾“å‡º if (win) printf_s(\"You win. \"); else if (lose) printf_s(\"You lose. \"); else printf_s(\"You chickened out. \");\t}\tsystem(\"pause\"); return 0;}","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"ç®—æ³•ç«èµ›å…¥é—¨ç»å…¸ç¬¬å››ç« ","path":"/2019/08/18/algorithm4/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 | // t4.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚ // #include \"stdafx.h\" //ç´ æ•°åˆ¤æ–­æ–¹æ³•2 #includestdio.h #includestdlib.h #includetime.h #includemath.h #includestring.h #define Max 1000000 #define maxn 100 int left, chance; //è¿˜éœ€è¦leftä½ç½®ï¼Œé”™chanceä¹‹åå°±ä¼šè¾“ char s[maxn], s2[maxn]; //ç­”æ¡ˆæ˜¯å­—ç¬¦ä¸²s, ç©å®¶çŒœçš„å­—æ¯åºåˆ—æ˜¯s2 int win, lose; // win=1 è¡¨ç¤ºå·²ç»èµ¢äº†; lose=1 è¡¨ç¤ºå·²ç»è¾“äº† long fac(int n) { if (0 == n || 1 == n) { return 1; } else { return fac(n - 1)*n; } } // éé€’å½’çš„é˜¶ä¹˜æ–¹æ³• long fact(int n) { long iRes = 1; for (int i = 1; i = n; i++) { iRes *= i; } return iRes; } int combination(int m, int n) { long iRes = fac(n) / (fac(m)*fac(n - m)); // long iRes = fact(n) /(fac(m)*fac(n-m)); printf_s(\"%ld %.2lf \", iRes, (double)clock() / CLOCKS_PER_SEC); return iRes; } // åˆ½å­æ‰‹æ¸¸æˆ---guesså‡½æ•° void guess(char ch) { int bad = 1; for (int i = 0; i strlen(s); i++) if (s[i] == ch) { left--; s[i] = ' '; bad = 0; } if (bad) --chance; if (!chance) lose = 1; if (!left) win = 1; } int main() { /* å…³é”®: 1 ç”¨ç´ æ•°ç­›é€‰æ³•å…ˆé¢„å¤„ç†ï¼Œé»˜è®¤åˆšå¼€å§‹å…¨ä¸ºç´ æ•°ï¼Œç„¶åå¯¹ç´ æ•°çš„å€æ•°æ ‡è®°ä¸ºéç´ æ•°ï¼Œ for(int j = i*i ; j = 10000 ; j += i){iPrimeArr[j] = 1;} 2 é€šè¿‡å¼€æ ¹å·åˆ¤æ–­ç´ æ•°æ—¶ï¼Œå¯ä»¥ç”¨floor,æ³¨æ„æµ®ç‚¹æ•°åŠ ä¸Š0.5ï¼Œint iRadical = floor(sqrt(n*1.0) + 0.5); 3 å¯ä»¥ç”¨assert()å¯¹è¾“å…¥çš„åˆæ³•æ€§è¿›è¡Œæ ¡éªŒï¼Œassert(n = 5 n = 10000);void assert(int exp),å¦‚æœè¡¨è¾¾å¼çš„å€¼ä¸º0åˆ™é€€å‡ºã€‚*/ //int sum = 1; //for (int i = 3; i = Max; i += 2) //{ //\t//å› ä¸ºå¶æ•°é™¤äº†2 éƒ½ä¸æ˜¯è´¨æ•° //\tint j; //\tfor (j = 2; j = (int)sqrt(i); j++)//åˆ©ç”¨ä¸Šè¿°ç»“è®ºåˆ¤æ–­ // if (i%j == 0) break; //\tif (j (int)sqrt(i)) // sum++; //} //printf_s(\"Time used = %0.2f s \", (double)clock() / CLOCKS_PER_SEC); //printf_s(\"%d \", sum); //int a, b; //scanf_s(\"%d %d\", a, b); //combination(a, b); //åˆ½å­æ‰‹æ¸¸æˆ /* æ¸¸æˆè§„åˆ™æ˜¯è¿™æ ·çš„ï¼šè®¡ç®—æœºæƒ³ä¸€ä¸ªå•è¯è®©ä½ çŒœï¼Œä½ æ¯æ¬¡å¯ä»¥çŒœä¸€ä¸ªå­—æ¯ã€‚ å¦‚æœå•è¯é‡Œæœ‰é‚£ä¸ªå­—æ¯ï¼Œæ‰€æœ‰è¯¥å­—æ¯ä¼šæ˜¾ç¤ºå‡ºæ¥ï¼›å¦‚æœæ²¡æœ‰é‚£ä¸ªå­—æ¯ï¼Œåˆ™è®¡ç®—æœºä¼šåœ¨ä¸€å¹…â€œåˆ½å­æ‰‹â€ç”»ä¸Šå¡«ä¸€ç¬”ã€‚ è¿™å¹…ç”»ä¸€å…±éœ€è¦7ç¬”å°±èƒ½å®Œæˆï¼Œå› æ­¤ä½ æœ€å¤šåªèƒ½é”™6æ¬¡ã€‚ æ³¨æ„ï¼ŒçŒœä¸€ä¸ªå·²ç»çŒœè¿‡çš„å­—æ¯ä¹Ÿç®—é”™ã€‚ åœ¨æœ¬é¢˜ä¸­ï¼Œä½ çš„ä»»åŠ¡æ˜¯ç¼–å†™ä¸€ä¸ªâ€œè£åˆ¤â€ç¨‹åºï¼Œè¾“å…¥å•è¯å’Œç©å®¶çš„çŒœæµ‹ï¼Œåˆ¤æ–­ç©å®¶èµ¢äº†ï¼ˆYou win.ï¼‰ã€ è¾“äº†ï¼ˆYou lose.ï¼‰è¿˜æ˜¯æ”¾å¼ƒäº†ï¼ˆYou chickened out.ï¼‰ã€‚ æ¯ç»„æ•°æ®åŒ…å«3è¡Œï¼Œç¬¬1è¡Œæ˜¯æ¸¸æˆç¼–å·ï¼ˆ-1ä¸ºè¾“å…¥ç»“æŸæ ‡è®°ï¼‰ï¼Œç¬¬2è¡Œæ˜¯è®¡ç®—æœºæƒ³çš„å•è¯ï¼Œç¬¬3è¡Œæ˜¯ç©å®¶çš„çŒœæµ‹ã€‚ åä¸¤è¡Œä¿è¯åªå«å°å†™å­—æ¯ã€‚ */ //char ans[21]; //char gus[28]; //int times, yes; //int chances = 7; //int win = 0; //int lose = 0; //scanf_s(\"%d\", times); //while (times != -1) //{ //\tprintf_s(\"Round %d \", times); //\tscanf_s(\"%s %s\", ans, gus); //\tprintf_s(\"Round %s s \", ans, gus); //\tyes = 0; //\twin = 0, lose = 0, chances = 7; //\tfor (int i = 0; i strlen(gus); i++) //\t{ // int flag = 0; // for (int j = 0; j strlen(ans); j++) // { // if (ans[j] == gus[i]) // { // yes++; // flag = 1;//æ‰¾åˆ°ä¹‹åä¸é€€å‡ºï¼Œå› ä¸ºæœ‰ä¸€ä¸ªæœ‰ç›¸åŒçš„å­—æ¯ // } // if (flag == 0) // { // chances--; // } // if (chances == 0) // { // lose = 1; // printf_s(\"You lose. \"); // } // else if (yes == strlen(ans)) // { // win = 1; // printf_s(\"You win. \"); // break; // } // if (win != 1 lose != 1) // { // printf(\"You chickened out. \"); // } // scanf_s(\"%d\", times); // } //\t} //} int rnd; while (scanf_s(\"%d%s%s\", rnd, s, s2) == 3 rnd != -1) { printf_s(\"%Round %d \", rnd); win = lose = 0; left = strlen(s); chance = 7; for (int i = 0; i strlen(s2); i++) { guess(s2[i]); //çŒœä¸€ä¸ªå­—æ¯ if (win || lose) break; //æ£€æŸ¥çŠ¶æ€ } //æ ¹æ®ç»“æœè¿›è¡Œè¾“å‡º if (win) printf_s(\"You win. \"); else if (lose) printf_s(\"You lose. \"); else printf_s(\"You chickened out. \"); } system(\"pause\"); return 0; } â€”|â€”","tags":["C++"],"categories":["ç®—æ³•"]},{"title":"LNMS","path":"/2019/08/16/20190816-LNMS/","content":"locality NMSLNMSæ˜¯åœ¨EASTæ–‡æœ¬æ£€æµ‹ä¸­æå‡ºçš„ï¼ä¸»è¦åŸå› ï¼šæ–‡æœ¬æ£€æµ‹é¢ä¸´çš„æ˜¯æˆåƒä¸Šä¸‡ä¸ªå‡ ä½•ä½“ï¼Œå¦‚æœç”¨æ™®é€šçš„NMSï¼Œå…¶è®¡ç®—å¤æ‚åº¦ï¼Œnæ˜¯å‡ ä½•ä½“çš„ä¸ªæ•°ï¼Œè¿™æ˜¯ä¸å¯æ¥å—çš„ï¼å¯¹ä¸Šè¿°æ—¶é—´å¤æ‚åº¦é—®é¢˜ï¼ŒEASTæå‡ºäº†åŸºäºè¡Œåˆå¹¶å‡ ä½•ä½“çš„æ–¹æ³•ï¼Œå½“ç„¶è¿™æ˜¯åŸºäºé‚»è¿‘å‡ ä¸ªå‡ ä½•ä½“æ˜¯é«˜åº¦ç›¸å…³çš„å‡è®¾ï¼æ³¨æ„ï¼šè¿™é‡Œåˆå¹¶çš„å››è¾¹å½¢åæ ‡æ˜¯é€šè¿‡ä¸¤ä¸ªç»™å®šå››è¾¹å½¢çš„å¾—åˆ†è¿›è¡ŒåŠ æƒå¹³å‡çš„ï¼Œä¹Ÿå°±æ˜¯è¯´è¿™é‡Œæ˜¯â€œå¹³å‡â€è€Œä¸æ˜¯â€é€‰æ‹©â€å‡ ä½•ä½“*,ç›®çš„æ˜¯å‡å°‘è®¡ç®—é‡ï¼åŸºæœ¬æ­¥éª¤1.å…ˆå¯¹æ‰€æœ‰çš„output boxé›†åˆç»“åˆç›¸åº”çš„é˜ˆå€¼ï¼ˆå¤§äºé˜ˆå€¼åˆ™è¿›è¡Œåˆå¹¶ï¼Œå°äºé˜ˆå€¼åˆ™ä¸å’Œå¹¶ï¼‰ï¼Œä¾æ¬¡éå†è¿›è¡ŒåŠ æƒåˆå¹¶ï¼Œå¾—åˆ°åˆå¹¶åçš„bboxé›†åˆï¼›2.å¯¹åˆå¹¶åçš„bboxé›†åˆè¿›è¡Œæ ‡å‡†çš„NMSæ“ä½œ def detect(score_map, geo_map, timer, score_map_thresh=1e-5, box_thresh=1e-8, nms_thres=0.1): ''' restore text boxes from score map and geo map :param score_map: bs* 128 * 128 * 1 :param geo_map: ## geo_map = bs * 128 * 128 * 5 :param timer: :param score_map_thresh: threshhold for score map :param box_thresh: threshhold for boxes :param nms_thres: threshold for nms :return: ''' if len(score_map.shape) == 4: score_map = score_map[0, :, :, 0] geo_map = geo_map[0, :, :, ] # filter the score map xy_text = np.argwhere(score_map score_map_thresh) # sort the text boxes via the y axis xy_text = xy_text[np.argsort(xy_text[:, 0])] # restore start = time.time() text_box_restored = restore_rectangle(xy_text[:, ::-1]*4, geo_map[xy_text[:, 0], xy_text[:, 1], :]) # N*4*2 #print('{} text boxes before nms'.format(text_box_restored.shape[0])) boxes = np.zeros((text_box_restored.shape[0], 9), dtype=np.float32) boxes[:, :8] = text_box_restored.reshape((-1, 8)) boxes[:, 8] = score_map[xy_text[:, 0], xy_text[:, 1]] timer['restore'] = time.time() - start # å¾—åˆ°box çš„åæ ‡ä»¥åŠåˆ†æ•° # nms part start = time.time() # boxes = nms_locality.nms_locality(boxes.astype(np.float64), nms_thres) boxes = lanms.merge_quadrangle_n9(boxes.astype('float32'), nms_thres) timer['nms'] = time.time() - start if boxes.shape[0] == 0: return None, timer # here we filter some low score boxes by the average score map, this is different from the orginal paper for i, box in enumerate(boxes): mask = np.zeros_like(score_map, dtype=np.uint8) cv2.fillPoly(mask, box[:8].reshape((-1, 4, 2)).astype(np.int32) // 4, 1) boxes[i, 8] = cv2.mean(score_map, mask)[0] boxes = boxes[boxes[:, 8] box_thresh] return boxes, timer import numpy as npfrom shapely.geometry import Polygondef intersection(g, p): #å–g,pä¸­çš„å‡ ä½•ä½“ä¿¡æ¯ç»„æˆå¤šè¾¹å½¢ g = Polygon(g[:8].reshape((4, 2))) p = Polygon(p[:8].reshape((4, 2))) # åˆ¤æ–­g,pæ˜¯å¦ä¸ºæœ‰æ•ˆçš„å¤šè¾¹å½¢å‡ ä½•ä½“ if not g.is_valid or not p.is_valid: return 0 # å–ä¸¤ä¸ªå‡ ä½•ä½“çš„äº¤é›†å’Œå¹¶é›† inter = Polygon(g).intersection(Polygon(p)).area union = g.area + p.area - inter if union == 0: return 0 else: return inter/uniondef weighted_merge(g, p): # å–g,pä¸¤ä¸ªå‡ ä½•ä½“çš„åŠ æƒï¼ˆæƒé‡æ ¹æ®å¯¹åº”çš„æ£€æµ‹å¾—åˆ†è®¡ç®—å¾—åˆ°ï¼‰ g[:8] = (g[8] * g[:8] + p[8] * p[:8])/(g[8] + p[8]) #åˆå¹¶åçš„å‡ ä½•ä½“çš„å¾—åˆ†ä¸ºä¸¤ä¸ªå‡ ä½•ä½“å¾—åˆ†çš„æ€»å’Œ g[8] = (g[8] + p[8]) return gdef standard_nms(S, thres): #æ ‡å‡†NMS order = np.argsort(S[:, 8])[::-1] keep = [] while order.size 0: i = order[0] keep.append(i) ovr = np.array([intersection(S[i], S[t]) for t in order[1:]]) inds = np.where(ovr = thres)[0] order = order[inds+1] return S[keep]def nms_locality(polys, thres=0.3): ''' locality aware nms of EAST :param polys: a N*9 numpy array. first 8 coordinates, then prob :return: boxes after nms ''' S = [] #åˆå¹¶åçš„å‡ ä½•ä½“é›†åˆ p = None #åˆå¹¶åçš„å‡ ä½•ä½“ for g in polys: if p is not None and intersection(g, p) thres: #è‹¥ä¸¤ä¸ªå‡ ä½•ä½“çš„ç›¸äº¤é¢ç§¯å¤§äºæŒ‡å®šçš„é˜ˆå€¼ï¼Œåˆ™è¿›è¡Œåˆå¹¶ p = weighted_merge(g, p) else: #åä¹‹ï¼Œåˆ™ä¿ç•™å½“å‰çš„å‡ ä½•ä½“ if p is not None: S.append(p) p = g if p is not None: S.append(p) if len(S) == 0: return np.array([]) return standard_nms(np.array(S), thres)if __name__ == '__main__': # 343,350,448,135,474,143,369,359 print(Polygon(np.array([[343, 350], [448, 135], [474, 143], [369, 359]])).area) å‚è€ƒåšå®¢ï¼š https://www.jianshu.com/p/4934875f7eb6","tags":["è®¡ç®—æœºè§†è§‰"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"ç®—æ³•ç«èµ›å…¥é—¨ç»å…¸ç¬¬ä¸‰ç« ","path":"/2019/08/16/20190816-algorithm3/","content":"ä»Šå¤©å¿ƒæƒ…ä¸å¥½ï¼Œæ‰€ä»¥å°±æ•²äº†è¿™ä¸€ç‚¹ä»£ç ï¼Œå¸Œæœ›æ˜å¤©çš„çŠ¶æ€èƒ½å¥½ä¸€ç‚¹ï¼ // t3.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚//#include \"stdafx.h\"#include \"stdio.h\"#include stdlib.h#includemath.h#includetime.h#define maxn 10000000 +10char s[maxn];int main(){\t//é¢˜ç›®1 ç»Ÿè®¡ä¸ªæ•°\t/*int count =0;\tint temp;\twhile (~scanf_s(\"%d\", temp))\t{ count++;\t}\tprintf_s(\"%d\", count);*/ //æ˜¾ç¤ºä¸å‡ºæ¥\t//è¾“å…¥ä¸€äº›æ•°ï¼Œæ±‚æœ€å¤§å€¼ã€æœ€å°å€¼å’Œå¹³å‡æ•°\t//int min, max, n, sum,count = 0;\t//float avg;\t//scanf_s(\"%d\", n);\t//max = n;\t//min = n;\t//sum = n;\t//count++;\t//while (scanf_s(\"%d\", n)!=EOF)\t//{\t//\tcount++;\t//\tsum += n;\t//\tif (n max)\t// max = n;\t//\tif (n min)\t// min = n;\t//}\t//avg = sum*1.0 / count;\t//printf_s(\"%d %d %f \", max,min,avg);\t//5.è¾“å…¥ä¸€äº›æ•° ï¼Œæ±‚å‡ºä»–ä»¬çš„æ–¹å·®\t/*double ave, sum = 0, varance, psum = 0;\tint n.a[110], count = 0;\twhile (scanf_s(\"%d\", n) != EOF)\t{ a[count++] = n; sum += n;\t}\tavg = sum*1.0 / count;\tfor (int i = 0; i count; i++) psum += (a[i] - ave)*(a[i] - ave);\tvariance = psum / count;\tprint(\"%lf\", variance);*/\tscanf_s(\"%s\", s);\tint tot = 0;\tfor (int i = 0;s[i]; i++) { if (s[i] == '1') tot++;\t}\tprintf_s(\"%d \", tot);\tsystem(\"pause\"); return 0;}","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"LNMS","path":"/2019/08/16/LNMS/","content":"locality NMSLNMSæ˜¯åœ¨EASTæ–‡æœ¬æ£€æµ‹ä¸­æå‡ºçš„ï¼ä¸»è¦åŸå› ï¼šæ–‡æœ¬æ£€æµ‹é¢ä¸´çš„æ˜¯æˆåƒä¸Šä¸‡ä¸ªå‡ ä½•ä½“ï¼Œå¦‚æœç”¨æ™®é€šçš„NMSï¼Œå…¶è®¡ç®—å¤æ‚åº¦ï¼Œnæ˜¯å‡ ä½•ä½“çš„ä¸ªæ•°ï¼Œè¿™æ˜¯ä¸å¯æ¥å—çš„ï¼å¯¹ä¸Šè¿°æ—¶é—´å¤æ‚åº¦é—®é¢˜ï¼ŒEASTæå‡ºäº†åŸºäºè¡Œåˆå¹¶å‡ ä½•ä½“çš„æ–¹æ³•ï¼Œå½“ç„¶è¿™æ˜¯åŸºäºé‚»è¿‘å‡ ä¸ªå‡ ä½•ä½“æ˜¯é«˜åº¦ç›¸å…³çš„å‡è®¾ï¼æ³¨æ„ï¼šè¿™é‡Œåˆå¹¶çš„å››è¾¹å½¢åæ ‡æ˜¯é€šè¿‡ä¸¤ä¸ªç»™å®šå››è¾¹å½¢çš„å¾—åˆ†è¿›è¡ŒåŠ æƒå¹³å‡çš„ï¼Œä¹Ÿå°±æ˜¯è¯´è¿™é‡Œæ˜¯â€œå¹³å‡â€è€Œä¸æ˜¯â€é€‰æ‹©â€å‡ ä½•ä½“*,ç›®çš„æ˜¯å‡å°‘è®¡ç®—é‡ï¼åŸºæœ¬æ­¥éª¤1.å…ˆå¯¹æ‰€æœ‰çš„output boxé›†åˆç»“åˆç›¸åº”çš„é˜ˆå€¼ï¼ˆå¤§äºé˜ˆå€¼åˆ™è¿›è¡Œåˆå¹¶ï¼Œå°äºé˜ˆå€¼åˆ™ä¸å’Œå¹¶ï¼‰ï¼Œä¾æ¬¡éå†è¿›è¡ŒåŠ æƒåˆå¹¶ï¼Œå¾—åˆ°åˆå¹¶åçš„bboxé›†åˆï¼›2.å¯¹åˆå¹¶åçš„bboxé›†åˆè¿›è¡Œæ ‡å‡†çš„NMSæ“ä½œ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 | def detect(score_map, geo_map, timer, score_map_thresh=1e-5, box_thresh=1e-8, nms_thres=0.1): ''' restore text boxes from score map and geo map :param score_map: bs* 128 * 128 * 1 :param geo_map: ## geo_map = bs * 128 * 128 * 5 :param timer: :param score_map_thresh: threshhold for score map :param box_thresh: threshhold for boxes :param nms_thres: threshold for nms :return: ''' if len(score_map.shape) == 4: score_map = score_map[0, :, :, 0] geo_map = geo_map[0, :, :, ] # filter the score map xy_text = np.argwhere(score_map score_map_thresh) # sort the text boxes via the y axis xy_text = xy_text[np.argsort(xy_text[:, 0])] # restore start = time.time() text_box_restored = restore_rectangle(xy_text[:, ::-1]*4, geo_map[xy_text[:, 0], xy_text[:, 1], :]) # N*4*2 #print('{} text boxes before nms'.format(text_box_restored.shape[0])) boxes = np.zeros((text_box_restored.shape[0], 9), dtype=np.float32) boxes[:, :8] = text_box_restored.reshape((-1, 8)) boxes[:, 8] = score_map[xy_text[:, 0], xy_text[:, 1]] timer['restore'] = time.time() - start # å¾—åˆ°box çš„åæ ‡ä»¥åŠåˆ†æ•° # nms part start = time.time() # boxes = nms_locality.nms_locality(boxes.astype(np.float64), nms_thres) boxes = lanms.merge_quadrangle_n9(boxes.astype('float32'), nms_thres) timer['nms'] = time.time() - start if boxes.shape[0] == 0: return None, timer # here we filter some low score boxes by the average score map, this is different from the orginal paper for i, box in enumerate(boxes): mask = np.zeros_like(score_map, dtype=np.uint8) cv2.fillPoly(mask, box[:8].reshape((-1, 4, 2)).astype(np.int32) // 4, 1) boxes[i, 8] = cv2.mean(score_map, mask)[0] boxes = boxes[boxes[:, 8] box_thresh] return boxes, timer â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 | import numpy as np from shapely.geometry import Polygon def intersection(g, p): #å–g,pä¸­çš„å‡ ä½•ä½“ä¿¡æ¯ç»„æˆå¤šè¾¹å½¢ g = Polygon(g[:8].reshape((4, 2))) p = Polygon(p[:8].reshape((4, 2))) # åˆ¤æ–­g,pæ˜¯å¦ä¸ºæœ‰æ•ˆçš„å¤šè¾¹å½¢å‡ ä½•ä½“ if not g.is_valid or not p.is_valid: return 0 # å–ä¸¤ä¸ªå‡ ä½•ä½“çš„äº¤é›†å’Œå¹¶é›† inter = Polygon(g).intersection(Polygon(p)).area union = g.area + p.area - inter if union == 0: return 0 else: return inter/union def weighted_merge(g, p): # å–g,pä¸¤ä¸ªå‡ ä½•ä½“çš„åŠ æƒï¼ˆæƒé‡æ ¹æ®å¯¹åº”çš„æ£€æµ‹å¾—åˆ†è®¡ç®—å¾—åˆ°ï¼‰ g[:8] = (g[8] * g[:8] + p[8] * p[:8])/(g[8] + p[8]) #åˆå¹¶åçš„å‡ ä½•ä½“çš„å¾—åˆ†ä¸ºä¸¤ä¸ªå‡ ä½•ä½“å¾—åˆ†çš„æ€»å’Œ g[8] = (g[8] + p[8]) return g def standard_nms(S, thres): #æ ‡å‡†NMS order = np.argsort(S[:, 8])[::-1] keep = [] while order.size 0: i = order[0] keep.append(i) ovr = np.array([intersection(S[i], S[t]) for t in order[1:]]) inds = np.where(ovr = thres)[0] order = order[inds+1] return S[keep] def nms_locality(polys, thres=0.3): ''' locality aware nms of EAST :param polys: a N*9 numpy array. first 8 coordinates, then prob :return: boxes after nms ''' S = [] #åˆå¹¶åçš„å‡ ä½•ä½“é›†åˆ p = None #åˆå¹¶åçš„å‡ ä½•ä½“ for g in polys: if p is not None and intersection(g, p) thres: #è‹¥ä¸¤ä¸ªå‡ ä½•ä½“çš„ç›¸äº¤é¢ç§¯å¤§äºæŒ‡å®šçš„é˜ˆå€¼ï¼Œåˆ™è¿›è¡Œåˆå¹¶ p = weighted_merge(g, p) else: #åä¹‹ï¼Œåˆ™ä¿ç•™å½“å‰çš„å‡ ä½•ä½“ if p is not None: S.append(p) p = g if p is not None: S.append(p) if len(S) == 0: return np.array([]) return standard_nms(np.array(S), thres) if __name__ == '__main__': # 343,350,448,135,474,143,369,359 print(Polygon(np.array([[343, 350], [448, 135], [474, 143], [369, 359]])).area) â€”|â€” å‚è€ƒåšå®¢ï¼š https://www.jianshu.com/p/4934875f7eb6","tags":["OpenCV"],"categories":["æ·±åº¦å­¦ä¹ "]},{"title":"ç®—æ³•ç«èµ›å…¥é—¨ç»å…¸ç¬¬ä¸‰ç« ","path":"/2019/08/16/algorithm3/","content":"ä»Šå¤©å¿ƒæƒ…ä¸å¥½ï¼Œæ‰€ä»¥å°±æ•²äº†è¿™ä¸€ç‚¹ä»£ç ï¼Œå¸Œæœ›æ˜å¤©çš„çŠ¶æ€èƒ½å¥½ä¸€ç‚¹ï¼ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 | // t3.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚ // #include \"stdafx.h\" #include \"stdio.h\" #include stdlib.h #includemath.h #includetime.h #define maxn 10000000 +10 char s[maxn]; int main() { //é¢˜ç›®1 ç»Ÿè®¡ä¸ªæ•° /*int count =0; int temp; while (~scanf_s(\"%d\", temp)) { count++; } printf_s(\"%d\", count);*/ //æ˜¾ç¤ºä¸å‡ºæ¥ //è¾“å…¥ä¸€äº›æ•°ï¼Œæ±‚æœ€å¤§å€¼ã€æœ€å°å€¼å’Œå¹³å‡æ•° //int min, max, n, sum,count = 0; //float avg; //scanf_s(\"%d\", n); //max = n; //min = n; //sum = n; //count++; //while (scanf_s(\"%d\", n)!=EOF) //{ //\tcount++; //\tsum += n; //\tif (n max) // max = n; //\tif (n min) // min = n; //} //avg = sum*1.0 / count; //printf_s(\"%d %d %f \", max,min,avg); //5.è¾“å…¥ä¸€äº›æ•° ï¼Œæ±‚å‡ºä»–ä»¬çš„æ–¹å·® /*double ave, sum = 0, varance, psum = 0; int n.a[110], count = 0; while (scanf_s(\"%d\", n) != EOF) { a[count++] = n; sum += n; } avg = sum*1.0 / count; for (int i = 0; i count; i++) psum += (a[i] - ave)*(a[i] - ave); variance = psum / count; print(\"%lf\", variance);*/ scanf_s(\"%s\", s); int tot = 0; for (int i = 0;s[i]; i++) { if (s[i] == '1') tot++; } printf_s(\"%d \", tot); system(\"pause\"); return 0; } â€”|â€”","tags":["C++"],"categories":["å…¶ä»–"]},{"title":"ç®—æ³•ç«èµ›å…¥é—¨ç»å…¸ç¬¬äºŒç« ","path":"/2019/08/15/20190815-algorithm2/","content":"// t2.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚//#include \"stdafx.h\"#include \"stdio.h\"#include stdlib.h#includemath.h#includetime.hint main(){\t////3n+1é—®é¢˜ã€\t//int n, count = 0; //å½“nè¿‡å¤§çš„æ—¶å€™ 2*næº¢å‡º\t//\t//scanf_s(\"%d\", n);\t//long long n2 = n;\t//while (n2 1)\t//{\t//\tif (n2 % 2 == 1) n2 = n2 * 3 + 1;\t//\telse n2 /= 2;\t//\tcount++;\t//}\t//printf_s(\"%d \", count);\t//è¿‘ä¼¼è®¡ç®—\t/*double sum = 0;\tfor (int i = 0;; i++) { double term = 1.0 / (i * 2 + 1); if (i % 2 == 0) sum += term; else sum -= term; if (term 1e-6) break;\t}\tprintf_s(\"%.6f \", sum);*/\t// é˜¶ä¹˜ä¹‹å’Œ åªä¿å­˜åå…­ä½\t//int n, S = 0;\t//scanf_s(\"%d\", n);\t//for (int i = 1; i = n; i++)\t//{\t//\tint factorial = 1;\t//\tfor (int j = 1; j = i; j++)\t// factorial *= j;\t//\tS += factorial;\t//}\t//printf_s(\"%d \", S % 1000000);\t// é˜¶ä¹˜ä¹‹å’Œ2, ä¼˜åŒ–ç‰ˆæœ¬\t//int n, S = 0;\t//const int MOD = 1000000;\t//scanf_s(\"%d\", n);\t//for (int i = 1; i = n; i++)\t//{\t//\tint factorial = 1;\t//\tfor (int j = 1; j = i; j++)\t// factorial = (factorial *j)%MOD;\t//\tS += factorial;\t//}\t//printf_s(\"%d \", S %MOD);\t//printf_s(\"Time used = %.2f \", (double)clock() / CLOCKS_PER_SEC); // å¾—åˆ°ç¨‹åºè¿è¡Œçš„æ—¶é—´ å•ä½ï¼šç§’\t// æ•°æ®ç»Ÿè®¡ //ä¹ é¢˜2.1 æ°´ä»™èŠ±æ•°ç›®\t/*int sum;\tfor (int i = 1; i 10; i++) for (int k = 0; k 10; k++) for (int j = 0; j 10; j++) { sum = i * 100 + 10 * k + j; if(sum == i*i*i+j*j*j + k*k*k) printf_s(\"%d \", sum); }*/\t//ä¹ é¢˜2.2 éŸ©ä¿¡ç‚¹å…µ ç›¸ä¼ éŸ©ä¿¡æ‰æ™ºè¿‡äººï¼Œä»ä¸ç›´æ¥æ¸…ç‚¹è‡ªå·±å†›é˜Ÿçš„äººæ•°ï¼Œåªè¦è®©å£«å…µå…ˆåä»¥ä¸‰äººä¸€æ’ã€äº”äººä¸€æ’ã€ä¸ƒäººä¸€æ’åœ°å˜æ¢é˜Ÿå½¢ï¼Œ\t//è€Œä»–æ¯æ¬¡åªæ ä¸€çœ¼é˜Ÿä¼çš„æ’å°¾å°±çŸ¥é“æ€»äººæ•°äº†ã€‚è¾“å…¥åŒ…å«å¤šç»„æ•°æ®ï¼Œæ¯ç»„æ•°æ®åŒ…å«3ä¸ªéè´Ÿæ•´æ•°aï¼Œbï¼Œcï¼Œè¡¨ç¤ºæ¯ç§é˜Ÿå½¢æ’å°¾çš„äººæ•°ï¼ˆaï¼œ3ï¼Œbï¼œ5ï¼Œcï¼œ7ï¼‰ï¼Œ\t//è¾“å‡ºæ€»äººæ•°çš„æœ€å°å€¼ï¼ˆæˆ–æŠ¥å‘Šæ— è§£ï¼‰ã€‚å·²çŸ¥æ€»äººæ•°ä¸å°äº10ï¼Œä¸è¶…è¿‡100ã€‚è¾“å…¥åˆ°æ–‡ä»¶ç»“æŸä¸ºæ­¢ã€‚\t//int i, a, b, c;\t//scanf_s(\"%d%d%d\", a, b, c);\t//for (i = 0; i = 100; i++) {\t//\tif (i % 3 == a i % 5 == b i % 7 == c)\t// printf_s(\"%d \", i);\t//\t//}\t//if (i % 3 != a i % 5 != b i % 7 != c i100)\t// printf_s(\"No answer \"); //ä¹ é¢˜2.3 å€’ä¸‰è§’å½¢\t//int n, /* è¾“å‡ºnè¡Œ; n=20 */\t//\ti, /* æ‰“å°ç¬¬iè¡Œ */\t//\tj;\t//scanf_s(\"%d\", n);\t//for (i = 1; i = n; i = i + 1) {\t//\t/* åœ¨ç¬¬iè¡Œï¼Œæ‰“å°(i-1)ä¸ªç©ºæ ¼ */\t//\tfor (j = 1; j = i - 1; j = j + 1) printf_s(\" \");\t//\t/* åœ¨ç¬¬iè¡Œï¼Œæ‰“å°(2*n-2*i+1)ä¸ª# */\t//\tfor (j = 1; j = (2 * n - 2 * i + 1); j = j + 1) printf_s(\"#\");\t//\tprintf_s(\" \"); /* è¾“å‡ºç»“æŸåæ¢è¡Œï¼Œå¦åˆ™æ‰€æœ‰çš„#å·åœ¨åŒä¸€è¡Œè¾“å‡º */\t//}\t//ä¹ é¢˜2.4 å­åºåˆ—çš„å’Œ è¾“å…¥ä¸¤ä¸ªæ­£æ•´æ•°nï¼œmï¼œ10 6 ï¼Œè¾“å‡º ï¼Œä¿ç•™5ä½å°æ•°ã€‚è¾“å…¥åŒ…å«å¤šç»„æ•°æ®ï¼Œ æ³¨ï¼šé™·é˜±å°±æ˜¯åœ¨nç‰¹åˆ«å¤§æ—¶å¦‚æœç›´æ¥n*nå°±ä¼šæº¢å‡ºï¼Œæ‰€ä»¥åªèƒ½è¿é™¤ä¸¤æ¬¡\t//int count = 0;\t//while (1) {\t//\tint n = 0;\t//\tint m = 0;\t//\tscanf_s(\"%d\", n);\t//\tscanf_s(\"%d\", m);\t//\tif (n == mn == 0) {\t// break;\t//\t}\t//\tcount++;\t//\tdouble sum = 0;\t//\tfor (int i = n; i = m; i++) {\t// sum += 1.0 / i / i;\t//\t}\t//\tprintf_s(\"Case %d:%.5f \", count, sum);\t//} //ä¹ é¢˜2.5 åˆ†æ•°åŒ–å°æ•°ï¼ˆdecimalï¼‰ //è¾“å…¥æ­£æ•´æ•°aï¼Œbï¼Œcï¼Œè¾“å‡ºa / bçš„å°æ•°å½¢å¼ï¼Œç²¾ç¡®åˆ°å°æ•°ç‚¹åcä½ã€‚aï¼Œbâ‰¤10 ^ 6ï¼Œcâ‰¤100ã€‚è¾“å…¥åŒ…å«å¤šç»„æ•°æ®ï¼Œç»“æŸæ ‡è®°ä¸ºaï¼bï¼cï¼0ã€‚ // int count = 0;\t//while (1) {\t//\tint a, b, c;\t//\tint k, d, i;\t//\tscanf_s(\"%d\", a);\t//\tscanf_s(\"%d\", b);\t//\tscanf_s(\"%d\", c);\t//\tif (a == 0b == 0 c==0 ) {\t// break;\t//\t}\t//\tcount++;\t//\tfor (i = 0; ic - 1; i++)\t//\t{\t// /*æœºæ™ºåœ°æŠŠä½™æ•°æ”¾å¤§åå€ï¼Œä½¿ä¹‹é™¤ä»¥bå¹¶å–æ¨¡*/\t// k = (k%b) * 10;\t// printf_s(\"Case %d:%.5f \", count, k / b);\t//\t}\t//\tk = (k%b) * 10;\t//\td = (k%b) * 10 / b;\t//\tif (d = 5)//åˆ¤æ–­ç¬¬c+1ä½å°æ•°æ˜¯å¦å¤§äºç­‰äº5ï¼Œif yes,ç¬¬cä½å°æ•°è¦è¿›1\t//\t{\t// printf_s(\"Case %d:%.5f \", count, k / b + 1);\t//\t}\t//\telse\t//\t{\t// printf_s(\"Case %d:%.5f \", count, k / b);\t//\t}\t//\t//} int abc,def,ghi; int a[10],count=0; memset(a,0,sizeof(a)); // å°†aæ•°ç»„ä¸­çš„å€¼å…¨éƒ¨è®¾ç½®ä¸º0 for (abc = 123;abc 333;abc ++) { // åŸºæœ¬å¯ä»¥ç¡®å®šabcçš„æœ€å°å€¼å’Œæœ€å¤§å€¼ def = 2 * abc; ghi = 3 * abc; // è®¾ç½®æ•°ç»„ä¸­æ‰€æœ‰å¯¹åº”çš„9ä½æ•°å­—ä½ç½®çš„å€¼1 a[abc/100] = 1; // a a[abc/10%10] = 1; // b a[abc%10] = 1; // c a[def/100] = 1; // d a[def/10%10] = 1; // e a[def%10] = 1; // f a[ghi/100] = 1; // g a[ghi/10%10] = 1; // h a[ghi%10] = 1; // i int i; for (i=1;i=9;i++) { count += a[i]; } if (count == 9) { printf_s(\"%d %d %d \",abc,def,ghi); } // é‡ç½®count å’Œaæ•°ç»„ count = 0; memset(a,0,sizeof(a)); }\tsystem(\"pause\"); return 0;}","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"ç®—æ³•ç«èµ›å…¥é—¨ç»å…¸ç¬¬äºŒç« ","path":"/2019/08/15/algorithm2/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 | // t2.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚ // #include \"stdafx.h\" #include \"stdio.h\" #include stdlib.h #includemath.h #includetime.h int main() { ////3n+1é—®é¢˜ã€ //int n, count = 0; //å½“nè¿‡å¤§çš„æ—¶å€™ 2*næº¢å‡º // //scanf_s(\"%d\", n); //long long n2 = n; //while (n2 1) //{ //\tif (n2 % 2 == 1) n2 = n2 * 3 + 1; //\telse n2 /= 2; //\tcount++; //} //printf_s(\"%d \", count); //è¿‘ä¼¼è®¡ç®— /*double sum = 0; for (int i = 0;; i++) { double term = 1.0 / (i * 2 + 1); if (i % 2 == 0) sum += term; else sum -= term; if (term 1e-6) break; } printf_s(\"%.6f \", sum);*/ // é˜¶ä¹˜ä¹‹å’Œ åªä¿å­˜åå…­ä½ //int n, S = 0; //scanf_s(\"%d\", n); //for (int i = 1; i = n; i++) //{ //\tint factorial = 1; //\tfor (int j = 1; j = i; j++) // factorial *= j; //\tS += factorial; //} //printf_s(\"%d \", S % 1000000); // é˜¶ä¹˜ä¹‹å’Œ2, ä¼˜åŒ–ç‰ˆæœ¬ //int n, S = 0; //const int MOD = 1000000; //scanf_s(\"%d\", n); //for (int i = 1; i = n; i++) //{ //\tint factorial = 1; //\tfor (int j = 1; j = i; j++) // factorial = (factorial *j)%MOD; //\tS += factorial; //} //printf_s(\"%d \", S %MOD); //printf_s(\"Time used = %.2f \", (double)clock() / CLOCKS_PER_SEC); // å¾—åˆ°ç¨‹åºè¿è¡Œçš„æ—¶é—´ å•ä½ï¼šç§’ // æ•°æ®ç»Ÿè®¡ //ä¹ é¢˜2.1 æ°´ä»™èŠ±æ•°ç›® /*int sum; for (int i = 1; i 10; i++) for (int k = 0; k 10; k++) for (int j = 0; j 10; j++) { sum = i * 100 + 10 * k + j; if(sum == i*i*i+j*j*j + k*k*k) printf_s(\"%d \", sum); }*/ //ä¹ é¢˜2.2 éŸ©ä¿¡ç‚¹å…µ ç›¸ä¼ éŸ©ä¿¡æ‰æ™ºè¿‡äººï¼Œä»ä¸ç›´æ¥æ¸…ç‚¹è‡ªå·±å†›é˜Ÿçš„äººæ•°ï¼Œåªè¦è®©å£«å…µå…ˆåä»¥ä¸‰äººä¸€æ’ã€äº”äººä¸€æ’ã€ä¸ƒäººä¸€æ’åœ°å˜æ¢é˜Ÿå½¢ï¼Œ //è€Œä»–æ¯æ¬¡åªæ ä¸€çœ¼é˜Ÿä¼çš„æ’å°¾å°±çŸ¥é“æ€»äººæ•°äº†ã€‚è¾“å…¥åŒ…å«å¤šç»„æ•°æ®ï¼Œæ¯ç»„æ•°æ®åŒ…å«3ä¸ªéè´Ÿæ•´æ•°aï¼Œbï¼Œcï¼Œè¡¨ç¤ºæ¯ç§é˜Ÿå½¢æ’å°¾çš„äººæ•°ï¼ˆaï¼œ3ï¼Œbï¼œ5ï¼Œcï¼œ7ï¼‰ï¼Œ //è¾“å‡ºæ€»äººæ•°çš„æœ€å°å€¼ï¼ˆæˆ–æŠ¥å‘Šæ— è§£ï¼‰ã€‚å·²çŸ¥æ€»äººæ•°ä¸å°äº10ï¼Œä¸è¶…è¿‡100ã€‚è¾“å…¥åˆ°æ–‡ä»¶ç»“æŸä¸ºæ­¢ã€‚ //int i, a, b, c; //scanf_s(\"%d%d%d\", a, b, c); //for (i = 0; i = 100; i++) { //\tif (i % 3 == a i % 5 == b i % 7 == c) // printf_s(\"%d \", i); // //} //if (i % 3 != a i % 5 != b i % 7 != c i100) // printf_s(\"No answer \"); //ä¹ é¢˜2.3 å€’ä¸‰è§’å½¢ //int n, /* è¾“å‡ºnè¡Œ; n=20 */ //\ti, /* æ‰“å°ç¬¬iè¡Œ */ //\tj; //scanf_s(\"%d\", n); //for (i = 1; i = n; i = i + 1) { //\t/* åœ¨ç¬¬iè¡Œï¼Œæ‰“å°(i-1)ä¸ªç©ºæ ¼ */ //\tfor (j = 1; j = i - 1; j = j + 1) printf_s(\" \"); //\t/* åœ¨ç¬¬iè¡Œï¼Œæ‰“å°(2*n-2*i+1)ä¸ª# */ //\tfor (j = 1; j = (2 * n - 2 * i + 1); j = j + 1) printf_s(\"#\"); //\tprintf_s(\" \"); /* è¾“å‡ºç»“æŸåæ¢è¡Œï¼Œå¦åˆ™æ‰€æœ‰çš„#å·åœ¨åŒä¸€è¡Œè¾“å‡º */ //} //ä¹ é¢˜2.4 å­åºåˆ—çš„å’Œ è¾“å…¥ä¸¤ä¸ªæ­£æ•´æ•°nï¼œmï¼œ10 6 ï¼Œè¾“å‡º ï¼Œä¿ç•™5ä½å°æ•°ã€‚è¾“å…¥åŒ…å«å¤šç»„æ•°æ®ï¼Œ æ³¨ï¼šé™·é˜±å°±æ˜¯åœ¨nç‰¹åˆ«å¤§æ—¶å¦‚æœç›´æ¥n*nå°±ä¼šæº¢å‡ºï¼Œæ‰€ä»¥åªèƒ½è¿é™¤ä¸¤æ¬¡ //int count = 0; //while (1) { //\tint n = 0; //\tint m = 0; //\tscanf_s(\"%d\", n); //\tscanf_s(\"%d\", m); //\tif (n == mn == 0) { // break; //\t} //\tcount++; //\tdouble sum = 0; //\tfor (int i = n; i = m; i++) { // sum += 1.0 / i / i; //\t} //\tprintf_s(\"Case %d:%.5f \", count, sum); //} //ä¹ é¢˜2.5 åˆ†æ•°åŒ–å°æ•°ï¼ˆdecimalï¼‰ //è¾“å…¥æ­£æ•´æ•°aï¼Œbï¼Œcï¼Œè¾“å‡ºa / bçš„å°æ•°å½¢å¼ï¼Œç²¾ç¡®åˆ°å°æ•°ç‚¹åcä½ã€‚aï¼Œbâ‰¤10 ^ 6ï¼Œcâ‰¤100ã€‚è¾“å…¥åŒ…å«å¤šç»„æ•°æ®ï¼Œç»“æŸæ ‡è®°ä¸ºaï¼bï¼cï¼0ã€‚ // int count = 0; //while (1) { //\tint a, b, c; //\tint k, d, i; //\tscanf_s(\"%d\", a); //\tscanf_s(\"%d\", b); //\tscanf_s(\"%d\", c); //\tif (a == 0b == 0 c==0 ) { // break; //\t} //\tcount++; //\tfor (i = 0; ic - 1; i++) //\t{ // /*æœºæ™ºåœ°æŠŠä½™æ•°æ”¾å¤§åå€ï¼Œä½¿ä¹‹é™¤ä»¥bå¹¶å–æ¨¡*/ // k = (k%b) * 10; // printf_s(\"Case %d:%.5f \", count, k / b); //\t} //\tk = (k%b) * 10; //\td = (k%b) * 10 / b; //\tif (d = 5)//åˆ¤æ–­ç¬¬c+1ä½å°æ•°æ˜¯å¦å¤§äºç­‰äº5ï¼Œif yes,ç¬¬cä½å°æ•°è¦è¿›1 //\t{ // printf_s(\"Case %d:%.5f \", count, k / b + 1); //\t} //\telse //\t{ // printf_s(\"Case %d:%.5f \", count, k / b); //\t} // //} int abc,def,ghi; int a[10],count=0; memset(a,0,sizeof(a)); // å°†aæ•°ç»„ä¸­çš„å€¼å…¨éƒ¨è®¾ç½®ä¸º0 for (abc = 123;abc 333;abc ++) { // åŸºæœ¬å¯ä»¥ç¡®å®šabcçš„æœ€å°å€¼å’Œæœ€å¤§å€¼ def = 2 * abc; ghi = 3 * abc; // è®¾ç½®æ•°ç»„ä¸­æ‰€æœ‰å¯¹åº”çš„9ä½æ•°å­—ä½ç½®çš„å€¼1 a[abc/100] = 1; // a a[abc/10%10] = 1; // b a[abc%10] = 1; // c a[def/100] = 1; // d a[def/10%10] = 1; // e a[def%10] = 1; // f a[ghi/100] = 1; // g a[ghi/10%10] = 1; // h a[ghi%10] = 1; // i int i; for (i=1;i=9;i++) { count += a[i]; } if (count == 9) { printf_s(\"%d %d %d \",abc,def,ghi); } // é‡ç½®count å’Œaæ•°ç»„ count = 0; memset(a,0,sizeof(a)); } system(\"pause\"); return 0; } â€”|â€”","tags":["C++"],"categories":["å…¶ä»–"]},{"title":"ç®—æ³•ç«èµ›å…¥é—¨ç»å…¸ç¬¬ä¸€ç« ","path":"/2019/08/14/20190814-algorithm1/","content":"// t1.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚//#include \"stdafx.h\"#include \"stdio.h\"#include stdlib.h#includemath.hint main(){\t//printf(\"%.1f \", 8.0 / 5.0); // %ç‚¹åé¢æ˜¯ä¿ç•™å‡ ä½æµ®ç‚¹å°æ•°1.6\t//printf(\"%.1f \", 8 / 5); // %ç‚¹åé¢æ˜¯ä¿ç•™å‡ ä½æµ®ç‚¹å°æ•° ç»“æœä¸º0\t//printf(\"%d \", 8.0 / 5.0); // æ•´æ•°å€¼ç”¨%dè¾“å‡ºï¼Œå®æ•°ç”¨%fè¾“å‡º # int æ˜¯2å­—èŠ‚ ï¼Œfloatæ˜¯4å­—èŠ‚ æº¢å‡º\t// //æ•´æ•°/æ•´æ•°ç­‰ä¸æ•´æ•°ï¼Œæµ®ç‚¹æ•°é™¤ä»¥æµ®ç‚¹æ•°ç­‰äºæµ®ç‚¹æ•°\t//printf(\"%.8f \", 1 + 2 * sqrt(3) / (5 - 0.1));\t//int a, b;\t//scanf_s(\"%d%d\", a, b); //scanf_s ä¸­çš„å ä½ç¬¦è¦å’Œå˜é‡çš„æ•°æ®ç±»å‹ä¸€ä¸€å¯¹åº”ï¼Œè¦åœ¨å˜é‡å‰é¢åŠ ä¸Šâ€œâ€ç¬¦å·\t//printf(\"%d \", a+b);\t//const double pi = acos(-1.0);\t//double r, h, s1, s2, s;\t//scanf_s(\"%1f%1f\", r, h);\t//s1 = pi*r*r;\t//s2 = 2 * pi*r*h;\t//s = s1*2.0 + s2;\t//printf_s(\"Area = %.3f \", s);\t//äº¤æ¢å˜é‡\t//int a, b, m, n;\t//scanf_s(\"%d%d\", a, b);\t///*a = a + b;\t//b = a - b;\t//a = a - b;*/\t//printf_s(\"%d %d \", b, a);\t//int a, b, m, n;\t//scanf_s(\"%d%d\", n, m);\t///*a = a + b;\t//b = a - b;\t//a = a - b;*/\t//a = (4*n-m) / 2;\t//b = n - a;\t//if (m % 2 == 1 || a 0 || b 0)\t//\tprintf_s(\"No answer! \");\t//else\t//\tprintf_s(\"Answer is chicken %d rabbit %d \", a, b);\t//exp1 è¾“å…¥ä¸‰ä¸ªæ•´æ•°ï¼Œæ±‚å‡ºä»–ä»¬çš„å¹³å‡æ•°ï¼Œä¿ç•™ä¸‰ä½å°æ•°\t//int a, b, c;\t//scanf_s(\"%d%d%d\", a, b, c);\t//printf_s(\"%.3f\", (a + b + c) / 3.0);\t//exp2 è¾“å…¥åæ°æ¸©åº¦f,è¾“å‡ºå¯¹åº”çš„æ‘„æ°æ¸©åº¦c,ä¿ç•™3ä½å°æ•°.æç¤º : c = 5(f-32)/9.\t//float f, c;\t//scanf_s(\"%f\", f);\t//c = 5*(f - 32) / 9.0;\t//printf_s(\"æ‘„åˆ¶æ¸©åº¦ä¸ºï¼š %.3f\", c);\t//exp3\t//int n;\t//scanf_s(\"%d\", n);\t//printf_s(\"%d\", n*(n + 1) / 2);\t//exp4 è¾“å…¥æ­£æ•´æ•°ã€Š360 ï¼Œè¾“å‡ºæ­£å¼¦ä½™å¼¦å‡½æ•°å€¼ã€‚æç¤ºï¼šä½¿ç”¨æ•°å­¦å‡½æ•°\t//int n; // #define PI 3.1415926\t//scanf_s(\"%d\", n);\t//if (n 0 || n360)\t//\tprintf_s(\"error\");\t//else\t//{\t//\tprintf_s(\"%.2f \", cos(n*PI/180));\t//\tprintf_s(\"%.2f\", sin(n*PI / 180));\t//}\t// // exp5 ä¸€ä»¶è¡£æœ95å…ƒï¼Œè‹¥æ¶ˆè´¹æ»¡300å…ƒå¯ä»¥æ‰“85æŠ˜ï¼Œè¾“å…¥è´­ä¹°è¡£æœçš„ç®€ä¹¦ï¼Œè¾“å‡ºéœ€è¦æ”¯ä»˜çš„é‡‘é¢ã€‚ä¿ç•™ä¸¤ä½å°æ•°\t//int n,money;\t//scanf_s(\"%d\", n);\t//money = n * 95;\t//if (money 300)\t//\tprintf_s(\"%.2f\", money* 0.85);\t//else\t//\tprintf_s(\"%.2f\", money);\t//\t//exp6 åˆ¤æ–­ä¸‰ä¸ªè¾¹æ˜¯å¦èƒ½æ„æˆç›´è§’ä¸‰è§’å½¢ è¾¹é•¿å‡ä¸ºæ­£æ•´æ•°\t//int a, b, c;\t//scanf_s(\"%d%d%d\", a, b, c);\t//if (((a*a + b*b)== c*c) || ((a*a + c*c) == b*b) || ((c*c + b*b) == a*a))\t//\tprintf_s(\"yes \");\t//else\t//\tprintf_s(\"not a triangle \");\t//exp7 åˆ¤æ–­ä¸€ä¸ªå¹´æ˜¯ä¸æ˜¯é—°å¹´ èƒ½è¢«4æ•´é™¤ä½†æ˜¯ä¸èƒ½è¢«100æ•´é™¤çš„å¹´ä»½\t//int n;\t//scanf_s(\"%d\", n);\t//if (n % 4 == 0 n % 100 != 0)\t//\tprintf_s(\"yes \");\t//else\t//\tprintf_s(\"no \");\t//\t//q1: intç±»å‹çš„æœ€å¤§å€¼å’Œæœ€å°å€¼æ˜¯å¤šå°‘ 2^(n - 1) - 1 -2^(n - 1)\t//q2; doubleæµ®ç‚¹æ•°èƒ½ç²¾ç¡®åˆ°å¤šå°‘ä½å°æ•°ï¼Ÿ éµå¾ªIEEEæ ‡å‡†çš„8å­—èŠ‚ï¼ˆ64ä½ï¼‰çš„doubleèƒ½è¡¨ç¤ºçš„æœ‰æ•ˆæ•°å­—çš„ä½æ•°æ˜¯ï¼š15 ~ 16\t//q3: doubleç±»å‹çš„æœ€å¤§æ­£æ•°å€¼å’Œæœ€å°æ­£æ•°å€¼\t//q4: é€»è¾‘è¿ç®—ç¬¦å·çš„ä¼˜å…ˆçº§\tsystem(\"pause\"); return 0;} æ€è€ƒé¢˜è§£æï¼š https://blog.csdn.net/panderang/article/details/54096426","tags":["ç®—æ³•"],"categories":["ç®—æ³•"]},{"title":"ç®—æ³•ç«èµ›å…¥é—¨ç»å…¸ç¬¬ä¸€ç« ","path":"/2019/08/14/algorithm1/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 | // t1.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚ // #include \"stdafx.h\" #include \"stdio.h\" #include stdlib.h #includemath.h int main() { //printf(\"%.1f \", 8.0 / 5.0); // %ç‚¹åé¢æ˜¯ä¿ç•™å‡ ä½æµ®ç‚¹å°æ•°1.6 //printf(\"%.1f \", 8 / 5); // %ç‚¹åé¢æ˜¯ä¿ç•™å‡ ä½æµ®ç‚¹å°æ•° ç»“æœä¸º0 //printf(\"%d \", 8.0 / 5.0); // æ•´æ•°å€¼ç”¨%dè¾“å‡ºï¼Œå®æ•°ç”¨%fè¾“å‡º # int æ˜¯2å­—èŠ‚ ï¼Œfloatæ˜¯4å­—èŠ‚ æº¢å‡º // //æ•´æ•°/æ•´æ•°ç­‰ä¸æ•´æ•°ï¼Œæµ®ç‚¹æ•°é™¤ä»¥æµ®ç‚¹æ•°ç­‰äºæµ®ç‚¹æ•° //printf(\"%.8f \", 1 + 2 * sqrt(3) / (5 - 0.1)); //int a, b; //scanf_s(\"%d%d\", a, b); //scanf_s ä¸­çš„å ä½ç¬¦è¦å’Œå˜é‡çš„æ•°æ®ç±»å‹ä¸€ä¸€å¯¹åº”ï¼Œè¦åœ¨å˜é‡å‰é¢åŠ ä¸Šâ€œâ€ç¬¦å· //printf(\"%d \", a+b); //const double pi = acos(-1.0); //double r, h, s1, s2, s; //scanf_s(\"%1f%1f\", r, h); //s1 = pi*r*r; //s2 = 2 * pi*r*h; //s = s1*2.0 + s2; //printf_s(\"Area = %.3f \", s); //äº¤æ¢å˜é‡ //int a, b, m, n; //scanf_s(\"%d%d\", a, b); ///*a = a + b; //b = a - b; //a = a - b;*/ //printf_s(\"%d %d \", b, a); //int a, b, m, n; //scanf_s(\"%d%d\", n, m); ///*a = a + b; //b = a - b; //a = a - b;*/ //a = (4*n-m) / 2; //b = n - a; //if (m % 2 == 1 || a 0 || b 0) //\tprintf_s(\"No answer! \"); //else //\tprintf_s(\"Answer is chicken %d rabbit %d \", a, b); //exp1 è¾“å…¥ä¸‰ä¸ªæ•´æ•°ï¼Œæ±‚å‡ºä»–ä»¬çš„å¹³å‡æ•°ï¼Œä¿ç•™ä¸‰ä½å°æ•° //int a, b, c; //scanf_s(\"%d%d%d\", a, b, c); //printf_s(\"%.3f\", (a + b + c) / 3.0); //exp2 è¾“å…¥åæ°æ¸©åº¦f,è¾“å‡ºå¯¹åº”çš„æ‘„æ°æ¸©åº¦c,ä¿ç•™3ä½å°æ•°.æç¤º : c = 5(f-32)/9. //float f, c; //scanf_s(\"%f\", f); //c = 5*(f - 32) / 9.0; //printf_s(\"æ‘„åˆ¶æ¸©åº¦ä¸ºï¼š %.3f\", c); //exp3 //int n; //scanf_s(\"%d\", n); //printf_s(\"%d\", n*(n + 1) / 2); //exp4 è¾“å…¥æ­£æ•´æ•°ã€Š360 ï¼Œè¾“å‡ºæ­£å¼¦ä½™å¼¦å‡½æ•°å€¼ã€‚æç¤ºï¼šä½¿ç”¨æ•°å­¦å‡½æ•° //int n; // #define PI 3.1415926 //scanf_s(\"%d\", n); //if (n 0 || n360) //\tprintf_s(\"error\"); //else //{ //\tprintf_s(\"%.2f \", cos(n*PI/180)); //\tprintf_s(\"%.2f\", sin(n*PI / 180)); //} // // exp5 ä¸€ä»¶è¡£æœ95å…ƒï¼Œè‹¥æ¶ˆè´¹æ»¡300å…ƒå¯ä»¥æ‰“85æŠ˜ï¼Œè¾“å…¥è´­ä¹°è¡£æœçš„ç®€ä¹¦ï¼Œè¾“å‡ºéœ€è¦æ”¯ä»˜çš„é‡‘é¢ã€‚ä¿ç•™ä¸¤ä½å°æ•° //int n,money; //scanf_s(\"%d\", n); //money = n * 95; //if (money 300) //\tprintf_s(\"%.2f\", money* 0.85); //else //\tprintf_s(\"%.2f\", money); // //exp6 åˆ¤æ–­ä¸‰ä¸ªè¾¹æ˜¯å¦èƒ½æ„æˆç›´è§’ä¸‰è§’å½¢ è¾¹é•¿å‡ä¸ºæ­£æ•´æ•° //int a, b, c; //scanf_s(\"%d%d%d\", a, b, c); //if (((a*a + b*b)== c*c) || ((a*a + c*c) == b*b) || ((c*c + b*b) == a*a)) //\tprintf_s(\"yes \"); //else //\tprintf_s(\"not a triangle \"); //exp7 åˆ¤æ–­ä¸€ä¸ªå¹´æ˜¯ä¸æ˜¯é—°å¹´ èƒ½è¢«4æ•´é™¤ä½†æ˜¯ä¸èƒ½è¢«100æ•´é™¤çš„å¹´ä»½ //int n; //scanf_s(\"%d\", n); //if (n % 4 == 0 n % 100 != 0) //\tprintf_s(\"yes \"); //else //\tprintf_s(\"no \"); // //q1: intç±»å‹çš„æœ€å¤§å€¼å’Œæœ€å°å€¼æ˜¯å¤šå°‘ 2^(n - 1) - 1 -2^(n - 1) //q2; doubleæµ®ç‚¹æ•°èƒ½ç²¾ç¡®åˆ°å¤šå°‘ä½å°æ•°ï¼Ÿ éµå¾ªIEEEæ ‡å‡†çš„8å­—èŠ‚ï¼ˆ64ä½ï¼‰çš„doubleèƒ½è¡¨ç¤ºçš„æœ‰æ•ˆæ•°å­—çš„ä½æ•°æ˜¯ï¼š15 ~ 16 //q3: doubleç±»å‹çš„æœ€å¤§æ­£æ•°å€¼å’Œæœ€å°æ­£æ•°å€¼ //q4: é€»è¾‘è¿ç®—ç¬¦å·çš„ä¼˜å…ˆçº§ system(\"pause\"); return 0; } â€”|â€” æ€è€ƒé¢˜è§£æï¼š https://blog.csdn.net/panderang/article/details/54096426","tags":["C++"],"categories":["å…¶ä»–"]},{"title":"æ–‡æœ¬æ£€æµ‹è°ƒç ”","path":"/2019/07/29/20190729-æ–‡æœ¬æ£€æµ‹è°ƒç ”/","content":"æ–‡æœ¬æ£€æµ‹é—®é¢˜ï¼Œå¹¿ä¹‰ä¸Šæ¥è¯´å¯ä»¥çœ‹åšæ˜¯ä¸€ä¸ªç›®æ ‡æ£€æµ‹çš„é—®é¢˜ï¼Œä½†æ˜¯ç›¸å½“äºç›®æ ‡æ£€æµ‹è¦ç®€å•çš„å¤šã€‚å› ä¸ºç›®æ ‡æ£€æµ‹å¾€å¾€é™¤äº†èƒŒæ™¯è¿˜æœ‰å…¶ä»–çš„ç±»ï¼Œè€Œæ–‡æœ¬æ£€æµ‹ï¼Œåªéœ€è¦æ£€æµ‹èƒŒæ™¯å’Œæ–‡æœ¬ç±»ä¸¤ä¸ªé—®é¢˜ã€‚å› æ­¤å¯ä»¥é‡‡ç”¨ç›®æ ‡æ£€æµ‹æˆ–è€…åˆ†å‰²çš„æ–¹æ³•æ¥è¿›è¡Œæ–‡æœ¬æ£€æµ‹ã€‚è€Œè§†é¢‘ä¸­çš„æ–‡æœ¬æ£€æµ‹ï¼Œä¹Ÿå¯ä»¥çœ‹åšæ˜¯è§†é¢‘ä¸­çš„ç›®æ ‡æ£€æµ‹ä¸­çš„ä¸€ç§ï¼Œæ„Ÿè§‰åº”è¯¥ä¹Ÿå¯ä»¥ç”¨è§†é¢‘ä¸­çš„ç›®æ ‡æ£€æµ‹+è·Ÿè¸ªæ¥åšã€‚ é€šå¸¸ç›®æ ‡è·Ÿè¸ªé¢ä¸´çš„æå¤§éš¾ç‚¹ï¼šç‰©ä½“å˜å½¢ã€äº®åº¦å˜åŒ–ã€å¿«é€Ÿç§»åŠ¨ã€èƒŒæ™¯å¹²æ‰°è¦†ç›–ã€‚å…¶ä¸­æœ€ä¸»è¦çš„ä¸‰ä¸ªéš¾é¢˜åˆ†åˆ«æ˜¯ç›®æ ‡èƒŒæ™¯çš„å˜åŒ–ï¼Œç‰©ä½“æœ¬èº«çš„å˜åŒ–ï¼Œå…‰ç…§å¼ºåº¦çš„å˜åŒ–ã€‚ å…‰æµæ³•å¸§é—´å·®åˆ†æ³•èƒŒæ™¯å·®åˆ†æ³•","tags":["è®¡ç®—æœºè§†è§‰"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"æ–‡æœ¬æ£€æµ‹è°ƒç ”","path":"/2019/07/29/æ–‡æœ¬æ£€æµ‹è°ƒç ”/","content":"æ–‡æœ¬æ£€æµ‹é—®é¢˜ï¼Œå¹¿ä¹‰ä¸Šæ¥è¯´å¯ä»¥çœ‹åšæ˜¯ä¸€ä¸ªç›®æ ‡æ£€æµ‹çš„é—®é¢˜ï¼Œä½†æ˜¯ç›¸å½“äºç›®æ ‡æ£€æµ‹è¦ç®€å•çš„å¤šã€‚å› ä¸ºç›®æ ‡æ£€æµ‹å¾€å¾€é™¤äº†èƒŒæ™¯è¿˜æœ‰å…¶ä»–çš„ç±»ï¼Œè€Œæ–‡æœ¬æ£€æµ‹ï¼Œåªéœ€è¦æ£€æµ‹èƒŒæ™¯å’Œæ–‡æœ¬ç±»ä¸¤ä¸ªé—®é¢˜ã€‚å› æ­¤å¯ä»¥é‡‡ç”¨ç›®æ ‡æ£€æµ‹æˆ–è€…åˆ†å‰²çš„æ–¹æ³•æ¥è¿›è¡Œæ–‡æœ¬æ£€æµ‹ã€‚è€Œè§†é¢‘ä¸­çš„æ–‡æœ¬æ£€æµ‹ï¼Œä¹Ÿå¯ä»¥çœ‹åšæ˜¯è§†é¢‘ä¸­çš„ç›®æ ‡æ£€æµ‹ä¸­çš„ä¸€ç§ï¼Œæ„Ÿè§‰åº”è¯¥ä¹Ÿå¯ä»¥ç”¨è§†é¢‘ä¸­çš„ç›®æ ‡æ£€æµ‹+è·Ÿè¸ªæ¥åšã€‚ é€šå¸¸ç›®æ ‡è·Ÿè¸ªé¢ä¸´çš„æå¤§éš¾ç‚¹ï¼šç‰©ä½“å˜å½¢ã€äº®åº¦å˜åŒ–ã€å¿«é€Ÿç§»åŠ¨ã€èƒŒæ™¯å¹²æ‰°è¦†ç›–ã€‚å…¶ä¸­æœ€ä¸»è¦çš„ä¸‰ä¸ªéš¾é¢˜åˆ†åˆ«æ˜¯ç›®æ ‡èƒŒæ™¯çš„å˜åŒ–ï¼Œç‰©ä½“æœ¬èº«çš„å˜åŒ–ï¼Œå…‰ç…§å¼ºåº¦çš„å˜åŒ–ã€‚ å…‰æµæ³•å¸§é—´å·®åˆ†æ³•èƒŒæ™¯å·®åˆ†æ³•","tags":["ç›®æ ‡æ£€æµ‹"],"categories":["è®¡ç®—æœºè§†è§‰"]},{"title":"c++ä¸­çº§æ•™ç¨‹  STL queue","path":"/2019/07/26/20190726-STLqueue/","content":"STL queue é˜Ÿåˆ—ï¼š FIFO å…ˆè¿›å…ˆå‡º è‡ªé€‚åº”å®¹å™¨ï¼ˆå®¹å™¨é€‚é…å™¨ï¼‰ æ ˆé€‚é…å™¨ STL queue queueint, dequeint q;queueint, listint q;q.empty()q.size()q.front()q.back()q.pop()q.push(item) å¯ä»¥ç”¨listå’Œdequeåšqueueå…ˆè¿›å…ˆå‡ºï¼Œåè¿›åå‡º // queue.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚//#include \"stdafx.h\"#includeiostream#includequeue#includelist#includedequeusing namespace std;int main(){\tqueueint, dequeint a;\tqueueint, listint b;\t//queueint, vectorint c; ä¸å¯ä»¥ï¼Œå› ä¸ºvectorä¸èƒ½è¿›è¡Œä¸¤ç«¯æ“ä½œ\t//é˜Ÿåˆ—æœ‰ä»€ä¹ˆç”¨é€”ï¼Ÿï¼Ÿï¼Ÿ\tqueueint q;\tq.push(10);\tq.push(5);\tq.push(-1);\tq.push(20);\tcout \"ç°åœ¨é˜Ÿåˆ—é‡Œæœ‰\" q.size() \"ä¸ªæ•°æ® \" endl;\tcout \"é˜Ÿé¦–çš„æ•°æ®ï¼š\" q.front() endl;\tcout \"é˜Ÿå°¾çš„æ•°æ®ï¼š\" q.back() endl;\tq.pop();\tcout \"æ–°çš„é˜Ÿé¦–çš„æ•°æ®ï¼š\" q.front() endl;\twhile (q.size() != 0)\t{ cout \" åˆ é™¤\" a.front() endl; q.pop();\t}\tif (q.empty())\t{ cout \"é˜Ÿåˆ—ä¸ºç©ºï¼\"endl;\t}\tsystem(\"pause\"); return 0;} ä¼˜å…ˆçº§é˜Ÿåˆ— priority_queue è‡ªé€‚åº”å®¹å™¨ï¼ˆå®¹å™¨é€‚é…å™¨ï¼‰ï¼šä¸èƒ½ä½¿ç”¨list æœ€å¤§å€¼ä¼˜å…ˆçº§é˜Ÿåˆ—ã€æœ€å°å€¼ä¼˜å…ˆçº§é˜Ÿåˆ—(å€¼è¶Šå¤§ï¼Œä¼˜å…ˆçº§è¶Šé«˜ï¼Œå€¼è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜) ä¼˜å…ˆçº§é˜Ÿåˆ—é€‚é…å™¨ STL priority_queue priority_queueint, dequeint pg1;//å¯¹é˜Ÿåˆ—é‡Œçš„æ•°æ®è¿›è¡Œéšæœºæ“ä½œï¼Œæ‰€ä»¥ä¸èƒ½ä½¿ç”¨listpriority_queueint, vectorint pg2; //vectoræ˜¯é»˜è®¤çš„ //è°“è¯priority_queueint, vectorint, greaterint pg2; //vectoræ˜¯é»˜è®¤çš„ï¼Œæœ€å°ä¼˜å…ˆé˜Ÿåˆ—pg.empty()pg.size()pg.top()pg.pop()pg.push(item)~~~~~~~~~~ // priority_queue.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚//#include \"stdafx.h\"#includeiostream#includedeque#includevector#includequeueusing namespace std;int main(){\tpriority_queueint, dequeint pg1; //å¯¹é˜Ÿåˆ—é‡Œçš„æ•°æ®è¿›è¡Œéšæœºæ“ä½œï¼Œæ‰€ä»¥ä¸èƒ½ä½¿ç”¨list\tpriority_queueint, vectorint pg2; //vectoræ˜¯é»˜è®¤çš„ //è°“è¯\tpriority_queueint, vectorint, greaterint pg2; //vectoræ˜¯é»˜è®¤çš„ï¼Œæœ€å°ä¼˜å…ˆé˜Ÿåˆ—\tpg2.push(10);\tpg2.push(5);\tpg2.push(-1);\tpg2.push(20);\tcout \"ä¼˜å…ˆçº§é˜Ÿåˆ—ä¸€å…±æœ‰ï¼š \" pg2.size() \"ä¸ªæ•°æ®\" endl;\tcout pg2.top() endl;\twhile (!pg2.empty())\t{ cout \"ä»ä¼˜å…ˆçº§é˜Ÿåˆ—é‡Œåˆ é™¤ï¼š \" pg2.top() endl; pg2.pop();\t}\tsystem(\"pause\"); return 0;}ä¼˜å…ˆçº§é˜Ÿåˆ—ä¸€å…±æœ‰ï¼š 4ä¸ªæ•°æ® ä»ä¼˜å…ˆçº§é˜Ÿåˆ—é‡Œåˆ é™¤ï¼š 20ä»ä¼˜å…ˆçº§é˜Ÿåˆ—é‡Œåˆ é™¤ï¼š 10ä»ä¼˜å…ˆçº§é˜Ÿåˆ—é‡Œåˆ é™¤ï¼š 5ä»ä¼˜å…ˆçº§é˜Ÿåˆ—é‡Œåˆ é™¤ï¼š -1è¯·æŒ‰ä»»æ„é”®ç»§ç»­. . .~~","tags":["C++"],"categories":["C++"]},{"title":"c++ä¸­çº§æ•™ç¨‹ é¡ºåºå®¹å™¨çš„å®šä¹‰","path":"/2019/07/26/20190726-é¡ºåºå®¹å™¨çš„å®šä¹‰/","content":"é¡ºåºå®¹å™¨çš„å®šä¹‰ é¡ºåºå®¹å™¨ vector list deque é¡ºåºå®¹å™¨é€‚é…å™¨ stack queue priority_queue","tags":["C++"],"categories":["C++"]},{"title":"c++ä¸­çº§æ•™ç¨‹  STL queue","path":"/2019/07/26/STLqueue/","content":"STL queue é˜Ÿåˆ—ï¼š FIFO å…ˆè¿›å…ˆå‡º è‡ªé€‚åº”å®¹å™¨ï¼ˆå®¹å™¨é€‚é…å™¨ï¼‰ æ ˆé€‚é…å™¨ STL queue 1 2 3 4 5 6 7 8 | queueint, dequeint q; queueint, listint q; q.empty() q.size() q.front() q.back() q.pop() q.push(item) â€”|â€” å¯ä»¥ç”¨listå’Œdequeåšqueueå…ˆè¿›å…ˆå‡ºï¼Œåè¿›åå‡º 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 | // queue.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚ // #include \"stdafx.h\" #includeiostream #includequeue #includelist #includedeque using namespace std; int main() { queueint, dequeint a; queueint, listint b; //queueint, vectorint c; ä¸å¯ä»¥ï¼Œå› ä¸ºvectorä¸èƒ½è¿›è¡Œä¸¤ç«¯æ“ä½œ //é˜Ÿåˆ—æœ‰ä»€ä¹ˆç”¨é€”ï¼Ÿï¼Ÿï¼Ÿ queueint q; q.push(10); q.push(5); q.push(-1); q.push(20); cout \"ç°åœ¨é˜Ÿåˆ—é‡Œæœ‰\" q.size() \"ä¸ªæ•°æ® \" endl; cout \"é˜Ÿé¦–çš„æ•°æ®ï¼š\" q.front() endl; cout \"é˜Ÿå°¾çš„æ•°æ®ï¼š\" q.back() endl; q.pop(); cout \"æ–°çš„é˜Ÿé¦–çš„æ•°æ®ï¼š\" q.front() endl; while (q.size() != 0) { cout \" åˆ é™¤\" a.front() endl; q.pop(); } if (q.empty()) { cout \"é˜Ÿåˆ—ä¸ºç©ºï¼\"endl; } system(\"pause\"); return 0; } â€”|â€” ä¼˜å…ˆçº§é˜Ÿåˆ— priority_queue è‡ªé€‚åº”å®¹å™¨ï¼ˆå®¹å™¨é€‚é…å™¨ï¼‰ï¼šä¸èƒ½ä½¿ç”¨list æœ€å¤§å€¼ä¼˜å…ˆçº§é˜Ÿåˆ—ã€æœ€å°å€¼ä¼˜å…ˆçº§é˜Ÿåˆ—(å€¼è¶Šå¤§ï¼Œä¼˜å…ˆçº§è¶Šé«˜ï¼Œå€¼è¶Šå°ä¼˜å…ˆçº§è¶Šé«˜) ä¼˜å…ˆçº§é˜Ÿåˆ—é€‚é…å™¨ STL priority_queue 1 2 3 4 5 6 7 8 9 10 11 | priority_queueint, dequeint pg1; //å¯¹é˜Ÿåˆ—é‡Œçš„æ•°æ®è¿›è¡Œéšæœºæ“ä½œï¼Œæ‰€ä»¥ä¸èƒ½ä½¿ç”¨list priority_queueint, vectorint pg2; //vectoræ˜¯é»˜è®¤çš„ //è°“è¯ priority_queueint, vectorint, greaterint pg2; //vectoræ˜¯é»˜è®¤çš„ï¼Œæœ€å°ä¼˜å…ˆé˜Ÿåˆ— pg.empty() pg.size() pg.top() pg.pop() pg.push(item) ~~~~~~~~~~ â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 | // priority_queue.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚ // #include \"stdafx.h\" #includeiostream #includedeque #includevector #includequeue using namespace std; int main() { priority_queueint, dequeint pg1; //å¯¹é˜Ÿåˆ—é‡Œçš„æ•°æ®è¿›è¡Œéšæœºæ“ä½œï¼Œæ‰€ä»¥ä¸èƒ½ä½¿ç”¨list priority_queueint, vectorint pg2; //vectoræ˜¯é»˜è®¤çš„ //è°“è¯ priority_queueint, vectorint, greaterint pg2; //vectoræ˜¯é»˜è®¤çš„ï¼Œæœ€å°ä¼˜å…ˆé˜Ÿåˆ— pg2.push(10); pg2.push(5); pg2.push(-1); pg2.push(20); cout \"ä¼˜å…ˆçº§é˜Ÿåˆ—ä¸€å…±æœ‰ï¼š \" pg2.size() \"ä¸ªæ•°æ®\" endl; cout pg2.top() endl; while (!pg2.empty()) { cout \"ä»ä¼˜å…ˆçº§é˜Ÿåˆ—é‡Œåˆ é™¤ï¼š \" pg2.top() endl; pg2.pop(); } system(\"pause\"); return 0; } ä¼˜å…ˆçº§é˜Ÿåˆ—ä¸€å…±æœ‰ï¼š 4ä¸ªæ•°æ® 20 ä»ä¼˜å…ˆçº§é˜Ÿåˆ—é‡Œåˆ é™¤ï¼š 20 ä»ä¼˜å…ˆçº§é˜Ÿåˆ—é‡Œåˆ é™¤ï¼š 10 ä»ä¼˜å…ˆçº§é˜Ÿåˆ—é‡Œåˆ é™¤ï¼š 5 ä»ä¼˜å…ˆçº§é˜Ÿåˆ—é‡Œåˆ é™¤ï¼š -1 è¯·æŒ‰ä»»æ„é”®ç»§ç»­. . . ~~ â€”|â€”","tags":["C++"],"categories":["C++"]},{"title":"c++ä¸­çº§æ•™ç¨‹ é¡ºåºå®¹å™¨çš„å®šä¹‰","path":"/2019/07/26/é¡ºåºå®¹å™¨çš„å®šä¹‰/","content":"é¡ºåºå®¹å™¨çš„å®šä¹‰ é¡ºåºå®¹å™¨ vector list deque é¡ºåºå®¹å™¨é€‚é…å™¨ stack queue priority_queue","tags":["C++"],"categories":["C++"]},{"title":"firstblog","path":"/2019/07/25/20190725-firstblog/","content":"ä»¥ä¸Šæ˜¯æ‘˜è¦ Hello World","tags":["å…¶ä»–"],"categories":["å…¶ä»–"]},{"title":"c++ä¸­çº§æ•™ç¨‹  STL stack","path":"/2019/07/25/20190725-STL-stack/","content":"STL stack (å †) æ ˆï¼š LIFO åè¿›å…ˆå‡º è‡ªé€‚åº”å®¹å™¨ï¼ˆå®¹å™¨é€‚é…å™¨ï¼‰ æ ˆé€‚é…å™¨ STL stack stackint, dequeint s;stackint, vectorint s;stackint, listint s;s.empty()s.size()s.pop()s.push(item) // stack1.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚//#include \"stdafx.h\"#include iostream#include vector#include list#include stackusing namespace std;int main(){\tstackint, dequeint a;\tstackint, vectorint b;\tstackint, listint c;\tstackint d; //é»˜è®¤ç”¨deque\t//ä»€ä¹ˆæ˜¯å †æ ˆï¼Ÿ å…ˆè¿›åå‡ºï¼Œåè¿›å…ˆå‡º\td.push(25);\td.push(10);\td.push(1);\td.push(5);\tint x = 0;\tcout \"ç°åœ¨æ ˆé‡Œä¸€å…±æœ‰ï¼š\" d.size() \"ä¸ªæ•°æ®ã€‚\" endl;\twhile (d.empty() == false)\t{ x = d.top(); //æŸ¥çœ‹æ•°æ®å¹¶ä¸”è¿”å› d.pop();//åˆ é™¤ï¼Œä¸è¿”å› cout x endl;\t}\t//x = d.top(); //æŸ¥çœ‹æ•°æ®å¹¶ä¸”è¿”å›\t//d.pop();//åˆ é™¤ï¼Œä¸è¿”å›\t//cout x endl;\tcout \"ç°åœ¨æ ˆé‡Œä¸€å…±æœ‰ï¼š\" d.size() \"ä¸ªæ•°æ®ã€‚\" endl;\tsystem(\"pause\"); return 0;}ç°åœ¨æ ˆé‡Œä¸€å…±æœ‰ï¼š4ä¸ªæ•°æ®ã€‚ ç°åœ¨æ ˆé‡Œä¸€å…±æœ‰ï¼š0ä¸ªæ•°æ®ã€‚è¯·æŒ‰ä»»æ„é”®ç»§ç»­. . .","tags":["C++"],"categories":["ç®—æ³•"]},{"title":"c++ä¸­çº§æ•™ç¨‹(ä¸€)","path":"/2019/07/25/20190725-é¡ºåºå®¹å™¨-STL-deque-ç±»/","content":"é¡ºåºå®¹å™¨ STL deque ç±» dequeæ˜¯ä¸€ä¸ªåŠ¨æ€æ•°ç»„ dequeä¸vector éå¸¸ç±»ä¼¼ dequeå¯ä»¥åœ¨æ•°ç»„å¼€å¤´å’Œæœ«å°¾æ’å…¥å’Œåˆ é™¤æ•°æ® // demo3.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚//#include \"stdafx.h\"#include iostream#include deque#include algorithmusing namespace std;int main(){dequeint a;a.push_back(3);a.push_back(4);a.push_back(5);a.push_back(6);//vectoråªèƒ½push_backa.push_front(2);a.push_front(1);a.push_front(9);a.push_front(8);for (size_t nCount = 0; nCount a.size(); ++nCount){cout \"a[\"nCount\"]\" \"= \" a[nCount] endl;}cout endlendl;a.pop_front();// å‰é¢åˆ é™¤a.pop_back();// åé¢åˆ é™¤cout \"åˆ é™¤ä¹‹åï¼š\" endl;/*for (size_t nCount = 0; nCount a.size(); ++nCount){cout \"a[\" nCount \"]\" a[nCount] endl;}*/dequeint::iterator iElementLocater; //è¿™è¾¹ä½¿ç”¨äº†è¿­ä»£å™¨ distence å¯ä»¥è®¡ç®—å½“å‰for (iElementLocater = a.begin();iElementLocater != a.end();++iElementLocater){size_t nOffset = distance(a.begin(), iElementLocater);//distence å¯ä»¥è®¡ç®—å½“å‰ä¸‹æ ‡ä¸beginå¼€å§‹çš„ï¼Œè·ç¦»æ­£å¥½æ˜¯ä¸‹æ ‡cout \"a[\" nOffset \"]\" \"= \"*iElementLocater endl;}system(\"pause\");return 0;}a[0]= 8a[1]= 9a[2]= 1a[3]= 2a[4]= 3a[5]= 4a[6]= 5a[7]= 6åˆ é™¤ä¹‹åï¼ša[0]= 9a[1]= 1a[2]= 2a[3]= 3a[4]= 4a[5]= 5è¯·æŒ‰ä»»æ„é”®ç»§ç»­. . . é †åºå®¹å™¨ STL list é¡ å®ä¾‹åŒ–std::listå¯¹è±¡ åœ¨listå¼€å¤´æ’å…¥å…ƒç´  åœ¨listæœ«å°¾æ’å…¥å…ƒç´  åœ¨listä¸­é—´æ’å…¥å…ƒç´  åˆ é™¤listä¸­çš„å…ƒç´  å¯¹listä¸­çš„å…ƒç´ è¿›è¡Œåè½¬å’Œæ’åº // list.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚//#include \"stdafx.h\"#includeiostream#includelistusing namespace std;void PrintListContent(const listint listInput);int main(){listint a;listint b;b.push_back(100);b.push_back(200);b.push_back(300);b.push_back(400);b.push_back(500);PrintListContent(b);cout endl;a.push_front(4);a.push_front(3);a.push_front(2);a.push_front(1);a.push_back(5);//ä½¿ç”¨é“¾è¡¨æ•°æ®ï¼Œä¸èƒ½ä½¿ç”¨ä¸‹æ ‡ï¼Œåªèƒ½ä½¿ç”¨è¿­ä»£å™¨listint::iterator iter;iter = a.begin();a.insert(iter, 10);// åœ¨beginå‰é¢æ’å…¥10ï¼Œç¬¬ä¸€ä¸ªå‚æ•°è¿­ä»£å™¨ï¼ŒæŒ‡å®šæ’å…¥çš„ä½ç½®a.insert(a.end(),10);PrintListContent(a);//å°†bæ’å…¥åˆ°aä¹‹ä¸­a.insert(a.begin(), b.begin(), b.end());a.insert(iter,++b.begin(),--b.end())PrintListContent(a);system(\"pause\");return 0;}void PrintListContent(const listint listInput){//ä¼šæ˜¯ä¸€ä¸ªåº•å±‚ constï¼Œå³å…¶æ‰€æŒ‡å¯¹è±¡å¯ä»¥æ”¹å˜ï¼Œä½†ä¸èƒ½æ”¹å˜å…¶æ‰€æŒ‡å¯¹è±¡çš„å€¼ã€‚listint::const_iterator iter;for (iter = listInput.begin(); iter != listInput.end(); ++iter){cout *iter endl;}} // listdelet.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚//#include \"stdafx.h\"#includeiostream#includelistusing namespace std;void PrintListContent(const listint listInput);int main(){\tlistint a;\ta.push_front(4);\ta.push_front(3);\tlistint::iterator iElementValueTwo;\tiElementValueTwo = a.insert(a.begin(),2); // inset æ‰è¿”å›è¿­ä»£å™¨è¿­ä»£å™¨æŒ‡å‘è¿™ä¸ªä½ç½®\ta.push_front(1);\ta.push_back(0);\tcout \"åˆ é™¤ä¹‹å‰\" endl;\tPrintListContent(a);\t// åˆ é™¤2\tcout \"åˆ é™¤ä¹‹å\" endl;\ta.erase(iElementValueTwo);\t//a.erase(a.beigin(),iElementValueTwo); åˆ é™¤ä»ç¬¬ä¸€ä¸ªè¿­ä»£å™¨åˆ°ç¬¬äºŒä¸ªè¿­ä»£å™¨æ‰€æœ‰çš„æ•°æ®\tPrintListContent(a);\tsystem(\"pause\"); return 0;}void PrintListContent(const listint listInput){\t//ä¼šæ˜¯ä¸€ä¸ªåº•å±‚ constï¼Œå³å…¶æ‰€æŒ‡å¯¹è±¡å¯ä»¥æ”¹å˜ï¼Œä½†ä¸èƒ½æ”¹å˜å…¶æ‰€æŒ‡å¯¹è±¡çš„å€¼ã€‚\tcout \"{\";\tlistint::const_iterator iter;\tfor (iter = listInput.begin(); iter != listInput.end(); ++iter)\t{ cout *iter \" \";\t}\tcout \"}\" endl;}åˆ é™¤ä¹‹å‰{1 2 3 4 0 }åˆ é™¤ä¹‹å{1 3 4 0 }è¯·æŒ‰ä»»æ„é”®ç»§ç»­. . . // list3.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚//#include \"stdafx.h\"#includeiostream#includelistusing namespace std;void PrintListContent(const listint listInput);int main(){\tlistint a;\ta.push_front(4);\ta.push_front(3);\ta.push_front(2);\ta.push_front(1);\tPrintListContent(a);\tcout \"åè½¬ä¹‹åçš„æ•°æ®ï¼š\" endl;\ta.reverse();\tPrintListContent(a);\tlistint b;\tb.push_front(4);\tb.push_front(53);\tb.push_front(24);\tb.push_front(132);\tPrintListContent(b);\tcout \"æ’åºä¹‹åçš„æ•°æ®ï¼š\" endl;\tb.sort();\tPrintListContent(b);\tsystem(\"pause\"); return 0;}void PrintListContent(const listint listInput){\t//ä¼šæ˜¯ä¸€ä¸ªåº•å±‚ constï¼Œå³å…¶æ‰€æŒ‡å¯¹è±¡å¯ä»¥æ”¹å˜ï¼Œä½†ä¸èƒ½æ”¹å˜å…¶æ‰€æŒ‡å¯¹è±¡çš„å€¼ã€‚\tcout \"{\";\tlistint::const_iterator iter;\tfor (iter = listInput.begin(); iter != listInput.end(); ++iter)\t{ cout *iter \" \";\t}\tcout \"}\" endl;}{1 2 3 4 }åè½¬ä¹‹åçš„æ•°æ®ï¼š{4 3 2 1 }{132 24 53 4 }æ’åºä¹‹åçš„æ•°æ®ï¼š{4 24 53 132 }è¯·æŒ‰ä»»æ„é”®ç»§ç»­. . .","tags":["C++"],"categories":["C++"]},{"title":"c++ä¸­çº§æ•™ç¨‹  STL stack","path":"/2019/07/25/STL-stack/","content":"STL stack (å †) æ ˆï¼š LIFO åè¿›å…ˆå‡º è‡ªé€‚åº”å®¹å™¨ï¼ˆå®¹å™¨é€‚é…å™¨ï¼‰ æ ˆé€‚é…å™¨ STL stack 1 2 3 4 5 6 7 | stackint, dequeint s; stackint, vectorint s; stackint, listint s; s.empty() s.size() s.pop() s.push(item) â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 | // stack1.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚ // #include \"stdafx.h\" #include iostream #include vector #include list #include stack using namespace std; int main() { stackint, dequeint a; stackint, vectorint b; stackint, listint c; stackint d; //é»˜è®¤ç”¨deque //ä»€ä¹ˆæ˜¯å †æ ˆï¼Ÿ å…ˆè¿›åå‡ºï¼Œåè¿›å…ˆå‡º d.push(25); d.push(10); d.push(1); d.push(5); int x = 0; cout \"ç°åœ¨æ ˆé‡Œä¸€å…±æœ‰ï¼š\" d.size() \"ä¸ªæ•°æ®ã€‚\" endl; while (d.empty() == false) { x = d.top(); //æŸ¥çœ‹æ•°æ®å¹¶ä¸”è¿”å› d.pop();//åˆ é™¤ï¼Œä¸è¿”å› cout x endl; } //x = d.top(); //æŸ¥çœ‹æ•°æ®å¹¶ä¸”è¿”å› //d.pop();//åˆ é™¤ï¼Œä¸è¿”å› //cout x endl; cout \"ç°åœ¨æ ˆé‡Œä¸€å…±æœ‰ï¼š\" d.size() \"ä¸ªæ•°æ®ã€‚\" endl; system(\"pause\"); return 0; } ç°åœ¨æ ˆé‡Œä¸€å…±æœ‰ï¼š4ä¸ªæ•°æ®ã€‚ 5 1 10 25 ç°åœ¨æ ˆé‡Œä¸€å…±æœ‰ï¼š0ä¸ªæ•°æ®ã€‚ è¯·æŒ‰ä»»æ„é”®ç»§ç»­. . . â€”|â€”","tags":["C++"],"categories":["C++"]},{"title":"firstblog","path":"/2019/07/25/firstblog/","content":"ä»¥ä¸Šæ˜¯æ‘˜è¦ Hello World","tags":["ç¼–ç¨‹"],"categories":["å…¶ä»–"]},{"title":"c++ä¸­çº§æ•™ç¨‹(ä¸€)","path":"/2019/07/25/é¡ºåºå®¹å™¨-STL-deque-ç±»/","content":"é¡ºåºå®¹å™¨ STL deque ç±» dequeæ˜¯ä¸€ä¸ªåŠ¨æ€æ•°ç»„ dequeä¸vector éå¸¸ç±»ä¼¼ dequeå¯ä»¥åœ¨æ•°ç»„å¼€å¤´å’Œæœ«å°¾æ’å…¥å’Œåˆ é™¤æ•°æ® 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 | // demo3.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚ // #include \"stdafx.h\" #include iostream #include deque #include algorithm using namespace std; int main() { dequeint a; a.push_back(3); a.push_back(4); a.push_back(5); a.push_back(6); //vectoråªèƒ½push_back a.push_front(2); a.push_front(1); a.push_front(9); a.push_front(8); for (size_t nCount = 0; nCount a.size(); ++nCount) { cout \"a[\"nCount\"]\" \"= \" a[nCount] endl; } cout endlendl; a.pop_front();// å‰é¢åˆ é™¤ a.pop_back();// åé¢åˆ é™¤ cout \"åˆ é™¤ä¹‹åï¼š\" endl; /*for (size_t nCount = 0; nCount a.size(); ++nCount) { cout \"a[\" nCount \"]\" a[nCount] endl; }*/ dequeint::iterator iElementLocater; //è¿™è¾¹ä½¿ç”¨äº†è¿­ä»£å™¨ distence å¯ä»¥è®¡ç®—å½“å‰ for (iElementLocater = a.begin(); iElementLocater != a.end(); ++iElementLocater) { size_t nOffset = distance(a.begin(), iElementLocater);//distence å¯ä»¥è®¡ç®—å½“å‰ä¸‹æ ‡ä¸beginå¼€å§‹çš„ï¼Œè·ç¦»æ­£å¥½æ˜¯ä¸‹æ ‡ cout \"a[\" nOffset \"]\" \"= \"*iElementLocater endl; } system(\"pause\"); return 0; } a[0]= 8 a[1]= 9 a[2]= 1 a[3]= 2 a[4]= 3 a[5]= 4 a[6]= 5 a[7]= 6 åˆ é™¤ä¹‹åï¼š a[0]= 9 a[1]= 1 a[2]= 2 a[3]= 3 a[4]= 4 a[5]= 5 è¯·æŒ‰ä»»æ„é”®ç»§ç»­. . . â€”|â€” é †åºå®¹å™¨ STL list é¡ å®ä¾‹åŒ–std::listå¯¹è±¡ åœ¨listå¼€å¤´æ’å…¥å…ƒç´  åœ¨listæœ«å°¾æ’å…¥å…ƒç´  åœ¨listä¸­é—´æ’å…¥å…ƒç´  åˆ é™¤listä¸­çš„å…ƒç´  å¯¹listä¸­çš„å…ƒç´ è¿›è¡Œåè½¬å’Œæ’åº 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 | // list.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚ // #include \"stdafx.h\" #includeiostream #includelist using namespace std; void PrintListContent(const listint listInput); int main() { listint a; listint b; b.push_back(100); b.push_back(200); b.push_back(300); b.push_back(400); b.push_back(500); PrintListContent(b); cout endl; a.push_front(4); a.push_front(3); a.push_front(2); a.push_front(1); a.push_back(5); //ä½¿ç”¨é“¾è¡¨æ•°æ®ï¼Œä¸èƒ½ä½¿ç”¨ä¸‹æ ‡ï¼Œåªèƒ½ä½¿ç”¨è¿­ä»£å™¨ listint::iterator iter; iter = a.begin(); a.insert(iter, 10);// åœ¨beginå‰é¢æ’å…¥10ï¼Œç¬¬ä¸€ä¸ªå‚æ•°è¿­ä»£å™¨ï¼ŒæŒ‡å®šæ’å…¥çš„ä½ç½® a.insert(a.end(),10); PrintListContent(a); //å°†bæ’å…¥åˆ°aä¹‹ä¸­ a.insert(a.begin(), b.begin(), b.end()); a.insert(iter,++b.begin(),--b.end()) PrintListContent(a); system(\"pause\"); return 0; } void PrintListContent(const listint listInput) { //ä¼šæ˜¯ä¸€ä¸ªåº•å±‚ constï¼Œå³å…¶æ‰€æŒ‡å¯¹è±¡å¯ä»¥æ”¹å˜ï¼Œä½†ä¸èƒ½æ”¹å˜å…¶æ‰€æŒ‡å¯¹è±¡çš„å€¼ã€‚ listint::const_iterator iter; for (iter = listInput.begin(); iter != listInput.end(); ++iter) { cout *iter endl; } } â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 | // listdelet.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚ // #include \"stdafx.h\" #includeiostream #includelist using namespace std; void PrintListContent(const listint listInput); int main() { listint a; a.push_front(4); a.push_front(3); listint::iterator iElementValueTwo; iElementValueTwo = a.insert(a.begin(),2); // inset æ‰è¿”å›è¿­ä»£å™¨è¿­ä»£å™¨æŒ‡å‘è¿™ä¸ªä½ç½® a.push_front(1); a.push_back(0); cout \"åˆ é™¤ä¹‹å‰\" endl; PrintListContent(a); // åˆ é™¤2 cout \"åˆ é™¤ä¹‹å\" endl; a.erase(iElementValueTwo); //a.erase(a.beigin(),iElementValueTwo); åˆ é™¤ä»ç¬¬ä¸€ä¸ªè¿­ä»£å™¨åˆ°ç¬¬äºŒä¸ªè¿­ä»£å™¨æ‰€æœ‰çš„æ•°æ® PrintListContent(a); system(\"pause\"); return 0; } void PrintListContent(const listint listInput) { //ä¼šæ˜¯ä¸€ä¸ªåº•å±‚ constï¼Œå³å…¶æ‰€æŒ‡å¯¹è±¡å¯ä»¥æ”¹å˜ï¼Œä½†ä¸èƒ½æ”¹å˜å…¶æ‰€æŒ‡å¯¹è±¡çš„å€¼ã€‚ cout \"{\"; listint::const_iterator iter; for (iter = listInput.begin(); iter != listInput.end(); ++iter) { cout *iter \" \"; } cout \"}\" endl; } åˆ é™¤ä¹‹å‰ {1 2 3 4 0 } åˆ é™¤ä¹‹å {1 3 4 0 } è¯·æŒ‰ä»»æ„é”®ç»§ç»­. . . â€”|â€” 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 | // list3.cpp : å®šä¹‰æ§åˆ¶å°åº”ç”¨ç¨‹åºçš„å…¥å£ç‚¹ã€‚ // #include \"stdafx.h\" #includeiostream #includelist using namespace std; void PrintListContent(const listint listInput); int main() { listint a; a.push_front(4); a.push_front(3); a.push_front(2); a.push_front(1); PrintListContent(a); cout \"åè½¬ä¹‹åçš„æ•°æ®ï¼š\" endl; a.reverse(); PrintListContent(a); listint b; b.push_front(4); b.push_front(53); b.push_front(24); b.push_front(132); PrintListContent(b); cout \"æ’åºä¹‹åçš„æ•°æ®ï¼š\" endl; b.sort(); PrintListContent(b); system(\"pause\"); return 0; } void PrintListContent(const listint listInput) { //ä¼šæ˜¯ä¸€ä¸ªåº•å±‚ constï¼Œå³å…¶æ‰€æŒ‡å¯¹è±¡å¯ä»¥æ”¹å˜ï¼Œä½†ä¸èƒ½æ”¹å˜å…¶æ‰€æŒ‡å¯¹è±¡çš„å€¼ã€‚ cout \"{\"; listint::const_iterator iter; for (iter = listInput.begin(); iter != listInput.end(); ++iter) { cout *iter \" \"; } cout \"}\" endl; } {1 2 3 4 } åè½¬ä¹‹åçš„æ•°æ®ï¼š {4 3 2 1 } {132 24 53 4 } æ’åºä¹‹åçš„æ•°æ®ï¼š {4 24 53 132 } è¯·æŒ‰ä»»æ„é”®ç»§ç»­. . . â€”|â€”","tags":["C++"],"categories":["ç®—æ³•"]},{"title":"å…³äºæˆ‘","path":"/about/index.html","content":"ğŸ‘‹ Hi, Iâ€™m Caius Lu è®¡ç®—æœºè§†è§‰ç ”ç©¶è€… | å¤§æ¨¡å‹åˆå­¦è€… | AI æ¢ç´¢è€… | æŠ€æœ¯åšä¸» ğŸ¯ å…³äºæˆ‘æˆ‘æ˜¯ä¸€åä¸“æ³¨äºè®¡ç®—æœºè§†è§‰å’Œæ·±åº¦å­¦ä¹ é¢†åŸŸçš„æŠ€æœ¯ç ”ç©¶è€…ã€‚åŒæ—¶æˆ‘ä¹Ÿæ˜¯ä¸€åå¤§æ¨¡å‹(LLM)åˆå­¦è€…ï¼Œæ­£åœ¨ç§¯ææ¢ç´¢è¿™ä¸€å‰æ²¿é¢†åŸŸã€‚ çƒ­è¡·äºæ¢ç´¢ AI æŠ€æœ¯çš„æ— é™å¯èƒ½ï¼Œå°¤å…¶åœ¨ä»¥ä¸‹æ–¹å‘æœ‰æ·±å…¥ç ”ç©¶ï¼š ğŸ¯ ç›®æ ‡æ£€æµ‹ä¸è·Ÿè¸ª - å¤šç›¸æœºè¡Œäººè·Ÿè¸ªã€ReID ğŸ“ å‡ ä½•è§†è§‰ - å•åº”æ€§çŸ©é˜µã€ç›¸æœºæ ‡å®šã€SLAM ğŸ§  æ·±åº¦å­¦ä¹  - ç¥ç»ç½‘ç»œæ¶æ„ã€æ¨¡å‹ä¼˜åŒ– ğŸ¤– å¤§æ¨¡å‹ (LLM) - å‚æ•°é«˜æ•ˆå¾®è°ƒ (PEFT/LoRA)ã€RAG ğŸš— æ™ºæ…§äº¤é€š - äº¤é€šåœºæ™¯æ„ŸçŸ¥ä¸åˆ†æ ğŸ”— è”ç³»æ–¹å¼GitHub ğŸ‘‰ ç‚¹å‡»è®¿é—® GitHub ä¸»é¡µ (github.com/caiusy) è®¿é—®æˆ‘çš„ GitHub ä¸»é¡µï¼ŒæŸ¥çœ‹å¼€æºé¡¹ç›®å’Œä»£ç ã€‚ åšå®¢ ğŸ‘‰ ç‚¹å‡»è®¿é—®åšå®¢é¦–é¡µ è¿™é‡Œè®°å½•æˆ‘çš„æŠ€æœ¯ç¬”è®°å’Œå­¦ä¹ å¿ƒå¾—ã€‚ ğŸ“Š GitHub ç»Ÿè®¡ ğŸ› ï¸ æŠ€æœ¯æ ˆè¯­è¨€ æ¡†æ¶ å·¥å…· ğŸ“ åšå®¢å†…å®¹æœ¬åšå®¢ä¸»è¦åˆ†äº«ä»¥ä¸‹å†…å®¹ï¼š è®ºæ–‡é˜…è¯»ç¬”è®° - è®¡ç®—æœºè§†è§‰é¢†åŸŸå‰æ²¿è®ºæ–‡è§£è¯» å¤§æ¨¡å‹æ¢ç´¢ - LLM æ ¸å¿ƒåŸç†ã€LoRA å¾®è°ƒå®æˆ˜ æŠ€æœ¯æ•™ç¨‹ - æ·±åº¦å­¦ä¹ ã€å›¾åƒå¤„ç†å®è·µæ•™ç¨‹ é¡¹ç›®å®è·µ - çœŸå®é¡¹ç›®ç»éªŒåˆ†äº« å­¦ä¹ å¿ƒå¾— - æŠ€æœ¯æˆé•¿è·¯ä¸Šçš„æ€è€ƒ ğŸ“¬ ç•™è¨€å¦‚æœä½ æœ‰ä»»ä½•é—®é¢˜æˆ–æƒ³æ³•ï¼Œæ¬¢è¿åœ¨ä¸‹æ–¹ç•™è¨€äº¤æµï¼"}]