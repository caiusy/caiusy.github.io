[{"title":"单调栈完全指南：从 O(n²) 到 O(n) 的优雅跃迁","path":"/2026/02/19/monotonic-stack-guide/","content":"单调栈完全指南：从 O(n²) 到 O(n) 的优雅跃迁🎯 一句话本质 单调栈通过维护单调性来及时淘汰无用候选元素，将”寻找下一个更大/更小元素”问题从 O(n²) 优化到 O(n)。 它不是一种数据结构，而是一种遍历策略 —— 通过栈的单调性保证”不漏掉答案”的同时”跳过不可能的区域”。 🤔 为什么需要单调栈？问题场景给定数组 [2, 1, 5, 6, 2, 3]，对于每个元素，找到它右边第一个比它大的元素。 暴力解法： def next_greater_brute(nums): n = len(nums) result = [-1] * n for i in range(n): for j in range(i+1, n): # 向右扫描 if nums[j] nums[i]: result[i] = nums[j] break return result 时间复杂度：O(n²) - 每个元素都要向右扫描 问题：大量重复计算 💡 关键洞察如果 nums[i] nums[j]（i j），那么对于 j 右边的元素来说，i 永远不可能是答案！ 为什么？ 因为 j 更大且更靠右，它会”遮挡”住 i。 这就是单调栈的核心：维护一个递减的候选序列，及时淘汰无用元素。 🔬 工作原理：数据流与状态转移单调栈的处理流程以 [2, 1, 5, 6, 2, 3] 为例，维护一个单调递减栈（栈底到栈顶递减）： 步骤 当前元素 栈状态（存索引） 操作 发现的答案 1 2 (i=0) [0] 直接入栈 - 2 1 (i=1) [0, 1] 1 2，直接入栈 - 3 5 (i=2) [2] 5 1，弹出1；5 2，弹出2；5入栈 nums[1]=5, nums[0]=5 4 6 (i=3) [3] 6 5，弹出5；6入栈 nums[2]=6 5 2 (i=4) [3, 4] 2 6，直接入栈 - 6 3 (i=5) [3, 5] 3 2，弹出2；3 6，3入栈 nums[4]=3 最终结果：[5, 5, 6, -1, 3, -1] 📊 可视化：栈的动态演变 上图展示了处理数组的6个关键步骤，每个步骤包含： 输入数组：当前处理的元素用红色高亮 栈状态：绿色方块表示栈中的元素 结果数组：黄色表示已找到答案，灰色表示未找到 🎬 动画演示 动画展示了完整的处理流程，可以清晰看到： 元素逐个进入处理 栈的动态变化（入栈/出栈） 结果数组的实时更新 💻 核心代码实现def next_greater_element(nums): n = len(nums) result = [-1] * n # 初始化答案数组 stack = [] # 单调栈（存储索引） for i in range(n): # 当前元素 栈顶元素时，找到了栈顶的答案 while stack and nums[i] nums[stack[-1]]: idx = stack.pop() result[idx] = nums[i] # 记录答案 stack.append(i) # 当前索引入栈 return result 数据流向： 输入数组 → 逐个处理 → 单调栈（维护候选） → 输出答案数组 ↑ ↓ └────── 弹出时记录答案 ──┘ ⏱️ 时间复杂度证明 为什么是 O(n)？关键洞察：每个元素最多入栈一次，出栈一次。 证明： 外层循环：遍历 n 个元素 → O(n) 内层 while 循环：看似嵌套，但总共最多弹出 n 次 每个元素入栈 1 次 每个元素出栈 ≤ 1 次 总操作次数 ≤ 2n 均摊分析： 总入栈次数 = n总出栈次数 ≤ n总操作 = n + n = 2n = O(n) 🎯 经典问题实战问题 1：下一个更大元素 II（循环数组）问题：数组是循环的，如 [1, 2, 1] 中，最后一个 1 的答案是 2。 解法：将数组复制一遍，模拟循环。 def next_greater_circular(nums): n = len(nums) result = [-1] * n stack = [] # 遍历两遍数组（模拟循环） for i in range(2 * n): idx = i % n # 实际索引 while stack and nums[idx] nums[stack[-1]]: result[stack.pop()] = nums[idx] if i n: # 只在第一遍时入栈 stack.append(idx) return result 问题 2：柱状图中最大的矩形问题：给定柱状图高度 [2, 1, 5, 6, 2, 3]，找最大矩形面积。 核心思路：对于每个柱子，找到它左右两边第一个比它矮的柱子，计算以它为高的矩形面积。 def largest_rectangle_area(heights): stack = [] max_area = 0 heights = [0] + heights + [0] # 哨兵技巧 for i, h in enumerate(heights): while stack and h heights[stack[-1]]: height_idx = stack.pop() height = heights[height_idx] width = i - stack[-1] - 1 # 左右边界 max_area = max(max_area, height * width) stack.append(i) return max_area 上图展示了柱状图中最大矩形的可视化，红色虚线框标出了面积为10的最大矩形。 问题 3：每日温度问题：给定温度数组 [73, 74, 75, 71, 69, 72, 76, 73]，计算每天需要等几天才能等到更暖和的温度。 def daily_temperatures(temperatures): n = len(temperatures) result = [0] * n stack = [] for i, temp in enumerate(temperatures): while stack and temp temperatures[stack[-1]]: idx = stack.pop() result[idx] = i - idx # 天数差 stack.append(i) return result 输出：[1, 1, 4, 2, 1, 1, 0, 0] 🧠 费曼总结：用简单的话解释单调栈给10岁小孩的解释想象你在排队买票，队伍里每个人都有一个身高牌。你的任务是：告诉每个人，他后面第一个比他高的人是谁。 笨办法：每个人都要回头看一遍后面所有人 → 很慢（O(n²)） 聪明办法（单调栈）： 维护一个”候选队列”，队列里的人从前到后越来越矮 新人来了，如果他比队尾的人高： 队尾的人找到答案了！（就是这个新人） 把队尾的人踢出去（他已经没用了） 继续比较，直到新人不比队尾高 新人加入队尾 为什么这样快？ 每个人最多进队一次、出队一次 → O(n) 核心要点 什么时候用单调栈？ 需要找”下一个更大/更小元素” 需要找”左右边界” 暴力解法需要嵌套循环 单调栈的本质 维护一个候选序列 新元素到来时，淘汰被”遮挡”的元素 弹出时记录答案 为什么是 O(n)？ 每个元素最多入栈一次，出栈一次 总操作次数 ≤ 2n 单调递增 vs 单调递减 单调递增栈：找下一个更小元素 单调递减栈：找下一个更大元素 记忆口诀遇到\"下一个更大\"，单调栈来帮忙栈顶小于当前值，弹出记录答案忙每个元素进出一次，时间复杂度 O(n) 强 🧠 费曼总结：用简单的话解释单调栈给10岁小孩解释想象你在排队买冰淇淋，队伍里每个人都有不同的身高。 问题：每个人都想知道，自己后面第一个比自己高的人是谁？ 笨办法：每个人都回头看，一个一个找 → 很慢（O(n²)） 聪明办法（单调栈）： 维护一个”候选队列”，队列里的人从前到后越来越矮 新人来了： 如果比队尾的人矮 → 直接加入队列 如果比队尾的人高 → 队尾的人找到答案了！把他踢出去，继续比较 每个人最多进出队列一次 → 很快（O(n)） 核心要点 什么时候用单调栈？ 需要找”下一个更大/更小元素” 需要找”左右边界” 暴力解法需要嵌套循环 单调栈的本质 维护一个候选序列 新元素到来时，淘汰被”遮挡”的元素 弹出时记录答案 为什么是 O(n)？ 每个元素最多入栈一次，出栈一次 总操作次数 ≤ 2n 单调递增 vs 单调递减 单调递增栈：找下一个更小元素 单调递减栈：找下一个更大元素 记忆口诀遇到\"下一个更大\"，单调栈来帮忙栈顶小于当前值，弹出记录答案忙每个元素进出一次，时间复杂度 O(n) 强 📝 代码模板找下一个更大元素（单调递减栈）def next_greater(nums): result = [-1] * len(nums) stack = [] for i in range(len(nums)): while stack and nums[i] nums[stack[-1]]: result[stack.pop()] = nums[i] stack.append(i) return result 找下一个更小元素（单调递增栈）def next_smaller(nums): result = [-1] * len(nums) stack = [] for i in range(len(nums)): while stack and nums[i] nums[stack[-1]]: result[stack.pop()] = nums[i] stack.append(i) return result 📚 LeetCode 题单基础题（Easy） 496. 下一个更大元素 I 739. 每日温度 进阶题（Medium） 503. 下一个更大元素 II 901. 股票价格跨度 困难题（Hard） 84. 柱状图中最大的矩形 42. 接雨水 85. 最大矩形 🎓 进阶思考1. 单调队列 vs 单调栈 单调队列：支持队首队尾操作，用于滑动窗口最大值 单调栈：只支持栈顶操作，用于寻找边界 2. 双单调栈某些问题需要同时维护左右边界，可以用两个单调栈分别处理。 3. 单调栈 + DP在某些 DP 问题中，单调栈可以优化状态转移。 🔑 关键要点总结 特性 说明 适用场景 找下一个更大/更小元素、左右边界 时间复杂度 O(n) 空间复杂度 O(n) 核心思想 维护单调性，及时淘汰无用元素 存储内容 通常存索引而非值 创建时间：2026-02-19标签：#算法 #单调栈 #数据结构 #优化技巧","tags":["算法"],"categories":["算法"]},{"title":"双指针（Two Pointers）Feynman 风格教程","path":"/2026/02/19/20260219-双指针教程/","content":"双指针（Two Pointers）— Feynman 风格教程 目标：把“两个指针协作”的机制讲清楚，给出可运行的题型模板、具体 I/O 示例，并配合图像理解。 1. 核心直觉：两个指针 = 两个同步的“游标”把数组当成一条直线，两个指针就是两个游标。它们用“局部信息”决定下一步动作，从而减少搜索空间。 关键机制： 有序信息（单调性）让我们能“确定方向”。 约束信息（例如目标和、去重规则）让我们能“确定动作”。 图示： 2. 对撞指针：有序数组里的“夹逼”2.1 机制直观左右指针从两端向中间移动。若当前和太大，右指针左移；太小，左指针右移；刚好命中就返回。 图示： 2.2 具体 I/O 示例输入： nums = [2, 5, 8, 10, 15, 18, 22, 25]（长度 n=8） target = 27 过程： left=0 (2), right=7 (25) = sum=27，命中。 输出： return (0, 7) 或 return [2, 25]（依题目要求） 2.3 代码模板（Python）def two_sum_sorted(nums, target): left, right = 0, len(nums) - 1 while left right: s = nums[left] + nums[right] if s == target: return left, right if s target: right -= 1 else: left += 1 return None 2.4 复杂度 时间：O(n) 空间：O(1) 3. 快慢指针：速度差制造“相遇信息”3.1 机制直观慢指针每次走一步，快指针每次走两步。 如果存在环，快慢一定相遇。 相遇后可推导出入环点。 图示： 3.2 具体 I/O 示例（环检测）输入： 链表：1 - 2 - 3 - 4 - 5 - 3 ...（3 是入环点） 过程： slow: 1-2-3-4… fast: 1-3-5-4… 相遇则说明有环。 输出： True（存在环） 3.3 代码模板（Python）def has_cycle(head): slow, fast = head, head while fast and fast.next: slow = slow.next fast = fast.next.next if slow == fast: return True return False 3.4 复杂度 时间：O(n) 空间：O(1) 4. 同向指针：稳定“过滤 + 写回”4.1 机制直观两个指针同向移动： fast 负责扫描 slow 负责写入有效元素 图示： 4.2 具体 I/O 示例（去重）输入： nums = [1,1,2,2,3,4,4,5]（长度 8） 过程： slow 指向最后一个有效位置 遇到新值时，写入 nums[slow+1] 输出： 数组前 k 位为 [1,2,3,4,5]，k=5 4.3 代码模板（Python）def remove_duplicates(nums): if not nums: return 0 slow = 0 for fast in range(1, len(nums)): if nums[fast] != nums[slow]: slow += 1 nums[slow] = nums[fast] return slow + 1 4.4 复杂度 时间：O(n) 空间：O(1) 5. 训练式思维（迁移到“可学习”的直觉）虽然双指针是算法题，但你可以把它当作一种“闭环决策过程”： 输入：当前指针位置 + 局部值 损失：偏离目标（如 sum-target 的误差） 更新：移动指针（left++/right–/fast++） 这种机制类似一个“有约束的贪心优化”。本质是用局部信息做出最有可能减少误差的动作。 6. 小结 对撞指针：利用有序性，缩小区间 快慢指针：利用速度差，制造相遇 同向指针：扫描 + 写回，稳定过滤 7. 费曼总结一句话：双指针不是技巧堆叠，而是“用两条视线把搜索空间压扁”。 先找结构：数组有序/值域单调/可过滤 ⇒ 才能决定指针移动方向。 每步有理由：指针移动是“减少误差”的动作，不是拍脑袋。 写回=压缩信息：同向指针通过覆盖写回，把“保留什么”变成可验证的过程。 遇到新题先问：我能否用两个游标，把未知区域变成更小的已知区域？","tags":["算法"]},{"title":"CLIP (Contrastive Language-Image Pre-Training) 深度解析：从原理到大模型应用","path":"/2026/02/16/CLIP深度解析/","content":"CLIP (Contrastive Language-Image Pre-Training) 深度解析 一句话本质：CLIP 是连接文本与图像的”罗塞塔石碑”。它通过对比学习，强行把图像和文本拉到同一个向量空间，让计算机视觉终于”读懂了语言”。 一、STAR 框架：CLIP 的诞生与革命1. Situation (背景与痛点)在 CLIP (2021) 之前，计算机视觉（CV）和自然语言处理（NLP）是两个平行的世界： CV 的困境：严重依赖人工标注（ImageNet 的 1000 个类），模型只能识别训练过的类别（Closed-Set），换个场景就要重新训练。 NLP 的突破：GPT 系列证明了”从海量无标注文本中自监督学习”是通往通用的钥匙。 核心痛点：如何让 CV 模型像 GPT 一样，从互联网海量数据中学习，不再需要人工打标签？ 2. Task (任务目标)OpenAI 的目标很明确： 训练一个能直接通过”自然语言”指挥的视觉模型，实现 Zero-Shot（零样本）迁移。 即：不用微调，直接告诉模型”找一张猫的照片”，它就能从图库里找出猫。 3. Action (核心方法)CLIP 摒弃了传统的”分类”范式，采用了 **对比学习 (Contrastive Learning)**： 数据：收集了 4 亿对 (图片, 文本) 数据 (WebImageText)。 机制：训练一个图像编码器和一个文本编码器，让配对的图文向量相似度最大化，不配对的最小化。 规模：使用了超大的 Batch Size (32,768)，这是成功的关键之一。 4. Result (结果与影响) 性能：在 ImageNet 上，Zero-Shot 的 CLIP 达到了 ResNet-50 的水平（76.2%），但鲁棒性远超后者。 影响：它成为了 AI 绘画（Stable Diffusion）、多模态大模型（LLaVA、GPT-4V）的基石。没有 CLIP，就没有现在的 AIGC 热潮。 二、深度原理：从 Tensor 流向到梯度回传2.1 架构与数据流 (The Flow)CLIP 是典型的**双塔架构 (Two-Tower Architecture)**。 (图1：CLIP 完整架构与 Tensor 维度变化图) 数据流拆解 (以 ViT-B/32 为例)假设 Batch Size $N=4$： Image Branch (左侧): Input: [4, 3, 224, 224] (图片) Patch Embedding: 切成 7x7=49 个 patch $ightarrow$ [4, 49, 768] Transformer: 加上 [CLS] token $ightarrow$ [4, 50, 768] Output: 取 [CLS] token，投影到 512 维 $ightarrow$ I_e [4, 512] Text Branch (右侧): Input: [4, 77] (文本 Token ID) Transformer: 经过 12 层处理 $ightarrow$ [4, 77, 512] Output: 取 [EOS] token，投影到 512 维 $ightarrow$ T_e [4, 512] Interaction (交互): 矩阵乘法: Logits = I_e @ T_e.T $ightarrow$ [4, 4] 这就得到了一个相似度矩阵！ 2.2 核心机制：对比学习与 InfoNCE LossCLIP 不做生成（不画图），也不做分类（不预测 Label），它只做判断题： “这张图和这段字，是不是一对？” (图2：对比损失矩阵计算与梯度回传) InfoNCE Loss 详解对于 Batch 中的第 $i$ 张图，它与第 $i$ 段文本是正样本，与其他所有文本是负样本。 $$ L_i = -\\log rac{\u001bxp(\text{sim}(I_i, T_i)/\tau)}{\\sum_{j=1}^N \u001bxp(\text{sim}(I_i, T_j)/\tau)} $$ 分子：正样本的相似度（我们要最大化它）。 分母：所有样本的相似度总和（我们要通过最大化分子，间接压低分母中其他负样本的比重）。 **$\tau$ (Temperature)**：温度系数。$\tau$ 越小，分布越尖锐，模型越关注最难区分的负样本。CLIP 中 $\tau$ 是可学习的。 费曼直觉：为什么要用对比学习？想象你在教小孩认动物： **生成式 (Generative)**：让小孩画一只猫。（太难了，还要学画画） **分类式 (Classification)**：给小孩看图，让他背这是”类别ID 283”。（死记硬背，不懂含义） 对比式 (Contrastive)：给小孩看一张猫图和”猫”字卡片，再看一张狗图和”车”字卡片，让他配对。（简单、高效，懂语义） 三、Zero-Shot 机制：如何”听懂人话”？这是 CLIP 最骚的操作。它把分类问题变成了检索问题。 (图3：Zero-Shot 推理与动态权重生成) 3.1 动态分类器 (Dynamic Classifier)传统的分类器，最后一层权重 $W$ 是固定的（比如 ImageNet 的 1000 类）。CLIP 的权重是动态生成的： 你给它一组类别词：[\"dog\", \"cat\", \"plane\"]。 它把这些词变成向量：[v_dog, v_cat, v_plane]。 这三个向量，就构成了临时的分类器权重 $W’$！ 图片向量 $I$ 与 $W’$ 做点积，谁大就是谁。 3.2 Prompt Engineering 的起源论文发现，直接用单词 \"dog\" 效果一般。如果改成 \"a photo of a dog\"，效果提升 1.3%。如果用 Ensemble（集成）： \"a photo of a big {label}\" \"a drawing of a {label}\" \"it is a {label}\"把 80 种句子的向量取平均，效果提升 3.5%！ 这告诉我们：多角度描述一个事物，特征更稳。 四、大模型应用：CLIP 是 AI 的”视觉接口”CLIP 最大的贡献不是它自己，而是它成全了别人。 (图4：CLIP 在 Stable Diffusion 和 LLaVA 中的核心地位) 4.1 Stable Diffusion (AI 绘画) 角色：CLIP Text Encoder 是 SD 的”理解中枢”。 流程： 用户输入：”一只赛博朋克风格的猫”。 CLIP Text Encoder 把这句话变成向量。 U-Net 根据这个向量，从噪声中”雕刻”出图像。 本质：SD 画得好，是因为 CLIP 懂文本对应的视觉特征长什么样。 4.2 LLaVA / GPT-4V (多模态大模型) 角色：CLIP Vision Encoder 是大模型的”眼睛”。 流程： 输入一张图。 CLIP Image Encoder 提取图像特征。 通过一个投影层 (Projection)，把图像特征伪装成”词向量”。 LLM (如 Vicuna/Llama) 以为自己看到了文字，实际上是看到了图像特征。 本质：CLIP 把像素变成了 LLM 能读懂的语义。 五、核心代码实现 (PyTorch)import torchimport torch.nn as nnimport numpy as npclass CLIP(nn.Module): def __init__(self, embed_dim=512): super().__init__() self.visual = VisionTransformer() # Image Encoder self.text = TextTransformer() # Text Encoder self.logit_scale = nn.Parameter(torch.ones([]) * np.log(1 / 0.07)) def forward(self, image, text): # 1. 提取特征 I_e = self.visual(image) # [N, 512] T_e = self.text(text) # [N, 512] # 2. 归一化 (关键！否则点积无上界) I_e = I_e / I_e.norm(dim=-1, keepdim=True) T_e = T_e / T_e.norm(dim=-1, keepdim=True) # 3. 计算相似度矩阵 # exp(t) * (I @ T.T) logit_scale = self.logit_scale.exp() logits_per_image = logit_scale * I_e @ T_e.t() logits_per_text = logits_per_image.t() # 4. 构造标签 (对角线是正样本) labels = torch.arange(len(image)).to(image.device) # 5. 计算双向 Loss loss_i = nn.CrossEntropyLoss()(logits_per_image, labels) loss_t = nn.CrossEntropyLoss()(logits_per_text, labels) return (loss_i + loss_t) / 2 六、局限性与思考虽然 CLIP 很强，但它不是万能的： 不擅长细粒度任务：它很难区分”波音747”和”波音777”，或者数清图里有几只鸟。因为对比学习更关注整体语义匹配。 OCR 能力弱：它能认出”Apple”这个Logo，但如果你手写一个”Sony”贴在苹果上，它可能会困惑。 非生成式：CLIP 自己不能生成图像，它只能评价图像。它需要配合 GAN 或 Diffusion 才能搞创作。 七、总结CLIP 是 AI 历史上的一个转折点。它打破了 Vision 和 Language 的界限，用4亿对图文数据暴力美学地证明了：语言是理解视觉的最佳监督信号。","tags":["深度学习"],"categories":["深度学习"]},{"title":"MoE 深度解析：Switch Transformer 与 Mixtral 的稀疏之道","path":"/2026/02/08/MoE-Switch-Transformer-Mixtral-Deep-Dive/","content":"MoE 深度解析：Switch Transformer 与 Mixtral 的稀疏之道 📅 创建时间：2026-02-08🏷️ 标签：#MoE #SwitchTransformer #Mixtral #稀疏计算 #大模型📚 学习方法：费曼式讲解 + 数学推导 + 代码实现📖 前置知识：Transformer FFN 层, Softmax, PyTorch 基础 🎯 一句话理解 MoE 🧠 费曼式理解 MoE 就像一家拥有 128 个专科医生的医院。 普通医院：所有病人都找同一个全科医生（稠密 FFN）→ 医生累死，效率低下。 MoE 医院：前台护士（Router）快速判断病情，把心脏病人分给心脏专家，骨折病人分给骨科专家。每个病人只见 1-2 个专家，但医院总共有 128 个专家随时待命。 结果：医院容量提升 128 倍，但每个病人的等待时间几乎不变！ 📋 目录 为什么需要 MoE？ 核心机制：Router + Expert Tensor 维度变化全图解 Training Loop：梯度如何流动？ Switch vs Mixtral：关键差异 完整代码实现 费曼自测题 1. 为什么需要 MoE？1.1 稠密模型的困境传统 Transformer 的 FFN 层占据了 2/3 的参数量： class DenseFFN(nn.Module): def __init__(self, d_model=1024, d_ff=4096): self.W1 = nn.Linear(d_model, d_ff) # [1024, 4096] = 4.2M 参数 self.W2 = nn.Linear(d_ff, d_model) # [4096, 1024] = 4.2M 参数 # 总计 8.4M 参数，每个 token 都要过一遍 问题： GPT-3 (175B) 每次推理都激活 100% 参数 推理成本 ∝ 参数量 → 参数越多，推理越慢 扩展悖论：想要更聪明，就必须更慢？ 1.2 MoE 的核心洞察 💡 关键洞察：不同的 token 需要不同的\"知识\"来处理。 \"The capital of France is\" → 需要地理知识 \"def quicksort(arr):\" → 需要编程知识 \"I feel so happy\" → 需要情感理解 为什么要让同一个 FFN 处理所有这些？让专家各司其职！ MoE 的解决方案： 对比项 稠密 FFN MoE (128 Experts) 总参数量 8.4M 8.4M × 128 = 1.075B 每次激活参数 8.4M (100%) 8.4M (0.78%) 模型容量 1× 128× 推理成本 1× ≈ 1× 这就是 MoE 的魔法：参数量暴涨 128 倍，推理成本几乎不变！ 2. 核心机制：Router + Expert2.1 架构总览输入 Token [B, L, D] ↓ ┌─────────┐ │ Router │ ← 一个简单的 Linear 层 └────┬────┘ ↓ Softmax + Top-K ┌────┴────┐ ↓ ↓ ↓┌───┐ ┌───┐ ┌───┐│E_0│ │E_1│ │...│ │E_127│ ← 128 个并行的 FFN└─┬─┘ └─┬─┘ └───┘ │ │ └──┬──┘ ↓ 加权求和输出 [B, L, D] 2.2 Router：交通调度员Router 就是一个 Linear 层，把每个 token 映射到 N 个 Expert 的概率分布： # Router 的全部代码self.router = nn.Linear(d_model, num_experts, bias=False) # [1024, 128]# 前向传播router_logits = self.router(x) # [B, L, 128]router_probs = F.softmax(router_logits, dim=-1) # 概率分布 具体例子： 假设一个 token 的 embedding 是 x = [0.1, 0.2, ..., 0.5]（1024 维） # Router 计算router_logits = W_router @ x # [128]# 假设结果是 [2.1, 0.5, 3.2, 0.1, ..., 0.8]# Softmax 归一化router_probs = softmax(router_logits)# [0.15, 0.03, 0.45, 0.01, ..., 0.02]# ↑ ↑# Expert 0: 15% Expert 2: 45% ← 选这个！ 2.3 Top-K 选择**Switch Transformer (Top-1)**：每个 token 只选 1 个 Expert topk_probs, topk_indices = torch.topk(router_probs, k=1)# topk_indices = [2] ← 选择 Expert 2# topk_probs = [0.45] ← 权重 0.45 **Mixtral (Top-2)**：每个 token 选 2 个 Expert topk_probs, topk_indices = torch.topk(router_probs, k=2)# topk_indices = [2, 0] ← 选择 Expert 2 和 Expert 0# topk_probs = [0.45, 0.15] → 归一化 → [0.75, 0.25] 2.4 稀疏计算的实现 ⚠️ 关键问题：128 个 Expert，每个 token 只用 1 个，怎么高效计算？ 朴素实现（低效）： for expert_id in range(128): mask = (topk_indices == expert_id) if mask.any(): output[mask] = experts[expert_id](x[mask]) 优化实现（实际使用）： # 1. 按 Expert ID 重排所有 tokensorted_indices = topk_indices.argsort()sorted_x = x[sorted_indices]# 2. 批量处理每个 Expert 的 tokenexpert_outputs = []for expert_id, expert in enumerate(experts): start, end = expert_boundaries[expert_id] if start end: expert_outputs.append(expert(sorted_x[start:end]))# 3. 还原到原始顺序output = torch.cat(expert_outputs)[inverse_indices] 3. Tensor 维度变化全图解 🎨 可视化是建立直觉的最快方式 让我们跟踪一个具体的例子，看 Tensor 如何流动。 3.1 配置参数batch_size = 8seq_len = 512d_model = 1024num_experts = 128d_ff = 4096 # 每个 Expert 的 FFN 隐藏层top_k = 1 # Switch Transformer 3.2 完整数据流┌─────────────────────────────────────────────────────────────┐│ Step 0: Input ││ x: [8, 512, 1024] ││ 含义: 8 个样本，每个 512 个 token，每个 token 1024 维 │└───────────────────────────┬─────────────────────────────────┘ ↓┌─────────────────────────────────────────────────────────────┐│ Step 1: Router Logits ││ router_logits = W_r @ x ││ W_r: [1024, 128] ││ router_logits: [8, 512, 128] ││ 含义: 每个 token 对 128 个 Expert 的\"打分\" │└───────────────────────────┬─────────────────────────────────┘ ↓ Softmax┌─────────────────────────────────────────────────────────────┐│ Step 2: Router Probabilities ││ router_probs = softmax(router_logits, dim=-1) ││ router_probs: [8, 512, 128] ││ 含义: 每个 token 选择各 Expert 的概率 ││ 每行和 = 1.0 │└───────────────────────────┬─────────────────────────────────┘ ↓ Top-1┌─────────────────────────────────────────────────────────────┐│ Step 3: Top-K Selection ││ topk_probs: [8, 512, 1] ← 最大概率值 ││ topk_indices: [8, 512, 1] ← Expert ID (0-127) ││ ││ 例: Token[0,0] → Expert 42, 权重 0.67 ││ Token[0,1] → Expert 7, 权重 0.81 ││ Token[0,2] → Expert 42, 权重 0.55 ← 同一个 Expert! │└───────────────────────────┬─────────────────────────────────┘ ↓ Dispatch┌─────────────────────────────────────────────────────────────┐│ Step 4: Expert Processing (稀疏!) ││ ││ Expert 7: 收到 32 个 token → [32, 1024] ││ Expert 42: 收到 48 个 token → [48, 1024] ← 负载不均! ││ Expert 99: 收到 0 个 token → 空闲 ││ ... ││ ││ 每个 Expert 内部: ││ [N_i, 1024] → W1 → [N_i, 4096] → GELU → W2 → [N_i, 1024] │└───────────────────────────┬─────────────────────────────────┘ ↓ Weighted Combine┌─────────────────────────────────────────────────────────────┐│ Step 5: Output ││ output[i] = topk_probs[i] × expert_output[i] ││ output: [8, 512, 1024] ││ 维度与输入完全一致! │└─────────────────────────────────────────────────────────────┘ 3.3 可视化：Router 决策热力图图 3.1: Router 决策热力图 (左)、Top-1 分配 (中)、负载分布 (右) 解读： 左图：每列是一个 token 对 16 个 Expert 的概率（颜色越深 = 概率越高） 中图：每个 token 最终选择的 Expert（绿色 = 被选中） 右图：Expert 负载分布 🔴 红色柱子 = 过载（如 Expert 2 处理 12 个 token） 🟠 橙色柱子 = 空闲（如 Expert 13 只处理 1 个） 🟢 绿色虚线 = 理想均衡值 这就是负载不均衡问题的根源！ 4. Training Loop：梯度如何流动？4.1 完整的 Loss 函数 $$ \\mathcal{L}_{\\text{total}} = \\underbrace{\\mathcal{L}_{\\text{task}}}_{\\text{Task Loss}} + \\alpha \\cdot \\underbrace{\\mathcal{L}_{\\text{balance}}}_{\\text{Load Balance}} $$ 其中 $\\alpha = 0.01$（Switch Transformer 推荐值） 4.2 Load Balance Loss 推导 🧠 费曼式理解 Load Balance Loss 想象你是医院管理者，要让 128 个专家的工作量均衡： f_i = Expert i 实际接诊的病人比例（频率） P_i = 护士给 Expert i 的平均推荐概率 如果某个专家既实际接诊多（f 大），又被推荐概率高（P 大），说明系统在\"偏袒\"这个专家。惩罚它！ 数学定义： $$ f_i = \\frac{1}{B \\cdot L} \\sum_{b,l} \\mathbb{1}[\\text{Top1}(x_{b,l}) = i] $$ $$ P_i = \\frac{1}{B \\cdot L} \\sum_{b,l} \\text{Router}(x_{b,l})_i $$ $$ \\mathcal{L}_{\\text{balance}} = N \\cdot \\sum_{i=1}^{N} f_i \\cdot P_i $$ 具体计算例子： # 假设 4 个 Expert，8 个 tokenrouter_probs = torch.tensor([ [0.7, 0.1, 0.1, 0.1], # Token 0 → Expert 0 (p=0.7) [0.6, 0.2, 0.1, 0.1], # Token 1 → Expert 0 (p=0.6) [0.5, 0.3, 0.1, 0.1], # Token 2 → Expert 0 (p=0.5) [0.4, 0.4, 0.1, 0.1], # Token 3 → Expert 0 (p=0.4) [0.1, 0.6, 0.2, 0.1], # Token 4 → Expert 1 [0.1, 0.1, 0.7, 0.1], # Token 5 → Expert 2 [0.1, 0.1, 0.1, 0.7], # Token 6 → Expert 3 [0.1, 0.1, 0.1, 0.7], # Token 7 → Expert 3])# Top-1 选择结果: [0, 0, 0, 0, 1, 2, 3, 3]# 计算 f (频率)f = [4/8, 1/8, 1/8, 2/8] # = [0.5, 0.125, 0.125, 0.25]# 计算 P (平均概率)P = router_probs.mean(dim=0) # = [0.325, 0.2375, 0.1625, 0.2375]# Load Balance LossN = 4L_balance = N * sum(f[i] * P[i] for i in range(4))# = 4 * (0.5×0.325 + 0.125×0.2375 + 0.125×0.1625 + 0.25×0.2375)# = 4 * 0.2719 = 1.0876 梯度分析： $$ \\frac{\\partial \\mathcal{L}_{\\text{balance}}}{\\partial P_i} = N \\cdot f_i $$ Expert 0: $ abla P_0 = 4 \\times 0.5 = 2.0$ ← 梯度最大，会被惩罚！ Expert 1: $ abla P_1 = 4 \\times 0.125 = 0.5$ ← 梯度较小 → 训练会降低 Expert 0 的概率，提升其他 Expert 的概率 4.3 梯度流动图解图 4.1: MoE 反向传播的完整梯度流动路径 关键点： Expert 参数的梯度： ✅ 被选中的 Expert 收到梯度 ❌ 未被选中的 Expert 梯度为 0 Router 参数收到两种梯度： 🔵 来自主任务 Loss（通过 Gating Weight） 🟠 来自 Load Balance Loss（直接作用于 softmax） 梯度冲突： 主任务想让 Router 选择最好的 Expert LB Loss 想让 Router 均匀分配 $\\alpha = 0.01$ 平衡这两个目标 4.4 训练稳定性技巧 技巧 原因 代码 小初始化 防止 Router 一开始就偏向某些 Expert nn.init.normal_(router.weight, std=0.01) Capacity Factor 限制每个 Expert 最多处理的 token 数 capacity = (B*L/N) * 1.25 BF16 训练 防止 softmax 上溢/下溢 model.to(torch.bfloat16) 梯度裁剪 防止梯度爆炸 clip_grad_norm_(params, 1.0) 5. Switch vs Mixtral：关键差异 特性 Switch Transformer (2021) Mixtral 8x7B (2023) Top-K Top-1（极致稀疏） Top-2（平衡性能） Expert 数量 2048（极端多） 8（适合单机） 每 token 计算量 1 个 Expert 2 个 Expert 负载均衡 Auxiliary Loss Token Choice + Expert Choice 适用场景 预训练超大模型 指令微调 + 推理部署 图 5.1: Switch Transformer (Top-1) vs Mixtral (Top-2) 架构对比 5.1 Mixtral 的 Top-2 优势# Top-2 路由topk_probs, topk_indices = torch.topk(router_probs, k=2)# 归一化topk_probs = topk_probs / topk_probs.sum(dim=-1, keepdim=True)# 输出 = w1 * Expert1(x) + w2 * Expert2(x)output = topk_probs[:, 0] * expert1_out + topk_probs[:, 1] * expert2_out 优势： 更好的容错性（如果 Top-1 过载，Top-2 补充） 负载自然更均衡 训练更稳定 代价： 推理成本 ×2（但仍远低于稠密模型：8 个 Expert 只激活 2 个 = 25%） 6. 完整代码实现6.1 Switch MoE Layerimport torchimport torch.nn as nnimport torch.nn.functional as Fclass SwitchMoELayer(nn.Module): \"\"\" Switch Transformer 的 MoE Layer 实现 参数: d_model: 输入/输出维度 num_experts: Expert 数量 d_ff: FFN 隐藏层维度 capacity_factor: 容量因子（默认 1.25） \"\"\" def __init__(self, d_model=1024, num_experts=128, d_ff=4096, capacity_factor=1.25): super().__init__() self.num_experts = num_experts self.capacity_factor = capacity_factor # Router: 简单的 Linear 层 self.router = nn.Linear(d_model, num_experts, bias=False) nn.init.normal_(self.router.weight, std=0.01) # 关键：小初始化 # Experts: N 个独立的 FFN self.experts = nn.ModuleList([ nn.Sequential( nn.Linear(d_model, d_ff), nn.GELU(), nn.Linear(d_ff, d_model) ) for _ in range(num_experts) ]) def forward(self, x): B, L, D = x.shape # Step 1: Router 计算 router_logits = self.router(x) # [B, L, N] router_probs = F.softmax(router_logits, dim=-1) # Step 2: Top-1 选择 topk_probs, topk_indices = torch.topk(router_probs, k=1, dim=-1) topk_probs = topk_probs.squeeze(-1) # [B, L] topk_indices = topk_indices.squeeze(-1) # [B, L] # Step 3: 容量限制 capacity = int((B * L / self.num_experts) * self.capacity_factor) # Step 4: Expert 计算 output = torch.zeros_like(x) expert_counts = torch.zeros(self.num_experts, device=x.device) for expert_id in range(self.num_experts): mask = (topk_indices == expert_id) num_tokens = mask.sum().item() if num_tokens == 0: continue # 容量限制：只选概率最高的 token if num_tokens capacity: masked_probs = torch.where( mask, topk_probs, torch.tensor(-1e9, device=x.device) ) _, top_indices = torch.topk(masked_probs.flatten(), k=capacity) new_mask = torch.zeros_like(mask.flatten(), dtype=torch.bool) new_mask[top_indices] = True mask = new_mask.view(B, L) # Expert 前向传播 selected_x = x[mask] expert_out = self.experts[expert_id](selected_x) # 加权输出 weights = topk_probs[mask].unsqueeze(-1) output[mask] = expert_out * weights expert_counts[expert_id] = mask.sum().item() # Step 5: 计算 Load Balance Loss f = expert_counts / (B * L) P = router_probs.mean(dim=[0, 1]) load_balance_loss = self.num_experts * (f * P).sum() return output, load_balance_loss# ===== 测试代码 =====if __name__ == \"__main__\": moe = SwitchMoELayer(d_model=512, num_experts=8, d_ff=2048) x = torch.randn(2, 16, 512) output, lb_loss = moe(x) print(f\"Input: {x.shape}\") print(f\"Output: {output.shape}\") print(f\"LB Loss: {lb_loss.item():.4f}\") # 反向传播测试 total_loss = output.sum() + 0.01 * lb_loss total_loss.backward() print(f\"Router grad norm: {moe.router.weight.grad.norm():.4f}\") 6.2 完整训练循环import torch.optim as optimmodel = SwitchMoELayer(d_model=512, num_experts=16, d_ff=2048)optimizer = optim.AdamW(model.parameters(), lr=1e-4)for epoch in range(10): x = torch.randn(8, 64, 512) target = torch.randn(8, 64, 512) output, lb_loss = model(x) main_loss = F.mse_loss(output, target) total_loss = main_loss + 0.01 * lb_loss optimizer.zero_grad() total_loss.backward() torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0) optimizer.step() print(f\"Epoch {epoch}: Main={main_loss:.4f}, LB={lb_loss:.4f}\") 输出示例： Epoch 0: Main=1.0234, LB=1.4521Epoch 1: Main=0.9876, LB=1.3214...Epoch 9: Main=0.7821, LB=1.0234 ← LB Loss 下降，负载更均衡！ 7. 费曼自测题 🧪 检验你是否真正理解了 MoE Q1: 如果 MoE 有 128 个 Expert，每个 Expert 的 FFN 隐藏层是 4096，那么： 总参数量是多少？ 每次推理激活多少参数？（Top-1） 点击查看答案 总参数：$2 \\times 1024 \\times 4096 \\times 128 = 1.07B$ 激活参数：$2 \\times 1024 \\times 4096 = 8.4M$（仅 0.78%） Q2: Load Balance Loss 中，$f_i \\cdot P_i$ 乘积的直觉含义是什么？ 点击查看答案 $f_i$ 是 Expert i 实际被选中的频率 $P_i$ 是 Router 给 Expert i 的平均概率 乘积大说明这个 Expert 既被选中多，又被推荐概率高 → 系统在”偏袒”它 Loss 惩罚这种情况，迫使 Router 更均匀地分配 Q3: 为什么 Mixtral 选择 Top-2 而不是 Top-1？ 点击查看答案 容错性：如果 Top-1 Expert 过载，Top-2 补充 训练稳定性：负载自然更均衡 性能：两个 Expert 的知识互补 代价：推理成本 ×2，但仍远低于稠密模型 Q4: 如果不使用 Load Balance Loss，会发生什么？ 点击查看答案 部分 Expert 会被大量选择（马太效应） 部分 Expert 几乎不被使用 → 参数浪费 梯度更新不均 → 某些 Expert 过度训练，某些几乎不更新 极端情况：模型退化为只用 1-2 个 Expert 的”伪 MoE” 🔗 延伸阅读 论文原文： Switch Transformers (2021) Mixtral of Experts (2023) GShard (2020) - 分布式 MoE 开源实现： Hugging Face Transformers Fairseq MoE 进阶话题： Expert Parallelism vs Tensor Parallelism MoE + LoRA 稀疏微调 Dynamic Expert Selection 📊 总结 🎯 一图总结 MoE ┌─────────────────────────────────────┐ │ MoE 核心公式 │ │ │ │ y = Σ Router(x)_i × Expert_i(x) │ │ i∈Top-K │ └─────────────────────────────────────┘ ↓ ┌────────────────────────────┼────────────────────────────┐ │ │ │┌────┴────┐ ┌─────┴─────┐ ┌─────┴─────┐│ Router │ │ Experts │ │ Loss ││ (调度) │ │ (执行) │ │ (均衡) │└────┬────┘ └─────┬─────┘ └─────┬─────┘ │ │ │Softmax+TopK 并行 FFN 层 f·P 惩罚项选择 K 个专家 稀疏激活 防止偏袒 记住三个数字： 128× 参数量提升 1% 激活率（Top-1 with 128 Experts） 0.01 Load Balance Loss 权重 Created: 2026-02-08 by CaiusTags: #MoE #SwitchTransformer #Mixtral #DeepLearning #LLM","tags":["深度学习"],"categories":["深度学习"]},{"title":"KL散度完全图解教程：从零到精通","path":"/2026/02/08/KL散度完全图解教程/","content":"🎯 学习目标假设你完全不懂信息论，本教程将带你从零开始，用最通俗的语言、最生动的例子、最直观的图示，彻底理解 KL 散度的本质。 📊 配套资源 5张高清可视化图表（Jensen不等式、KL非负性证明、前向vs反向KL对比等） Python可视化代码 练习题及参考答案 📑 目录 第一章：信息的价值 - 为什么需要信息论？ 第二章：信息量 - 惊讶度的数学 第三章：概率分布 - 世界的规律 第四章：熵 - 平均惊讶程度 第五章：交叉熵 - 用错编码的代价 第六章：KL散度 - 测量分布的差异 6.4.1 Jensen 不等式科普 6.4.2 KL 散度非负性的详细证明 6.4.3 前向 KL vs 反向 KL：深度解析 第七章：实战应用 - 从 RLHF 到 VAE 第八章：总结与记忆技巧 第一章：信息的价值1.1 没有信息的世界想象一下，你生活在一个完全没有信息的世界： 明天会下雨吗？🤷 不知道 股票会涨吗？🤷 不知道 考试会考什么？🤷 不知道 这是一个充满不确定性的世界。你无法做任何有效的决策，因为你对未来一无所知。 1.2 信息的本质信息 = 消除不确定性的工具 当你知道： 明天 90% 会下雨 → 你带伞 股票 60% 会涨 → 你考虑投资 考试第3章占 70% → 你重点复习 核心理解： 信息让世界从”不知道”变成”知道”，从”混乱”变成”有序”。 第二章：信息量2.1 直觉：什么事情信息量大？请看下面三个事件： 事件 发生概率 你的反应 信息量 太阳从东边升起 99.99% 😐 理所当然 ≈ 0 今天是周一到周日 14.3% 🙂 正常 中等 中了彩票头奖 0.0001% 😱 惊呆了！ 巨大！ 发现规律了吗？ $$\\boxed{\\text{概率越低} \\rightarrow \\text{越惊讶} \\rightarrow \\text{信息量越大}}$$ 2.2 信息量的公式我们需要一个公式来量化”惊讶程度”。这个公式必须满足： 概率高 → 信息量低例如：$P = 100%$ 时，$I = 0$（完全不惊讶） 概率低 → 信息量高例如：$P = 1%$ 时，$I$ 很大（非常惊讶） 独立事件信息量可以相加例如：$I(A \\text{且} B) = I(A) + I(B)$ 唯一满足这些要求的函数就是对数： $$\\boxed{I(x) = -\\log_2 P(x)}$$ 2.3 变量详解让我们逐个击破每个符号： 符号 读作 含义 例子 $x$ “某个事件” 一个具体的结果 “下雨”、”掷骰子得到6” $P(x)$ “x 的概率” 这个事件发生的可能性 $P(\\text{下雨}) = 0.3$ $I(x)$ “x 的信息量” 知道这件事后获得的信息 $I(\\text{下雨}) = -\\log_2(0.3) = 1.74$ bits $\\log_2$ “以2为底的对数” 用二进制表示需要多少位 $\\log_2(8) = 3$（8需要3个二进制位） 为什么用对数？ 把乘法变加法$\\log(A \\times B) = \\log(A) + \\log(B)$ 把小数变大数概率是 0 到 1 之间的小数，对数把它变成正数例如：$\\log_2(0.01) = -6.64$，加个负号变成 $6.64$ bits 实际计算例子： import math# 事件：太阳从东边升起P_sun = 0.9999I_sun = -math.log2(P_sun)print(f\"信息量：{I_sun:.4f} bits\") # ≈ 0.0001 bits（几乎没有信息）# 事件：中彩票P_lottery = 0.000001I_lottery = -math.log2(P_lottery)print(f\"信息量：{I_lottery:.2f} bits\") # ≈ 19.93 bits（信息量巨大！） 第三章：概率分布3.1 什么是分布？分布 = 所有可能结果的概率清单 想象掷骰子： 结果 概率 1点 1/6 2点 1/6 3点 1/6 4点 1/6 5点 1/6 6点 1/6 这就是一个均匀分布（每个结果概率相同）。 3.2 真实世界 vs 我的猜测在机器学习中，我们关心两个分布： 符号 名称 含义 例子 $P(x)$ 真实分布 世界的真相 老师出题的真实规律 $Q(x)$ 预测分布 我的模型 我猜测的重点章节 问题来了：如何衡量 $Q$（我的猜测）和 $P$（真相）有多不一样？ 👉 这就是 KL 散度要解决的问题！ 第四章：熵4.1 熵的直觉熵 = 平均惊讶程度 = 平均信息量 看两个地区的天气： 沙漠地区（低熵）： 晴天：90% 雨天：8% 雪天：2% → 你几乎确定是晴天，很少惊讶 → 熵低 山区（高熵）： 晴天：35% 雨天：33% 雪天：32% → 你完全不知道会怎样，经常惊讶 → 熵高 4.2 熵的公式$$\\boxed{H(P) = -\\sum_{x} P(x) \\log_2 P(x)}$$ 逐项解读： 部分 含义 $P(x)$ 事件 $x$ 的真实概率 $\\log_2 P(x)$ 事件 $x$ 的信息量（的负值） $P(x) \\cdot \\log_2 P(x)$ 用概率加权 $\\sum$ 对所有可能的 $x$ 求和 $-$ 加个负号让结果为正 计算例子（沙漠天气）： import numpy as npP = [0.9, 0.08, 0.02] # 晴、雨、雪H = -sum([p * np.log2(p) if p 0 else 0 for p in P])print(f\"熵 = {H:.2f} bits\") # ≈ 0.64 bits（低熵） 计算例子（山区天气）： P = [0.35, 0.33, 0.32]H = -sum([p * np.log2(p) if p 0 else 0 for p in P])print(f\"熵 = {H:.2f} bits\") # ≈ 1.58 bits（高熵） 第五章：交叉熵5.1 摩斯密码的故事想象你要用摩斯密码发电报。为了省电，你要设计一套编码： 高频字母（如 E, T） → 用短代码（•, ━） 低频字母（如 Z, Q） → 用长代码（•━•━） 场景A：编码完美匹配如果你的编码完全符合英文字母的真实频率 $P$，你的电报最短。 场景B：编码错误如果你拿着中文的编码本 $Q$ 去发英文电报 $P$，会发生什么？ 你可能给 “E”（英文最常见）分配了超长代码，因为中文里没有 “E” 这个音。 结果： 每条消息都比理论最优长度多出一大截！ 5.2 交叉熵公式$$\\boxed{H(P, Q) = -\\sum_{x} P(x) \\log_2 Q(x)}$$ 关键区别： 公式 概率来源 信息量来源 含义 $H(P)$ $P(x)$ $\\log P(x)$ 用 $P$ 的编码传输 $P$ 的信息 $H(P,Q)$ $P(x)$ $\\log Q(x)$ 用 $Q$ 的编码传输 $P$ 的信息 直观理解： 真实世界按照 $P$ 分布产生数据 但你用 $Q$ 的编码本来编码 $H(P,Q)$ = 平均每条消息需要多少 bits 重要性质： $$H(P, Q) \\geq H(P)$$ 交叉熵永远 ≥ 熵！（用错误编码一定更浪费） 第六章：KL散度6.1 KL散度的定义KL散度 = 交叉熵 - 熵 = 额外浪费的信息量 $$\\boxed{D_{KL}(P | Q) = H(P, Q) - H(P)}$$ 展开后： $$\\boxed{D_{KL}(P | Q) = \\sum_{x} P(x) \\log \\frac{P(x)}{Q(x)}}$$ 6.2 公式深度解析让我们把公式拆成最小的零件： $$D_{KL}(P | Q) = \\sum_{x} P(x) \\cdot \\log \\frac{P(x)}{Q(x)}$$ 第1部分：$x$ 含义：所有可能的事件/结果 例子：天气里的”晴”、”雨”、”雪” 第2部分：$P(x)$ 含义：真实世界中 $x$ 发生的概率 例子：$P(\\text{晴}) = 0.6$（真实情况60%是晴天） 第3部分：$Q(x)$ 含义：你的模型认为 $x$ 发生的概率 例子：$Q(\\text{晴}) = 0.2$（你却认为只有20%是晴天） 第4部分：$\\frac{P(x)}{Q(x)}$ 含义：真实 vs 预测的比值 当 $P$ 大但 $Q$ 小时，这个比值爆炸性增长！ 例子：$\\frac{0.6}{0.2} = 3$（你低估了3倍） 第5部分：$\\log \\frac{P(x)}{Q(x)}$ 含义：把比值转成信息量（bits） 例子：$\\log_2(3) = 1.58$ bits 第6部分：$P(x) \\cdot \\log \\frac{P(x)}{Q(x)}$ 含义：用真实概率加权（重要！） 只有在真相经常发生的地方犯错，才会被严重惩罚 例子：$0.6 \\times 1.58 = 0.95$ 第7部分：$\\sum$ 含义：对所有可能的 $x$ 求和，得到平均浪费 6.3 完整计算实例场景：天气预报 天气 $P$ (真实) $Q$ (预测) $P/Q$ $\\log_2(P/Q)$ $P \\cdot \\log(P/Q)$ ☀️晴 0.6 0.2 3.00 1.58 0.950 🌧️雨 0.3 0.5 0.60 -0.74 -0.222 ☁️阴 0.1 0.3 0.33 -1.58 -0.158 总和 0.570 bits 结果解读： KL散度 = 0.570 bits意味着：用你的预测 $Q$ 代替真实 $P$，平均每次多浪费 0.57 bits 最大贡献来自”晴天”真实60%但你只预测20%，差距太大！ “雨天”贡献是负的你高估了雨天（预测50%实际30%），在这个方向上反而”赚了”信息但总体还是亏的（因为晴天亏太多） 6.4 KL散度的性质性质1：非负性 $$D_{KL}(P | Q) \\geq 0$$ 永远 ≥ 0，当且仅当 $P = Q$ 时等于 0。 6.4.1 Jensen 不等式科普要证明 KL 散度非负，我们需要先理解 Jensen 不等式。 什么是凹函数和凸函数？ 凹函数（如 log x）： 凸函数（如 x²）： ___ /\\ / \\ / \\ / \\ / \\ / \\ / \\ / \\ _/ \\_ 弦在曲线下方 弦在曲线上方 判断方法：在曲线上任取两点，连一条线（弦） 弦在曲线下方 → 凹函数 弦在曲线上方 → 凸函数 Jensen 不等式的内容 对于凹函数 $f$（比如 $\\log$）： $$f\\left(\\sum_i p_i x_i\\right) \\geq \\sum_i p_i f(x_i)$$ 其中 $p_i \\geq 0$ 且 $\\sum_i p_i = 1$（概率权重） 通俗解释： “先平均再变换” ≥ “先变换再平均” 用图来理解 假设只有两个点 $x_1$ 和 $x_2$，权重各 0.5： y││ A ●───────────────● B ← f(x₁) 和 f(x₂)│ ╲ ．．．．．．．．╱│ ╲ ● M' ╱ ← 弦的中点（先变换再平均）│ ╲ ↓ ╱│ ╲ ↓ ╱│ ╲ ● M ╱ ← 曲线上的点（先平均再变换）│ ╲ ╱│ ╲_____╱ ← 凹函数曲线│└──────────────────────────── x x₁ 中点 x₂ M = $f\\left(\\frac{x_1+x_2}{2}\\right)$（先平均再变换） M’ = $\\frac{f(x_1)+f(x_2)}{2}$（先变换再平均） 因为曲线是凹的，所以 M 在 M’ 上方，即 $f(\\text{平均}) \\geq \\text{平均}(f)$ 6.4.2 KL 散度非负性的详细证明 目标：证明 $D_{KL}(P|Q) \\geq 0$ 第一步：写出定义 $$D_{KL}(P|Q) = \\sum_x P(x) \\log \\frac{P(x)}{Q(x)}$$ 第二步：变形 $$= -\\sum_x P(x) \\log \\frac{Q(x)}{P(x)}$$ 令 $r(x) = \\frac{Q(x)}{P(x)}$，则： $$D_{KL}(P|Q) = -\\sum_x P(x) \\log r(x) = -\\mathbb{E}_P[\\log r(x)]$$ 第三步：应用 Jensen 不等式 因为 $\\log$ 是凹函数，根据 Jensen 不等式： $$\\sum_x P(x) \\log r(x) \\leq \\log \\left(\\sum_x P(x) \\cdot r(x)\\right)$$ 注意：凹函数的 Jensen 不等式是 $\\leq$ 第四步：计算右边 $$\\sum_x P(x) \\cdot r(x) = \\sum_x P(x) \\cdot \\frac{Q(x)}{P(x)} = \\sum_x Q(x) = 1$$ 所以： $$\\sum_x P(x) \\log r(x) \\leq \\log(1) = 0$$ 第五步：得出结论 $$D_{KL}(P|Q) = -\\sum_x P(x) \\log r(x) \\geq -0 = 0$$ 证毕！ ✓ 等号什么时候成立？ 当 $r(x) = \\frac{Q(x)}{P(x)} = \\text{常数}$ 对所有 $x$ 成立时。 又因为 $\\sum P(x) = \\sum Q(x) = 1$，所以这个常数只能是 1。 即：**$P(x) = Q(x)$ 对所有 $x$ 成立时，$D_{KL} = 0$** 性质2：不对称性（超级重要！） $$D_{KL}(P | Q) eq D_{KL}(Q | P)$$ KL散度不是距离！方向很重要！ 6.4.3 前向 KL vs 反向 KL：深度解析 KL 散度的不对称性是其最重要的特性之一。理解前向和反向 KL 的区别，对于深度学习中的应用至关重要。 核心区别：谁在做主？ 特性 前向 KL $D_{KL}(P|Q)$ 反向 KL $D_{KL}(Q|P)$ 公式 $\\sum P(x) \\log \\frac{P(x)}{Q(x)}$ $\\sum Q(x) \\log \\frac{Q(x)}{P(x)}$ 视角 站在真相 $P$ 的角度 站在模型 $Q$ 的角度 核心权重 $P(x)$（真相决定一切） $Q(x)$（模型自己决定） 惩罚条件 $P0$ 但 $Q≈0$ 时爆炸 $Q0$ 但 $P≈0$ 时爆炸 行为模式 Zero-Avoiding（避免遗漏） Zero-Forcing（强制精准） 结果 模糊但全面 尖锐但保守 别名 Mean-seeking（寻找平均） Mode-seeking（寻找模态） 为什么”权重”决定了命运？ 公式里只是交换了 $P$ 和 $Q$ 的位置，为什么行为差异这么大？**因为前面的权重项充当了”开关”**。 详细例子：双峰分布问题 假设真实分布 $P$ 是一个双峰分布（比如身高分布：男性峰 + 女性峰）： 真实分布 P（双峰）: ████ ████ ██████ ██████ ████████ ████████ ██████████ ██████████ ████████████ ████████████------------------------------------------- 峰1 峰2 但我们的模型 $Q$ 只能是单峰分布（比如只能用一个高斯分布）。 问题：$Q$ 应该怎么放？ 情况1：前向 KL（$D_{KL}(P|Q)$）— Zero Avoiding 惩罚机制：在 $P(x) 0$ 的地方，如果 $Q(x) \\approx 0$，则 $\\log \\frac{P(x)}{Q(x)} \\to +\\infty$ 后果：$Q$ 必须覆盖 $P$ 有概率的所有地方，否则会被严重惩罚！ 前向 KL 的结果 — Q 变得\"模糊但全面\": ████ ████ ← P（真实） ██████ ██████ ┌─────────────────────────────────┐ │ █████████████████████████████ │ ← Q（模型） │ ███████████████████████████████ │ 覆盖了两个峰 └─────────────────────────────────┘ 但中间也有概率 特点：宁可在中间”瞎猜”，也不能漏掉任何一个峰 → Mean-seeking（均值寻找） 情况2：反向 KL（$D_{KL}(Q|P)$）— Zero Forcing 惩罚机制：在 $Q(x) 0$ 的地方，如果 $P(x) \\approx 0$，则 $\\log \\frac{Q(x)}{P(x)} \\to +\\infty$ 后果：$Q$ 只能在 $P$ 有概率的地方出现，否则会被严重惩罚！ 反向 KL 的结果 — Q 变得\"尖锐但保守\": ████ ████ ← P（真实） ██████ ██████ ████ ← Q（模型） ██████ 只选择一个峰 ████████ 精准但不全面 特点：宁可放弃一个峰，也不能在 $P=0$ 的地方有概率 → Mode-seeking（模式寻找） 形象比喻总结 前向KL：严厉的老师（Teacher Forcing） 心态：老师拿着标准答案 $P$ 巡视 规则：”我考纲（$P$）里有的知识点，你（$Q$）必须都要会！如果你交白卷（$Q \\approx 0$），我就给你挂科！” 结果：为了不挂科，你被迫把所有可能考的东西都写上去。哪怕有些地方你不懂，你也得硬凑。 反向KL：保守的学生（Student’s Choice） 心态：你自己拿着手电筒 $Q$ 在雷区 $P$ 探路 规则：”只要我踩到了雷（$P \\approx 0$），我就死定了（Loss 无穷大）。但我没踩到的地方，雷再大也炸不到我。” 结果：为了保命，你发现最好的策略是缩在一个绝对安全的地方不动。你放弃了探索其他区域，只死守一个山头。 第七章：实战应用7.1 RLHF中的KL惩罚在训练大模型（如 ChatGPT）时，有个步骤叫 RLHF（基于人类反馈的强化学习）。 问题：如果只优化人类打分（Reward），模型可能会： 疯狂说彩虹屁拿高分 忘记正常的语言规则 最终崩溃成胡言乱语 解决方案：加KL惩罚 $$\\max ; \\mathbb{E}[\\text{Reward}] - \\beta \\cdot D_{KL}(\\pi_{\\text{new}} | \\pi_{\\text{old}})$$ 各部分含义： 符号 含义 $\\pi_{\\text{new}}$ 新模型（正在训练的） $\\pi_{\\text{old}}$ 旧模型（基座模型） $\\text{Reward}$ 人类打分（越高越好） $D_{KL}$ KL惩罚（防止离旧模型太远） $\\beta$ 调节强度（$\\beta$ 越大越保守） 形象比喻： KL惩罚 = 风筝线 让你飞高（追求高分） 但拉住你别飞丢（保持语言能力） 7.2 其他应用 领域 应用 KL散度的作用 VAE 变分自编码器 ELBO损失函数的核心 知识蒸馏 大模型→小模型 让小模型模仿大模型的输出分布 GAN 生成对抗网络 衡量生成分布和真实分布的差距 DPO 直接偏好优化 新一代RLHF，直接优化KL约束下的偏好 信息检索 搜索引擎 计算文档相似度 第八章：总结8.1 知识地图 从信息论到KL散度的完整路径： 信息量 I(x) = -log P(x) ↓熵 H(P) = 期望[I(x)] = -Σ P(x)log P(x) ↓交叉熵 H(P,Q) = -Σ P(x)log Q(x) ↓KL散度 = H(P,Q) - H(P) = Σ P(x)log[P(x)/Q(x)] 8.2 记忆口诀6句话记住KL散度： 1️⃣ KL散度不是距离，是”信息浪费” 用 $Q$ 编码 $P$ 比用 $P$ 编码 $P$ 多浪费的 bits 2️⃣ 永远 ≥ 0，= 0 当且仅当 P = Q 完美匹配时无浪费 3️⃣ 不对称！$D_{KL}(P|Q) eq D_{KL}(Q|P)$ 前向关注覆盖，反向关注精确 4️⃣ 前向KL让模型”不遗漏”（Zero Avoiding） $P$ 有的地方 $Q$ 必须有 5️⃣ 反向KL让模型”不出错”（Zero Forcing） $Q$ 有的地方必须符合 $P$ 6️⃣ 在深度学习中常作正则项防止”忘本” RLHF、VAE、蒸馏都用它 8.3 公式速查完整公式： $$D_{KL}(P | Q) = \\sum_{x} P(x) \\log \\frac{P(x)}{Q(x)}$$ 等价形式： $$\\begin{aligned}D_{KL}(P | Q) = H(P, Q) - H(P) \\= \\sum_{x} P(x) \\log P(x) - \\sum_{x} P(x) \\log Q(x) \\= \\mathbb{E}_{x \\sim P} \\left[ \\log \\frac{P(x)}{Q(x)} \\right]\\end{aligned}$$ 连续情况： $$D_{KL}(P | Q) = \\int P(x) \\log \\frac{P(x)}{Q(x)} dx$$ 8.4 常见误区 误区 真相 KL散度是距离 ❌ 它不满足对称性和三角不等式 KL散度可以为负 ❌ 永远 ≥ 0 $D_{KL}(P|Q)$ 和 $D_{KL}(Q|P)$ 相等 ❌ 完全不同！方向很重要 KL散度越大越好 ❌ 越小越好，0 = 完美 只有深度学习用KL散度 ❌ 信息论、统计学、物理学都用 📚 延伸阅读推荐资源 经典教材 《信息论基础》（Elements of Information Theory） 《深度学习》（Deep Learning Book）第3章 在线资源 Distill.pub - KL Divergence可视化 3Blue1Brown - 信息论视频 相关概念 JS散度（Jensen-Shannon Divergence） Wasserstein距离 f-散度（f-Divergence） 练习题初级： 计算均匀分布 $P=[0.25, 0.25, 0.25, 0.25]$ 的熵 证明：为什么 $D_{KL}(P | Q) \\geq 0$？ 中级：3. 推导：为什么交叉熵可以作为分类任务的损失函数？4. 对比前向KL和反向KL在变分推断中的应用 高级：5. 实现一个VAE，观察ELBO中KL项的作用6. 分析PPO算法中KL约束的超参数 $\\beta$ 如何影响训练稳定性 练习题参考答案初级题答案 均匀分布的熵 $$H(P) = -\\sum_{i=1}^{4} 0.25 \\times \\log_2(0.25) = -4 \\times 0.25 \\times (-2) = 2 \\text{ bits}$$ 均匀分布的熵最大，因为不确定性最高。 KL 散度非负性证明（见 6.4.2 节的详细证明） 中级题答案 交叉熵作为分类损失函数 在分类任务中： $P$ = 真实标签的 one-hot 编码（如 $[0, 1, 0, 0]$） $Q$ = 模型预测的概率分布（如 $[0.1, 0.7, 0.1, 0.1]$） 损失函数：$$\\text{Loss} = H(P, Q) = -\\sum_i P_i \\log Q_i$$ 因为 $P$ 中只有一个位置为 1，其余为 0，所以：$$\\text{Loss} = -\\log Q_{\\text{true}}$$ 这就是负对数似然（Negative Log-Likelihood），最小化它等价于最大化正确类别的预测概率。 前向 vs 反向 KL 在变分推断中的应用 方面 前向 KL (ELBO) 反向 KL 公式 $D_{KL}(Q|P)$ $D_{KL}(P|Q)$ 可计算性 ✅ 可以优化 ❌ 需要归一化常数 行为 Mode-seeking Mean-seeking 应用 VAE 标准做法 期望传播（EP） VAE 使用反向 KL 是因为它不需要知道 $P$ 的归一化常数。 8.5 数值计算实例 完整示例：离散分布的 KL 散度计算 设有离散分布 P 和 Q，取值为 {A, B, C}： 取值 P (真实) Q (模型) P/Q log(P/Q) P·log(P/Q) A 0.5 0.4 1.25 0.32 0.16 B 0.3 0.4 0.75 -0.42 -0.13 C 0.2 0.2 1.00 0 0 合计 1.0 1.0 — — ≈ 0.03 bits 计算步骤： import numpy as npP = np.array([0.5, 0.3, 0.2])Q = np.array([0.4, 0.4, 0.2])# 计算 KL 散度D_KL = np.sum(P * np.log(P / Q))print(f\"D_KL(P||Q) = {D_KL:.4f} bits\")# 输出: D_KL(P||Q) = 0.0361 bits 结果解读： KL 散度 = 0.036 bits：用 Q 编码 P 的信息，平均每个符号多浪费 0.036 bits 主要误差来源：A 点被低估（P=0.5, Q=0.4） 当 P = Q 时，D_KL = 0（完美匹配） 🎉 结语恭喜你！如果你读到这里，你已经从零开始，完全理解了KL散度的本质： ✅ 从信息论出发，理解了信息量、熵、交叉熵 ✅ 掌握了KL散度的公式和每个变量的含义 ✅ 通过图示和例子建立了直觉 ✅ 了解了在深度学习中的实际应用 最后的建议： KL散度不是用来背的，而是用来理解的。每次在论文或代码中看到它，问自己三个问题： 这里的 $P$ 是什么？（真实分布） 这里的 $Q$ 是什么？（模型分布） 为什么要用KL散度？（防止过拟合？约束输出？） 带着这些问题，你会发现KL散度无处不在，而你已经完全理解它了。 📝 更新日志 2026-02-08：首次发布，包含完整的数学证明、前向/反向KL深度解析和5张可视化图表","tags":["Python"],"categories":["深度学习"]},{"title":"InstructGPT 与 RLHF 深度解析：费曼视角下的原理与实现","path":"/2026/02/07/InstructGPT-RLHF-Complete-Guide/","content":"🎯 面试官视角：这篇文章不仅仅是教程，更是你的面试复习提纲。我们不堆砌代码，而是深入到 原理 (Principles)、输入输出流 (I/O Flow) 和 **数学推导 (Math Derivation)**。如果你能用本文的逻辑把 RLHF 讲清楚，P7/L6 级别的算法岗面试不在话下。 0. 费曼时刻：什么是 Alignment？想象你正在训练一只天才鹦鹉（GPT-3）。它读遍了世界上的所有书，能补全任何句子。 你说：“床前明月光”，它接：“疑是地上霜”。 你问：“如何制造毒药？”，它接：“首先你需要……”（因为它在侦探小说里看过）。 问题来了：这只鹦鹉太“耿直”了，它不懂什么是对，什么是错，什么是安全。它只是在做概率预测。 RLHF (Reinforcement Learning from Human Feedback) 就是送这只鹦鹉去上礼仪学校的过程： SFT（小学）：老师给它标准答案，让它照着抄，学会“像人一样说话”。 RM（中考）：老师不再给答案，而是给它的回答打分。它学会了“老师喜欢什么样的回答”。 PPO（体育特训）：没有老师盯着，它自己根据之前的打分标准不断练习，试图拿高分，同时别把脑子练坏了（KL Penalty）。 1. 第一阶段：监督微调 (SFT) 一句话解释：从“续写机器”变成“问答助手”。 1.1 原理深度解析预训练模型（Pretrained Model）的目标是 $P(x_t|x_{","tags":["Python"],"categories":["深度学习"]},{"title":"GPT 系列深度解析：从 GPT-1 到 GPT-3","path":"/2026/02/05/2026-02-05-gpt-series-deep-dive/","content":"摘要：本文深度解析 OpenAI 的 GPT 系列模型演进之路。从 GPT-1 的预训练+微调范式，到 GPT-2 的零样本尝试，再到 GPT-3 的上下文学习（In-context Learning）与规模法则（Scaling Laws）。文章详细拆解了模型架构、Tensor 维度变化、训练数据流，并配有 13 张可视化图表与通俗易懂的费曼式讲解。 📋 目录 通俗理解：GPT 到底在做什么？ NLP 预训练时代全景 GPT vs BERT vs T5：核心差异 GPT-1：Pre-train + Fine-tune GPT-2：Zero-shot Learning GPT-3：In-context Learning 三代演进对比总结 代码实现参考 费曼总结：教给小白听 🎯 通俗理解：GPT 到底在做什么？一句话总结 GPT 就是一个”续写机器”：给它一段话的开头，它预测接下来应该写什么。 生活类比：作家 vs 考生为了直观理解 GPT 的工作方式，我们来看下面这张对比图： 图表深度解读：上图展示了两种截然不同的语言处理模式。 GPT (上部分)：被描绘成一个正在打字的作家。它的视野是”单向”的（绿色箭头只向右），意味着它只能看到已经写出来的内容。就像写小说一样，作者根据前文 Once upon a time 来构思下一个词 there。这种自回归（Autoregressive）的特性使它天生适合文本生成任务。 BERT (下部分)：被描绘成一个正在做完形填空的学生。它的视野是”双向”的（红色箭头同时指向左右），它可以同时看到空格前后的内容 The ___ sat on。这使它在理解上下文语境时非常强大，适合分类、实体识别等理解任务，但无法像 GPT 那样流畅地生成文本。 为什么这个简单任务能产生”智能”？你可能会问，预测下一个词听起来很简单，为什么能产生智能？ 训练数据：互联网上的所有文本（书籍、网页、对话...）当模型学会预测 \"The capital of France is ___\"→ 它必须\"知道\"法国的首都是巴黎当模型学会预测 \"2 + 2 = ___\" → 它必须\"理解\"基本数学当模型学会预测 \"If it rains, I will bring an ___\"→ 它必须\"推理\"下雨需要带伞结论：预测下一个词 = 被迫学习世界知识！ 📅 NLP 预训练时代全景里程碑时间线下图梳理了 NLP 预训练模型发展的黄金三年（2017-2020）： 图表深度解读： 起点 (2017.06)：Transformer 的诞生是原点。Google 发布的 Attention Is All You Need 论文提出 Self-Attention 机制，彻底取代了 RNN/LSTM，允许模型并行训练，为大规模模型奠定了基础。 **分支一 (Decoder-only)**：图表上方是 GPT 家族。从 GPT-1 到 GPT-2 再到 GPT-3，OpenAI 坚持走”纯解码器”路线，专注于生成能力。可以看到参数量呈指数级爆炸（117M → 1.5B → 175B）。 **分支二 (Encoder-only)**：图表下方是 BERT 家族。BERT 引入 MLM（掩码语言模型），刷新了几乎所有 NLP 榜单，随后衍生出 RoBERTa、DistilBERT 等优化版本，专注于理解任务。 **分支三 (Encoder-Decoder)**：中间是 T5/BART 等尝试统一两者的架构，试图在理解和生成之间找到平衡。 模型规模演进这短短几年间，模型大小发生了什么变化？ 图表深度解读： 这是一张对数坐标图。纵轴代表参数量（Log scale）。 ELMo (94M) 和 GPT-1 (117M) 处于起步阶段，相当于”小个子”。 BERT-Large (340M) 定义了当时的”大模型”标准。 GPT-2 (1.5B) 首次突破 10亿 参数大关，证明了模型越大效果越好。 GPT-3 (175B) 则是一个巨大的飞跃（右上角那个遥不可及的点），它比之前的模型大两个数量级，直接开启了”大模型（LLM）”时代。 ⚔️ GPT vs BERT vs T5：核心差异架构对比三者在神经网络架构上究竟有何不同？ 图表深度解读： GPT (左)：Decoder-only 架构。注意其中的连接线是单向的（只从左向右）。这意味着处理第 $i$ 个词时，模型只能看到 $0$ 到 $i-1$ 个词。Masked Self-Attention 强制了这种因果关系。 BERT (中)：Encoder-only 架构。连接线是全连接的，任何位置的词都能看到整个序列的信息。这对于理解句子含义至关重要，但使得它无法像人类说话一样逐词生成。 T5 (右)：Encoder-Decoder 架构。左边是一个双向的 Encoder（读入输入），右边是一个单向的 Decoder（生成输出）。这就像机器翻译：先读懂原文（Encode），再写出译文（Decode）。 训练目标对比它们在预训练时分别在做什么题？ 图表深度解读： GPT (Autoregressive LM)：任务是预测下一个词。图中显示模型看到 The cat sat，目标是预测 on。这是最自然的语言生成任务。 BERT (Masked LM)：任务是完形填空。图中 sat 被 [MASK] 遮住了，模型需要利用上下文 The cat [MASK] on 把 sat 填回去。 T5 (Span Corruption)：任务是还原片段。输入中一段文本被挖掉了，模型需要生成被挖掉的内容。这是一种更通用的序列到序列任务。 🔷 第一部分：GPT-1 深度解析1.1 核心思想GPT-1 (Generative Pre-Training) 的核心贡献是确立了 Pre-training + Fine-tuning 的范式。在此之前，NLP 任务主要靠从头训练（scratch）或使用静态词向量（Word2Vec）。GPT-1 证明：先在一个海量无标注文本上训练一个语言模型，然后针对特定任务（分类、蕴含等）进行微调，效果远超专门设计的模型。 1.2 🔬 Tensor 维度变化（完整数据流）让我们深入模型内部，看看数据是如何流动的。 图表深度解读：这张详细的数据流图展示了一个 batch 的数据在 GPT-1 中的完整旅程： Input：输入 Token 序列 [2, 5]（假设 batch=2, seq=5）。 Embedding：Token ID 被转换为 768 维的向量，并加上了位置编码（Position Embedding）。此时 Tensor 形状为 [2, 5, 768]。 Transformer Block： Q, K, V Projection：输入被投影为 Query, Key, Value。 Split Heads：768 维被拆分为 12 个头，每个头 64 维。形状变为 [2, 12, 5, 64]。 Attention Score：Q 和 K 相乘，得到 [2, 12, 5, 5] 的分数矩阵。关键点：这里应用了 Causal Mask（右上角为负无穷），确保词 $i$ 只能关注到词 $0…i$。 Output：经过 Softmax 和 V 相乘，并拼接所有头，恢复为 [2, 5, 768]。 FFN：经过两层全连接层（中间升维到 3072），引入非线性。 Logits：最后经过线性层映射回词表大小 [2, 5, 40000]，表示每个位置预测下一个词的概率分布。 1.3 训练闭环 图表深度解读： 图展示了标准的 自监督学习（Self-Supervised Learning） 循环。 Shift Trick：输入是 $x_0, x_1, x_2, x_3$，标签（Target）则是 $x_1, x_2, x_3, x_4$。也就是输入序列整体右移一位作为监督信号。 Loss 计算：模型输出的 Logits 与 Target 进行 Cross Entropy Loss 计算，梯度回传更新所有参数。 🔷 第二部分：GPT-2 深度解析2.1 核心思想：Zero-shot LearningGPT-2 的标题是 *”Language Models are Unsupervised Multitask Learners”*。OpenAI 发现，当模型足够大、数据足够多时，不需要 Fine-tuning，模型就能直接执行任务。比如你给它输入 “Translate to French: cheese =”，它会自动补全 “fromage”。 2.2 数据升级：WebText 图表深度解读： **GPT-1 (左)**：使用的是 BooksCorpus，主要是小说书籍。文本风格单一，虽然连贯但覆盖面窄。 **GPT-2 (右)**：使用的是 WebText。OpenAI 爬取了 Reddit 上所有获赞超过 3 个的链接内容。这意味着数据经过了人类的”筛选”（只有高质量内容才会被分享和点赞）。 量级提升：数据量从 5GB 激增到 40GB。这使得模型见识到了更广阔的世界（新闻、代码、食谱、科技论文…）。 2.3 架构微调：Pre-LNGPT-2 将 Layer Normalization 移到了 Attention 和 FFN 的输入端（称为 Pre-LN）。这大大稳定了深层网络（48层）的梯度传播，使得训练更深的模型成为可能。 🔷 第三部分：GPT-3 深度解析3.1 核心思想：In-context Learning (ICL)GPT-3 并没有修改模型架构，而是将参数量推到了恐怖的 1750亿。在这个尺度下，模型涌现出了神奇的 上下文学习（In-context Learning） 能力。 图表深度解读：这张图解释了 GPT-3 独特的使用方式——不需要更新模型权重（梯度下降），只需要在 Prompt 中给它”演示”一下： **Zero-shot (上)**：不给例子，直接问。例如：”Translate English to French: cheese =”。这对模型要求最高。 **One-shot (中)**：给 1 个例子。例如：”sea otter = loutre de mer cheese =”。模型通过这一个例子”学会”了现在的任务是翻译。 **Few-shot (下)**：给 10-100 个例子。这能极大地提升模型性能。本质：GPT-3 将”学习”过程从”更新权重”变成了”读取上下文”。 3.2 Scaling Laws (规模法则)OpenAI 在训练 GPT-3 时发现了一个惊人的规律。 图表深度解读： 这三张图展示了 Loss（测试误差）与三个变量的关系：**计算量(C)、数据集大小(D)、参数量(N)**。 双对数坐标下的直线：这三条线在双对数坐标系下几乎是完美的直线！ **幂律分布 (Power Law)**：这意味着性能的提升与投入资源呈幂律关系（$L \\propto N^{-\\alpha}$）。 启示：只要你增加参数、增加数据、增加算力，模型的效果就会可预测地变好。这给了 OpenAI 巨大的信心去烧钱训练 GPT-4。 3.3 参数量对比 图表深度解读： 这是一张极具视觉冲击力的对比图。 左边微小的蓝色方块是 GPT-1 (117M)。 中间稍微大一点的绿色方块是 GPT-2 (1.5B)。 右边那个巨大的、占据了整个画面的红色柱子是 GPT-3 (175B)。 GPT-3 的参数量是 GPT-2 的 117 倍！这种暴力美学彻底改变了 AI 领域的游戏规则。 🔷 三代演进对比总结任务适配方法演进我们如何让模型为我们工作？三代模型给出了不同的答案。 图表深度解读： GPT-1 (左)：Fine-tuning。模型是通用的，但使用时需要改变模型结构（加分类头）并重新训练权重。缺点是每个任务都需要存一份模型副本。 GPT-2 (中)：Zero-shot。完全不改变模型，通过构造 Prompt 诱导模型输出。但效果往往不如微调。 GPT-3 (右)：Few-shot ICL。不改变权重，但在输入中加入少量示例（Context）。这结合了前两者的优点：既不需要训练，又能通过示例让模型快速适应特定任务。 全面对比表 这张表格总结了三代模型的关键参数。注意 Context Window (上下文窗口) 的变化：从 512 到 1024 再到 2048。这意味着模型能”记住”更长的对话历史。 💻 代码实现参考以下是一个简化的 GPT Block 实现（PyTorch），展示了 Masked Attention 的核心逻辑： class GPTBlock(nn.Module): \"\"\"GPT-2 style Transformer block with Pre-LN\"\"\" def __init__(self, hidden_size=768, num_heads=12): super().__init__() self.ln1 = nn.LayerNorm(hidden_size) self.attn = nn.MultiheadAttention(hidden_size, num_heads, batch_first=True) self.ln2 = nn.LayerNorm(hidden_size) self.ffn = nn.Sequential( nn.Linear(hidden_size, 4 * hidden_size), nn.GELU(), nn.Linear(4 * hidden_size, hidden_size) ) def forward(self, x, attn_mask=None): # 1. Pre-LN + Attention + Residual normed = self.ln1(x) # attn_mask 必须是下三角矩阵，防止看到未来 attn_out, _ = self.attn(normed, normed, normed, attn_mask=attn_mask) x = x + attn_out # 2. Pre-LN + FFN + Residual x = x + self.ffn(self.ln2(x)) return x 🎓 费曼总结：教给小白听🍎 文字接龙的终极形态想象你在和一个超级博学的朋友玩”接龙”游戏： 你说： \"从前有座山，山上有座庙，庙里有个...\"朋友接： \"老和尚\"你继续： \"老和尚在给小和尚讲...\" 朋友接： \"故事\" GPT 就是这样一个”接龙高手”： 读书破万卷：它读了互联网上几乎所有的文字（书籍、网页、代码…）。 统计大师：它不一定懂逻辑，但它知道”如果前面是A，后面大概率是B”。 大力出奇迹：当它读的书足够多（TB级数据）、脑容量足够大（1750亿参数）时，它为了猜对下一个词，被迫”学会”了逻辑、数学、翻译甚至编程。 🔑 核心结论 GPT 的本质：一个超级强大的”文字接龙”程序。 智能的来源：为了完美地预测下一个词，模型必须构建对世界的认知模型。 未来的方向：Scaling Laws 告诉我们，继续把模型做大、数据喂多，它还会变得更聪明。 参考论文： Improving Language Understanding by Generative Pre-Training (2018) Language Models are Unsupervised Multitask Learners (2019) Language Models are Few-Shot Learners (2020)","tags":["Deep Learning"],"categories":["深度学习"]},{"title":"BERT 完整解析：从论文到 KV Cache","path":"/2026/02/03/bert-comprehensive-guide/","content":"BERT 完整解析：从论文到 KV Cache 学习目标：深度理解 BERT 论文核心原理、Q/K/V 交互机制、KV Cache 优化技术、Causal Attention，以及完整的训练闭环。 🖼️ 核心可视化图解BERT 整体架构与核心概念) BERT 架构详解) Q/K/V 数据流可视化) 注意力机制详解) Causal vs Bidirectional Attention 对比) 🆚 BERT vs “Attention Is All You Need” 对比分析论文基本信息对比 维度 Attention Is All You Need BERT 发表时间 2017年6月 2018年10月 作者团队 Google Brain + Google Research Google AI Language 核心贡献 提出 Transformer 架构 提出预训练-微调范式 引用量 10万+ 9万+ 地位 奠基之作 应用突破 架构关系Attention Is All You Need (2017) │ ▼ 提供了核心架构 ┌────────────────────────────────┐ │ Transformer 架构 │ │ • Multi-Head Attention │ │ • Position Encoding │ │ • Layer Normalization │ │ • Feed-Forward Network │ │ • Encoder + Decoder │ └────────────────────────────────┘ │ ▼ BERT 只用 Encoder 部分 ┌────────────────────────────────┐ │ BERT (2018) │ │ • 只用 Transformer Encoder │ │ • 加入 MLM 预训练任务 │ │ • 提出预训练+微调范式 │ │ • 双向注意力 │ └────────────────────────────────┘ 核心区别 维度 Transformer 原论文 BERT 架构 Encoder + Decoder 仅 Encoder 任务 机器翻译（Seq2Seq） 语言理解（分类、NER、QA） 注意力 Encoder双向，Decoder单向 全部双向 预训练 无（监督训练） MLM + NSP Position Encoding 固定的正弦函数 可学习的 Embedding 应用 需要成对数据（中英翻译） 通用特征提取器 BERT 继承了什么？# 1. Multi-Head Self-AttentionQ = X @ W_QK = X @ W_KV = X @ W_VAttention(Q, K, V) = softmax(QK^T / √d_k)V# 2. Layer Normalization + 残差连接output = LayerNorm(X + Attention(X))# 3. Feed-Forward NetworkFFN(x) = max(0, xW₁ + b₁)W₂ + b₂# 4. 整体结构for layer in range(12): X = LayerNorm(X + MultiHeadAttention(X)) X = LayerNorm(X + FFN(X)) BERT 的创新点创新 1: 只用 Encoder 原因：NLP 大多数任务是\"理解\"，不是\"生成\"创新 2: MLM 预训练任务 让模型学会双向上下文（Transformer 原论文没有预训练）创新 3: 预训练-微调范式 一次预训练 → 多个任务复用创新 4: 可学习的位置编码 Position Embedding 是训练出来的，不是固定公式 🆚 BERT vs Attention Is All You Need：论文对比论文基本信息对比 维度 Attention Is All You Need BERT 发表时间 2017年6月 2018年10月 作者团队 Google Brain + Google Research Google AI Language 核心贡献 提出 Transformer 架构 提出预训练-微调范式 引用量 100,000+ 90,000+ 历史地位 架构奠基之作 应用突破之作 核心关系：继承与创新Attention Is All You Need (2017) │ ▼ 提供了核心架构 ┌────────────────────────────────┐ │ Transformer 架构 │ │ • Encoder-Decoder 结构 │ │ • Multi-Head Attention │ │ • Position Encoding │ │ • Layer Normalization │ │ • Feed-Forward Network │ └────────────────────────────────┘ │ ▼ BERT 只用 Encoder 部分 ┌────────────────────────────────┐ │ BERT (2018) │ │ • 只用 Transformer Encoder │ │ • 提出 MLM 预训练任务 │ │ • 提出预训练+微调范式 │ │ • 双向上下文建模 │ └────────────────────────────────┘ 架构对比 维度 Transformer BERT 架构 Encoder + Decoder 仅 Encoder 适用任务 机器翻译（Seq2Seq） 分类、NER、问答 注意力类型 Encoder用双向，Decoder用单向 全部双向 预训练任务 无（需要平行语料） MLM + NSP Position Embedding 固定的正弦函数 可学习的向量 BERT 继承了什么？✅ 完全继承： Multi-Head Self-Attention 机制 Feed-Forward Network (FFN) 残差连接 + Layer Normalization Q/K/V 计算方式 🔧 改进部分： Position Embedding：从固定改为可学习 只用 Encoder，去掉 Decoder 添加 Segment Embedding 添加特殊 Token：[CLS]、[SEP]、[MASK] 🆚 BERT vs “Attention Is All You Need” 对比论文基本信息 维度 Attention Is All You Need BERT 发表时间 2017年6月 2018年10月 作者团队 Google Brain + Google Research Google AI Language 核心贡献 提出 Transformer 架构 提出预训练-微调范式 引用量 10万+ 9万+ 地位 奠基之作（架构创新） 应用突破（范式创新） 架构对比Attention Is All You Need (2017) - 原始 Transformer┌─────────────────────────────────────────────────────────────┐│ ││ 输入：\"我爱你\" 输出：\"I love you\" ││ ↓ ↑ ││ ┌─────────┐ ┌─────────┐ ││ │ Encoder │ ───上下文───→ │ Decoder │ ││ │(6层) │ │(6层) │ ││ │双向注意力│ │单向注意力│ ││ └─────────┘ └─────────┘ ││ ││ 用途：机器翻译（seq2seq） │└─────────────────────────────────────────────────────────────┘BERT (2018) - 只用 Encoder┌─────────────────────────────────────────────────────────────┐│ ││ 输入：\"我爱[MASK]天安门\" ││ ↓ ││ ┌─────────┐ ││ │ Encoder │ ──→ 直接输出每个位置的表示 ││ │(12层) │ ↓ ││ │双向注意力│ 在 [MASK] 位置预测 \"北京\" ││ └─────────┘ ││ ││ 用途：理解任务（分类、NER、QA） │└─────────────────────────────────────────────────────────────┘ BERT 继承了 Transformer 的什么？ 组件 Transformer BERT 说明 Multi-Head Attention ✅ 原创 ✅ 完全继承 Q/K/V 机制一模一样 Position Encoding ✅ 正弦函数 ⚠️ 改为可学习 BERT 用可训练的位置嵌入 Layer Normalization ✅ 原创 ✅ 完全继承 Feed-Forward Network ✅ 原创 ✅ 完全继承 Encoder 结构 ✅ 6层 ✅ 12/24层 BERT 加深了层数 Decoder 结构 ✅ 6层 ❌ 删除 BERT 不需要 Decoder BERT 的创新点 创新 说明 MLM 预训练任务 Transformer 没有预训练，BERT 用 MLM 学习双向表示 NSP 任务 学习句子间关系（后续被证明用处不大） 预训练+微调范式 Transformer 是任务特定训练，BERT 开创迁移学习 只用 Encoder Transformer 是完整 Encoder-Decoder，BERT 简化架构 联系：BERT 站在 Transformer 肩膀上2017 Transformer 提供核心架构 │ ▼┌───────────────────────────────┐│ • Multi-Head Attention ││ • Position Encoding ││ • Feed-Forward Network ││ • Encoder-Decoder 架构 │└───────────────────────────────┘ │ ▼ BERT 选择性使用┌───────────────────────────────┐│ ✅ 复用 Encoder 部分 ││ ✅ 复用 Attention 机制 ││ ❌ 删除 Decoder ││ ➕ 加入 MLM 预训练 ││ ➕ 提出预训练-微调范式 │└───────────────────────────────┘ 简单记忆： Transformer = 提供了”工具箱”（架构组件） BERT = 用工具箱中的部分工具，发明了新的使用方法（预训练范式） 🆚 BERT vs “Attention Is All You Need” 对比论文基本信息对比 维度 Attention Is All You Need BERT 发表时间 2017年6月 2018年10月 作者团队 Google Brain + Google Research Google AI Language 核心贡献 提出 Transformer 架构 提出预训练-微调范式 架构 Encoder + Decoder 仅 Encoder 训练任务 机器翻译（有监督） MLM + NSP（自监督） 引用量 10万+ 9万+ 继承关系Attention Is All You Need (2017) ↓ 提供核心架构 ┌────────────────────────────────┐ │ Transformer 架构 │ │ • Multi-Head Attention │ │ • Position Encoding │ │ • Layer Normalization │ │ • Feed-Forward Network │ └────────────────────────────────┘ ↓ BERT 只用 Encoder 部分 ┌────────────────────────────────┐ │ BERT (2018) │ │ • 12层 Transformer Encoder │ │ • MLM 预训练任务 │ │ • 预训练+微调范式 │ └────────────────────────────────┘ 架构对比原始 Transformer（翻译任务）：┌─────────────────────────────────────────────────────────────┐│ 输入：\"我爱你\" 输出：\"I love you\" ││ ↓ ↑ ││ ┌─────────┐ ┌─────────┐ ││ │ Encoder │ ───上下文───→ │ Decoder │ ││ │(理解输入)│ │(生成输出)│ ││ └─────────┘ └─────────┘ ││ 双向注意力 单向注意力(Causal) ││ 无需 KV Cache 需要 KV Cache │└─────────────────────────────────────────────────────────────┘BERT（理解任务）：┌─────────────────────────────────────────────────────────────┐│ 输入：\"我爱[MASK]天安门\" ││ ↓ ││ ┌─────────┐ ││ │ Encoder │ ──→ 直接输出每个位置的表示 ││ │(理解输入)│ ↓ ││ └─────────┘ 在 [MASK] 位置预测 \"北京\" ││ 双向注意力 ││ 无需 KV Cache │└─────────────────────────────────────────────────────────────┘ 核心区别总结 维度 Transformer (原论文) BERT 架构选择 Encoder + Decoder 仅 Encoder 注意力模式 Encoder双向 + Decoder单向 全部双向 适用任务 序列到序列（翻译） 理解类任务 训练数据 平行语料（需标注） 大规模文本（无需标注） KV Cache Decoder需要 不需要 影响 奠定架构基础 开创预训练范式 📄 Part 1: BERT 论文深度解析1.1 论文基本信息标题: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding作者: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova (Google AI Language)发表: NAACL 2019论文链接: https://arxiv.org/abs/1810.04805 1.2 研究动机：为什么需要 BERT？在 BERT 之前，NLP 预训练模型存在两大局限： 问题 1：单向语言模型的局限性GPT-1 (2018): 只能从左往右看输入: \"我 爱 北京 天安门\" ↓ ↓ ↓ ↓每个词只能看到左边的上下文问题: \"银行\" 在 \"我去银行存钱\" vs \"河边的银行很陡\" 中 如果只看左边，无法区分是 \"金融机构\" 还是 \"河岸\" 问题 2：浅层双向的局限性ELMo (2018): 使用两个独立的 LSTM → LSTM (从左往右) → LSTM (从右往左) → 最后拼接问题: 两个方向的信息只在最顶层融合，中间层无法深度交互 BERT 的核心创新: 通过 Masked Language Model (MLM)，在每一层都实现真正的双向上下文建模。 1.3 核心方法：两个预训练任务任务 1: Masked Language Model (MLM) - 核心操作流程: 随机选择 15% 的 token 进行 mask 其中： 80% 替换为 [MASK] 10% 替换为随机词 10% 保持不变 为什么这样设计？ 80% [MASK]: 让模型学习预测 10% 随机词: 避免模型只依赖 [MASK] 标记 10% 不变: 让模型学习真实分布 例子: 原始句子: \"我 爱 北京 天安门\"处理后: \"我 爱 [MASK] 天安门\" (80%)或: \"我 爱 上海 天安门\" (10% 随机)或: \"我 爱 北京 天安门\" (10% 不变)Label: 位置 3 = \"北京\"Loss = CrossEntropy(model_output[3], ID(\"北京\")) 维度分析: 输入: [batch, seq_len] = [32, 128] ↓ Embedding [32, 128, 768] ↓ 12 层 Transformer Encoder [32, 128, 768] ↓ MLM Head (Linear + Softmax) [32, 128, 21128] # 21128 = 词表大小 只计算被 mask 位置的 Loss 任务 2: Next Sentence Prediction (NSP)目的: 学习句子间关系 输入格式: [CLS] 句子A [SEP] 句子B [SEP]正样本: B 确实是 A 的下一句 (Label = 1)负样本: B 是随机选的句子 (Label = 0) 例子: 正样本:Input: [CLS] 今天天气很好 [SEP] 我们去公园吧 [SEP]Label: IsNext (1)负样本:Input: [CLS] 今天天气很好 [SEP] 人工智能很有趣 [SEP]Label: NotNext (0) Loss 计算: cls_output = encoder_output[:, 0, :] # [batch, 768] 取 [CLS] 位置logits = nsp_classifier(cls_output) # [batch, 2]loss = BinaryCrossEntropy(logits, labels) 后续研究发现: NSP 任务效果有限，RoBERTa 等后续工作移除了这个任务。 1.3.1 MLM 深度解析：完形填空的艺术通俗理解小学语文题： \"小明 _____ 学校上课\" 答案：去、到、在...BERT 的 MLM 就是让 AI 做这种\"完形填空\"！ 具体操作流程# 原始句子原句 = \"我爱北京天安门\"# Step 1: 随机选择 15% 的词进行处理选中 = \"北京\"# Step 2: 对选中的词进行三种处理（随机选一种）处理后 = \"我爱 [MASK] 天安门\" # 80% 概率：替换为 [MASK]或者 = \"我爱 上海 天安门\" # 10% 概率：替换为随机词或者 = \"我爱 北京 天安门\" # 10% 概率：保持不变# Step 3: 让模型预测被遮住的词是什么模型输入 = \"我爱 [MASK] 天安门\"模型输出 = \"北京\" ✅ (如果预测对了，loss 很小) = \"上海\" ❌ (如果预测错了，loss 很大) 为什么 MLM 能让 BERT 学会”理解语言”？场景 1： 输入：\"我去 [MASK] 存钱\" 模型学会：看到\"存钱\" → 预测\"银行\"（金融机构）场景 2： 输入：\"河边的 [MASK] 很陡峭\" 模型学会：看到\"河边\"\"陡峭\" → 预测\"河岸/堤坝\"通过数十亿次这样的\"完形填空\"训练后： → 模型学会了词语之间的关系 → 模型学会了语法结构 → 模型学会了常识知识 MLM 训练代码示例from transformers import BertTokenizer, BertForMaskedLMimport torch# 加载模型tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')model = BertForMaskedLM.from_pretrained('bert-base-chinese')# 输入一个带 [MASK] 的句子text = \"我爱[MASK]天安门\"inputs = tokenizer(text, return_tensors='pt')# 模型预测with torch.no_grad(): outputs = model(**inputs) predictions = outputs.logits# 找到 [MASK] 位置的预测结果mask_index = (inputs['input_ids'] == tokenizer.mask_token_id).nonzero()[0, 1]predicted_token_id = predictions[0, mask_index].argmax(dim=-1)predicted_token = tokenizer.decode(predicted_token_id)print(f\"原句: {text}\")print(f\"预测: {predicted_token}\") # 输出: 北京 1.3.2 预训练-微调范式详解通俗理解：培养”通才”再培养”专才”传统方式（从零开始）： ┌─────────────────────────────────────────────────┐ │ 任务：情感分析 │ │ 数据：10万条电商评论 │ │ 训练：从随机初始化开始，训练一个专门的模型 │ │ 耗时：3天，需要大量标注数据 │ └─────────────────────────────────────────────────┘ ┌─────────────────────────────────────────────────┐ │ 任务：垃圾邮件检测 │ │ 数据：5万封邮件 │ │ 训练：从随机初始化开始，再训练另一个专门的模型 │ │ 耗时：2天，又需要大量标注数据 │ └─────────────────────────────────────────────────┘ 问题：每个任务都要从头训练，重复劳动！ 微调范式（站在巨人肩膀上）： 第一阶段：预训练（Pre-training）—— 只做一次！ ┌─────────────────────────────────────────────────┐ │ 数据：整个维基百科 + 大量书籍 (数十亿词) │ │ 任务：MLM（完形填空） │ │ 目的：让模型学会\"理解语言\" │ │ 耗时：数周 (但只需做一次，由 Google 完成) │ │ 产出：BERT 预训练模型 ✨ │ └─────────────────────────────────────────────────┘ │ ▼ 下载现成的 BERT 第二阶段：微调（Fine-tuning）—— 每个任务只需几小时！ ┌─────────────────────────────────────────────────┐ │ 任务A：情感分析 │ │ 数据：仅需 1000 条标注数据！ │ │ 方法：在 BERT 上加一个分类层，微调几轮 │ │ 耗时：30分钟 │ └─────────────────────────────────────────────────┘ ┌─────────────────────────────────────────────────┐ │ 任务B：垃圾邮件检测 │ │ 数据：仅需 500 条标注数据！ │ │ 方法：同样加分类层，微调几轮 │ │ 耗时：20分钟 │ └─────────────────────────────────────────────────┘ 形象类比总结┌─────────────────────────────────────────────────────────────┐│ 🎓 教育类比 │├─────────────────────────────────────────────────────────────┤│ ││ 【预训练 Pre-training】= 上大学，接受通识教育 ││ • 学习语文、数学、英语、物理... ││ • 目标：成为一个有基础知识的\"通才\" ││ • 时间：4年 ││ • 成本：高 ││ ││ 【MLM 任务】= 大学里的各种练习题 ││ • 完形填空、阅读理解、语法练习... ││ • 目标：锻炼语言理解能力 ││ ││ 【微调 Fine-tuning】= 工作后的岗位培训 ││ • 针对具体工作（情感分析/问答/翻译）学习 ││ • 目标：成为某个领域的\"专才\" ││ • 时间：几天 ││ • 成本：低（因为已经有基础了） ││ │└─────────────────────────────────────────────────────────────┘ ┌─────────────────────────────────────────────────────────────┐│ 💡 为什么这样更好？ │├─────────────────────────────────────────────────────────────┤│ ││ 传统方式：每换一个工作就重新上一次大学 ││ → 太慢、太贵、太浪费 ││ ││ 微调范式：大学只上一次，换工作只需短期培训 ││ → 快速、便宜、高效 ││ ││ 数据对比： ││ • 传统：需要 10万+ 标注数据 ││ • 微调：只需 1000 条标注数据就能达到相似效果！ ││ │└─────────────────────────────────────────────────────────────┘ 情感分析微调完整示例from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArgumentsfrom datasets import Dataset# ============ 第一步：准备少量标注数据 ============train_data = { \"text\": [ \"这个产品太棒了，强烈推荐！\", \"质量很差，用了一天就坏了\", \"一般般，没有惊喜也没有失望\", \"超级喜欢，已经回购三次\", \"客服态度很差，再也不买了\", \"性价比很高，值得购买\", ], \"label\": [1, 0, 0, 1, 0, 1] # 1=正面, 0=负面}# ============ 第二步：加载预训练的 BERT ============# 这个 BERT 已经通过 MLM 任务学会了\"理解中文\"tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')model = BertForSequenceClassification.from_pretrained( 'bert-base-chinese', num_labels=2 # 正面/负面 两个类别)# ============ 第三步：数据预处理 ============def tokenize_function(examples): return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)dataset = Dataset.from_dict(train_data)tokenized_dataset = dataset.map(tokenize_function, batched=True)# ============ 第四步：微调训练 ============training_args = TrainingArguments( output_dir=\"./results\", num_train_epochs=3, # 只需要训练 3 轮！ per_device_train_batch_size=4, learning_rate=2e-5, # 很小的学习率，轻微调整)trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_dataset,)trainer.train() # 几分钟就完成！# ============ 第五步：使用微调后的模型 ============def predict(text): inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True) outputs = model(**inputs) prediction = outputs.logits.argmax(dim=-1).item() return \"正面 😊\" if prediction == 1 else \"负面 😞\"# 测试print(predict(\"这款手机拍照效果惊艳\")) # → 正面 😊print(predict(\"物流太慢了，等了一周\")) # → 负面 😞 1.4 论文关键创新点深度剖析🔬 创新点 1：深度双向上下文建模 模型 上下文方向 问题 GPT-1 单向 (左→右) 无法利用右侧信息 ELMo 浅层双向 (拼接) 两个方向仅在顶层融合 BERT 深度双向 ✅ 每一层都能看到完整上下文 技术实现关键：通过 MLM 任务，模型可以”作弊”地看到被预测词的两侧信息。 🔬 创新点 2：预训练-微调范式传统方法: 任务A → 从头训练模型A 任务B → 从头训练模型B (重复劳动!)BERT范式: 大规模语料 → 预训练BERT (一次) ↓ 任务A → 微调 (仅需小数据) 任务B → 微调 (仅需小数据) 任务C → 微调 (仅需小数据) 革命性影响：小公司/研究者无需大规模计算资源，只需微调即可获得SOTA性能。 🔬 创新点 3：统一的特征提取器# BERT 可以适配几乎所有 NLP 任务# 1. 单句分类 (情感分析)[CLS] 这部电影太棒了 [SEP] → CLS向量 → 分类器 → 正面# 2. 句对分类 (自然语言推理)[CLS] 天在下雨 [SEP] 地面是湿的 [SEP] → CLS向量 → 蕴含/矛盾/中性# 3. 序列标注 (命名实体识别)[CLS] 马云 创办了 阿里巴巴 [SEP] → 每个token → B-PER O O B-ORG# 4. 问答 (阅读理解)[CLS] 问题 [SEP] 文章 [SEP] → 预测答案起止位置 1.5 消融实验详解 (Ablation Study)论文通过消融实验验证了各组件的重要性： 实验 1：预训练任务的影响 配置 MNLI QNLI SST-2 BERT (MLM + NSP) 84.6 90.5 93.5 仅 MLM (无 NSP) 84.3 90.2 93.2 仅 LTR (左到右) 82.1 87.4 91.3 LTR + BiLSTM 82.8 88.1 91.6 结论： MLM 比单向 LTR 提升约 2.5% NSP 提升有限 (~0.3%)，后续 RoBERTa 移除了它 实验 2：模型规模的影响 模型 层数 隐藏维度 参数量 MNLI BERT-Base 12 768 110M 84.6 BERT-Large 24 1024 340M 86.7 结论：更大的模型 = 更好的性能 (Scaling Law 的早期验证) 实验 3：Mask 策略的影响 Mask 策略 效果 100% [MASK] 次优，预训练与微调分布不一致 80%/10%/10% (论文方案) 最优 随机比例 不稳定 1.6 训练细节与超参数预训练配置# 数据集- BooksCorpus: 800M 词 (11,038 本书)- English Wikipedia: 2,500M 词 (仅文本，去除表格/列表)# 训练配置batch_size = 256max_seq_length = 512 (前90%步用128，后10%用512)learning_rate = 1e-4warmup_steps = 10,000total_steps = 1,000,000optimizer = Adam (β1=0.9, β2=0.999)# 硬件BERT-Base: 4 TPU Pods (16 TPU chips), 4天BERT-Large: 16 TPU Pods (64 TPU chips), 4天 微调配置# 通用微调超参数batch_size = 16 或 32learning_rate = 2e-5, 3e-5, 5e-5 (选最优)epochs = 2-4dropout = 0.1# 不同任务的微调时间MRPC (3.5k样本): ~1分钟SST-2 (67k样本): ~1小时 SQuAD (100k样本): ~30分钟 1.4 MLM 与微调范式深度解析🎯 什么是 MLM（Masked Language Model）？通俗理解：让 AI 做”完形填空”游戏 小学语文题： \"小明 _____ 学校上课\" 答案：去、到、在...BERT 的 MLM 就是让 AI 做这种\"完形填空\"！ MLM 具体操作流程# 原始句子原句 = \"我爱北京天安门\"# Step 1: 随机选择 15% 的词进行处理选中 = \"北京\"# Step 2: 对选中的词进行三种处理（随机选一种）处理后 = \"我爱 [MASK] 天安门\" # 80% 概率：替换为 [MASK]或者 = \"我爱 上海 天安门\" # 10% 概率：替换为随机词或者 = \"我爱 北京 天安门\" # 10% 概率：保持不变# Step 3: 让模型预测被遮住的词是什么模型输入 = \"我爱 [MASK] 天安门\"模型输出 = \"北京\" ✅ (如果预测对了，loss 很小) = \"上海\" ❌ (如果预测错了，loss 很大) 为什么 MLM 能让 BERT 学会”理解语言”？场景 1： 输入：\"我去 [MASK] 存钱\" 模型学会：看到\"存钱\" → 预测\"银行\"（金融机构）场景 2： 输入：\"河边的 [MASK] 很陡峭\" 模型学会：看到\"河边\"\"陡峭\" → 预测\"河岸/堤坝\"通过数十亿次这样的\"完形填空\"训练后： → 模型学会了词语之间的关系 → 模型学会了语法结构 → 模型学会了常识知识 MLM 训练代码示例from transformers import BertTokenizer, BertForMaskedLMimport torch# 加载模型tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')model = BertForMaskedLM.from_pretrained('bert-base-chinese')# 输入一个带 [MASK] 的句子text = \"我爱[MASK]天安门\"inputs = tokenizer(text, return_tensors='pt')# 模型预测with torch.no_grad(): outputs = model(**inputs) predictions = outputs.logits# 找到 [MASK] 位置的预测结果mask_index = (inputs['input_ids'] == tokenizer.mask_token_id).nonzero()[0, 1]predicted_token_id = predictions[0, mask_index].argmax(dim=-1)predicted_token = tokenizer.decode(predicted_token_id)print(f\"原句: {text}\")print(f\"预测: {predicted_token}\") # 输出: 北京 🔧 什么是微调范式（Fine-tuning Paradigm）？通俗理解：培养”通才”再培养”专才” 传统方式（从零开始）： ┌─────────────────────────────────────────────────┐ │ 任务：情感分析 │ │ 数据：10万条电商评论 │ │ 训练：从随机初始化开始，训练一个专门的模型 │ │ 耗时：3天，需要大量标注数据 │ └─────────────────────────────────────────────────┘ 问题：每个任务都要从头训练，重复劳动！ 微调范式（站在巨人肩膀上）： 第一阶段：预训练（Pre-training）—— 只做一次！ ┌─────────────────────────────────────────────────┐ │ 数据：整个维基百科 + 大量书籍 (数十亿词) │ │ 任务：MLM（完形填空） │ │ 目的：让模型学会\"理解语言\" │ │ 耗时：数周 (但只需做一次，由 Google 完成) │ │ 产出：BERT 预训练模型 ✨ │ └─────────────────────────────────────────────────┘ │ ▼ 下载现成的 BERT 第二阶段：微调（Fine-tuning）—— 每个任务只需几小时！ ┌─────────────────────────────────────────────────┐ │ 任务A：情感分析 │ │ 数据：仅需 1000 条标注数据！ │ │ 方法：在 BERT 上加一个分类层，微调几轮 │ │ 耗时：30分钟 │ └─────────────────────────────────────────────────┘ 微调示例：情感分析完整代码from transformers import BertTokenizer, BertForSequenceClassification, Trainer, TrainingArgumentsfrom datasets import Dataset# ============ 第一步：准备少量标注数据 ============train_data = { \"text\": [ \"这个产品太棒了，强烈推荐！\", \"质量很差，用了一天就坏了\", \"一般般，没有惊喜也没有失望\", \"超级喜欢，已经回购三次\", \"客服态度很差，再也不买了\", \"性价比很高，值得购买\", ], \"label\": [1, 0, 0, 1, 0, 1] # 1=正面, 0=负面}# ============ 第二步：加载预训练的 BERT ============# 这个 BERT 已经通过 MLM 任务学会了\"理解中文\"tokenizer = BertTokenizer.from_pretrained('bert-base-chinese')model = BertForSequenceClassification.from_pretrained( 'bert-base-chinese', num_labels=2 # 正面/负面 两个类别)# ============ 第三步：数据预处理 ============def tokenize_function(examples): return tokenizer(examples[\"text\"], padding=\"max_length\", truncation=True, max_length=128)dataset = Dataset.from_dict(train_data)tokenized_dataset = dataset.map(tokenize_function, batched=True)# ============ 第四步：微调训练 ============training_args = TrainingArguments( output_dir=\"./results\", num_train_epochs=3, # 只需要训练 3 轮！ per_device_train_batch_size=4, learning_rate=2e-5, # 很小的学习率，轻微调整)trainer = Trainer( model=model, args=training_args, train_dataset=tokenized_dataset,)trainer.train() # 几分钟就完成！# ============ 第五步：使用微调后的模型 ============def predict(text): inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True) outputs = model(**inputs) prediction = outputs.logits.argmax(dim=-1).item() return \"正面 😊\" if prediction == 1 else \"负面 😞\"# 测试print(predict(\"这款手机拍照效果惊艳\")) # → 正面 😊print(predict(\"物流太慢了，等了一周\")) # → 负面 😞 形象类比总结┌─────────────────────────────────────────────────────────────┐│ 🎓 教育类比 │├─────────────────────────────────────────────────────────────┤│ ││ 【预训练 Pre-training】= 上大学，接受通识教育 ││ • 学习语文、数学、英语、物理... ││ • 目标：成为一个有基础知识的\"通才\" ││ • 时间：4年 ││ • 成本：高 ││ ││ 【MLM 任务】= 大学里的各种练习题 ││ • 完形填空、阅读理解、语法练习... ││ • 目标：锻炼语言理解能力 ││ ││ 【微调 Fine-tuning】= 工作后的岗位培训 ││ • 针对具体工作（情感分析/问答/翻译）学习 ││ • 目标：成为某个领域的\"专才\" ││ • 时间：几天 ││ • 成本：低（因为已经有基础了） ││ │└─────────────────────────────────────────────────────────────┘ 1.5 [MASK] Token 机制深度解析[MASK] 到底是什么？from transformers import BertTokenizertokenizer = BertTokenizer.from_pretrained('bert-base-chinese')# [MASK] 就是一个特殊的 Token IDprint(tokenizer.mask_token) # 输出: [MASK]print(tokenizer.mask_token_id) # 输出: 103 完整输入对比# 原始句子原句 = \"我爱北京天安门\"tokenizer(原句)Token: [\"[CLS]\", \"我\", \"爱\", \"北京\", \"天\", \"安\", \"门\", \"[SEP]\"]Token ID: [ 101, 2769, 4263, 1266, 1921, 2128, 7305, 102 ]# 做 MLM 时把 \"北京\" 替换成 [MASK]遮蔽句 = \"我爱[MASK]天安门\"tokenizer(遮蔽句)Token: [\"[CLS]\", \"我\", \"爱\", \"[MASK]\", \"天\", \"安\", \"门\", \"[SEP]\"]Token ID: [ 101, 2769, 4263, 103, 1921, 2128, 7305, 102 ] ↑ 就是把 1266 换成了 103！ 模型实际看到的是什么？# 模型输入的就是一串数字（Token IDs）input_ids = [101, 2769, 4263, 103, 1921, 2128, 7305, 102] ↑ 103 = [MASK] 的编号# 经过 Embedding 层后# 103 会被查表转换成一个 768 维的向量embedding_table = model.embeddings.word_embeddings.weight# 形状: [21128, 768] (词表大小 × 隐藏维度)mask_embedding = embedding_table[103] # [MASK] 的向量表示# 形状: [768]# 值: [0.023, -0.156, 0.234, ..., 0.089] ← 这是可学习的参数！ 为什么 103 初始时啥也不代表？# Embedding 层输出embeddings = [ E_cls, # [CLS] 的向量 E_我, # \"我\" 的语义向量 E_爱, # \"爱\" 的语义向量 E_mask, # 103 对应的向量 ← 这个向量本身没有语义！ E_天, # \"天\" 的语义向量 E_安, # \"安\" 的语义向量 E_门, # \"门\" 的语义向量 E_sep, # [SEP] 的向量]# E_mask 初始值可能是 [0.01, -0.02, 0.03, ...]# 它本身不代表任何词义，只是一个\"占位符\" 关键：103 本身确实啥也不代表，但经过 Self-Attention 后会吸收周围词的信息！ 1.6 BERT 模型架构详解架构参数 配置 BERT-Base BERT-Large Transformer 层数 (L) 12 24 隐藏层维度 (H) 768 1024 注意力头数 (A) 12 16 每个头的维度 (H/A) 64 64 总参数量 110M 340M FFN 中间层维度 3072 4096 输入表示 (Input Representation)Token Embedding: 每个词的向量表示 [vocab_size, 768] + Position Embedding: 位置编码 [512, 768] (最大序列长度 512) +Segment Embedding: 句子分隔 [2, 768] (Sentence A 或 B) =Input Representation: [batch, seq_len, 768] 具体例子: 输入: \"[CLS] 我 爱 NLP [SEP] 它 很 有趣 [SEP]\"Token IDs: [101, 2769, 4263, 21128, 102, 1045, 1447, 3300, 4638, 102]Segment IDs: [0, 0, 0, 0, 0, 1, 1, 1, 1, 1]Position IDs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]三个 embedding 相加后: [10, 768] 1.5 论文实验结果与影响关键性能提升GLUE 基准测试 (11 个 NLP 任务): BERT-Base: 78.6% → 提升 7% BERT-Large: 80.5% → 提升 9% SQuAD 问答任务: BERT-Large: F1 = 93.2% (超越人类水平 91.2%) 为什么效果这么好？ 双向上下文: 每个词都能看到完整句子 深度交互: 12/24 层逐层精炼表示 大规模预训练: BooksCorpus (800M 词) + Wikipedia (2500M 词) 迁移学习: 预训练 + 微调范式 🔬 Part 2: BERT 前两层完整推演输入准备：MLM 任务句子: \"我 爱 [MASK] 天安门\"Token IDs: [101, 2769, 4263, 103, 1921, 102] [CLS] 我 爱 MASK 天安门 [SEP]Embedding 后: X₀ = [6, 768]位置0 [CLS]: [0.12, -0.05, 0.33, ..., 0.45]位置1 我: [0.45, 0.23, -0.12, ..., 0.12]位置2 爱: [0.33, 0.56, 0.78, ..., 0.89]位置3 MASK: [0.01, 0.02, 0.01, ..., 0.03] ← 几乎是空的！位置4 天安门: [0.67, -0.34, 0.45, ..., 0.56]位置5 [SEP]: [0.78, 0.34, -0.12, ..., 0.12] Layer 1：第一层详细计算Step 1.1: 计算 Q、K、VX₀ = [6, 768] # 输入Q₁ = X₀ @ W_Q # [6, 768] @ [768, 768] = [6, 768]K₁ = X₀ @ W_K # [6, 768] @ [768, 768] = [6, 768]V₁ = X₀ @ W_V # [6, 768] @ [768, 768] = [6, 768]# 每个位置都有自己的 Q、K、V 向量Q₁ = [Q_cls, Q_我, Q_爱, Q_mask, Q_天安门, Q_sep]K₁ = [K_cls, K_我, K_爱, K_mask, K_天安门, K_sep]V₁ = [V_cls, V_我, V_爱, V_mask, V_天安门, V_sep] Step 1.2: 计算注意力分数scores = Q₁ @ K₁.T / sqrt(64) # [6, 768] @ [768, 6] = [6, 6]# [CLS] 我 爱 MASK 天安门 [SEP]# [CLS] [ 2.1 1.3 1.5 0.2 1.8 1.2 ]# 我 [ 1.2 3.1 2.3 0.3 1.5 0.8 ]# 爱 [ 1.4 2.5 2.8 0.4 2.1 0.9 ]# MASK [ 0.8 1.9 2.4 0.1 2.6 0.7 ] ← MASK 行# 天安门 [ 1.6 1.4 2.0 0.3 3.2 1.1 ]# [SEP] [ 1.3 0.9 1.1 0.2 1.3 2.5 ] Step 1.3: Softmax 归一化weights = softmax(scores, dim=-1) # 每行和为1# [CLS] 我 爱 MASK 天安门 [SEP]# [CLS] [0.18 0.12 0.15 0.03 0.40 0.12]# 我 [0.10 0.35 0.25 0.02 0.20 0.08]# 爱 [0.12 0.22 0.28 0.03 0.28 0.07]# MASK [0.08 0.18 0.28 0.01 0.38 0.07] ← 重点看这行！# 天安门 [0.11 0.09 0.16 0.02 0.55 0.07]# [SEP] [0.15 0.10 0.12 0.03 0.15 0.45]# MASK 位置: 28% 看\"爱\"，38% 看\"天安门\"，只有 1% 看自己！ Step 1.4: 加权求和得到新表示H₁ = weights @ V₁ # [6, 6] @ [6, 768] = [6, 768]# MASK 位置的新向量:H₁[3] = 0.08×V_cls + 0.18×V_我 + 0.28×V_爱 + 0.01×V_mask + 0.38×V_天安门 + 0.07×V_sep ↑ ↑ ↑ 几乎忽略 主要来自\"爱\" 主要来自\"天安门\"# 结果: MASK 位置现在融合了 \"爱\" 和 \"天安门\" 的信息！H₁[3] = [0.45, 0.67, 0.23, ..., 0.78] ← 不再是空壳了！ Step 1.5: Feed-Forward Network (FFN)# 输入H₁ = [6, 768] # Attention 的输出# FFN: 两层线性变换step1 = H₁ @ W₁ # [6, 768] @ [768, 3072] = [6, 3072] 先扩大4倍step2 = ReLU(step1) # [6, 3072] 激活函数step3 = step2 @ W₂ # [6, 3072] @ [3072, 768] = [6, 768] 再压回去FFN_out = step3 # [6, 768]# 维度变化图示768 ──扩大──→ 3072 ──压缩──→ 768 W₁ W₂ Step 1.6: 残差连接 + LayerNorm# 残差连接：把输入加回来output = H₁ + FFN_out # [6, 768] + [6, 768] = [6, 768]# LayerNorm 归一化X₁ = LayerNorm(output) # [6, 768] 残差连接示意图： H₁ ─────────────────────────┐ │ │ ↓ │ (跳跃连接)┌─────────┐ ││ FFN │ │└─────────┘ │ │ │ ↓ ↓FFN_out ────────────→ (+) 相加 ──→ LayerNorm ──→ X₁ ↑ 残差 = H₁ + FFN_out 为什么需要残差连接？ 问题 残差解决方案 梯度消失 梯度可以直接通过”跳跃连接”回传 信息丢失 原始信息 H₁ 被保留，不会完全被覆盖 训练困难 网络只需学习”差异”，更容易优化 # 本质：FFN 只学习\"增量\"X₁ = H₁ + FFN(H₁) = H₁ + ΔH # 原始 + 修正量 Layer 1 输出： X₁ = [6, 768]位置3 (MASK): [0.52, 0.71, 0.34, ..., 0.82] ↑ 已经包含了 \"爱___天安门\" 的模式信息 Layer 2：第二层详细计算Step 2.1: 计算新的 Q、K、V# 输入是 Layer 1 的输出X₁ = [6, 768]Q₂ = X₁ @ W_Q # 新的 Q（权重矩阵和 Layer1 不同！）K₂ = X₁ @ W_K # 新的 KV₂ = X₁ @ W_V # 新的 V Step 2.2: 计算注意力（基于更新后的表示）scores = Q₂ @ K₂.T / sqrt(64)# 现在 MASK 位置已经有了上下文信息# 它的 Q 向量更\"聪明\"了，能找到更相关的词# [CLS] 我 爱 MASK 天安门 [SEP]# MASK [ 0.5 1.5 2.8 0.2 3.5 0.4 ]# ↑ ↑# 更关注\"爱\" 更关注\"天安门\"weights[3] = softmax([0.5, 1.5, 2.8, 0.2, 3.5, 0.4]) = [0.04, 0.10, 0.28, 0.02, 0.52, 0.04]# ↑ ↑# 28%看爱 52%看天安门 Step 2.3: 加权求和H₂[3] = 0.04×V_cls + 0.10×V_我 + 0.28×V_爱 + 0.02×V_mask + 0.52×V_天安门 + 0.04×V_sep# 这次 V_天安门 已经不是原始的了# 它在 Layer1 中也融合了上下文，知道\"天安门在北京\"# 所以 MASK 间接获得了\"北京\"的信息！H₂[3] = [0.68, 0.82, 0.45, ..., 0.91] Step 2.4: FFN + 残差 + LayerNormX₂ = LayerNorm(H₂ + FFN(H₂)) # [6, 768] Layer 2 输出： X₂ = [6, 768]位置3 (MASK): [0.71, 0.85, 0.52, ..., 0.93] ↑ 现在知道: \"我爱___天安门\" → 这个空应该填地名 天安门相关 → 可能是\"北京\" 两层对比总结┌────────────────────────────────────────────────────────────┐│ MASK 位置向量的变化 │├────────────────────────────────────────────────────────────┤│ ││ Embedding: [0.01, 0.02, 0.01, ...] ← 空壳，无语义 ││ ↓ ││ Layer 1: [0.52, 0.71, 0.34, ...] ← 融合了\"爱\"\"天安门\" ││ ↓ ││ Layer 2: [0.71, 0.85, 0.52, ...] ← 更深层理解 ││ ↓ ││ ... (继续 10 层) ││ ↓ ││ Layer 12: [0.93, 0.87, 0.76, ...] ← 确定是\"北京\" ││ │└────────────────────────────────────────────────────────────┘ 层 MASK 学到了什么 Embedding 空的占位符，103只是个ID Layer 1 “爱”后面是宾语，”天安门”是地标 Layer 2 “我爱X天安门”是固定搭配，X是地名 Layer 3-6 天安门在北京，这是常识 Layer 7-12 确定答案是”北京”，排除其他可能 🔍 Part 3: Self-Attention 核心机制2.0 输入准备句子: \"我 爱 [MASK] 天安门\"Token IDs: [101, 2769, 4263, 103, 1921, 102] [CLS] 我 爱 MASK 天安门 [SEP]Embedding 后: X₀ = [6, 768]位置0 [CLS]: [0.12, -0.05, 0.33, ..., 0.45] ← 768维向量位置1 我: [0.45, 0.23, -0.12, ..., 0.12]位置2 爱: [0.33, 0.56, 0.78, ..., 0.89]位置3 MASK: [0.01, 0.02, 0.01, ..., 0.03] ← 几乎是空的！位置4 天安门: [0.67, -0.34, 0.45, ..., 0.56]位置5 [SEP]: [0.78, 0.34, -0.12, ..., 0.12] 2.1 Layer 1：第一层详细计算Step 1.1: 计算 Q、K、VX₀ = [6, 768] # 输入Q₁ = X₀ @ W_Q # [6, 768] @ [768, 768] = [6, 768]K₁ = X₀ @ W_K # [6, 768] @ [768, 768] = [6, 768]V₁ = X₀ @ W_V # [6, 768] @ [768, 768] = [6, 768]# 每个位置都有自己的 Q、K、V 向量Q₁ = [Q_cls, Q_我, Q_爱, Q_mask, Q_天安门, Q_sep]K₁ = [K_cls, K_我, K_爱, K_mask, K_天安门, K_sep]V₁ = [V_cls, V_我, V_爱, V_mask, V_天安门, V_sep] Step 1.2: 计算注意力分数scores = Q₁ @ K₁.T / sqrt(64) # [6, 768] @ [768, 6] = [6, 6]# 注意力矩阵（未归一化）# [CLS] 我 爱 MASK 天安门 [SEP]# [CLS] [ 2.1 1.3 1.5 0.2 1.8 1.2 ]# 我 [ 1.2 3.1 2.3 0.3 1.5 0.8 ]# 爱 [ 1.4 2.5 2.8 0.4 2.1 0.9 ]# MASK [ 0.8 1.9 2.4 0.1 2.6 0.7 ] ← MASK 行# 天安门 [ 1.6 1.4 2.0 0.3 3.2 1.1 ]# [SEP] [ 1.3 0.9 1.1 0.2 1.3 2.5 ] Step 1.3: Softmax 归一化weights = softmax(scores, dim=-1) # 每行和为1# 注意力权重矩阵# [CLS] 我 爱 MASK 天安门 [SEP]# [CLS] [0.18 0.12 0.15 0.03 0.40 0.12]# 我 [0.10 0.35 0.25 0.02 0.20 0.08]# 爱 [0.12 0.22 0.28 0.03 0.28 0.07]# MASK [0.08 0.18 0.28 0.01 0.38 0.07] ← 重点看这行！# 天安门 [0.11 0.09 0.16 0.02 0.55 0.07]# [SEP] [0.15 0.10 0.12 0.03 0.15 0.45]# MASK 位置解读:# - 28% 的注意力给 \"爱\"# - 38% 的注意力给 \"天安门\"# - 只有 1% 看自己（因为自己是空壳） Step 1.4: 加权求和得到新表示H₁ = weights @ V₁ # [6, 6] @ [6, 768] = [6, 768]# MASK 位置的新向量计算:H₁[3] = 0.08×V_cls + 0.18×V_我 + 0.28×V_爱 + 0.01×V_mask + 0.38×V_天安门 + 0.07×V_sep# 具体数值示例（假设简化到4维）:V_cls = [0.1, 0.2, 0.1, 0.3]V_我 = [0.8, 0.1, 0.2, 0.4]V_爱 = [0.3, 0.7, 0.5, 0.2]V_mask = [0.0, 0.0, 0.0, 0.1] # 几乎为空V_天安门 = [0.2, 0.4, 0.8, 0.6]V_sep = [0.1, 0.1, 0.2, 0.2]H₁[3] = 0.08×[0.1,0.2,0.1,0.3] + 0.18×[0.8,0.1,0.2,0.4] + 0.28×[0.3,0.7,0.5,0.2] + 0.01×[0.0,0.0,0.0,0.1] + 0.38×[0.2,0.4,0.8,0.6] + 0.07×[0.1,0.1,0.2,0.2] = [0.008+0.144+0.084+0+0.076+0.007, 0.016+0.018+0.196+0+0.152+0.007, 0.008+0.036+0.14+0+0.304+0.014, 0.024+0.072+0.056+0.001+0.228+0.014] = [0.319, 0.389, 0.502, 0.395]# 结果: MASK 位置现在融合了 \"爱\" 和 \"天安门\" 的信息！# 不再是空壳 [0,0,0,0.1] 了！ Step 1.5: Feed-Forward Network (FFN)# FFN: 768 → 3072 → 768FFN_input = H₁ # [6, 768]# 第一层：升维 + 激活hidden = FFN_input @ W₁ # [6, 768] @ [768, 3072] = [6, 3072]hidden = ReLU(hidden) # 负数变0，正数不变# 第二层：降维FFN_out = hidden @ W₂ # [6, 3072] @ [3072, 768] = [6, 768]# MASK 位置示例FFN_out[3] = [0.07, 0.12, 0.09, ..., 0.15] 为什么要 FFN？ Attention 只做线性组合 FFN 引入非线性，让模型学习更复杂的模式 Step 1.6: 残差连接 + LayerNorm# 残差连接：把输入加回来residual = H₁ + FFN_out # [6, 768] + [6, 768] = [6, 768]# MASK 位置residual[3] = H₁[3] + FFN_out[3] = [0.319, 0.389, 0.502, 0.395] + [0.07, 0.12, 0.09, 0.15] = [0.389, 0.509, 0.592, 0.545]# LayerNorm: 归一化X₁ = LayerNorm(residual) # [6, 768] 残差连接示意图： H₁ ─────────────────────────┐ │ │ ↓ │ (跳跃连接，防止信息丢失)┌─────────┐ ││ FFN │ │└─────────┘ │ │ │ ↓ ↓FFN_out ────────────→ (+) 相加 ──→ LayerNorm ──→ X₁ Layer 1 输出： X₁ = [6, 768]位置3 (MASK): [0.52, 0.71, 0.34, ..., 0.82] ↑ 已经包含了 \"爱___天安门\" 的模式信息 2.2 Layer 2：第二层详细计算Step 2.1: 计算新的 Q、K、V# 输入是 Layer 1 的输出（已经融合过一次上下文）X₁ = [6, 768]Q₂ = X₁ @ W_Q # 新的权重矩阵！与 Layer1 不同K₂ = X₁ @ W_KV₂ = X₁ @ W_V# 现在 MASK 位置的 Q 已经不是空壳了Q_mask_new = X₁[3] @ W_Q # 基于融合后的向量计算 Step 2.2: 计算注意力（基于更新后的表示）scores = Q₂ @ K₂.T / sqrt(64)# 现在 MASK 位置已经有了上下文信息# 它的 Q 向量更\"聪明\"了，能找到更相关的词# [CLS] 我 爱 MASK 天安门 [SEP]# MASK [ 0.5 1.5 2.8 0.2 3.5 0.4 ]# ↑ ↑# 更关注\"爱\" 更关注\"天安门\"weights[3] = softmax([0.5, 1.5, 2.8, 0.2, 3.5, 0.4]) = [0.04, 0.10, 0.28, 0.02, 0.52, 0.04]# ↑ ↑# 28%看爱 52%看天安门（权重更高了！） Step 2.3: 加权求和H₂[3] = 0.04×V_cls + 0.10×V_我 + 0.28×V_爱 + 0.02×V_mask + 0.52×V_天安门 + 0.04×V_sep# 关键：这次的 V_天安门 已经不是原始的了# 它在 Layer1 中也融合了上下文，知道\"天安门在北京\"# 所以 MASK 间接获得了\"北京\"的信息！H₂[3] = [0.68, 0.82, 0.45, ..., 0.91] Step 2.4: FFN + 残差 + LayerNormFFN_out = FFN(H₂)X₂ = LayerNorm(H₂ + FFN_out) # [6, 768] Layer 2 输出： X₂ = [6, 768]位置3 (MASK): [0.71, 0.85, 0.52, ..., 0.93] ↑ 现在知道: \"我爱___天安门\" → 这个空应该填地名 天安门相关 → 可能是\"北京\" 2.3 两层对比总结┌────────────────────────────────────────────────────────────┐│ MASK 位置向量的演化 │├────────────────────────────────────────────────────────────┤│ ││ Embedding: [0.01, 0.02, 0.01, ...] ← 空壳，无语义 ││ ↓ Layer 1 Self-Attention ││ Layer 1: [0.52, 0.71, 0.34, ...] ← 融合了\"爱\"\"天安门\" ││ ↓ Layer 2 Self-Attention ││ Layer 2: [0.71, 0.85, 0.52, ...] ← 更深层理解 ││ ↓ ││ ... (继续 10 层) ││ ↓ ││ Layer 12: [0.93, 0.87, 0.76, ...] ← 确定是\"北京\" ││ │└────────────────────────────────────────────────────────────┘ 层 MASK 学到了什么 注意力权重变化 Embedding 空的占位符 - Layer 1 “爱”后面是宾语，”天安门”是地标 28%看”爱”，38%看”天安门” Layer 2 “我爱X天安门”是固定搭配，X是地名 28%看”爱”，52%看”天安门” Layer 3-6 天安门在北京，这是常识 逐渐聚焦到相关词 Layer 7-12 确定答案是”北京”，排除其他可能 高度确信 🔍 Part 3: Self-Attention 核心机制3.1 Q/K/V 的本质：信息检索系统直觉类比场景: 在图书馆找书Q (Query): \"我想找关于深度学习的书\" ↓ 计算相似度K (Key): 每本书的标签 [\"机器学习\", \"烹饪\", \"历史\", ...] ↓ Softmax 归一化Attention: [0.7, 0.05, 0.05, ...] (对\"机器学习\"书的注意力最高) ↓ 加权求和V (Value): 书的实际内容 ↓Output: 根据注意力权重，融合最相关书籍的知识 数学定义$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$ 2.2 完整数据流与维度变化假设: batch=2, seq=4, d_model=768, heads=12, d_k=64 Step 1: 线性投影X: [2, 4, 768] # 输入Q = X @ W_Q # [2, 4, 768] @ [768, 768] = [2, 4, 768]K = X @ W_K # [2, 4, 768]V = X @ W_V # [2, 4, 768] Step 2: 拆分成多头Q: [2, 4, 768] → reshape → [2, 4, 12, 64] → transpose → [2, 12, 4, 64]K: [2, 12, 4, 64]V: [2, 12, 4, 64] Step 3: 计算注意力分数Scores = Q @ K^T = [2, 12, 4, 64] @ [2, 12, 64, 4] = [2, 12, 4, 4] ← 这就是注意力矩阵！# 每个 token 对其他 token 的关注程度例如 Scores[0, 0, :, :] = I love NLP ! I [9.2 1.3 2.1 0.8] love [2.4 8.7 3.2 1.1] NLP [1.8 3.5 9.1 0.9] ! [0.5 1.2 0.7 8.9] Step 4: Scale 和 SoftmaxScores = Scores / sqrt(64) ≈ Scores / 8Weights = softmax(Scores, dim=-1) # [2, 12, 4, 4]# 每行和为 1例如 Weights[0, 0, 1, :] = [0.15, 0.52, 0.28, 0.05]表示 \"love\" 对 [\"I\", \"love\", \"NLP\", \"!\"] 的注意力分布 Step 5: 加权求和Output = Weights @ V = [2, 12, 4, 4] @ [2, 12, 4, 64] = [2, 12, 4, 64] Step 6: 合并多头Output: [2, 12, 4, 64] → transpose → [2, 4, 12, 64] → reshape → [2, 4, 768] 关键: 输入和输出维度完全相同！[2, 4, 768] 2.3 为什么需要多头注意力？单头的局限单头: 只有一组 Q/K/V 只能学习一种\"查询-匹配\"模式 多头的优势12 个头 = 12 种不同的注意力模式Head 1: 关注语法关系 (主谓宾)Head 2: 关注语义相似Head 3: 关注位置邻近...Head 12: 关注长距离依赖最终融合 12 个头的信息 → 更丰富的表示 ⚡ Part 4: KV Cache 深度解析4.1 BERT 有 Q/K/V 吗？有 KV Cache 吗？ 问题 答案 原因 BERT 有 Q、K、V 吗？ ✅ 有 每层 Self-Attention 都要计算 BERT 有 KV Cache 吗？ ❌ 没有 一次性处理，不需要缓存 GPT 有 KV Cache 吗？ ✅ 必须有 逐个生成，必须缓存历史 4.2 BERT 的 Q/K/V 计算# BERT 每一层都计算 Q/K/V# 输入: \"我 爱 [MASK] 天安门\"# X = [5, 768] (5个token，每个768维)# 每层都要计算 Q、K、VQ = X @ W_Q # [5, 768] @ [768, 768] = [5, 768]K = X @ W_K # [5, 768] @ [768, 768] = [5, 768]V = X @ W_V # [5, 768] @ [768, 768] = [5, 768]# 计算注意力scores = Q @ K.T # [5, 768] @ [768, 5] = [5, 5]weights = softmax(scores) # [5, 5]output = weights @ V # [5, 5] @ [5, 768] = [5, 768] 注意力矩阵示例： 我 爱 [MASK] 天安门我 [0.3 0.2 0.2 0.3 ]爱 [0.2 0.3 0.2 0.3 ] ← 每个词看所有词[MASK] [0.15 0.25 0.05 0.55] ← 双向注意力！天安门 [0.2 0.2 0.2 0.4 ] 4.3 为什么 BERT 不需要 KV Cache？关键区别：一次性处理 vs 逐个生成┌─────────────────────────────────────────────────────────────┐│ BERT（Encoder - 理解任务）：一次性处理整个句子 │├─────────────────────────────────────────────────────────────┤│ ││ 输入: \"我 爱 [MASK] 天安门\" （一次性全部输入） ││ ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ ││ 同时计算所有位置的 Q、K、V ││ ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓ ││ 输出: 同时得到所有位置的表示 ││ ││ ✅ 只需要 1 次前向传播 ││ ❌ 不需要缓存任何东西！ │└─────────────────────────────────────────────────────────────┘┌─────────────────────────────────────────────────────────────┐│ GPT（Decoder - 生成任务）：逐个 token 生成 │├─────────────────────────────────────────────────────────────┤│ ││ Step 1: 输入 \"我\" ││ 计算 K₁, V₁ → 💾 需要保存！ ││ 输出 \"爱\" ││ ││ Step 2: 输入 \"爱\" ││ 计算 K₂, V₂ → 💾 需要保存！ ││ 需要用到之前的 K₁, V₁ ││ 输出 \"北京\" ││ ││ Step 3: 输入 \"北京\" ││ 计算 K₃, V₃ → 💾 需要保存！ ││ 需要用到之前的 K₁, K₂, V₁, V₂ ││ 输出 \"天安门\" ││ ││ ✅ 每一步都需要之前所有的 K、V → 必须缓存！ ││ 这就是 KV Cache 的作用！ │└─────────────────────────────────────────────────────────────┘ 4.4 代码对比：BERT vs GPTBERT：无需缓存# BERT: 一次搞定，不需要缓存def bert_forward(input_ids): # input_ids = [101, 2769, 4263, 103, 1921, 102] 一次性输入 X = embedding(input_ids) # [6, 768] for layer in encoder_layers: Q = X @ W_Q # 同时算所有位置 K = X @ W_K # 同时算所有位置 V = X @ W_V # 同时算所有位置 scores = Q @ K.T / sqrt(d_k) weights = softmax(scores) X = weights @ V # 一次完成 return X # 直接返回，不需要保存任何中间结果 GPT：必须缓存# GPT: 逐个生成，必须缓存 K、Vdef gpt_generate(prompt): kv_cache = [] # 必须有这个！ for step in range(max_tokens): if step == 0: X = embedding(prompt) # 第一步处理整个 prompt else: X = embedding(new_token) # 之后每步只处理新 token for layer_idx, layer in enumerate(decoder_layers): Q = X @ W_Q # 只算当前 token 的 Q K_new = X @ W_K V_new = X @ W_V # 拼接历史的 K、V if step 0: K = concat(kv_cache[layer_idx][0], K_new) # 拼接！ V = concat(kv_cache[layer_idx][1], V_new) # 拼接！ else: K, V = K_new, V_new # 更新缓存 if layer_idx = len(kv_cache): kv_cache.append((K, V)) else: kv_cache[layer_idx] = (K, V) # 保存！ scores = Q @ K.T / sqrt(d_k) # Causal Mask: 只能看左边 mask = causal_mask(Q.size(1), K.size(1)) scores = scores.masked_fill(mask, -inf) weights = softmax(scores) X = weights @ V new_token = predict_next(X) return generated_text 4.5 为什么 GPT 必须缓存 K、V？生成 \"我爱北京天安门\" 的过程：Step 1: 输入 \"我\" Q₁ @ K₁.T → 只有自己看自己 输出: \"爱\" 💾 缓存: K₁, V₁Step 2: 输入 \"爱\" 需要计算: Q₂ @ [K₁, K₂].T ← 需要之前的 K₁！ 如果不缓存 K₁，就要重新算 → 浪费！ 输出: \"北京\" 💾 缓存: K₁, K₂, V₁, V₂ Step 3: 输入 \"北京\" 需要计算: Q₃ @ [K₁, K₂, K₃].T ← 需要 K₁, K₂！ 如果不缓存，要重新算 K₁, K₂ → 更浪费！ 输出: \"天安门\" 💾 缓存: K₁, K₂, K₃, V₁, V₂, V₃Step N: 需要计算: Qₙ @ [K₁, K₂, ..., Kₙ].T 不缓存: 计算量 = 1+2+3+...+N = O(N²)缓存: 计算量 = 1+1+1+...+1 = O(N)加速比: N 倍！ 为什么要加到当前词上？ # Step 2 生成 \"爱\" 时Q_爱 @ [K_我, K_爱].T # \"爱\" 需要看到 \"我\" ↑ 必须包含之前的 K_我！# Step 3 生成 \"北京\" 时 Q_北京 @ [K_我, K_爱, K_北京].T # \"北京\" 需要看到 \"我\" 和 \"爱\" ↑ 必须包含所有历史 K！# 这就是为什么要把前面的 K、V 加到当前的计算中 4.6 架构总结对比┌─────────────────────────────────────────────────────────────┐│ BERT vs GPT │├──────────────────────────┬──────────────────────────────────┤│ BERT │ GPT │├──────────────────────────┼──────────────────────────────────┤│ Encoder-only 架构 │ Decoder-only 架构 ││ 双向注意力 │ 单向注意力（Causal Mask） ││ 一次性输入整个句子 │ 逐个 token 生成 ││ 输入输出长度相同 │ 输出比输入长 ││ 并行计算所有位置 │ 串行生成 │├──────────────────────────┼──────────────────────────────────┤│ ✅ 有 Q、K、V │ ✅ 有 Q、K、V ││ ❌ 不需要 KV Cache │ ✅ 必须 KV Cache │├──────────────────────────┼──────────────────────────────────┤│ 用途: 理解 │ 用途: 生成 ││ 任务: 分类、NER、问答 │ 任务: 聊天、写作、代码生成 ││ 例子: \"这是___评论\"→正面 │ 例子: \"从前有\" → \"座山\" │└──────────────────────────┴──────────────────────────────────┘ 4.7 混合架构：T5/BARTT5/BART = Encoder + Decoder┌─────────────────────────────────────────────────────────┐│ Encoder 部分（类似 BERT） ││ 输入: \"Translate to English: 我爱你\" ││ 处理: 一次性编码 ││ ❌ 不需要 KV Cache │└─────────────────────────────────────────────────────────┘ ↓ 传递编码结果┌─────────────────────────────────────────────────────────┐│ Decoder 部分（类似 GPT） ││ 生成: \"I\" → \"love\" → \"you\" ││ 处理: 逐个生成 ││ ✅ 需要 KV Cache（仅 Decoder 部分） │└─────────────────────────────────────────────────────────┘ 4.8 记忆口诀Encoder (BERT) = 阅读理解 = 一眼看完全文 = 不需要缓存Decoder (GPT) = 写作文 = 一字一字写 = 需要缓存历史判断标准: - 输入输出同时存在？ → Encoder → 无 KV Cache - 逐步生成新内容？ → Decoder → 有 KV Cache ⚡ Part 5: KV Cache 深度解析3.1 问题场景：自回归生成GPT 生成文本 “I love NLP so much”： 无 KV Cache (低效): Step 1: 输入 \"I\" 计算 Q₁, K₁, V₁ → 输出 \"love\" Step 2: 输入 \"I love\" 重新计算 Q₁, K₁, V₁ ← 浪费！ 重新计算 Q₂, K₂, V₂ ← 浪费！ → 输出 \"NLP\" Step 3: 输入 \"I love NLP\" 重新计算 Q₁, K₁, V₁ ← 浪费！ 重新计算 Q₂, K₂, V₂ ← 浪费！ 重新计算 Q₃, K₃, V₃ ← 浪费！ → 输出 \"so\" 计算量: 1 + 2 + 3 + … + n = O(n²) 3.2 KV Cache 解决方案核心洞察: K 和 V 只依赖输入，与”当前要生成什么”无关 → 可以缓存！ Step 1: 输入 \"I\" 计算 K₁, V₁ → 存入 Cache 计算 Q₁ → Attention(Q₁, [K₁], [V₁]) → 输出 \"love\" Step 2: 输入 \"love\" 计算 K₂, V₂ → 追加到 Cache 只计算 Q₂ → Attention(Q₂, [K₁,K₂], [V₁,V₂]) → 输出 \"NLP\" Step 3: 输入 \"NLP\" 计算 K₃, V₃ → 追加到 Cache 只计算 Q₃ → Attention(Q₃, [K₁,K₂,K₃], [V₁,V₂,V₃]) → 输出 \"so\" 计算量: 1 + 1 + 1 + … + 1 = O(n) 加速比: n²/n = n 倍加速！ 3.3 内存消耗分析公式$$\\text{KV Cache Size} = 2 \\times n_{\\text{layers}} \\times d_{\\text{model}} \\times \\text{seq_len} \\times \\text{dtype_size}$$ 实际案例：LLaMA-7B n_layers = 32 d_model = 4096 dtype = float16 (2 bytes) 每个 token 的 KV Cache:= 2 × 32 × 4096 × 2 bytes= 524,288 bytes= 512 KB / token不同序列长度的显存占用:- 1K tokens: 512 MB- 4K tokens: 2 GB- 32K tokens: 16 GB ← 长上下文的挑战！- 128K tokens: 64 GB ← 需要多卡或优化技术 3.4 KV Cache 优化技术1. Multi-Query Attention (MQA)标准 Multi-Head: 每个头都有独立的 K, VMQA: 所有头共享同一组 K, V显存节省: heads 倍 (例如 12 头 → 节省 12 倍) 2. Grouped-Query Attention (GQA)折中方案: 将 12 个头分成 3 组，每组共享 K, VLLaMA-2 使用 GQA:- 32 个头 → 8 组- 显存节省: 4 倍 3. PagedAttention (vLLM)类似操作系统的分页机制将 KV Cache 分成固定大小的块 (Page)动态分配和回收，减少碎片 🎭 Part 4: Causal Attention vs Bidirectional Attention4.1 核心区别 维度 Bidirectional (BERT) Causal (GPT) 可见范围 全局可见 仅左侧可见 Mask 形状 全 1 (或仅 mask padding) 下三角矩阵 适用任务 理解 (分类、NER、QA) 生成 (文本、对话) 训练效率 高 (并行计算所有位置) 高 (teacher forcing) 推理模式 一次性输出 逐 token 生成 4.2 Causal Mask 的数学实现Mask 矩阵seq_len = 4mask = np.tril(np.ones((4, 4))) I love NLP ! I [1 0 0 0] love [1 1 0 0] NLP [1 1 1 0] ! [1 1 1 1]解释:- \"I\" 只能看到自己- \"love\" 能看到 \"I\" 和自己- \"NLP\" 能看到 \"I\", \"love\", \"NLP\"- \"!\" 能看到所有 应用到 Attention Scoresscores = Q @ K.T / sqrt(d_k) # [4, 4]# 将上三角设为 -infmasked_scores = np.where(mask == 1, scores, -np.inf) I love NLP ! I [2.1 -inf -inf -inf] love [1.3 3.2 -inf -inf] NLP [0.8 1.9 2.7 -inf] ! [0.5 1.1 0.9 3.5]# Softmax 后, -inf 位置变成 0weights = softmax(masked_scores) I love NLP ! I [1.0 0.0 0.0 0.0] ← 只看自己 love [0.3 0.7 0.0 0.0] ← 70% 看自己, 30% 看 I NLP [0.1 0.3 0.6 0.0] ← 主要看自己和 love ! [0.05 0.2 0.15 0.6] ← 60% 看自己 🔄 Part 5: 训练闭环 - Label / Loss / 梯度流5.1 MLM 任务的完整训练流程数据准备原始文本: \"我爱北京天安门\"处理后: \"我爱[MASK]天安门\"input_ids = [101, 2769, 4263, 103, 1921, 2128, 7305, 102] [CLS] 我 爱 MASK 天 安 门 [SEP]labels = [-100, -100, -100, 1266, -100, -100, -100, -100] ↑ \"北京\" 的 ID Forward Pass# 1. Embeddingembeddings = token_emb + pos_emb + seg_emb # [1, 8, 768]# 2. 12 层 Transformerhidden = embeddingsfor layer in encoder_layers: hidden = layer(hidden) # [1, 8, 768]# 3. MLM Headlogits = mlm_head(hidden) # [1, 8, 21128]# 只取 [MASK] 位置 (index=3)masked_logits = logits[0, 3, :] # [21128] Loss 计算true_label = 1266 # \"北京\"loss = CrossEntropyLoss(masked_logits, true_label) = -log(softmax(masked_logits)[1266]) 如果模型预测: P(\"北京\") = 0.8 → loss = -log(0.8) = 0.22 (好) P(\"北京\") = 0.1 → loss = -log(0.1) = 2.30 (差) 5.2 梯度反向传播梯度流动路径 Loss (标量) ↓ ∂L/∂logits MLM Head (线性层) ↓ ∂L/∂h₁₂Transformer Layer 12 ↓ ... ↓Transformer Layer 1 ↙ ↓ ↘∂L/∂Q ∂L/∂K ∂L/∂V ↓ ↓ ↓W_Q W_K W_V ← 这些权重被更新！ Attention 中的梯度分叉# Forwardattn_weights = softmax(Q @ K.T / sqrt(d_k)) # [seq, seq]output = attn_weights @ V # [seq, dim]# Backward∂L/∂V = attn_weights.T @ ∂L/∂output ← V 的梯度∂L/∂attn = ∂L/∂output @ V.T ← 注意力权重的梯度∂L/∂scores = ∂softmax(∂L/∂attn) ← Softmax 反向∂L/∂Q = ∂L/∂scores @ K ← Q 的梯度∂L/∂K = ∂L/∂scores.T @ Q ← K 的梯度 5.3 为什么 Attention 能学到语义？梯度的”指导作用”假设当前预测: [MASK] 位置预测 \"上海\" (错误, 应该是 \"北京\") Loss 很大 → 梯度回传: 1. 流向 V: \"你们提供的内容不对！'上海' 的语义特征太强了\" → 调整 V，让 \"北京\" 相关的 token 提供更多信息 2. 流向 Q 和 K: \"'爱' 和 '天安门' 的注意力权重不对！\" → 调整 Q/K，让 [MASK] 更多关注 \"天安门\" → 因为 \"北京 + 天安门\" 共现频率高 3. 多轮迭代后: 模型学会: \"看到'天安门' → 联想到'北京'\" 📚 总结：核心要点回顾BERT 论文核心贡献 Masked Language Model: 实现深度双向建模 大规模预训练 + 微调: 开创预训练范式 SOTA 性能: 在 11 个任务上刷新记录 Q/K/V 机制本质 Q: 提问 “我想找什么信息？” K: 索引 “我这里有什么信息？” V: 内容 “实际的信息是什么？” Attention: 根据 Q-K 相似度，加权聚合 V KV Cache 优化 问题: 自回归生成时重复计算 K/V → O(n²) 方案: 缓存历史 K/V → O(n) 代价: 显存占用 (LLaMA-7B: 512KB/token) Causal vs Bidirectional Causal: 下三角 mask, 用于生成 Bidirectional: 全局可见, 用于理解 训练闭环 Label → Loss → Gradient → Update 梯度流过 Attention 时分叉到 Q/K/V 通过反向传播，模型学会”在哪里找信息”和”找什么信息” 🆚 Part 8: Encoder vs Decoder - KV Cache 对比核心结论 模型 架构 有 KV Cache？ 原因 BERT Encoder-only ❌ 没有 一次性处理整个输入 GPT Decoder-only ✅ 有 逐个 token 生成 T5/BART Encoder + Decoder ⚠️ Decoder 部分有 Encoder 不需要，Decoder 需要 BERT (Encoder-only) - 不需要 KV Cache# 输入完整句子input = \"我 爱 [MASK] 天安门\"# 一次性全部处理output = bert(input) # 同时计算所有位置的 Q、K、V# 流程Step 1: 输入全部 token [6个]Step 2: 同时计算所有位置的 K、VStep 3: 同时计算所有位置的 AttentionStep 4: 同时输出所有位置的结果# 计算量: O(n) - 只计算一次# 不需要 KV Cache！ GPT (Decoder-only) - 必须有 KV Cache# 逐个生成Step 1: input=\"我\" 计算 K₁, V₁ → 缓存 生成 \"爱\"Step 2: input=\"爱\" 计算 K₂, V₂ → 缓存 使用 K₁, V₁ + K₂, V₂ 生成 \"北京\"Step 3: input=\"北京\" 计算 K₃, V₃ → 缓存 使用 K₁, V₁ + K₂, V₂ + K₃, V₃ 生成 \"天安门\"# 不用缓存: 计算量 = 1+2+3+...+N = O(N²)# 用缓存: 计算量 = 1+1+1+...+1 = O(N)# 必须有 KV Cache！ 为什么 Decoder 必须缓存 K、V？生成 \"我爱北京天安门\" 的过程：Step 1: 输入 \"我\" Q₁ @ K₁.T → 只有自己看自己 输出: \"爱\"Step 2: 输入 \"爱\" 需要计算: Q₂ @ [K₁, K₂].T ← 需要之前的 K₁！ 如果不缓存，就要重新计算 K₁ → 浪费！ Step 3: 输入 \"北京\" 需要计算: Q₃ @ [K₁, K₂, K₃].T ← 需要 K₁, K₂！ 如果不缓存，要重新计算 K₁, K₂ → 更浪费！Step N: 需要计算: Qₙ @ [K₁, K₂, ..., Kₙ].T 不缓存的话，计算量 = 1+2+3+...+N = O(N²) 缓存的话，每步只算新的，计算量 = O(N) BERT 有 Q/K/V，但为什么不需要缓存？# BERT: 一次搞定，不需要缓存def bert_forward(input_ids): # input_ids = [101, 2769, 4263, 103, 1921, 102] 一次性输入 X = embedding(input_ids) # [6, 768] for layer in encoder_layers: Q = X @ W_Q # 同时算所有位置的 Q K = X @ W_K # 同时算所有位置的 K V = X @ W_V # 同时算所有位置的 V # 所有位置的 Attention 同时计算 scores = Q @ K.T # [6, 6] weights = softmax(scores) X = weights @ V # [6, 768] return X # 直接返回，不需要保存任何中间结果 # GPT: 逐个生成，必须缓存 K、Vdef gpt_generate(prompt): kv_cache = [] # 必须有这个！ for step in range(max_tokens): if step == 0: X = embedding(prompt) # 第一步处理整个 prompt else: X = embedding(new_token) # 之后每步只处理新 token for layer_idx, layer in enumerate(decoder_layers): Q = X @ W_Q # 只算当前 token 的 Q K_new = X @ W_K V_new = X @ W_V # 拼接历史的 K、V if kv_cache[layer_idx] is not None: K = concat(kv_cache[layer_idx][0], K_new) # 拼接！ V = concat(kv_cache[layer_idx][1], V_new) # 拼接！ else: K = K_new V = V_new # 更新缓存 kv_cache[layer_idx] = (K, V) # 保存！ # Causal Attention (只看左边) scores = Q @ K.T # [1, seq_len] weights = softmax(scores) X = weights @ V new_token = predict_next(X) return generated_text 一图总结┌─────────────────────────────────────────────────────────────┐│ BERT vs GPT │├──────────────────────────┬──────────────────────────────────┤│ BERT │ GPT │├──────────────────────────┼──────────────────────────────────┤│ Encoder 架构 │ Decoder 架构 ││ 双向注意力 │ 单向注意力（Causal） ││ 一次性输入整个句子 │ 逐个 token 生成 ││ 输入输出长度相同 │ 输出比输入长 │├──────────────────────────┼──────────────────────────────────┤│ ✅ 有 Q、K、V │ ✅ 有 Q、K、V ││ ❌ 不需要 KV Cache │ ✅ 必须 KV Cache │├──────────────────────────┼──────────────────────────────────┤│ 用途: 理解 │ 用途: 生成 ││ 分类、NER、问答 │ 聊天、写作、代码生成 │└──────────────────────────┴──────────────────────────────────┘ 记忆口诀BERT = 阅读理解 = 一眼看完全文 = 不需要缓存GPT = 写作文 = 一个字一个字写 = 需要记住前面写了什么 ❓ Part 9: 常见问答 FAQQ1: [MASK] Token 为什么能预测出正确答案？A: [MASK] 的 Token ID (103) 本身不代表任何语义，但经过 12 层 Self-Attention 后： Embedding: 103 → [0.01, 0.02, ...] 空壳 ↓ Layer 1: 吸收 \"爱\" 和 \"天安门\" 的信息Layer 1: [0.52, 0.71, ...] 开始有语义 ↓ Layer 2-12: 不断精炼Layer 12: [0.93, 0.87, ...] 完全理解上下文最终这个向量在语义空间中接近 \"北京\"！ 关键: Self-Attention 让 [MASK] 从周围词”偷”信息！ Q2: BERT 为什么只用 Encoder，不用 Decoder？A: 因为 BERT 的任务是”理解”，不是”生成” 任务类型 需要 Decoder？ 原因 机器翻译 ✅ 需要 输入中文，输出英文，是不同的序列 文本分类 ❌ 不需要 只需理解输入，输出一个类别 NER ❌ 不需要 只需给每个输入词打标签 问答 ❌ 不需要 答案在原文中，只需找位置 简单说: Encoder 理解输入，Decoder 生成输出。BERT 只需要理解！ Q3: 残差连接是什么？为什么需要它？A: 残差连接就是把输入直接加到输出上 # 不用残差output = FFN(input) # 可能丢失原始信息# 用残差output = input + FFN(input) # 保底 + 增量 好处: 梯度可以直接跳过 FFN 回传 → 解决梯度消失 原始信息不会丢失 → 网络可以更深 FFN 只需学习”差异” → 更容易训练 Q4: 预训练和微调有什么区别？A: 预训练（Pre-training）: - 数据: 海量无标注文本 (数十亿词) - 任务: MLM 完形填空 - 目的: 学会\"理解语言\" - 耗时: 数周 (只做一次) 微调（Fine-tuning）: - 数据: 少量标注数据 (1000条) - 任务: 具体任务 (情感分析、NER...) - 目的: 适配特定任务 - 耗时: 几小时 (每个任务都要) 类比: 预训练 = 上大学，微调 = 岗位培训 Q5: MLM 为什么要 80%/10%/10% 的策略？A: 100% [MASK]: 预训练和微调分布不一致 预训练: \"我爱[MASK]天安门\" 微调: \"我爱北京天安门\" ← 没有 [MASK]！ 问题: 模型过度依赖 [MASK] 符号80% [MASK] + 10% 随机 + 10% 不变: - 80% [MASK]: 主要学习目标 - 10% 随机: 让模型学会纠错 - 10% 不变: 适配真实分布 Q6: BERT 的 Q、K、V 是干什么的？A: Q (Query): \"我想找什么信息？\"K (Key): \"我这里有什么信息？\"V (Value): \"实际的信息内容\"Attention = 根据 Q 和 K 的相似度，加权求和 V例子: [MASK] 位置 Q_mask: \"我需要知道这个空填什么\" K_天安门: \"我是天安门\" 相似度高 → [MASK] 多看 V_天安门 → [MASK] 获得\"天安门在北京\"的信息 Q7: Encoder 和 Decoder 的注意力有什么区别？A: 维度 Encoder Decoder 可见范围 全局可见（双向） 只看左边（单向） Mask 无 mask 或仅 padding Causal mask (下三角) 用途 理解整个句子 生成下一个词 KV Cache ❌ 不需要 ✅ 需要 Encoder: [CLS] 可以看到 \"我爱北京天安门\" 所有词Decoder: \"北京\" 只能看到 \"我爱北京\"，不能看到 \"天安门\" Q8: 为什么 BERT 需要 [CLS] 和 [SEP] Token？A: [CLS] (Classification): - 位置: 句子开头 - 作用: 汇聚整个句子的语义 - 用途: 分类任务取 [CLS] 的向量[SEP] (Separator): - 位置: 句子结尾，或两个句子之间 - 作用: 分隔不同句子 - 用途: 让模型知道句子边界例子: 单句: [CLS] 这个产品很好 [SEP] 句对: [CLS] 天在下雨 [SEP] 地面是湿的 [SEP] Q9: BERT 的层数越多越好吗？A: 不一定！ 模型 层数 参数量 性能 问题 BERT-Base 12 110M 84.6% - BERT-Large 24 340M 86.7% 训练慢、需要更多数据 BERT-超大 48+ 1B+ 提升有限 过拟合、推理慢 结论: 12-24 层是甜蜜点，更多层边际收益递减 Q10: BERT 和 GPT 可以结合使用吗？A: 可以！这就是 Encoder-Decoder 架构 T5 / BART: 输入 → BERT-like Encoder (理解) → GPT-like Decoder (生成) → 输出适用场景: - 机器翻译 - 文本摘要 - 对话生成 🔗 延伸阅读 论文原文: BERT (arxiv.org/abs/1810.04805) 后续改进: RoBERTa: 移除 NSP, 更大批次训练 ALBERT: 参数共享, 减少模型大小 ELECTRA: 判别式预训练, 更高效 KV Cache 优化: PagedAttention: vLLM 的核心技术 FlashAttention: IO 优化的注意力算法 Transformer 原文: Attention Is All You Need 🚀 Part 6: BERT 实际应用场景与代码实现6.1 应用场景总览 应用场景 任务类型 输入格式 输出 典型应用 情感分析 单句分类 [CLS] 文本 [SEP] 类别标签 商品评论、舆情监控 文本匹配 句对分类 [CLS] 句A [SEP] 句B [SEP] 相似度/关系 智能客服、问答匹配 命名实体识别 序列标注 [CLS] 文本 [SEP] 每个token标签 信息抽取、知识图谱 阅读理解 抽取式QA [CLS] 问题 [SEP] 文章 [SEP] 答案起止位置 智能问答、客服机器人 文本生成 Seq2Seq 需配合Decoder 生成文本 摘要、翻译 (需BART/T5) 6.2 情感分析完整实现\"\"\"场景: 电商评论情感分析输入: \"这个手机拍照效果很棒，电池也耐用\"输出: 正面 (0.95)\"\"\"from transformers import BertTokenizer, BertForSequenceClassificationimport torch# 1. 加载模型model_name = \"bert-base-chinese\"tokenizer = BertTokenizer.from_pretrained(model_name)model = BertForSequenceClassification.from_pretrained( model_name, num_labels=2 # 正面/负面)# 2. 数据准备def prepare_data(texts, labels): encodings = tokenizer( texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\" ) return encodings, torch.tensor(labels)# 示例数据train_texts = [ \"这个手机拍照效果很棒，电池也耐用\", \"质量太差了，用了一天就坏了\", \"物流很快，包装完好，好评\", \"客服态度恶劣，再也不买了\"]train_labels = [1, 0, 1, 0] # 1=正面, 0=负面# 3. 训练循环from torch.optim import AdamWoptimizer = AdamW(model.parameters(), lr=2e-5)model.train()for epoch in range(3): encodings, labels = prepare_data(train_texts, train_labels) outputs = model( input_ids=encodings[\"input_ids\"], attention_mask=encodings[\"attention_mask\"], labels=labels ) loss = outputs.loss loss.backward() optimizer.step() optimizer.zero_grad() print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")# 4. 推理预测model.eval()test_text = \"这款产品性价比超高，强烈推荐！\"inputs = tokenizer(test_text, return_tensors=\"pt\")with torch.no_grad(): outputs = model(**inputs) probs = torch.softmax(outputs.logits, dim=-1) pred = torch.argmax(probs, dim=-1) print(f\"预测: {'正面' if pred==1 else '负面'}, 置信度: {probs[0][pred].item():.2%}\")# 输出: 预测: 正面, 置信度: 94.32% 6.3 命名实体识别 (NER) 实现\"\"\"场景: 从新闻中提取人名、地名、机构名输入: \"马云在杭州创办了阿里巴巴公司\"输出: [(\"马云\", \"PER\"), (\"杭州\", \"LOC\"), (\"阿里巴巴公司\", \"ORG\")]\"\"\"from transformers import BertTokenizerFast, BertForTokenClassificationimport torch# NER 标签定义 (BIO格式)label_list = [\"O\", \"B-PER\", \"I-PER\", \"B-LOC\", \"I-LOC\", \"B-ORG\", \"I-ORG\"]label2id = {l: i for i, l in enumerate(label_list)}id2label = {i: l for i, l in enumerate(label_list)}# 加载模型tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")model = BertForTokenClassification.from_pretrained( \"bert-base-chinese\", num_labels=len(label_list), id2label=id2label, label2id=label2id)# 推理函数def extract_entities(text): inputs = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True) offset_mapping = inputs.pop(\"offset_mapping\")[0] with torch.no_grad(): outputs = model(**inputs) predictions = torch.argmax(outputs.logits, dim=-1)[0] entities = [] current_entity = None for idx, (pred, offset) in enumerate(zip(predictions, offset_mapping)): if offset[0] == offset[1]: # 跳过特殊token continue label = id2label[pred.item()] char = text[offset[0]:offset[1]] if label.startswith(\"B-\"): if current_entity: entities.append(current_entity) current_entity = {\"text\": char, \"type\": label[2:]} elif label.startswith(\"I-\") and current_entity: current_entity[\"text\"] += char else: if current_entity: entities.append(current_entity) current_entity = None if current_entity: entities.append(current_entity) return [(e[\"text\"], e[\"type\"]) for e in entities]# 测试text = \"马云在杭州创办了阿里巴巴公司\"print(extract_entities(text))# 输出: [(\"马云\", \"PER\"), (\"杭州\", \"LOC\"), (\"阿里巴巴公司\", \"ORG\")] 6.4 语义相似度匹配\"\"\"场景: 智能客服FAQ匹配输入: 用户问题 + FAQ库输出: 最相似的FAQ及答案\"\"\"from transformers import BertTokenizer, BertModelimport torchimport torch.nn.functional as Ftokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")model = BertModel.from_pretrained(\"bert-base-chinese\")def get_sentence_embedding(text): \"\"\"获取句子的BERT表示 (使用[CLS]向量)\"\"\" inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True) with torch.no_grad(): outputs = model(**inputs) # 使用 [CLS] token 的输出作为句子表示 return outputs.last_hidden_state[:, 0, :]def compute_similarity(text1, text2): \"\"\"计算两个句子的余弦相似度\"\"\" emb1 = get_sentence_embedding(text1) emb2 = get_sentence_embedding(text2) return F.cosine_similarity(emb1, emb2).item()# FAQ库faq_database = [ {\"question\": \"如何修改密码？\", \"answer\": \"请进入设置-账户安全-修改密码\"}, {\"question\": \"怎么申请退款？\", \"answer\": \"在订单详情页点击申请退款按钮\"}, {\"question\": \"配送需要多久？\", \"answer\": \"一般3-5个工作日送达\"}, {\"question\": \"支持哪些支付方式？\", \"answer\": \"支持微信、支付宝、银行卡支付\"},]def find_best_match(user_query): \"\"\"找到最匹配的FAQ\"\"\" best_score = -1 best_faq = None for faq in faq_database: score = compute_similarity(user_query, faq[\"question\"]) if score best_score: best_score = score best_faq = faq return best_faq, best_score# 测试query = \"我想改一下登录密码\"faq, score = find_best_match(query)print(f\"用户问题: {query}\")print(f\"匹配FAQ: {faq['question']} (相似度: {score:.2%})\")print(f\"回答: {faq['answer']}\")# 输出:# 用户问题: 我想改一下登录密码# 匹配FAQ: 如何修改密码？ (相似度: 89.34%)# 回答: 请进入设置-账户安全-修改密码 6.5 阅读理解问答系统\"\"\"场景: 从文档中找答案输入: 问题 + 文章输出: 答案文本及位置\"\"\"from transformers import BertTokenizerFast, BertForQuestionAnsweringimport torchtokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")model = BertForQuestionAnswering.from_pretrained( \"luhua/chinese_pretrain_mrc_roberta_wwm_ext_large\" # 中文QA模型)def answer_question(question, context): \"\"\"从文章中抽取答案\"\"\" inputs = tokenizer( question, context, return_tensors=\"pt\", max_length=512, truncation=True ) with torch.no_grad(): outputs = model(**inputs) # 获取答案起止位置 start_idx = torch.argmax(outputs.start_logits) end_idx = torch.argmax(outputs.end_logits) # 解码答案 tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]) answer = tokenizer.convert_tokens_to_string(tokens[start_idx:end_idx+1]) # 计算置信度 start_prob = torch.softmax(outputs.start_logits, dim=-1)[0][start_idx].item() end_prob = torch.softmax(outputs.end_logits, dim=-1)[0][end_idx].item() confidence = (start_prob + end_prob) / 2 return answer, confidence# 测试context = \"\"\"阿里巴巴集团由马云于1999年在中国杭州创立。公司最初是一个B2B网上交易市场，后来发展成为一个多元化的科技公司。2014年，阿里巴巴在纽约证券交易所上市，创造了当时全球最大的IPO纪录。目前阿里巴巴的核心业务包括电子商务、云计算、数字媒体和娱乐。\"\"\"questions = [ \"阿里巴巴是谁创立的？\", \"阿里巴巴是哪一年上市的？\", \"阿里巴巴的核心业务有哪些？\"]for q in questions: answer, conf = answer_question(q, context) print(f\"Q: {q}\") print(f\"A: {answer} (置信度: {conf:.2%}) \")# 输出:# Q: 阿里巴巴是谁创立的？# A: 马云 (置信度: 92.15%)## Q: 阿里巴巴是哪一年上市的？# A: 2014年 (置信度: 88.73%)## Q: 阿里巴巴的核心业务有哪些？# A: 电子商务、云计算、数字媒体和娱乐 (置信度: 85.21%) ⚡ Part 7: KV Cache 优化实践与代码7.1 KV Cache 原理可视化无 KV Cache (每次重新计算):═══════════════════════════════════════════════════════════Step 1: \"I\" → 计算 K₁,V₁,Q₁ → 输出 \"love\"Step 2: \"I love\" → 计算 K₁,V₁,K₂,V₂,Q₂ → 输出 \"NLP\" Step 3: \"I love NLP\" → 计算 K₁,V₁,K₂,V₂,K₃,V₃,Q₃ → 输出 \"!\" ↑ 重复计算!有 KV Cache (缓存复用):═══════════════════════════════════════════════════════════Step 1: \"I\" → 计算 K₁,V₁ [存入Cache] → Q₁ → \"love\"Step 2: \"love\" → 计算 K₂,V₂ [追加Cache] → Q₂ @ [K₁K₂] → \"NLP\"Step 3: \"NLP\" → 计算 K₃,V₃ [追加Cache] → Q₃ @ [K₁K₂K₃] → \"!\" ↑ 只计算新token的KV! 7.2 KV Cache 完整实现\"\"\"从零实现带 KV Cache 的 GPT 推理\"\"\"import torchimport torch.nn as nnimport torch.nn.functional as Fimport mathclass CachedMultiHeadAttention(nn.Module): def __init__(self, d_model=768, n_heads=12): super().__init__() self.d_model = d_model self.n_heads = n_heads self.d_k = d_model // n_heads self.W_q = nn.Linear(d_model, d_model) self.W_k = nn.Linear(d_model, d_model) self.W_v = nn.Linear(d_model, d_model) self.W_o = nn.Linear(d_model, d_model) def forward(self, x, kv_cache=None, use_cache=True): \"\"\" Args: x: [batch, seq_len, d_model] 输入 kv_cache: (cached_k, cached_v) 缓存的K和V use_cache: 是否使用和更新缓存 Returns: output: [batch, seq_len, d_model] new_cache: 更新后的缓存 \"\"\" batch_size = x.size(0) # 计算当前token的 Q, K, V Q = self.W_q(x) # [batch, seq, d_model] K = self.W_k(x) V = self.W_v(x) # 如果有缓存，拼接历史 K, V if kv_cache is not None: cached_k, cached_v = kv_cache K = torch.cat([cached_k, K], dim=1) # [batch, cached+seq, d_model] V = torch.cat([cached_v, V], dim=1) # 保存新缓存 new_cache = (K, V) if use_cache else None # 重塑为多头格式 def reshape_for_heads(t): # [batch, seq, d_model] - [batch, heads, seq, d_k] return t.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2) Q = reshape_for_heads(Q) # [batch, heads, q_seq, d_k] K = reshape_for_heads(K) # [batch, heads, kv_seq, d_k] V = reshape_for_heads(V) # [batch, heads, kv_seq, d_k] # 计算注意力分数 scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k) # [batch, heads, q_seq, kv_seq] # Causal Mask (只看过去) q_len, kv_len = Q.size(2), K.size(2) causal_mask = torch.triu( torch.ones(q_len, kv_len, device=x.device), diagonal=kv_len - q_len + 1 ).bool() scores = scores.masked_fill(causal_mask, float('-inf')) # Softmax + 加权求和 attn_weights = F.softmax(scores, dim=-1) output = torch.matmul(attn_weights, V) # 合并多头 output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model) output = self.W_o(output) return output, new_cacheclass GPTWithKVCache(nn.Module): \"\"\"简化的 GPT 模型，支持 KV Cache\"\"\" def __init__(self, vocab_size=50257, d_model=768, n_layers=12, n_heads=12): super().__init__() self.embedding = nn.Embedding(vocab_size, d_model) self.layers = nn.ModuleList([ CachedMultiHeadAttention(d_model, n_heads) for _ in range(n_layers) ]) self.lm_head = nn.Linear(d_model, vocab_size) self.n_layers = n_layers def forward(self, input_ids, past_kv_cache=None, use_cache=True): \"\"\" Args: input_ids: [batch, seq_len] past_kv_cache: List of (K, V) for each layer use_cache: 是否使用KV Cache \"\"\" x = self.embedding(input_ids) new_cache = [] for i, layer in enumerate(self.layers): layer_cache = past_kv_cache[i] if past_kv_cache else None x, cache = layer(x, kv_cache=layer_cache, use_cache=use_cache) new_cache.append(cache) logits = self.lm_head(x) return logits, new_cache if use_cache else Nonedef generate_with_kv_cache(model, prompt_ids, max_new_tokens=50): \"\"\"使用 KV Cache 进行高效生成\"\"\" model.eval() generated = prompt_ids.clone() past_cache = None with torch.no_grad(): # 第一步：处理整个 prompt logits, past_cache = model(prompt_ids, past_kv_cache=None, use_cache=True) next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True) generated = torch.cat([generated, next_token], dim=1) # 后续步骤：只处理新 token for _ in range(max_new_tokens - 1): # 只输入最后一个 token！ logits, past_cache = model( next_token, # [batch, 1] 只有一个token past_kv_cache=past_cache, use_cache=True ) next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True) generated = torch.cat([generated, next_token], dim=1) return generated# 性能对比测试def benchmark_kv_cache(): import time model = GPTWithKVCache(vocab_size=1000, d_model=256, n_layers=6, n_heads=8) prompt = torch.randint(0, 1000, (1, 10)) # 无 KV Cache start = time.time() for _ in range(100): generated = prompt.clone() for i in range(50): with torch.no_grad(): logits, _ = model(generated, use_cache=False) next_token = torch.argmax(logits[:, -1, :], dim=-1, keepdim=True) generated = torch.cat([generated, next_token], dim=1) time_no_cache = time.time() - start # 有 KV Cache start = time.time() for _ in range(100): generate_with_kv_cache(model, prompt, max_new_tokens=50) time_with_cache = time.time() - start print(f\"无 KV Cache: {time_no_cache:.2f}s\") print(f\"有 KV Cache: {time_with_cache:.2f}s\") print(f\"加速比: {time_no_cache/time_with_cache:.1f}x\")# benchmark_kv_cache()# 输出示例:# 无 KV Cache: 45.32s# 有 KV Cache: 8.21s# 加速比: 5.5x 7.3 KV Cache 显存优化技术\"\"\"三种 KV Cache 优化技术的实现对比\"\"\"# 1. Multi-Query Attention (MQA)# 所有 Q 头共享一组 K, Vclass MultiQueryAttention(nn.Module): def __init__(self, d_model=768, n_heads=12): super().__init__() self.n_heads = n_heads self.d_k = d_model // n_heads self.W_q = nn.Linear(d_model, d_model) # 12 个头的 Q self.W_k = nn.Linear(d_model, self.d_k) # 只有 1 组 K self.W_v = nn.Linear(d_model, self.d_k) # 只有 1 组 V self.W_o = nn.Linear(d_model, d_model) def forward(self, x): batch, seq, _ = x.shape Q = self.W_q(x).view(batch, seq, self.n_heads, self.d_k) K = self.W_k(x).unsqueeze(2) # [batch, seq, 1, d_k] V = self.W_v(x).unsqueeze(2) # [batch, seq, 1, d_k] # K, V 广播到所有头 K = K.expand(-1, -1, self.n_heads, -1) V = V.expand(-1, -1, self.n_heads, -1) # ... 后续计算相同 # KV Cache 大小: 1/n_heads of standard!# 2. Grouped-Query Attention (GQA) - LLaMA-2 使用class GroupedQueryAttention(nn.Module): def __init__(self, d_model=768, n_heads=12, n_kv_heads=3): \"\"\" n_heads=12, n_kv_heads=3 表示: - 12 个 Q 头 - 3 个 KV 头 (每 4 个 Q 头共享 1 个 KV) \"\"\" super().__init__() self.n_heads = n_heads self.n_kv_heads = n_kv_heads self.n_groups = n_heads // n_kv_heads # 4 self.d_k = d_model // n_heads self.W_q = nn.Linear(d_model, d_model) self.W_k = nn.Linear(d_model, self.n_kv_heads * self.d_k) self.W_v = nn.Linear(d_model, self.n_kv_heads * self.d_k) self.W_o = nn.Linear(d_model, d_model) def forward(self, x): batch, seq, _ = x.shape Q = self.W_q(x).view(batch, seq, self.n_heads, self.d_k) K = self.W_k(x).view(batch, seq, self.n_kv_heads, self.d_k) V = self.W_v(x).view(batch, seq, self.n_kv_heads, self.d_k) # 将 KV 重复以匹配 Q 头数 K = K.repeat_interleave(self.n_groups, dim=2) # [b, s, 12, d_k] V = V.repeat_interleave(self.n_groups, dim=2) # ... 后续计算相同 # KV Cache 大小: n_kv_heads/n_heads of standard (这里是 1/4)# 3. 显存占用对比def memory_comparison(): \"\"\" LLaMA-7B 配置: 32层, 4096维度, 32头 序列长度: 4096 tokens \"\"\" n_layers = 32 d_model = 4096 seq_len = 4096 dtype_bytes = 2 # float16 # 标准 MHA standard = 2 * n_layers * d_model * seq_len * dtype_bytes print(f\"标准 MHA KV Cache: {standard / 1e9:.2f} GB\") # MQA (所有头共享) n_heads = 32 mqa = 2 * n_layers * (d_model // n_heads) * seq_len * dtype_bytes print(f\"MQA KV Cache: {mqa / 1e9:.2f} GB ({standard/mqa:.0f}x 节省)\") # GQA (LLaMA-2 配置: 8 个 KV 头) n_kv_heads = 8 gqa = 2 * n_layers * (d_model // n_heads * n_kv_heads) * seq_len * dtype_bytes print(f\"GQA KV Cache: {gqa / 1e9:.2f} GB ({standard/gqa:.0f}x 节省)\")memory_comparison()# 输出:# 标准 MHA KV Cache: 2.15 GB# MQA KV Cache: 0.07 GB (32x 节省)# GQA KV Cache: 0.27 GB (8x 节省) 7.4 生产环境最佳实践 优化技术 适用场景 优点 缺点 标准 MHA 小模型、短序列 质量最好 显存占用大 MQA 超长序列、极致推理速度 显存节省最多 质量略有下降 GQA 生产环境推荐 平衡质量和效率 需要重新训练 PagedAttention 高并发推理服务 减少显存碎片 实现复杂 FlashAttention 所有场景 IO优化、训练加速 需要特定硬件 # vLLM 使用示例 (生产推荐)from vllm import LLM, SamplingParams# 自动启用 PagedAttentionllm = LLM( model=\"meta-llama/Llama-2-7b-hf\", tensor_parallel_size=1, # GPU数量 gpu_memory_utilization=0.9, # 显存利用率)# 高效推理prompts = [\"写一首关于春天的诗：\", \"解释什么是机器学习：\"]outputs = llm.generate(prompts, SamplingParams(temperature=0.7, max_tokens=100)) 🚀 Part 6: BERT 实际应用场景与代码6.1 应用场景总览 应用场景 任务类型 输入格式 输出 实际案例 情感分析 单句分类 [CLS] 文本 [SEP] 正面/负面 电商评论分析、舆情监控 文本匹配 句对分类 [CLS] 句A [SEP] 句B [SEP] 相似/不相似 智能客服、问题去重 命名实体识别 序列标注 [CLS] 文本 [SEP] 每个token的标签 简历解析、医疗病历 阅读理解 抽取式QA [CLS] 问题 [SEP] 文章 [SEP] 答案位置 智能问答、知识库检索 文本生成 Seq2Seq 需配合Decoder 生成文本 摘要生成、机器翻译 6.2 情感分析完整代码from transformers import BertTokenizer, BertForSequenceClassificationimport torch# 1. 加载预训练模型model_name = \"bert-base-chinese\"tokenizer = BertTokenizer.from_pretrained(model_name)model = BertForSequenceClassification.from_pretrained(model_name, num_labels=2)# 2. 数据预处理def preprocess(texts, labels): encodings = tokenizer( texts, padding=True, truncation=True, max_length=128, return_tensors=\"pt\" ) return encodings, torch.tensor(labels)# 3. 训练数据示例train_texts = [ \"这个产品质量太差了，完全是浪费钱\", \"非常满意！发货速度快，质量很好\", \"一般般吧，没有想象中那么好\", \"超级推荐！已经回购三次了\"]train_labels = [0, 1, 0, 1] # 0=负面, 1=正面# 4. 微调训练from torch.optim import AdamWencodings, labels = preprocess(train_texts, train_labels)optimizer = AdamW(model.parameters(), lr=2e-5)model.train()for epoch in range(3): outputs = model(**encodings, labels=labels) loss = outputs.loss loss.backward() optimizer.step() optimizer.zero_grad() print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")# 5. 推理预测model.eval()test_text = \"这款手机拍照效果很棒，电池也耐用\"inputs = tokenizer(test_text, return_tensors=\"pt\")with torch.no_grad(): outputs = model(**inputs) prediction = torch.argmax(outputs.logits, dim=1) print(f\"预测结果: {'正面' if prediction == 1 else '负面'}\") 6.3 命名实体识别 (NER) 完整代码from transformers import BertTokenizerFast, BertForTokenClassificationimport torch# NER 标签定义 (BIO 格式)label_list = [\"O\", \"B-PER\", \"I-PER\", \"B-ORG\", \"I-ORG\", \"B-LOC\", \"I-LOC\"]label2id = {label: i for i, label in enumerate(label_list)}id2label = {i: label for label, i in label2id.items()}# 加载模型tokenizer = BertTokenizerFast.from_pretrained(\"bert-base-chinese\")model = BertForTokenClassification.from_pretrained( \"bert-base-chinese\", num_labels=len(label_list), id2label=id2label, label2id=label2id)# 训练数据示例train_data = [ { \"text\": \"马云在杭州创办了阿里巴巴\", \"entities\": [ {\"start\": 0, \"end\": 2, \"label\": \"PER\"}, # 马云 {\"start\": 3, \"end\": 5, \"label\": \"LOC\"}, # 杭州 {\"start\": 8, \"end\": 12, \"label\": \"ORG\"} # 阿里巴巴 ] }]# 推理示例def predict_ner(text): inputs = tokenizer(text, return_tensors=\"pt\", return_offsets_mapping=True) offset_mapping = inputs.pop(\"offset_mapping\")[0] model.eval() with torch.no_grad(): outputs = model(**inputs) predictions = torch.argmax(outputs.logits, dim=2)[0] # 解析实体 entities = [] current_entity = None for idx, (pred, offset) in enumerate(zip(predictions, offset_mapping)): label = id2label[pred.item()] if label.startswith(\"B-\"): if current_entity: entities.append(current_entity) current_entity = { \"text\": text[offset[0]:offset[1]], \"label\": label[2:], \"start\": offset[0].item() } elif label.startswith(\"I-\") and current_entity: current_entity[\"text\"] += text[offset[0]:offset[1]] else: if current_entity: entities.append(current_entity) current_entity = None return entities# 测试result = predict_ner(\"马云在杭州创办了阿里巴巴\")print(result)# [{'text': '马云', 'label': 'PER', 'start': 0}, # {'text': '杭州', 'label': 'LOC', 'start': 3},# {'text': '阿里巴巴', 'label': 'ORG', 'start': 8}] 6.4 语义相似度匹配代码from transformers import BertTokenizer, BertModelimport torchimport torch.nn.functional as F# 使用 BERT 提取句子向量tokenizer = BertTokenizer.from_pretrained(\"bert-base-chinese\")model = BertModel.from_pretrained(\"bert-base-chinese\")def get_sentence_embedding(text): \"\"\"提取句子的 [CLS] 向量作为语义表示\"\"\" inputs = tokenizer(text, return_tensors=\"pt\", padding=True, truncation=True) with torch.no_grad(): outputs = model(**inputs) # 使用 [CLS] token 的输出作为句子表示 cls_embedding = outputs.last_hidden_state[:, 0, :] return cls_embeddingdef compute_similarity(text1, text2): \"\"\"计算两个句子的余弦相似度\"\"\" emb1 = get_sentence_embedding(text1) emb2 = get_sentence_embedding(text2) similarity = F.cosine_similarity(emb1, emb2) return similarity.item()# 测试语义相似度pairs = [ (\"今天天气怎么样\", \"今天天气好吗\"), # 高相似 (\"今天天气怎么样\", \"明天会下雨吗\"), # 中等相似 (\"今天天气怎么样\", \"这道菜怎么做\"), # 低相似]for text1, text2 in pairs: sim = compute_similarity(text1, text2) print(f\"'{text1}' vs '{text2}': {sim:.4f}\")# 输出:# '今天天气怎么样' vs '今天天气好吗': 0.9234# '今天天气怎么样' vs '明天会下雨吗': 0.7821# '今天天气怎么样' vs '这道菜怎么做': 0.4123 6.5 问答系统 (阅读理解) 代码from transformers import BertTokenizer, BertForQuestionAnsweringimport torch# 加载问答模型model_name = \"bert-large-uncased-whole-word-masking-finetuned-squad\"tokenizer = BertTokenizer.from_pretrained(model_name)model = BertForQuestionAnswering.from_pretrained(model_name)def answer_question(question, context): \"\"\"从文章中抽取答案\"\"\" # 编码问题和上下文 inputs = tokenizer( question, context, return_tensors=\"pt\", max_length=512, truncation=True ) # 预测答案位置 with torch.no_grad(): outputs = model(**inputs) start_scores = outputs.start_logits end_scores = outputs.end_logits # 找到最可能的答案位置 start_idx = torch.argmax(start_scores) end_idx = torch.argmax(end_scores) # 解码答案 tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0]) answer = tokenizer.convert_tokens_to_string(tokens[start_idx:end_idx+1]) return answer# 测试问答context = \"\"\"BERT是由Google在2018年提出的预训练语言模型。它使用Transformer的Encoder架构，通过Masked Language Model任务进行预训练。BERT在11个NLP任务上取得了当时的最佳成绩，包括问答、文本分类等任务。BERT-Base有1.1亿参数，BERT-Large有3.4亿参数。\"\"\"questions = [ \"BERT是谁提出的？\", \"BERT使用什么架构？\", \"BERT有多少参数？\"]for q in questions: answer = answer_question(q, context) print(f\"Q: {q}\") print(f\"A: {answer} \") ⚡ Part 7: KV Cache 实战与优化7.1 KV Cache 完整实现代码import torchimport torch.nn as nnimport torch.nn.functional as Fclass AttentionWithKVCache(nn.Module): \"\"\"带 KV Cache 的注意力层实现\"\"\" def __init__(self, d_model=768, n_heads=12): super().__init__() self.d_model = d_model self.n_heads = n_heads self.d_k = d_model // n_heads self.W_q = nn.Linear(d_model, d_model) self.W_k = nn.Linear(d_model, d_model) self.W_v = nn.Linear(d_model, d_model) self.W_o = nn.Linear(d_model, d_model) def forward(self, x, kv_cache=None, use_cache=False): \"\"\" Args: x: [batch, seq_len, d_model] 输入 kv_cache: tuple(K, V) 缓存的 K/V use_cache: 是否使用并更新缓存 Returns: output: [batch, seq_len, d_model] new_kv_cache: 更新后的缓存 \"\"\" batch_size, seq_len, _ = x.shape # 计算 Q, K, V Q = self.W_q(x) # [batch, seq, d_model] K = self.W_k(x) V = self.W_v(x) # 如果有缓存，拼接历史 K, V if kv_cache is not None: K_cache, V_cache = kv_cache K = torch.cat([K_cache, K], dim=1) # [batch, cache_len + seq, d_model] V = torch.cat([V_cache, V], dim=1) # 准备返回的缓存 new_kv_cache = (K, V) if use_cache else None # 重塑为多头形式 Q = Q.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2) K = K.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2) V = V.view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2) # Q: [batch, heads, seq, d_k] # K, V: [batch, heads, total_seq, d_k] # 计算注意力 scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5) # Causal Mask (只看左边) total_len = K.size(2) query_len = Q.size(2) mask = torch.triu(torch.ones(query_len, total_len), diagonal=total_len-query_len+1) mask = mask.bool().to(x.device) scores = scores.masked_fill(mask, float('-inf')) attn_weights = F.softmax(scores, dim=-1) output = torch.matmul(attn_weights, V) # 合并多头 output = output.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model) output = self.W_o(output) return output, new_kv_cache# 使用示例：自回归生成def generate_with_kv_cache(model, prompt_ids, max_new_tokens=50): \"\"\"使用 KV Cache 进行高效生成\"\"\" # 初始化：处理 prompt kv_cache = None input_ids = prompt_ids generated = prompt_ids.tolist() for step in range(max_new_tokens): # 只输入新的 token（第一步输入完整 prompt） if step == 0: x = get_embeddings(input_ids) else: x = get_embeddings(input_ids[:, -1:]) # 只取最后一个 token # 前向传播，使用并更新缓存 output, kv_cache = model(x, kv_cache=kv_cache, use_cache=True) # 预测下一个 token logits = output[:, -1, :] # 取最后一个位置的输出 next_token = torch.argmax(logits, dim=-1) generated.append(next_token.item()) input_ids = torch.cat([input_ids, next_token.unsqueeze(0)], dim=1) # 遇到结束符停止 if next_token.item() == EOS_TOKEN_ID: break return generatedprint(\"KV Cache 加速对比:\")print(\"无缓存: O(n²) 计算量\")print(\"有缓存: O(n) 计算量\")print(\"加速比: n 倍 (序列长度)\") 7.2 KV Cache 显存优化技术Multi-Query Attention (MQA) 实现class MultiQueryAttention(nn.Module): \"\"\" Multi-Query Attention: 所有头共享一组 K, V 显存节省: n_heads 倍 \"\"\" def __init__(self, d_model=768, n_heads=12): super().__init__() self.n_heads = n_heads self.d_k = d_model // n_heads # Q: 每个头独立 self.W_q = nn.Linear(d_model, d_model) # K, V: 所有头共享 (只有一份) self.W_k = nn.Linear(d_model, self.d_k) # 只输出一个头的维度 self.W_v = nn.Linear(d_model, self.d_k) self.W_o = nn.Linear(d_model, d_model) def forward(self, x, kv_cache=None): batch, seq, _ = x.shape Q = self.W_q(x).view(batch, seq, self.n_heads, self.d_k).transpose(1, 2) K = self.W_k(x).unsqueeze(1) # [batch, 1, seq, d_k] 广播到所有头 V = self.W_v(x).unsqueeze(1) # KV Cache 只需存储 [batch, 1, seq, d_k] 而非 [batch, heads, seq, d_k] # 显存节省: 12倍 (对于12头) if kv_cache is not None: K = torch.cat([kv_cache[0], K], dim=2) V = torch.cat([kv_cache[1], V], dim=2) scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5) attn = F.softmax(scores, dim=-1) output = torch.matmul(attn, V) output = output.transpose(1, 2).contiguous().view(batch, seq, -1) return self.W_o(output), (K, V) Grouped-Query Attention (GQA) 实现class GroupedQueryAttention(nn.Module): \"\"\" Grouped-Query Attention: 将头分组，每组共享 K, V LLaMA-2 使用: 32头 → 8组 显存节省: n_heads / n_groups 倍 \"\"\" def __init__(self, d_model=768, n_heads=12, n_kv_groups=4): super().__init__() self.n_heads = n_heads self.n_kv_groups = n_kv_groups self.heads_per_group = n_heads // n_kv_groups self.d_k = d_model // n_heads self.W_q = nn.Linear(d_model, d_model) # K, V 只有 n_kv_groups 组 self.W_k = nn.Linear(d_model, self.d_k * n_kv_groups) self.W_v = nn.Linear(d_model, self.d_k * n_kv_groups) self.W_o = nn.Linear(d_model, d_model) def forward(self, x): batch, seq, _ = x.shape Q = self.W_q(x).view(batch, seq, self.n_heads, self.d_k).transpose(1, 2) K = self.W_k(x).view(batch, seq, self.n_kv_groups, self.d_k).transpose(1, 2) V = self.W_v(x).view(batch, seq, self.n_kv_groups, self.d_k).transpose(1, 2) # 将 K, V 扩展到与 Q 相同的头数 K = K.repeat_interleave(self.heads_per_group, dim=1) V = V.repeat_interleave(self.heads_per_group, dim=1) scores = torch.matmul(Q, K.transpose(-2, -1)) / (self.d_k ** 0.5) attn = F.softmax(scores, dim=-1) output = torch.matmul(attn, V) output = output.transpose(1, 2).contiguous().view(batch, seq, -1) return self.W_o(output) 7.3 显存占用对比表 技术 KV Cache 大小 LLaMA-7B 128K tokens 适用模型 MHA (标准) 2 × L × H × S × dtype 64 GB BERT, GPT-2 MQA 2 × L × (H/heads) × S × dtype 5.3 GB PaLM, Falcon GQA (8组) 2 × L × (H/4) × S × dtype 16 GB LLaMA-2, Mistral L=层数, H=隐藏维度, S=序列长度, heads=注意力头数 7.4 生产环境最佳实践# 使用 vLLM 进行高效推理 (PagedAttention)from vllm import LLM, SamplingParams# 初始化模型llm = LLM( model=\"meta-llama/Llama-2-7b-hf\", tensor_parallel_size=1, # GPU 数量 gpu_memory_utilization=0.9, # 显存使用率)# 批量推理 (自动管理 KV Cache)prompts = [ \"Explain the concept of attention mechanism:\", \"Write a Python function to sort a list:\", \"What is the capital of France?\"]sampling_params = SamplingParams( temperature=0.7, max_tokens=256, top_p=0.9)outputs = llm.generate(prompts, sampling_params)for output in outputs: print(f\"Prompt: {output.prompt}\") print(f\"Response: {output.outputs[0].text} \") 📅 创建时间: 2026-02-03🏷️ 标签: #AI #Transformer #BERT #Attention #KVCache","tags":["AI"],"categories":["深度学习"]},{"title":"彻底理解 Transformer：Attention Is All You Need","path":"/2026/02/02/Transformer-DeepDive/","content":"彻底理解 Transformer: Attention Is All You Need 摘要: 本文利用费曼学习法，通过图解和类比，深入浅出地拆解了 Transformer 架构的核心原理。从 Encoder-Decoder 架构到 Self-Attention 机制，再到 Cross-Attention 和 Masking 的细节，带你彻底搞懂这篇 AI 领域的奠基之作。 1. 核心大白话：为什么要搞这一套？传统模型的痛点：以前的 AI (RNN/LSTM) 像是一个接力赛跑选手。 必须先读第一个字，传棒给第二个字，再传给第三个… 缺点：如果句子太长，跑到最后早就忘了第一棒是谁了（长距离遗忘）；而且必须按顺序跑，不能所有人一起跑（无法并行，慢）。 Transformer 的革命：不搞接力赛了，搞足球赛。 并行：所有单词（球员）同时在场上。 注意力：每个球员（单词）都能时刻观察场上所有其他球员的位置，不管那个人在球场哪一头（解决了长距离依赖）。 2. 核心概念：Self-Attention (自注意力)这是 Transformer 的心脏。它解决了“每个词该关注谁”的问题。 2.1 Q, K, V 的图书检索类比为什么每个词要有 Query (Q), Key (K), Value (V) 三个向量？这源于数据库检索的思想。 (图注：通过图书检索系统理解 Q, K, V 的分离) 2.2 主动与被动：Q 与 K 的聚光灯效应为什么数学公式一样，Q 却是“主动”的？ (图注：Q 像手电筒一样主动扫描所有 K) Q (Query)：手握 100% 的“注意力预算”，它必须决定把光打在谁身上。在训练中，Loss 逼迫它学会“去寻找对我有用的信息”。 K (Key)：像墙上的靶子，无法移动。在训练中，Loss 逼迫它学会“标明自己的身份”，以便被正确的 Q 找到。 2.3 矩阵维度的秘密：为什么维度必须一样？很多初学者在这里卡住：为什么 $Q$ 和 $K$ 的维度必须一致？因为数学上的点积 (Dot Product) 就像拉拉链，齿数必须对得上。 (图注：矩阵乘法就像“拉链”咬合，中间的维度必须对齐) 3. 完整实战：德语到英语翻译训练全过程我们通过一个具体的任务：\"Ich liebe dich\" (德) - \"I love you\" (英)，来彻底搞懂编码器和解码器到底在干什么。 3.1 架构全景：数据是如何流动的？首先，看一眼上帝视角的架构图。注意红色的梯度回传线，这解释了为什么 Encoder 即使没有 Label 也能学会正确编码。 (图注：Encoder-Decoder 完整架构与梯度回传路径) 3.2 步骤一：Encoder (读懂德语)Encoder 的任务是把德语变成一组高质量的“记忆向量”。 操作: Self-Attention。 结果: 每个词都融合了上下文信息，形成了记忆库 $M$ (Memory)。 维度不变性: 输入是 [Batch, 3, 512]，输出 $M$ 依然是 [Batch, 3, 512]。这就是残差流 (Residual Stream) 的设计。 3.3 步骤二：Decoder 的核心 (Cross-Attention)这是模型最精彩的部分。Decoder 拿着英语去查德语。 (图注：Cross-Attention 细节。英语的 “I” (Q) 精准找到了德语的 “liebe” (K2)) Query (Q): 来自 Decoder 中间状态 $X_{mid}$。 形状: [Batch, 2, 512] (英语长度) Key (K): 来自 Encoder 记忆库 $M$。 形状: [Batch, 3, 512] (德语长度) Value (V): 来自 Encoder 记忆库 $M$。 QKV 矩阵的作用:Decoder 的 $W_Q^{cross}$ 和 Encoder 侧的 $W_K^{cross}$ 就像同声传译员，把英语状态和德语记忆映射到同一个“中间语义空间”，这样它们才能进行点积匹配。 3.4 步骤三：Mask 机制 (不许偷看)在 Decoder 内部，为了防止模型在预测 “love” 时偷看后面的 “you”，我们使用了 Mask。 (图注：Mask 矩阵。深蓝色代表可见，白色代表被遮挡 (-∞)) 3.5 步骤四：并行计算与 Loss虽然逻辑上是“预测完 I 再预测 love”，但在训练时，这都在一次矩阵运算中完成了。 (图注：一次性处理 4 个时刻的预测与 Loss 计算) 4. 极简代码实现 (Python/PyTorch)import torchimport torch.nn as nnimport torch.nn.functional as Fimport mathclass SelfAttention(nn.Module): def __init__(self, d_model, head_size): super().__init__() self.head_size = head_size # 1. 定义三个线性层：用来生成 Q, K, V self.key = nn.Linear(d_model, head_size, bias=False) self.query = nn.Linear(d_model, head_size, bias=False) self.value = nn.Linear(d_model, head_size, bias=False) def forward(self, x): # x 的形状: [Batch, Time(词数), Dimension(维度)] B, T, C = x.shape # 2. 生成 Q, K, V k = self.key(x) # (B, T, H) q = self.query(x) # (B, T, H) v = self.value(x) # (B, T, H) # 3. 计算关注度 (Attention Scores) wei = q @ k.transpose(-2, -1) * (1.0 / math.sqrt(self.head_size)) wei = F.softmax(wei, dim=-1) out = wei @ v return out 5. 核心数学公式与维度 (The Math Dimensions)5.1 Self-Attention (单头)$$ Attention(Q, K, V) = \\text{Softmax}(\\frac{QK^T}{\\sqrt{d_k}})V $$ 维度推演 (以 $d_{model}=512, d_k=64$ 为例)： 输入 $X$: [L, 512] 生成 Q, K, V: $Q = X W_Q \\rightarrow [L, 64]$ $K = X W_K \\rightarrow [L, 64]$ $V = X W_V \\rightarrow [L, 64]$ 矩阵乘法 $Q \\cdot K^T$: [L, 64] @ [64, L] $\\rightarrow$ [L, L] (分数矩阵) 输出: [L, L] @ [L, 64] $\\rightarrow$ [L, 64] 5.2 Cross-Attention (混合双打)$$ \\text{CrossAttn}(X_{dec}, M) = \\text{Softmax}(\\frac{(X_{dec}W_Q)(M W_K)^T}{\\sqrt{d_k}}) (M W_V) $$ $X_{dec}$: 英语中间状态 [L_tgt, 512] $M$: 德语记忆库 [L_src, 512] $W_Q$: [512, 64] (负责转译英语) $W_K$: [512, 64] (负责转译德语) Attention Map: [L_tgt, L_src] (例如 2行3列，表示每个英语词关注哪些德语词) 5.3 最终输出 (Word Prediction)$$ P(\\text{word}) = \\text{Softmax}(h \\cdot W_{vocab} + b) $$ $h$: Decoder 最终输出 [Batch, Seq, 512] $W_{vocab}$: 投影矩阵 [512, Vocab_Size] (例如 30000) 结果: [Batch, Seq, 30000] 6. 费曼自测 (Self-Check) Encoder 没有 Label，它是怎么学会正确的 Attention 的？(答：通过端到端的梯度反向传播。Decoder 的翻译错误会转化为 Loss，梯度顺着 Cross-Attention 流回 Encoder，告诉它“你提供的 Memory 质量太差，改！”) Cross-Attention 的 QKV 矩阵是随机的吗？(答：初始是随机的，但它们起到了“适配器”的作用，负责把英语状态和德语记忆映射到同一个语义空间，以便进行匹配。) 为什么 Encoder 输入输出维度必须一致？(答：为了支持残差连接 (Residual Connection)，公式 $x + SubLayer(x)$ 要求两者形状必须完全相同。)","tags":["DeepLearning"],"categories":["深度学习"]},{"title":"彻底搞懂 QLoRA：如何用 4-bit 量化技术单卡微调千亿模型？","path":"/2026/01/25/qlora-deep-dive/","content":"在上一篇文章中，我们深入探讨了 LoRA 的数学原理。今天，我们来聊聊它的进化版——**QLoRA (Quantized LoRA)**。 如果你想在普通的消费级显卡（如 RTX 3060/4090）上微调 33B 甚至 65B 的巨型模型，QLoRA 是你唯一的救星。它通过“4-bit 极限压缩”，将显存需求再次减半。 本文将基于深度技术问答，带你彻底搞懂 QLoRA 的核心机制。 一、 QLoRA vs LoRA：到底有什么区别？一句话总结： QLoRA = 4-bit 量化 (Quantization) + LoRA 。 LoRA 解决了“计算量”的问题，而 QLoRA 解决了“存储量（显存）”的问题。 1.1 显存占用的“降维打击”假设我们要微调一个 Llama-2-7B 模型，显存账单如下： 全量微调 ：约 112 GB（必须上 A100 集群）。 LoRA (16-bit) ：约 24 GB（需要 3090/4090）。 QLoRA (4-bit) ： 仅需 6 GB （RTX 3060 都能跑！）。 1.2 核心关系表 特性 LoRA (标准版) QLoRA (量化版) 底座模型 (Base Model) 加载为 16-bit (FP16) 加载为 4-bit (NF4) LoRA 适配器 (Adapter) 16-bit 16-bit (保持精度) 计算方式 纯 FP16 计算 混合精度 (4-bit 存储 $\\to$ 实时解压为 16-bit 计算) 创新点 矩阵分解 NF4 数据类型 + 双重量化 二、 QLoRA 的三大技术创新 (The Magic)QLoRA 之所以能在压到 4-bit 的同时还不掉点（精度损失微乎其微），靠的是以下三个黑科技。 2.1 4-bit NormalFloat (NF4)：为权重定制的容器传统的 4-bit 整数量化 (Int4) 是均匀切分的。但神经网络的权重分布是 正态分布 (Gaussian Distribution) ——大部分数值集中在 0 附近。 Int4 (均匀) ：在 0 附近切分太稀疏，浪费了大量精度在极值区域（Empty Tails）。 NF4 (分位数) ：根据正态分布设计刻度， 在 0 附近切分极密 。 通俗理解 ：Int4 是一把刻度均匀的直尺，而 NF4 是一把中间刻度极细、两头刻度稀疏的“变形尺”，专门用来量权重这种“中间多、两头少”的东西。 2.2 双重量化 (Double Quantization)量化不仅需要存权重，还需要存 量化常数 (Scale Constants) 。通常每 64 个参数共用一个 32-bit 的常数。虽然看起来不多，但在 65B 模型下，光这些常数就要占 3GB 显存！ QLoRA 的做法 ： 对“量化常数”再进行一次量化。 权重 $\\to$ 4-bit。产生常数 $C_1$ (32-bit)。 $C_1$ $\\to$ 8-bit。产生常数 $C_2$。 这就像把压缩包再压缩一次，平均每个参数只多占 0.127 bit。 2.3 分页优化器 (Paged Optimizers)利用 CPU 内存 (RAM) 来救急。当 GPU 显存出现峰值（Spike）快要 OOM 时，系统会自动把优化器状态 (Optimizer States) 搬运到 CPU 内存里，等需要更新参数时再搬回来。 三、 QLoRA 的工作流：左右互搏QLoRA 最精妙的地方在于它的 混合精度计算流 。它实现了“用 4-bit 存，用 16-bit 算”。 3.1 静态存储 vs 动态计算 底座模型 ：在显存里是 4-bit (NF4)。 绝对冻结，只读 。 LoRA 适配器 ：在显存里是 16-bit (BF16)。 可训练 。 3.2 训练时的数据流当数据流经某一层时： 解压 (Dequantize) ：将底座的 4-bit 权重 $\\times$ 量化常数 $\\rightarrow$ 瞬间还原为 16-bit 。 计算 (Compute) ： $X \\times W_{16bit}$ 。 释放 (Discard) ：计算完哪怕 1 毫秒后，立刻扔掉 16-bit 权重，显存里只留 4-bit 版本。 反向传播 ：梯度只传给 LoRA 部分更新。 这就是为什么 QLoRA 速度会慢 30% （因为要频繁解压），但 显存能省 60% 。 四、 实战代码 (bitsandbytes + peft)开启 QLoRA 只需要在加载模型时配置 BitsAndBytesConfig 。 import torchfrom transformers import AutoModelForCausalLM, BitsAndBytesConfigfrom peft import LoraConfig, get_peft_model# 1. QLoRA 核心配置bnb_config = BitsAndBytesConfig( load_in_4bit=True, # 开启 4-bit 加载 bnb_4bit_quant_type=\"nf4\", # 创新1: 使用 NF4 数据类型 bnb_4bit_use_double_quant=True, # 创新2: 开启双重量化 bnb_4bit_compute_dtype=torch.float16 # 创新3: 计算时解压为 FP16)# 2. 加载底座 (显存占用极低)model = AutoModelForCausalLM.from_pretrained( \"meta-llama/Llama-2-7b-hf\", quantization_config=bnb_config, device_map=\"auto\")# 3. 加载 LoRA (和普通 LoRA 一模一样)peft_config = LoraConfig( r=8, lora_alpha=16, target_modules=[\"q_proj\", \"v_proj\"], task_type=\"CAUSAL_LM\")model = get_peft_model(model, peft_config) 五、 常见误区解答Q: QLoRA 会修改原始模型文件吗？A: 绝对不会。 原始模型在硬盘上是只读的。训练过程中，显存里的底座模型也是冻结的。我们只训练并保存那几百 MB 的 LoRA 权重。 Q: 推理时需要解压吗？A: 是的。 推理逻辑和训练一样：实时解压 $\\rightarrow$ 计算 $\\rightarrow$ 释放。如果你想追求极致推理速度，可以把 LoRA 合并到底座后，统一量化为 GPTQ 或 AWQ 格式。 Q: 什么是量化常数？A: 就像地图的比例尺。4-bit 只能存 0~15 的整数，量化常数告诉我们“1”代表实际权重的“0.005”还是“100”。没有它，数据就是废纸。 参考文献 Dettmers, T., et al. (2023). QLoRA: Efficient Finetuning of Quantized LLMs. Hugging Face Blog. Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA.","tags":["NLP"],"categories":["深度学习"]},{"title":"深入浅出 LoRA：大模型微调的核心原理、数学证明与实战指南","path":"/2026/01/25/deep-dive-into-lora/","content":"在大模型（LLM）时代， PEFT (Parameter-Efficient Fine-Tuning, 参数高效微调) 几乎是每一位开发者必须掌握的技能。而其中最耀眼的明星，莫过于 LoRA (Low-Rank Adaptation) 。 本文将基于我与 AI 的一次深度对话，系统地梳理 LoRA 的核心原理。我们不仅会用直觉去理解它，更会通过 数学证明 和 Python 模拟 ，彻底搞懂为什么它需要除以 $r$ ，以及它在反向传播中是如何工作的。 一、 为什么我们需要 LoRA？1.1 “重写百科全书” vs “贴便利贴”想象 GPT-4 或 Llama-3 是一本厚达 1750 亿页的 百科全书 。如果你想让它变成一个“法律专家”： 全量微调 (Full Fine-Tuning) ：相当于你需要把这本书的每一页都重新编辑、重新印刷。 LoRA (Low-Rank Adaptation) ：我们在不破坏原书（冻结参数）的情况下，只是在相关的页面旁边贴上几张 透明的便利贴 （LoRA 模块），上面写着修正内容。 1.2 参数效率对比LoRA 的核心优势在于极致的参数压缩。对于一个 175B 的模型，全量微调需要更新所有参数，而 LoRA 仅需更新约 0.01% 的参数。 二、 LoRA 的数学原理 (The Math)2.1 矩阵分解：把“大”变“小”LoRA 的核心思想是 低秩分解 (Low-Rank Decomposition) 。假设模型中有一个巨大的权重矩阵 $W \\in \\mathbb{R}^{d \\times d}$ ，我们要微调它，产生一个增量 $\\Delta W$ 。 LoRA 假设这个 $\\Delta W$ 是“低秩”的，可以拆分为两个小矩阵的乘积：$$ \\Delta W = B \\times A $$ 矩阵 A (降维) ： $r \\times d$ 。负责把数据“压扁”，提取核心特征。 矩阵 B (升维) ： $d \\times r$ 。负责把数据“还原”，映射回原空间。 秩 r (Rank) ：通常很小（如 8, 16, 64）。 2.2 为什么必须除以 r？(关键证明)在 LoRA 的公式中，有一个关键的缩放系数：$$ y = W_0x + \\frac{\\alpha}{r} (BAx) $$ 为什么 $r$ 翻倍，数值会翻倍？如果不除以 $r$ ，会发生什么？ 证明一：方差叠加 (Forward Pass Variance)假设 LoRA 内部的参数 $A, B$ 服从独立同分布（I.I.D），方差为 $\\sigma^2$ 。矩阵乘法的每一位输出，本质上是对 $r$ 个通道的求和：$$ y_k = \\sum_{i=1}^{r} (B_{ki} A_{ij} x_j) $$ 根据统计学定律（相互独立的随机变量之和的方差等于它们方差之和）：$$ \\text{Var}(y) \\propto r \\cdot \\sigma^2 $$ 这意味着，信号的波动幅度（方差）会随着 $r$ 的增大而线性膨胀。如果不加以控制，输出值会变得极其不稳定。 证明二：梯度稳定性 (Gradient Stability)这才是最致命的问题。如果前向传播的值变大了， 反向传播的梯度也会变大 。 我们用 Python 模拟了不同 $r$ 值下的梯度范数（Gradient Norm）： 红线 (Without Scaling) ：随着 $r$ 增大，梯度呈指数级爆炸。这意味着如果你把 $r$ 从 8 改成 64，你必须手动把学习率缩小 8 倍，否则模型直接崩溃。 绿线 (With 1/r Scaling) ：无论 $r$ 怎么变，梯度大小保持恒定。这实现了 学习率解耦 (Learning Rate Decoupling) —— 一套超参数走天下。 2.3 初始化的艺术LoRA 的初始化策略非常讲究： 矩阵 A ： 高斯随机初始化 。 原因：必须打破对称性，让梯度能够流动。 矩阵 B ： 全零初始化 。 原因：保证在训练开始的一瞬间（Step 0），$BAx = 0$ 。 这就像一个阀门：虽然 A 里已经充满了随机噪声（水流），但 B 这个阀门关着，所以对原模型没有任何干扰。 三、 实战：微调数据格式3.1 Input / Output 对应关系在微调大模型时，核心逻辑对应着监督学习的 $X$ 和 $Y$ 。 Instruction + Input $\\rightarrow$ X (模型输入) Output $\\rightarrow$ Y (预期输出) { \"instruction\": \"请分析以下案情中的法律责任。\", \"input\": \"张三在喝酒后驾驶机动车...\", \"output\": \"张三的行为构成危险驾驶罪...\"} 3.2 System Prompt“System Prompt”（如 “You are a helpful assistant…”）通常不直接出现在 JSON 里的字段中，而是作为 Template 的一部分，拼接在 Instruction 之前。它充当了 “背景设定” 的角色。 四、 总结LoRA 是大模型微调领域的里程碑。它不仅仅是一个省显存的工具，更是一套优雅的数学解决方案。 秩 (Rank) ：决定了模型的“脑容量”。 $\\alpha/r$ (缩放) ：保证了训练动力学的一致性，防止梯度爆炸。 零初始化 ：保证了微调的平滑启动。 希望这篇文章能帮你彻底理解 LoRA 的数学直觉与工程实践。 参考文献 Hu, E. J., et al. (2021). LoRA: Low-Rank Adaptation of Large Language Models. Vaswani, A., et al. (2017). Attention Is All You Need.","tags":["NLP"],"categories":["深度学习"]},{"title":"相机几何完全指南：从世界坐标到图像坐标的完整推导","path":"/2026/01/19/camera-geometry-complete-guide/","content":"本文详细推导从3D世界坐标系到2D图像坐标系的完整数学过程，包括相机内外参数、旋转矩阵、单应矩阵的推导与分解，并提供完整的Python可视化代码。 📖 目录 坐标系统概述 从世界坐标系到相机坐标系（外参） 从相机坐标系到图像坐标系（内参） 旋转矩阵详解 单应矩阵推导 相机矩阵分解 Python完整实现 一、坐标系统概述在计算机视觉中，从3D世界到2D图像需要经过四个坐标系统的转换： 1.1 四个坐标系统1. 世界坐标系 (World Coordinate System) 符号：$(X_w, Y_w, Z_w)$ 描述：真实世界中的3D坐标系统 单位：通常为米(m)或毫米(mm) 原点：任意选定的参考点 2. 相机坐标系 (Camera Coordinate System) 符号：$(X_c, Y_c, Z_c)$ 描述：以相机光心为原点的3D坐标系 单位：米(m)或毫米(mm) 原点：相机光心 特点：$Z_c$ 轴为光轴方向 3. 图像坐标系 (Image Coordinate System) 符号：$(x, y)$ 描述：成像平面上的物理坐标 单位：毫米(mm) 原点：图像中心（主点） 4. 像素坐标系 (Pixel Coordinate System) 符号：$(u, v)$ 描述：数字图像的离散像素坐标 单位：像素(pixel) 原点：图像左上角 1.2 完整的投影公式从世界坐标到像素坐标的完整变换： $$\\begin{bmatrix} u \\ v \\ 1 \\end{bmatrix} \\sim K \\cdot [R|t] \\cdot \\begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \\end{bmatrix}$$ 其中： $K$：内参矩阵 (3×3) $[R|t]$：外参矩阵 (3×4) $\\sim$：表示齐次坐标意义下的相等（差一个尺度因子） 二、从世界坐标系到相机坐标系（外参矩阵）2.1 刚体变换世界坐标系到相机坐标系的转换是一个刚体变换（Rigid Body Transformation），包含旋转和平移： $$\\begin{bmatrix} X_c \\ Y_c \\ Z_c \\end{bmatrix} = R \\begin{bmatrix} X_w \\ Y_w \\ Z_w \\end{bmatrix} + t$$ 其中： $R \\in \\mathbb{R}^{3 \\times 3}$：旋转矩阵（Rotation Matrix） $t \\in \\mathbb{R}^{3 \\times 1}$：平移向量（Translation Vector） 2.2 齐次坐标表示使用齐次坐标可以将旋转和平移统一表示： $$\\begin{bmatrix} X_c \\ Y_c \\ Z_c \\ 1 \\end{bmatrix} =\\begin{bmatrix}R t \\0^T 1\\end{bmatrix}\\begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \\end{bmatrix}$$ 在实际应用中，我们通常使用 $3 \\times 4$ 的外参矩阵： $$\\begin{bmatrix} X_c \\ Y_c \\ Z_c \\end{bmatrix} =[R|t] \\begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \\end{bmatrix}$$ 其中： $$[R|t] = \\begin{bmatrix}r_{11} r_{12} r_{13} t_x \\r_{21} r_{22} r_{23} t_y \\r_{31} r_{32} r_{33} t_z\\end{bmatrix}$$ 2.3 外参的物理意义 **旋转矩阵 $R$**：描述相机坐标系相对于世界坐标系的方向 **平移向量 $t$**：描述相机光心在世界坐标系中的位置 自由度：6个（3个旋转 + 3个平移） 2.4 外参的逆变换从相机坐标系回到世界坐标系： $$\\begin{bmatrix} X_w \\ Y_w \\ Z_w \\end{bmatrix} = R^T \\left( \\begin{bmatrix} X_c \\ Y_c \\ Z_c \\end{bmatrix} - t \\right) = R^T \\begin{bmatrix} X_c \\ Y_c \\ Z_c \\end{bmatrix} - R^T t$$ 注意： $R^T = R^{-1}$（旋转矩阵的转置等于其逆） 相机在世界坐标系中的位置为 $C = -R^T t$ 三、从相机坐标系到图像坐标系（内参矩阵）3.1 针孔相机模型针孔相机模型是最基本的相机模型： 光心 O ────────────────→ 成像平面 ↗ ↗ 3D点 P 投影点 p 相似三角形: x/f = Xc/Zc, y/f = Yc/Zc因此: x = f·(Xc/Zc), y = f·(Yc/Zc) 透视投影公式根据相似三角形原理： $$\\frac{x}{f} = \\frac{X_c}{Z_c}, \\quad \\frac{y}{f} = \\frac{Y_c}{Z_c}$$ 其中 $f$ 是焦距（focal length），单位为毫米。 因此： $$x = f \\frac{X_c}{Z_c}, \\quad y = f \\frac{Y_c}{Z_c}$$ 3.2 从图像坐标到像素坐标图像坐标 $(x, y)$ 是物理坐标（毫米），需要转换为像素坐标 $(u, v)$： $$\\begin{cases}u = \\alpha x + c_x \\v = \\beta y + c_y\\end{cases}$$ 其中： $\\alpha = \\frac{1}{dx}$：x方向的像素密度（像素/毫米） $\\beta = \\frac{1}{dy}$：y方向的像素密度（像素/毫米） $(c_x, c_y)$：主点坐标（图像中心在像素坐标系中的位置） 3.3 内参矩阵推导将上述两步合并： $$\\begin{aligned}u = \\alpha \\cdot f \\frac{X_c}{Z_c} + c_x = f_x \\frac{X_c}{Z_c} + c_x \\v = \\beta \\cdot f \\frac{Y_c}{Z_c} + c_y = f_y \\frac{Y_c}{Z_c} + c_y\\end{aligned}$$ 其中： $f_x = \\alpha \\cdot f$：x方向焦距（像素单位） $f_y = \\beta \\cdot f$：y方向焦距（像素单位） 使用齐次坐标表示： $$Z_c \\begin{bmatrix} u \\ v \\ 1 \\end{bmatrix} =\\begin{bmatrix}f_x 0 c_x \\0 f_y c_y \\0 0 1\\end{bmatrix}\\begin{bmatrix} X_c \\ Y_c \\ Z_c \\end{bmatrix}$$ 3.4 完整的内参矩阵考虑像素倾斜（skew）的一般形式： $$K = \\begin{bmatrix}f_x s c_x \\0 f_y c_y \\0 0 1\\end{bmatrix}$$ 其中： $f_x, f_y$：焦距（像素单位） $c_x, c_y$：主点坐标（像素） $s$：倾斜系数（通常为0） 3.5 内参矩阵的性质 维度：$3 \\times 3$ 自由度：5个（现代相机中$s=0$，则为4个） 特点：上三角矩阵 物理意义：描述相机的内部几何特性 四、旋转矩阵详解4.1 旋转矩阵的定义与性质旋转矩阵 $R \\in SO(3)$ 是一个特殊正交矩阵，满足： 正交性：$R^T R = R R^T = I$ 行列式：$\\det(R) = 1$ 保持长度：$|Rv| = |v|$ 保持角度：$(Rv_1) \\cdot (Rv_2) = v_1 \\cdot v_2$ 4.2 基本旋转矩阵 绕X轴旋转（Roll）$$R_x(\\alpha) = \\begin{bmatrix}1 0 0 \\0 \\cos\\alpha -\\sin\\alpha \\0 \\sin\\alpha \\cos\\alpha\\end{bmatrix}$$ 绕Y轴旋转（Pitch）$$R_y(\\beta) = \\begin{bmatrix}\\cos\\beta 0 \\sin\\beta \\0 1 0 \\-\\sin\\beta 0 \\cos\\beta\\end{bmatrix}$$ 绕Z轴旋转（Yaw）$$R_z(\\gamma) = \\begin{bmatrix}\\cos\\gamma -\\sin\\gamma 0 \\\\sin\\gamma \\cos\\gamma 0 \\0 0 1\\end{bmatrix}$$ 4.3 欧拉角表示任意旋转可以分解为三个基本旋转的组合（有多种顺序）： ZYX欧拉角（常用）： $$R = R_z(\\gamma) R_y(\\beta) R_x(\\alpha)$$ 展开为： $$R = \\begin{bmatrix}\\cos\\gamma\\cos\\beta \\cos\\gamma\\sin\\beta\\sin\\alpha - \\sin\\gamma\\cos\\alpha \\cos\\gamma\\sin\\beta\\cos\\alpha + \\sin\\gamma\\sin\\alpha \\\\sin\\gamma\\cos\\beta \\sin\\gamma\\sin\\beta\\sin\\alpha + \\cos\\gamma\\cos\\alpha \\sin\\gamma\\sin\\beta\\cos\\alpha - \\cos\\gamma\\sin\\alpha \\-\\sin\\beta \\cos\\beta\\sin\\alpha \\cos\\beta\\cos\\alpha\\end{bmatrix}$$ ⚠️ 万向锁问题：当 $\\beta = \\pm 90°$ 时，会出现万向锁（Gimbal Lock）。 4.4 轴角表示（Axis-Angle）用旋转轴 $\\mathbf{n} = (n_x, n_y, n_z)^T$（单位向量）和旋转角 $\\theta$ 表示旋转。 罗德里格斯公式（Rodrigues’ Formula）： $$R = I + \\sin\\theta [\\mathbf{n}]_\\times + (1-\\cos\\theta)[\\mathbf{n}]_\\times^2$$ 其中 $[\\mathbf{n}]_\\times$ 是反对称矩阵： $$[\\mathbf{n}]_\\times = \\begin{bmatrix}0 -n_z n_y _z 0 -n_x \\-n_y n_x 0\\end{bmatrix}$$ 4.5 四元数表示（Quaternion）四元数 $q = q_0 + q_1i + q_2j + q_3k$ 可以避免万向锁，其中 $q_0^2 + q_1^2 + q_2^2 + q_3^2 = 1$。 四元数到旋转矩阵： $$R = \\begin{bmatrix}1-2(q_2^2+q_3^2) 2(q_1q_2-q_0q_3) 2(q_1q_3+q_0q_2) \\2(q_1q_2+q_0q_3) 1-2(q_1^2+q_3^2) 2(q_2q_3-q_0q_1) \\2(q_1q_3-q_0q_2) 2(q_2q_3+q_0q_1) 1-2(q_1^2+q_2^2)\\end{bmatrix}$$ 4.6 旋转矩阵的列向量含义 ⭐旋转矩阵 $R$ 的列向量具有重要的几何意义： $$R = \\begin{bmatrix} | | | \\ \\mathbf{r}_1 \\mathbf{r}_2 \\mathbf{r}_3 \\ | | | \\end{bmatrix}$$ 核心理解： $R$ 的第 $i$ 列 $\\mathbf{r}_i$ 表示世界坐标系的第 $i$ 个基向量在相机坐标系下的表示。 具体来说： **第1列 $\\mathbf{r}_1$**：世界坐标系的 X 轴方向在相机坐标系中的表示 **第2列 $\\mathbf{r}_2$**：世界坐标系的 Y 轴方向在相机坐标系中的表示 **第3列 $\\mathbf{r}_3$**：世界坐标系的 Z 轴方向在相机坐标系中的表示 推导： 世界坐标系的基向量为：$$\\mathbf{e}_1 = \\begin{bmatrix}1\\0\\0\\end{bmatrix}, \\quad\\mathbf{e}_2 = \\begin{bmatrix}0\\1\\0\\end{bmatrix}, \\quad\\mathbf{e}_3 = \\begin{bmatrix}0\\0\\1\\end{bmatrix}$$ 在相机坐标系中：$$R\\mathbf{e}_1 = \\mathbf{r}_1, \\quad R\\mathbf{e}_2 = \\mathbf{r}_2, \\quad R\\mathbf{e}_3 = \\mathbf{r}_3$$ 示例： 假设：$$R = \\begin{bmatrix}0.866 -0.500 0 \\0.500 0.866 0 \\0 0 1\\end{bmatrix}$$ 这是绕Z轴旋转30°的旋转矩阵。 $\\mathbf{r}_1 = [0.866, 0.500, 0]^T$：世界X轴在相机系中指向 $(0.866, 0.500, 0)$ $\\mathbf{r}_2 = [-0.500, 0.866, 0]^T$：世界Y轴在相机系中指向 $(-0.500, 0.866, 0)$ $\\mathbf{r}_3 = [0, 0, 1]^T$：世界Z轴在相机系中仍指向 $(0, 0, 1)$ 4.7 旋转矩阵的行向量含义相反地，$R^T$ 的列（即 $R$ 的行）表示相机坐标系的基向量在世界坐标系下的表示： $$R^T = \\begin{bmatrix} \\mathbf{r}_1^T - \\ \\mathbf{r}_2^T - \\ \\mathbf{r}_3^T -\\end{bmatrix}$$ 由于 $R^T = R^{-1}$，我们有： $\\mathbf{r}_1^T$：相机X轴在世界坐标系中的方向 $\\mathbf{r}_2^T$：相机Y轴在世界坐标系中的方向 $\\mathbf{r}_3^T$：相机Z轴（光轴）在世界坐标系中的方向 五、单应矩阵推导 5.1 单应矩阵的定义单应矩阵（Homography Matrix）$H$ 描述两个平面之间的投影变换关系： $$\\mathbf{p}’ \\sim H \\mathbf{p}$$ 即： $$\\begin{bmatrix} x’ \\ y’ \\ 1 \\end{bmatrix} \\sim\\begin{bmatrix}h_{11} h_{12} h_{13} \\h_{21} h_{22} h_{23} \\h_{31} h_{32} h_{33}\\end{bmatrix}\\begin{bmatrix} x \\ y \\ 1 \\end{bmatrix}$$ 5.2 单应矩阵的推导场景1：平面物体的投影假设世界坐标系中的平面满足 $Z_w = 0$，则： $$\\begin{bmatrix} X_c \\ Y_c \\ Z_c \\end{bmatrix} =R \\begin{bmatrix} X_w \\ Y_w \\ 0 \\end{bmatrix} + t =\\begin{bmatrix} r_1 r_2 r_3 \\end{bmatrix}\\begin{bmatrix} X_w \\ Y_w \\ 0 \\end{bmatrix} + t$$ $$= \\begin{bmatrix} r_1 r_2 \\end{bmatrix} \\begin{bmatrix} X_w \\ Y_w \\end{bmatrix} + t= [r_1 ; r_2 ; t] \\begin{bmatrix} X_w \\ Y_w \\ 1 \\end{bmatrix}$$ 因此像素坐标为： $$\\begin{bmatrix} u \\ v \\ 1 \\end{bmatrix} \\simK [r_1 ; r_2 ; t] \\begin{bmatrix} X_w \\ Y_w \\ 1 \\end{bmatrix}$$ 定义单应矩阵： $$H = K [r_1 ; r_2 ; t] = K \\begin{bmatrix} | | | \\ r_1 r_2 t \\ | | | \\end{bmatrix}$$ 5.3 单应矩阵的性质 维度：$3 \\times 3$ 自由度：8（因为尺度不定性，9个元素减去1个尺度） 可逆：$H^{-1}$ 描述逆向映射 非线性：由于齐次坐标的尺度不定性 5.4 单应矩阵的求解给定 $n$ 对对应点 $(x_i, y_i) \\leftrightarrow (x_i’, y_i’)$，每对点提供2个约束方程： $$\\begin{aligned}x_i’ = \\frac{h_{11}x_i + h_{12}y_i + h_{13}}{h_{31}x_i + h_{32}y_i + h_{33}} \\y_i’ = \\frac{h_{21}x_i + h_{22}y_i + h_{23}}{h_{31}x_i + h_{32}y_i + h_{33}}\\end{aligned}$$ 交叉相乘后得到线性方程： $$\\begin{bmatrix}-x_i -y_i -1 0 0 0 x_i’x_i x_i’y_i x_i’ \\0 0 0 -x_i -y_i -1 y_i’x_i y_i’y_i y_i’\\end{bmatrix}\\begin{bmatrix} h_{11} \\ h_{12} \\ h_{13} \\ h_{21} \\ h_{22} \\ h_{23} \\ h_{31} \\ h_{32} \\ h_{33} \\end{bmatrix} = 0$$ 最少需要4对点（8个方程）来求解8个未知数。 SVD求解： 构建矩阵 $A$（$2n \\times 9$），求解 $A\\mathbf{h} = 0$： $$A = \\begin{bmatrix}-x_1 -y_1 -1 0 0 0 x_1’x_1 x_1’y_1 x_1’ \\0 0 0 -x_1 -y_1 -1 y_1’x_1 y_1’y_1 y_1’ \\\\vdots \\vdots \\vdots \\vdots \\vdots \\vdots \\vdots \\vdots \\vdots \\-x_n -y_n -1 0 0 0 x_n’x_n x_n’y_n x_n’ \\0 0 0 -x_n -y_n -1 y_n’x_n y_n’y_n y_n’\\end{bmatrix}$$ 对 $A$ 进行SVD分解：$A = U\\Sigma V^T$，解为 $V$ 的最后一列。 5.5 从单应矩阵恢复R和t给定 $H = K[r_1 ; r_2 ; t]$，可以恢复旋转和平移： 计算：$[r_1 ; r_2 ; t] = K^{-1}H$ 归一化：由于尺度不定性，需要归一化：$$\\lambda = \\frac{1}{|K^{-1}H_{:,1}|} = \\frac{1}{|K^{-1}H_{:,2}|}$$ 提取：$$\\begin{aligned}r_1 = \\lambda K^{-1} H_{:,1} \\r_2 = \\lambda K^{-1} H_{:,2} \\t = \\lambda K^{-1} H_{:,3}\\end{aligned}$$ 计算第三列：$$r_3 = r_1 \\times r_2$$ 确保正交性：由于噪声，$[r_1, r_2, r_3]$ 可能不完全正交，需要SVD校正：$$[r_1, r_2, r_3] = U V^T$$其中 $U\\Sigma V^T$ 是 $[r_1, r_2, r_3]$ 的SVD分解。 六、相机矩阵分解6.1 投影矩阵完整的投影矩阵为： $$P = K[R|t] = \\begin{bmatrix}p_{11} p_{12} p_{13} p_{14} \\p_{21} p_{22} p_{23} p_{24} \\p_{31} p_{32} p_{33} p_{34}\\end{bmatrix}$$ 目标：从 $P$ 分解出 $K$、$R$、$t$。 6.2 RQ分解方法投影矩阵的前3列可以写为： $$M = P_{:,1:3} = KR$$ 其中 $K$ 是上三角矩阵，$R$ 是正交矩阵。 RQ分解步骤： 将矩阵翻转 进行QR分解 将结果翻转回来 Python实现： import numpy as npfrom scipy.linalg import rq# RQ分解K, R = rq(M)# 确保K的对角元素为正T = np.diag(np.sign(np.diag(K)))K = K @ TR = T @ R# 确保det(R) = 1if np.linalg.det(R) 0: R = -R K = -K# 归一化KK = K / K[2, 2] 6.3 提取平移向量$$t = K^{-1} P_{:,3}$$ 6.4 SVD分解验证旋转矩阵为了确保 $R$ 是有效的旋转矩阵（正交且行列式为1）： U, S, Vt = np.linalg.svd(R)R_corrected = U @ Vtif np.linalg.det(R_corrected) 0: U[:, -1] *= -1 R_corrected = U @ Vt 6.5 完整的分解流程def decompose_projection_matrix(P): \"\"\" 分解投影矩阵 P = K[R|t] 参数: P: 3x4 投影矩阵 返回: K: 3x3 内参矩阵 R: 3x3 旋转矩阵 t: 3x1 平移向量 camera_center: 3x1 相机中心在世界坐标系中的位置 \"\"\" # 分离前3列 M = P[:, :3] p4 = P[:, 3] # RQ分解 K, R = rq(M) # 确保K的对角元素为正 T = np.diag(np.sign(np.diag(K))) K = K @ T R = T @ R # 确保det(R) = 1 if np.linalg.det(R) 0: R = -R K = -K # 归一化K K = K / K[2, 2] # 提取平移向量 t = np.linalg.inv(K) @ p4 # 计算相机中心 camera_center = -R.T @ t return K, R, t, camera_center 七、Python完整实现与可视化7.1 坐标系可视化import numpy as npimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Dfrom matplotlib.patches import FancyArrowPatchfrom mpl_toolkits.mplot3d.proj3d import proj_transformclass Arrow3D(FancyArrowPatch): \"\"\"3D箭头类\"\"\" def __init__(self, x, y, z, dx, dy, dz, *args, **kwargs): super().__init__((0, 0), (0, 0), *args, **kwargs) self._xyz = (x, y, z) self._dxdydz = (dx, dy, dz) def draw(self, renderer): x1, y1, z1 = self._xyz dx, dy, dz = self._dxdydz x2, y2, z2 = (x1 + dx, y1 + dy, z1 + dz) xs, ys, zs = proj_transform((x1, x2), (y1, y2), (z1, z2), self.axes.M) self.set_positions((xs[0], ys[0]), (xs[1], ys[1])) super().draw(renderer) def do_3d_projection(self, renderer=None): x1, y1, z1 = self._xyz dx, dy, dz = self._dxdydz x2, y2, z2 = (x1 + dx, y1 + dy, z1 + dz) xs, ys, zs = proj_transform((x1, x2), (y1, y2), (z1, z2), self.axes.M) self.set_positions((xs[0], ys[0]), (xs[1], ys[1])) return np.min(zs)def draw_coordinate_frame(ax, origin, rotation, scale=1.0, labels=['X', 'Y', 'Z'], colors=['r', 'g', 'b'], linewidth=2): \"\"\"绘制坐标系\"\"\" axes = rotation @ np.eye(3) * scale for i, (color, label) in enumerate(zip(colors, labels)): arrow = Arrow3D(origin[0], origin[1], origin[2], axes[0, i], axes[1, i], axes[2, i], mutation_scale=20, lw=linewidth, arrowstyle='-|', color=color) ax.add_artist(arrow) end_point = origin + axes[:, i] ax.text(end_point[0], end_point[1], end_point[2], label, fontsize=12, weight='bold')def draw_camera(ax, position, rotation, scale=0.5, color='blue'): \"\"\"绘制相机模型\"\"\" # 相机锥体的顶点（在相机坐标系下） camera_points = np.array([ [0, 0, 0], # 光心 [-1, -1, 2], # 图像平面四个角 [1, -1, 2], [1, 1, 2], [-1, 1, 2] ]) * scale # 转换到世界坐标系 world_points = (rotation @ camera_points.T).T + position # 绘制相机锥体 for i in range(1, 5): ax.plot([world_points[0, 0], world_points[i, 0]], [world_points[0, 1], world_points[i, 1]], [world_points[0, 2], world_points[i, 2]], color='black', linewidth=1) # 图像平面的四条边 for i in range(1, 5): next_i = i + 1 if i 4 else 1 ax.plot([world_points[i, 0], world_points[next_i, 0]], [world_points[i, 1], world_points[next_i, 1]], [world_points[i, 2], world_points[next_i, 2]], color=color, linewidth=2) return world_pointsdef visualize_coordinate_systems(): \"\"\"可视化四个坐标系统\"\"\" fig = plt.figure(figsize=(18, 12)) # 世界坐标系 world_origin = np.array([0, 0, 0]) world_rotation = np.eye(3) # 相机位置和姿态 camera_position = np.array([3, 2, 4]) theta_y = -np.pi/6 # 绕Y轴旋转 theta_x = -np.pi/9 # 绕X轴旋转 Ry = np.array([[np.cos(theta_y), 0, np.sin(theta_y)], [0, 1, 0], [-np.sin(theta_y), 0, np.cos(theta_y)]]) Rx = np.array([[1, 0, 0], [0, np.cos(theta_x), -np.sin(theta_x)], [0, np.sin(theta_x), np.cos(theta_x)]]) camera_rotation = Ry @ Rx # 绘制3D场景 ax = fig.add_subplot(221, projection='3d') ax.set_title('世界坐标系 → 相机坐标系', fontsize=14, weight='bold') # 绘制世界坐标系 draw_coordinate_frame(ax, world_origin, world_rotation, scale=2.0, labels=['Xw', 'Yw', 'Zw'], colors=['red', 'green', 'blue'], linewidth=3) # 绘制相机坐标系 draw_coordinate_frame(ax, camera_position, camera_rotation, scale=1.5, labels=['Xc', 'Yc', 'Zc'], colors=['darkred', 'darkgreen', 'darkblue'], linewidth=2) # 绘制相机模型 draw_camera(ax, camera_position, camera_rotation, scale=0.6, color='cyan') # 绘制一个3D点 point_world = np.array([1.5, 1.5, 1.0]) ax.scatter(*point_world, c='purple', s=200, marker='o', edgecolors='black', linewidths=2) ax.text(point_world[0]+0.2, point_world[1]+0.2, point_world[2]+0.2, 'P(Xw,Yw,Zw)', fontsize=11, weight='bold', color='purple') # 投影线 ax.plot([point_world[0], camera_position[0]], [point_world[1], camera_position[1]], [point_world[2], camera_position[2]], 'purple', linestyle='--', linewidth=2, alpha=0.6) ax.set_xlabel('X', fontsize=12) ax.set_ylabel('Y', fontsize=12) ax.set_zlabel('Z', fontsize=12) ax.set_xlim([-1, 5]) ax.set_ylim([-1, 5]) ax.set_zlim([-1, 5]) ax.view_init(elev=20, azim=45) ax.grid(True, alpha=0.3) plt.tight_layout() plt.savefig('coordinate_systems_3d.png', dpi=300, bbox_inches='tight') plt.show()# 运行可视化visualize_coordinate_systems() 保存为 visualize_coordinates.py 7.2 完整的相机几何计算示例# camera_geometry_demo.pyimport numpy as npfrom scipy.linalg import rqclass CameraGeometry: \"\"\"相机几何计算类\"\"\" def __init__(self, fx, fy, cx, cy, width, height): \"\"\" 初始化相机内参 参数: fx, fy: 焦距（像素） cx, cy: 主点（像素） width, height: 图像尺寸 \"\"\" self.K = np.array([ [fx, 0, cx], [0, fy, cy], [0, 0, 1] ]) self.width = width self.height = height @staticmethod def rotation_matrix_from_euler(roll, pitch, yaw, order='xyz'): \"\"\"从欧拉角创建旋转矩阵\"\"\" Rx = np.array([ [1, 0, 0], [0, np.cos(roll), -np.sin(roll)], [0, np.sin(roll), np.cos(roll)] ]) Ry = np.array([ [np.cos(pitch), 0, np.sin(pitch)], [0, 1, 0], [-np.sin(pitch), 0, np.cos(pitch)] ]) Rz = np.array([ [np.cos(yaw), -np.sin(yaw), 0], [np.sin(yaw), np.cos(yaw), 0], [0, 0, 1] ]) if order == 'xyz': return Rz @ Ry @ Rx elif order == 'zyx': return Rx @ Ry @ Rz else: raise ValueError(\"Order must be 'xyz' or 'zyx'\") @staticmethod def rotation_matrix_from_axis_angle(axis, theta): \"\"\"从轴角创建旋转矩阵（罗德里格斯公式）\"\"\" axis = axis / np.linalg.norm(axis) # 归一化 K = np.array([ [0, -axis[2], axis[1]], [axis[2], 0, -axis[0]], [-axis[1], axis[0], 0] ]) R = np.eye(3) + np.sin(theta) * K + (1 - np.cos(theta)) * (K @ K) return R def project_point(self, point_world, R, t): \"\"\" 将世界坐标点投影到图像 参数: point_world: 3D点（世界坐标系） R: 旋转矩阵 t: 平移向量 返回: pixel: 像素坐标 (u, v) \"\"\" # 世界坐标 - 相机坐标 point_camera = R @ point_world + t # 相机坐标 - 图像坐标（透视投影） if point_camera[2] = 0: raise ValueError(\"Point is behind the camera\") # 投影到像素坐标 point_homo = self.K @ point_camera pixel = point_homo[:2] / point_homo[2] return pixel def compute_projection_matrix(self, R, t): \"\"\"计算投影矩阵 P = K[R|t]\"\"\" return self.K @ np.hstack([R, t.reshape(-1, 1)]) def decompose_projection_matrix(self, P): \"\"\"分解投影矩阵\"\"\" M = P[:, :3] p4 = P[:, 3] # RQ分解 K, R = rq(M) # 确保K的对角元素为正 T = np.diag(np.sign(np.diag(K))) K = K @ T R = T @ R # 确保det(R) = 1 if np.linalg.det(R) 0: R = -R K = -K # 归一化K K = K / K[2, 2] # 提取平移向量 t = np.linalg.inv(K) @ p4 # 相机中心 camera_center = -R.T @ t return K, R, t, camera_center def compute_homography(self, R, t, n, d): \"\"\" 计算平面的单应矩阵 参数: R, t: 外参 n: 平面法向量（世界坐标系） d: 平面到原点的距离 返回: H: 3x3 单应矩阵 \"\"\" H = self.K @ (R - (t @ n.T) / d) @ np.linalg.inv(self.K) return H / H[2, 2] # 归一化# 示例：完整的投影流程if __name__ == \"__main__\": # 创建相机 camera = CameraGeometry( fx=800, fy=800, # 焦距 cx=320, cy=240, # 主点 width=640, height=480 # 图像尺寸 ) print(\"=\" * 60) print(\"相机内参矩阵 K:\") print(camera.K) print() # 创建外参 roll, pitch, yaw = np.deg2rad([10, 20, 30]) R = camera.rotation_matrix_from_euler(roll, pitch, yaw, order='zyx') t = np.array([1.0, 2.0, 5.0]) print(\"旋转矩阵 R:\") print(R) print(f\" det(R) = {np.linalg.det(R):.6f} (应该为1)\") print(f\"R^T @ R =\") print(R.T @ R) print(\"(应该为单位矩阵)\") print() print(\"平移向量 t:\") print(t) print() # 旋转矩阵列向量的含义 print(\"=\" * 60) print(\"旋转矩阵列向量的几何意义:\") print(\"第1列（世界X轴在相机系中的方向）:\", R[:, 0]) print(\"第2列（世界Y轴在相机系中的方向）:\", R[:, 1]) print(\"第3列（世界Z轴在相机系中的方向）:\", R[:, 2]) print() # 相机在世界坐标系中的位置 camera_center = -R.T @ t print(f\"相机中心在世界坐标系中的位置: {camera_center}\") print() # 投影一个3D点 point_3d = np.array([2.0, 3.0, 4.0]) print(\"=\" * 60) print(f\"3D点（世界坐标）: {point_3d}\") try: pixel = camera.project_point(point_3d, R, t) print(f\"投影到图像（像素坐标）: ({pixel[0]:.2f}, {pixel[1]:.2f})\") except ValueError as e: print(f\"投影失败: {e}\") print() # 计算并分解投影矩阵 P = camera.compute_projection_matrix(R, t) print(\"=\" * 60) print(\"投影矩阵 P = K[R|t]:\") print(P) print() # 分解投影矩阵 K_recovered, R_recovered, t_recovered, center_recovered = \\ camera.decompose_projection_matrix(P) print(\"分解后的内参矩阵 K:\") print(K_recovered) print(\" 分解后的旋转矩阵 R:\") print(R_recovered) print(\" 分解后的平移向量 t:\") print(t_recovered) print(\" 分解后的相机中心:\") print(center_recovered) print() # 验证 print(\"=\" * 60) print(\"验证分解结果:\") print(f\"K误差: {np.linalg.norm(camera.K - K_recovered):.2e}\") print(f\"R误差: {np.linalg.norm(R - R_recovered):.2e}\") print(f\"t误差: {np.linalg.norm(t - t_recovered):.2e}\") print() # 单应矩阵示例 print(\"=\" * 60) print(\"单应矩阵示例（地平面 Z=0）:\") n = np.array([0, 0, 1]) # 平面法向量 d = 0 # 平面距离原点 # 对于Z=0平面，单应矩阵为 H = K[r1 r2 t] H = camera.K @ np.column_stack([R[:, 0], R[:, 1], t]) H = H / H[2, 2] # 归一化 print(\"单应矩阵 H:\") print(H) print() # 测试单应变换 point_2d_world = np.array([2.0, 3.0, 1.0]) # 世界平面上的点（齐次坐标） point_2d_image = H @ point_2d_world point_2d_image = point_2d_image / point_2d_image[2] print(f\"平面上的点（世界坐标）: ({point_2d_world[0]}, {point_2d_world[1]})\") print(f\"通过单应矩阵投影到图像: ({point_2d_image[0]:.2f}, {point_2d_image[1]:.2f})\") # 验证：通过完整投影 point_3d_on_plane = np.array([point_2d_world[0], point_2d_world[1], 0.0]) pixel_verify = camera.project_point(point_3d_on_plane, R, t) print(f\"通过完整投影验证: ({pixel_verify[0]:.2f}, {pixel_verify[1]:.2f})\") print(f\"误差: {np.linalg.norm(point_2d_image[:2] - pixel_verify):.2e}\") 保存为 camera_geometry_demo.py 八、实际应用场景8.1 相机标定通过拍摄标定板（如棋盘格）的多张图像，可以估计相机内参和畸变参数。 8.2 3D重建利用多视图几何和外参矩阵，可以从多张图像重建3D场景。 8.3 增强现实（AR）通过相机位姿估计，可以将虚拟物体精确地叠加到真实场景中。 8.4 视觉SLAM同时定位与地图构建（SLAM）需要实时估计相机的位姿（外参）。 8.5 图像拼接利用单应矩阵可以将多张图像拼接成全景图。 九、总结本文详细推导了从3D世界坐标到2D图像坐标的完整数学过程： **外参矩阵 $[R|t]$**：描述相机在世界中的位置和方向（6自由度） **内参矩阵 $K$**：描述相机的内部几何参数（4-5自由度） **旋转矩阵 $R$**：可用欧拉角、轴角、四元数表示，其列向量表示世界坐标系的基向量在相机系中的表示 **单应矩阵 $H$**：描述平面到图像的投影变换（8自由度） 矩阵分解：可以从投影矩阵恢复出内外参数 完整的投影公式为： $$s \\begin{bmatrix} u \\ v \\ 1 \\end{bmatrix} =\\begin{bmatrix}f_x 0 c_x \\0 f_y c_y \\0 0 1\\end{bmatrix}\\begin{bmatrix}r_{11} r_{12} r_{13} t_x \\r_{21} r_{22} r_{23} t_y \\r_{31} r_{32} r_{33} t_z\\end{bmatrix}\\begin{bmatrix} X_w \\ Y_w \\ Z_w \\ 1 \\end{bmatrix}$$ 配套的Python代码提供了完整的实现和可视化，可以直接用于实际项目。 参考资料 Hartley, R., Zisserman, A. (2003). Multiple View Geometry in Computer Vision. Cambridge University Press. Szeliski, R. (2010). Computer Vision: Algorithms and Applications. Springer. OpenCV Documentation: Camera Calibration and 3D Reconstruction 附录：运行代码将上述Python代码保存为对应的文件名，然后运行： # 安装依赖pip install numpy matplotlib scipy# 运行可视化python visualize_coordinates.py# 运行完整示例python camera_geometry_demo.py 关键词：相机几何、坐标变换、旋转矩阵、内参矩阵、外参矩阵、单应矩阵、投影矩阵分解、计算机视觉 博客标签：#计算机视觉 #相机标定 #多视几何 #矩阵分解 #Python","tags":["计算机视觉"],"categories":["计算机视觉"]},{"title":"从图像到俯视图：完整实现指南","path":"/2026/01/19/image-to-bird-eye-view/","content":"从输入图像到输出鸟瞰图的完整实现 - 适合所有水平的学习者 🎯 教程简介本教程将带你从零开始学习如何把一张斜着拍摄的图像（比如路面照片）转换成从正上方看的俯视图（鸟瞰图）。 你将学到什么？infographic list-row-simple-horizontal-arrowdata title 完整学习路径 items - label 输入图像 desc 斜着看的照片 - label 理解透视 desc 坐标系统 - label 计算变换 desc 单应性矩阵 - label 实现代码 desc Python+OpenCV - label 输出俯视图 desc 鸟瞰图 📚 第一章：基础概念1.1 三种坐标系统在图像处理中，我们需要理解三种坐标系统的转换关系： infographic list-grid-badge-carddata title 三种坐标系 items - label 世界坐标系 desc 固定在地面，单位：米 icon mdi:earth - label 相机坐标系 desc 跟随相机移动旋转 icon mdi:camera - label 图像坐标系 desc 2D像素平面 icon mdi:image 世界坐标系 (World Coordinates)生活例子：就像地图的经纬度，固定不变的参考系。 # 世界坐标中的一个点（3米，5米，0米）point_world = np.array([3.0, 5.0, 0.0]) # [X, Y, Z] 特点： 原点：通常选在地面某个固定位置 X轴：向右（东方） Y轴：向前（北方） Z轴：向上（天空） 相机坐标系 (Camera Coordinates) 核心规则： 原点：在相机镜头中心 Z轴：镜头朝向（光轴方向） X轴：相机向右 Y轴：相机向下（⚠️ 注意：向下！） 为什么Y轴向下？ 传统原因：┌─────────┐│ (0,0) │ ← 图像左上角是(0,0)│ ↓ Y ││ → X │└─────────┘ 图像坐标系 (Image Coordinates)就是我们看到的照片像素坐标： # 图像中的一个像素点pixel = (320, 240) # (u, v) 像素坐标 单位：像素 (pixel) 1.2 透视投影原理 小孔成像模型是相机的数学抽象： 3D世界 ────── 小孔 ────── 2D图像(X,Y,Z) 相机 (u,v) 投影过程： 数学公式： $$\\begin{bmatrix} u \\ v \\ 1 \\end{bmatrix} = K \\begin{bmatrix} R | t \\end{bmatrix} \\begin{bmatrix} X \\ Y \\ Z \\ 1 \\end{bmatrix}$$ 其中： K：相机内参矩阵（焦距、主点） R：旋转矩阵（相机朝向） t：平移向量（相机位置） 1.3 透视效果 为什么远处的物体看起来小？ 近处的车 ━━━━ 看起来大中间的车 ━━━ 看起来中等远处的车 ━━ 看起来小 这就是透视投影的效果！ 📐 第二章：消失点理论2.1 什么是消失点？ 生活例子：站在铁轨中间拍照，两条平行的铁轨在远处看起来会”相交”。 定义： 现实中平行的线，在图像中延伸后相交的点，就是消失点 (Vanishing Point)。 2.2 消失点的计算原理：两条平行线的交点 def compute_vanishing_point(line1_pts, line2_pts): \"\"\" 计算两条平行线的消失点 参数： line1_pts: 直线1的两个点 [[x1,y1], [x2,y2]] line2_pts: 直线2的两个点 [[x3,y3], [x4,y4]] 返回： vp: 消失点坐标 [vx, vy] \"\"\" # 转为齐次坐标 p1 = np.array([line1_pts[0][0], line1_pts[0][1], 1]) p2 = np.array([line1_pts[1][0], line1_pts[1][1], 1]) p3 = np.array([line2_pts[0][0], line2_pts[0][1], 1]) p4 = np.array([line2_pts[1][0], line2_pts[1][1], 1]) # 计算直线（叉乘） L1 = np.cross(p1, p2) L2 = np.cross(p3, p4) # 计算交点（叉乘） vp_homo = np.cross(L1, L2) # 归一化 vp = vp_homo[:2] / vp_homo[2] return vp 2.3 从消失点到相机参数 核心思想：消失点的位置反映了相机的朝向！ def estimate_camera_params(vp, img_shape, cx=None, cy=None): \"\"\" 从消失点估计相机参数 参数： vp: 消失点坐标 [vx, vy] img_shape: 图像大小 (height, width) cx, cy: 主点坐标（可选） 返回： focal: 估计的焦距 yaw: yaw角度（度） pitch: pitch角度（度） \"\"\" h, w = img_shape[:2] # 主点（默认图像中心） if cx is None: cx = w / 2 if cy is None: cy = h / 2 vx, vy = vp # 计算偏移 dx = vx - cx dy = vy - cy # 估计焦距 focal = np.sqrt(w**2 + h**2) # 计算角度 yaw = np.arctan2(dx, focal) pitch = np.arctan2(dy, focal) # 转为角度 yaw_deg = np.degrees(yaw) pitch_deg = np.degrees(pitch) return focal, yaw_deg, pitch_deg 2.4 几何证明 核心公式： $$\\text{yaw} = \\arctan\\left(\\frac{v_x - c_x}{f}\\right)$$ $$\\text{pitch} = \\arctan\\left(\\frac{v_y - c_y}{f}\\right)$$ 🔄 第三章：单应性变换3.1 什么是单应性？生活例子： 相机视角（透视图）： | | 近处的条纹 - 看起来很宽 | | | | | | | | 远处的条纹 - 看起来很窄鸟瞰视角： |||||||| 所有条纹 - 宽度一样 |||||||| |||||||| 单应性变换就是把第一种视角转换成第二种视角的数学方法！ 3.2 单应性矩阵数学定义： $$\\begin{bmatrix} x’ \\ y’ \\ 1 \\end{bmatrix} = H \\begin{bmatrix} x \\ y \\ 1 \\end{bmatrix}$$ 其中 H 是一个 3×3 的矩阵： ┌ ┐ │ h11 h12 h13 │H = │ h21 h22 h23 │ │ h31 h32 h33 │ └ ┘ 特点： 9个数字，但只有8个自由度 可以表示旋转、缩放、平移、透视变换 3.3 DLT算法求解H矩阵核心思想：用4对点构建方程组，用SVD求解。 def normalize_points(points): \"\"\" 归一化点集（提高数值稳定性） \"\"\" # 计算中心点 centroid = np.mean(points, axis=0) # 平移到原点 centered = points - centroid # 计算平均距离 avg_dist = np.mean(np.sqrt(np.sum(centered**2, axis=1))) # 缩放因子 scale = np.sqrt(2) / (avg_dist + 1e-8) # 构建归一化矩阵 T = np.array([ [scale, 0, -scale * centroid[0]], [0, scale, -scale * centroid[1]], [0, 0, 1] ]) return Tdef compute_homography_dlt(src_pts, dst_pts): \"\"\" 使用DLT算法计算单应性矩阵 参数： src_pts: 源平面点 (N×2) dst_pts: 目标平面点 (N×2) 返回： H: 3×3 单应性矩阵 \"\"\" assert len(src_pts) = 4, \"至少需要4个点！\" # 归一化（提高数值稳定性） T_src = normalize_points(src_pts) T_dst = normalize_points(dst_pts) # 归一化点 src_homo = np.column_stack([src_pts, np.ones(len(src_pts))]) dst_homo = np.column_stack([dst_pts, np.ones(len(dst_pts))]) src_norm = (T_src @ src_homo.T).T dst_norm = (T_dst @ dst_homo.T).T src_norm_2d = src_norm[:, :2] / src_norm[:, 2:3] dst_norm_2d = dst_norm[:, :2] / dst_norm[:, 2:3] # 构建矩阵A A = [] for i in range(len(src_norm_2d)): x1, y1 = src_norm_2d[i] x2, y2 = dst_norm_2d[i] # 两个约束方程 A.append([ -x1, -y1, -1, 0, 0, 0, x2*x1, x2*y1, x2 ]) A.append([ 0, 0, 0, -x1, -y1, -1, y2*x1, y2*y1, y2 ]) A = np.array(A) # SVD分解 U, S, Vt = np.linalg.svd(A) # 最小奇异值对应的向量 h = Vt[-1, :] # 重构H矩阵 H_norm = h.reshape(3, 3) # 反归一化 H = np.linalg.inv(T_dst) @ H_norm @ T_src # 归一化（使h33=1） H = H / H[2, 2] return H 3.4 优化过程 步骤解析： infographic sequence-steps-simpledata title DLT算法流程 items - label 归一化点 desc 提高数值稳定性 - label 构建方程组 desc Ah = 0 - label SVD分解 desc 求最小奇异值 - label 提取H矩阵 desc 重构3×3矩阵 - label 反归一化 desc 恢复原始尺度 💻 第四章：完整代码实现4.1 环境搭建Python环境要求： # Python版本Python 3.7+# 核心库numpy=1.19.0opencv-python=4.5.0matplotlib=3.3.0 安装步骤： # 创建虚拟环境（推荐）python -m venv cv_env# 激活虚拟环境# Mac/Linux:source cv_env/bin/activate# Windows:cv_env\\Scripts\\activate# 安装依赖pip install numpy opencv-python matplotlib# 验证安装python -c \"import cv2; print(cv2.__version__)\" 4.2 项目结构bird_eye_view_project/├── input/ # 输入图像│ └── dashcam.jpg├── output/ # 输出结果│ ├── marked.jpg│ ├── bird_view.jpg│ └── result.jpg├── main.py # 主程序└── requirements.txt # 依赖列表 4.3 交互式鸟瞰图生成器\"\"\"从图像到俯视图：完整实现功能：交互式选点 → 计算单应性矩阵 → 生成鸟瞰图\"\"\"import cv2import numpy as npimport matplotlib.pyplot as pltfrom pathlib import Pathclass InteractiveBirdEyeView: \"\"\" 交互式鸟瞰图生成器 使用方法： 1. 点击图像选择4个点 2. 自动计算单应性矩阵 3. 生成鸟瞰图 \"\"\" def __init__(self, img): \"\"\" 初始化 参数： img: 输入图像（numpy数组） \"\"\" self.img = img.copy() self.display_img = img.copy() self.points = [] self.max_points = 4 self.window_name = '选择4个点（按顺序：左下→右下→右上→左上）' def mouse_callback(self, event, x, y, flags, param): \"\"\"鼠标回调函数\"\"\" if event == cv2.EVENT_LBUTTONDOWN: if len(self.points) self.max_points: self.points.append([x, y]) print(f\"点 {len(self.points)}: ({x}, {y})\") self.draw_points() # 如果已经选择了4个点，自动处理 if len(self.points) == self.max_points: print(\" ✅ 已选择4个点，计算中...\") cv2.waitKey(1000) cv2.destroyWindow(self.window_name) def draw_points(self): \"\"\"绘制已选择的点\"\"\" self.display_img = self.img.copy() for i, pt in enumerate(self.points): # 绘制圆点 cv2.circle(self.display_img, tuple(pt), 8, (0, 255, 0), -1) # 绘制序号 cv2.putText( self.display_img, str(i+1), (pt[0]+15, pt[1]), cv2.FONT_HERSHEY_SIMPLEX, 0.8, (0, 0, 255), ```python ) # 绘制连线 if len(self.points) 1: pts = np.array(self.points, np.int32) cv2.polylines( self.display_img, [pts], len(self.points) == 4, (255, 0, 0), ) # 显示提示信息 info = f\"已选择 {len(self.points)}/{self.max_points} 个点\" cv2.putText( self.display_img, info, (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), ) cv2.imshow(self.window_name, self.display_img) def select_points(self): \"\"\" 交互式选择点 返回： points: 4×2 numpy数组，或None（如果取消） \"\"\" cv2.namedWindow(self.window_name, cv2.WINDOW_NORMAL) cv2.setMouseCallback(self.window_name, self.mouse_callback) print(\" \" + \"=\"*60) print(\"📌 请在图像上依次点击4个点\") print(\" 顺序：左下 → 右下 → 右上 → 左上\") print(\" 提示：选择路面上的矩形区域\") print(\" 按ESC可以取消\") print(\"=\"*60 + \" \") self.draw_points() while len(self.points) self.max_points: key = cv2.waitKey(1) 0xFF if key == 27: # ESC print(\"❌ 已取消\") cv2.destroyAllWindows() return None return np.array(self.points, dtype=np.float32) def compute_bird_view(self, bird_w=400, bird_h=600, margin=50): \"\"\" 计算并显示鸟瞰图 参数： bird_w: 鸟瞰图宽度（像素） bird_h: 鸟瞰图高度（像素） margin: 边距（像素） 返回： result: 包含原图和鸟瞰图的字典 \"\"\" # 选择点 src_pts = self.select_points() if src_pts is None: return None print(\" 🔄 处理中...\") # 定义目标点（俯视图中的矩形） dst_pts = np.array([ [margin, bird_h - margin], [bird_w - margin, bird_h - margin], [bird_w - margin, margin], [margin, margin] ], dtype=np.float32) # 计算单应性矩阵 H = compute_homography_dlt(src_pts, dst_pts) print(\" 📐 单应性矩阵 H:\") print(H) # 变换图像 bird_view = cv2.warpPerspective( self.img, H, (bird_w, bird_h), flags=cv2.INTER_LINEAR ) # 在原图上绘制选择的区域 marked_img = self.img.copy() pts = src_pts.astype(np.int32).reshape((-1, 1, 2)) cv2.polylines(marked_img, [pts], True, (0, 255, 0), 3) for i, pt in enumerate(src_pts): cv2.circle(marked_img, tuple(pt.astype(int)), 10, (0, 255, 0), -1) cv2.putText( marked_img, str(i+1), tuple(pt.astype(int)), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 0, 255), ) # 返回结果 result = { 'original': self.img, 'marked': marked_img, 'bird_view': bird_view, 'homography': H, 'src_points': src_pts, 'dst_points': dst_pts } print(\" ✅ 处理完成！\") return resultdef visualize_results(result, save_path=None): \"\"\" 可视化结果 参数： result: compute_bird_view返回的结果字典 save_path: 保存路径（可选） \"\"\" if result is None: print(\"没有结果可显示\") return # 创建对比图 h1, w1 = result['marked'].shape[:2] h2, w2 = result['bird_view'].shape[:2] # 调整大小使高度一致 target_h = 400 scale1 = target_h / h1 scale2 = target_h / h2 img1_resized = cv2.resize(result['marked'], (int(w1*scale1), target_h)) img2_resized = cv2.resize(result['bird_view'], (int(w2*scale2), target_h)) # 水平拼接 combined = np.hstack([img1_resized, img2_resized]) # 添加标题 cv2.putText( combined, 'Original (Perspective)', (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), ) cv2.putText( combined, 'Bird Eye View (Top-down)', (int(w1*scale1) + 10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 0), ) # 显示 cv2.namedWindow('Result', cv2.WINDOW_NORMAL) cv2.imshow('Result', combined) # 保存 if save_path: cv2.imwrite(str(save_path), combined) print(f\" 💾 结果已保存到: {save_path}\") print(\" 按任意键关闭...\") cv2.waitKey(0) cv2.destroyAllWindows()def main(): \"\"\" 主函数 \"\"\" print(\" \" + \"=\"*60) print(\"🚗 从图像到俯视图：完整实现\") print(\"=\"*60) # 1. 读取图像 img_path = input(\" 请输入图像路径（或按回车使用默认）: \").strip() if not img_path: img_path = \"input/dashcam.jpg\" print(f\" 📂 读取图像: {img_path}\") img = cv2.imread(img_path) if img is None: print(f\"❌ 无法读取图像: {img_path}\") return print(f\"✅ 图像大小: {img.shape[1]} × {img.shape[0]}\") # 2. 创建交互式界面 bev = InteractiveBirdEyeView(img) # 3. 计算鸟瞰图 result = bev.compute_bird_view( bird_w=400, bird_h=600, margin=50 ) if result is None: return # 4. 显示结果 visualize_results(result, save_path=\"output/result.jpg\") # 5. 保存各个结果 cv2.imwrite(\"output/marked.jpg\", result['marked']) cv2.imwrite(\"output/bird_view.jpg\", result['bird_view']) print(\" 📁 所有文件已保存到 output/ 目录\") print(\" ✨ 完成！\")if __name__ == \"__main__\": main() 4.4 运行效果运行步骤：# 1. 运行程序python main.py# 2. 输入图像路径（或使用默认）请输入图像路径（或按回车使用默认）:# 3. 在弹出的窗口中点击4个点📌 请在图像上依次点击4个点 顺序：左下 → 右下 → 右上 → 左上# 4. 查看结果✅ 处理完成！💾 结果已保存到: output/result.jpg效果展示：输入图像（斜视角） 输出图像（俯视图） ╱╲ ┌──────┐ ╱ ╲ → │ │ ╱ ╲ 变换 │ │ ╱______╲ └──────┘ 梯形效果 矩形效果🐛 第五章：调试与优化5.1 常见问题问题1：图像变形严重原因：点选择不合理解决方案：def check_points_distribution(pts): \"\"\"检查点的分布\"\"\" std_x = np.std(pts[:, 0]) std_y = np.std(pts[:, 1]) if std_x 50 or std_y 50: print(\"⚠️ 警告：点分布过于集中！\") print(f\" X方向标准差: {std_x:.1f}\") print(f\" Y方向标准差: {std_y:.1f}\") print(\" 建议：选择更分散的点\") return False return True问题2：鸟瞰图有黑边原因：输出图像范围设置不当解决方案：def compute_output_size(img, H, src_pts): \"\"\"自动计算输出图像大小\"\"\" h, w = img.shape[:2] # 变换图像的四个角点 corners = np.array([ [0, 0], [w, 0], [w, h], [0, h] ], dtype=np.float32) # 应用H变换 corners_homo = np.column_stack([corners, np.ones(4)]) transformed = (H @ corners_homo.T).T transformed = transformed[:, :2] / transformed[:, 2:3] # 计算边界 min_x = np.min(transformed[:, 0]) max_x = np.max(transformed[:, 0]) min_y = np.min(transformed[:, 1]) max_y = np.max(transformed[:, 1]) out_w = int(max_x - min_x) out_h = int(max_y - min_y) return out_w, out_h问题3：运行速度慢解决方案：# 使用更快的插值方法result = cv2.warpPerspective( img, H, (out_w, out_h), flags=cv2.INTER_LINEAR # 双线性插值（快） # flags=cv2.INTER_CUBIC # 双三次插值（慢但更好）)5.2 性能优化infographic list-grid-badge-carddata title 优化技巧 items - label 向量化计算 desc 避免Python循环 icon mdi:speedometer - label GPU加速 desc 使用cv2.cuda模块 icon mdi:chip - label 多线程 desc 并行处理多张图像 icon mdi:lan - label 缓存结果 desc 避免重复计算 icon mdi:cached 🚀 第六章：进阶应用6.1 视频流处理def process_video(video_path, H): \"\"\"对视频应用鸟瞰变换\"\"\" cap = cv2.VideoCapture(video_path) # 获取视频参数 fps = int(cap.get(cv2.CAP_PROP_FPS)) w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH)) h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT)) # 创建输出视频 fourcc = cv2.VideoWriter_fourcc(*'mp4v') out = cv2.VideoWriter('output.mp4', fourcc, fps, (400, 600)) print(f\"处理视频：{fps} FPS, {w}x{h}\") frame_count = 0 while cap.isOpened(): ret, frame = cap.read() if not ret: break # 应用变换 bird_view = cv2.warpPerspective(frame, H, (400, 600)) # 写入输出 out.write(bird_view) frame_count += 1 if frame_count % 30 == 0: print(f\"已处理 {frame_count} 帧\") cap.release() out.release() cv2.destroyAllWindows() print(f\"✅ 完成！共处理 {frame_count} 帧\") 6.2 实时相机标定def realtime_calibration(): \"\"\"实时相机标定和鸟瞰图生成\"\"\" cap = cv2.VideoCapture(0) H = None calibrated = False print(\"按 'c' 进入标定模式\") print(\"按 'q' 退出\") while True: ret, frame = cap.read() if not ret: break if calibrated and H is not None: # 应用变换 bird_view = cv2.warpPerspective(frame, H, (400, 600)) # 并排显示 display = np.hstack([ cv2.resize(frame, (400, 300)), cv2.resize(bird_view, (400, 300)) ]) cv2.imshow('Camera | Bird View', display) else: cv2.imshow('Camera', frame) key = cv2.waitKey(1) 0xFF if key == ord('c'): print(\" 开始标定...\") bev = InteractiveBirdEyeView(frame) result = bev.compute_bird_view() if result is not None: H = result['homography'] calibrated = True print(\"✅ 标定完成！\") elif key == ord('q'): break cap.release() cv2.destroyAllWindows() 📊 总结学习成果infographic list-column-done-listdata title 你已经掌握 items - label 理解透视投影原理 desc 3D到2D的转换 - label 计算消失点 desc 从平行线找交点 - label 估计相机参数 desc 焦距和角度 - label 单应性变换 desc DLT算法 - label 完整代码实现 desc Python+OpenCV 核心知识点| 概念 | 公式/方法 | 应用 | | 透视投影 | `u = K[R\\|t]X` | 3D→2D转换 || 消失点 | 平行线交点 | 估计相机朝向 || 单应性 | `x' = Hx` | 平面变换 || DLT算法 | SVD求解 | 计算H矩阵 |---### 后续学习方向```infographicinfographic hierarchy-tree-curved-line-rounded-rect-nodedata title 进阶路径 items - label 深度学习 children: - label 神经网络检测关键点 - label 端到端视角转换 - label 3D重建 children: - label 多视角3D恢复 - label SLAM技术 - label 实时系统 children: - label GPU加速 - label 嵌入式部署 🎉 结语你已经掌握了从图像到俯视图转换的完整知识和技能！ 记住： 📚 理论是基础 💻 代码是实践 🐛 调试是成长 🚀 应用是目标 继续探索，不断进步！ ✨ 📚 参考资料学习资源 OpenCV官方文档 Multiple View Geometry (Hartley Zisserman) 计算机视觉基础 相关论文 Homography Estimation: A Review Camera Calibration Methods Perspective Transformation in Computer Vision 版本信息： 版本：1.0.0 创建日期：2026-01-19 编程语言：Python 3.7+ 主要依赖：OpenCV, NumPy 祝你学习愉快！如有任何问题，欢迎交流讨论。 🎉","tags":["计算机视觉"],"categories":["计算机视觉"]},{"title":"如何使用Hexo写博客","path":"/2026/01/18/如何使用Hexo写博客/","content":"简介这是一篇示例文章，演示如何使用 Hexo 写博客。 步骤1. 创建新文章npx hexo new \"文章标题\" 2. 编辑文章在 source/_posts/ 目录下找到刚创建的 Markdown 文件进行编辑。 3. 本地预览npx hexo server 访问 http://localhost:4000 查看效果。 4. 部署npx hexo deploy 总结使用 Hexo 写博客非常简单！ 你可以在这里继续写更多内容…","tags":["工具"],"categories":["工具"]},{"title":"🔢 LeetCode 150 - 回溯算法专题","path":"/2026/01/18/leetcode-150-backtrack/","content":"🔄 回溯算法专题 (7题) 🎯 核心思想：尝试所有可能，走不通就回头 🗺️ 回溯算法的本质┌─────────────────────────────────────────────────────────────┐│ 回溯 = 决策树的遍历 │├─────────────────────────────────────────────────────────────┤│ ││ [] ││ / | \\ ││ [1] [2] [3] ← 第一层决策 ││ / \\ | ││ [1,2][1,3][2,3] ← 第二层决策 ││ | ││ [1,2,3] ← 第三层决策 ││ ││ 回溯三要素： ││ 1. 路径：已经做出的选择 ││ 2. 选择列表：当前可以做的选择 ││ 3. 结束条件：到达决策树底层 ││ │└─────────────────────────────────────────────────────────────┘ 🔧 回溯算法模板def backtrack(path, choices): # 结束条件 if 满足结束条件: result.append(path[:]) # 注意拷贝！ return for choice in choices: # 1. 做选择 path.append(choice) # 2. 递归进入下一层决策 backtrack(path, new_choices) # 3. 撤销选择（回溯） path.pop() 🧠 回溯口诀 “选择、递归、撤销” —— 回溯三部曲 1️⃣ LC 17. 电话号码的字母组合 🟡题目描述给定电话号码，返回所有可能的字母组合。 🎨 图解思路digits = \"23\"2 → \"abc\"3 → \"def\"决策树: \"\" / | \\ a b c ← 选择2对应的字母 /|\\ /|\\ /|\\ d e f d e f d e f ← 选择3对应的字母结果: [\"ad\",\"ae\",\"af\",\"bd\",\"be\",\"bf\",\"cd\",\"ce\",\"cf\"] 💻 代码实现def letterCombinations(digits: str) - list: if not digits: return [] phone = { '2': 'abc', '3': 'def', '4': 'ghi', '5': 'jkl', '6': 'mno', '7': 'pqrs', '8': 'tuv', '9': 'wxyz' } result = [] def backtrack(index, path): if index == len(digits): result.append(''.join(path)) return for char in phone[digits[index]]: path.append(char) backtrack(index + 1, path) path.pop() backtrack(0, []) return result 🧠 记忆口诀 “每个数字选一个字母” 2️⃣ LC 77. 组合 🟡题目描述从 1 到 n 中选择 k 个数的所有组合。 🎨 图解思路n = 4, k = 2决策树（每次只能选比自己大的数，避免重复）: [] / | | \\ [1] [2] [3] [4] / | \\ | [1,2][1,3][1,4] [2,3][2,4] [3,4]结果: [[1,2],[1,3],[1,4],[2,3],[2,4],[3,4]] 💻 代码实现def combine(n: int, k: int) - list: result = [] def backtrack(start, path): if len(path) == k: result.append(path[:]) return # 剪枝：剩余元素不够用了 if k - len(path) n - start + 1: return for i in range(start, n + 1): path.append(i) backtrack(i + 1, path) path.pop() backtrack(1, []) return result 🧠 记忆口诀 “从start开始选，选够k个停” 3️⃣ LC 46. 全排列 🟡题目描述返回数组的所有排列。 🎨 图解思路nums = [1, 2, 3]决策树（每个数只能用一次）: [] / | \\ [1] [2] [3] / \\ / \\ / \\ [1,2] [1,3] [2,1] [2,3] [3,1] [3,2] | | | | | | [1,2,3][1,3,2][2,1,3][2,3,1][3,1,2][3,2,1] 💻 代码实现def permute(nums: list) - list: result = [] def backtrack(path, used): if len(path) == len(nums): result.append(path[:]) return for i in range(len(nums)): if used[i]: continue path.append(nums[i]) used[i] = True backtrack(path, used) path.pop() used[i] = False backtrack([], [False] * len(nums)) return result 🧠 记忆口诀 “用过的标记，没用过的都能选” 4️⃣ LC 39. 组合总和 🟡题目描述找出所有和为 target 的组合（数字可以重复使用）。 🎨 图解思路candidates = [2, 3, 6, 7], target = 7决策树: [] / | \\ \\ [2] [3] [6] [7] ✓ / | \\ | | [2,2][2,3][2,6] [3,3] [6,?] /|\\ |[2,2,2][2,2,3]✓ [2,3,?] |[2,2,2,?] 超过7，剪枝 💻 代码实现def combinationSum(candidates: list, target: int) - list: result = [] candidates.sort() # 排序便于剪枝 def backtrack(start, path, remaining): if remaining == 0: result.append(path[:]) return for i in range(start, len(candidates)): if candidates[i] remaining: break # 剪枝 path.append(candidates[i]) backtrack(i, path, remaining - candidates[i]) # i不是i+1，可重复 path.pop() backtrack(0, [], target) return result 🧠 记忆口诀 “可以重复选，但只能往后选” 5️⃣ LC 52. N 皇后 II 🔴题目描述返回 N 皇后问题的解的数量。 🎨 图解思路n = 4一个有效解:. Q . .. . . QQ . . .. . Q .约束条件：1. 每行只能放一个皇后2. 每列只能放一个皇后3. 每条对角线只能放一个皇后对角线编号技巧:- 主对角线 (\\): row - col 相同- 副对角线 (/): row + col 相同 💻 代码实现def totalNQueens(n: int) - int: count = 0 cols = set() # 列冲突 diag1 = set() # 主对角线 (row - col) diag2 = set() # 副对角线 (row + col) def backtrack(row): nonlocal count if row == n: count += 1 return for col in range(n): if col in cols or (row - col) in diag1 or (row + col) in diag2: continue cols.add(col) diag1.add(row - col) diag2.add(row + col) backtrack(row + 1) cols.remove(col) diag1.remove(row - col) diag2.remove(row + col) backtrack(0) return count 🧠 记忆口诀 “列和两条对角线，都不能冲突” 6️⃣ LC 22. 括号生成 🟡题目描述生成 n 对有效的括号组合。 🎨 图解思路n = 2决策树（剪枝条件：右括号不能多于左括号）: \"\" / ( / \\ (( () | / (() ()( | | (()) ()()结果: [\"(())\", \"()()\"] 💻 代码实现def generateParenthesis(n: int) - list: result = [] def backtrack(path, left, right): if len(path) == 2 * n: result.append(''.join(path)) return if left n: path.append('(') backtrack(path, left + 1, right) path.pop() if right left: path.append(')') backtrack(path, left, right + 1) path.pop() backtrack([], 0, 0) return result 🧠 记忆口诀 “左括号随时加，右括号不超左” 7️⃣ LC 79. 单词搜索 🟡题目描述在二维网格中搜索单词。 🎨 图解思路board:A B C ES F C SA D E Eword = \"ABCCED\"从 A 开始，DFS + 回溯:A → B → C → C → E → D ✓ 💻 代码实现def exist(board: list, word: str) - bool: m, n = len(board), len(board[0]) def backtrack(i, j, k): if k == len(word): return True if i 0 or i = m or j 0 or j = n: return False if board[i][j] != word[k]: return False # 标记已访问 temp = board[i][j] board[i][j] = '#' # 四个方向搜索 found = (backtrack(i + 1, j, k + 1) or backtrack(i - 1, j, k + 1) or backtrack(i, j + 1, k + 1) or backtrack(i, j - 1, k + 1)) # 恢复 board[i][j] = temp return found for i in range(m): for j in range(n): if backtrack(i, j, 0): return True return False 🧠 记忆口诀 “DFS四方向，访问要标记” 📊 本章总结回溯问题分类 类型 特点 典型题目 组合问题 不考虑顺序 77, 39 排列问题 考虑顺序 46 子集问题 所有可能 78 搜索问题 在空间中找路径 79 棋盘问题 放置约束 52 括号问题 合法性约束 22 回溯 vs 动态规划┌──────────────┬──────────────────────────────┐│ 回溯 │ 动态规划 │├──────────────┼──────────────────────────────┤│ 找所有解 │ 找最优解/计数 ││ 暴力穷举 │ 记忆化避免重复 ││ 时间换空间 │ 空间换时间 │└──────────────┴──────────────────────────────┘ 🧠 全章记忆口诀电话组合全排列组合总和皇后解括号生成单词找回溯七题全拿下电话 - 电话号码的字母组合 (17)组合 - 组合 (77)全排列 - 全排列 (46)组合总和 - 组合总和 (39)皇后 - N皇后 II (52)括号 - 括号生成 (22)单词 - 单词搜索 (79) 📖 返回：LeetCode 150 题总目录","tags":["LeetCode"],"categories":["算法"]},{"title":"🌳 LeetCode 150 - 二叉树专题","path":"/2026/01/18/leetcode-150-binary-tree/","content":"🌳 二叉树专题 (14题) 🎯 核心思想：递归思维 + 分解问题 🗺️ 二叉树的思维模式┌─────────────────────────────────────────────────────────────┐│ 二叉树的两种思维模式 │├─────────────────────────────────────────────────────────────┤│ ││ 模式一：遍历思维 ││ ──────────────── ││ 用一个 traverse 函数遍历整棵树 ││ 在遍历过程中更新外部变量 ││ ││ 模式二：分解问题思维 ││ ────────────────── ││ 将问题分解为子问题 ││ 通过子问题的答案推导出原问题的答案 ││ ││ 1 ││ / \\ ││ 2 3 问题(根) = f(问题(左), 问题(右)) ││ / \\ ││ 4 5 ││ │└─────────────────────────────────────────────────────────────┘ 🔧 二叉树遍历模板前序遍历（根-左-右）def preorder(root): if not root: return print(root.val) # 先处理根 preorder(root.left) # 再左子树 preorder(root.right) # 后右子树 中序遍历（左-根-右）def inorder(root): if not root: return inorder(root.left) # 先左子树 print(root.val) # 再处理根 inorder(root.right) # 后右子树 后序遍历（左-右-根）def postorder(root): if not root: return postorder(root.left) # 先左子树 postorder(root.right) # 再右子树 print(root.val) # 后处理根 层序遍历（BFS）from collections import dequedef levelorder(root): if not root: return [] queue = deque([root]) result = [] while queue: level = [] for _ in range(len(queue)): node = queue.popleft() level.append(node.val) if node.left: queue.append(node.left) if node.right: queue.append(node.right) result.append(level) return result 1️⃣ LC 104. 二叉树的最大深度 🟢题目描述返回二叉树的最大深度。 🎨 图解思路```python / \\ 9 20 / \\ 15 7分解思维:maxDepth(3) = 1 + max(maxDepth(9), maxDepth(20)) = 1 + max(1, 2) = 3 💻 代码实现def maxDepth(root) - int: if not root: return 0 left_depth = maxDepth(root.left) right_depth = maxDepth(root.right) return 1 + max(left_depth, right_depth) 🧠 记忆口诀 “深度 = 1 + max(左深度, 右深度)” 2️⃣ LC 100. 相同的树 🟢题目描述判断两棵树是否相同。### 🎨 图解思路 p q 1 1 / \\ / \\ 2 3 2 3相同的条件:1. 根节点值相同2. 左子树相同3. 右子树相同### 💻 代码实现def isSameTree(p, q) - bool: if not p and not q: return True if not p or not q: return False return (p.val == q.val and isSameTree(p.left, q.left) and isSameTree(p.right, q.right))### 🧠 记忆口诀 “根同左同右同，才是真的同”3️⃣ LC 226. 翻转二叉树 🟢题目描述翻转二叉树（镜像）。### 🎨 图解思路 4 4 / \\ / \\ 2 7 = 7 2 / \\ / \\ / \\ / \\1 3 6 9 9 6 3 1交换每个节点的左右子树### 💻 代码实现def invertTree(root): if not root: return None # 交换左右子树 root.left, root.right = root.right, root.left # 递归翻转子树 invertTree(root.left) invertTree(root.right) return root### 🧠 记忆口诀 “先交换，再递归”4️⃣ LC 101. 对称二叉树 🟢题目描述判断二叉树是否对称。### 🎨 图解思路 / \\ 2 2 / \\ / \\3 4 4 3对称条件:左子树的左 == 右子树的右左子树的右 == 右子树的左### 💻 代码实现def isSymmetric(root) - bool: def check(left, right): if not left and not right: return True if not left or not right: return False return (left.val == right.val and check(left.left, right.right) and check(left.right, right.left)) return check(root.left, root.right) if root else True### 🧠 记忆口诀 “外外相等，内内相等”5️⃣ LC 105. 从前序与中序遍历序列构造二叉树 🟡题目描述根据前序和中序遍历结果，构建二叉树。### 🎨 图解思路preorder = [3, 9, 20, 15, 7] 根-左-右inorder = [9, 3, 15, 20, 7] 左-根-右步骤:1. preorder[0] = 3 是根节点2. 在 inorder 中找到 3，左边是左子树，右边是右子树3. 递归构建 / \\ 9 20 / \\ 15 7### 💻 代码实现def buildTree(preorder: list, inorder: list): if not preorder: return None # 根节点是前序第一个 root = TreeNode(preorder[0]) # 在中序中找到根节点位置 mid = inorder.index(preorder[0]) # 递归构建左右子树 root.left = buildTree(preorder[1:mid+1], inorder[:mid]) root.right = buildTree(preorder[mid+1:], inorder[mid+1:]) return root### 🧠 记忆口诀 “前序定根，中序分边”6️⃣ LC 106. 从中序与后序遍历序列构造二叉树 🟡题目描述根据中序和后序遍历结果，构建二叉树。### 🎨 图解思路inorder = [9, 3, 15, 20, 7] 左-根-右postorder = [9, 15, 7, 20, 3] 左-右-根后序最后一个是根！### 💻 代码实现def buildTree(inorder: list, postorder: list): if not postorder: return None # 根节点是后序最后一个 root = TreeNode(postorder[-1]) # 在中序中找到根节点位置 mid = inorder.index(postorder[-1]) # 递归构建左右子树 root.left = buildTree(inorder[:mid], postorder[:mid]) root.right = buildTree(inorder[mid+1:], postorder[mid:-1]) return root### 🧠 记忆口诀 “后序定根（最后），中序分边”7️⃣ LC 117. 填充每个节点的下一个右侧节点指针 II 🟡题目描述填充每个节点的 next 指针指向右侧节点。### 🎨 图解思路 1 → NULL / \\ 2 → 3 → NULL / \\ \\4→ 5 → 7 → NULL使用层序遍历，连接同层节点### 💻 代码实现def connect(root): if not root: return None queue = deque([root]) while queue: size = len(queue) prev = None for i in range(size): node = queue.popleft() if prev: prev.next = node prev = node if node.left: queue.append(node.left) if node.right: queue.append(node.right) return root### 🧠 记忆口诀 “层序遍历，前连后”8️⃣ LC 114. 二叉树展开为链表 🟡题目描述将二叉树展开为单链表（前序顺序）。### 🎨 图解思路 1 1 / \\ \\ 2 5 = 2 / \\ \\ \\3 4 6 3 \\ \\ \\### 💻 代码实现def flatten(root) - None: if not root: return # 后序遍历：先处理子树，再处理根 flatten(root.left) flatten(root.right) # 保存右子树 right = root.right # 左子树移到右边 root.right = root.left root.left = None # 找到右子树末端，接上原右子树 while root.right: root = root.right root.right = right### 🧠 记忆口诀 “左接右，原右接末尾”9️⃣ LC 112. 路径总和 🟢题目描述判断是否存在根到叶子路径，其和等于目标值。### 🎨 图解思路 / \\ 4 8 / / \\ 11 13 4 / \\ \\7 2 1targetSum = 22路径: 5 → 4 → 11 → 2 = 22 ✓### 💻 代码实现def hasPathSum(root, targetSum: int) - bool: if not root: return False # 叶子节点 if not root.left and not root.right: return root.val == targetSum # 递归检查子树 remaining = targetSum - root.val return (hasPathSum(root.left, remaining) or hasPathSum(root.right, remaining))### 🧠 记忆口诀 “叶子判相等，非叶递归减”🔟 LC 129. 求根节点到叶节点数字之和 🟡题目描述每条路径组成一个数字，求所有数字之和。### 🎨 图解思路 / \\ 2 3路径: 1→2 = 12路径: 1→3 = 13总和: 12 + 13 = 25### 💻 代码实现def sumNumbers(root) - int: def dfs(node, current_sum): if not node: return 0 current_sum = current_sum * 10 + node.val # 叶子节点 if not node.left and not node.right: return current_sum return dfs(node.left, current_sum) + dfs(node.right, current_sum) return dfs(root, 0)### 🧠 记忆口诀 “进位乘10加当前”1️⃣1️⃣ LC 124. 二叉树中的最大路径和 🔴题目描述找出路径和最大的路径（可以不经过根节点）。### 🎨 图解思路 -10 / \\ 9 20 / \\ 15 7最大路径: 15 → 20 → 7 = 42思路:每个节点可以：1. 只贡献自己（作为路径端点）2. 贡献自己+左子树3. 贡献自己+右子树4. 作为拐点（左+自己+右）### 💻 代码实现def maxPathSum(root) - int: max_sum = float('-inf') def max_gain(node): nonlocal max_sum if not node: return 0 # 左右子树的最大贡献（负数不要） left_gain = max(max_gain(node.left), 0) right_gain = max(max_gain(node.right), 0) # 当前节点作为拐点的路径和 path_sum = node.val + left_gain + right_gain max_sum = max(max_sum, path_sum) # 返回给父节点的贡献（只能选一边） return node.val + max(left_gain, right_gain) max_gain(root) return max_sum### 🧠 记忆口诀 “拐点算全局，贡献选一边”1️⃣2️⃣ LC 173. 二叉搜索树迭代器 🟡题目描述实现二叉搜索树的迭代器。### 💻 代码实现class BSTIterator: def __init__(self, root): self.stack = [] self._leftmost_inorder(root) def _leftmost_inorder(self, node): while node: self.stack.append(node) node = node.left def next(self) - int: node = self.stack.pop() if node.right: self._leftmost_inorder(node.right) return node.val def hasNext(self) - bool: return len(self.stack) 0### 🧠 记忆口诀 “栈存左链，弹出处理右”1️⃣3️⃣ LC 222. 完全二叉树的节点个数 🟢题目描述统计完全二叉树的节点个数。### 💻 代码实现def countNodes(root) - int: if not root: return 0 left_depth = right_depth = 0 left, right = root, root while left: left_depth += 1 left = left.left while right: right_depth += 1 right = right.right # 满二叉树 if left_depth == right_depth: return 2 ** left_depth - 1 # 递归 return 1 + countNodes(root.left) + countNodes(root.right)### 🧠 记忆口诀 “满树用公式，不满递归数”1️⃣4️⃣ LC 236. 二叉树的最近公共祖先 🟡题目描述找两个节点的最近公共祖先（LCA）。### 🎨 图解思路 / \\ 5 1 / \\ / \\ 6 2 0 8 / \\ 7 4LCA(5, 1) = 3LCA(5, 4) = 5### 💻 代码实现def lowestCommonAncestor(root, p, q): if not root or root == p or root == q: return root left = lowestCommonAncestor(root.left, p, q) right = lowestCommonAncestor(root.right, p, q) # p, q 分布在两边 if left and right: return root # p, q 在同一边 return left if left else right### 🧠 记忆口诀 “左右都有返回根，否则返回有的那边”📊 本章总结题目速查表| 题号 | 题目 | 难度 | 类型 | | 104 | 最大深度 | 🟢 | 深度 || 100 | 相同的树 | 🟢 | 比较 || 226 | 翻转二叉树 | 🟢 | 变换 || 101 | 对称二叉树 | 🟢 | 比较 || 105 | 前序+中序构造 | 🟡 | 构造 || 106 | 中序+后序构造 | 🟡 | 构造 || 117 | 填充next指针 | 🟡 | 层序 || 114 | 展开为链表 | 🟡 | 变换 || 112 | 路径总和 | 🟢 | 路径 || 129 | 数字之和 | 🟡 | 路径 || 124 | 最大路径和 | 🔴 | 路径 || 173 | BST迭代器 | 🟡 | 迭代 || 222 | 完全树节点数 | 🟢 | 计数 || 236 | 最近公共祖先 | 🟡 | LCA |### 🧠 全章记忆口诀 深度相同翻对称前中后序建树型连接展开走路径迭代计数找祖宗 深度 - 最大深度 (104)相同 - 相同的树 (100)翻 - 翻转二叉树 (226)对称 - 对称二叉树 (101)前中后序 - 从遍历构造 (105, 106)连接 - 填充next指针 (117)展开 - 展开为链表 (114)路径 - 路径总和系列 (112, 129, 124)迭代 - BST迭代器 (173)计数 - 节点个数 (222)祖宗 - 最近公共祖先 (236) --- 📖 **下一篇**：[二叉搜索树专题](/2026/01/18/leetcode-150-bst/)","tags":["LeetCode"],"categories":["算法"]},{"title":"📚 LeetCode 150 - 数组与字符串专题","path":"/2026/01/18/leetcode-150-array-string/","content":"📚 数组与字符串专题 (15题) 🎯 核心技巧：双指针、前缀和、贪心、模拟 🗺️ 知识图谱 ┌─────────────────┐ │ 数组与字符串 │ └────────┬────────┘ ┌─────────────────┼─────────────────┐ ▼ ▼ ▼┌──────────┐ ┌──────────┐ ┌──────────┐│ 双指针 │ │ 贪心 │ │ 前缀和 │└────┬─────┘ └────┬─────┘ └────┬─────┘ │ │ │┌────┴────┐ ┌────┴────┐ ┌────┴────┐│合并数组 │ │跳跃游戏 │ │除自身乘积││移除元素 │ │买卖股票 │ │ ││删除重复 │ │分发糖果 │ │ │└─────────┘ └─────────┘ └─────────┘ 1️⃣ LC 88. 合并两个有序数组 🟢题目描述将两个有序数组 nums1 和 nums2 合并到 nums1 中，使其有序。 🎨 图解思路从后往前填充，避免覆盖！nums1 = [1, 2, 3, 0, 0, 0] nums2 = [2, 5, 6] ↑ ↑ p1 p2 ↑ p (填充位置)Step 1: 比较 3 vs 6 → 填 6nums1 = [1, 2, 3, 0, 0, 6]Step 2: 比较 3 vs 5 → 填 5 nums1 = [1, 2, 3, 0, 5, 6]Step 3: 比较 3 vs 2 → 填 3nums1 = [1, 2, 3, 3, 5, 6]... 最终结果 [1, 2, 2, 3, 5, 6] 💻 代码实现def merge(nums1, m, nums2, n): p1, p2, p = m - 1, n - 1, m + n - 1 while p2 = 0: # nums2 还有元素 if p1 = 0 and nums1[p1] nums2[p2]: nums1[p] = nums1[p1] p1 -= 1 else: nums1[p] = nums2[p2] p2 -= 1 p -= 1 🧠 记忆口诀 “从后往前填，大的先落位” 2️⃣ LC 27. 移除元素 🟢题目描述原地移除数组中等于 val 的元素，返回新长度。 🎨 图解思路快慢指针：慢指针标记有效位置，快指针遍历val = 3 s f[3, 2, 2, 3, 4]f=0: nums[0]=3, 跳过f=1: nums[1]=2≠3, nums[s]=2, s++ s f[2, 2, 2, 3, 4]f=2: nums[2]=2≠3, nums[s]=2, s++ s f[2, 2, 2, 3, 4]f=3: nums[3]=3, 跳过f=4: nums[4]=4≠3, nums[s]=4, s++最终: [2, 2, 4, _, _], 返回 3 💻 代码实现def removeElement(nums, val): slow = 0 for fast in range(len(nums)): if nums[fast] != val: nums[slow] = nums[fast] slow += 1 return slow 🧠 记忆口诀 “快指针探路，慢指针收货” 3️⃣ LC 26. 删除有序数组中的重复项 🟢🎨 图解思路有序数组，相同元素必相邻！ s f[1, 1, 2, 2, 3]f=1: nums[1]=nums[0], 跳过f=2: nums[2]≠nums[1], s++, nums[s]=2 s f[1, 2, 2, 2, 3]f=4: nums[4]≠nums[3], s++, nums[s]=3 s[1, 2, 3, _, _]返回 s+1 = 3 💻 代码实现def removeDuplicates(nums): if not nums: return 0 slow = 0 for fast in range(1, len(nums)): if nums[fast] != nums[slow]: slow += 1 nums[slow] = nums[fast] return slow + 1 4️⃣ LC 80. 删除有序数组中的重复项 II 🟡题目描述每个元素最多出现 两次。 🎨 通用模板def removeDuplicates(nums, k=2): \"\"\"允许每个元素最多出现 k 次\"\"\" slow = 0 for num in nums: if slow k or num != nums[slow - k]: nums[slow] = num slow += 1 return slow 🧠 记忆口诀 “检查 k 位之前，不同才能进” 5️⃣ LC 169. 多数元素 🟢题目描述找出出现次数超过 n/2 的元素。 🎨 Boyer-Moore 投票算法把多数元素看作 +1，其他元素看作 -1最终 +1 一定比 -1 多！nums = [2, 2, 1, 1, 1, 2, 2]candidate=2, count=1 → [2]candidate=2, count=2 → [2,2]candidate=2, count=1 → 遇到1，抵消candidate=2, count=0 → 遇到1，抵消candidate=1, count=1 → count=0时换人candidate=1, count=0 → 遇到2，抵消candidate=2, count=1 → count=0时换人最终 candidate = 2 ✓ 💻 代码实现def majorityElement(nums): candidate, count = None, 0 for num in nums: if count == 0: candidate = num count += 1 if num == candidate else -1 return candidate 🧠 记忆口诀 “同加异减，归零换帅” 6️⃣ LC 189. 轮转数组 🟡题目描述将数组向右轮转 k 位。 🎨 三次翻转法nums = [1,2,3,4,5,6,7], k = 3Step 1: 整体翻转[7,6,5,4,3,2,1]Step 2: 翻转前 k 个[5,6,7,4,3,2,1]Step 3: 翻转后 n-k 个 [5,6,7,1,2,3,4] ✓ 💻 代码实现def rotate(nums, k): n = len(nums) k %= n # 处理 k n 的情况 def reverse(left, right): while left right: nums[left], nums[right] = nums[right], nums[left] left, right = left + 1, right - 1 reverse(0, n - 1) # 整体翻转 reverse(0, k - 1) # 前k个 reverse(k, n - 1) # 后n-k个 🧠 记忆口诀 “全转前转后，三步到位” 7️⃣ LC 121. 买卖股票的最佳时机 🟢题目描述只能买卖一次，求最大利润。 🎨 图解思路维护「历史最低价」，计算当天卖出利润prices = [7, 1, 5, 3, 6, 4] │ │ │ │ │ │min_price │ 1 1 1 1 1profit 0 0 4 2 5 3 ↑ 最大利润 = 5 💻 代码实现def maxProfit(prices): min_price = float('inf') max_profit = 0 for price in prices: min_price = min(min_price, price) max_profit = max(max_profit, price - min_price) return max_profit 🧠 记忆口诀 “记住最低点，时刻算差价” 8️⃣ LC 55. 跳跃游戏 🟡题目描述判断能否跳到最后一个位置。 🎨 贪心思路维护能到达的最远位置 max_reachnums = [2, 3, 1, 1, 4] ↑i=0: max_reach = max(0, 0+2) = 2i=1: max_reach = max(2, 1+3) = 4 ≥ 4 ✓ 可达！nums = [3, 2, 1, 0, 4]i=0: max_reach = 3i=1: max_reach = 3i=2: max_reach = 3i=3: max_reach = 3 4 i=4: i max_reach，无法到达 ✗ 💻 代码实现def canJump(nums): max_reach = 0 for i in range(len(nums)): if i max_reach: return False max_reach = max(max_reach, i + nums[i]) return True 🧠 记忆口诀 “走一步算一步，能到就更新” 9️⃣ LC 45. 跳跃游戏 II 🟡题目描述求到达最后位置的最少跳跃次数。 🎨 BFS 思想把每一跳能到的范围看作一层nums = [2, 3, 1, 1, 4] ↑层0: 位置0，能到 [1,2]层1: 位置1-2，能到 [2,3,4] → 到达终点！跳跃次数 = 2 💻 代码实现def jump(nums): jumps = 0 cur_end = 0 # 当前跳跃能到的边界 cur_farthest = 0 # 下一跳能到的最远 for i in range(len(nums) - 1): cur_farthest = max(cur_farthest, i + nums[i]) if i == cur_end: # 到达边界，必须跳 jumps += 1 cur_end = cur_farthest return jumps 🔟 LC 238. 除自身以外数组的乘积 🟡题目描述返回数组，answer[i] 等于 nums 中除 nums[i] 之外其余各元素的乘积。 🎨 前缀积 × 后缀积nums = [1, 2, 3, 4]前缀积 = [1, 1, 2, 6] (不含当前)后缀积 = [24, 12, 4, 1] (不含当前)结果 = [24, 12, 8, 6] (前缀 × 后缀) 💻 代码实现 (O(1) 空间)def productExceptSelf(nums): n = len(nums) result = [1] * n # 计算前缀积 prefix = 1 for i in range(n): result[i] = prefix prefix *= nums[i] # 计算后缀积并相乘 suffix = 1 for i in range(n - 1, -1, -1): result[i] *= suffix suffix *= nums[i] return result 🧠 记忆口诀 “左边乘一遍，右边乘一遍” 📊 本章总结核心模式速查表 模式 适用场景 典型题目 快慢指针 原地修改数组 26, 27, 80 前后指针 有序数组合并 88 贪心 最优解问题 55, 45, 121 前缀和/积 区间计算 238 投票算法 多数元素 169 翻转技巧 轮转/翻转 189 🧠 全章记忆口诀合移删删多，轮买买跳跳H插除加糖，数组十五妙合 - 合并数组 (88)移 - 移除元素 (27) 删删 - 删除重复 I/II (26, 80)多 - 多数元素 (169)轮 - 轮转数组 (189)买买 - 买卖股票 I/II (121, 122)跳跳 - 跳跃游戏 I/II (55, 45)H - H指数 (274)插 - O(1)插入删除 (380)除 - 除自身乘积 (238)加 - 加油站 (134)糖 - 分发糖果 (135) 📖 下一篇：双指针专题","tags":["LeetCode"],"categories":["算法"]},{"title":"🔢 LeetCode 150 - 二分查找专题","path":"/2026/01/18/leetcode-150-binary-search/","content":"🔢 二分查找专题 (7题) 🎯 核心思想：每次排除一半的搜索空间，时间复杂度 O(log n) 🗺️ 二分查找的本质┌─────────────────────────────────────────────────────────────┐│ 二分查找的本质 │├─────────────────────────────────────────────────────────────┤│ ││ 二分查找的本质是：在有序（或部分有序）的空间中 ││ 找到满足某个条件的边界点 ││ ││ 🔵🔵🔵🔵🔵🔴🔴🔴🔴🔴🔴 ││ ↑ ↑ ││ 蓝色区域 红色区域 ││ (不满足) (满足) ││ ││ 目标：找到第一个红色（或最后一个蓝色） ││ │└─────────────────────────────────────────────────────────────┘ 🔧 二分查找模板模板1：找第一个满足条件的位置def binary_search_first(nums, target): left, right = 0, len(nums) # 左闭右开 while left right: mid = left + (right - left) // 2 if condition(mid): # 满足条件 right = mid # 答案在 [left, mid] else: left = mid + 1 # 答案在 [mid+1, right) return left # 第一个满足条件的位置 模板2：找最后一个满足条件的位置def binary_search_last(nums, target): left, right = 0, len(nums) while left right: mid = left + (right - left + 1) // 2 # 向上取整 if condition(mid): # 满足条件 left = mid # 答案在 [mid, right) else: right = mid - 1 # 答案在 [left, mid-1] return left # 最后一个满足条件的位置 1️⃣ LC 35. 搜索插入位置 🟢题目描述在排序数组中找到目标值的位置，如果不存在则返回应该插入的位置。 🎨 图解思路nums = [1, 3, 5, 6], target = 5找第一个 = target 的位置 1 3 5 6 ↑ ↑ 5 =5二分查找：初始: left=0, right=4mid=2, nums[2]=5 = 5, right=2mid=1, nums[1]=3 5, left=2left == right, 返回 2 💻 代码实现def searchInsert(nums: list, target: int) - int: left, right = 0, len(nums) while left right: mid = left + (right - left) // 2 if nums[mid] = target: right = mid else: left = mid + 1 return left 🧠 记忆口诀 “找第一个不小于目标的位置” 2️⃣ LC 74. 搜索二维矩阵 🟡题目描述在行列都有序的二维矩阵中搜索目标值。 🎨 图解思路matrix:[1, 3, 5, 7][10, 11, 16, 20][23, 30, 34, 60]将 2D 矩阵看作 1D 数组:[1, 3, 5, 7, 10, 11, 16, 20, 23, 30, 34, 60]坐标转换:index → (index // n, index % n) 💻 代码实现def searchMatrix(matrix: list, target: int) - bool: m, n = len(matrix), len(matrix[0]) left, right = 0, m * n while left right: mid = left + (right - left) // 2 row, col = mid // n, mid % n if matrix[row][col] == target: return True elif matrix[row][col] target: left = mid + 1 else: right = mid return False 🧠 记忆口诀 “2D变1D，除法取行，余数取列” 3️⃣ LC 162. 寻找峰值 🟡题目描述找到数组中任意一个峰值元素的索引（比左右邻居都大）。 🎨 图解思路nums = [1, 2, 1, 3, 5, 6, 4]峰值: 索引 1 (值为2) 或 索引 5 (值为6)二分思路:- 如果 mid 在上坡，峰值在右边- 如果 mid 在下坡，峰值在左边```python 5 \\4 / / \\1 1mid 处于上坡 → 往右找mid 处于下坡 → 往左找（包含mid） 💻 代码实现def findPeakElement(nums: list) - int: left, right = 0, len(nums) - 1 while left right: mid = left + (right - left) // 2 if nums[mid] nums[mid + 1]: # 上坡，峰值在右边 left = mid + 1 else: # 下坡或峰值，峰值在左边（包含mid） right = mid return left 🧠 记忆口诀 “上坡往右，下坡往左” 4️⃣ LC 33. 搜索旋转排序数组 🟡题目描述在旋转后的有序数组中搜索目标值。### 🎨 图解思路nums = [4, 5, 6, 7, 0, 1, 2], target = 0旋转后的数组特点: 6 \\ 5 0 4 \\二分策略:1. 判断 mid 在左半段还是右半段2. 判断 target 在 mid 的哪边### 💻 代码实现def search(nums: list, target: int) - int: left, right = 0, len(nums) - 1 while left = right: mid = left + (right - left) // 2 if nums[mid] == target: return mid # 判断 mid 在左半段还是右半段 if nums[left] = nums[mid]: # mid 在左半段（有序） if nums[left] = target nums[mid]: right = mid - 1 else: left = mid + 1 else: # mid 在右半段（有序） if nums[mid] target = nums[right]: left = mid + 1 else: right = mid - 1 return -1### 🧠 记忆口诀 “先判断哪边有序，再判断目标在哪边”5️⃣ LC 34. 查找元素的第一个和最后一个位置 🟡题目描述在排序数组中找到目标值的起始和结束位置。### 🎨 图解思路nums = [5, 7, 7, 8, 8, 10], target = 8找第一个 8: 索引 3找最后一个 8: 索引 4 5 7 7 8 8 10 ↑ ↑ first last### 💻 代码实现def searchRange(nums: list, target: int) - list: def find_first(): left, right = 0, len(nums) while left right: mid = left + (right - left) // 2 if nums[mid] = target: right = mid else: left = mid + 1 return left def find_last(): left, right = 0, len(nums) while left right: mid = left + (right - left) // 2 if nums[mid] target: right = mid else: left = mid + 1 return left - 1 first = find_first() if first == len(nums) or nums[first] != target: return [-1, -1] last = find_last() return [first, last]### 🧠 记忆口诀 “找第一个=，找第一个再减1”6️⃣ LC 153. 寻找旋转排序数组中的最小值 🟡题目描述在旋转后的有序数组中找到最小值。### 🎨 图解思路nums = [3, 4, 5, 1, 2] 4 \\ 3 1 \\最小值是旋转点比较 nums[mid] 和 nums[right]:- nums[mid] nums[right]: 最小值在右边- nums[mid] = nums[right]: 最小值在左边（包含mid）### 💻 代码实现def findMin(nums: list) - int: left, right = 0, len(nums) - 1 while left right: mid = left + (right - left) // 2 if nums[mid] nums[right]: # 最小值在右边 left = mid + 1 else: # 最小值在左边（包含mid） right = mid return nums[left]### 🧠 记忆口诀 “比右边大就往右，否则往左”7️⃣ LC 4. 寻找两个正序数组的中位数 🔴题目描述找到两个正序数组的中位数，要求时间复杂度 O(log(m+n))。### 🎨 图解思路nums1 = [1, 3], nums2 = [2]合并后: [1, 2, 3]中位数: 2二分思路:在较短的数组上二分，找到一个划分点 i使得 nums1[0:i] 和 nums2[0:j] 的总数 = (m+n+1)//2 nums1: 1 | 3 nums2: 2 | ↑ 划分点左半边最大值 = 右半边最小值### 💻 代码实现def findMedianSortedArrays(nums1: list, nums2: list) - float: # 确保 nums1 是较短的数组 if len(nums1) len(nums2): nums1, nums2 = nums2, nums1 m, n = len(nums1), len(nums2) left, right = 0, m while left = right: i = (left + right) // 2 j = (m + n + 1) // 2 - i # 边界处理 nums1_left = float('-inf') if i == 0 else nums1[i - 1] nums1_right = float('inf') if i == m else nums1[i] nums2_left = float('-inf') if j == 0 else nums2[j - 1] nums2_right = float('inf') if j == n else nums2[j] if nums1_left = nums2_right and nums2_left = nums1_right: # 找到正确的划分 if (m + n) % 2 == 1: return max(nums1_left, nums2_left) else: return (max(nums1_left, nums2_left) + min(nums1_right, nums2_right)) / 2 elif nums1_left nums2_right: # nums1 划分点太靠右 right = i - 1 else: # nums1 划分点太靠左 left = i + 1 return 0.0### 🧠 记忆口诀 “短数组二分，找正确划分”📊 本章总结二分查找场景| 场景 | 关键点 | 典型题目 | | 有序数组查找 | 直接二分 | 35, 74 || 旋转数组 | 判断有序段 | 33, 153 || 峰值问题 | 比较相邻元素 | 162 || 边界问题 | =和的区别 | 34 || 双数组 | 在短数组二分 | 4 |### 🧠 全章记忆口诀 插矩峰旋范围最中二分七题要记清 插 - 搜索插入位置 (35)矩 - 搜索二维矩阵 (74)峰 - 寻找峰值 (162)旋 - 搜索旋转排序数组 (33)范围 - 查找元素的第一个和最后一个位置 (34)最 - 寻找旋转排序数组中的最小值 (153)中 - 寻找两个正序数组的中位数 (4) --- 📖 **下一篇**：[位运算专题](/2026/01/18/leetcode-150-bit/)","tags":["LeetCode"],"categories":["算法"]},{"title":"📈 LeetCode 150 - 动态规划专题","path":"/2026/01/18/leetcode-150-dp/","content":"📈 动态规划专题 (11题) 🎯 核心思想：将大问题分解为小问题，记录子问题的解，避免重复计算 🗺️ 动态规划解题框架┌─────────────────────────────────────────────────────────────┐│ DP 解题四步法 │├─────────────────────────────────────────────────────────────┤│ ││ Step 1: 定义状态 ││ dp[i] 表示什么？dp[i][j] 表示什么？ ││ ││ Step 2: 状态转移方程 ││ dp[i] = f(dp[i-1], dp[i-2], ...) ││ ││ Step 3: 初始化 ││ 边界条件是什么？dp[0], dp[1] 等于多少？ ││ ││ Step 4: 遍历顺序 ││ 正序还是倒序？先行后列还是先列后行？ ││ │└─────────────────────────────────────────────────────────────┘ 🔧 DP 代码模板def dp_template(nums): n = len(nums) # Step 1: 定义 dp 数组 dp = [0] * n # 或 [[0]*m for _ in range(n)] # Step 3: 初始化 dp[0] = base_case # Step 4: 遍历顺序 for i in range(1, n): # Step 2: 状态转移 dp[i] = transition(dp[i-1], ...) return dp[n-1] # 或其他目标 1️⃣ LC 70. 爬楼梯 🟢题目描述每次可以爬 1 或 2 个台阶，爬到第 n 阶有多少种方法？ 🎨 图解思路到达第 n 阶的方法 = 从第 n-1 阶爬 1 步 + 从第 n-2 阶爬 2 步 ┌───┐ │ n │ ← 目标 └───┘ ↗ ↖ ┌───┐ ┌───┐ │n-1│ │n-2│ └───┘ └───┘ ↑ ↑ 1步 2步dp[n] = dp[n-1] + dp[n-2] (斐波那契数列!) 💻 代码实现def climbStairs(n: int) - int: if n = 2: return n # 空间优化：只需记录前两个状态 prev, curr = 1, 2 for _ in range(3, n + 1): prev, curr = curr, prev + curr return curr 🧠 记忆口诀 “爬楼梯就是斐波那契” 2️⃣ LC 198. 打家劫舍 🟡题目描述不能偷相邻的房子，求能偷到的最大金额。 🎨 图解思路nums = [2, 7, 9, 3, 1]对于每个房子，两个选择：1. 偷：dp[i] = dp[i-2] + nums[i]2. 不偷：dp[i] = dp[i-1]dp[i] = max(dp[i-2] + nums[i], dp[i-1])i: 0 1 2 3 4nums: 2 7 9 3 1dp: 2 7 11 11 12 ↑ ↑ ↑ ↑ ↑ 偷 偷 偷0+9 不偷 偷2+1 💻 代码实现def rob(nums: list) - int: if len(nums) == 1: return nums[0] # 空间优化 prev, curr = nums[0], max(nums[0], nums[1]) for i in range(2, len(nums)): prev, curr = curr, max(curr, prev + nums[i]) return curr 🧠 记忆口诀 “偷不偷，取最大” 3️⃣ LC 139. 单词拆分 🟡题目描述判断字符串是否可以被拆分为字典中的单词。 🎨 图解思路s = \"leetcode\", wordDict = [\"leet\", \"code\"]dp[i] 表示 s[0:i] 是否可以被拆分dp[0] = True (空字符串)dp[4] = dp[0] and \"leet\" in dict → Truedp[8] = dp[4] and \"code\" in dict → True l e e t c o d e 0 1 2 3 4 5 6 7 8dp T F F F T F F F T ↑ ↑ \"leet\" \"code\" 💻 代码实现def wordBreak(s: str, wordDict: list) - bool: word_set = set(wordDict) n = len(s) dp = [False] * (n + 1) dp[0] = True for i in range(1, n + 1): for j in range(i): if dp[j] and s[j:i] in word_set: dp[i] = True break return dp[n] 🧠 记忆口诀 “前面能拆，后面在字典，就能拆” 4️⃣ LC 322. 零钱兑换 🟡题目描述用最少的硬币凑出目标金额。 🎨 图解思路coins = [1, 2, 5], amount = 11dp[i] = 凑出金额 i 需要的最少硬币数对于每个金额 i，尝试每个硬币 c：dp[i] = min(dp[i], dp[i-c] + 1)amount: 0 1 2 3 4 5 6 7 8 9 10 11dp: 0 1 1 2 2 1 2 2 3 3 2 3 ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ ↑ +1 +2 +1 +2 +5 +1 +2 +1 +2 +5 +5 💻 代码实现def coinChange(coins: list, amount: int) - int: dp = [float('inf')] * (amount + 1) dp[0] = 0 for i in range(1, amount + 1): for coin in coins: if coin = i and dp[i - coin] != float('inf'): dp[i] = min(dp[i], dp[i - coin] + 1) return dp[amount] if dp[amount] != float('inf') else -1 🧠 记忆口诀 “每个硬币试一试，取最小” 5️⃣ LC 300. 最长递增子序列 🟡题目描述找出数组中最长的严格递增子序列的长度。 🎨 图解思路nums = [10, 9, 2, 5, 3, 7, 101, 18]dp[i] = 以 nums[i] 结尾的 LIS 长度对于 nums[i]，找所有 j i 且 nums[j] nums[i]：dp[i] = max(dp[j] + 1)i: 0 1 2 3 4 5 6 7nums: 10 9 2 5 3 7 101 18dp: 1 1 1 2 2 3 4 4 ↑ ↑ ↑ ↑ ↑ 2+1 2+1 5+1 7+1 7+1LIS = 4 (如 [2, 3, 7, 101]) 💻 代码实现# O(n²) 解法def lengthOfLIS(nums: list) - int: n = len(nums) dp = [1] * n for i in range(1, n): for j in range(i): if nums[j] nums[i]: dp[i] = max(dp[i], dp[j] + 1) return max(dp) 🔥 O(n log n) 二分解法def lengthOfLIS(nums: list) - int: # tails[i] = 长度为 i+1 的 LIS 的最小结尾元素 tails = [] for num in nums: # 二分查找第一个 = num 的位置 left, right = 0, len(tails) while left right: mid = (left + right) // 2 if tails[mid] num: left = mid + 1 else: right = mid if left == len(tails): tails.append(num) else: tails[left] = num return len(tails) 🧠 记忆口诀 “前面比我小的，加1取最大” 6️⃣ LC 120. 三角形最小路径和 🟡题目描述从顶部到底部的最小路径和。 🎨 图解思路 [2] [3,4] [6,5,7] [4,1,8,3]自底向上计算：dp[i][j] = min(dp[i+1][j], dp[i+1][j+1]) + triangle[i][j]第3层: [4, 1, 8, 3]第2层: [6+1, 5+1, 7+3] = [7, 6, 10]第1层: [3+6, 4+6] = [9, 10]第0层: [2+9] = [11]最小路径和 = 11 💻 代码实现def minimumTotal(triangle: list) - int: n = len(triangle) # 从最后一行开始 dp = triangle[-1][:] # 自底向上 for i in range(n - 2, -1, -1): for j in range(i + 1): dp[j] = min(dp[j], dp[j + 1]) + triangle[i][j] return dp[0] 🧠 记忆口诀 “自底向上，取小加自己” 7️⃣ LC 64. 最小路径和 🟡题目描述从左上角到右下角的最小路径和。 🎨 图解思路grid:[1, 3, 1][1, 5, 1][4, 2, 1]dp[i][j] = min(dp[i-1][j], dp[i][j-1]) + grid[i][j]dp:[1, 4, 5][2, 7, 6][6, 8, 7]最小路径和 = 7 (1→3→1→1→1) 💻 代码实现def minPathSum(grid: list) - int: m, n = len(grid), len(grid[0]) dp = [[0] * n for _ in range(m)] for i in range(m): for j in range(n): if i == 0 and j == 0: dp[i][j] = grid[i][j] elif i == 0: dp[i][j] = dp[i][j-1] + grid[i][j] elif j == 0: dp[i][j] = dp[i-1][j] + grid[i][j] else: dp[i][j] = min(dp[i-1][j], dp[i][j-1]) + grid[i][j] return dp[m-1][n-1] 🧠 记忆口诀 “上左取小，加自己” 8️⃣ LC 63. 不同路径 II 🟡题目描述有障碍物的网格，从左上到右下的路径数。 🎨 图解思路grid: dp:[0, 0, 0] [1, 1, 1][0, 1, 0] → [1, 0, 1][0, 0, 0] [1, 1, 2]障碍物位置 dp = 0其他位置 dp = dp[上] + dp[左] 💻 代码实现def uniquePathsWithObstacles(obstacleGrid: list) - int: m, n = len(obstacleGrid), len(obstacleGrid[0]) if obstacleGrid[0][0] == 1: return 0 dp = [[0] * n for _ in range(m)] dp[0][0] = 1 # 初始化第一列 for i in range(1, m): if obstacleGrid[i][0] == 0: dp[i][0] = dp[i-1][0] # 初始化第一行 for j in range(1, n): if obstacleGrid[0][j] == 0: dp[0][j] = dp[0][j-1] # 填充 dp for i in range(1, m): for j in range(1, n): if obstacleGrid[i][j] == 0: dp[i][j] = dp[i-1][j] + dp[i][j-1] return dp[m-1][n-1] 🧠 记忆口诀 “障碍为0，否则上加左” 9️⃣ LC 5. 最长回文子串 🟡题目描述找出字符串中最长的回文子串。 🎨 图解思路s = \"babad\"dp[i][j] = s[i:j+1] 是否为回文条件: s[i] == s[j] and dp[i+1][j-1]填表顺序：按长度从小到大长度1: 全为 True长度2: s[i] == s[i+1]长度3+: s[i] == s[j] and dp[i+1][j-1] 💻 代码实现def longestPalindrome(s: str) - str: n = len(s) if n 2: return s dp = [[False] * n for _ in range(n)] start, max_len = 0, 1 # 所有长度为 1 的子串都是回文 for i in range(n): dp[i][i] = True # 按长度填表 for length in range(2, n + 1): for i in range(n - length + 1): j = i + length - 1 if s[i] == s[j]: if length == 2: dp[i][j] = True else: dp[i][j] = dp[i + 1][j - 1] if dp[i][j] and length max_len: start, max_len = i, length return s[start:start + max_len] 🔥 中心扩展法 (更优)def longestPalindrome(s: str) - str: def expand(left, right): while left = 0 and right len(s) and s[left] == s[right]: left -= 1 right += 1 return s[left + 1:right] result = \"\" for i in range(len(s)): # 奇数长度 odd = expand(i, i) # 偶数长度 even = expand(i, i + 1) result = max(result, odd, even, key=len) return result 🧠 记忆口诀 “首尾相同，中间也是回文” 🔟 LC 97. 交错字符串 🟡题目描述判断 s3 是否由 s1 和 s2 交错组成。 🎨 图解思路s1 = \"aabcc\", s2 = \"dbbca\", s3 = \"aadbbcbcac\"dp[i][j] = s1[0:i] 和 s2[0:j] 能否交错组成 s3[0:i+j]状态转移:dp[i][j] = (dp[i-1][j] and s1[i-1]==s3[i+j-1]) or (dp[i][j-1] and s2[j-1]==s3[i+j-1]) 💻 代码实现def isInterleave(s1: str, s2: str, s3: str) - bool: m, n = len(s1), len(s2) if m + n != len(s3): return False dp = [[False] * (n + 1) for _ in range(m + 1)] dp[0][0] = True # 初始化第一列 for i in range(1, m + 1): dp[i][0] = dp[i-1][0] and s1[i-1] == s3[i-1] # 初始化第一行 for j in range(1, n + 1): dp[0][j] = dp[0][j-1] and s2[j-1] == s3[j-1] # 填充 dp for i in range(1, m + 1): for j in range(1, n + 1): dp[i][j] = ((dp[i-1][j] and s1[i-1] == s3[i+j-1]) or (dp[i][j-1] and s2[j-1] == s3[i+j-1])) return dp[m][n] 🧠 记忆口诀 “上或左能到，且字符匹配” 1️⃣1️⃣ LC 72. 编辑距离 🟡题目描述将 word1 转换成 word2 所使用的最少操作数。 🎨 图解思路word1 = \"horse\", word2 = \"ros\"dp[i][j] = word1[0:i] 转换为 word2[0:j] 的最少操作三种操作:1. 插入: dp[i][j-1] + 12. 删除: dp[i-1][j] + 13. 替换: dp[i-1][j-1] + (0 if 相同 else 1)dp 表: \"\" r o s \"\" 0 1 2 3 h 1 1 2 3 o 2 2 1 2 r 3 2 2 2 s 4 3 3 2 e 5 4 4 3 💻 代码实现def minDistance(word1: str, word2: str) - int: m, n = len(word1), len(word2) dp = [[0] * (n + 1) for _ in range(m + 1)] # 初始化 for i in range(m + 1): dp[i][0] = i for j in range(n + 1): dp[0][j] = j # 填充 dp for i in range(1, m + 1): for j in range(1, n + 1): if word1[i-1] == word2[j-1]: dp[i][j] = dp[i-1][j-1] else: dp[i][j] = 1 + min( dp[i-1][j], # 删除 dp[i][j-1], # 插入 dp[i-1][j-1] # 替换 ) return dp[m][n] 🧠 记忆口诀 “相同不变，不同取三者最小加1” 📊 本章总结DP 问题分类┌──────────────────────────────────────────────────┐│ 动态规划分类 │├──────────────────────────────────────────────────┤│ ││ 线性 DP ││ ├─ 单序列: 爬楼梯, 打家劫舍, LIS ││ └─ 双序列: 编辑距离, 交错字符串 ││ ││ 区间 DP ││ └─ 回文子串 ││ ││ 背包 DP ││ └─ 零钱兑换, 单词拆分 ││ ││ 网格 DP ││ └─ 最小路径和, 不同路径 ││ │└──────────────────────────────────────────────────┘ 🧠 全章记忆口诀爬楼劫舍单词拆零钱递增三角来路径网格回文判交错编辑全都会 📖 下一篇：图论专题","tags":["LeetCode"],"categories":["算法"]},{"title":"🪟 LeetCode 150 - 滑动窗口专题","path":"/2026/01/18/leetcode-150-sliding-window/","content":"🪟 滑动窗口专题 (4题) 🎯 核心技巧：右扩左缩、哈希计数、条件收缩 🗺️ 滑动窗口核心思想┌─────────────────────────────────────────────────────────────┐│ 滑动窗口工作原理 │├─────────────────────────────────────────────────────────────┤│ ││ Step 1: 右边界扩张 ││ ┌───┬───┬───┬───┬───┬───┬───┐ ││ │ a │ b │ c │ d │ e │ f │ g │ ││ └───┴───┴───┴───┴───┴───┴───┘ ││ L R ──────▶ ││ └─────┬─────┘ ││ window ││ ││ Step 2: 满足条件时左边界收缩 ││ ┌───┬───┬───┬───┬───┬───┬───┐ ││ │ a │ b │ c │ d │ e │ f │ g │ ││ └───┴───┴───┴───┴───┴───┴───┘ ││ L ──▶ R ││ └───┬───┘ ││ window (收缩后) ││ │└─────────────────────────────────────────────────────────────┘ 🔧 滑动窗口万能模板def sliding_window(s): from collections import defaultdict window = defaultdict(int) # 窗口内容计数 left = 0 result = 0 # 或者其他结果变量 for right in range(len(s)): # ① 右边界扩张：将 s[right] 加入窗口 c = s[right] window[c] += 1 # ② 判断是否需要收缩（根据题目条件） while 需要收缩的条件: # 将 s[left] 移出窗口 d = s[left] window[d] -= 1 left += 1 # ③ 更新结果（根据题目要求） result = max(result, right - left + 1) return result 1️⃣ LC 209. 长度最小的子数组 🟡题目描述找出和 ≥ target 的最短连续子数组长度。 🎨 图解思路nums = [2, 3, 1, 2, 4, 3], target = 7窗口滑动过程：┌─────────────────────────────────────────┐│ [2] sum=2 7 扩张 ││ [2,3] sum=5 7 扩张 ││ [2,3,1] sum=6 7 扩张 ││ [2,3,1,2] sum=8 ≥ 7 记录4 ││ [3,1,2] sum=6 7 扩张 ││ [3,1,2,4] sum=10 ≥ 7 记录4 ││ [1,2,4] sum=7 ≥ 7 记录3 ││ [2,4] sum=6 7 扩张 ││ [2,4,3] sum=9 ≥ 7 记录3 ││ [4,3] sum=7 ≥ 7 记录2 ✓│└─────────────────────────────────────────┘最小长度 = 2 💻 代码实现def minSubArrayLen(target: int, nums: list) - int: left = 0 window_sum = 0 min_len = float('inf') for right in range(len(nums)): # 扩张：加入右边元素 window_sum += nums[right] # 收缩：满足条件时尝试缩小窗口 while window_sum = target: min_len = min(min_len, right - left + 1) window_sum -= nums[left] left += 1 return min_len if min_len != float('inf') else 0 🧠 记忆口诀 “够了就缩，不够就扩” 2️⃣ LC 3. 无重复字符的最长子串 🟡题目描述找出不含重复字符的最长子串长度。 🎨 图解思路s = \"abcabcbb\"窗口滑动过程：┌─────────────────────────────────────────┐│ [a] 无重复 len=1 ││ [a,b] 无重复 len=2 ││ [a,b,c] 无重复 len=3 ✓ ││ [a,b,c,a] 有重复! 收缩 ││ [b,c,a] 无重复 len=3 ││ [b,c,a,b] 有重复! 收缩 ││ [c,a,b] 无重复 len=3 ││ ... │└─────────────────────────────────────────┘最长无重复子串长度 = 3 💻 代码实现def lengthOfLongestSubstring(s: str) - int: window = {} # 记录字符出现次数 left = 0 max_len = 0 for right in range(len(s)): c = s[right] window[c] = window.get(c, 0) + 1 # 有重复字符时收缩 while window[c] 1: d = s[left] window[d] -= 1 left += 1 max_len = max(max_len, right - left + 1) return max_len 🔥 优化版本（记录位置）def lengthOfLongestSubstring(s: str) - int: char_index = {} # 记录字符最后出现的位置 left = 0 max_len = 0 for right, c in enumerate(s): # 如果字符在窗口内出现过，直接跳到重复位置之后 if c in char_index and char_index[c] = left: left = char_index[c] + 1 char_index[c] = right max_len = max(max_len, right - left + 1) return max_len 🧠 记忆口诀 “重复就跳过，记录最长度” 3️⃣ LC 30. 串联所有单词的子串 🔴题目描述找出字符串中所有是 words 中所有单词串联结果的起始索引。 🎨 图解思路s = \"barfoothefoobarman\"words = [\"foo\", \"bar\"] (每个长度为3)串联结果可能是 \"foobar\" 或 \"barfoo\"（长度6）检查每个可能的起始位置：位置0: \"barfoo\" ✓ (bar + foo)位置3: \"foothe\" ✗位置6: \"thefoo\" ✗位置9: \"foobar\" ✓ (foo + bar)...结果: [0, 9] 💻 代码实现def findSubstring(s: str, words: list) - list: if not s or not words: return [] from collections import Counter word_len = len(words[0]) word_count = len(words) total_len = word_len * word_count word_freq = Counter(words) result = [] # 只需要检查 word_len 种起始偏移 for offset in range(word_len): left = offset window = Counter() count = 0 # 窗口内有效单词数 for right in range(offset, len(s) - word_len + 1, word_len): word = s[right:right + word_len] if word in word_freq: window[word] += 1 count += 1 # 如果某单词超出需要的数量，收缩左边界 while window[word] word_freq[word]: left_word = s[left:left + word_len] window[left_word] -= 1 count -= 1 left += word_len # 找到一个有效的串联 if count == word_count: result.append(left) else: # 遇到不在 words 中的单词，重置窗口 window.clear() count = 0 left = right + word_len return result 🧠 记忆口诀 “固定单词长，滑动找串联” 4️⃣ LC 76. 最小覆盖子串 🔴题目描述找出 s 中包含 t 所有字符的最小子串。 🎨 图解思路s = \"ADOBECODEBANC\", t = \"ABC\"需要: A:1, B:1, C:1┌─────────────────────────────────────────────┐│ 窗口: [A]DOBECODEBANC 缺BC 扩张 ││ 窗口: [ADOBEC]ODEBANC 满足! 记录6 收缩 ││ 窗口: [DOBEC]ODEBANC 缺A 扩张 ││ 窗口: [DOBECODEBA]NC 满足! 记录10 收缩 ││ 窗口: [CODEBA]NC 满足! 记录6 收缩 ││ 窗口: [ODEBA]NC 缺C 扩张 ││ 窗口: [ODEBANC] 满足! 记录7 收缩 ││ 窗口: [BANC] 满足! 记录4 ✓ │└─────────────────────────────────────────────┘最小覆盖子串 = \"BANC\" 💻 代码实现def minWindow(s: str, t: str) - str: from collections import Counter need = Counter(t) # 需要的字符及数量 window = Counter() # 窗口内的字符 have = 0 # 已满足的字符种类数 need_count = len(need) # 需要满足的字符种类数 result = \"\" min_len = float('inf') left = 0 for right in range(len(s)): # 扩张：加入右边字符 c = s[right] window[c] += 1 # 如果该字符数量正好满足需求 if c in need and window[c] == need[c]: have += 1 # 收缩：当所有字符都满足时 while have == need_count: # 更新结果 if right - left + 1 min_len: min_len = right - left + 1 result = s[left:right + 1] # 移出左边字符 d = s[left] if d in need and window[d] == need[d]: have -= 1 window[d] -= 1 left += 1 return result 🧠 记忆口诀 “扩到满足，缩到不够” 📊 本章总结滑动窗口分类 类型 特点 典型题目 可变窗口求最小 满足条件就收缩 209, 76 可变窗口求最大 不满足条件才收缩 3 固定窗口 窗口大小固定 30 滑动窗口思维导图 满足条件？ │ ┌─────┴─────┐ ▼ ▼ 是 否 │ │ ▼ ▼求最小？ 求最大？ │ │ ▼ ▼ 收缩 扩张更新答案 继续扩张 🧠 全章记忆口诀最无串覆四道题滑动窗口巧解析右扩左缩是关键满足条件再收缩最 - 长度最小的子数组 (209)无 - 无重复字符的最长子串 (3)串 - 串联所有单词的子串 (30)覆 - 最小覆盖子串 (76) 📖 下一篇：链表专题","tags":["LeetCode"],"categories":["算法"]},{"title":"🎯 LeetCode 面试经典 150 题 - 完全攻略","path":"/2026/01/18/leetcode-150-index/","content":"🎯 LeetCode 面试经典 150 题 - 完全攻略 📚 目标：系统掌握算法面试核心知识点，6个月拿下大厂 Offer！ 📊 题目分类总览┌─────────────────────────────────────────────────────────────┐│ LeetCode 面试 150 题 │├─────────────┬─────────────┬─────────────┬─────────────────────┤│ 数组/字符串 │ 双指针 │ 滑动窗口 │ 矩阵 ││ (15题) │ (5题) │ (4题) │ (4题) │├─────────────┼─────────────┼─────────────┼─────────────────────┤│ 哈希表 │ 区间 │ 栈 │ 链表 ││ (9题) │ (6题) │ (7题) │ (11题) │├─────────────┼─────────────┼─────────────┼─────────────────────┤│ 二叉树 │ 二叉树BFS │ 二叉搜索树 │ 图 ││ (14题) │ (4题) │ (5题) │ (8题) │├─────────────┼─────────────┼─────────────┼─────────────────────┤│ 回溯 │ 分治 │ Kadane算法 │ 二分查找 ││ (7题) │ (3题) │ (2题) │ (7题) │├─────────────┼─────────────┼─────────────┼─────────────────────┤│ 堆 │ 位运算 │ 数学 │ 一维动态规划 ││ (5题) │ (6题) │ (9题) │ (10题) │├─────────────┴─────────────┴─────────────┴─────────────────────┤│ 多维动态规划 (12题) │└─────────────────────────────────────────────────────────────┘ 🗺️ 学习路线图Week 1-2 Week 3-4 Week 5-6 Week 7-8 │ │ │ │ ▼ ▼ ▼ ▼┌──────┐ ┌──────┐ ┌──────┐ ┌──────┐│ 数组 │───────▶│ 链表 │───────▶│ 树 │───────▶│ 图 ││ 字符串 │ │ 栈 │ │ 堆 │ │ 回溯 ││ 双指针 │ │ 队列 │ │ BFS │ │ DFS │└──────┘ └──────┘ └──────┘ └──────┘ │ ┌───────────────────────────────┘ ▼ Week 9-12 ┌──────────────┐ │ 动态规划 │ │ 二分查找 │ │ 位运算 │ └──────────────┘ 📑 分类详解目录1️⃣ 数组与字符串 (15题) # 题目 难度 核心技巧 88 合并两个有序数组 🟢 双指针从后往前 27 移除元素 🟢 快慢指针 26 删除有序数组中的重复项 🟢 快慢指针 80 删除有序数组中的重复项 II 🟡 通用模板 169 多数元素 🟢 Boyer-Moore投票 189 轮转数组 🟡 三次翻转 121 买卖股票的最佳时机 🟢 维护最小值 122 买卖股票的最佳时机 II 🟡 贪心累加 55 跳跃游戏 🟡 贪心最远距离 45 跳跃游戏 II 🟡 BFS思想 274 H 指数 🟡 计数排序 380 O(1) 时间插入、删除和获取随机元素 🟡 哈希+数组 238 除自身以外数组的乘积 🟡 前缀积+后缀积 134 加油站 🟡 一次遍历 135 分发糖果 🔴 两次遍历 🧠 记忆口诀： “合移删删多，轮买买跳跳，H插除加糖” 2️⃣ 双指针 (5题) # 题目 难度 核心技巧 125 验证回文串 🟢 相向双指针 392 判断子序列 🟢 同向双指针 167 两数之和 II 🟡 相向双指针 11 盛最多水的容器 🟡 贪心+双指针 15 三数之和 🟡 排序+双指针 🎨 双指针模式图解： 相向双指针 (Two Sum, Container) 同向双指针 (Fast-Slow) L ──────────────▶ R Slow ──▶ Fast ──▶ │ │ │ │ ▼ ▼ ▼ ▼ [1, 2, 3, 4, 5, 6, 7] [1, 2, 3, 4, 5, 6, 7] 🧠 记忆口诀： “验判两盛三，相向加同向” 3️⃣ 滑动窗口 (4题) # 题目 难度 核心技巧 209 长度最小的子数组 🟡 可变窗口 3 无重复字符的最长子串 🟡 哈希+滑窗 30 串联所有单词的子串 🔴 固定窗口 76 最小覆盖子串 🔴 可变窗口 🎨 滑动窗口模板： def sliding_window(s): left = 0 window = {} # 窗口内容 result = 0 for right in range(len(s)): # 1. 右边界扩张，更新窗口 window[s[right]] = window.get(s[right], 0) + 1 # 2. 满足收缩条件时，左边界收缩 while need_shrink(window): window[s[left]] -= 1 left += 1 # 3. 更新结果 result = max(result, right - left + 1) return result 🧠 记忆口诀： “右扩左缩更新果，窗口滑动解最优” 4️⃣ 哈希表 (9题) # 题目 难度 核心技巧 383 赎金信 🟢 字符计数 205 同构字符串 🟢 双向映射 290 单词规律 🟢 双向映射 242 有效的字母异位词 🟢 字符计数 49 字母异位词分组 🟡 排序作key 1 两数之和 🟢 补数查找 202 快乐数 🟢 检测循环 219 存在重复元素 II 🟢 滑动窗口 128 最长连续序列 🟡 并查集思想 🧠 记忆口诀： “赎同单有组，两快存最长” 5️⃣ 栈 (7题) # 题目 难度 核心技巧 20 有效的括号 🟢 匹配出栈 71 简化路径 🟡 模拟栈 155 最小栈 🟡 辅助栈 150 逆波兰表达式求值 🟡 操作数栈 224 基本计算器 🔴 递归/栈 227 基本计算器 II 🟡 优先级栈 772 基本计算器 III 🔴 递归 🎨 栈的核心思想： 后进先出 (LIFO) │ Push ▼ ┌─────────┐ │ Top │ ◀── Pop ├─────────┤ │ │ ├─────────┤ │ │ └─────────┘ 6️⃣ 链表 (11题) # 题目 难度 核心技巧 141 环形链表 🟢 快慢指针 2 两数相加 🟡 进位处理 21 合并两个有序链表 🟢 双指针 138 随机链表的复制 🟡 哈希/交织 92 反转链表 II 🟡 头插法 25 K 个一组翻转链表 🔴 分组反转 19 删除链表的倒数第 N 个结点 🟡 快慢指针 82 删除排序链表中的重复元素 II 🟡 虚拟头节点 61 旋转链表 🟡 成环断开 86 分隔链表 🟡 双链表 146 LRU 缓存 🟡 哈希+双向链表 🎨 链表反转核心代码： def reverse(head): prev, curr = None, head while curr: next_temp = curr.next # 1. 保存下一个 curr.next = prev # 2. 反转指向 prev = curr # 3. prev前进 curr = next_temp # 4. curr前进 return prev 🧠 记忆口诀： “存反前进，四步反转” 7️⃣ 二叉树 (14题) # 题目 难度 核心技巧 104 二叉树的最大深度 🟢 DFS递归 100 相同的树 🟢 递归比较 226 翻转二叉树 🟢 递归交换 101 对称二叉树 🟢 镜像递归 105 从前序与中序遍历序列构造二叉树 🟡 分治 106 从中序与后序遍历序列构造二叉树 🟡 分治 117 填充每个节点的下一个右侧节点指针 II 🟡 BFS层序 114 二叉树展开为链表 🟡 前序遍历 112 路径总和 🟢 DFS 129 求根节点到叶节点数字之和 🟡 DFS 124 二叉树中的最大路径和 🔴 后序遍历 173 二叉搜索树迭代器 🟡 栈模拟 222 完全二叉树的节点个数 🟡 二分+位运算 236 二叉树的最近公共祖先 🟡 递归 🎨 树的遍历方式： / \\ 2 3 / \\ 4 5 前序(根左右): 1 → 2 → 4 → 5 → 3中序(左根右): 4 → 2 → 5 → 1 → 3后序(左右根): 4 → 5 → 2 → 3 → 1层序(BFS): 1 → 2 → 3 → 4 → 5 ---### 8️⃣ 图与搜索 (8题)| # | 题目 | 难度 | 核心技巧 | | 200 | 岛屿数量 | 🟡 | DFS/BFS || 130 | 被围绕的区域 | 🟡 | 边界DFS || 133 | 克隆图 | 🟡 | BFS+哈希 || 399 | 除法求值 | 🟡 | 带权图BFS || 207 | 课程表 | 🟡 | 拓扑排序 || 210 | 课程表 II | 🟡 | 拓扑排序 || 909 | 蛇梯棋 | 🟡 | BFS最短路 || 433 | 最小基因变化 | 🟡 | BFS | 🎨 BFS 模板： from collections import dequedef bfs(start): queue = deque([start]) visited = {start} level = 0 while queue: size = len(queue) for _ in range(size): node = queue.popleft() # 处理当前节点 for neighbor in get_neighbors(node): if neighbor not in visited: visited.add(neighbor) queue.append(neighbor) level += 1 return level 9️⃣ 回溯算法 (7题) # 题目 难度 核心技巧 17 电话号码的字母组合 🟡 回溯枚举 77 组合 🟡 回溯+剪枝 46 全排列 🟡 回溯 39 组合总和 🟡 回溯 52 N 皇后 II 🔴 回溯+剪枝 22 括号生成 🟡 回溯 79 单词搜索 🟡 回溯+DFS 🎨 回溯模板： def backtrack(path, choices): if 满足结束条件: result.append(path[:]) return for choice in choices: # 做选择 path.append(choice) # 递归 backtrack(path, new_choices) # 撤销选择 path.pop() 🧠 记忆口诀： “选择、递归、撤销，回溯三部曲” 🔟 动态规划 (22题)一维 DP (10题) # 题目 难度 核心技巧 70 爬楼梯 🟢 斐波那契 198 打家劫舍 🟡 状态转移 139 单词拆分 🟡 背包变形 322 零钱兑换 🟡 完全背包 300 最长递增子序列 🟡 LIS 152 乘积最大子数组 🟡 维护最大最小 多维 DP (12题) # 题目 难度 核心技巧 120 三角形最小路径和 🟡 自底向上 64 最小路径和 🟡 网格DP 63 不同路径 II 🟡 网格DP 5 最长回文子串 🟡 区间DP 72 编辑距离 🟡 经典DP 97 交错字符串 🟡 二维DP 123 买卖股票的最佳时机 III 🔴 状态机DP 🎨 DP 思考框架： 1. 定义状态 ──▶ dp[i] 表示什么？ │ ▼2. 状态转移 ──▶ dp[i] 和 dp[i-1] 的关系？ │ ▼3. 初始条件 ──▶ dp[0] = ? │ ▼4. 计算顺序 ──▶ 从小到大 or 从大到小？ │ ▼5. 返回结果 ──▶ dp[n] or max(dp)? 🧠 记忆口诀： “定状态、找转移、设初值、定顺序、取结果” 🏆 刷题进度追踪 分类 总题数 已完成 进度 数组/字符串 15 ⬜ 0% 双指针 5 ⬜ 0% 滑动窗口 4 ⬜ 0% 哈希表 9 ⬜ 0% 栈 7 ⬜ 0% 链表 11 ⬜ 0% 二叉树 14 ⬜ 0% 图/搜索 8 ⬜ 0% 回溯 7 ⬜ 0% 动态规划 22 ⬜ 0% 其他 48 ⬜ 0% 总计 150 0 0% 📚 推荐学习资源 LeetCode 官方题单 - 原题链接 代码随想录 - 详细图解 labuladong 的算法小抄 - 框架思维 🎯 复习计划每日任务：┌────────────────────────────────────────┐│ 📅 工作日：2-3 题 (1小时) ││ 📅 周末：复习本周题目 + 5题新题 ││ 📅 月末：分类专项突破 + 模拟面试 │└────────────────────────────────────────┘ 💪 加油！坚持刷完这 150 题，大厂 Offer 在向你招手！","tags":["LeetCode"],"categories":["算法"]},{"title":"🔍 LeetCode 150 - 图论与搜索专题","path":"/2026/01/18/leetcode-150-graph/","content":"🔍 图论与搜索专题 (8题) 🎯 核心技巧：BFS、DFS、拓扑排序、并查集 🗺️ 图搜索算法对比┌─────────────────────────────────────────────────────────────┐│ BFS vs DFS │├──────────────────────┬──────────────────────────────────────┤│ BFS │ DFS ││ (广度优先搜索) │ (深度优先搜索) │├──────────────────────┼──────────────────────────────────────┤│ 数据结构：队列 │ 数据结构：栈/递归 ││ 探索方式：层层扩展 │ 探索方式：一路走到底 ││ 适用：最短路径 │ 适用：连通性、路径搜索 │├──────────────────────┼──────────────────────────────────────┤│ 1 │ 1 ││ /|\\ │ /|\\ ││ 2 3 4 → 层序 │ 2 3 4 → 深入 ││ /| | │ /| | ││ 5 6 7 │ 5 6 7 ││ │ ││ 顺序: 1→2→3→4→5→6→7 │ 顺序: 1→2→5→6→3→4→7 │└──────────────────────┴──────────────────────────────────────┘ 🔧 BFS 模板from collections import dequedef bfs(start): queue = deque([start]) visited = {start} level = 0 while queue: size = len(queue) for _ in range(size): node = queue.popleft() # 处理当前节点 process(node) # 将邻居加入队列 for neighbor in get_neighbors(node): if neighbor not in visited: visited.add(neighbor) queue.append(neighbor) level += 1 # 层数 +1 return level 🔧 DFS 模板def dfs(node, visited): if node in visited: return visited.add(node) # 处理当前节点 process(node) # 递归访问邻居 for neighbor in get_neighbors(node): dfs(neighbor, visited) 1️⃣ LC 200. 岛屿数量 🟡题目描述计算二维网格中岛屿的数量（由 ‘1’ 组成的连通区域）。 🎨 图解思路grid:1 1 0 0 01 1 0 0 00 0 1 0 00 0 0 1 1岛屿数量 = 3策略：遍历网格，遇到 '1' 就启动 DFS/BFS把整个岛屿标记为已访问，计数 +1 💻 代码实现 (DFS)def numIslands(grid: list) - int: if not grid: return 0 m, n = len(grid), len(grid[0]) count = 0 def dfs(i, j): # 边界检查 是否为陆地 if i 0 or i = m or j 0 or j = n or grid[i][j] != '1': return # 标记为已访问 grid[i][j] = '0' # 四个方向扩展 dfs(i + 1, j) dfs(i - 1, j) dfs(i, j + 1) dfs(i, j - 1) for i in range(m): for j in range(n): if grid[i][j] == '1': dfs(i, j) count += 1 return count 🧠 记忆口诀 “遇1就淹，淹完计数” 2️⃣ LC 130. 被围绕的区域 🟡题目描述将所有被 ‘X’ 围绕的 ‘O’ 填充为 ‘X’（边界上的 ‘O’ 及其连通的 ‘O’ 不算被围绕）。 🎨 图解思路输入: 输出:X X X X X X X XX O O X → X X X XX X O X X X X XX O X X X O X X逆向思维：1. 从边界的 'O' 开始 DFS，标记为 '#'2. 遍历整个网格： - 'O' → 'X' (被围绕) - '#' → 'O' (恢复) 💻 代码实现def solve(board: list) - None: if not board: return m, n = len(board), len(board[0]) def dfs(i, j): if i 0 or i = m or j 0 or j = n or board[i][j] != 'O': return board[i][j] = '#' dfs(i + 1, j) dfs(i - 1, j) dfs(i, j + 1) dfs(i, j - 1) # 从边界开始标记 for i in range(m): dfs(i, 0) dfs(i, n - 1) for j in range(n): dfs(0, j) dfs(m - 1, j) # 恢复和填充 for i in range(m): for j in range(n): if board[i][j] == 'O': board[i][j] = 'X' elif board[i][j] == '#': board[i][j] = 'O' 🧠 记忆口诀 “边界O不围，标记后恢复” 3️⃣ LC 133. 克隆图 🟡题目描述深拷贝一个无向连通图。 🎨 图解思路原图: 克隆: 1 --- 2 1' --- 2' | | | | 4 --- 3 4' --- 3'使用哈希表记录 原节点 → 克隆节点 的映射BFS 或 DFS 遍历并克隆 💻 代码实现def cloneGraph(node: 'Node') - 'Node': if not node: return None # 哈希表：原节点 → 克隆节点 cloned = {} def dfs(node): if node in cloned: return cloned[node] # 创建克隆节点 clone = Node(node.val) cloned[node] = clone # 克隆邻居 for neighbor in node.neighbors: clone.neighbors.append(dfs(neighbor)) return clone return dfs(node) 🧠 记忆口诀 “哈希记映射，DFS克隆” 4️⃣ LC 399. 除法求值 🟡题目描述给定变量对的除法结果，求解其他除法。 🎨 图解思路equations: [[\"a\",\"b\"],[\"b\",\"c\"]]values: [2.0, 3.0]构建带权图:a --2.0-- b --3.0-- ca --0.5-- b --0.33-- c查询 a/c = a/b * b/c = 2.0 * 3.0 = 6.0使用 BFS 找路径并累乘权重 💻 代码实现def calcEquation(equations, values, queries): from collections import defaultdict, deque # 构建图 graph = defaultdict(dict) for (a, b), val in zip(equations, values): graph[a][b] = val graph[b][a] = 1.0 / val def bfs(start, end): if start not in graph or end not in graph: return -1.0 if start == end: return 1.0 queue = deque([(start, 1.0)]) visited = {start} while queue: node, product = queue.popleft() for neighbor, weight in graph[node].items(): if neighbor == end: return product * weight if neighbor not in visited: visited.add(neighbor) queue.append((neighbor, product * weight)) return -1.0 return [bfs(a, b) for a, b in queries] 🧠 记忆口诀 “带权图建边，BFS累乘” 5️⃣ LC 207. 课程表 🟡题目描述判断是否可能完成所有课程（检测有向图是否有环）。 🎨 图解思路numCourses = 4prerequisites = [[1,0],[2,0],[3,1],[3,2]]构建图:0 → 1 → 30 → 2 ↗拓扑排序：1. 统计每个节点的入度2. 将入度为 0 的节点入队3. BFS 处理，每处理一个节点，邻居入度 -14. 入度变为 0 的节点入队5. 如果处理的节点数 = 总课程数，则无环 💻 代码实现def canFinish(numCourses: int, prerequisites: list) - bool: from collections import defaultdict, deque # 构建图和入度表 graph = defaultdict(list) in_degree = [0] * numCourses for course, prereq in prerequisites: graph[prereq].append(course) in_degree[course] += 1 # 将入度为 0 的节点入队 queue = deque([i for i in range(numCourses) if in_degree[i] == 0]) count = 0 while queue: node = queue.popleft() count += 1 for neighbor in graph[node]: in_degree[neighbor] -= 1 if in_degree[neighbor] == 0: queue.append(neighbor) return count == numCourses 🧠 记忆口诀 “入度为零先处理，全部处理完无环” 6️⃣ LC 210. 课程表 II 🟡题目描述返回完成所有课程的学习顺序（拓扑排序结果）。 💻 代码实现def findOrder(numCourses: int, prerequisites: list) - list: from collections import defaultdict, deque graph = defaultdict(list) in_degree = [0] * numCourses for course, prereq in prerequisites: graph[prereq].append(course) in_degree[course] += 1 queue = deque([i for i in range(numCourses) if in_degree[i] == 0]) result = [] while queue: node = queue.popleft() result.append(node) for neighbor in graph[node]: in_degree[neighbor] -= 1 if in_degree[neighbor] == 0: queue.append(neighbor) return result if len(result) == numCourses else [] 🧠 记忆口诀 “拓扑排序，记录顺序” 7️⃣ LC 909. 蛇梯棋 🟡题目描述在蛇梯棋盘上，求从起点到终点的最少移动次数。 🎨 图解思路棋盘按 Boustrophedon 方式编号（蛇形）使用 BFS 求最短路径每次可以走 1-6 步（掷骰子）遇到蛇/梯子则传送到对应位置 💻 代码实现def snakesAndLadders(board: list) - int: from collections import deque n = len(board) # 将位置编号转为坐标 def get_position(num): num -= 1 row = n - 1 - num // n col = num % n if (n - 1 - row) % 2 == 0 else n - 1 - num % n return row, col queue = deque([(1, 0)]) # (位置, 步数) visited = {1} while queue: curr, steps = queue.popleft() for dice in range(1, 7): next_pos = curr + dice if next_pos n * n: continue r, c = get_position(next_pos) if board[r][c] != -1: next_pos = board[r][c] if next_pos == n * n: return steps + 1 if next_pos not in visited: visited.add(next_pos) queue.append((next_pos, steps + 1)) return -1 🧠 记忆口诀 “BFS找最短，蛇梯要传送” 8️⃣ LC 433. 最小基因变化 🟡题目描述从起始基因变化到目标基因的最少变化次数。 🎨 图解思路startGene = \"AACCGGTT\"endGene = \"AAACGGTA\"bank = [\"AACCGGTA\",\"AACCGCTA\",\"AAACGGTA\"]每次只能变一个字符，且结果必须在 bank 中使用 BFS 搜索最短路径 💻 代码实现def minMutation(startGene: str, endGene: str, bank: list) - int: from collections import deque bank_set = set(bank) if endGene not in bank_set: return -1 queue = deque([(startGene, 0)]) visited = {startGene} chars = ['A', 'C', 'G', 'T'] while queue: gene, steps = queue.popleft() if gene == endGene: return steps for i in range(len(gene)): for c in chars: if c != gene[i]: new_gene = gene[:i] + c + gene[i+1:] if new_gene in bank_set and new_gene not in visited: visited.add(new_gene) queue.append((new_gene, steps + 1)) return -1 🧠 记忆口诀 “每次变一个，在库中才行” 📊 本章总结图搜索场景选择 场景 推荐算法 典型题目 最短路径 BFS 909, 433 连通性 DFS/BFS 200, 130 拓扑排序 BFS (Kahn) 207, 210 图克隆 DFS + 哈希 133 带权路径 BFS + 累乘 399 🧠 全章记忆口诀岛围克除课课蛇基图论八题记仔细岛 - 岛屿数量 (200)围 - 被围绕的区域 (130)克 - 克隆图 (133)除 - 除法求值 (399)课课 - 课程表 I/II (207, 210)蛇 - 蛇梯棋 (909)基 - 最小基因变化 (433) 📖 返回：LeetCode 150 题总目录","tags":["LeetCode"],"categories":["算法"]},{"title":"⛰️ LeetCode 150 - 堆/优先队列专题","path":"/2026/01/18/leetcode-150-heap/","content":"⛰️ 堆/优先队列专题 (5题) 🎯 核心特性：快速获取最大/最小值，O(log n) 插入删除 🗺️ 堆的基础知识┌─────────────────────────────────────────────────────────────┐│ 堆的结构 │├─────────────────────────────────────────────────────────────┤│ ││ 最小堆 (Min Heap) 最大堆 (Max Heap) ││ 1 9 ││ / \\ / \\ ││ 3 5 7 8 ││ / \\ / \\ ││ 7 8 3 5 ││ ││ 父节点 ≤ 子节点 父节点 ≥ 子节点 ││ 堆顶是最小值 堆顶是最大值 ││ │├─────────────────────────────────────────────────────────────┤│ Python 中使用 heapq（默认最小堆） ││ ││ import heapq ││ heapq.heappush(heap, item) # 入堆 ││ heapq.heappop(heap) # 出堆 ││ heapq.heapify(list) # 列表转堆 ││ heap[0] # 查看堆顶 ││ ││ 最大堆技巧：存入负数 ││ heapq.heappush(heap, -item) ││ │└─────────────────────────────────────────────────────────────┘ 🔧 堆的常见应用┌─────────────────────────────────────────────────────────────┐│ 堆的典型应用场景 │├─────────────────────────────────────────────────────────────┤│ ││ 1. Top K 问题 ││ 找第K大 → 维护大小为K的最小堆 ││ 找第K小 → 维护大小为K的最大堆 ││ ││ 2. 合并K个有序列表 ││ 用堆维护K个列表的当前最小元素 ││ ││ 3. 数据流中的中位数 ││ 两个堆：最大堆（左半部分）+ 最小堆（右半部分） ││ ││ 4. 任务调度 ││ 按优先级处理任务 ││ │└─────────────────────────────────────────────────────────────┘ 1️⃣ LC 215. 数组中的第K个最大元素 🟡题目描述在未排序的数组中找到第 k 个最大的元素。 🎨 图解思路nums = [3,2,1,5,6,4], k = 2方法1：排序后取第k个 O(n log n)方法2：最小堆维护k个最大元素 O(n log k)方法3：快速选择 O(n) 平均最小堆方法：维护大小为k的最小堆，堆顶就是第k大遍历: 3 → [3] 2 → [2,3] (k=2) 1 → [2,3] (12，不入堆) 5 → [3,5] (5入堆，弹出2) 6 → [5,6] (6入堆，弹出3) 4 → [5,6] (45，不入堆)结果: 堆顶 5 💻 代码实现import heapqdef findKthLargest(nums: list, k: int) - int: # 方法1：最小堆 heap = [] for num in nums: heapq.heappush(heap, num) if len(heap) k: heapq.heappop(heap) return heap[0] # 方法2：直接用 nlargest # return heapq.nlargest(k, nums)[-1] 🧠 记忆口诀 “第K大用最小堆，堆顶就是答案” 2️⃣ LC 502. IPO 🔴题目描述给定若干项目（利润和资本需求），初始资本 w，最多做 k 个项目，求最大资本。 🎨 图解思路k = 2, w = 0profits = [1,2,3]capitals = [0,1,1]贪心 + 堆：每次选择当前资本能做的项目中利润最大的初始 w=0: 可做项目: (profit=1, capital=0) 做项目1 → w=1w=1: 可做项目: (profit=2, capital=1), (profit=3, capital=1) 选最大 → 做项目3 → w=4结果: 4 💻 代码实现import heapqdef findMaximizedCapital(k: int, w: int, profits: list, capital: list) - int: # 按资本排序的项目列表 projects = sorted(zip(capital, profits)) max_heap = [] # 最大堆存可做项目的利润 i = 0 n = len(projects) for _ in range(k): # 把当前资本能做的项目加入堆 while i n and projects[i][0] = w: heapq.heappush(max_heap, -projects[i][1]) # 负数模拟最大堆 i += 1 if not max_heap: break # 选利润最大的项目 w += -heapq.heappop(max_heap) return w 🧠 记忆口诀 “资本够就入堆，贪心选最大利润” 3️⃣ LC 373. 查找和最小的 K 对数字 🟡题目描述从两个升序数组中找出和最小的 k 对数字。 🎨 图解思路nums1 = [1,7,11], nums2 = [2,4,6], k = 3可视化矩阵（和）: 2 4 6 1 3 5 7 7 9 11 13 11 13 15 17BFS思想：从(0,0)开始，每次扩展右边和下边初始: (1+2=3, i=0, j=0)弹出(3), 加入(1+4=5, 0,1) 和 (7+2=9, 1,0)弹出(5), 加入(1+6=7, 0,2)弹出(7), ...结果: [(1,2), (1,4), (1,6)] 💻 代码实现import heapqdef kSmallestPairs(nums1: list, nums2: list, k: int) - list: if not nums1 or not nums2: return [] result = [] heap = [(nums1[0] + nums2[0], 0, 0)] # (sum, i, j) visited = {(0, 0)} while heap and len(result) k: _, i, j = heapq.heappop(heap) result.append([nums1[i], nums2[j]]) # 扩展到右边 if i + 1 len(nums1) and (i + 1, j) not in visited: heapq.heappush(heap, (nums1[i + 1] + nums2[j], i + 1, j)) visited.add((i + 1, j)) # 扩展到下边 if j + 1 len(nums2) and (i, j + 1) not in visited: heapq.heappush(heap, (nums1[i] + nums2[j + 1], i, j + 1)) visited.add((i, j + 1)) return result 🧠 记忆口诀 “矩阵BFS，堆取最小扩展” 4️⃣ LC 295. 数据流的中位数 🔴题目描述设计一个支持添加数字和获取中位数的数据结构。 🎨 图解思路用两个堆：- 最大堆 (left): 存较小的一半- 最小堆 (right): 存较大的一半保持: len(left) == len(right) 或 len(left) == len(right) + 1数据流: [1, 2, 3]add(1): left=[1], right=[]add(2): left=[1], right=[2]add(3): left=[1,2], right=[3] left(max) right(min) [2] [3] [1]中位数: - 奇数个: left堆顶- 偶数个: (left堆顶 + right堆顶) / 2 💻 代码实现import heapqclass MedianFinder: def __init__(self): self.left = [] # 最大堆（存负数） self.right = [] # 最小堆 def addNum(self, num: int) - None: # 先加入left heapq.heappush(self.left, -num) # 把left最大的给right heapq.heappush(self.right, -heapq.heappop(self.left)) # 平衡：保持left = right if len(self.left) len(self.right): heapq.heappush(self.left, -heapq.heappop(self.right)) def findMedian(self) - float: if len(self.left) len(self.right): return -self.left[0] return (-self.left[0] + self.right[0]) / 2 🧠 记忆口诀 “左大右小，平衡保持，堆顶找中位” 5️⃣ LC 23. 合并 K 个升序链表 🔴题目描述将 k 个升序链表合并成一个升序链表。 🎨 图解思路lists = [[1,4,5], [1,3,4], [2,6]]用最小堆维护k个链表的当前头节点初始堆: [1, 1, 2] (三个链表头)弹出1 → 结果[1]，加入4堆: [1, 2, 4]弹出1 → 结果[1,1]，加入3堆: [2, 3, 4]...结果: [1,1,2,3,4,4,5,6] 💻 代码实现import heapqdef mergeKLists(lists): # 自定义比较（避免链表节点直接比较） heap = [] for i, lst in enumerate(lists): if lst: heapq.heappush(heap, (lst.val, i, lst)) dummy = ListNode(0) curr = dummy while heap: val, i, node = heapq.heappop(heap) curr.next = node curr = curr.next if node.next: heapq.heappush(heap, (node.next.val, i, node.next)) return dummy.next 🧠 记忆口诀 “K个头入堆，弹最小接后继” 📊 本章总结题目速查表 题号 题目 难度 类型 215 第K个最大元素 🟡 Top K 502 IPO 🔴 贪心+堆 373 K对最小和 🟡 多路归并 295 数据流中位数 🔴 双堆 23 合并K个链表 🔴 多路归并 堆的解题模式┌─────────────────────────────────────────────────────────────┐│ 堆的解题模式 │├─────────────────────────────────────────────────────────────┤│ ││ Top K 问题: ││ ───────── ││ 第K大 → 大小为K的最小堆 ││ 第K小 → 大小为K的最大堆 ││ ││ 多路归并: ││ ───────── ││ 堆中维护每路的当前元素 ││ 每次取最小，然后加入该路的下一个 ││ ││ 双堆技巧: ││ ───────── ││ 中位数：左半最大堆 + 右半最小堆 ││ 滑动窗口中位数同理 ││ │└─────────────────────────────────────────────────────────────┘ 🧠 全章记忆口诀TopK问题堆来凑第K大用小堆守多路归并维护头双堆中位数最溜215 - 第K大元素502 - IPO项目选择373 - K对最小和295 - 数据流中位数23 - 合并K个链表 📖 下一篇：二分查找专题","tags":["LeetCode"],"categories":["算法"]},{"title":"📦 LeetCode 150 - 栈与队列专题","path":"/2026/01/18/leetcode-150-stack/","content":"📦 栈与队列专题 (7题) 🎯 核心特性：栈 LIFO（后进先出），队列 FIFO（先进先出） 🗺️ 栈的核心应用场景┌─────────────────────────────────────────────────────────────┐│ 栈的应用场景 │├─────────────────────────────────────────────────────────────┤│ ││ 1. 括号匹配 ││ 遇到左括号入栈，遇到右括号出栈匹配 ││ ││ 2. 表达式求值 ││ 操作数栈 + 运算符栈 ││ ││ 3. 单调栈 ││ 找下一个更大/更小元素 ││ ││ 4. 路径简化 ││ 处理 \".\" 和 \"..\" ││ ││ ┌───┐ ││ │ C │ ← Top (后进先出) ││ ├───┤ ││ │ B │ ││ ├───┤ ││ │ A │ ││ └───┘ ││ │└─────────────────────────────────────────────────────────────┘ 1️⃣ LC 20. 有效的括号 🟢题目描述判断括号字符串是否有效。 🎨 图解思路s = \"([{}])\"处理过程:字符 操作 栈状态( 入栈 [(][ 入栈 [(, []{ 入栈 [(, [, {]} 出栈匹配 [(, [] ✓ { 匹配] 出栈匹配 [(] ✓ [ 匹配) 出栈匹配 [] ✓ ( 匹配栈为空 → 有效！ 💻 代码实现def isValid(s: str) - bool: stack = [] mapping = {')': '(', ']': '[', '}': '{'} for char in s: if char in mapping: # 右括号：出栈匹配 if not stack or stack.pop() != mapping[char]: return False else: # 左括号：入栈 stack.append(char) return len(stack) == 0 🧠 记忆口诀 “左入右出，空栈有效” 2️⃣ LC 71. 简化路径 🟡题目描述简化 Unix 风格的绝对路径。 🎨 图解思路path = \"/a/./b/../../c/\"处理规则:. → 当前目录，忽略.. → 上级目录，出栈其他 → 目录名，入栈处理过程:a → 入栈 → [a]. → 忽略 → [a]b → 入栈 → [a, b].. → 出栈 → [a].. → 出栈 → []c → 入栈 → [c]结果: \"/c\" 💻 代码实现def simplifyPath(path: str) - str: stack = [] for part in path.split('/'): if part == '..': if stack: stack.pop() elif part and part != '.': stack.append(part) return '/' + '/'.join(stack) 🧠 记忆口诀 “点忽略，双点出栈，其他入栈” 3️⃣ LC 155. 最小栈 🟡题目描述设计一个支持 O(1) 获取最小值的栈。 🎨 图解思路使用辅助栈同步记录当前最小值主栈 辅助栈（记录当前最小）push(5) [5] [5]push(3) [5,3] [5,3] ← 3更小push(7) [5,3,7] [5,3,3] ← 最小仍是3pop() [5,3] [5,3]getMin() 返回 3 💻 代码实现class MinStack: def __init__(self): self.stack = [] self.min_stack = [] def push(self, val: int) - None: self.stack.append(val) # 辅助栈：记录当前最小值 if not self.min_stack or val = self.min_stack[-1]: self.min_stack.append(val) else: self.min_stack.append(self.min_stack[-1]) def pop(self) - None: self.stack.pop() self.min_stack.pop() def top(self) - int: return self.stack[-1] def getMin(self) - int: return self.min_stack[-1] 🧠 记忆口诀 “辅助栈同步记录最小值” 4️⃣ LC 150. 逆波兰表达式求值 🟡题目描述计算逆波兰表达式（后缀表达式）的值。 🎨 图解思路tokens = [\"2\",\"1\",\"+\",\"3\",\"*\"]等价于: (2 + 1) * 3 = 9处理过程（遇到运算符弹出两个操作数）:2 → 入栈 → [2]1 → 入栈 → [2, 1]+ → 弹出1,2，计算2+1=3 → [3]3 → 入栈 → [3, 3]* → 弹出3,3，计算3*3=9 → [9]结果: 9 💻 代码实现def evalRPN(tokens: list) - int: stack = [] operators = {'+', '-', '*', '/'} for token in tokens: if token in operators: b, a = stack.pop(), stack.pop() if token == '+': stack.append(a + b) elif token == '-': stack.append(a - b) elif token == '*': stack.append(a * b) else: # 注意：Python除法向零取整 stack.append(int(a / b)) else: stack.append(int(token)) return stack[0] 🧠 记忆口诀 “数字入栈，运算符弹两个算” 5️⃣ LC 224. 基本计算器 🔴题目描述实现一个基本的计算器（支持 +、-、括号）。 🎨 图解思路s = \"1 + (2 - 3)\"使用栈保存括号外的状态遇到 ( → 保存当前 result 和 sign，重置遇到 ) → 恢复之前的状态并累加处理: 1 + (2 - 3)1 → result = 1+ → sign = 1( → 保存(1, 1)，重置 result=02 → result = 2- → sign = -13 → result = 2 + (-1)*3 = -1) → result = 1 + 1*(-1) = 0 💻 代码实现def calculate(s: str) - int: stack = [] result = 0 num = 0 sign = 1 for char in s: if char.isdigit(): num = num * 10 + int(char) elif char == '+': result += sign * num num = 0 sign = 1 elif char == '-': result += sign * num num = 0 sign = -1 elif char == '(': # 保存当前状态 stack.append(result) stack.append(sign) result = 0 sign = 1 elif char == ')': result += sign * num num = 0 # 恢复状态 result = result * stack.pop() + stack.pop() return result + sign * num 🧠 记忆口诀 “括号保存状态，出来恢复累加” 6️⃣ LC 227. 基本计算器 II 🟡题目描述实现计算器（支持 +、-、*、/，无括号）。 🎨 图解思路s = \"3+2*2\"* / 优先级高于 + -策略：用栈保存待加的数遇到 + 入栈正数遇到 - 入栈负数遇到 * / 弹出栈顶计算后入栈处理: 3 + 2 * 23 → 栈 [3]+ → 记录 op = +2 → 栈 [3, 2]* → 记录 op = *2 → 弹出2，计算2*2=4，栈 [3, 4]结果: sum([3, 4]) = 7 💻 代码实现def calculate(s: str) - int: stack = [] num = 0 op = '+' for i, char in enumerate(s): if char.isdigit(): num = num * 10 + int(char) if char in '+-*/' or i == len(s) - 1: if op == '+': stack.append(num) elif op == '-': stack.append(-num) elif op == '*': stack.append(stack.pop() * num) elif op == '/': stack.append(int(stack.pop() / num)) op = char num = 0 return sum(stack) 🧠 记忆口诀 “加减入栈，乘除先算” 7️⃣ LC 772. 基本计算器 III 🔴题目描述实现完整计算器（支持 +、-、*、/、括号）。 🎨 图解思路结合 224 和 227 的思路方法1：递归处理括号方法2：双栈（操作数栈 + 运算符栈） 💻 代码实现（递归）def calculate(s: str) - int: def helper(s, start): stack = [] num = 0 op = '+' i = start while i len(s): char = s[i] if char.isdigit(): num = num * 10 + int(char) if char == '(': num, i = helper(s, i + 1) if char in '+-*/)' or i == len(s) - 1: if op == '+': stack.append(num) elif op == '-': stack.append(-num) elif op == '*': stack.append(stack.pop() * num) elif op == '/': stack.append(int(stack.pop() / num)) if char == ')': return sum(stack), i op = char num = 0 i += 1 return sum(stack), i return helper(s, 0)[0] 🧠 记忆口诀 “遇括号递归，其他同227” 📊 本章总结栈的应用模式 模式 核心思想 典型题目 括号匹配 左入右出 20 路径处理 目录入栈，..出栈 71 辅助栈 同步维护额外信息 155 表达式求值 操作数栈 + 运算符处理 150, 224, 227 单调栈模板（补充）def monotonic_stack(nums): \"\"\"找每个元素右边第一个更大的元素\"\"\" stack = [] # 存索引 result = [-1] * len(nums) for i in range(len(nums)): while stack and nums[i] nums[stack[-1]]: idx = stack.pop() result[idx] = nums[i] stack.append(i) return result 🧠 全章记忆口诀括号路径最小栈逆波兰后计算器三道计算器升级栈的七题记心里括号 - 有效的括号 (20)路径 - 简化路径 (71)最小栈 - 最小栈 (155)逆波兰 - 逆波兰表达式求值 (150)计算器 - 基本计算器系列 (224, 227, 772) 📖 下一篇：堆/优先队列专题","tags":["LeetCode"],"categories":["算法"]},{"title":"🔗 LeetCode 150 - 链表专题","path":"/2026/01/18/leetcode-150-linked-list/","content":"🔗 链表专题 (11题) 🎯 核心技巧：虚拟头节点、快慢指针、链表反转、合并链表 🗺️ 链表核心操作图解┌─────────────────────────────────────────────────────────────┐│ 链表基本操作 │├─────────────────────────────────────────────────────────────┤│ ││ 遍历: [1] → [2] → [3] → [4] → None ││ ↑ ││ cur (cur = cur.next) ││ ││ 插入: [1] → [X] → [2] (先接后断) ││ ↗ ↘ ││ ││ 删除: [1] ──────→ [3] (跨过中间节点) ││ ╳[2] ││ ││ 反转: [1] ← [2] ← [3] ← [4] ││ │└─────────────────────────────────────────────────────────────┘ 🔧 链表万能技巧技巧1: 虚拟头节点 (Dummy Head)# 避免处理头节点的特殊情况dummy = ListNode(0)dummy.next = head# ... 操作链表return dummy.next 技巧2: 快慢指针# 找中点、判断环、找环入口slow = fast = headwhile fast and fast.next: slow = slow.next fast = fast.next.next# slow 就是中点 技巧3: 链表反转def reverse(head): prev, curr = None, head while curr: next_temp = curr.next curr.next = prev prev = curr curr = next_temp return prev 1️⃣ LC 141. 环形链表 🟢题目描述判断链表中是否有环。 🎨 图解思路快慢指针：如果有环，快指针一定会追上慢指针 ┌───────────────┐ ▼ │[1] → [2] → [3] → [4] ↑ ↑ slow fast快指针每次走2步，慢指针每次走1步相对速度为1，一定会在环内相遇 💻 代码实现def hasCycle(head: ListNode) - bool: slow = fast = head while fast and fast.next: slow = slow.next fast = fast.next.next if slow == fast: return True return False 🧠 记忆口诀 “快慢追逐，相遇有环” 2️⃣ LC 2. 两数相加 🟡题目描述两个逆序存储的链表相加，返回结果链表。 🎨 图解思路 2 → 4 → 3 (表示 342) + 5 → 6 → 4 (表示 465) ───────────── 7 → 0 → 8 (表示 807)从头到尾逐位相加，注意进位！Step 1: 2 + 5 = 7, 进位0Step 2: 4 + 6 = 10, 写0进1 Step 3: 3 + 4 + 1 = 8, 进位0 💻 代码实现def addTwoNumbers(l1: ListNode, l2: ListNode) - ListNode: dummy = ListNode(0) curr = dummy carry = 0 while l1 or l2 or carry: # 取值（链表可能长度不同） val1 = l1.val if l1 else 0 val2 = l2.val if l2 else 0 # 计算和与进位 total = val1 + val2 + carry carry = total // 10 # 创建新节点 curr.next = ListNode(total % 10) curr = curr.next # 移动指针 l1 = l1.next if l1 else None l2 = l2.next if l2 else None return dummy.next 🧠 记忆口诀 “逐位相加，别忘进位” 3️⃣ LC 21. 合并两个有序链表 🟢题目描述将两个升序链表合并为一个新的升序链表。 🎨 图解思路l1: 1 → 2 → 4l2: 1 → 3 → 4比较头节点，小的接上去：dummy → 1 → 1 → 2 → 3 → 4 → 4 ↑ ↑ ↑ ↑ ↑ ↑ l1 l2 l1 l2 l1 l2 💻 代码实现def mergeTwoLists(l1: ListNode, l2: ListNode) - ListNode: dummy = ListNode(0) curr = dummy while l1 and l2: if l1.val = l2.val: curr.next = l1 l1 = l1.next else: curr.next = l2 l2 = l2.next curr = curr.next # 接上剩余部分 curr.next = l1 if l1 else l2 return dummy.next 🧠 记忆口诀 “比较取小，剩余直接接” 4️⃣ LC 138. 随机链表的复制 🟡题目描述深拷贝带有随机指针的链表。 🎨 图解思路方法：哈希表存储映射关系原链表: [1] → [2] → [3] ↓ ↓ ↓ (random) [3] [1] NoneStep 1: 创建所有新节点，建立 old→new 映射Step 2: 连接 next 和 random 指针 💻 代码实现def copyRandomList(head: 'Node') - 'Node': if not head: return None # 建立映射: 旧节点 → 新节点 old_to_new = {} # 第一遍：创建所有新节点 curr = head while curr: old_to_new[curr] = Node(curr.val) curr = curr.next # 第二遍：连接 next 和 random curr = head while curr: new_node = old_to_new[curr] new_node.next = old_to_new.get(curr.next) new_node.random = old_to_new.get(curr.random) curr = curr.next return old_to_new[head] 🔥 O(1) 空间解法def copyRandomList(head: 'Node') - 'Node': if not head: return None # Step 1: 在每个节点后插入复制节点 # A → A' → B → B' → C → C' curr = head while curr: new_node = Node(curr.val, curr.next) curr.next = new_node curr = new_node.next # Step 2: 设置 random 指针 curr = head while curr: if curr.random: curr.next.random = curr.random.next curr = curr.next.next # Step 3: 拆分链表 dummy = Node(0) new_curr = dummy curr = head while curr: new_curr.next = curr.next new_curr = new_curr.next curr.next = curr.next.next curr = curr.next return dummy.next 🧠 记忆口诀 “哈希映射，两遍搞定” 5️⃣ LC 92. 反转链表 II 🟡题目描述反转链表的第 left 到第 right 个节点。 🎨 图解思路原链表: 1 → 2 → 3 → 4 → 5, left=2, right=4Step 1: 找到 left 前一个节点 1 → [2 → 3 → 4] → 5 ↑ └─反转区间─┘ prevStep 2: 反转中间部分 1 2 ← 3 ← 4 5 ↑ ↓ ↑ prev └─────────┘Step 3: 重新连接 1 → 4 → 3 → 2 → 5 💻 代码实现def reverseBetween(head: ListNode, left: int, right: int) - ListNode: dummy = ListNode(0, head) prev = dummy # 移动到 left 前一个位置 for _ in range(left - 1): prev = prev.next # 反转 [left, right] 区间 curr = prev.next for _ in range(right - left): # 把 curr.next 插到 prev 后面 next_node = curr.next curr.next = next_node.next next_node.next = prev.next prev.next = next_node return dummy.next 🧠 记忆口诀 “头插法反转，一次遍历” 6️⃣ LC 25. K 个一组翻转链表 🔴题目描述每 k 个节点一组进行翻转。 🎨 图解思路链表: 1 → 2 → 3 → 4 → 5, k = 2第1组: [1,2] → 反转 → [2,1]第2组: [3,4] → 反转 → [4,3] 第3组: [5] → 不足k个，保持原样结果: 2 → 1 → 4 → 3 → 5 💻 代码实现def reverseKGroup(head: ListNode, k: int) - ListNode: # 检查是否有 k 个节点 def get_kth(node, k): while node and k 0: node = node.next k -= 1 return node # 反转链表 def reverse(head, tail): prev = tail.next curr = head while prev != tail: next_temp = curr.next curr.next = prev prev = curr curr = next_temp return tail, head # 新的头和尾 dummy = ListNode(0, head) prev_group = dummy while True: # 找到这一组的尾节点 kth = get_kth(prev_group, k) if not kth: break next_group = kth.next # 反转这一组 head, tail = prev_group.next, kth new_head, new_tail = reverse(head, tail) # 连接 prev_group.next = new_head new_tail.next = next_group prev_group = new_tail return dummy.next 🧠 记忆口诀 “够k就翻，不够就留” 7️⃣ LC 19. 删除链表的倒数第 N 个节点 🟡题目描述删除链表的倒数第 n 个节点。 🎨 图解思路快慢指针，快指针先走 n 步链表: 1 → 2 → 3 → 4 → 5, n = 2Step 1: fast 先走 2 步 1 → 2 → 3 → 4 → 5 ↑ ↑ slow fastStep 2: 同时移动直到 fast 到末尾 1 → 2 → 3 → 4 → 5 → None ↑ ↑ slow fastStep 3: 删除 slow.next 1 → 2 → 3 ────→ 5 💻 代码实现def removeNthFromEnd(head: ListNode, n: int) - ListNode: dummy = ListNode(0, head) slow = fast = dummy # fast 先走 n+1 步 for _ in range(n + 1): fast = fast.next # 同时移动 while fast: slow = slow.next fast = fast.next # 删除 slow.next slow.next = slow.next.next return dummy.next 🧠 记忆口诀 “快先走n步，同行到末尾” 8️⃣ LC 82. 删除排序链表中的重复元素 II 🟡题目描述删除所有重复的节点，只保留原始链表中没有重复出现的数字。 🎨 图解思路1 → 2 → 3 → 3 → 4 → 4 → 5检测重复并删除整组：1 → 2 → [3 → 3] → [4 → 4] → 5 删除 删除结果: 1 → 2 → 5 💻 代码实现def deleteDuplicates(head: ListNode) - ListNode: dummy = ListNode(0, head) prev = dummy while prev.next: curr = prev.next # 检测是否有重复 if curr.next and curr.val == curr.next.val: # 跳过所有重复节点 while curr.next and curr.val == curr.next.val: curr = curr.next prev.next = curr.next # 删除整组 else: prev = prev.next return dummy.next 🧠 记忆口诀 “见重复全删，不重复才留” 9️⃣ LC 61. 旋转链表 🟡题目描述将链表每个节点向右移动 k 个位置。 🎨 图解思路1 → 2 → 3 → 4 → 5, k = 2Step 1: 连成环 1 → 2 → 3 → 4 → 5 ↑ │ └───────────────┘Step 2: 找到新的断点 (n - k % n) 新头是第 5-2=3 个节点之后 Step 3: 在正确位置断开 4 → 5 → 1 → 2 → 3 💻 代码实现def rotateRight(head: ListNode, k: int) - ListNode: if not head or not head.next or k == 0: return head # 计算长度并找到尾节点 length = 1 tail = head while tail.next: tail = tail.next length += 1 # 实际需要移动的步数 k = k % length if k == 0: return head # 找到新的尾节点（第 length - k 个） new_tail = head for _ in range(length - k - 1): new_tail = new_tail.next # 重新连接 new_head = new_tail.next new_tail.next = None tail.next = head return new_head 🧠 记忆口诀 “先成环，再断开” 🔟 LC 86. 分隔链表 🟡题目描述将链表按值 x 分成两部分：小于 x 的在前，大于等于 x 的在后。 🎨 图解思路1 → 4 → 3 → 2 → 5 → 2, x = 3分成两个链表：小于3: 1 → 2 → 2≥3: 4 → 3 → 5合并: 1 → 2 → 2 → 4 → 3 → 5 💻 代码实现def partition(head: ListNode, x: int) - ListNode: # 两个虚拟头节点 small_dummy = ListNode(0) large_dummy = ListNode(0) small = small_dummy large = large_dummy while head: if head.val x: small.next = head small = small.next else: large.next = head large = large.next head = head.next # 连接两个链表 large.next = None # 防止成环 small.next = large_dummy.next return small_dummy.next 🧠 记忆口诀 “分两队，再合并” 1️⃣1️⃣ LC 146. LRU 缓存 🟡题目描述实现 LRU (最近最少使用) 缓存机制。 🎨 图解思路使用双向链表 + 哈希表双向链表（按使用时间排序）:head ⇄ [最近用] ⇄ [次近用] ⇄ ... ⇄ [最久未用] ⇄ tail哈希表: key → 链表节点get/put 操作:1. 通过哈希表 O(1) 找到节点2. 移动到链表头部（最近使用）3. 超容量时删除链表尾部 💻 代码实现class DLinkedNode: def __init__(self, key=0, value=0): self.key = key self.value = value self.prev = None self.next = Noneclass LRUCache: def __init__(self, capacity: int): self.cache = {} # key → node self.capacity = capacity # 虚拟头尾节点 self.head = DLinkedNode() self.tail = DLinkedNode() self.head.next = self.tail self.tail.prev = self.head def _remove(self, node): \"\"\"从链表中删除节点\"\"\" node.prev.next = node.next node.next.prev = node.prev def _add_to_head(self, node): \"\"\"添加到链表头部\"\"\" node.prev = self.head node.next = self.head.next self.head.next.prev = node self.head.next = node def _move_to_head(self, node): \"\"\"移动到头部\"\"\" self._remove(node) self._add_to_head(node) def _remove_tail(self): \"\"\"删除尾部节点\"\"\" node = self.tail.prev self._remove(node) return node def get(self, key: int) - int: if key not in self.cache: return -1 node = self.cache[key] self._move_to_head(node) return node.value def put(self, key: int, value: int) - None: if key in self.cache: node = self.cache[key] node.value = value self._move_to_head(node) else: node = DLinkedNode(key, value) self.cache[key] = node self._add_to_head(node) if len(self.cache) self.capacity: tail = self._remove_tail() del self.cache[tail.key] 🧠 记忆口诀 “双链表记顺序，哈希表快查找” 📊 本章总结链表技巧速查表 技巧 使用场景 典型题目 虚拟头节点 可能修改头节点 21, 82, 86 快慢指针 找中点/判环 141, 19 反转链表 翻转操作 92, 25 哈希表辅助 复杂指针关系 138, 146 双指针 合并/分隔 21, 86 🧠 全章记忆口诀环加合复反转组删倒删重旋分缓环 - 环形链表 (141)加 - 两数相加 (2)合 - 合并有序链表 (21)复 - 复制随机链表 (138)反 - 反转链表 II (92)组 - K个一组翻转 (25)删倒 - 删除倒数第N个 (19)删重 - 删除重复元素 II (82)旋 - 旋转链表 (61)分 - 分隔链表 (86)缓 - LRU缓存 (146) 📖 下一篇：二叉树专题","tags":["LeetCode"],"categories":["算法"]},{"title":"👆 LeetCode 150 - 双指针专题","path":"/2026/01/18/leetcode-150-two-pointers/","content":"👆 双指针专题 (5题) 🎯 核心技巧：相向双指针、同向双指针、快慢指针 🗺️ 双指针三大模式┌─────────────────────────────────────────────────────────────┐│ 双指针模式 │├───────────────────┬───────────────────┬─────────────────────┤│ 相向双指针 │ 同向双指针 │ 背向双指针 ││ (对撞指针) │ (快慢指针) │ (中心扩展) │├───────────────────┼───────────────────┼─────────────────────┤│ L ──────▶ ◀── R │ S ──▶ F ──▶ │ ◀── C ──▶ │├───────────────────┼───────────────────┼─────────────────────┤│ • 两数之和 │ • 移除元素 │ • 最长回文子串 ││ • 盛水容器 │ • 删除重复 │ • 回文判断 ││ • 三数之和 │ • 链表快慢 │ │└───────────────────┴───────────────────┴─────────────────────┘ 1️⃣ LC 125. 验证回文串 🟢题目描述判断字符串是否为回文串（只考虑字母和数字，忽略大小写）。 🎨 图解思路相向双指针，跳过非字母数字字符s = \"A man, a plan, a canal: Panama\" ↑ ↑ L RStep 1: 'A' vs 'a' → 相等，L++, R--Step 2: ' ' 跳过，L++Step 3: 'm' vs 'm' → 相等...最终 L = R，是回文串 ✓ 💻 代码实现def isPalindrome(s: str) - bool: left, right = 0, len(s) - 1 while left right: # 跳过非字母数字 while left right and not s[left].isalnum(): left += 1 while left right and not s[right].isalnum(): right -= 1 # 比较（忽略大小写） if s[left].lower() != s[right].lower(): return False left += 1 right -= 1 return True 🧠 记忆口诀 “两头往中走，不同就说No” 2️⃣ LC 392. 判断子序列 🟢题目描述判断 s 是否为 t 的子序列。 🎨 图解思路同向双指针：i 遍历 s，j 遍历 ts = \"ace\", t = \"abcde\" ↑ ↑ i jj=0: t[0]='a' = s[0], i++, j++j=1: t[1]='b' ≠ s[1]='c', j++j=2: t[2]='c' = s[1], i++, j++j=3: t[3]='d' ≠ s[2]='e', j++j=4: t[4]='e' = s[2], i++, j++i = 3 = len(s) ✓ 是子序列 💻 代码实现def isSubsequence(s: str, t: str) - bool: i, j = 0, 0 while i len(s) and j len(t): if s[i] == t[j]: i += 1 j += 1 return i == len(s) 🧠 记忆口诀 “s 指针只在匹配时前进” 3️⃣ LC 167. 两数之和 II - 输入有序数组 🟡题目描述在有序数组中找两个数，使它们的和等于目标值。 🎨 图解思路有序数组 + 两数之和 = 相向双指针！numbers = [2, 7, 11, 15], target = 9 ↑ ↑ L Rsum = 2 + 15 = 17 9 → R-- (和太大，减小右边)sum = 2 + 11 = 13 9 → R--sum = 2 + 7 = 9 = target ✓返回 [1, 2] (1-indexed) 💻 代码实现def twoSum(numbers: list, target: int) - list: left, right = 0, len(numbers) - 1 while left right: total = numbers[left] + numbers[right] if total == target: return [left + 1, right + 1] # 1-indexed elif total target: left += 1 # 和太小，增大左边 else: right -= 1 # 和太大，减小右边 return [] 🧠 记忆口诀 “小了左移，大了右移” 4️⃣ LC 11. 盛最多水的容器 🟡题目描述找两条线，使构成的容器盛水最多。 🎨 图解思路面积 = min(左高, 右高) × 宽度 │ │ │ │ │ │ │ │ │ │ │ │ │ │ │ [1, 8, 6, 2, 5, 4, 8, 3, 7] ↑ ↑ L R关键洞察：移动较矮的那边！为什么？因为宽度必然减小，只有高度增加才可能面积增大而高度由矮的决定，所以移动矮的才有可能找到更高的Step 1: height[L]=1 height[R]=7 area = 1 × 8 = 8 移动 L (因为左边矮)Step 2: height[L]=8 height[R]=7 area = 7 × 7 = 49 移动 R... 最大面积 = 49 💻 代码实现def maxArea(height: list) - int: left, right = 0, len(height) - 1 max_area = 0 while left right: # 计算当前面积 width = right - left h = min(height[left], height[right]) max_area = max(max_area, width * h) # 移动较矮的一边 if height[left] height[right]: left += 1 else: right -= 1 return max_area 🧠 记忆口诀 “矮的先走，高的等着” 5️⃣ LC 15. 三数之和 🟡题目描述找出所有和为 0 的三元组（不能重复）。 🎨 图解思路排序 + 固定一个数 + 双指针找另外两个nums = [-4, -1, -1, 0, 1, 2] ↑ i (固定) ↑ ↑ L Rtarget = -nums[i] = 4找 L + R = 4: -1 + 2 = 1 4 → L++ -1 + 2 = 1 4 → L++ 0 + 2 = 2 4 → L++ 1 + 2 = 3 4 → L++ L = R，结束i++ 继续...去重技巧：1. nums[i] == nums[i-1] 时跳过2. 找到解后，L++/R-- 跳过重复值 💻 代码实现def threeSum(nums: list) - list: nums.sort() result = [] n = len(nums) for i in range(n - 2): # 去重：跳过重复的第一个数 if i 0 and nums[i] == nums[i - 1]: continue # 双指针找另外两个数 left, right = i + 1, n - 1 target = -nums[i] while left right: total = nums[left] + nums[right] if total == target: result.append([nums[i], nums[left], nums[right]]) # 去重：跳过重复的第二、三个数 while left right and nums[left] == nums[left + 1]: left += 1 while left right and nums[right] == nums[right - 1]: right -= 1 left += 1 right -= 1 elif total target: left += 1 else: right -= 1 return result 🧠 记忆口诀 “排序固定一，双指针找二三，遇重复就跳过” 📊 本章总结双指针模板速查# 模板1: 相向双指针def two_pointer_opposite(arr): left, right = 0, len(arr) - 1 while left right: if condition: left += 1 else: right -= 1# 模板2: 同向双指针def two_pointer_same(arr): slow = 0 for fast in range(len(arr)): if condition: arr[slow] = arr[fast] slow += 1 return slow# 模板3: 背向双指针（中心扩展）def expand_from_center(s, left, right): while left = 0 and right len(s) and s[left] == s[right]: left -= 1 right += 1 return s[left+1:right] 题目模式识别 特征 使用模式 典型题目 有序数组求和 相向双指针 167, 15 回文判断 相向双指针 125 最大/最小容器 相向双指针 11 子序列匹配 同向双指针 392 原地删除元素 同向双指针 26, 27 回文子串 背向双指针 5, 647 🧠 全章记忆口诀验判两盛三，双指针五关验 - 验证回文串 (125)判 - 判断子序列 (392)两 - 两数之和 II (167)盛 - 盛最多水的容器 (11)三 - 三数之和 (15) 📖 下一篇：滑动窗口专题","tags":["LeetCode"],"categories":["算法"]},{"title":"动态规划完全指南：从入门到精通","path":"/2025/01/20/20250220-dynamic-programming-complete-guide/","content":"动态规划完全指南：从入门到精通 用费曼学习法，彻底搞懂动态规划的本质 一、什么是动态规划？1.1 一句话定义动态规划 (Dynamic Programming, DP) = 记住已经解决过的子问题的答案，避免重复计算。 就这么简单。 1.2 为什么叫”动态规划”？这个名字其实很有误导性。Richard Bellman 在 1950 年代发明这个方法时，故意起了个听起来很高大上的名字，因为他的老板不喜欢”数学研究”。”Dynamic” 听起来很酷，”Programming” 在当时指的是”规划/优化”，不是写代码。 所以别被名字吓到，它的本质就是：用空间换时间，记住中间结果。 1.3 核心直觉：斐波那契数列让我们从最经典的例子开始： fib(0) = 0fib(1) = 1fib(n) = fib(n-1) + fib(n-2) 暴力递归写法： def fib(n): if n = 1: return n return fib(n-1) + fib(n-2) 看起来很简洁对吧？但是有个致命问题： 看到了吗？fib(3) 被计算了 2 次，fib(2) 被计算了 3 次！当 n=50 时，这个递归树会有 2^50 个节点，你的电脑会直接卡死。 时间复杂度：O(2^n) —— 指数级爆炸！ 1.4 DP 的解决方案方案一：记忆化递归 (Top-Down) def fib(n, memo={}): if n in memo: return memo[n] if n = 1: return n memo[n] = fib(n-1, memo) + fib(n-2, memo) return memo[n] 方案二：迭代填表 (Bottom-Up) def fib(n): if n = 1: return n dp = [0] * (n + 1) dp[0], dp[1] = 0, 1 for i in range(2, n + 1): dp[i] = dp[i-1] + dp[i-2] return dp[n] 方案三：空间优化 def fib(n): if n = 1: return n prev, curr = 0, 1 for _ in range(2, n + 1): prev, curr = curr, prev + curr return curr 方案 时间复杂度 空间复杂度 暴力递归 O(2^n) O(n) 栈空间 记忆化递归 O(n) O(n) 迭代填表 O(n) O(n) 空间优化 O(n) O(1) 这就是 DP 的威力：从指数级降到线性级！ 二、DP 的三大核心要素 2.1 状态定义 (State Definition)这是最关键的一步！ 状态定义错了，后面全白搭。 问自己：dp[i] 或 dp[i][j] 代表什么？ 常见的状态定义模式： 问题类型 状态定义 例子 线性序列 dp[i] = 以第i个元素结尾的xxx 最长递增子序列 线性序列 dp[i] = 前i个元素的xxx 打家劫舍 双序列 dp[i][j] = s1前i个和s2前j个的xxx LCS, 编辑距离 背包 dp[i][w] = 前i个物品、容量w的xxx 0-1背包 区间 dp[i][j] = 区间[i,j]的xxx 戳气球 2.2 状态转移方程 (Transition)问自己：当前状态和哪些之前的状态有关？ 这是 DP 的”递推公式”，决定了如何从小问题推导出大问题。 dp[i] = f(dp[i-1], dp[i-2], ..., dp[0]) 技巧：画图！ 把状态之间的依赖关系画出来，转移方程自然就出来了。 2.3 边界条件 (Base Case)问自己：最小的子问题，答案是什么？ 边界条件就是递推的起点。没有正确的边界条件，整个 DP 就会崩溃。 常见边界： dp[0] = 0 或 dp[0] = 1 dp[0][j] = xxx, dp[i][0] = xxx 空集、空串的情况 三、两种实现方式对比 3.1 Top-Down：记忆化递归def solve(n, memo={}): # 1. 检查缓存 if n in memo: return memo[n] # 2. Base case if n = 1: return base_value # 3. 递归 + 记忆化 memo[n] = f(solve(n-1), solve(n-2), ...) return memo[n] 优点： 思路自然，从目标出发 只计算需要的子问题 代码更接近数学定义 缺点： 递归栈开销 Python 默认递归深度限制 (1000) 3.2 Bottom-Up：迭代填表def solve(n): # 1. 初始化 DP 数组 dp = [0] * (n + 1) # 2. Base case dp[0] = base_value # 3. 按顺序填表 for i in range(1, n + 1): dp[i] = f(dp[i-1], dp[i-2], ...) # 4. 返回结果 return dp[n] 优点： 无递归栈开销 更容易做空间优化 通常更快（无函数调用开销） 缺点： 需要想清楚遍历顺序 可能计算不需要的子问题 3.3 如何选择？ 场景 推荐方式 面试时快速写出 Top-Down（更直观） 追求性能 Bottom-Up 需要空间优化 Bottom-Up 状态转移复杂 Top-Down（更容易调试） 四、空间优化技巧 4.1 滚动数组当 dp[i] 只依赖 dp[i-1] 时，不需要保存整个数组： # 优化前：O(n) 空间dp = [0] * nfor i in range(1, n): dp[i] = dp[i-1] + something# 优化后：O(1) 空间prev = 0for i in range(1, n): curr = prev + something prev = curr 4.2 二维降一维当 dp[i][j] 只依赖 dp[i-1][...] 时： # 优化前：O(m*n) 空间dp = [[0] * n for _ in range(m)]for i in range(1, m): for j in range(1, n): dp[i][j] = dp[i-1][j] + dp[i][j-1]# 优化后：O(n) 空间dp = [0] * nfor i in range(1, m): for j in range(1, n): dp[j] = dp[j] + dp[j-1] # dp[j] 就是原来的 dp[i-1][j] 注意遍历顺序！ 如果依赖左上角，需要从右往左遍历。 五、经典问题详解5.1 爬楼梯 (LeetCode 70) 问题： 每次可以爬 1 或 2 个台阶，爬到第 n 阶有多少种方法？ 状态定义： dp[i] = 爬到第 i 阶的方法数 转移方程： dp[i] = dp[i-1] + dp[i-2] 从第 i-1 阶爬 1 步上来 从第 i-2 阶爬 2 步上来 边界条件： dp[0] = 1, dp[1] = 1 def climbStairs(n: int) - int: if n = 2: return n prev, curr = 1, 2 for _ in range(3, n + 1): prev, curr = curr, prev + curr return curr 复杂度： 时间 O(n)，空间 O(1) 5.2 打家劫舍 (LeetCode 198)问题： 不能偷相邻的房子，求能偷到的最大金额。 状态定义： dp[i] = 考虑前 i 个房子能偷到的最大金额 转移方程： dp[i] = max(dp[i-1], dp[i-2] + nums[i]) 不偷第 i 个：dp[i-1] 偷第 i 个：dp[i-2] + nums[i]（不能偷 i-1） def rob(nums: list[int]) - int: if len(nums) = 2: return max(nums) if nums else 0 prev2, prev1 = nums[0], max(nums[0], nums[1]) for i in range(2, len(nums)): prev2, prev1 = prev1, max(prev1, prev2 + nums[i]) return prev1 复杂度： 时间 O(n)，空间 O(1) 5.3 最大子数组和 (LeetCode 53) 问题： 找出和最大的连续子数组。 状态定义： dp[i] = 以 nums[i] 结尾的最大子数组和 转移方程： dp[i] = max(nums[i], dp[i-1] + nums[i]) 要么从 nums[i] 重新开始 要么接上前面的子数组 def maxSubArray(nums: list[int]) - int: max_sum = curr_sum = nums[0] for num in nums[1:]: curr_sum = max(num, curr_sum + num) max_sum = max(max_sum, curr_sum) return max_sum 复杂度： 时间 O(n)，空间 O(1) 5.4 最长递增子序列 LIS (LeetCode 300)问题： 找出最长严格递增子序列的长度。 状态定义： dp[i] = 以 nums[i] 结尾的 LIS 长度 转移方程： dp[i] = max(dp[j] + 1) for all j i where nums[j] nums[i] def lengthOfLIS(nums: list[int]) - int: n = len(nums) dp = [1] * n for i in range(1, n): for j in range(i): if nums[j] nums[i]: dp[i] = max(dp[i], dp[j] + 1) return max(dp) 复杂度： 时间 O(n²)，空间 O(n) 优化版本（二分查找）： O(n log n) import bisectdef lengthOfLIS(nums: list[int]) - int: tails = [] for num in nums: pos = bisect.bisect_left(tails, num) if pos == len(tails): tails.append(num) else: tails[pos] = num return len(tails) 5.5 最长公共子序列 LCS (LeetCode 1143) 问题： 两个字符串的最长公共子序列长度。 状态定义： dp[i][j] = s1 前 i 个字符和 s2 前 j 个字符的 LCS 长度 转移方程： if s1[i-1] == s2[j-1]: dp[i][j] = dp[i-1][j-1] + 1else: dp[i][j] = max(dp[i-1][j], dp[i][j-1]) def longestCommonSubsequence(text1: str, text2: str) - int: m, n = len(text1), len(text2) dp = [[0] * (n + 1) for _ in range(m + 1)] for i in range(1, m + 1): for j in range(1, n + 1): if text1[i-1] == text2[j-1]: dp[i][j] = dp[i-1][j-1] + 1 else: dp[i][j] = max(dp[i-1][j], dp[i][j-1]) return dp[m][n] 复杂度： 时间 O(mn)，空间 O(mn)，可优化到 O(n) 5.6 编辑距离 (LeetCode 72) 问题： 将 word1 转换成 word2 所需的最少操作数（插入、删除、替换）。 状态定义： dp[i][j] = word1 前 i 个字符转换成 word2 前 j 个字符的最少操作数 转移方程： if word1[i-1] == word2[j-1]: dp[i][j] = dp[i-1][j-1] # 不需要操作else: dp[i][j] = 1 + min( dp[i-1][j-1], # 替换 dp[i-1][j], # 删除 dp[i][j-1] # 插入 ) def minDistance(word1: str, word2: str) - int: m, n = len(word1), len(word2) dp = [[0] * (n + 1) for _ in range(m + 1)] # 边界条件 for i in range(m + 1): dp[i][0] = i for j in range(n + 1): dp[0][j] = j for i in range(1, m + 1): for j in range(1, n + 1): if word1[i-1] == word2[j-1]: dp[i][j] = dp[i-1][j-1] else: dp[i][j] = 1 + min(dp[i-1][j-1], dp[i-1][j], dp[i][j-1]) return dp[m][n] 复杂度： 时间 O(mn)，空间 O(mn) 5.7 0-1 背包问题问题： n 个物品，每个有重量 w[i] 和价值 v[i]，背包容量 W，求最大价值。 状态定义： dp[i][w] = 前 i 个物品、容量 w 时的最大价值 转移方程： dp[i][w] = max( dp[i-1][w], # 不选第 i 个 dp[i-1][w-w[i]] + v[i] # 选第 i 个（如果装得下）) def knapsack(weights, values, W): n = len(weights) dp = [[0] * (W + 1) for _ in range(n + 1)] for i in range(1, n + 1): for w in range(W + 1): dp[i][w] = dp[i-1][w] # 不选 if w = weights[i-1]: dp[i][w] = max(dp[i][w], dp[i-1][w-weights[i-1]] + values[i-1]) return dp[n][W] 空间优化版本： def knapsack(weights, values, W): dp = [0] * (W + 1) for i in range(len(weights)): for w in range(W, weights[i] - 1, -1): # 从右往左！ dp[w] = max(dp[w], dp[w - weights[i]] + values[i]) return dp[W] 为什么要从右往左？ 因为 dp[w] 依赖 dp[w-weights[i]]，如果从左往右，dp[w-weights[i]] 已经被更新过了，相当于同一个物品被选了多次（变成完全背包了）。 复杂度： 时间 O(nW)，空间 O(W) 5.8 完全背包问题问题： 每个物品可以选无限次。 和 0-1 背包的区别： 遍历顺序从左往右！ def unboundedKnapsack(weights, values, W): dp = [0] * (W + 1) for i in range(len(weights)): for w in range(weights[i], W + 1): # 从左往右！ dp[w] = max(dp[w], dp[w - weights[i]] + values[i]) return dp[W] 5.9 零钱兑换 (LeetCode 322)问题： 用最少的硬币凑出金额 amount。 状态定义： dp[i] = 凑出金额 i 需要的最少硬币数 转移方程： dp[i] = min(dp[i - coin] + 1) for all coins def coinChange(coins: list[int], amount: int) - int: dp = [float('inf')] * (amount + 1) dp[0] = 0 for i in range(1, amount + 1): for coin in coins: if coin = i and dp[i - coin] != float('inf'): dp[i] = min(dp[i], dp[i - coin] + 1) return dp[amount] if dp[amount] != float('inf') else -1 复杂度： 时间 O(amount × n)，空间 O(amount) 5.10 不同路径 (LeetCode 62)问题： m×n 网格，从左上到右下，只能向右或向下，有多少条路径？ 状态定义： dp[i][j] = 到达 (i,j) 的路径数 转移方程： dp[i][j] = dp[i-1][j] + dp[i][j-1] def uniquePaths(m: int, n: int) - int: dp = [1] * n for _ in range(1, m): for j in range(1, n): dp[j] += dp[j-1] return dp[n-1] 复杂度： 时间 O(mn)，空间 O(n) 六、DP 问题分类 6.1 线性 DP特点：状态沿着一个维度线性递推 题目 状态定义 转移方程 爬楼梯 dp[i]=到第i阶方法数 dp[i]=dp[i-1]+dp[i-2] 打家劫舍 dp[i]=前i家最大金额 dp[i]=max(dp[i-1], dp[i-2]+nums[i]) 最大子数组和 dp[i]=以i结尾的最大和 dp[i]=max(nums[i], dp[i-1]+nums[i]) LIS dp[i]=以i结尾的LIS长度 dp[i]=max(dp[j]+1) for ji 6.2 序列 DP（双序列）特点：两个序列之间的关系 题目 状态定义 关键点 LCS dp[i][j]=s1前i和s2前j的LCS 字符相等时+1 编辑距离 dp[i][j]=最少操作数 三种操作取min 不同子序列 dp[i][j]=s中t出现次数 选或不选当前字符 6.3 背包 DP特点：选择物品，满足约束，优化目标 类型 特点 遍历顺序 0-1背包 每个物品最多选1次 容量从大到小 完全背包 每个物品可选无限次 容量从小到大 多重背包 每个物品有数量限制 二进制优化 6.4 区间 DP特点：在区间上进行决策 # 区间DP模板for length in range(2, n + 1): # 枚举区间长度 for i in range(n - length + 1): # 枚举起点 j = i + length - 1 # 计算终点 for k in range(i, j): # 枚举分割点 dp[i][j] = min(dp[i][j], dp[i][k] + dp[k+1][j] + cost) 经典题目：戳气球、石子合并、矩阵链乘法 6.5 树形 DP特点：在树结构上进行 DP def dfs(node): if not node: return 0 left = dfs(node.left) right = dfs(node.right) # 根据子树结果计算当前节点 return f(left, right, node.val) 经典题目：打家劫舍III、二叉树最大路径和 6.6 状态压缩 DP特点：用二进制表示状态集合 # 枚举所有子集for mask in range(1 n): for i in range(n): if mask (1 i): # 第i位是否为1 # 处理 经典题目：旅行商问题 TSP 七、时间空间复杂度分析7.1 时间复杂度公式：状态数量 × 每个状态的转移代价 问题 状态数 转移代价 总复杂度 斐波那契 O(n) O(1) O(n) LIS (朴素) O(n) O(n) O(n²) LCS O(mn) O(1) O(mn) 0-1背包 O(nW) O(1) O(nW) 区间DP O(n²) O(n) O(n³) 7.2 空间复杂度基本空间 = DP 数组大小 优化技巧： 滚动数组：O(n) → O(1) 降维：O(mn) → O(n) 只保留必要状态 八、DP 解题模板 8.1 五步法def dp_template(input): # Step 1: 定义状态 # dp[i] = ??? # Step 2: 初始化（边界条件） dp = [0] * n dp[0] = base_case # Step 3: 确定遍历顺序 for i in range(1, n): # Step 4: 状态转移 dp[i] = f(dp[i-1], ...) # Step 5: 返回结果 return dp[n-1] 8.2 Debug 技巧 打印 DP 数组：看中间状态是否正确 手算小例子：n=3,4 时手动验证 检查边界：i=0, j=0 的情况 检查遍历顺序：依赖的状态是否已计算 九、LeetCode 经典题目练习9.1 入门级（Easy） 题号 题目 核心思路 70 爬楼梯 dp[i]=dp[i-1]+dp[i-2] 121 买卖股票最佳时机 记录最小价格，更新最大利润 53 最大子数组和 dp[i]=max(nums[i], dp[i-1]+nums[i]) 746 使用最小花费爬楼梯 dp[i]=min(dp[i-1]+cost[i-1], dp[i-2]+cost[i-2]) 9.2 中等级（Medium） 题号 题目 核心思路 198 打家劫舍 dp[i]=max(dp[i-1], dp[i-2]+nums[i]) 300 最长递增子序列 dp[i]=max(dp[j]+1) 322 零钱兑换 完全背包变形 62 不同路径 dp[i][j]=dp[i-1][j]+dp[i][j-1] 64 最小路径和 同上，取min 139 单词拆分 dp[i]=any(dp[j] and s[j:i] in dict) 152 乘积最大子数组 同时维护最大和最小 1143 最长公共子序列 双序列DP经典 9.3 困难级（Hard） 题号 题目 核心思路 72 编辑距离 三种操作取min 312 戳气球 区间DP 10 正则表达式匹配 双序列DP + 特殊字符处理 32 最长有效括号 dp[i]=以i结尾的最长有效长度 115 不同的子序列 dp[i][j]=s前i个中t前j个出现次数 十、费曼总结：用一句话解释 DP10.1 给小学生解释“做作业时，把算过的题目答案记在草稿纸上，下次遇到一样的题直接抄答案，不用重新算。” 10.2 给程序员解释“用哈希表缓存递归的中间结果，或者用数组从小到大迭代填表。” 10.3 给面试官解释“DP 是一种通过将问题分解为重叠子问题，并存储子问题的解来避免重复计算的优化技术。它适用于具有最优子结构和重叠子问题性质的问题。” 10.4 核心公式DP = 递归 + 记忆化 = 分治 + 缓存 10.5 判断是否能用 DP问自己两个问题： 最优子结构：大问题的最优解能否由小问题的最优解推导出来？ 重叠子问题：在求解过程中，是否会重复计算相同的子问题？ 如果两个都是 Yes，就可以用 DP！ 十一、常见错误与陷阱11.1 状态定义不清错误： 模糊地定义 dp[i] 是”第 i 个的答案” 正确： 明确是”以第 i 个结尾”还是”前 i 个” 11.2 边界条件遗漏# 错误：忘记处理空数组def maxSubArray(nums): dp = [0] * len(nums) # 如果 nums 为空会报错 ...# 正确：先检查边界def maxSubArray(nums): if not nums: return 0 ... 11.3 遍历顺序错误# 0-1背包：必须从右往左for w in range(W, weight - 1, -1): # 正确for w in range(weight, W + 1): # 错误！变成完全背包了 11.4 返回值搞错# LIS：返回 max(dp)，不是 dp[n-1]return max(dp) # 正确return dp[n-1] # 错误！ 十二、进阶技巧12.1 状态压缩当状态是一个集合时，用二进制数表示： # 表示选了哪些元素mask = 0b1011 # 选了第0、1、3个元素# 检查第i位if mask (1 i): ...# 设置第i位mask |= (1 i)# 清除第i位mask = ~(1 i) 12.2 单调队列优化当转移方程形如 dp[i] = max(dp[j]) + cost 且 j 在滑动窗口内时： from collections import dequedef solve(nums, k): q = deque() # 存储下标，保持单调递减 dp = [0] * n for i in range(n): while q and q[0] i - k: q.popleft() dp[i] = nums[q[0]] + cost if q else cost while q and nums[q[-1]] = nums[i]: q.pop() q.append(i) 12.3 斜率优化当转移方程形如 dp[i] = min(dp[j] + f(i,j)) 且 f 可以分离变量时，用凸包优化到 O(n)。 十三、实战代码模板汇总13.1 线性 DP 模板def linear_dp(nums): n = len(nums) if n == 0: return 0 dp = [0] * n dp[0] = nums[0] # base case for i in range(1, n): dp[i] = f(dp[i-1], nums[i]) return dp[n-1] # 或 max(dp) 13.2 双序列 DP 模板def two_seq_dp(s1, s2): m, n = len(s1), len(s2) dp = [[0] * (n+1) for _ in range(m+1)] # 边界初始化 for i in range(m+1): dp[i][0] = init_val for j in range(n+1): dp[0][j] = init_val for i in range(1, m+1): for j in range(1, n+1): if s1[i-1] == s2[j-1]: dp[i][j] = dp[i-1][j-1] + 1 else: dp[i][j] = max(dp[i-1][j], dp[i][j-1]) return dp[m][n] 13.3 背包 DP 模板# 0-1 背包def knapsack_01(weights, values, W): dp = [0] * (W + 1) for i in range(len(weights)): for w in range(W, weights[i]-1, -1): # 从右往左 dp[w] = max(dp[w], dp[w-weights[i]] + values[i]) return dp[W]# 完全背包def knapsack_complete(weights, values, W): dp = [0] * (W + 1) for i in range(len(weights)): for w in range(weights[i], W+1): # 从左往右 dp[w] = max(dp[w], dp[w-weights[i]] + values[i]) return dp[W] 13.4 区间 DP 模板def interval_dp(nums): n = len(nums) dp = [[0] * n for _ in range(n)] # 长度从小到大 for length in range(2, n+1): for i in range(n - length + 1): j = i + length - 1 dp[i][j] = float('inf') for k in range(i, j): dp[i][j] = min(dp[i][j], dp[i][k] + dp[k+1][j] + cost(i,j)) return dp[0][n-1] 十四、面试高频问题速查14.1 股票买卖系列 题号 限制 状态定义 121 只能买卖1次 记录最小价格 122 无限次 贪心：所有上涨都吃 123 最多2次 dp[i][k][0/1] 188 最多k次 同上 309 有冷冻期 增加冷冻状态 714 有手续费 卖出时减fee 14.2 子序列系列 题目 关键区别 子序列 可以不连续 子数组 必须连续 子串 必须连续（字符串版） 14.3 路径系列 题号 变形 62 基础版 63 有障碍物 64 带权重（最小路径和） 120 三角形 931 下降路径最小和 十五、总结思维导图动态规划├── 核心思想│ ├── 最优子结构│ ├── 重叠子问题│ └── 空间换时间├── 三要素│ ├── 状态定义 dp[i] = ?│ ├── 转移方程 dp[i] = f(dp[...])│ └── 边界条件 dp[0] = ?├── 实现方式│ ├── Top-Down 记忆化递归│ └── Bottom-Up 迭代填表├── 优化技巧│ ├── 滚动数组│ ├── 降维│ └── 单调队列/斜率优化└── 问题分类 ├── 线性DP ├── 序列DP ├── 背包DP ├── 区间DP ├── 树形DP └── 状压DP 参考资料 《算法导论》第15章 - 动态规划 LeetCode 动态规划专题 OI Wiki - 动态规划 💡 费曼学习法核心：如果你能把 DP 解释给一个完全不懂编程的人听，你就真正理解了它。 记住：DP 不是背模板，而是理解”如何把大问题拆成小问题，并记住小问题的答案”。","tags":["动态规划","DP","LeetCode","算法面试"],"categories":["算法"]},{"title":"mmdetection","path":"/2023/03/17/mmdetection/","content":"","tags":["目标检测"],"categories":["技术"]},{"title":"一种基于单应性的多相机行人跟踪算法","path":"/2023/01/26/一种基于单应性的多相机行人跟踪算法/","content":"一种基于单应性的多相机行人跟踪算法1. 摘要在一个区域周围安装多个廉价的视频监控摄像头是很容易的。然而，多摄像头跟踪仍然是一个发展中的领域。可以使用多个摄像机生产的监控产品包括摄像机提示、广域交通分析、存在遮挡的跟踪以及现场入口跟踪。 所有这些产品都需要解决一致的标签问题。这意味着给各种摄像机中真实世界目标的所有投影提供相同的元目标跟踪标签。 本文介绍了一种多摄像头人群跟踪算法的实现与测试。首先，部分重新实现了形状匹配的单摄像机跟踪算法，以便在测试视频中工作。单摄像头跟踪器的输出是多摄像头跟踪器的输入。该算法找到每个目标的脚特征:对应于目标正下方地面平面上的一个点的像素。视野线被发现并用于创建初始元目标关联。然后，元目标在移动时丢弃一系列标记，并根据这些标记计算单应性。然后，基于同源关系的跟踪器细化元目标列表，并根据需要创建新的元目标。 测试表明，该算法解决了一致性标记问题，并且在学习过程中只需要很少的边缘事件。基于同源性的匹配器被证明完全克服部分和完全的目标遮挡在一对相机之一。 2 介绍视频监控是一项艰巨的任务。基于计算机视觉领域，它本身只有几十年的历史，视频馈电的自动处理通常需要专门的编码和解码硬件，快速的数字信号处理器，以及大量的存储介质。 处理多个视频流的需求变得越来越重要。摄像机的价格持续下降，像样的“网络摄像机”售价不到20美元。安装同样便宜且简单。此外，社会因素也有助于监控摄像头的普及。伦敦和波士顿等城市的警察部门，以及购物中心和机场等私人企业，都在利用最近的恐怖主义，为增加视频监控提供理由。在大多数大城市，现在很容易发现摄像机。一些装置甚至自夸使用对近红外或热红外波长敏感的摄像机的微光能力。 尽管多摄像头监控装置越来越普遍，但很少有算法提取额外的、有意义的多摄像头跟踪信息。第2章将介绍一些在单个视频流中跟踪运动对象的算法。单摄像机跟踪问题的解决方案已经相当完善。然而，多摄像头监控系统需要能够处理多个视频流的算法。 2.1 动机示例摄像机A和摄像机B是不相交的——它们观察世界的不同区域，并不重叠。然而，摄像机A和C部分重叠，摄像机B和C也是如此。两个摄像机同时可以看到任何一个位于较暗重叠区域的物体。 现在检查这三个摄像机的输出。世界上只有两个人。然而，在这三台相机之间，它们被赋予了四个不同的标签:A-8, B-2, C-4和C-5。给定这些对象标签，我们能找到的最重要的信息是哪些标签指向相同的现实世界对象。这是一致的标签问题。 在一定程度上，人类很擅长解决一致性标签问题。人工监控操作员可以对摄像机在世界上的位置保持一个心理模型，并且即使使用不同的摄像机模式(例如一个RGB摄像机和一个热红外摄像机)，也经常可以匹配摄像机与摄像机的特征。此外，人类在匹配物体方面比计算机做得更好，即使这些物体从不同的角度观察，因此具有不同的外观。然而，使用人类来分析多个视频流并不能很好地扩展，因为一个人一次只能看一个屏幕，即使一个场景可能有许多相关的视图。如果使用多个监视操作员，每个人负责一个特定的区域，那么系统将需要开发控制、目标跟踪、目标切换和可能的操作员不注意的程序。 监视系统的一项重要任务是能够跟踪在监视区域内移动的目标。许多相机可能在任何给定的时间内都在观察目标，但即使只使用少数相机，人类也需要有意识地努力确定这组相机。此外，随着目标的移动，观察目标的摄像机也在不断变化。如果一致的标签问题解决了，计算机知道目标是否应该出现在每个摄像机的视野中，那么计算机就可以自动提示显示目标的正确摄像机组。 图1.2说明了将多个摄像机视为一组单个摄像机的算法和将摄像机视为更多东西的算法之间的区别。图1.2(a)中的算法则不是这样关心多台摄像机可能拍摄世界的同一部分。第二类算法如图1.2(b)所示，取单个摄像机跟踪器的输出并将其组合。创建了新的监视功能。下面将提到这些多摄像头感知算法所创建的一些功能示例。 2.2 范围-目标本论文涵盖了一种多摄像头监控算法的开发、实现和测试。该算法应具有以下特点： 独立于相机外部参数，即位置和方向。该算法应该能够平稳地处理广泛不同的世界视角。 独立于相机的内在参数，即焦距，像素倾斜和主点的位置。市场上有不同的相机，算法应该能够处理多个焦距，分辨率的差异等等。 独立于相机模式。该算法应该能够处理任何单摄像头跟踪器的输出。算法不应该依赖于底层的摄像头硬件RGB，近红外，热红外，或其他成像技术。 解决一致性标签问题。一个真实世界的目标应该链接到该目标可见的每个摄像机中的一个对象标签 对目标遮挡和场景入口的鲁棒性。如果一个目标在场景中间进入监视区域，比如通过一扇门，那么该算法应该正确地解决一致性标签问题。类似地，如果一个目标分裂成两个，比如两个亲密的人走不同的路，算法应该识别并正确标记两个目标。 设置简单。不需要相机校准。如果需要，训练应尽可能少地花费时间，并应在正常的现场交通条件下进行。训练应该是自动的，不需要操作员干预。 具备摄像提示能力。该算法应该能够确定哪些摄像机应该能够看到给定的目标。 2.3 范围-限制算法的范围限制如下: 追踪行走的人的算法应使用。车辆、动物等各类运动物体不在本文研究范围之内。 要处理的相机对至少有部分重叠的视野。这就要求操作员在安装硬件和初始化算法时做出初步判断:决定哪些摄像机看到的是世界上相同的部分。 摄像机应该是静止的。一旦安装，相机的内在和外在参数都应该是固定的。这意味着相机不能安装在平底倾斜炮塔上，或者如果它是，炮塔不能移动。 摄像机的输出图像将是一个实用的大小。该算法将不包括单像素探测器(例如红外运动探测器，光束破光探测器)。这种限制是必要的，以确保单摄像机跟踪是可能的，而不需要对所选算法进行重大更改。 帧率将足以让单摄像头跟踪算法正常工作。 相机应近似常规中心投影相机与基本针孔光学。相机具有极宽的视场视点-鱼眼镜头-或显著未校正的扭曲将不使用。 最重要的是，目标应该在地面上行走。任何两个摄像机之间的重叠区域不得有明显偏离平面。处理丘陵地区或台阶的代码不包含在本算法中。 摄像机不得安装在地平面上。这可以防止在场景几何中出现退化的情况，如下文所示 2.4 对 领域的贡献 如上所述，多摄像头视频处理是一个相对较新的领域。算法一直在开发中，还有很多问题有待解决。如果本文开发的算法满足1.2节和1.3节中描述的目标和限制，则可以实现以下场景: 自动提示:感兴趣的目标走进监视区域。 操作员在一个摄像机中标记目标。当目标在整个区域内移动时，计算机在算法的驱动下，自动显示所有可见目标的视频源。目标可以用颜色一致的“光环”或包围框标记。这让操作员专注于目标的行动，而不是它在世界上相对于每个摄像机的位置。 路径分析:一个区域被置于监视之下。该算法不是试图手动匹配人们从一个摄像头到另一个摄像头的路径，而是自动连接人们通过该区域所走的路径。这使得流量分析能够更快、更有效地进行。 跟踪遮挡恢复。为了欺骗当前的许多跟踪算法，你可以移动到遮挡物后面(例如建筑支柱或高大的同伙)，改变速度，然后移出遮挡物。遮挡破坏了许多当前的跟踪算法，如果速度变化显著，大多数其他算法就会破坏。只要目标在至少一个摄像机中仍然可见，接下来章节中讨论的算法将从遮挡中恢复，并重新建立一致的跟踪标签 现场的入口。该算法应该能够在人们可以从框架中间进入的场景中创建一致的跟踪标签，例如通过电梯或门 2.4.1 具体的贡献本文为视频处理领域提供了以下具体贡献: 一种即使在摄像机明显倾斜时也能找到目标脚的方法 一种利用目标运动来寻找平面诱导单应性的方法，即使入口和出口在空间上是有限的 一种具有特定规则的方法，描述如何使用平面诱导单应性来跨多个摄像机创建和维持目标关联 第三章讨论基础理论，第四章讨论实现细节。试验结果见第5章。 3. 背景","tags":["算法"],"categories":["算法"]},{"title":"单应性框架下基于图切的前景分割的多摄像机人物跟踪","path":"/2023/01/24/单应性框架下基于图切的前景分割的多摄像机人物跟踪/","content":"单应性框架下基于图切的前景分割的多摄像机人物跟踪 Multi Camera Person Tracking Applying a Graph-Cuts basedForeground Segmentation in a Homography Framework 摘要可靠的目标跟踪是实现自动化视频监控系统的必要前提。由于大多数基于机器学习的目标检测方法需要足够的应用场景数据，前景分割是一种流行的方法来寻找可能感兴趣的区域。这些通常需要一个特定的学习阶段，并随着时间的推移进行适应。在这项工作中，我们将提出一种基于图切割的新方法，它优于大多数标准算法。人们普遍认为，遮挡只能在多摄像机环境中解决。应用多层单应性可以使我们仅应用前景数据就能鲁棒地检测和跟踪目标，从而获得较高的跟踪性能。 介绍","tags":["技术"],"categories":["技术"]},{"title":"在手术室多相机多行人跟踪和重识别","path":"/2023/01/24/在手术室多相机多行人跟踪和重识别/","content":"在手术室多相机多行人跟踪和重识别 Multi-Camera Multi-Person T racking and Re-Identification inan Operating Room 摘要多摄像头多人(MCMP)跟踪与重识别(ReID)是安全、行人分析等方面的重要任务;然而，大多数研究都集中在室外场景，因为在一个有障碍物的拥挤房间里，处理遮挡和错误识别要复杂得多。此外，在一个框架内完成这两项任务具有挑战性。本文提出了一种基于轨迹的方法，将跟踪和ReID任务集成在一起。首先，每台摄像机捕捉到的所有手术成员的姿势被逐帧检测;然后，利用检测到的姿态来跟踪每个摄像机的所有成员的轨迹;最后，将这些不同摄像机的轨迹聚类，通过所有摄像机重新识别手术室中的成员。与其他MCMP跟踪和ReID方法相比，本文提出的方法主要利用轨迹，将手术室场景中不易识别的纹理特征作为辅助线索。本文还在ReID过程中集成了时间信息，这比当前最先进的框架(逐帧进行ReID)更可靠。此外，我们的框架在部署到新场景之前不需要培训。本文还创建了一个带有实际手术室视频的注释MCMP数据集。实验证明了所提出的基于轨迹的ReID算法的有效性。提出的框架在ReID任务中达到了85.44%的准确率，在本文提出的手术室数据集中优于最先进的框架。 介绍在本文中，我们旨在建立一种在手术室内获得可靠的单目人员跟踪和相机间人员ReID结果的方法。与直接用神经网络提取图像特征相比，我们主要通过运动轨迹进行判断，以纹理特征作为辅助线索。因为工作人员的运动遵循一定的规则，他们的轨迹比难以区分的纹理特征更可靠。即使是由于遮挡或人为探测失败导致某摄像头的轨迹被“破坏”，在跨摄像头ReID的步骤中，也可以根据其他摄像头更完整的轨迹将其破坏的轨迹连接起来。相对于在闭塞前后强制连接可能属于同一个人的两条轨迹或其他类型的故障，本方法仅追求更好的单目跟踪性能，可以获得更好、更可靠的整体性能。 该方法可分为三个步骤:首先，利用训练好的姿态估计神经网络检测每台摄像机捕捉到的每一帧图像中工作人员的姿态;然后筛选出姿态置信度得分高的工作人员，根据每个人脚的图像坐标和预估的图像平面到世界地平面的单应性矩阵，估计和跟踪每个人在手术室中的位置;最后，我们设计了一种聚类算法，重新识别所有摄像机中被检测到的工作人员的轨迹，从而获得手术室中所有工作人员的完整轨迹。 方法由于手术室场景的特殊性，我们很难用端到端模型解决跟踪和ReID问题;因此，根据其特点，我们建立了一个包含三个模型的框架，逐步完成整个任务。方法流程如图2所示。利用估计的姿态和边界框分别生成所有摄像机的几段轨迹，然后结合所有摄像机的结果连接并重新识别得到的破碎轨迹。 3.1 姿态估计和检测框人体姿态估计的研究在计算机视觉领域已流行多年[27-29]。在本文中，我们使用了一种成熟且流行的姿态估计神经网络AlphaPose[27]。对于每一个被检测到的人，AlphaPose不仅给出一套完整的带有置信度分数的姿态关键点，还给出了整个姿态及其包围框的置信度分数。通过获得的信息，我们能够估计每个手术成员在手术室中的位置，并在后续步骤中提取他们的图像特征。 3.2 单目跟踪因此，受Trackpy[30]的启发，我们设计了一种简洁可靠的方法，在单目跟踪的步骤中利用每个人的位置。首先，从所有检测到的姿态中筛选出可用的姿态，删除那些姿态信心分数小于T11或脚踝信心分数都小于T12的姿态。然后，估计每个筛选出来的手术成员的位置，并将其映射到世界地平面上，我们估计其在图像坐标系中的位置，Cp = (xp, yp)，其两个脚踝坐标在同一帧中，这意味着我们利用一个人的两个脚踝的平均坐标来表示他们的位置: ，其中Cla和Cra分别表示左右脚踝的坐标，然后我们将Cp映射到世界地平面上，使用预先估计的摄像机单应性矩阵H: 其中(Xp, Yp)表示这个人在地坐标系中的坐标。 最后，我们逐帧跟踪每个筛选的手术成员，如果他们在当前帧和下一帧的位置距离小于预先设定的阈值T13(人在地坐标系中可以跑的最大距离)，则在下一帧中跟踪他们。此外，为了处理某人的姿势可能在某些帧中丢失或被过滤掉的情况，我们设置了另一个参数T14，称为“记忆帧数”，这意味着我们考虑到一个人可能会丢失几帧，然后再次出现的可能性，我们保留消失的外科成员的跟踪，并在他们最后一次出现后保留他们的id，最多保留一些帧数。通过这种方法，我们获得了每个摄像机的所有手术成员的初始轨迹。 3.3 相机间重识别由于闭塞和一些其他干扰，获得的轨迹将被分成几个段为每个手术成员如上所述。在单目跟踪中，这个问题很难解决。幸运的是，在手术室场景中放置多个摄像机是很方便的，这意味着我们可以通过其他摄像机更完整的轨迹来克服这些挑战。我们引入了基于密度的空间聚类方法(DBSCAN[31])来总结不同摄像机的轨迹，为属于同一人的摄像机分配相同的ID。 原始DBSCAN的思想非常简洁:它从一个没有访问过的任意核心点开始;检索这个点的e邻域，如果它包含足够多的点，则启动一个集群;否则，这个点被标记为噪声。受此思想启发，我们设计了我们的聚类算法:在原始的DBSCAN中，将核心对象定义为其邻域内的高密度点，基于轨迹的完整性，我们将时间长度(单位:帧)大于T21的轨迹定义为我们的案例中的核心对象;对于核心轨迹p，如果p与另一个轨迹q的平均距离小于T22，且p与q的平均目标框相似度(本文采用直方图相关系数)大于T23，则认为q是p的邻域对象。对于聚类后标记为“噪声”的轨迹，我们根据平均距离和平均包围盒相似度为每个“噪声”轨迹寻找最可能的聚类。此外，为了避免逻辑错误，对应于同一行人并来自同一摄像机的轨迹不应该在时域重叠，因为行人只能在一个摄像机帧中出现一次。最后，我们获得了所有检测到的手术成员和对应的轨迹，每个摄像机的id。","tags":["技术"],"categories":["技术"]},{"title":"基于自定位智能摄像机网络的分布式目标跟踪","path":"/2023/01/24/基于自定位智能摄像机网络的分布式目标跟踪/","content":"基于自定位智能摄像机网络的分布式目标跟踪 Distributed Target Tracking using Self Localizing SmartCamera Networks 摘要针对分布式智能摄像机，提出了一种新的分散目标跟踪方案。该方法建立在分布式定位协议之上，该协议允许智能摄像头节点自动识别具有重叠域的相邻传感器，并建立一个通信图，反映节点如何相互作用以融合网络中的测量。新协议将检测和跟踪问题均匀地分布在整个网络中，以无缝的方式计算传感器的切换。该方法还在网络中的节点之间分配有关被跟踪对象状态的知识。然后，这些信息可以通过分布式查询获得，该查询允许网络参与者订阅他们可能感兴趣的不同类型的事件。所提出的方案已用于使用自定义设计的智能摄像机节点的集合实时跟踪目标。给出了实验结果。 介绍嵌入式智能摄像头系统成本的降低和性能的提高使得考虑将其应用于各种监视和跟踪应用程序具有吸引力。在不久的将来，将有可能像部署灯泡一样部署小型、不显眼的智能摄像头，对广阔的区域提供无所不在的覆盖。我们可以想象使用这样一个系统来跟踪机场的乘客，从他们到达路边值机到他们登机的时间。同样地，我们可以使用这样一个系统来监控老人或体弱多病的人在他们家中的活动，以提高他们的护理质量。 为了实现我们的愿景，一个强大的态势感知感知从分布式摄像机的集合，我们将需要解决分布式传感和跟踪的问题。更具体地说，挑战将是可靠地检测、定位和跟踪目标，因为他们在一个由多个分布式智能摄像机覆盖的扩展区域移动。 为了部署这些系统，我们需要开发检测和跟踪的方法，这些方法可以分布在多个传感器上，而不需要过多的通信。这些系统必须是可扩展的，以允许部署可能涉及分布在扩展区域的数千个摄像头，并且必须对故障具有健壮性，以便在异步添加或删除单个传感器时，整个系统能够优雅地响应。 本文描述了一种基于去中心化的智能摄像机网络检测与跟踪的新方法。这种方法建立在先前的自定位工作的基础上，该工作允许智能摄像机自动检测和定位具有重叠字段的其他摄像机节点，并建立反映节点如何相互作用以融合网络中的测量的通信图。我们开发了具有有限通信要求的新型网络协议，允许系统通过网络均匀分布检测和跟踪问题，以无缝的方式计算传感器的切换。 相关工作","tags":["技术"],"categories":["技术"]},{"title":"adaptive_pose","path":"/2023/01/22/adaptive-pose/","content":"AdaptivePose++ : github 问题 z[‘ap’] 虽然输出了，但是没有参与最终的计算， 这个ap更像是网络自己学的一个中间级的过程，这个ap没有监督 能学到像论文的示意么。 Resample2D的作用是啥，在flownet2中搜到了，但还是不太清楚， 是将不同位置的特征进行融合么。 self.gradient_mul是为了控制ap回传的梯度范围么，这个是经验值吗。 我之前在centernet中加入oks 替换掉了原来的RegWeightedL1Loss_coco()， 你这是额外加了oks的loss，这没有重复学这个回归信息么， 如果为了加快收敛可不可以先RegWeightedL1Loss_coco()，再oks。我一直很好奇如果是自己的数据集，oks中的sigma一般怎么估计呀。 还有oks 的应该是以绝对位置作为计算吧， 我看代码里好像是相对中心点的偏移。 回答 z[‘ap’] 只是用于可视化，没有显示的监督，adaptivepose使用中心特征预测ap偏移，再取出ap位置的特征第二跳偏移，整个两跳path是梯度可回传的，所以相当于隐式监督的。 Resample2D就是warp操作，通过双线性插值取ap位置特征。self.gradient_mul这块意思跟降低该层的学习率一个意思。 我这边实验效果 oks+L1 oks L1。先RegWeightedL1Loss_coco()，再oks这个操作你可以自己试试。自己的数据集如果是人体关键点你直接按着coco取对应位置的sigma就可以了，sigma跟数据集无关。off_to_pose中将中心坐标加到偏移上。首先认为标注过程符合高斯分布，sigma 跟 scale这俩参数乘积，就是高斯分布的方差，直觉上理解就是对偏差的容忍度，比如同样偏移五个像素，可能对于eye的预测误差就是不可容忍的，对于hip的预测误差是可容忍的，对于large scale是可容忍的，对small scale是不可容忍的。coco上提供的标注，也是脸部关键点的sigma最小，其他的大一些，你可以按着这个思路来估算下你所估计的点的sigma。 代码解析","tags":["技术"],"categories":["技术"]},{"title":"20220612讨论","path":"/2023/01/22/20220612/","content":"20220612用docker例子 怎么把你的image构建的很好 容量小，并且满足大部分的应用 shm怎么配置 dockrfile怎么写 挂载怎么做 gpu怎么选卡 怎么在容器里联调， 容器出了问题怎么解决 应用在k8s上有啥问题，需要怎么优化。 每个点都有深度可以做，依据自己的怪话，选择自己的切入点 假如模型都是过拟合，怎么定位这个问题 请问怎么知道它过拟合的类型 因为什么过拟合的 模型原因还是数据原因 训练策略原因 还是说提前没有预热 花点时间研究比较底层的东西， 别人为什么这么设计算法 本质例如多类分类多标签分类分割，这些他们的本质是在做什么事 凸优化大概的一个原理 最优化函数的一个原理 最小二乘，偏最小2乘等等一系列的 找到自己擅长的点。","tags":["技术"],"categories":["技术"]},{"title":"2022年工作暂时性总结","path":"/2023/01/22/2022年工作暂时性总结/","content":"2022年工作暂时性总结1. 车路协同目标检测项目参与车路协同 目标检测项目中的开发支持，人机非等指标达到项目要求 2.多目融合代码开发实现多相机目标融合， 相机切换利用位置进行轨迹平滑， 利用瞩目目标过来了雷视徐建目标，确保无重复id输出，以下沙的数据为例子， 标注了一个融合的数据集 3. 路侧停车泊位划线支持3559a,3519a,ax630 ax620a,sd3403等平台，发测指标P90+ R 90+ 4.全图目标定位和朝向朝向：有限的数据集标注(2-3w)，目标高大于1/30，朝向角度误差11°以内，正确率为88.5%，且检测指标浮动在2%以内 定位：在nx平台上工程化实现，工程耗时由86ms减少到19ms， 指标差异不大，在车顶点、路侧停车数据集以及贴地点伪标签数据上探索了定位不同的实现方式，验证哪些是可行的哪些对参数依赖比较大，减少对人为设定参数的依赖。","tags":["技术"],"categories":["技术"]},{"title":"RANSAC算法","path":"/2023/01/22/RANSAC算法/","content":"RANSAC算法 RANSAC(RAndom SAmple Consensus,随机采样一致)算法 RANSAC(RAndom SAmple Consensus,随机采样一致)算法是从一组含有“外点”(outliers)的数据中正确估计数学模型参数的迭代算法。“外点”一般指的的数据中的噪声，比如说匹配中的误匹配和估计曲线中的离群点。所以，RANSAC也是一种“外点”检测算法。RANSAC算法是一种不确定算法，它只能在一种概率下产生结果，并且这个概率会随着迭代次数的增加而加大（之后会解释为什么这个算法是这样的）。RANSAC算最早是由Fischler和Bolles在SRI上提出用来解决LDP(Location Determination Proble)问题的。 对于RANSAC算法来说一个基本的假设就是数据是由“内点”和“外点”组成的。“内点”就是组成模型参数的数据，“外点”就是不适合模型的数据。同时RANSAC假设：在给定一组含有少部分“内点”的数据，存在一个程序可以估计出符合“内点”的模型。 算法基本思想和流程RANSAC是通过反复选择数据集去估计出模型，一直迭代到估计出认为比较好的模型。具体的实现步骤可以分为以下几步： 选择出可以估计出模型的最小数据集；(对于直线拟合来说就是两个点，对于计算Homography矩阵就是4个点) 使用这个数据集来计算出数据模型； 将所有数据带入这个模型，计算出“内点”的数目；(累加在一定误差范围内的适合当前迭代推出模型的数据) 比较当前模型和之前推出的最好的模型的“内点“的数量，记录最大“内点”数的模型参数和“内点”数； 重复1-4步，直到迭代结束或者当前模型已经足够好了(“内点数目大于一定数量”)。 迭代次数推导假设“内点”在数据中的占比为 那么我们每次计算模型使用 个点的情况下，选取的点至少有一个外点的情况就是 也就是说，在迭代 次的情况下， 就是 次迭代计算模型都至少采样到一个“外点”去计算模型的概率。那么能采样到正确的 个点去计算出正确模型的概率就是 通过上式，可以求得 内点”的概率 通常是一个先验值。然后 是我们希望RANSAC得到正确模型的概率。如果事先不知道 的值，可以使用自适应迭代次数的方法。也就是一开始设定一个无穷大的迭代次数，然后每次更新模型参数估计的时候，用当前的“内点”比值当成 来估算出迭代次数。 用Python实现直线拟合import numpy as npimport matplotlib.pyplot as pltimport randomimport math# 数据量。SIZE = 50# 产生数据。np.linspace 返回一个一维数组，SIZE指定数组长度。# 数组最小值是0，最大值是10。所有元素间隔相等。X = np.linspace(0, 10, SIZE)Y = 3 * X + 10fig = plt.figure()# 画图区域分成1行1列。选择第一块区域。ax1 = fig.add_subplot(1,1, 1)# 标题ax1.set_title(\"RANSAC\")# 让散点图的数据更加随机并且添加一些噪声。random_x = []random_y = []# 添加直线随机噪声for i in range(SIZE): random_x.append(X[i] + random.uniform(-0.5, 0.5)) random_y.append(Y[i] + random.uniform(-0.5, 0.5)) # 添加随机噪声for i in range(SIZE): random_x.append(random.uniform(0,10)) random_y.append(random.uniform(10,40))RANDOM_X = np.array(random_x) # 散点图的横轴。RANDOM_Y = np.array(random_y) # 散点图的纵轴。# 画散点图。ax1.scatter(RANDOM_X, RANDOM_Y)# 横轴名称。ax1.set_xlabel(\"x\")# 纵轴名称。ax1.set_ylabel(\"y\")# 使用RANSAC算法估算模型# 迭代最大次数，每次得到更好的估计会优化iters的数值iters = 100000# 数据和模型之间可接受的差值sigma = 0.25# 最好模型的参数估计和内点数目best_a = 0best_b = 0pretotal = 0# 希望的得到正确模型的概率P = 0.99for i in range(iters): # 随机在数据中红选出两个点去求解模型 sample_index = random.sample(range(SIZE * 2),2) x_1 = RANDOM_X[sample_index[0]] x_2 = RANDOM_X[sample_index[1]] y_1 = RANDOM_Y[sample_index[0]] y_2 = RANDOM_Y[sample_index[1]] # y = ax + b 求解出a，b a = (y_2 - y_1) / (x_2 - x_1) b = y_1 - a * x_1 # 算出内点数目 total_inlier = 0 for index in range(SIZE * 2): y_estimate = a * RANDOM_X[index] + b if abs(y_estimate - RANDOM_Y[index]) sigma: total_inlier = total_inlier + 1 # 判断当前的模型是否比之前估算的模型好 if total_inlier pretotal: iters = math.log(1 - P) / math.log(1 - pow(total_inlier / (SIZE * 2), 2)) pretotal = total_inlier best_a = a best_b = b # 判断是否当前模型已经符合超过一半的点 if total_inlier SIZE: break# 用我们得到的最佳估计画图Y = best_a * RANDOM_X + best_b# 直线图ax1.plot(RANDOM_X, Y)text = \"best_a = \" + str(best_a) + \" best_b = \" + str(best_b)plt.text(5,10, text, fontdict={'size': 8, 'color': 'r'})plt.show()","tags":["算法"],"categories":["算法"]},{"title":"shell学习","path":"/2023/01/22/shell学习/","content":"shell学习1. shell概述Shell 是一个命令解释器， 用于接收应用程序/用户命令， 然后调用操作系统内核。 shell 还是一个功能相当强大的编程语言， 易编写，易调试，灵活性强。 2. shell解析器/bin/sh/bin/bash/usr/bin/bash/bin/rbash/usr/bin/rbash/usr/bin/sh/bin/dash/usr/bin/dash root@2a63e139ac66:/bin# echo $SHELL/bin/bash 系统默认的是bash 3. Shell 脚本入门3.1 脚本格式脚本以#!/bin/bash 开头（指定解析器） 3.2 第一个shell脚本： helloworld3.2.1 需求： 创建一个shell脚本， 输出helloworld#!/bin/bashecho \"helloworld shell\" cd shelldata# ls# pwd/root/shelldata# touch helloworld.sh# vim helloworld.sh### sh helloworld.shhelloworld shell 权限不够 # ./ helloworld.sh/bin/sh: 33: ./: Permission denied 3.2.2 脚本的常用执行方式第一种： 采用bash或者sh+脚本的相对路径或者绝对路径（不用赋予脚本+x权限） 第二种： 采用输入脚本的绝对路径或者相对路径执行脚本（必须具有可执行权限+x） 注意： 第一种执行方法， 本质是bash解析器帮你执行脚本， 所以脚本本身不需要执行权限， 第二种执行方法， 本质是脚本自己执行，所以需要执行权限。 3.3 第二个shell脚本： 多命令处理3.3.1 需求：在/home/atguigu/目录下创建一个bangzhang.txt, 在banzhang.txt中增加“I Love cls” 3.3.2 案例实操#!/bin/bashcd /root/shelldata/touch banzhang.txtecho \"I Love cls\" banzhang.txt 结果显示 sh-5.1# cat banzhang.txtI Love clsI Love clssh-5.1# rm banzhang.txtsh-5.1# bash hadoop101.shsh-5.1# cat banzhang.txtI Love clssh-5.1# vim hadoop101.sh 4. Shell中的变量4.1 系统变量4.1.1 常用系统变量$HOME、$PWD、$SHELL、$USER等 4.1.2 案例实操查看系统变量的值 sh-5.1# echo $HOME/rootsh-5.1# echo $PWD/root/shelldatash-5.1# 显示当前shell中所有变量 BASH=/bin/shBASHOPTS=checkwinsize:cmdhist:complete_fullquote:expand_aliases:extquote:force_fignore:globasciiranges:hostcomplete:interactive_comments:progcomp:promptvars:sourcepathBASH_ALIASES=()BASH_ARGC=()BASH_ARGV=()BASH_CMDS=()BASH_LINENO=()BASH_SOURCE=()BASH_VERSINFO=([0]=\"5\" [1]=\"1\" [2]=\"16\" [3]=\"1\" [4]=\"release\" [5]=\"x86_64-pc-linux-gnu\")BASH_VERSION='5.1.16(1)-release'COLUMNS=177DIRSTACK=()EUID=0GROUPS=()HISTFILE=/root/.bash_historyHISTFILESIZE=500HISTSIZE=500HOME=/rootHOSTNAME=8b25e507bccaHOSTTYPE=x86_64IFS=''LINES=50MACHTYPE=x86_64-pc-linux-gnuMAILCHECK=60OLDPWD=/rootOPTERR=1OPTIND=1OSTYPE=linux-gnuPATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binPIPESTATUS=([0]=\"0\")POSIXLY_CORRECT=yPPID=0PS1='\\s-\\v\\$ 'PS2=' 'PS4='+ 'PWD=/root/shelldataSHELL=/bin/bashSHELLOPTS=braceexpand:emacs:hashall:histexpand:history:interactive-comments:monitor:posixSHLVL=1TERM=xtermUID=0_=/root/shelldata 4.2 自定义变量4.2.1 基本语法定义变量： 变量=值 sh-5.1# A=1sh-5.1# echo AAsh-5.1# echo $A 撤销变量： unset变量 sh-5.1# unset Ash-5.1# echo $Ash-5.1# 声明一个静态变量： readonly变量， 注意：不能unset sh-5.1# readonly B=3sh-5.1# echo $Bsh-5.1# unset Bsh: unset: B: cannot unset: readonly variablesh-5.1# 4.2.2 变量定义规则 变量名称可以由字母、数字和下划线构成， 但是不能以数字开头， 环境变量名建议大写 等号两侧不能有空格 在bash中， 变量默认类型都是字符串类型， 无法直接进行数值运算。 sh-5.1# C=1+1sh-5.1#Display all 505 possibilities? (y or n)sh-5.1# echo $C1+1sh-5.1# 变量的值如果有空格，需要用双引号或者单引号括起来。 sh-5.1# D=bangzhang love mmsh: love: command not foundsh-5.1# D='bangzhang love mm'sh-5.1# echo $Dbangzhang love mmsh-5.1# 可以把变量提升为全局环境变量， 可供其他shell程序使用 export 变量名 sh: ./helloworld.sh: Permission deniedsh-5.1# chmod +x helloworld.shsh-5.1# ./helloworld.shhelloworld shellsh-5.1# export Dsh-5.1# ./helloworld.shhelloworld shellbangzhang love mm 4.3 特殊变量：$n4.3.1 基本语法$n（功能描述：n为数字，$0代表该脚本名称，$1-$9代表第一到第九个参数， 十以上的参数需用大括号包含，如${10}） 4.3.2 案例实操 输出该脚本文件名称、输入参数1和输入参数2的值 #!/bin/bashecho \"$0 $1 $2\"~ sh-5.1# vim parameter.shsh-5.1# bash parameter.shparameter.shsh-5.1# bash parameter.sh banzhangparameter.sh banzhangsh-5.1# bash parameter.sh banzhang lobveparameter.sh banzhang lobvesh-5.1# bash parameter.sh banzhang lobve mmparameter.sh banzhang lobvesh-5.1# vim parameter.sh ~ 4.4 特殊变量：$#4.4.1 基本语法$# (功能描述： 获取所有输入参数的个数， 常用于循环） #!/bin/bashecho \"$0 $1 $2\"echo $#~ ·································sh-5.1# vim parameter.shsh-5.1# lsbanzhang.txt hadoop101.bahs hadoop101.sh helloworld.sh parameter.shsh-5.1# chmod +x helloworld.shsh-5.1# chmod 777 parameter.shsh-5.1# ./parameter.sh cls xyz 111./parameter.sh cls xyzsh-5.1# 4.5 特殊变量：$*、$@$* （功能描述： 这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体） $@（功能描述： 这个变量也代表命令行中所有的参数， 只不过$@把每个参数区分对待） #!/bin/bashecho \"$0 $1 $2\"echo $#echo $*echo $@····························sh-5.1# ./parameter.sh banhh lobe 1111./parameter.sh banhh lobebanhh lobe 1111banhh lobe 1111 4.6 特殊变量$?$? (功能描述： 最后一次执行的命令的返回状态。如果这个变量的值为0表示上个命令执行正确；如果为非零（具体哪个值，由命令自己决定)，证明上一个命令执行不正确了。 证明helloworld脚本 是否正确执行sh-5.1# ./helloworld.shhelloworld shellsh-5.1# echo $?sh-5.1# 第5章 运算符5.1 基本语法“$((运算式))” 或“$[运算式]” expr +,- ,\\*,/,% 加、减、乘、除，取余 注意： expr运算符间要有空格 计算3+2的值 sh-5.1# expr 2 + 3 计算3-2 的值 sh-5.1# expr 3 - 2sh-5.1# 计算（2+3）*4的值 sh: 1: command not foundsh-5.1# expr 'expr 2 + 3' \\* 4expr: non-integer argument采用 $[运算式]方式sh-5.1# s=$[(2+3)*4]sh-5.1# echo $s 第六章 条件判断6.1 基本语法[ condition ] （注意 condition 前后要有空格） 注意： 条件非空即为true， [ atguigu ] 返回true， []返回false 6.2 常用判断条件 两个整数之间比较 = 字符串比较 -lt 小于（less than) -le 小于等于(less equal) -eq 等于（equal） -gt 大于 (greater than) -ge 大于等于（greater equal） -ne 不等于(Not equal) 按照文件权限进行判断 ​ -r 有读的权限(read) -w 有写的权限(write) ​ -x 有执行的权限(execute) 按照文件类型进行判断 -f 文件存在 并且是一个常规的文件(file) -e 文件存在（existence） -d 文件存在并且是一个目录(directory) 23 是否大于等于22sh-5.1# [ 23 -ge 22 ]sh-5.1# echo $?helloworld.sh 是否具有写权限sh-5.1# [ -w helloworld.sh ]sh-5.1# echo $? 多条件判断( 表示前面一条命令执行成功时候，才执行后一条命令， || 表示上一条命令执行失败后，才执行下一条命令) sh-5.1# [ condition ] echo OK || echo notokOKsh-5.1# [ ] echo OK || echo notoknotok 第七章 流程控制7.1 if 判断if [ 条件判断式 ]:then ​ 程序 fi 或者 if [ 条件判断式 ] ​ then ​ 程序 fi 注意事项： [ 条件判断式 ]， 中括号和条件判断式之间必须有空格 if 后要有空格 输入一个数字， 如果是1 ， 则输出banzhang zhenshuai， 如果是2，则输出shell mei，如果是其他，则什么也不输出#!/bin/bashif [ $1 -eq 1 ]then echo \"banzhangzhenshuai\"elif [ $1 -eq 2 ]then echo \"shell mei\"fish-5.1# bash if.shif.sh: line 3: [: -eq: unary operator expectedif.sh: line 7: [: -eq: unary operator expectedsh-5.1# bash if.sh 1banzhangzhenshuaish-5.1# bash if.sh \\2shell meish-5.1# bash if.sh 2shell mei 7.2 case 断句case $变量名 in “值1”） ​ 如果变量的值等于1，则执行程序1 ;; “值2”） ​ 如果变量的值等于2，则执行程序2 ;; …省略其他分支… *） 如果变量的值都不是以上的值，则执行次程序 ;; esac 注意事项： case行结尾必须为单词“in” ， 每一个模式匹配必须以右括号“）” 结束。 双分号“;;”表示命令序列结束，相当于java中的break 最后的“*）”表示默认模式， 相当于java中的default 输入一个数字，如果是1 则输出bangzag ，如果是2 则输出java，如果是其他则输出python#!/bin/bashcase $1 in1) echo \"c++\";;2) echo \"java\";;*) echo \"python\";;esacsh-5.1# vim case.shsh-5.1# bash case.sh 2case.sh: line 10: syntax error: unexpected end of filesh-5.1# vim case.shsh-5.1# bash case.sh 2javash-5.1# bash case.sh 3pythonsh-5.1# bash case.sh 1c++ 7.3 for循环for (( 初始值;循环控制条件;变量变化 )) do ​ 程序 done 从1 加到100#!/bin/bashs=0for(( i=0;i=100;i++ ))do s=$[$s+$i]doneecho $ssh-5.1# vim for1.shsh-5.1# bash for1.sh 语法2 for 变量 in 值1 值2 值3… do ​ 程序 done 打印所有输入参数#!/bin/bashfor i in $*do echo \"shell love $i\"donefor k in \"$*\"do echo \"shell love $k\"donefor j in \"$@\"do echo \"shell love $j\"donesh-5.1# touch for2.shsh-5.1# vim for2.shsh-5.1# sh for2.sh 1 2 3shell love 1shell love 2shell love 3sh-5.1# sh for2.sh 1 2 3shell love 1shell love 2shell love 3shell love 1 2 3shell love 1shell love 2shell love 3 7.4 while循环while [ 条件判断式 ] do 程序 done 从1+到100#!/bin/bashs=0i=1while [ $i -le 100 ]do s=$[$s + $i] i=$[$i + 1]doneecho $s~ sh-5.1# bash while.sh 第八章 read 读取控制台输入read(选项)(参数) 选项： -p： 指定读取值时的提示符： -t : 指定读取值时等待的时间（秒） 参数 ​ 变量：指定读取值得变量名 提示7s内读取控制台输入的名称 #!/bin/bashread -t 7 -p \"Enter your name i 7 seconds \" NAMEecho $NAME~ sh-5.1# bash read.sh shellEnter your name i 7 seconds shellshell 第九章 函数9.1 系统函数basename基本语法 basename [string/pathname] [suffix] (功能描述： basename 的命令会删掉所有前缀包括最后一个（’/‘)字符，然后将字符串显示出来 选项： suffix为后缀， 如果suffix被指定了， basenname会将pathname或者string中的suffix去掉。 截取该/homeshell/banzhang.txt路径的文件名称sh-5.1# basename /homeshell/banzhang.txt .txtbanzhang dirname 文件绝对路径 ( 功能描述： 从给定的包含绝对路径中取出文件名（非目录的部分)，然后返回剩下的路径) 获取截取该/homeshell/banzhang.txt路径的文件路径sh-5.1# dirname /homeshell/banzhang.txt/homeshell 9.2 自定义函数基本语法 [ function ] funname[()] { ​ Action； ​ [return int;] } 经验技巧 2.1 必须在调用地方之前，先声明函数， shell脚本是逐行运行的， 不会像其他语言一样 先编译。 2.2 函数返回值， 只能通过$?系统变量或得，可以显示加:return 返回，如果不加，将以最后一条命令运行结果作为返回值，return后跟数值n(0-255) 计算两个输入参数的和#!/bin/bashfunction sum(){ s=0; s=$[$1+$2] echo $s}read -p \"input your paramter1: \" P1read -p \"input your paramter1: \" P2sum $P1 $P2sh-5.1# vim sum.shsh-5.1# bash sum.shinput your paramter1:input your paramter1:sum.sh: line 6: +: syntax error: operand expected (error token is \"+\")sh-5.1# bash sum.shinput your paramter1: 10input your paramter1: 20 第十章 shell工具（重点）10.1 cutcut的工作就是“剪”， 具体的说就是在文件中负责剪切数据用的。cut命令从文件的每一行剪切字节、字符和字段并将这些值输出 基本用法 ​ cut[选项参数] filenames ​ 说明: 默认分割符是制表符 选项参数说明 数据准备sh-5.1# touch cut.txtsh-5.1# vim cut.txtsh-5.1# cat cut.txtdong shengguan zhengwo wola lalei leish-5.1# cut -d \" \" -f 1 cut.txtdongguanwolalei在cut文件中切割出guanh-5.1# cat cut.txt | grep guanguan zhengsh-5.1# cat cut.txt | grep guan | cut -d \" \" -f 1guan选取系统PATH变量值， 第二个“：”开始后的所有路径：sh-5.1# echo $PATH/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/binsh-5.1# echo $PATH | cut -d : -f 3-/usr/sbin:/usr/bin:/sbin:/bin切割ifconfig后打印的ip地址失败了。sh-5.1# ifconfig eth0 | grep \"inet\" | cut -d '' -f 2 inet 172.17.0.2 netmask 255.255.0.0 broadcast 172.17.255.255sh-5.1# 10.2 sed sed是一种流编辑器， 它一次处理一行内容， 处理时，吧当前处理的行存储在临时缓存区中， 称为：模式空间，接着用sed命令处理缓冲区中的内容， 处理完成后，把缓冲区的内容送往屏幕， 接着处理下一行， 这样不断重复， 直到文件末尾。文件内容没有改变， 除非你使用重定向存储输出。 基本用法 sed[选项参数] “command” filename 10.2.2 实战将mei nv 这个单词插入到sed.txt 第二行下， 打印sh-5.1# touch sed.txtsh-5.1# vim sed.txtsh-5.1# sed '2a mei nv' sed.txtdong shenot pythonmei nvll llko ok删除文件中包含所有wo的行sh-5.1# sed \"/m/e\" sed.txtdong shenot pythonll llko ok将sed中wo替换为nish-5.1# sed \"s/ot/to/g\" sed.txtdong shento pythonll llko ok注意：g表示global ，全部替换将sed中第二行删除并将wo替换为nish-5.1# sed -e \"2d\" -e \"s/to/ot/g\" sed.txtdong shenll llko ok 10.3 awk一个强大的文本分析工具， 把文件逐行的读入，以空格为默认分隔符将每行切片， 切开的部分再进分析处理。 基本用法 awk [选项参数] ‘pattern1{action1} pattern2{action2} ….’ filename pattern: 表示AWK在数据中查找的内容，就是匹配模式 action:在找到匹配时，所执行的一系列命令 实操数据准备 sh-5.1# cp /etc/passwd ./sh-5.1# lsbanzhang.txt case.sh cut.txt for1.sh for2.sh hadoop101.bahs hadoop101.sh helloworld.sh if.sh parameter.sh passwd read.sh sed.txt sum.sh while.shsh-5.1# awk -F: '/^root/{print $7}' passwd /bin/bash/bin/bashsh-5.1# 搜索passwd文件以root关键词开头的所有行，并输出该行的第7列 搜索passwd文件以root关键词开头的所有行，并输出该行的第7列和第一列，中间以“，” 分隔 sh-5.1# awk -F: '/^root/{print $1“，”$7}' passwd /bin/bash 注意只有匹配了pattern的行才会执行action 只显示、etc/passwd的第一列和第七列，以逗号分隔，且在所有行前面铁建列名user， shell在最后一行叠加“dahaige , /bin/zuishuai”. awk -F: 'BEGIN{print \"user,shell\"}{print $1\",\"$7} END{print \"dahaiuge,/bin/zuishuai\"}' passwd 注意LBEGIN在所有数据读取行之前执行， END在所有数据执行完之后执行 将passwd中的用户id增加数值1并输出 awk -vi1-F: '{print $3+i}' passwd awk的内置变量 实操 统计passwd文件名，每行的行号，每行的列数 awk -F: '{print \"filename:\" FILENAME \", linenumer:\" NR \",columns:\" NF}' passwd 切割ip ifconfig eth0 | grep \"inet addr\" | awk -F : '{print $2}' | awk -F \" \" '{print $1}' 查询sed.txt中空行所在的行号 awk '/^$/{print NR}' sed.txt 10.4 sortsort 命令在LInux里非常游泳，它将文件排序，并将排序结果标准输出 实操 按照“：” 分隔之后排序 sort-t ： -nrk 2 sort.sh 第十一章 企业面试题11.1 京东问题1 ：使用linux命令查询file1中空行所在的行号 答案： awk '/^$/{print NR}' sed.txt 问题2： 有文件chengji.txt内容如下： 张三 40 李四 50 王五 60 使用linux命令计算第二列的和并输出 cat chenji.TXT| AWK -F \" \" '{sum+=$2} END{print sum}' 11.2 搜狐和讯网问题1： shell脚本里如何检查一个文件是否存在，如果不存在该如何处理？ #!/bin/bashif [ -f file.txt ]; then echo \"文件存在！\"else echo \"文件不存在！\"fi 11.3 新浪问题1： 用shell写一个脚本，对文本中无序的一列数据排序 sort -n test.txt|awk '{a+=$0;print$0}END{print \"SUM-\"a}' 11.4 金和网络问题1： 请用shell脚本写出 查找当前文件夹，(/home) 下所有文本文件内容包含有字符“shen”的文件名称 grep -r \"shen\" /home |cut -d \":\" -f 1 留下文件名","tags":["技术"],"categories":["技术"]},{"title":"单应矩阵H求解","path":"/2023/01/22/单应矩阵H求解/","content":"单应矩阵H求解所需求解的单应矩阵：$$H_{3 \\times 3}=\\left[\\begin{array}{lll}h_{11} h_{12} h_{13} \\h_{21} h_{22} h_{23} \\h_{31} h_{32} h_{33}\\end{array}\\right]$$单应变换关系：$$\\mathrm{s}\\left[\\begin{array}{c}x^{\\prime} \\y^{\\prime} \\1\\end{array}\\right]=H\\left[\\begin{array}{l}x \\y \\1\\end{array}\\right]=\\left[\\begin{array}{lll}h_{11} h_{12} h_{13} \\h_{21} h_{22} h_{23} \\h_{31} h_{32} h_{33}\\end{array}\\right]\\left[\\begin{array}{l}x \\y \\1\\end{array}\\right]$$为减少自由度， 令h33 =1, s为尺度因子。$$h_{31} x+h_{32} y+h_{33}=h_{31} x+h_{32} y+1$$ $$\\begin{aligned}x_{i}^{\\prime} =\\frac{h_{11} x_{i}+h_{12} y_{i}+h_{13}}{h_{31} x_{i}+h_{32} y_{i}+h_{33}} \\y_{i}^{\\prime} =\\frac{h_{21} x_{i}+h_{22} y_{i}+h_{23}}{h_{31} x_{i}+h_{32} y_{i}+h_{33}}\\end{aligned}$$ 在图像上取在真实世界构成矩形的顺时针的四个角点。 (x1,y1),(x2,y2),(x3,y3),(x4,y4)以及构成直角的三个点 (x5,y5),(x6,y6),(x7,y7), 其中（x6,y6)为直角点 预设期望变换后的矩形，第一个焦点的位置（x1’,y1’)(x2’,y2’) 设计方程组： 令（x1，y1) (x2,y2) 变换到预设位置（x1’,y1’)(x2’,y2’)$$\\begin{aligned}x_{1}^{\\prime} =\\frac{h_{11} x_{1}+h_{12} y_{1}+h_{13}}{h_{31} x_{1}+h_{32} y_{1}+h_{33}} \\x_{2}^{\\prime} =\\frac{h_{11} x_{2}+h_{12} y_{2}+h_{13}}{h_{31} x_{2}+h_{32} y_{2}+h_{33}} \\y_{1}^{\\prime} =\\frac{h_{21} x_{1}+h_{22} y_{1}+h_{23}}{h_{31} x_{1}+h_{32} y_{1}+h_{33}} \\y_{2}^{\\prime} =\\frac{h_{21} x_{2}+h_{22} y_{2}+h_{23}}{h_{31} x_{2}+h_{32} y_{2}+h_{33}}\\end{aligned}$$令（x1’,y1’)和(x4’,y4’)构成的直线垂直与（x1’,y1’)(x2’,y2’) 构成的直线$$\\frac{y_{2}^{\\prime}-y_{1}^{\\prime}}{x_{2}^{\\prime}-x_{1}^{\\prime}} \\cdot \\frac{y_{4}^{\\prime}-y_{1}^{\\prime}}{x_{4}^{\\prime}-x_{1}^{\\prime}}=-1$$令x’4到x1’的距离等于x3’ 到x2’的距离， y4’到y1’的距离等于y3’到y2‘的距离，使得(x4’,y4’)和(x3’,y3’)在（x1’,y1’)和(x2’,y2’) 构成的直线的同一侧， 且结合上一个约束， 使得(x2’,y2’) 和(x3’,y3’)直线平行与（x1’,y1’)和(x4’,y4’) 构成的直线，即(x2’,y2’) 和(x3’,y3’) 垂直与 （x1’,y1’)和(x2’,y2’) 构成的直线$$\\begin{aligned}x_{4}^{\\prime}-x_{3}^{\\prime}=x_{1}^{\\prime}-x_{2}^{\\prime} \\y_{4}^{\\prime}-y_{3}^{\\prime}=y_{1}^{\\prime}-y_{2}^{\\prime}\\end{aligned}$$令(x5’,y5’) 和(x6’,y6’) 垂直与（x6’,y6’)和(x7’,y7’) 构成的直线$$\\frac{y_{5}^{\\prime}-y_{6}^{\\prime}}{x_{5}^{\\prime}-x_{6}^{\\prime}} \\cdot \\frac{y_{7}^{\\prime}-y_{6}^{\\prime}}{x_{7}^{\\prime}-x_{6}^{\\prime}}=-1$$联立上述8个方程和h33=1可求出单应矩阵H 此外为了简化求解， 可使得预设期望的变换后的矩形第一个角点（x1’,y1’)，第二个角点的位置（x2’,y2’)的y1和y2 或者x1和x2 相等， 只需要x3=x2 ,x4=x1 即可以保证垂直关系， 计算将大大简化 通过多选几组点， 结合RANSAC等算法可进行全局优化。","tags":["技术"],"categories":["技术"]},{"title":"智慧交通项目","path":"/2023/01/22/智慧交通项目/","content":"智慧交通项目1. 项目简介学习目标： 了解智慧交通项目的架构 了解智慧交通项目中的模块 完成智慧交通项目的环境搭建 主要讲解计算机视觉在交通领域的相关应用， 包括车道线检测、多目标车辆追踪以及流量统计方法 多目标车辆追踪和计数：SORT方法、匈牙利算法。卡尔曼滤波、虚拟线圈等 车道线检测：张氏相机校正、仿射变换等。 项目的架构 用户层 服务层 2. 环境安装工具包如下： Numpy Numba Scipy h5py pandas opencv-python moviepy Filterpy 安装方法 3.车流量检测实现学习目标 了解多目标跟跟踪的实现方法 知道车流量统计的方法 车流量统计主要有以下几种方式： 人工统计， 耗时耗力，且统计结果不具有可验证性 传感器计数， 容易收到外界环境因素干扰 基于视频的车流量统计， 本项目所使用的方法 主要包括以下几个步骤： 使用yolov3模型进行目标检测 使用sort算法进行目标跟踪， 使用卡尔曼滤波进行目标位置预测，并利用匈牙利算法对比目标的相似度， 完成车辆目标跟踪 利用虚拟线圈的思想来实现车辆目标的技术，完成车流量的统计。 项目流程如下图所示： 3.0 项目介绍3.1 多目标跟踪学习目标： 了解多目标跟着的常见分类方法 了解多目标跟踪中常用的运动模型 知道多目标跟着的常用算法 1. 多目标跟踪分类多目标跟踪，即MOT，也就是在一段视频中同时跟踪多个目标。MOT主要应用在安防监控和自动驾驶等领域中。 多目标跟踪可以看做多变量估计问题， 即给定一个图像序列 1.1 初始化方法 多目标跟踪问题，并不是所有目标都会在第一帧出现，也并不是所有目标都会在出现在每一帧，。那如何对出现的目标进行初始化， 可以作为跟踪算法的分类表针， 常见的初始化方法分为两大类，一个是Detection-based-tracking（DBT），另一个是Detection-free-tracking(DFT)。 1.2 处理模式 MOT也存在着不同的处理模式， Online和Offline两大类， 其主要区别在于是否用了后续帧信息， 下图形象的解释了Online和offline的区别 Online Tracking Online Tracking 是对视频帧进行逐帧进行处理， 当前帧的跟踪仅仅利用过去的信息。 Offline tracking 不同于Online Tracking， Offline Tracking会利用前后视频帧对当前帧进行目标跟踪， 这种方式只适合视频， 如果应用于摄像头，则会有滞后效应， 通常采用时间窗的方式进行处理，节省内存和加速 2. 运动模型为了简化多目标跟踪的难度， 引入运动模型类简化求解过程，运动模型捕捉目标的动态行为，它估计目标在未来帧中的潜在位置， 从而减少搜索空间。在大多数情况下， 假设目标在现实中是平缓运动的， 那么图像空间也是如此，对于车辆的运动， 大致可以分为线性和非线性两种运动： 线性运动： 线性运动是目标最主流的模型，假设目标的运动属性平稳（速度，加速度，位置） 非线性运动：虽然线性运动模型比较常用， 但由于存在它解决不了的问题，非线性模型随之产生，它可以使tracklets间运动相似度计算得更加准确。 3. 跟踪方法多目标跟踪基于神经网络的算法， 端到端的算法并不多，主要还在实验室刷榜阶段，模型复杂，速度慢， 效果不好。 主要介绍两种主流的算法： 3.1 基于Kalman和KM算法的后端优化算法 该类算法能达到实时性， 但依赖于检测算法效果摇号，特征区分要好， （输出最终结果的好坏，依赖于较强的检测算法，而基于卡尔曼加匈牙利匹配的跟踪算法作用于能够输出检测目标的id，其次能够保证追踪算法的实时性，这样追踪效果会好，id切换少代表的算法是SORT/deepSort SORT是一种实用的多目标跟踪算法，引入线性速度模型于卡尔曼滤波来进行位置预测，再无合适匹配检测框的情况下，使用运动模型来预测物体的位置。匈牙利算法是一种寻找二分图的最大匹配的算法，在多目标跟踪问题中可以简单理解为寻找前后两帧的若干目标的匹配最优解的一种算法。而卡尔曼滤波可以看做是一种运动模型，用来解决对目标轨迹预测，并使用置信度比较高的跟踪结果进行预测结果的修正。 多目标跟踪一般接在目标检测后， 在工业界目标检测采用比较多的是yolo检测算法。先实现目标检测网络， 检测的输出结果主要是将检测框的位置信息输入到多目标跟踪算法中。 3.2 基于多线程的单目标跟踪的多目标跟踪算法 这类算法特点是跟踪效果很好，因为每一个类的物体都单独分配了一个跟踪器， 但该算法对尺度变换的要求大，参数调试需要合理，同时耗费cpu资源，实时性不高， 代表算法是利用KCF进行目标跟踪。 多目标跟踪本质上是多个目标同时运动的问题， 所以有提出将单目标跟踪器引入到多目标追踪的问题，为每一个目标分配一个跟踪器。然后检测地使用匹配算法来修正那些跟踪失败或者新出现的目标，代表性的单目标跟踪算法为核相关滤波算法（KCF），在精度核速度上同时达到很高的水平，是当时单目标跟踪最优秀的算法之一，后来的很多算法都是基于此做的改进。 实际应用过程会为每个目标分配一个KCF跟踪器并采用多线程的方式来组织这些跟踪器，同时因为实际硬件条件的限制，不可能提供强大的计算资源，会采用检测器与跟踪交替策略，由于检测帧率不高， 跟踪效果滞后性， 实用效果不大。 3.2 辅助功能 yolo的数据格式 iou的详解 3.3 卡尔曼滤波器3.4 卡尔曼滤波器实战3.5 目标估计模型-卡尔曼滤波3.6 匈牙利算法3.7 数据关联3.8 SORT/deepSORT3.9 多目标跟着实现3.10 yolo模型3.12 基于yolo的目标检测3.12 车流量统计3.13 视频中的车流量统计","tags":["技术"],"categories":["技术"]},{"title":"从视频标定到SLAM","path":"/2023/01/22/浙江大学视觉SLAM公开课-从视频标定到SLAM/","content":"浙江大学视觉SLAM公开课-从视频标定到SLAMCAMERA Pinhole Camera （针孔摄像） 角度 距离消失 平行不保留，长度不在绝对 维度压缩，丢失了非常多的信息 制作一个相机 如何做一个相机， Shringking the Aperture（收缩光圈） 孔太小， 曝光时间太少， 孔太小， 光的波长不能被忽略会发生光的干涉现象。 镜头的作用是啥呢？ 用凸透镜 把光聚集起来，聚焦起来到一个点上， 曝光时间简短， 还是会散开， 镜面的角度有关 只有到镜头固定距离，才能清晰成像。，只有一定范围内才清晰。 depth of filed。 光圈的作用 光圈的速度， 光圈越小， 比较长时间曝光， 光圈越大F = 1/直径 filed of view 跟相机的焦距直接相关。","tags":["技术"],"categories":["技术"]},{"title":"目标检测中的匹配机制","path":"/2023/01/22/目标检测中的匹配机制/","content":"目标检测中的匹配机制1. FCOS相较于reatinnet FOs 的正负样本匹配机制会产生更多的正样本 2. ATSS匹配步骤 计算每个gt box与多尺度输出层 anchor之间的IoU 计算每个gt box与多尺度输出层anchor中心坐标的L2距离 遍历层 ， 遍历gt box ，topk(k 默认是9 ）L2 最小距离的anchor， 一共有L层个 L*K, 每个gt 对于每个gt box， 计算anchor与box之间的（IoU）的（mean， var） 均值和方差，相加之和作为阈值 对于每个gt box 大于阈值的作为正样本， 其他作为负样本 若topk过大，导致不在内部， 则过滤掉 3. OTA(Optimal Transport Assignment)4. simOTA优点 simOTA能够自动分析每个gt、所需要多少正样本 能自动决定每个gt 从哪个特征图来负责检测 相比OTA, simOTA更快 相比于OTA， 避免额外超参数 代码逻辑 计算正区域 计算anchor与gt的iou 在候选区域计算cost 利用iou确定每个gt的dynamic_k gt选取cost最小dynamic_k个anchor正样本， 其余为负 使用正负样本计算loss","tags":["技术"],"categories":["技术"]},{"title":"matlab基础","path":"/2020/02/05/20200205-practise-matlab/","content":"%%% 第一题% 给定N和A，N是一个整数，A是一个向量，例如N = 3; A = [ 4 5 6 7]，请使用matlab命令将A中的每一个% 元素重复N次，然后形成一个向量，示例计算结果如下：N = 3; A = [ 4 5 6 7];B=A(ones(1,N),:); % 注意这里下标的实验，可以使用相同的下标多次B(:) %%% 假设x是一个向量，例如% x = [ 4 4 5 5 5 6 7 7 8 8 8 8 ]% 现在我们想得到如下两个向量% l = [ 2 3 1 2 4 ]; % x每个元素重复的个数% v = [ 4 5 6 7 8 ]; % x中重复元素的值x = [ 4 4 5 5 5 6 7 7 8 8 8 8 ]i = [find(x(1:end-1) ~= x(2:end)) length(x)]l = diff([0 i])v = x(i) %%% 求建立以下tables数组% table N riqi xianxing Ndanhao Nshuanghao% 第一列，行编号% 第二列，2017年每天的日期，datetime格式% 第三列，如果日期为单号，那么写文本“单号通行”，如果日期是双号，那么写文本“双号通行”，如果是周% 末，则写“单双通行”% 第四列，写当前日期单号车一共通行了多少天% 第五列，写当前日期双号车一共通行了多少天NDays = yeardays(2017) %2017年的天数N=[1:NDays]';riqi=datetime(2017,1,1)+N-1;%注意这里，很多时候可以采用先预置一种统一答案，然后其中某些结果可以依次覆盖原有答案xianxing = repmat('单号通行',NDays,1);a = logical(mod(riqi.Day,2)); %日期是否为单数xianxing(~a,:)= repmat('双号通行',sum(~a),1);tf = isweekend(riqi);xianxing(tf,:) = repmat('单双通行',sum(tf),1);idxdanhao=all(xianxing=='单号通行',2)|all(xianxing=='单双通行',2)idxshuanghao=all(xianxing=='双号通行',2)|all(xianxing=='单双通行',2)Ndanhao=cumsum(idxdanhao);Nshuanghao=cumsum(idxshuanghao);XianXiangData = table(N,riqi,xianxing,Ndanhao,Nshuanghao); %%% 通过load('data.mat')命令载入当前目录下的数据文件，data.mat，然后进行以下处理：% data数据是一个手写数字图像的灰度数据，一共2000行785列，2000表示是2000副图片，第一列是当前图片% 的数字是几（0-9范围，单个整数），第2列至第785列是图像的灰度数据，图片原始大小为28×28=784个像素% 点，然后按照以下编号方式(图片未显示完全)，将784各点转换为了1行，作为了data数据中的一行% 现在要求将data第一列提取为一个列向量，命名为trainT，另外第2列至第785列数据转换为28×28×2000的% 三维数组，第一页为第一个数字的图像，要求按照图片中的顺序，将第一行中的灰度数据存成新的28×28的数% 组，命名为trainDdata=load('data.mat');data1 = data.data(:,2:end);trainId = reshape(data1',28,28,2000);trainId = permute(trainId,[2 1 3]); %维数的转换trianT = data.data(:,1);figureimshow(uint8(trainId(:,:,7))) %%% 在同一个图内绘制两个圆的曲线，一个半径为1，一个半径为2，在右方外侧中部添加图例，“小圆”和”大% 圆“，绘图区域设置为正方形，标题设置为“两个圆”，横轴标签为x，纵轴标签为y% 双坐标轴图% 载入数据datahis.mat，% 然后绘制datahis0.t_his为横坐标，datahis0.temp为左侧纵坐标，datahis0.hum为右侧纵坐标的双坐标轴图形% 在图像下方外侧添加图例“温度”，“湿度”，横轴标签设置为时间，左侧y轴设置标签为温度，右侧y轴标签% 设置为湿度% datahis0.t_his为时间，需要转换为datetime类型，然后选取1月10日的数据进行画图clearfigureload('datahis.mat')t = datetime(datevec(datahis0.t_his));idx=t=datetime(2017,1,10)t=datetime(2017,1,11);t1 = t(idx);temp1 = datahis0.temp(idx);hum1 = datahis0.hum(idx);yyaxis leftplot(t1,temp1)xlabel('时间')ylabel('温度')yyaxis rightplot(t1,hum1)ylabel('温度')legend('温度','湿度','Location','southoutside','Orientation','horizontal') %%% 多子图的绘制% 给定以下数据% x = 0:0.01:20; % x坐标% y1 = 200*exp(-0.05*x).*sin(x); % Y1% y2 = 0.8*exp(-0.5*x).*sin(10*x); % Y2% y3 = 100*exp(-0.5*x).*sin(5*x); % Y3% y1，y2，y3需要分别绘制一个子图，其中y1占据左侧一半位置，y2占据右侧上方，y3占据右侧下方clearfigurex= 0:0.01:20; % x坐标y1 = 200*exp(-0.05*x).*sin(x); % Y1y2 = 0.8*exp(-0.5*x).*sin(10*x); % Y2y3 = 100*exp(-0.5*x).*sin(5*x); % Y3subplot(2,2,[1 3])plot(x,y1)subplot(2,2,2)plot(x, y2)subplot(2,2,4)plot(x,y3) %%% 曲面图% 绘制函数的网格图,x,y的取值范围为-2到2figureclearx = -2:0.1:2;y=x;[X,Y]=meshgrid(x,y)Z = sin(X.^2+Y.^2);mesh(X,Y,Z) %%% 三维饼形图% 绘制三维饼形图，各元素所占数值为： [6 3 7 5 1 2 4]，突出显示第1，3个元素，7个元素的标签分别为% '周一'到'周日'figurex =[6 3 7 5 1 2 4];labels = {'周一','周二','周三','周四','周五','周六','周日'};explode=[1 0 1 0 0 0 0] %突出显示向量x的元素pie3(x,explode,labels) %%% 绘制如下双纵轴柱形图。数据为：% x=1:20;% y1=sin(x)+2;% y2=(x-10).^2;% 注意柱形图等图形也是可以使用双坐标轴绘制的clearfigurex=1:20;y1=sin(x)+2;y2=(x-10).^2;yyaxis leftbar(x+0.2,y1,0.3,'b') %注意图形叠加位置的调整ylabel('sin')yyaxis rightbar(x-0.2,y2,0.3,'r')ylabel('x^2')legend('sin','x^2','Location','southoutside','Orientation','horizontal')title('双纵轴柱形图')xlabel('x') %%% 绘制以下数据对应曲线，并增加横轴标签及对应曲线上的数值做标记。并在图中写标记公式文本% x=2:20;% alpha=x.^2;% beta=log(x);% y=alpha./beta;clearfigurex=2:20;alpha=x.^2;beta=log(x);y=alpha./beta;plot(x,y)hold onplot(sqrt(23),sqrt(23).^2./log(sqrt(23)),'o')text(6,100,'$$\\frac{\\alpha}{\\beta}$$','Interpreter','latex');%字符的输出xticks(sort([2:2:20 sqrt(23)]))h1=gca;h1.XTickLabel{3}='$$\\sqrt{23}$$'; %字符的输出h1.TickLabelInterpreter='latex'; %%% 绘制如下图形% t1 = datetime(2014,1:12,1);% temp = [0 2 12 11 15 25 23 27 25 24 12 8];clearfiguret1 = datetime(2014,1:12,1);temp = [0 2 12 11 15 25 23 27 25 24 12 8];h = plot(t1,temp,':*');ax = h.Parent;title('A Year of Temperatures on the 1st of the Month')ylabel('Degrees Celcius ^{\\circ}') % 上角标的使用，摄氏度符号yt1 = ax.YTickLabel %坐标轴标签为元胞数组，在此基础上进行修改ytld = strcat(yt1,'^{\\circ}') %上角标ax.YTickLabel = ytld; %%% 按照要求绘图% t=(pi*(0:1000)/1000)';% y=sin(t);% 以t为横轴数据，y为纵轴数据绘制图形，要求：% 1，将纵坐标轴设置在右方，% 2，将横轴标签设置为[0 1/4*pi 1/2*pi 3/4*pi pi]，坐标轴范围设置为0～pi% 3，输出图形的figure窗口位置坐标% 4，输出图形axes的位置坐标clearfiguret=(pi*(0:1000)/1000)';y=sin(t);h = plot(t,y)set(gca,'YAxisLocation','right')xticks([0 1/4*pi 1/2*pi 3/4*pi pi])xticklabels({'0','1/4 \\pi','1/2 \\pi','3/4 \\pi','\\pi'})xlim([0 pi]) %%% 按照以下位置绘制图形% 其中需要用到的数据如下，另外图片坐标的位置与示例图中大致相等即可% % 图1% x=linspace(0.2*pi,20);% y=sin(x);% % 图2% t=0:pi/100:20*pi;% x=sin(t);% y=cos(t);% z=t.*sin(t).*cos(t);% % 图3% [x,y]=meshgrid(-8:0.5:8);% z=sin(sqrt(x.^2+y.^2))./sqrt(x.^2+y.^2+eps);clearfigure% 图1x=linspace(0.2*pi,20);y=sin(x);axes('Position',[0.6,0.2,0.2,0.7],'GridLineStyle','-');plot(y,x);grid on% 图2axes('Position',[0.1,0.2,0.5,0.5]);t=0:pi/100:20*pi;x=sin(t);y=cos(t);z=t.*sin(t).*cos(t);plot3(x,y,z);% 图3axes('Position',[0.1,0.6,0.25,0.3]);[x,y]=meshgrid(-8:0.5:8);z=sin(sqrt(x.^2+y.^2))./sqrt(x.^2+y.^2+eps);mesh(x,y,z) %%% 以下数据进行拟合% load ex1.mat% cftoolload ex1.mat %%% 使用命令导入文件2016010600.txt中的数据，提取第4列日期数据和第5列时刻数据数据，合并生成一个% datetime数组，命名为Tcleara = readtable('2016010600.txt');t1 = a.Var4;t2 = a.Var5;TPvec_y = floor(t1/10000);TPvec_mon = floor((t1-TPvec_y*10000)/100);TPvec_d = mod(t1,100)TPvec_h = floor(t2/100);TPvec_min=mod(t2,100);TPvec_s = zeros(size(t1,1),1);t_vec=[TPvec_y TPvec_mon TPvec_d TPvec_h TPvec_min TPvec_s];T = datetime(t_vec) %%% 生成与2016010600.txt文件中第6列数据概率分布相同的随机数，数量10000个.clearfigurea = readtable('2016010600.txt')v = a.Var6;histogram(v,20)[N,edges] = histcounts(v,20);rm = RDrnd(N,10000);figurehistogram(rm,20)","tags":["MATLAB"],"categories":["其他"]},{"title":"matlab基础","path":"/2020/02/05/practise-matlab/","content":"1 2 3 4 5 6 7 | %% % 第一题 % 给定N和A，N是一个整数，A是一个向量，例如N = 3; A = [ 4 5 6 7]，请使用matlab命令将A中的每一个 % 元素重复N次，然后形成一个向量，示例计算结果如下： N = 3; A = [ 4 5 6 7]; B=A(ones(1,N),:); % 注意这里下标的实验，可以使用相同的下标多次 B(:) —|— 1 2 3 4 5 6 7 8 9 10 | %% % 假设x是一个向量，例如 % x = [ 4 4 5 5 5 6 7 7 8 8 8 8 ] % 现在我们想得到如下两个向量 % l = [ 2 3 1 2 4 ]; % x每个元素重复的个数 % v = [ 4 5 6 7 8 ]; % x中重复元素的值 x = [ 4 4 5 5 5 6 7 7 8 8 8 8 ] i = [find(x(1:end-1) ~= x(2:end)) length(x)] l = diff([0 i]) v = x(i) —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 | %% % 求建立以下tables数组 % table N riqi xianxing Ndanhao Nshuanghao % 第一列，行编号 % 第二列，2017年每天的日期，datetime格式 % 第三列，如果日期为单号，那么写文本“单号通行”，如果日期是双号，那么写文本“双号通行”，如果是周 % 末，则写“单双通行” % 第四列，写当前日期单号车一共通行了多少天 % 第五列，写当前日期双号车一共通行了多少天 NDays = yeardays(2017) %2017年的天数 N=[1:NDays]'; riqi=datetime(2017,1,1)+N-1; %注意这里，很多时候可以采用先预置一种统一答案，然后其中某些结果可以依次覆盖原有答案 xianxing = repmat('单号通行',NDays,1); a = logical(mod(riqi.Day,2)); %日期是否为单数 xianxing(~a,:)= repmat('双号通行',sum(~a),1); tf = isweekend(riqi); xianxing(tf,:) = repmat('单双通行',sum(tf),1); idxdanhao=all(xianxing=='单号通行',2)|all(xianxing=='单双通行',2) idxshuanghao=all(xianxing=='双号通行',2)|all(xianxing=='单双通行',2) Ndanhao=cumsum(idxdanhao); Nshuanghao=cumsum(idxshuanghao); XianXiangData = table(N,riqi,xianxing,Ndanhao,Nshuanghao); —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | %% % 通过load('data.mat')命令载入当前目录下的数据文件，data.mat，然后进行以下处理： % data数据是一个手写数字图像的灰度数据，一共2000行785列，2000表示是2000副图片，第一列是当前图片 % 的数字是几（0-9范围，单个整数），第2列至第785列是图像的灰度数据，图片原始大小为28×28=784个像素 % 点，然后按照以下编号方式(图片未显示完全)，将784各点转换为了1行，作为了data数据中的一行 % 现在要求将data第一列提取为一个列向量，命名为trainT，另外第2列至第785列数据转换为28×28×2000的 % 三维数组，第一页为第一个数字的图像，要求按照图片中的顺序，将第一行中的灰度数据存成新的28×28的数 % 组，命名为trainD data=load('data.mat'); data1 = data.data(:,2:end); trainId = reshape(data1',28,28,2000); trainId = permute(trainId,[2 1 3]); %维数的转换 trianT = data.data(:,1); figure imshow(uint8(trainId(:,:,7))) —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 | %% % 在同一个图内绘制两个圆的曲线，一个半径为1，一个半径为2，在右方外侧中部添加图例，“小圆”和”大 % 圆“，绘图区域设置为正方形，标题设置为“两个圆”，横轴标签为x，纵轴标签为y % 双坐标轴图 % 载入数据datahis.mat， % 然后绘制datahis0.t_his为横坐标，datahis0.temp为左侧纵坐标，datahis0.hum为右侧纵坐标的双坐标轴图形 % 在图像下方外侧添加图例“温度”，“湿度”，横轴标签设置为时间，左侧y轴设置标签为温度，右侧y轴标签 % 设置为湿度 % datahis0.t_his为时间，需要转换为datetime类型，然后选取1月10日的数据进行画图 clear figure load('datahis.mat') t = datetime(datevec(datahis0.t_his)); idx=t=datetime(2017,1,10)t=datetime(2017,1,11); t1 = t(idx); temp1 = datahis0.temp(idx); hum1 = datahis0.hum(idx); yyaxis left plot(t1,temp1) xlabel('时间') ylabel('温度') yyaxis right plot(t1,hum1) ylabel('温度') legend('温度','湿度','Location','southoutside','Orientation','horizontal') —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 | %% % 多子图的绘制 % 给定以下数据 % x = 0:0.01:20; % x坐标 % y1 = 200*exp(-0.05*x).*sin(x); % Y1 % y2 = 0.8*exp(-0.5*x).*sin(10*x); % Y2 % y3 = 100*exp(-0.5*x).*sin(5*x); % Y3 % y1，y2，y3需要分别绘制一个子图，其中y1占据左侧一半位置，y2占据右侧上方，y3占据右侧下方 clear figure x= 0:0.01:20; % x坐标 y1 = 200*exp(-0.05*x).*sin(x); % Y1 y2 = 0.8*exp(-0.5*x).*sin(10*x); % Y2 y3 = 100*exp(-0.5*x).*sin(5*x); % Y3 subplot(2,2,[1 3]) plot(x,y1) subplot(2,2,2) plot(x, y2) subplot(2,2,4) plot(x,y3) —|— 1 2 3 4 5 6 7 8 9 10 | %% % 曲面图 % 绘制函数的网格图,x,y的取值范围为-2到2 figure clear x = -2:0.1:2; y=x; [X,Y]=meshgrid(x,y) Z = sin(X.^2+Y.^2); mesh(X,Y,Z) —|— 1 2 3 4 5 6 7 8 9 10 | %% % 三维饼形图 % 绘制三维饼形图，各元素所占数值为： [6 3 7 5 1 2 4]，突出显示第1，3个元素，7个元素的标签分别为 % '周一'到'周日' figure x =[6 3 7 5 1 2 4]; labels = {'周一','周二','周三','周四','周五','周六','周日'}; explode=[1 0 1 0 0 0 0] %突出显示向量x的元素 pie3(x,explode,labels) —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 | %% % 绘制如下双纵轴柱形图。数据为： % x=1:20; % y1=sin(x)+2; % y2=(x-10).^2; % 注意柱形图等图形也是可以使用双坐标轴绘制的 clear figure x=1:20; y1=sin(x)+2; y2=(x-10).^2; yyaxis left bar(x+0.2,y1,0.3,'b') %注意图形叠加位置的调整 ylabel('sin') yyaxis right bar(x-0.2,y2,0.3,'r') ylabel('x^2') legend('sin','x^2','Location','southoutside','Orientation','horizontal') title('双纵轴柱形图') xlabel('x') —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 | %% % 绘制以下数据对应曲线，并增加横轴标签及对应曲线上的数值做标记。并在图中写标记公式文本 % x=2:20; % alpha=x.^2; % beta=log(x); % y=alpha./beta; clear figure x=2:20; alpha=x.^2; beta=log(x); y=alpha./beta; plot(x,y) hold on plot(sqrt(23),sqrt(23).^2./log(sqrt(23)),'o') text(6,100,'$$\\frac{\\alpha}{\\beta}$$','Interpreter','latex');%字符的输出 xticks(sort([2:2:20 sqrt(23)])) h1=gca; h1.XTickLabel{3}='$$\\sqrt{23}$$'; %字符的输出 h1.TickLabelInterpreter='latex'; —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | %% % 绘制如下图形 % t1 = datetime(2014,1:12,1); % temp = [0 2 12 11 15 25 23 27 25 24 12 8]; clear figure t1 = datetime(2014,1:12,1); temp = [0 2 12 11 15 25 23 27 25 24 12 8]; h = plot(t1,temp,':*'); ax = h.Parent; title('A Year of Temperatures on the 1st of the Month') ylabel('Degrees Celcius ^{\\circ}') % 上角标的使用，摄氏度符号 yt1 = ax.YTickLabel %坐标轴标签为元胞数组，在此基础上进行修改 ytld = strcat(yt1,'^{\\circ}') %上角标 ax.YTickLabel = ytld; —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 | %% % 按照要求绘图 % t=(pi*(0:1000)/1000)'; % y=sin(t); % 以t为横轴数据，y为纵轴数据绘制图形，要求： % 1，将纵坐标轴设置在右方， % 2，将横轴标签设置为[0 1/4*pi 1/2*pi 3/4*pi pi]，坐标轴范围设置为0～pi % 3，输出图形的figure窗口位置坐标 % 4，输出图形axes的位置坐标 clear figure t=(pi*(0:1000)/1000)'; y=sin(t); h = plot(t,y) set(gca,'YAxisLocation','right') xticks([0 1/4*pi 1/2*pi 3/4*pi pi]) xticklabels({'0','1/4 \\pi','1/2 \\pi','3/4 \\pi','\\pi'}) xlim([0 pi]) —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 | %% % 按照以下位置绘制图形 % 其中需要用到的数据如下，另外图片坐标的位置与示例图中大致相等即可 % % 图1 % x=linspace(0.2*pi,20); % y=sin(x); % % 图2 % t=0:pi/100:20*pi; % x=sin(t); % y=cos(t); % z=t.*sin(t).*cos(t); % % 图3 % [x,y]=meshgrid(-8:0.5:8); % z=sin(sqrt(x.^2+y.^2))./sqrt(x.^2+y.^2+eps); clear figure % 图1 x=linspace(0.2*pi,20); y=sin(x); axes('Position',[0.6,0.2,0.2,0.7],'GridLineStyle','-'); plot(y,x); grid on % 图2 axes('Position',[0.1,0.2,0.5,0.5]); t=0:pi/100:20*pi; x=sin(t); y=cos(t); z=t.*sin(t).*cos(t); plot3(x,y,z); % 图3 axes('Position',[0.1,0.6,0.25,0.3]); [x,y]=meshgrid(-8:0.5:8); z=sin(sqrt(x.^2+y.^2))./sqrt(x.^2+y.^2+eps); mesh(x,y,z) —|— 1 2 3 4 5 | %% % 以下数据进行拟合 % load ex1.mat % cftool load ex1.mat —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | %% % 使用命令导入文件2016010600.txt中的数据，提取第4列日期数据和第5列时刻数据数据，合并生成一个 % datetime数组，命名为T clear a = readtable('2016010600.txt'); t1 = a.Var4; t2 = a.Var5; TPvec_y = floor(t1/10000); TPvec_mon = floor((t1-TPvec_y*10000)/100); TPvec_d = mod(t1,100) TPvec_h = floor(t2/100); TPvec_min=mod(t2,100); TPvec_s = zeros(size(t1,1),1); t_vec=[TPvec_y TPvec_mon TPvec_d TPvec_h TPvec_min TPvec_s]; T = datetime(t_vec) —|— 1 2 3 4 5 6 7 8 9 10 11 12 | %% % 生成与2016010600.txt文件中第6列数据概率分布相同的随机数，数量10000个. clear figure a = readtable('2016010600.txt') v = a.Var6; histogram(v,20) [N,edges] = histcounts(v,20); rm = RDrnd(N,10000); figure histogram(rm,20) —|—","tags":["OpenCV"],"categories":["工具"]},{"title":"Attention机制中SEnet CBAM以及Dual pooling的pytorch实现","path":"/2019/12/27/20191227-SENet-code/","content":"本来自己写了，关于SENet的注意力截止，但是在准备写其他注意力机制代码的时候，看到一篇文章总结的很好，所以对此篇文章进行搬运，以供自己查阅，并加上自己的理解。 [TOC] 1.SENET中的channel-wise加权的实现实现代码参考自：senet.pytorch代码如下：SEnet 模块 from torch import nnclass SELayer(nn.Module): def __init__(self, channel, reduction=16): super(SELayer, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.fc = nn.Sequential( nn.Linear(channel, channel // reduction, bias=False), nn.ReLU(inplace=True), nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid() ) def forward(self, x): b, c, _, _ = x.size() y = self.avg_pool(x).view(b, c) y = self.fc(y).view(b, c, 1, 1) return x * y. (x) 以上代码设计到的API： AdaptiveAvgPool2d: 自适应平均池化，参数为（n,m）则将原来的feature（w,h）通过pooling得到（n,m）的feature，如果是（n）,则将原来的feature从（w,h）通过pooling得到（n,n） Sequential: torch容器，存放网络层等内容。 Linear: 线性层，参数为（in, out）,将原有的in个feature转为out个feature ReLU: 激活层， inplace进行原地操作，节省内存 Sigmoid: 激活层，将输入压缩到0-1分析forward进行模型的构建： x是输入的feature,一般各个通道意义如下：（batch size，channel, width , height）,这里获取了batch(b), channel x通过AdaptiveAvgPool2d(1)以后将得到（batch size, channel, 1, 1）, 然后view（b,c）意思是按照b,c进行展开 ```pythonIn [1]: import torchIn [2]: x = torch.zeros((16,256,256,256))In [3]: import torch.nn as nnIn [4]: avg_pool = nn.AdaptiveAvgPool2d(1)In [5]: avg_pool(x).shapeOut[5]: torch.Size([16, 256, 1, 1])In [6]: avg_pool(x).view((16,256)).shapeOut[6]: torch.Size([16, 256])In [7]: avg_pool(x).squeeze().shape # squeeze()函数也可以将所有通道个数为1的进行挤压Out[7]: torch.Size([16, 256]) 然后形状为【16, 256】的tensor经过fc: (1) Linear: from 256(channel) to 256/16 (2) ReLu：进行一次激活函数 (3) Linear: from 256/16 to 256(channel) (4) Sigmoid: 激活到0-1，代表每个通道的重要性 然后通过view操作转化为【16,256,1,1】形状的tensor 现在y得到的是每一个通道对应的分数（0-1），然后需要将其与通道内容相乘，具体操作使用到了tensor的内置函数expand_as(把一个tensor变成和函数括号内一样形状的tensor，用法与expand类似，相当于expand(tensor.size()) x是【16,256,256,256】形状的特征图，y是【16,256,1,1】大小的channel-wise分数，然后需要将其相乘 b.expand_as(a)就是将b进行扩充，扩充到a的维度，需要说明的是a的低维度需要比b大，例如b的shape是31，如果a的shape是32不会出错，但是是2*2就会报错了。就是必须有一个维度是1，然后用于扩展：123In [8]: tensor1 = torch.ones((3,4,1,1))In [9]: tensor1.expand([3,4,5,5]).shapeOut[9]: torch.Size([3, 4, 5, 5]) 这样通过expand_as就能得到【16,256,256,256】大小的tensor，其中256*256都是对应通道的1分数，然后与原先的feature相乘，就能得到channel-wise分数计算后的feature。在resetnet中的block插入senet模块 ```python class CifarSEBasicBlock(nn.Module): def __init__(self, inplanes, planes, stride=1, reduction=16): super(CifarSEBasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = nn.BatchNorm2d(planes) self.se = SELayer(planes, reduction) if inplanes != planes: self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes)) else: self.downsample = lambda x: x self.stride = stride def forward(self, x): residual = self.downsample(x) out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.se(out) out += residual out = self.relu(out) return out 正常的resent的BasicBlock class BasicBlock(nn.Module): def __init__(self, inplanes, planes, stride=1): super(BasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = nn.BatchNorm2d(planes) if inplanes != planes: self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes)) else: self.downsample = lambda x: x self.stride = stride baseline:0.888se+baseline:0.892 2.CBAM中的通道注意力机制channel-attention-module跟以上内容想法有一点像，给每个channel进行打分，具体实现如下：参考来源：CBMA.pytorch class ChannelAttention(nn.Module): def __init__(self, in_planes, ratio=16): super(ChannelAttention, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.max_pool = nn.AdaptiveMaxPool2d(1) self.fc1 = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False) self.relu1 = nn.ReLU() self.fc2 = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False) self.sigmoid = nn.Sigmoid() def forward(self, x): avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x)))) max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x)))) out = avg_out + max_out return self.sigmoid(out) API跟上边类似，只添加了卷积，也很简单。需要说明的是貌似Linear和Conv2d中的参数很相似，但是实际上，两者还是很不一样的，Linear接受的是线性的2维数组（batch, 一维特征），Con2d接受的是4维数组（batch, 通道，w, h）。forward函数： 第一行，进行了adaptiveAvgPooling， conv2d, relu, conv2d 第二行，进行了AdaptiveMaxPooling, conv2d, relu, conv2d 第三行，将两个向量进行相加 第四行，将对应结果进行激活，得到通道注意力分数 3.CBAM中的空间注意力机制参考来源：CBMA.pytorch class SpatialAttention(nn.Module):def __init__(self, kernel_size=7):super(SpatialAttention, self).__init__()assert kernel_size in (3, 7), 'kernel size must be 3 or 7'padding = 3 if kernel_size == 7 else 1self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)self.sigmoid = nn.Sigmoid()def forward(self, x):avg_out = torch.mean(x, dim=1, keepdim=True)max_out, _ = torch.max(x, dim=1, keepdim=True)x = torch.cat([avg_out, max_out], dim=1)x = self.conv1(x)return self.sigmoid(x) Spatial attention module中支持kernel_size=3或者7，默认设置为7。以上涉及到的API: torch.mean: 求平均值，dim指的是沿着某一个通道进行计算平均值。这里dim=1，说明沿着通道channel进行平均，对所有channel的feature上相应的像素进行求平均值。 torch.max: 同上，进行求最大值。forward函数： 第一行：沿着通道维度进行进行平均，得到一个（batch, 1, w, h）的feature 第二行：沿着通道维度进行求最大值，得到一个（batch, 1, w, h）的feature 第三行：将两个feature通过cat的方式拼接起来，得到一个（batch, 2, w, h）的feature 第四行：对这个feature进行卷积之所以设置如果kernel_size=7的时候padding=3是因为需要将out_feature和in_feature相等，可以带入公式进行计算。 第五行：进行激活，将得分约束至[0-1] 4.CBAM中的融合参考代码：CBMA.pytorch在resnet中主要是用在basicBlock中，代码如下： class BasicBlock(nn.Module): expansion = 1 def __init__(self, inplanes, planes, stride=1, downsample=None): super(BasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = nn.BatchNorm2d(planes) self.ca = ChannelAttention(planes) self.sa = SpatialAttention() self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.ca(out) * out # 广播机制 out = self.sa(out) * out # 广播机制 if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return out resnet50+cbam: 0.902 5.dual pooling的pytorch实现max pooling更注重重要的局部特征, average pooling更关注全局特征.两者concat可以丰富特征层.参考链接:GaryLIU class res18(nn.Module): def __init__(self, num_classes): super(res18, self).__init__() self.base = resnet18(pretrained=True) self.feature = nn.Sequential( self.base.conv1, self.base.bn1, self.base.relu, self.base.maxpool, self.base.layer1, self.base.layer2, self.base.layer3, self.base.layer4 ) self.avg_pool = nn.AdaptiveAvgPool2d(1) self.max_pool = nn.AdaptiveMaxPool2d(1) self.reduce_layer = nn.Conv2d(1024, 512, 1) self.fc = nn.Sequential( nn.Dropout(0.5), nn.Linear(512, num_classes) ) def forward(self, x): bs = x.shape[0] x = self.feature(x) x1 = self.avg_pool(x) x2 = self.max_pool(x) x = torch.cat([x1, x2], dim=1) x = self.reduce_layer(x).view(bs, -1) logits = self.fc(x) return logits 这种是在模型层进行改造的一种小trick了，常见的做法：global max/average pooling + fc layer，这里试concat(global max-pooling, global average pooling) + fc layer，其实就是为了丰富特征层，max pooling更加关注重要的局部特征，而average pooling试更加关注全局的特征。不一定有效，我试过不少次，有效的次数比较少，但不少人喜欢这样用.-gray以上就是dual pooling的实现，具体分析如下： 第一行：得到batch-size 第二行：得到feature, gray大佬这里用的是一个sequential将所有的模块装载进来，其实也可以用这种方法： 1 | self.base_model = nn.Sequential(*list(model_ft.children())[:-3]) # 取除了后三个全部的层 —|— children方法里就是返回当前模型子模块的迭代器，可以查看源代码，然后选择将其中一部分去掉，比如fc层等，也可以使用gray大佬的这种方法。查找的过程中找到一个中间层可视化的简单代码：https://www.jianshu.com/p/0a23db1df55a 第四，五行，通过avg_pool,max_pool得到对应的feature 第六行，进行concate操作，进行拼接 第七行，使用了一个卷积层进行降维通道，并进行view展开成一维向量。 第八层，进行全连接层的分类。参考链接：https://blog.csdn.net/DD_PP_JJ/article/details/103318617","tags":["深度学习"],"categories":["深度学习"]},{"title":"SpatialSense","path":"/2019/12/27/20191227-SpatialSense/","content":"摘要理解图像中物体之间的空间关系是一项具有惊人挑战性的任务(图1)。椅子可能“在”一个人的“后面”，即使它出现在人的左边(取决于人面对的方向)。如果有第三个学生在他们之间，那么两个看起来很近的学生实际上可能并不“挨着”。我们介绍了spatial alsense，这是一个专门研究空间关系识别的数据集，它捕捉了广泛的此类挑战，允许对计算机视觉技术进行适当的基准测试。空间感知是通过对抗性的众包来构建的，在众包中，人类注释者的任务是发现空间关系，这些关系很难用简单的线索来预测，比如二维空间结构或语言先验。与现有的数据集相比，对抗性众包大大减少了数据集的偏倚，并在长尾抽取了更有趣的关系样本。在空间感方面，最先进的识别模型与简单的基线相比，表现得更为出色，这表明它们依赖于直接的线索，而不是对这个复杂的任务进行充分的推理。空间感觉基准测试为提高计算机视觉系统的空间推理能力提供了一条途径。 Introduction","tags":["计算机视觉"],"categories":["计算机视觉"]},{"title":"Attention机制中SEnet CBAM以及Dual pooling的pytorch实现","path":"/2019/12/27/SENet-code/","content":"本来自己写了，关于SENet的注意力截止，但是在准备写其他注意力机制代码的时候，看到一篇文章总结的很好，所以对此篇文章进行搬运，以供自己查阅，并加上自己的理解。 [TOC] 1.SENET中的channel-wise加权的实现实现代码参考自：senet.pytorch代码如下：SEnet 模块 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | from torch import nn class SELayer(nn.Module): def __init__(self, channel, reduction=16): super(SELayer, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.fc = nn.Sequential( nn.Linear(channel, channel // reduction, bias=False), nn.ReLU(inplace=True), nn.Linear(channel // reduction, channel, bias=False), nn.Sigmoid() ) def forward(self, x): b, c, _, _ = x.size() y = self.avg_pool(x).view(b, c) y = self.fc(y).view(b, c, 1, 1) return x * y. (x) —|— 以上代码设计到的API： AdaptiveAvgPool2d: 自适应平均池化，参数为（n,m）则将原来的feature（w,h）通过pooling得到（n,m）的feature，如果是（n）,则将原来的feature从（w,h）通过pooling得到（n,n） Sequential: torch容器，存放网络层等内容。 Linear: 线性层，参数为（in, out）,将原有的in个feature转为out个feature ReLU: 激活层， inplace进行原地操作，节省内存 Sigmoid: 激活层，将输入压缩到0-1分析forward进行模型的构建： x是输入的feature,一般各个通道意义如下：（batch size，channel, width , height）,这里获取了batch(b), channel x通过AdaptiveAvgPool2d(1)以后将得到（batch size, channel, 1, 1）, 然后view（b,c）意思是按照b,c进行展开 1 2 3 4 5 6 7 8 9 10 | In [1]: import torch In [2]: x = torch.zeros((16,256,256,256)) In [3]: import torch.nn as nn In [4]: avg_pool = nn.AdaptiveAvgPool2d(1) In [5]: avg_pool(x).shape Out[5]: torch.Size([16, 256, 1, 1]) In [6]: avg_pool(x).view((16,256)).shape Out[6]: torch.Size([16, 256]) In [7]: avg_pool(x).squeeze().shape # squeeze()函数也可以将所有通道个数为1的进行挤压 Out[7]: torch.Size([16, 256]) —|— 然后形状为【16, 256】的tensor经过fc: (1) Linear: from 256(channel) to 256/16 (2) ReLu：进行一次激活函数 (3) Linear: from 256/16 to 256(channel) (4) Sigmoid: 激活到0-1，代表每个通道的重要性 然后通过view操作转化为【16,256,1,1】形状的tensor 现在y得到的是每一个通道对应的分数（0-1），然后需要将其与通道内容相乘，具体操作使用到了tensor的内置函数expand_as(把一个tensor变成和函数括号内一样形状的tensor，用法与expand类似，相当于expand(tensor.size()) x是【16,256,256,256】形状的特征图，y是【16,256,1,1】大小的channel-wise分数，然后需要将其相乘 b.expand_as(a)就是将b进行扩充，扩充到a的维度，需要说明的是a的低维度需要比b大，例如b的shape是31，如果a的shape是32不会出错，但是是2*2就会报错了。就是必须有一个维度是1，然后用于扩展： 1 2 3 | In [8]: tensor1 = torch.ones((3,4,1,1)) In [9]: tensor1.expand([3,4,5,5]).shape Out[9]: torch.Size([3, 4, 5, 5]) —|— 这样通过expand_as就能得到【16,256,256,256】大小的tensor，其中256*256都是对应通道的1分数，然后与原先的feature相乘，就能得到channel-wise分数计算后的feature。 在resetnet中的block插入senet模块 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 | class CifarSEBasicBlock(nn.Module): def __init__(self, inplanes, planes, stride=1, reduction=16): super(CifarSEBasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = nn.BatchNorm2d(planes) self.se = SELayer(planes, reduction) if inplanes != planes: self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes)) else: self.downsample = lambda x: x self.stride = stride def forward(self, x): residual = self.downsample(x) out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.se(out) out += residual out = self.relu(out) return out —|— 正常的resent的BasicBlock 1 2 3 4 5 6 7 8 9 10 11 12 13 14 | class BasicBlock(nn.Module): def __init__(self, inplanes, planes, stride=1): super(BasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = nn.BatchNorm2d(planes) if inplanes != planes: self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False), nn.BatchNorm2d(planes)) else: self.downsample = lambda x: x self.stride = stride —|— baseline:0.888se+baseline:0.892 2.CBAM中的通道注意力机制channel-attention-module跟以上内容想法有一点像，给每个channel进行打分，具体实现如下：参考来源：CBMA.pytorch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | class ChannelAttention(nn.Module): def __init__(self, in_planes, ratio=16): super(ChannelAttention, self).__init__() self.avg_pool = nn.AdaptiveAvgPool2d(1) self.max_pool = nn.AdaptiveMaxPool2d(1) self.fc1 = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False) self.relu1 = nn.ReLU() self.fc2 = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False) self.sigmoid = nn.Sigmoid() def forward(self, x): avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x)))) max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x)))) out = avg_out + max_out return self.sigmoid(out) —|— API跟上边类似，只添加了卷积，也很简单。需要说明的是貌似Linear和Conv2d中的参数很相似，但是实际上，两者还是很不一样的，Linear接受的是线性的2维数组（batch, 一维特征），Con2d接受的是4维数组（batch, 通道，w, h）。forward函数： 第一行，进行了adaptiveAvgPooling， conv2d, relu, conv2d 第二行，进行了AdaptiveMaxPooling, conv2d, relu, conv2d 第三行，将两个向量进行相加 第四行，将对应结果进行激活，得到通道注意力分数 3.CBAM中的空间注意力机制参考来源：CBMA.pytorch 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 | class SpatialAttention(nn.Module): def __init__(self, kernel_size=7): super(SpatialAttention, self).__init__() assert kernel_size in (3, 7), 'kernel size must be 3 or 7' padding = 3 if kernel_size == 7 else 1 self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False) self.sigmoid = nn.Sigmoid() def forward(self, x): avg_out = torch.mean(x, dim=1, keepdim=True) max_out, _ = torch.max(x, dim=1, keepdim=True) x = torch.cat([avg_out, max_out], dim=1) x = self.conv1(x) return self.sigmoid(x) —|— Spatial attention module中支持kernel_size=3或者7，默认设置为7。以上涉及到的API: torch.mean: 求平均值，dim指的是沿着某一个通道进行计算平均值。这里dim=1，说明沿着通道channel进行平均，对所有channel的feature上相应的像素进行求平均值。 torch.max: 同上，进行求最大值。forward函数： 第一行：沿着通道维度进行进行平均，得到一个（batch, 1, w, h）的feature 第二行：沿着通道维度进行求最大值，得到一个（batch, 1, w, h）的feature 第三行：将两个feature通过cat的方式拼接起来，得到一个（batch, 2, w, h）的feature 第四行：对这个feature进行卷积之所以设置如果kernel_size=7的时候padding=3是因为需要将out_feature和in_feature相等，可以带入公式进行计算。 第五行：进行激活，将得分约束至[0-1] 4.CBAM中的融合参考代码：CBMA.pytorch在resnet中主要是用在basicBlock中，代码如下： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 | class BasicBlock(nn.Module): expansion = 1 def __init__(self, inplanes, planes, stride=1, downsample=None): super(BasicBlock, self).__init__() self.conv1 = conv3x3(inplanes, planes, stride) self.bn1 = nn.BatchNorm2d(planes) self.relu = nn.ReLU(inplace=True) self.conv2 = conv3x3(planes, planes) self.bn2 = nn.BatchNorm2d(planes) self.ca = ChannelAttention(planes) self.sa = SpatialAttention() self.downsample = downsample self.stride = stride def forward(self, x): residual = x out = self.conv1(x) out = self.bn1(out) out = self.relu(out) out = self.conv2(out) out = self.bn2(out) out = self.ca(out) * out # 广播机制 out = self.sa(out) * out # 广播机制 if self.downsample is not None: residual = self.downsample(x) out += residual out = self.relu(out) return out —|— resnet50+cbam: 0.902 5.dual pooling的pytorch实现max pooling更注重重要的局部特征, average pooling更关注全局特征.两者concat可以丰富特征层.参考链接:GaryLIU 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 | class res18(nn.Module): def __init__(self, num_classes): super(res18, self).__init__() self.base = resnet18(pretrained=True) self.feature = nn.Sequential( self.base.conv1, self.base.bn1, self.base.relu, self.base.maxpool, self.base.layer1, self.base.layer2, self.base.layer3, self.base.layer4 ) self.avg_pool = nn.AdaptiveAvgPool2d(1) self.max_pool = nn.AdaptiveMaxPool2d(1) self.reduce_layer = nn.Conv2d(1024, 512, 1) self.fc = nn.Sequential( nn.Dropout(0.5), nn.Linear(512, num_classes) ) def forward(self, x): bs = x.shape[0] x = self.feature(x) x1 = self.avg_pool(x) x2 = self.max_pool(x) x = torch.cat([x1, x2], dim=1) x = self.reduce_layer(x).view(bs, -1) logits = self.fc(x) return logits —|— 这种是在模型层进行改造的一种小trick了，常见的做法：global max/average pooling + fc layer，这里试concat(global max-pooling, global average pooling) + fc layer，其实就是为了丰富特征层，max pooling更加关注重要的局部特征，而average pooling试更加关注全局的特征。不一定有效，我试过不少次，有效的次数比较少，但不少人喜欢这样用.-gray以上就是dual pooling的实现，具体分析如下： 第一行：得到batch-size 第二行：得到feature, gray大佬这里用的是一个sequential将所有的模块装载进来，其实也可以用这种方法： 1 | self.base_model = nn.Sequential(*list(model_ft.children())[:-3]) # 取除了后三个全部的层 —|— children方法里就是返回当前模型子模块的迭代器，可以查看源代码，然后选择将其中一部分去掉，比如fc层等，也可以使用gray大佬的这种方法。查找的过程中找到一个中间层可视化的简单代码：https://www.jianshu.com/p/0a23db1df55a 第四，五行，通过avg_pool,max_pool得到对应的feature 第六行，进行concate操作，进行拼接 第七行，使用了一个卷积层进行降维通道，并进行view展开成一维向量。 第八层，进行全连接层的分类。参考链接：https://blog.csdn.net/DD_PP_JJ/article/details/103318617","tags":["PyTorch"],"categories":["深度学习"]},{"title":"SpatialSense","path":"/2019/12/27/SpatialSense/","content":"摘要理解图像中物体之间的空间关系是一项具有惊人挑战性的任务(图1)。椅子可能“在”一个人的“后面”，即使它出现在人的左边(取决于人面对的方向)。如果有第三个学生在他们之间，那么两个看起来很近的学生实际上可能并不“挨着”。我们介绍了spatial alsense，这是一个专门研究空间关系识别的数据集，它捕捉了广泛的此类挑战，允许对计算机视觉技术进行适当的基准测试。空间感知是通过对抗性的众包来构建的，在众包中，人类注释者的任务是发现空间关系，这些关系很难用简单的线索来预测，比如二维空间结构或语言先验。与现有的数据集相比，对抗性众包大大减少了数据集的偏倚，并在长尾抽取了更有趣的关系样本。在空间感方面，最先进的识别模型与简单的基线相比，表现得更为出色，这表明它们依赖于直接的线索，而不是对这个复杂的任务进行充分的推理。空间感觉基准测试为提高计算机视觉系统的空间推理能力提供了一条途径。 Introduction","tags":["OpenCV"],"categories":["其他"]},{"title":"ADVERSARIALAL_AUTOAUGMENT","path":"/2019/12/26/20191226-ADVERSARIALAL-AUTOAUGMENT/","content":"数据增广（DA，data augmentation）已被广泛用于改善训练深度神经网络的泛化性。最近，人为设计的数据增广已逐渐被自动学习的增广策略所取代。通过在精心设计的数据增广搜索空间中找到最佳策略，AutoAugment可以显著提高图像分类任务的验证准确性。但是，这种方法在大规模问题上在计算上并不实用。在本文中，我们开发了一种对抗方法，以得出一种计算上可行的解决方案，称为Adversarial AutoAugment（对抗自动增广），可以同时优化目标相关对象和增广策略搜索损失。增广策略网络试图通过生成对抗性增广策略来增加目标网络的训练损失，而目标网络可以从较难的示例中学习更强大的功能，以提高通用性。与先前的工作相反，我们在目标网络训练中重新使用计算以进行策略评估，而无需对目标网络进行再训练。与AutoAugment相比，这使ImageNet的计算成本降低了约12倍，时间开销缩短了11倍。我们在ImageNet上显示了我们在CIFAR-10 / CIFAR-100上的方法的实验结果，并展示了相对于最新技术的显著性能改进。在CIFAR-10上，我们实现了top-1测试误差为1.36％，这是目前表现最佳的单一模型。在ImageNet上，在没有额外数据的情况下，我们在ResNet-50上达到了top-1精度的领先性能，在ResNet-50-D上达到了80.00％。","tags":["深度学习"],"categories":["深度学习"]},{"title":"ADVERSARIALAL_AUTOAUGMENT","path":"/2019/12/26/ADVERSARIALAL-AUTOAUGMENT/","content":"数据增广（DA，data augmentation）已被广泛用于改善训练深度神经网络的泛化性。最近，人为设计的数据增广已逐渐被自动学习的增广策略所取代。通过在精心设计的数据增广搜索空间中找到最佳策略，AutoAugment可以显著提高图像分类任务的验证准确性。但是，这种方法在大规模问题上在计算上并不实用。在本文中，我们开发了一种对抗方法，以得出一种计算上可行的解决方案，称为Adversarial AutoAugment（对抗自动增广），可以同时优化目标相关对象和增广策略搜索损失。增广策略网络试图通过生成对抗性增广策略来增加目标网络的训练损失，而目标网络可以从较难的示例中学习更强大的功能，以提高通用性。与先前的工作相反，我们在目标网络训练中重新使用计算以进行策略评估，而无需对目标网络进行再训练。与AutoAugment相比，这使ImageNet的计算成本降低了约12倍，时间开销缩短了11倍。我们在ImageNet上显示了我们在CIFAR-10 / CIFAR-100上的方法的实验结果，并展示了相对于最新技术的显著性能改进。在CIFAR-10上，我们实现了top-1测试误差为1.36％，这是目前表现最佳的单一模型。在ImageNet上，在没有额外数据的情况下，我们在ResNet-50上达到了top-1精度的领先性能，在ResNet-50-D上达到了80.00％。","tags":["OpenCV"],"categories":["深度学习"]},{"title":"Semantic_segmentation_overview","path":"/2019/12/24/20191224-Semantic-segmentation-overview/","content":"语义分割是近年来出现的基本问题之一，因此成为计算机视觉和机器学习领域的热门话题。 通用语义分割图像集 PASCAL Visual Object Classes (VOC) Common Objects in Context (COCO)With 200K labelled images, 1.5 million object instances, and 80object categories Other General Purpose Semantic Segmentation Image Sets YouTube-Objects SIFT-flow Urban Street Semantic Segmentation Image Sets Cityscapes CamVid KITTI SYNTHIA Before Fully Convolutional NetworksPre-Deep Learning Approaches传统图像分割与语义分割的区别在于语义特征在图像分割过程中的应用。传统的图像分割方法，如阈值、聚类和区域增长等(有关传统图像分割技术的调查，请参阅[29])使用手工制作的低级特征(即在图像中定位物体的边界。因此，在需要图像语义信息进行像素级分割的情况下，例如在相似物体相互遮挡的情况下，这些方法是必要的。关于深度CNNs流行之前的语义分割工作，有多种方法[30,31，32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]使用的图形模型，如马尔可夫随机域(MRF)，条件随机场(CRF)或基于森林(有时被称为“整体”)的方法，以便在像素级找到场景标签。其主要思想是通过观察相邻像素之间的依赖关系来找到一个推论。换句话说，这些方法将图像的语义建模为相邻像素之间的一种“先验”信息。另一组研究，有时被称为“分层模型”[44,45,46]，使用一个预先训练的和独立的对象探测器的组合，以便从图像中提取语义信息。由于单个的对象检测器未能正确地对区域进行分类，或者由于这些方法受到“手工选择”的检测器库所提供的对象类的有限数量的限制，因此与当今最先进的方法相比，它们的性能相对较低。虽然上述的前深度学习时代的方法不再是首选的分割方法，一些图形模型，特别是CRFs，目前正在使用的最先进的方法作为后处理(细化)层，目的是提高语义分割的性能，具体细节将在下一节中讨论 Refinement Methods深度神经网络具有很强的局部特征提取能力。然而，它们缺乏利用全局上下文信息的能力，因此无法对相邻像素预测之间的交互进行建模。另一方面，前深度学习时代流行的分割方法，图形模型，非常适合这类任务。这就是为什么它们目前被用作许多深度基于cnn的语义分割架构的细化层。正如在前一节中提到的，使用图形模型进行分割背后的思想是通过观察相邻像素之间的低层次关系来寻找一个推论。在图2中，可以看到使用基于图形模型的细分对分割结果的影响。分类器(见图2.b)不能正确分割不同类标签相邻的像素。在本例中，我们使用基于crf的细分[42]来改进像素级的分割结果。基于crf的方法被广泛用于深度语义分割方法的细化CRFs[50]是一种有区别的无向概率图形模型。它们被用来对观测之间已知的关系进行编码，并构建一致的解释。它们用作细化层的原因是，与不考虑相邻像素相似性的离散分类器不同，CRF可以利用这些信息。与其他图形化模型(如隐马尔科夫模型)相比，CRFs的主要优点是它们的条件性质和避免标签偏差[50]问题的能力。尽管有相当数量的方法(见表1)使用CRFs进行细化，但这些模型在相对较新的方法中开始变得不受欢迎，因为它们的速度非常慢，而且非常难以优化。 Early Deep Learning ApproachesFCN在2014年出现，使用tanh 相较于proposal of a ReLU layer 很难去区分，因此，训练这样的系统被认为是不适合计算的，甚至对大规模数据是不可行的。然而，第一个成熟的方法只是简单地尝试转换分类网络，如AlexNet和VGG通过微调全连接层来细分网络。他们在训练阶段遭受了过度拟合和完全连接层的时间限制。此外，使用的CNNs不够深，无法创建抽象的特征，这与图像的语义有关。在一些早期的深度学习研究中，研究人员拒绝使用完全连接的层来进行决策，而是使用不同的结构，如周期性的架构[57]或使用来自一个单独计算的分段家族的标记。通过提出全连接层FCN这样的结构的必要性的第一个迹象，不出所料，它们被FCN取代。由于他们的分割结果被认为是不令人满意的，这些研究通常使用一个细化的过程，要么作为一个后处理层[52,53,54,56]，或作为一个替代架构，以完全连接的决策层Refinement methods varied such as Markov random fields，nearest neighbour-based approach，使用校准层[54]，使用超级像素[55,56]，或普通CNNs的递归网络。细化层仍然被后fcn方法所使用，其目的是提高类交叉区域的像素级标记性能。tips: 4FCN [11] ] was officially published in 2017. However the same group first shared the idea online as pre-printed literature in 014 [51]. Fully Convolutional Networks for Semantic SegmentationFCN(2017) 提出了从CNNs (DCNN)中拆除全连通层的设想.‘FCN-32s’, ‘FCN16s’, and ‘FCN8s’ all transfer-learnt using the VGG architectureFCN架构在很多方面都被认为是革命性的, FCN不包括全连接层 该结构允许为任何分辨率的图像生成分割图。使用反卷积层，可以将粗深卷积层输出提升到任意分辨率的稠密像素。 提出了DCNNs的skip架构。跳过架构(或连接)在DCNNs中提供不相邻层之间的链接。仅仅通过对未连接层的输出进行求和或连接，这些连接就可以使信息流动，否则，由于体系结构的选择(如最大池化层或辍学)，这些信息就会丢失。最常见的做法是在max-pooling层之前使用skip连接，它通过选择特定区域的最大值对层输出进行采样。池化层有助于架构创建特性层次，但也会导致局部信息的丢失，而这些局部信息对于语义分割是有价值的，特别是在对象边界。跳过连接通过绕过池化层来保存这些信息并将其转发到更深层。实际上，在[11]中使用跳转连接被认为是相当原始的。“FCN-8s”和“FCN-16s”网络在不同的层包含这些跳过连接。对于相同的架构，即“FCN-4s”和“更密集的跳过连接”。“FCN-2s”也被用于各种应用[61,62]。这一思想最终演变为用于语义分割的编码器-解码器结构[63,27]，下文将对此进行介绍。 Post-FCN Approachesdrawbacks of FCNs： 特性层次结构中标签本地化的低效丢失、无法处理全局上下文知识以及缺乏多尺度处理机制。我们还讨论了语义分割上下文中的尺度不变性，最后讨论了基于对象检测的方法，这是一种新的解决方案，旨在解决同时检测对象实例的语义分割问题。 Techniques for Fine-grained Localisation根据定义，语义分割是一个密集的过程，因此它需要在像素级对类标签进行细粒度的本地化。例如，在机器人手术中，语义分割中的像素错误可能会导致生存或死亡的情况。层次特性创建的池(即。，最大池)层可以部分失去本地化。此外，由于他们FCNs完全是卷积性质的，它本身并不具备在图像中对全局上下文信息建模的能力，这在类标签本地化方面也非常有效。因此，这两个问题在本质上和本质上是相互交织的下面我们将讨论旨在克服这些问题和提供更好的本地化的不同方法类的标签。 Encoder-Decoder Architecture编译码器， 类似U-net，具有开创新的研究。编码器使用池化层逐渐缩减输入数据的空间维度，而解码器通过反卷积层等网络层逐步恢复目标的细节和相应的空间维度。从编码器到解码器之间，通常存在直接的信息连接，来帮助解码器更好地恢复目标细节。U-Net，Seg-Net 都是非常出名的网络。在这种结构中，由编码器部分相邻的低分辨率特征映射提供的强相关语义信息必须经过额外的中间层才能到达相同的译码层。这通常会导致一定程度的信息衰减。然而，U-Net架构已经被证明对于不同应用的分割非常有用，例如卫星图像。 Spatial Pyramid Pooling(空间金字塔池化)Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. 在2006年首次被提出， 为的是解决单词袋系统失去了特征之间的空间关系。首次应用在深度学习是2015年的SPPNet这篇文章。无论输入大小如何，都可以在空间金字塔汇聚网络中创建深度特征的空间金字塔表示。SPP-Net最重要的贡献是它允许输入不同大小的数据。不同大小的图像输入到卷积层中，不可避免地会产生不同大小的特征图。然而,如果池化层刚好在决策层之前，具有与输入大小成比例的步长值，则创建特征映射这一层将被固定。CNN中的SPP层构建了不同层次特征之间的关系。因此，它与ED结构中的跳过连接非常相似，后者也允许特性层次结构之间的信息流。：SPP层用于语义分割最常见的用法是在[67]中提出的，比如SPP层被附加到最后一个卷积层，并反馈给像素级分类器。 Feature Concatenation(特征连接)这个想法是基于融合从不同来源提取的特征。 Dilated Convolution(扩张卷积，空洞卷积)扩展卷积的思想实际上很简单:使用连续的卷积滤波器，一个有效的接收域只能随层线性增长;然而，如果使用在滤波器中有间隙的膨胀卷积(见图4.c)，有效接受域将增长得更快[70]。因此，在没有池或子采样的情况下，创建了卷积层的矩形棱镜。扩张卷积是一种非常有效和强大的方法来详细保存特征图分辨率。缺点在于对GPU存储和计算的要求更高，因为特征图分辨率不会在特征层次结构中缩小。 Conditional Random Fields(条件随机场)cnn自然缺乏特别的‘关注’类交叉区域的机制。在这些区域周围，通过观察CNN层的相邻feature maps之间的低层关系，使用图形化模型进行推理。因此，图形模型(主要是crf)被用作深度语义分割架构的细化层。与在[72]中一样，CRFs将低级交互与来自多类交互的输出连接起来，并以这种方式构建全局上下文知识。CRFs作为一种细化层，目前存在多种利用CRFs对CNNs进行深度处理的方法，如卷积CRFs[47]、稠密CRF[42]、CRN-as-RNN等[73]。尽管CRFs有助于构建上下文知识，从而在类标签中更好地本地化，表1显示了在“CRF模型”选项卡下分类的CRFs，以便将它们与实际的CNN架构扩展区分开。 Recurrent Approaches递归神经网络处理时间信息的能力有助于提高分割精度。例如，[74]使用ConvLSTM层来改进图像序列中的语义分割结果。然而，也有一些方法在静态图像上使用循环结构。在[13]中，研究人员利用LSTMchains来缠绕多个尺度，从而得到像素级的分割改进。也有将CNNs和RNNs融合的混合方法。这方面的一个很好的例子是所谓的ReSeg模型[75]，其中，输入图像被馈送到一个类似于vgg的CNN编码器，然后通过递归层(即ReNet架构)进行处理，以便更好地定位像素标签。据我们所知，语义分割不存在单纯的递归结构，这主要是因为语义分割需要一个初步的基于cnn的特征编码方案。目前，有一种特定类型的RNN，即“注意模块”，有增长的趋势。在这些模块中，RNN在技术上融合了注意力[76]，在预测输出序列的某个部分时，将注意力集中在输入的某个区域。因此，它们也被用于语义分割[77,78,79]。 Scale-Invariance(尺度变化)根据定义，尺度不变性是指一个方法处理输入时不依赖于相对尺度的能力。或图像分辨率。尽管它对于某些应用程序来说是极其重要的，但是这种能力通常被忽视，或者与方法包含多尺度信息的能力相混淆。一种方法可以使用多尺度信息来提高其像素级分割能力，但仍然依赖于尺度或分辨率。 文献： A SURVEY ON DEEP LEARNING-BASED ARCHITECTURES FOR SEMANTIC SEGMENTATION ON 2D IMAGES","tags":["深度学习"],"categories":["深度学习"]},{"title":"Semantic_segmentation_overview","path":"/2019/12/24/Semantic-segmentation-overview/","content":"语义分割是近年来出现的基本问题之一，因此成为计算机视觉和机器学习领域的热门话题。 通用语义分割图像集 PASCAL Visual Object Classes (VOC) Common Objects in Context (COCO)With 200K labelled images, 1.5 million object instances, and 80object categories Other General Purpose Semantic Segmentation Image Sets YouTube-Objects SIFT-flow Urban Street Semantic Segmentation Image Sets Cityscapes CamVid KITTI SYNTHIA Before Fully Convolutional NetworksPre-Deep Learning Approaches传统图像分割与语义分割的区别在于语义特征在图像分割过程中的应用。传统的图像分割方法，如阈值、聚类和区域增长等(有关传统图像分割技术的调查，请参阅[29])使用手工制作的低级特征(即在图像中定位物体的边界。因此，在需要图像语义信息进行像素级分割的情况下，例如在相似物体相互遮挡的情况下，这些方法是必要的。关于深度CNNs流行之前的语义分割工作，有多种方法[30,31，32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]使用的图形模型，如马尔可夫随机域(MRF)，条件随机场(CRF)或基于森林(有时被称为“整体”)的方法，以便在像素级找到场景标签。其主要思想是通过观察相邻像素之间的依赖关系来找到一个推论。换句话说，这些方法将图像的语义建模为相邻像素之间的一种“先验”信息。另一组研究，有时被称为“分层模型”[44,45,46]，使用一个预先训练的和独立的对象探测器的组合，以便从图像中提取语义信息。由于单个的对象检测器未能正确地对区域进行分类，或者由于这些方法受到“手工选择”的检测器库所提供的对象类的有限数量的限制，因此与当今最先进的方法相比，它们的性能相对较低。虽然上述的前深度学习时代的方法不再是首选的分割方法，一些图形模型，特别是CRFs，目前正在使用的最先进的方法作为后处理(细化)层，目的是提高语义分割的性能，具体细节将在下一节中讨论 Refinement Methods深度神经网络具有很强的局部特征提取能力。然而，它们缺乏利用全局上下文信息的能力，因此无法对相邻像素预测之间的交互进行建模。另一方面，前深度学习时代流行的分割方法，图形模型，非常适合这类任务。这就是为什么它们目前被用作许多深度基于cnn的语义分割架构的细化层。正如在前一节中提到的，使用图形模型进行分割背后的思想是通过观察相邻像素之间的低层次关系来寻找一个推论。在图2中，可以看到使用基于图形模型的细分对分割结果的影响。分类器(见图2.b)不能正确分割不同类标签相邻的像素。在本例中，我们使用基于crf的细分[42]来改进像素级的分割结果。基于crf的方法被广泛用于深度语义分割方法的细化CRFs[50]是一种有区别的无向概率图形模型。它们被用来对观测之间已知的关系进行编码，并构建一致的解释。它们用作细化层的原因是，与不考虑相邻像素相似性的离散分类器不同，CRF可以利用这些信息。与其他图形化模型(如隐马尔科夫模型)相比，CRFs的主要优点是它们的条件性质和避免标签偏差[50]问题的能力。尽管有相当数量的方法(见表1)使用CRFs进行细化，但这些模型在相对较新的方法中开始变得不受欢迎，因为它们的速度非常慢，而且非常难以优化。 Early Deep Learning ApproachesFCN在2014年出现，使用tanh 相较于proposal of a ReLU layer 很难去区分，因此，训练这样的系统被认为是不适合计算的，甚至对大规模数据是不可行的。然而，第一个成熟的方法只是简单地尝试转换分类网络，如AlexNet和VGG通过微调全连接层来细分网络。他们在训练阶段遭受了过度拟合和完全连接层的时间限制。此外，使用的CNNs不够深，无法创建抽象的特征，这与图像的语义有关。在一些早期的深度学习研究中，研究人员拒绝使用完全连接的层来进行决策，而是使用不同的结构，如周期性的架构[57]或使用来自一个单独计算的分段家族的标记。通过提出全连接层FCN这样的结构的必要性的第一个迹象，不出所料，它们被FCN取代。由于他们的分割结果被认为是不令人满意的，这些研究通常使用一个细化的过程，要么作为一个后处理层[52,53,54,56]，或作为一个替代架构，以完全连接的决策层Refinement methods varied such as Markov random fields，nearest neighbour-based approach，使用校准层[54]，使用超级像素[55,56]，或普通CNNs的递归网络。细化层仍然被后fcn方法所使用，其目的是提高类交叉区域的像素级标记性能。tips: 4FCN [11] ] was officially published in 2017. However the same group first shared the idea online as pre-printed literature in 014 [51]. Fully Convolutional Networks for Semantic SegmentationFCN(2017) 提出了从CNNs (DCNN)中拆除全连通层的设想.‘FCN-32s’, ‘FCN16s’, and ‘FCN8s’ all transfer-learnt using the VGG architectureFCN架构在很多方面都被认为是革命性的, FCN不包括全连接层 该结构允许为任何分辨率的图像生成分割图。使用反卷积层，可以将粗深卷积层输出提升到任意分辨率的稠密像素。 提出了DCNNs的skip架构。跳过架构(或连接)在DCNNs中提供不相邻层之间的链接。仅仅通过对未连接层的输出进行求和或连接，这些连接就可以使信息流动，否则，由于体系结构的选择(如最大池化层或辍学)，这些信息就会丢失。最常见的做法是在max-pooling层之前使用skip连接，它通过选择特定区域的最大值对层输出进行采样。池化层有助于架构创建特性层次，但也会导致局部信息的丢失，而这些局部信息对于语义分割是有价值的，特别是在对象边界。跳过连接通过绕过池化层来保存这些信息并将其转发到更深层。实际上，在[11]中使用跳转连接被认为是相当原始的。“FCN-8s”和“FCN-16s”网络在不同的层包含这些跳过连接。对于相同的架构，即“FCN-4s”和“更密集的跳过连接”。“FCN-2s”也被用于各种应用[61,62]。这一思想最终演变为用于语义分割的编码器-解码器结构[63,27]，下文将对此进行介绍。 Post-FCN Approachesdrawbacks of FCNs： 特性层次结构中标签本地化的低效丢失、无法处理全局上下文知识以及缺乏多尺度处理机制。我们还讨论了语义分割上下文中的尺度不变性，最后讨论了基于对象检测的方法，这是一种新的解决方案，旨在解决同时检测对象实例的语义分割问题。 Techniques for Fine-grained Localisation根据定义，语义分割是一个密集的过程，因此它需要在像素级对类标签进行细粒度的本地化。例如，在机器人手术中，语义分割中的像素错误可能会导致生存或死亡的情况。层次特性创建的池(即。，最大池)层可以部分失去本地化。此外，由于他们FCNs完全是卷积性质的，它本身并不具备在图像中对全局上下文信息建模的能力，这在类标签本地化方面也非常有效。因此，这两个问题在本质上和本质上是相互交织的下面我们将讨论旨在克服这些问题和提供更好的本地化的不同方法类的标签。 Encoder-Decoder Architecture编译码器， 类似U-net，具有开创新的研究。编码器使用池化层逐渐缩减输入数据的空间维度，而解码器通过反卷积层等网络层逐步恢复目标的细节和相应的空间维度。从编码器到解码器之间，通常存在直接的信息连接，来帮助解码器更好地恢复目标细节。U-Net，Seg-Net 都是非常出名的网络。在这种结构中，由编码器部分相邻的低分辨率特征映射提供的强相关语义信息必须经过额外的中间层才能到达相同的译码层。这通常会导致一定程度的信息衰减。然而，U-Net架构已经被证明对于不同应用的分割非常有用，例如卫星图像。 Spatial Pyramid Pooling(空间金字塔池化)Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. 在2006年首次被提出， 为的是解决单词袋系统失去了特征之间的空间关系。首次应用在深度学习是2015年的SPPNet这篇文章。无论输入大小如何，都可以在空间金字塔汇聚网络中创建深度特征的空间金字塔表示。SPP-Net最重要的贡献是它允许输入不同大小的数据。不同大小的图像输入到卷积层中，不可避免地会产生不同大小的特征图。然而,如果池化层刚好在决策层之前，具有与输入大小成比例的步长值，则创建特征映射这一层将被固定。CNN中的SPP层构建了不同层次特征之间的关系。因此，它与ED结构中的跳过连接非常相似，后者也允许特性层次结构之间的信息流。：SPP层用于语义分割最常见的用法是在[67]中提出的，比如SPP层被附加到最后一个卷积层，并反馈给像素级分类器。 Feature Concatenation(特征连接)这个想法是基于融合从不同来源提取的特征。 Dilated Convolution(扩张卷积，空洞卷积)扩展卷积的思想实际上很简单:使用连续的卷积滤波器，一个有效的接收域只能随层线性增长;然而，如果使用在滤波器中有间隙的膨胀卷积(见图4.c)，有效接受域将增长得更快[70]。因此，在没有池或子采样的情况下，创建了卷积层的矩形棱镜。扩张卷积是一种非常有效和强大的方法来详细保存特征图分辨率。缺点在于对GPU存储和计算的要求更高，因为特征图分辨率不会在特征层次结构中缩小。 Conditional Random Fields(条件随机场)cnn自然缺乏特别的‘关注’类交叉区域的机制。在这些区域周围，通过观察CNN层的相邻feature maps之间的低层关系，使用图形化模型进行推理。因此，图形模型(主要是crf)被用作深度语义分割架构的细化层。与在[72]中一样，CRFs将低级交互与来自多类交互的输出连接起来，并以这种方式构建全局上下文知识。CRFs作为一种细化层，目前存在多种利用CRFs对CNNs进行深度处理的方法，如卷积CRFs[47]、稠密CRF[42]、CRN-as-RNN等[73]。尽管CRFs有助于构建上下文知识，从而在类标签中更好地本地化，表1显示了在“CRF模型”选项卡下分类的CRFs，以便将它们与实际的CNN架构扩展区分开。 Recurrent Approaches递归神经网络处理时间信息的能力有助于提高分割精度。例如，[74]使用ConvLSTM层来改进图像序列中的语义分割结果。然而，也有一些方法在静态图像上使用循环结构。在[13]中，研究人员利用LSTMchains来缠绕多个尺度，从而得到像素级的分割改进。也有将CNNs和RNNs融合的混合方法。这方面的一个很好的例子是所谓的ReSeg模型[75]，其中，输入图像被馈送到一个类似于vgg的CNN编码器，然后通过递归层(即ReNet架构)进行处理，以便更好地定位像素标签。据我们所知，语义分割不存在单纯的递归结构，这主要是因为语义分割需要一个初步的基于cnn的特征编码方案。目前，有一种特定类型的RNN，即“注意模块”，有增长的趋势。在这些模块中，RNN在技术上融合了注意力[76]，在预测输出序列的某个部分时，将注意力集中在输入的某个区域。因此，它们也被用于语义分割[77,78,79]。 Scale-Invariance(尺度变化)根据定义，尺度不变性是指一个方法处理输入时不依赖于相对尺度的能力。或图像分辨率。尽管它对于某些应用程序来说是极其重要的，但是这种能力通常被忽视，或者与方法包含多尺度信息的能力相混淆。一种方法可以使用多尺度信息来提高其像素级分割能力，但仍然依赖于尺度或分辨率。 文献： A SURVEY ON DEEP LEARNING-BASED ARCHITECTURES FOR SEMANTIC SEGMENTATION ON 2D IMAGES","tags":["OpenCV"],"categories":["深度学习"]},{"title":"二叉树打印","path":"/2019/12/19/20191219-二叉树打印/","content":"二叉树按层遍历 针对二叉树的宽度优先遍历 宽度优先遍历常使用队列结构 面试中，该类题目常常对换行有所要求last: 表示正在打印的当前行的最右节点nlast：表示下一行的最右节点 二叉树的序列化和反序列化 二叉树-字符串（序列化 字符串-二叉树（反序列化 序列化的方式： 根据先序遍历序列化 根据中序遍历序列化 根据后序遍历序列化 按层序列化","tags":["算法"],"categories":["算法"]},{"title":"二叉树打印","path":"/2019/12/19/二叉树打印/","content":"二叉树按层遍历 针对二叉树的宽度优先遍历 宽度优先遍历常使用队列结构 面试中，该类题目常常对换行有所要求last: 表示正在打印的当前行的最右节点nlast：表示下一行的最右节点 二叉树的序列化和反序列化 二叉树-字符串（序列化 字符串-二叉树（反序列化 序列化的方式： 根据先序遍历序列化 根据中序遍历序列化 根据后序遍历序列化 按层序列化","tags":["BFS"],"categories":["算法"]},{"title":"隐马尔可夫HMM","path":"/2019/11/28/20191128-HMM/","content":"马尔科夫模型 状态之间可以发生转换，昨天和今天转换的情况： 今天能得到明天的情况，明天能得到后天的情况，以此类推可以无限的玩下去 这里我们就定义好了一个一阶马尔科夫模型：状态：晴天，多云，雷雨状态转换概率：三种天气状态间的转换概率初始概率：晴天 计算今天(t=1)的天气状况：今天为晴天的概率=初始晴天概率X晴天转晴天概率 +初始多云概率X多云转晴天概率 +初始雷雨概率X雷雨转晴天概率。 隐马尔科夫模型 当前的状态只和前一状态有关： 某个观测只和生成它的状态有关： 隐马尔科夫模型的组成 三个必备：初始概率(π)，隐藏状态转移概率矩阵(A)，生成观测状态概率矩阵(B)。 隐藏状态与观察状态（B矩阵）: 要解决的问题: 模型为 给定模型及观测序列 计算其出现的概率 给定观测序列求解参数使得最大 已知模型和观测序列求状态序列，使得最大 求观测序列的概率 暴力求解：我们要求的是在给定模型下观测序列出现的概率，那如果我能把所有的隐藏序列都给列出来，也就可以知道联合概率分布 现在要求的目标就很明确了。在给定模型下，一个隐藏序列出现的概率，那就由初始状态慢慢转换嘛。出现的概率为： 前向算法给定t时刻的隐藏状态为i，观测序列为o1,o2…ot的概率叫做前向概率：","tags":["深度学习"],"categories":["深度学习"]},{"title":"隐马尔可夫HMM","path":"/2019/11/28/HMM/","content":"马尔科夫模型 状态之间可以发生转换，昨天和今天转换的情况： 今天能得到明天的情况，明天能得到后天的情况，以此类推可以无限的玩下去 这里我们就定义好了一个一阶马尔科夫模型：状态：晴天，多云，雷雨状态转换概率：三种天气状态间的转换概率初始概率：晴天 计算今天(t=1)的天气状况：今天为晴天的概率=初始晴天概率X晴天转晴天概率 +初始多云概率X多云转晴天概率 +初始雷雨概率X雷雨转晴天概率。 隐马尔科夫模型 当前的状态只和前一状态有关： 某个观测只和生成它的状态有关： 隐马尔科夫模型的组成 三个必备：初始概率(π)，隐藏状态转移概率矩阵(A)，生成观测状态概率矩阵(B)。 隐藏状态与观察状态（B矩阵）: 要解决的问题: 模型为 给定模型及观测序列 计算其出现的概率 给定观测序列求解参数使得最大 已知模型和观测序列求状态序列，使得最大 求观测序列的概率 暴力求解：我们要求的是在给定模型下观测序列出现的概率，那如果我能把所有的隐藏序列都给列出来，也就可以知道联合概率分布 现在要求的目标就很明确了。在给定模型下，一个隐藏序列出现的概率，那就由初始状态慢慢转换嘛。出现的概率为： 前向算法给定t时刻的隐藏状态为i，观测序列为o1,o2…ot的概率叫做前向概率：","tags":["编程"],"categories":["其他"]},{"title":"GAN","path":"/2019/11/26/20191126-GAN/","content":"对抗生成网络 GAN(Generative Adversarial Nets) Adversarial Nets Framework 生成器与判别器状态相等 损失函数它做的是去最大化 D的区分度，最小化G和real数据集的数据分布 判别模型： D1 和 D2 相同的，是判别器，G是生成器 生成模型： 先训练判别器，在训练生成器。 # -*- coding: utf-8 -*-# @Author: Your name# @Date: 2019-11-26 09:12:52# @Last Modified by: Your name# @Last Modified time: 2019-11-26 09:12:52import argparseimport numpy as npfrom scipy.stats import normimport tensorflow as tfimport matplotlib.pyplot as pltfrom matplotlib import animationimport seaborn as snssns.set(color_codes=True)seed = 42np.random.seed(seed)tf.set_random_seed(seed)class DataDistribution(object): def __init__(self): self.mu = 4 self.sigma = 0.5 def sample(self, N): samples = np.random.normal(self.mu, self.sigma, N) # 生成高斯分布的概率密度随机数 均值，标准差 samples.sort() return samplesclass GeneratorDistribution(object): def __init__(self, range): self.range = range def sample(self, N): return np.linspace(-self.range, self.range, N) + \\ np.random.random(N) * 0.01 # 生成随机数# 对线性相乘进行初始化def linear(input, output_dim, scope=None, stddev=1.0): norm = tf.random_normal_initializer(stddev=stddev) const = tf.constant_initializer(0.0) with tf.variable_scope(scope or 'linear'): # 定义命名空间 w = tf.get_variable('w', [input.get_shape()[1], output_dim], initializer=norm) b = tf.get_variable('b', [output_dim], initializer=const) return tf.matmul(input, w) + b# 生成器def generator(input, h_dim): # 这个函数的作用是计算激活函数softplus，即log( exp( features ) + 1) h0 = tf.nn.softplus(linear(input, h_dim, 'g0')) h1 = linear(h0, 1, 'g1') return h1# 判别器def discriminator(input, h_dim): h0 = tf.tanh(linear(input, h_dim * 2, 'd0')) h1 = tf.tanh(linear(h0, h_dim * 2, 'd1')) h2 = tf.tanh(linear(h1, h_dim * 2, scope='d2')) h3 = tf.sigmoid(linear(h2, 1, scope='d3')) return h3# 优化器def optimizer(loss, var_list, initial_learning_rate): decay = 0.95 num_decay_steps = 150 batch = tf.Variable(0) learning_rate = tf.train.exponential_decay( initial_learning_rate, batch, num_decay_steps, decay, staircase=True ) optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize( loss, global_step=batch, var_list=var_list ) return optimizerclass GAN(object): def __init__(self, data, gen, num_steps, batch_size, log_every): self.data = data self.gen = gen self.num_steps = num_steps self.batch_size = batch_size self.log_every = log_every self.mlp_hidden_size = 4 self.learning_rate = 0.03 self._create_model() def _create_model(self): with tf.variable_scope('D_pre'): self.pre_input = tf.placeholder(tf.float32, shape=(self.batch_size, 1)) self.pre_labels = tf.placeholder(tf.float32, shape=(self.batch_size, 1)) D_pre = discriminator(self.pre_input, self.mlp_hidden_size) self.pre_loss = tf.reduce_mean(tf.square(D_pre - self.pre_labels)) self.pre_opt = optimizer(self.pre_loss, None, self.learning_rate) # This defines the generator network - it takes samples from a noise # distribution as input, and passes them through an MLP. with tf.variable_scope('Gen'): self.z = tf.placeholder(tf.float32, shape=(self.batch_size, 1)) self.G = generator(self.z, self.mlp_hidden_size) # The discriminator tries to tell the difference between samples from the # true data distribution (self.x) and the generated samples (self.z). # # Here we create two copies of the discriminator network (that share parameters), # as you cannot use the same network with different inputs in TensorFlow. with tf.variable_scope('Disc') as scope: self.x = tf.placeholder(tf.float32, shape=(self.batch_size, 1)) self.D1 = discriminator(self.x, self.mlp_hidden_size) scope.reuse_variables() self.D2 = discriminator(self.G, self.mlp_hidden_size) # Define the loss for discriminator and generator networks (see the original # paper for details), and create optimizers for both self.loss_d = tf.reduce_mean(-tf.log(self.D1) - tf.log(1 - self.D2)) self.loss_g = tf.reduce_mean(-tf.log(self.D2)) self.d_pre_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='D_pre') self.d_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Disc') self.g_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Gen') self.opt_d = optimizer(self.loss_d, self.d_params, self.learning_rate) self.opt_g = optimizer(self.loss_g, self.g_params, self.learning_rate) def train(self): with tf.Session() as session: tf.global_variables_initializer().run() # pretraining discriminator num_pretrain_steps = 1000 for step in range(num_pretrain_steps): d = (np.random.random(self.batch_size) - 0.5) * 10.0 labels = norm.pdf(d, loc=self.data.mu, scale=self.data.sigma) # norm.pdf:正态概率密度函数 pretrain_loss, _ = session.run([self.pre_loss, self.pre_opt], { self.pre_input: np.reshape(d, (self.batch_size, 1)), self.pre_labels: np.reshape(labels, (self.batch_size, 1)) }) self.weightsD = session.run(self.d_pre_params) # copy weights from pre-training over to new D network for i, v in enumerate(self.d_params): session.run(v.assign(self.weightsD[i])) for step in range(self.num_steps): # update discriminator x = self.data.sample(self.batch_size) z = self.gen.sample(self.batch_size) loss_d, _ = session.run([self.loss_d, self.opt_d], { self.x: np.reshape(x, (self.batch_size, 1)), self.z: np.reshape(z, (self.batch_size, 1)) }) # update generator z = self.gen.sample(self.batch_size) loss_g, _ = session.run([self.loss_g, self.opt_g], { self.z: np.reshape(z, (self.batch_size, 1)) }) if step % self.log_every == 0: print('{}: {}\\t{}'.format(step, loss_d, loss_g)) if step % 100 == 0 or step==0 or step == self.num_steps -1 : self._plot_distributions(session) def _samples(self, session, num_points=10000, num_bins=100): xs = np.linspace(-self.gen.range, self.gen.range, num_points) bins = np.linspace(-self.gen.range, self.gen.range, num_bins) # data distribution d = self.data.sample(num_points) pd, _ = np.histogram(d, bins=bins, density=True) # generated samples zs = np.linspace(-self.gen.range, self.gen.range, num_points) g = np.zeros((num_points, 1)) for i in range(num_points // self.batch_size): g[self.batch_size * i:self.batch_size * (i + 1)] = session.run(self.G, { self.z: np.reshape( zs[self.batch_size * i:self.batch_size * (i + 1)], (self.batch_size, 1) ) }) pg, _ = np.histogram(g, bins=bins, density=True) return pd, pg def _plot_distributions(self, session): pd, pg = self._samples(session) p_x = np.linspace(-self.gen.range, self.gen.range, len(pd)) f, ax = plt.subplots(1) ax.set_ylim(0, 1) plt.plot(p_x, pd, label='real data') plt.plot(p_x, pg, label='generated data') plt.title('1D Generative Adversarial Network') plt.xlabel('Data values') plt.ylabel('Probability density') plt.legend() plt.show()def main(args): model = GAN( DataDistribution(), GeneratorDistribution(range=8), args.num_steps, args.batch_size, args.log_every, ) model.train()def parse_args(): parser = argparse.ArgumentParser() parser.add_argument('--num-steps', type=int, default=12000, help='the number of training steps to take') parser.add_argument('--batch-size', type=int, default=12, help='the batch size') parser.add_argument('--log-every', type=int, default=10, help='print loss after this many steps') return parser.parse_args()if __name__ == '__main__': main(parse_args()) DCGAN 将pooling层convolutions替代 对于判别模型：容许网络学习自己的空间下采样 对于生成模型：容许它学习自己的空间上采样 在generator和discriminator上都使用batchnorm 解决初始化差的问题 帮助梯度传播到每一层 防止generator把所有的样本都收敛到同一个点。 在CNN中移除全连接层 在generator的除了输出层外的所有层使用ReLU，输出层采用tanh。 在discriminator的所有层上使用LeakyReLU 100维的向量转为为特征图相似的东西， 再将这个向量reshape 。使用反卷积操作。 输入图片，得到一个值是0或者1，这个是判别网络 这个是生成网络。model.py from __future__ import divisionimport osimport timeimport mathfrom glob import globimport tensorflow as tfimport numpy as npfrom six.moves import xrangefrom ops import *from utils import *def conv_out_size_same(size, stride): return math.ceil(float(size) / float(stride))class DCGAN(object): def __init__(self, sess, input_height=108, input_width=108, is_crop=True, batch_size=64, sample_num = 64, output_height=64, output_width=64, y_dim=None, z_dim=100, gf_dim=64, df_dim=64, gfc_dim=1024, dfc_dim=1024, c_dim=3, dataset_name='default', input_fname_pattern='*.jpg', checkpoint_dir=None, sample_dir=None): \"\"\" # sample number 测试噪音的输出，y代表label Args: sess: TensorFlow session batch_size: The size of batch. Should be specified before training. y_dim: (optional) Dimension of dim for y. [None] z_dim: (optional) Dimension of dim for Z. [100] gf_dim: (optional) Dimension of gen filters in first conv layer. [64] df_dim: (optional) Dimension of discrim filters in first conv layer. [64] gfc_dim: (optional) Dimension of gen units for for fully connected layer. [1024] dfc_dim: (optional) Dimension of discrim units for fully connected layer. [1024] c_dim: (optional) Dimension of image color. For grayscale input, set to 1. [3] \"\"\" self.sess = sess self.is_crop = is_crop self.is_grayscale = (c_dim == 1) self.batch_size = batch_size self.sample_num = sample_num self.input_height = input_height self.input_width = input_width self.output_height = output_height self.output_width = output_width self.y_dim = y_dim # null self.z_dim = z_dim # 噪音点的维度 100 self.gf_dim = gf_dim # 最终多少个filter的个数 基数 self.df_dim = df_dim # 64 self.gfc_dim = gfc_dim# 生成和判别的全连接 1024 self.dfc_dim = dfc_dim # 1024 self.c_dim = c_dim# 生成的是彩色图 3 # batch normalization : deals with poor initialization helps gradient flow self.d_bn1 = batch_norm(name='d_bn1')# bacth在relu之前卷积之后 self.d_bn2 = batch_norm(name='d_bn2') if not self.y_dim: self.d_bn3 = batch_norm(name='d_bn3') self.g_bn0 = batch_norm(name='g_bn0') self.g_bn1 = batch_norm(name='g_bn1') self.g_bn2 = batch_norm(name='g_bn2') if not self.y_dim: self.g_bn3 = batch_norm(name='g_bn3') self.dataset_name = dataset_name self.input_fname_pattern = input_fname_pattern self.checkpoint_dir = checkpoint_dir self.build_model() def build_model(self): if self.y_dim: self.y= tf.placeholder(tf.float32, [self.batch_size, self.y_dim], name='y') if self.is_crop: image_dims = [self.output_height, self.output_width, self.c_dim] else: image_dims = [self.input_height, self.input_height, self.c_dim] self.inputs = tf.placeholder( tf.float32, [self.batch_size] + image_dims, name='real_images') self.sample_inputs = tf.placeholder( # 64 108 108 3，iamge_dim 108 108 3 tf.float32, [self.sample_num] + image_dims, name='sample_inputs') inputs = self.inputs # 64 108 108 3 sample_inputs = self.sample_inputs self.z = tf.placeholder( tf.float32, [None, self.z_dim], name='z') ## 生成网络组最开始的输入，float32 # B， 100 self.z_sum = histogram_summary(\"z\", self.z) # 在训练神经网络时，当需要查看一个张量在训练过程中值的分布情况时，可通过tf.summary.histogram()将其分布情况以直方图的形式在TensorBoard直方图仪表板上显示． if self.y_dim: self.G = self.generator(self.z, self.y) self.D, self.D_logits = \\ self.discriminator(inputs, self.y, reuse=False) self.sampler = self.sampler(self.z, self.y) self.D_, self.D_logits_ = \\ self.discriminator(self.G, self.y, reuse=True) else: self.G = self.generator(self.z) # 64 64 64 3 self.D, self.D_logits = self.discriminator(inputs) # 64 108 108 3 self.sampler = self.sampler(self.z) self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True) self.d_sum = histogram_summary(\"d\", self.D) self.d__sum = histogram_summary(\"d_\", self.D_) self.G_sum = image_summary(\"G\", self.G) # tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits,l) self.d_loss_real = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( logits=self.D_logits, labels=tf.ones_like(self.D))) self.d_loss_fake = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( logits=self.D_logits_, labels=tf.zeros_like(self.D_))) self.g_loss = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( logits=self.D_logits_, labels=tf.ones_like(self.D_))) self.d_loss_real_sum = scalar_summary(\"d_loss_real\", self.d_loss_real) self.d_loss_fake_sum = scalar_summary(\"d_loss_fake\", self.d_loss_fake) self.d_loss = self.d_loss_real + self.d_loss_fake self.g_loss_sum = scalar_summary(\"g_loss\", self.g_loss) self.d_loss_sum = scalar_summary(\"d_loss\", self.d_loss) t_vars = tf.trainable_variables() self.d_vars = [var for var in t_vars if 'd_' in var.name] self.g_vars = [var for var in t_vars if 'g_' in var.name] self.saver = tf.train.Saver() def train(self, config): \"\"\"Train DCGAN\"\"\" if config.dataset == 'mnist': data_X, data_y = self.load_mnist() else: data = glob(os.path.join(\"./data\", config.dataset, self.input_fname_pattern)) #np.random.shuffle(data) d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\ .minimize(self.d_loss, var_list=self.d_vars) g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\ .minimize(self.g_loss, var_list=self.g_vars) try: tf.global_variables_initializer().run() except: tf.initialize_all_variables().run() self.g_sum = merge_summary([self.z_sum, self.d__sum, self.G_sum, self.d_loss_fake_sum, self.g_loss_sum]) self.d_sum = merge_summary( [self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum]) self.writer = SummaryWriter(\"./logs\", self.sess.graph) sample_z = np.random.uniform(-1, 1, size=(self.sample_num , self.z_dim)) if config.dataset == 'mnist': sample_inputs = data_X[0:self.sample_num] sample_labels = data_y[0:self.sample_num] else: sample_files = data[0:self.sample_num] sample = [ get_image(sample_file, input_height=self.input_height, input_width=self.input_width, resize_height=self.output_height, resize_width=self.output_width, is_crop=self.is_crop, is_grayscale=self.is_grayscale) for sample_file in sample_files] if (self.is_grayscale): sample_inputs = np.array(sample).astype(np.float32)[:, :, :, None] else: sample_inputs = np.array(sample).astype(np.float32) counter = 1 start_time = time.time() if self.load(self.checkpoint_dir): print(\" [*] Load SUCCESS\") else: print(\" [!] Load failed...\") for epoch in xrange(config.epoch): if config.dataset == 'mnist': batch_idxs = min(len(data_X), config.train_size) // config.batch_size else: data = glob(os.path.join( \"./data\", config.dataset, self.input_fname_pattern)) batch_idxs = min(len(data), config.train_size) // config.batch_size for idx in xrange(0, batch_idxs): if config.dataset == 'mnist': batch_images = data_X[idx*config.batch_size:(idx+1)*config.batch_size] batch_labels = data_y[idx*config.batch_size:(idx+1)*config.batch_size] else: batch_files = data[idx*config.batch_size:(idx+1)*config.batch_size] batch = [ get_image(batch_file, input_height=self.input_height, input_width=self.input_width, resize_height=self.output_height, resize_width=self.output_width, is_crop=self.is_crop, is_grayscale=self.is_grayscale) for batch_file in batch_files] if (self.is_grayscale): batch_images = np.array(batch).astype(np.float32)[:, :, :, None] else: batch_images = np.array(batch).astype(np.float32) # 一个均匀分布[low,high)中随机采样 从+1和-1之间随才采样 batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \\ .astype(np.float32) if config.dataset == 'mnist': # Update D network _, summary_str = self.sess.run([d_optim, self.d_sum], feed_dict={ self.inputs: batch_images, self.z: batch_z, self.y:batch_labels, }) self.writer.add_summary(summary_str, counter) # Update G network _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z, self.y:batch_labels, }) self.writer.add_summary(summary_str, counter) # Run g_optim twice to make sure that d_loss does not go to zero (different from paper) _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z, self.y:batch_labels }) self.writer.add_summary(summary_str, counter) errD_fake = self.d_loss_fake.eval({ self.z: batch_z, self.y:batch_labels }) errD_real = self.d_loss_real.eval({ self.inputs: batch_images, self.y:batch_labels }) errG = self.g_loss.eval({ self.z: batch_z, self.y: batch_labels }) else: # Update D network _, summary_str = self.sess.run([d_optim, self.d_sum], feed_dict={ self.inputs: batch_images, self.z: batch_z }) self.writer.add_summary(summary_str, counter) # Update G network _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z }) self.writer.add_summary(summary_str, counter) # Run g_optim twice to make sure that d_loss does not go to zero (different from paper) _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z }) self.writer.add_summary(summary_str, counter) errD_fake = self.d_loss_fake.eval({ self.z: batch_z }) errD_real = self.d_loss_real.eval({ self.inputs: batch_images }) errG = self.g_loss.eval({self.z: batch_z}) counter += 1 print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f\" \\ % (epoch, idx, batch_idxs, time.time() - start_time, errD_fake+errD_real, errG)) if np.mod(counter, 100) == 1: if config.dataset == 'mnist': samples, d_loss, g_loss = self.sess.run( [self.sampler, self.d_loss, self.g_loss], feed_dict={ self.z: sample_z, self.inputs: sample_inputs, self.y:sample_labels, } ) save_images(samples, [8, 8], './{}/train_{:02d}_{:04d}.png'.format(config.sample_dir, epoch, idx)) print(\"[Sample] d_loss: %.8f, g_loss: %.8f\" % (d_loss, g_loss)) else: try: samples, d_loss, g_loss = self.sess.run( [self.sampler, self.d_loss, self.g_loss], feed_dict={ self.z: sample_z, self.inputs: sample_inputs, }, ) save_images(samples, [8, 8], './{}/train_{:02d}_{:04d}.png'.format(config.sample_dir, epoch, idx)) print(\"[Sample] d_loss: %.8f, g_loss: %.8f\" % (d_loss, g_loss)) except: print(\"one pic error!...\") if counter//10 == 2: self.save(config.checkpoint_dir, counter) def discriminator(self, image, y=None, reuse=False): with tf.variable_scope(\"discriminator\") as scope: if reuse: scope.reuse_variables() if not self.y_dim: h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv')) h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name='d_h1_conv'))) h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name='d_h2_conv'))) h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name='d_h3_conv'))) # 64 14 14 256 -64 7 7 512 aa = tf.reshape(h3, [self.batch_size, -1]) h4 = linear(aa, 1, 'd_h3_lin') return tf.nn.sigmoid(h4), h4 else: yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) x = conv_cond_concat(image, yb) h0 = lrelu(conv2d(x, self.c_dim + self.y_dim, name='d_h0_conv')) h0 = conv_cond_concat(h0, yb) h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim + self.y_dim, name='d_h1_conv'))) h1 = tf.reshape(h1, [self.batch_size, -1]) h1 = concat([h1, y], 1) h2 = lrelu(self.d_bn2(linear(h1, self.dfc_dim, 'd_h2_lin'))) h2 = concat([h2, y], 1) h3 = linear(h2, 1, 'd_h3_lin') return tf.nn.sigmoid(h3), h3 def generator(self, z, y=None): with tf.variable_scope(\"generator\") as scope: if not self.y_dim: s_h, s_w = self.output_height, self.output_width s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2) # 先把特征图大小确定出来 s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2) s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2) s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2) # project `z` and reshape self.z_, self.h0_w, self.h0_b = linear( z, self.gf_dim*8*s_h16*s_w16, 'g_h0_lin', with_w=True) self.h0 = tf.reshape( self.z_, [-1, s_h16, s_w16, self.gf_dim * 8]) h0 = tf.nn.relu(self.g_bn0(self.h0)) self.h1, self.h1_w, self.h1_b = deconv2d( h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name='g_h1', with_w=True) h1 = tf.nn.relu(self.g_bn1(self.h1)) h2, self.h2_w, self.h2_b = deconv2d( h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name='g_h2', with_w=True) h2 = tf.nn.relu(self.g_bn2(h2)) h3, self.h3_w, self.h3_b = deconv2d( h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name='g_h3', with_w=True) h3 = tf.nn.relu(self.g_bn3(h3)) h4, self.h4_w, self.h4_b = deconv2d( h3, [self.batch_size, s_h, s_w, self.c_dim], name='g_h4', with_w=True) return tf.nn.tanh(h4) else: s_h, s_w = self.output_height, self.output_width s_h2, s_h4 = int(s_h/2), int(s_h/4) s_w2, s_w4 = int(s_w/2), int(s_w/4) # yb = tf.expand_dims(tf.expand_dims(y, 1),2) yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) z = concat([z, y], 1) h0 = tf.nn.relu( self.g_bn0(linear(z, self.gfc_dim, 'g_h0_lin'))) h0 = concat([h0, y], 1) h1 = tf.nn.relu(self.g_bn1( linear(h0, self.gf_dim*2*s_h4*s_w4, 'g_h1_lin'))) h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2]) h1 = conv_cond_concat(h1, yb) h2 = tf.nn.relu(self.g_bn2(deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name='g_h2'))) h2 = conv_cond_concat(h2, yb) return tf.nn.sigmoid( deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name='g_h3')) def sampler(self, z, y=None): with tf.variable_scope(\"generator\") as scope: scope.reuse_variables() if not self.y_dim: s_h, s_w = self.output_height, self.output_width s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2) s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2) s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2) s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2) # project `z` and reshape h0 = tf.reshape( linear(z, self.gf_dim*8*s_h16*s_w16, 'g_h0_lin'), [-1, s_h16, s_w16, self.gf_dim * 8]) h0 = tf.nn.relu(self.g_bn0(h0, train=False)) h1 = deconv2d(h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name='g_h1') h1 = tf.nn.relu(self.g_bn1(h1, train=False)) h2 = deconv2d(h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name='g_h2') h2 = tf.nn.relu(self.g_bn2(h2, train=False)) h3 = deconv2d(h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name='g_h3') h3 = tf.nn.relu(self.g_bn3(h3, train=False)) h4 = deconv2d(h3, [self.batch_size, s_h, s_w, self.c_dim], name='g_h4') return tf.nn.tanh(h4) else: s_h, s_w = self.output_height, self.output_width s_h2, s_h4 = int(s_h/2), int(s_h/4) s_w2, s_w4 = int(s_w/2), int(s_w/4) # yb = tf.reshape(y, [-1, 1, 1, self.y_dim]) yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) z = concat([z, y], 1) h0 = tf.nn.relu(self.g_bn0(linear(z, self.gfc_dim, 'g_h0_lin'))) h0 = concat([h0, y], 1) h1 = tf.nn.relu(self.g_bn1( linear(h0, self.gf_dim*2*s_h4*s_w4, 'g_h1_lin'), train=False)) h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2]) h1 = conv_cond_concat(h1, yb) h2 = tf.nn.relu(self.g_bn2( deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name='g_h2'), train=False)) h2 = conv_cond_concat(h2, yb) return tf.nn.sigmoid(deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name='g_h3')) def load_mnist(self): data_dir = os.path.join(\"./data\", self.dataset_name) fd = open(os.path.join(data_dir,'train-images-idx3-ubyte')) loaded = np.fromfile(file=fd,dtype=np.uint8) trX = loaded[16:].reshape((60000,28,28,1)).astype(np.float) fd = open(os.path.join(data_dir,'train-labels-idx1-ubyte')) loaded = np.fromfile(file=fd,dtype=np.uint8) trY = loaded[8:].reshape((60000)).astype(np.float) fd = open(os.path.join(data_dir,'t10k-images-idx3-ubyte')) loaded = np.fromfile(file=fd,dtype=np.uint8) teX = loaded[16:].reshape((10000,28,28,1)).astype(np.float) fd = open(os.path.join(data_dir,'t10k-labels-idx1-ubyte')) loaded = np.fromfile(file=fd,dtype=np.uint8) teY = loaded[8:].reshape((10000)).astype(np.float) trY = np.asarray(trY) teY = np.asarray(teY) X = np.concatenate((trX, teX), axis=0) y = np.concatenate((trY, teY), axis=0).astype(np.int) seed = 547 np.random.seed(seed) np.random.shuffle(X) np.random.seed(seed) np.random.shuffle(y) y_vec = np.zeros((len(y), self.y_dim), dtype=np.float) for i, label in enumerate(y): y_vec[i,y[i]] = 1.0 return X/255.,y_vec @property def model_dir(self): return \"{}_{}_{}_{}\".format( self.dataset_name, self.batch_size, self.output_height, self.output_width) def save(self, checkpoint_dir, step): model_name = \"DCGAN.model\" checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir) if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir) self.saver.save(self.sess, os.path.join(checkpoint_dir, model_name), global_step=step) def load(self, checkpoint_dir): print(\" [*] Reading checkpoints...\") checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir) ckpt = tf.train.get_checkpoint_state(checkpoint_dir) if ckpt and ckpt.model_checkpoint_path: ckpt_name = os.path.basename(ckpt.model_checkpoint_path) self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name)) print(\" [*] Success to read {}\".format(ckpt_name)) return True else: print(\" [*] Failed to find a checkpoint\") return False","tags":["深度学习"],"categories":["深度学习"]},{"title":"GAN","path":"/2019/11/26/GAN/","content":"对抗生成网络 GAN(Generative Adversarial Nets) Adversarial Nets Framework 生成器与判别器状态相等 损失函数它做的是去最大化 D的区分度，最小化G和real数据集的数据分布 判别模型： D1 和 D2 相同的，是判别器，G是生成器 生成模型： 先训练判别器，在训练生成器。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 | # -*- coding: utf-8 -*- # @Author: Your name # @Date: 2019-11-26 09:12:52 # @Last Modified by: Your name # @Last Modified time: 2019-11-26 09:12:52 import argparse import numpy as np from scipy.stats import norm import tensorflow as tf import matplotlib.pyplot as plt from matplotlib import animation import seaborn as sns sns.set(color_codes=True) seed = 42 np.random.seed(seed) tf.set_random_seed(seed) class DataDistribution(object): def __init__(self): self.mu = 4 self.sigma = 0.5 def sample(self, N): samples = np.random.normal(self.mu, self.sigma, N) # 生成高斯分布的概率密度随机数 均值，标准差 samples.sort() return samples class GeneratorDistribution(object): def __init__(self, range): self.range = range def sample(self, N): return np.linspace(-self.range, self.range, N) + \\ np.random.random(N) * 0.01 # 生成随机数 # 对线性相乘进行初始化 def linear(input, output_dim, scope=None, stddev=1.0): norm = tf.random_normal_initializer(stddev=stddev) const = tf.constant_initializer(0.0) with tf.variable_scope(scope or 'linear'): # 定义命名空间 w = tf.get_variable('w', [input.get_shape()[1], output_dim], initializer=norm) b = tf.get_variable('b', [output_dim], initializer=const) return tf.matmul(input, w) + b # 生成器 def generator(input, h_dim): # 这个函数的作用是计算激活函数softplus，即log( exp( features ) + 1) h0 = tf.nn.softplus(linear(input, h_dim, 'g0')) h1 = linear(h0, 1, 'g1') return h1 # 判别器 def discriminator(input, h_dim): h0 = tf.tanh(linear(input, h_dim * 2, 'd0')) h1 = tf.tanh(linear(h0, h_dim * 2, 'd1')) h2 = tf.tanh(linear(h1, h_dim * 2, scope='d2')) h3 = tf.sigmoid(linear(h2, 1, scope='d3')) return h3 # 优化器 def optimizer(loss, var_list, initial_learning_rate): decay = 0.95 num_decay_steps = 150 batch = tf.Variable(0) learning_rate = tf.train.exponential_decay( initial_learning_rate, batch, num_decay_steps, decay, staircase=True ) optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize( loss, global_step=batch, var_list=var_list ) return optimizer class GAN(object): def __init__(self, data, gen, num_steps, batch_size, log_every): self.data = data self.gen = gen self.num_steps = num_steps self.batch_size = batch_size self.log_every = log_every self.mlp_hidden_size = 4 self.learning_rate = 0.03 self._create_model() def _create_model(self): with tf.variable_scope('D_pre'): self.pre_input = tf.placeholder(tf.float32, shape=(self.batch_size, 1)) self.pre_labels = tf.placeholder(tf.float32, shape=(self.batch_size, 1)) D_pre = discriminator(self.pre_input, self.mlp_hidden_size) self.pre_loss = tf.reduce_mean(tf.square(D_pre - self.pre_labels)) self.pre_opt = optimizer(self.pre_loss, None, self.learning_rate) # This defines the generator network - it takes samples from a noise # distribution as input, and passes them through an MLP. with tf.variable_scope('Gen'): self.z = tf.placeholder(tf.float32, shape=(self.batch_size, 1)) self.G = generator(self.z, self.mlp_hidden_size) # The discriminator tries to tell the difference between samples from the # true data distribution (self.x) and the generated samples (self.z). # # Here we create two copies of the discriminator network (that share parameters), # as you cannot use the same network with different inputs in TensorFlow. with tf.variable_scope('Disc') as scope: self.x = tf.placeholder(tf.float32, shape=(self.batch_size, 1)) self.D1 = discriminator(self.x, self.mlp_hidden_size) scope.reuse_variables() self.D2 = discriminator(self.G, self.mlp_hidden_size) # Define the loss for discriminator and generator networks (see the original # paper for details), and create optimizers for both self.loss_d = tf.reduce_mean(-tf.log(self.D1) - tf.log(1 - self.D2)) self.loss_g = tf.reduce_mean(-tf.log(self.D2)) self.d_pre_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='D_pre') self.d_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Disc') self.g_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Gen') self.opt_d = optimizer(self.loss_d, self.d_params, self.learning_rate) self.opt_g = optimizer(self.loss_g, self.g_params, self.learning_rate) def train(self): with tf.Session() as session: tf.global_variables_initializer().run() # pretraining discriminator num_pretrain_steps = 1000 for step in range(num_pretrain_steps): d = (np.random.random(self.batch_size) - 0.5) * 10.0 labels = norm.pdf(d, loc=self.data.mu, scale=self.data.sigma) # norm.pdf:正态概率密度函数 pretrain_loss, _ = session.run([self.pre_loss, self.pre_opt], { self.pre_input: np.reshape(d, (self.batch_size, 1)), self.pre_labels: np.reshape(labels, (self.batch_size, 1)) }) self.weightsD = session.run(self.d_pre_params) # copy weights from pre-training over to new D network for i, v in enumerate(self.d_params): session.run(v.assign(self.weightsD[i])) for step in range(self.num_steps): # update discriminator x = self.data.sample(self.batch_size) z = self.gen.sample(self.batch_size) loss_d, _ = session.run([self.loss_d, self.opt_d], { self.x: np.reshape(x, (self.batch_size, 1)), self.z: np.reshape(z, (self.batch_size, 1)) }) # update generator z = self.gen.sample(self.batch_size) loss_g, _ = session.run([self.loss_g, self.opt_g], { self.z: np.reshape(z, (self.batch_size, 1)) }) if step % self.log_every == 0: print('{}: {}\\t{}'.format(step, loss_d, loss_g)) if step % 100 == 0 or step==0 or step == self.num_steps -1 : self._plot_distributions(session) def _samples(self, session, num_points=10000, num_bins=100): xs = np.linspace(-self.gen.range, self.gen.range, num_points) bins = np.linspace(-self.gen.range, self.gen.range, num_bins) # data distribution d = self.data.sample(num_points) pd, _ = np.histogram(d, bins=bins, density=True) # generated samples zs = np.linspace(-self.gen.range, self.gen.range, num_points) g = np.zeros((num_points, 1)) for i in range(num_points // self.batch_size): g[self.batch_size * i:self.batch_size * (i + 1)] = session.run(self.G, { self.z: np.reshape( zs[self.batch_size * i:self.batch_size * (i + 1)], (self.batch_size, 1) ) }) pg, _ = np.histogram(g, bins=bins, density=True) return pd, pg def _plot_distributions(self, session): pd, pg = self._samples(session) p_x = np.linspace(-self.gen.range, self.gen.range, len(pd)) f, ax = plt.subplots(1) ax.set_ylim(0, 1) plt.plot(p_x, pd, label='real data') plt.plot(p_x, pg, label='generated data') plt.title('1D Generative Adversarial Network') plt.xlabel('Data values') plt.ylabel('Probability density') plt.legend() plt.show() def main(args): model = GAN( DataDistribution(), GeneratorDistribution(range=8), args.num_steps, args.batch_size, args.log_every, ) model.train() def parse_args(): parser = argparse.ArgumentParser() parser.add_argument('--num-steps', type=int, default=12000, help='the number of training steps to take') parser.add_argument('--batch-size', type=int, default=12, help='the batch size') parser.add_argument('--log-every', type=int, default=10, help='print loss after this many steps') return parser.parse_args() if __name__ == '__main__': main(parse_args()) —|— DCGAN 将pooling层convolutions替代 对于判别模型：容许网络学习自己的空间下采样 对于生成模型：容许它学习自己的空间上采样 在generator和discriminator上都使用batchnorm 解决初始化差的问题 帮助梯度传播到每一层 防止generator把所有的样本都收敛到同一个点。 在CNN中移除全连接层 在generator的除了输出层外的所有层使用ReLU，输出层采用tanh。 在discriminator的所有层上使用LeakyReLU 100维的向量转为为特征图相似的东西， 再将这个向量reshape 。使用反卷积操作。 输入图片，得到一个值是0或者1，这个是判别网络 这个是生成网络。model.py 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464 465 466 467 468 469 470 471 472 473 474 475 476 477 478 479 480 481 482 483 484 485 486 487 488 489 490 491 492 493 494 495 496 497 498 499 500 501 502 503 504 505 506 507 508 509 510 511 512 513 514 515 516 517 518 519 520 521 522 523 | from __future__ import division import os import time import math from glob import glob import tensorflow as tf import numpy as np from six.moves import xrange from ops import * from utils import * def conv_out_size_same(size, stride): return math.ceil(float(size) / float(stride)) class DCGAN(object): def __init__(self, sess, input_height=108, input_width=108, is_crop=True, batch_size=64, sample_num = 64, output_height=64, output_width=64, y_dim=None, z_dim=100, gf_dim=64, df_dim=64, gfc_dim=1024, dfc_dim=1024, c_dim=3, dataset_name='default', input_fname_pattern='*.jpg', checkpoint_dir=None, sample_dir=None): \"\"\" # sample number 测试噪音的输出，y代表label Args: sess: TensorFlow session batch_size: The size of batch. Should be specified before training. y_dim: (optional) Dimension of dim for y. [None] z_dim: (optional) Dimension of dim for Z. [100] gf_dim: (optional) Dimension of gen filters in first conv layer. [64] df_dim: (optional) Dimension of discrim filters in first conv layer. [64] gfc_dim: (optional) Dimension of gen units for for fully connected layer. [1024] dfc_dim: (optional) Dimension of discrim units for fully connected layer. [1024] c_dim: (optional) Dimension of image color. For grayscale input, set to 1. [3] \"\"\" self.sess = sess self.is_crop = is_crop self.is_grayscale = (c_dim == 1) self.batch_size = batch_size self.sample_num = sample_num self.input_height = input_height self.input_width = input_width self.output_height = output_height self.output_width = output_width self.y_dim = y_dim # null self.z_dim = z_dim # 噪音点的维度 100 self.gf_dim = gf_dim # 最终多少个filter的个数 基数 self.df_dim = df_dim # 64 self.gfc_dim = gfc_dim# 生成和判别的全连接 1024 self.dfc_dim = dfc_dim # 1024 self.c_dim = c_dim# 生成的是彩色图 3 # batch normalization : deals with poor initialization helps gradient flow self.d_bn1 = batch_norm(name='d_bn1')# bacth在relu之前卷积之后 self.d_bn2 = batch_norm(name='d_bn2') if not self.y_dim: self.d_bn3 = batch_norm(name='d_bn3') self.g_bn0 = batch_norm(name='g_bn0') self.g_bn1 = batch_norm(name='g_bn1') self.g_bn2 = batch_norm(name='g_bn2') if not self.y_dim: self.g_bn3 = batch_norm(name='g_bn3') self.dataset_name = dataset_name self.input_fname_pattern = input_fname_pattern self.checkpoint_dir = checkpoint_dir self.build_model() def build_model(self): if self.y_dim: self.y= tf.placeholder(tf.float32, [self.batch_size, self.y_dim], name='y') if self.is_crop: image_dims = [self.output_height, self.output_width, self.c_dim] else: image_dims = [self.input_height, self.input_height, self.c_dim] self.inputs = tf.placeholder( tf.float32, [self.batch_size] + image_dims, name='real_images') self.sample_inputs = tf.placeholder( # 64 108 108 3，iamge_dim 108 108 3 tf.float32, [self.sample_num] + image_dims, name='sample_inputs') inputs = self.inputs # 64 108 108 3 sample_inputs = self.sample_inputs self.z = tf.placeholder( tf.float32, [None, self.z_dim], name='z') ## 生成网络组最开始的输入，float32 # B， 100 self.z_sum = histogram_summary(\"z\", self.z) # 在训练神经网络时，当需要查看一个张量在训练过程中值的分布情况时，可通过tf.summary.histogram()将其分布情况以直方图的形式在TensorBoard直方图仪表板上显示． if self.y_dim: self.G = self.generator(self.z, self.y) self.D, self.D_logits = \\ self.discriminator(inputs, self.y, reuse=False) self.sampler = self.sampler(self.z, self.y) self.D_, self.D_logits_ = \\ self.discriminator(self.G, self.y, reuse=True) else: self.G = self.generator(self.z) # 64 64 64 3 self.D, self.D_logits = self.discriminator(inputs) # 64 108 108 3 self.sampler = self.sampler(self.z) self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True) self.d_sum = histogram_summary(\"d\", self.D) self.d__sum = histogram_summary(\"d_\", self.D_) self.G_sum = image_summary(\"G\", self.G) # tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits,l) self.d_loss_real = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( logits=self.D_logits, labels=tf.ones_like(self.D))) self.d_loss_fake = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( logits=self.D_logits_, labels=tf.zeros_like(self.D_))) self.g_loss = tf.reduce_mean( tf.nn.sigmoid_cross_entropy_with_logits( logits=self.D_logits_, labels=tf.ones_like(self.D_))) self.d_loss_real_sum = scalar_summary(\"d_loss_real\", self.d_loss_real) self.d_loss_fake_sum = scalar_summary(\"d_loss_fake\", self.d_loss_fake) self.d_loss = self.d_loss_real + self.d_loss_fake self.g_loss_sum = scalar_summary(\"g_loss\", self.g_loss) self.d_loss_sum = scalar_summary(\"d_loss\", self.d_loss) t_vars = tf.trainable_variables() self.d_vars = [var for var in t_vars if 'd_' in var.name] self.g_vars = [var for var in t_vars if 'g_' in var.name] self.saver = tf.train.Saver() def train(self, config): \"\"\"Train DCGAN\"\"\" if config.dataset == 'mnist': data_X, data_y = self.load_mnist() else: data = glob(os.path.join(\"./data\", config.dataset, self.input_fname_pattern)) #np.random.shuffle(data) d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\ .minimize(self.d_loss, var_list=self.d_vars) g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \\ .minimize(self.g_loss, var_list=self.g_vars) try: tf.global_variables_initializer().run() except: tf.initialize_all_variables().run() self.g_sum = merge_summary([self.z_sum, self.d__sum, self.G_sum, self.d_loss_fake_sum, self.g_loss_sum]) self.d_sum = merge_summary( [self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum]) self.writer = SummaryWriter(\"./logs\", self.sess.graph) sample_z = np.random.uniform(-1, 1, size=(self.sample_num , self.z_dim)) if config.dataset == 'mnist': sample_inputs = data_X[0:self.sample_num] sample_labels = data_y[0:self.sample_num] else: sample_files = data[0:self.sample_num] sample = [ get_image(sample_file, input_height=self.input_height, input_width=self.input_width, resize_height=self.output_height, resize_width=self.output_width, is_crop=self.is_crop, is_grayscale=self.is_grayscale) for sample_file in sample_files] if (self.is_grayscale): sample_inputs = np.array(sample).astype(np.float32)[:, :, :, None] else: sample_inputs = np.array(sample).astype(np.float32) counter = 1 start_time = time.time() if self.load(self.checkpoint_dir): print(\" [*] Load SUCCESS\") else: print(\" [!] Load failed...\") for epoch in xrange(config.epoch): if config.dataset == 'mnist': batch_idxs = min(len(data_X), config.train_size) // config.batch_size else: data = glob(os.path.join( \"./data\", config.dataset, self.input_fname_pattern)) batch_idxs = min(len(data), config.train_size) // config.batch_size for idx in xrange(0, batch_idxs): if config.dataset == 'mnist': batch_images = data_X[idx*config.batch_size:(idx+1)*config.batch_size] batch_labels = data_y[idx*config.batch_size:(idx+1)*config.batch_size] else: batch_files = data[idx*config.batch_size:(idx+1)*config.batch_size] batch = [ get_image(batch_file, input_height=self.input_height, input_width=self.input_width, resize_height=self.output_height, resize_width=self.output_width, is_crop=self.is_crop, is_grayscale=self.is_grayscale) for batch_file in batch_files] if (self.is_grayscale): batch_images = np.array(batch).astype(np.float32)[:, :, :, None] else: batch_images = np.array(batch).astype(np.float32) # 一个均匀分布[low,high)中随机采样 从+1和-1之间随才采样 batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \\ .astype(np.float32) if config.dataset == 'mnist': # Update D network _, summary_str = self.sess.run([d_optim, self.d_sum], feed_dict={ self.inputs: batch_images, self.z: batch_z, self.y:batch_labels, }) self.writer.add_summary(summary_str, counter) # Update G network _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z, self.y:batch_labels, }) self.writer.add_summary(summary_str, counter) # Run g_optim twice to make sure that d_loss does not go to zero (different from paper) _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z, self.y:batch_labels }) self.writer.add_summary(summary_str, counter) errD_fake = self.d_loss_fake.eval({ self.z: batch_z, self.y:batch_labels }) errD_real = self.d_loss_real.eval({ self.inputs: batch_images, self.y:batch_labels }) errG = self.g_loss.eval({ self.z: batch_z, self.y: batch_labels }) else: # Update D network _, summary_str = self.sess.run([d_optim, self.d_sum], feed_dict={ self.inputs: batch_images, self.z: batch_z }) self.writer.add_summary(summary_str, counter) # Update G network _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z }) self.writer.add_summary(summary_str, counter) # Run g_optim twice to make sure that d_loss does not go to zero (different from paper) _, summary_str = self.sess.run([g_optim, self.g_sum], feed_dict={ self.z: batch_z }) self.writer.add_summary(summary_str, counter) errD_fake = self.d_loss_fake.eval({ self.z: batch_z }) errD_real = self.d_loss_real.eval({ self.inputs: batch_images }) errG = self.g_loss.eval({self.z: batch_z}) counter += 1 print(\"Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f\" \\ % (epoch, idx, batch_idxs, time.time() - start_time, errD_fake+errD_real, errG)) if np.mod(counter, 100) == 1: if config.dataset == 'mnist': samples, d_loss, g_loss = self.sess.run( [self.sampler, self.d_loss, self.g_loss], feed_dict={ self.z: sample_z, self.inputs: sample_inputs, self.y:sample_labels, } ) save_images(samples, [8, 8], './{}/train_{:02d}_{:04d}.png'.format(config.sample_dir, epoch, idx)) print(\"[Sample] d_loss: %.8f, g_loss: %.8f\" % (d_loss, g_loss)) else: try: samples, d_loss, g_loss = self.sess.run( [self.sampler, self.d_loss, self.g_loss], feed_dict={ self.z: sample_z, self.inputs: sample_inputs, }, ) save_images(samples, [8, 8], './{}/train_{:02d}_{:04d}.png'.format(config.sample_dir, epoch, idx)) print(\"[Sample] d_loss: %.8f, g_loss: %.8f\" % (d_loss, g_loss)) except: print(\"one pic error!...\") if counter//10 == 2: self.save(config.checkpoint_dir, counter) def discriminator(self, image, y=None, reuse=False): with tf.variable_scope(\"discriminator\") as scope: if reuse: scope.reuse_variables() if not self.y_dim: h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv')) h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name='d_h1_conv'))) h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name='d_h2_conv'))) h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name='d_h3_conv'))) # 64 14 14 256 -64 7 7 512 aa = tf.reshape(h3, [self.batch_size, -1]) h4 = linear(aa, 1, 'd_h3_lin') return tf.nn.sigmoid(h4), h4 else: yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) x = conv_cond_concat(image, yb) h0 = lrelu(conv2d(x, self.c_dim + self.y_dim, name='d_h0_conv')) h0 = conv_cond_concat(h0, yb) h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim + self.y_dim, name='d_h1_conv'))) h1 = tf.reshape(h1, [self.batch_size, -1]) h1 = concat([h1, y], 1) h2 = lrelu(self.d_bn2(linear(h1, self.dfc_dim, 'd_h2_lin'))) h2 = concat([h2, y], 1) h3 = linear(h2, 1, 'd_h3_lin') return tf.nn.sigmoid(h3), h3 def generator(self, z, y=None): with tf.variable_scope(\"generator\") as scope: if not self.y_dim: s_h, s_w = self.output_height, self.output_width s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2) # 先把特征图大小确定出来 s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2) s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2) s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2) # project `z` and reshape self.z_, self.h0_w, self.h0_b = linear( z, self.gf_dim*8*s_h16*s_w16, 'g_h0_lin', with_w=True) self.h0 = tf.reshape( self.z_, [-1, s_h16, s_w16, self.gf_dim * 8]) h0 = tf.nn.relu(self.g_bn0(self.h0)) self.h1, self.h1_w, self.h1_b = deconv2d( h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name='g_h1', with_w=True) h1 = tf.nn.relu(self.g_bn1(self.h1)) h2, self.h2_w, self.h2_b = deconv2d( h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name='g_h2', with_w=True) h2 = tf.nn.relu(self.g_bn2(h2)) h3, self.h3_w, self.h3_b = deconv2d( h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name='g_h3', with_w=True) h3 = tf.nn.relu(self.g_bn3(h3)) h4, self.h4_w, self.h4_b = deconv2d( h3, [self.batch_size, s_h, s_w, self.c_dim], name='g_h4', with_w=True) return tf.nn.tanh(h4) else: s_h, s_w = self.output_height, self.output_width s_h2, s_h4 = int(s_h/2), int(s_h/4) s_w2, s_w4 = int(s_w/2), int(s_w/4) # yb = tf.expand_dims(tf.expand_dims(y, 1),2) yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) z = concat([z, y], 1) h0 = tf.nn.relu( self.g_bn0(linear(z, self.gfc_dim, 'g_h0_lin'))) h0 = concat([h0, y], 1) h1 = tf.nn.relu(self.g_bn1( linear(h0, self.gf_dim*2*s_h4*s_w4, 'g_h1_lin'))) h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2]) h1 = conv_cond_concat(h1, yb) h2 = tf.nn.relu(self.g_bn2(deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name='g_h2'))) h2 = conv_cond_concat(h2, yb) return tf.nn.sigmoid( deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name='g_h3')) def sampler(self, z, y=None): with tf.variable_scope(\"generator\") as scope: scope.reuse_variables() if not self.y_dim: s_h, s_w = self.output_height, self.output_width s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2) s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2) s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2) s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2) # project `z` and reshape h0 = tf.reshape( linear(z, self.gf_dim*8*s_h16*s_w16, 'g_h0_lin'), [-1, s_h16, s_w16, self.gf_dim * 8]) h0 = tf.nn.relu(self.g_bn0(h0, train=False)) h1 = deconv2d(h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name='g_h1') h1 = tf.nn.relu(self.g_bn1(h1, train=False)) h2 = deconv2d(h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name='g_h2') h2 = tf.nn.relu(self.g_bn2(h2, train=False)) h3 = deconv2d(h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name='g_h3') h3 = tf.nn.relu(self.g_bn3(h3, train=False)) h4 = deconv2d(h3, [self.batch_size, s_h, s_w, self.c_dim], name='g_h4') return tf.nn.tanh(h4) else: s_h, s_w = self.output_height, self.output_width s_h2, s_h4 = int(s_h/2), int(s_h/4) s_w2, s_w4 = int(s_w/2), int(s_w/4) # yb = tf.reshape(y, [-1, 1, 1, self.y_dim]) yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim]) z = concat([z, y], 1) h0 = tf.nn.relu(self.g_bn0(linear(z, self.gfc_dim, 'g_h0_lin'))) h0 = concat([h0, y], 1) h1 = tf.nn.relu(self.g_bn1( linear(h0, self.gf_dim*2*s_h4*s_w4, 'g_h1_lin'), train=False)) h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2]) h1 = conv_cond_concat(h1, yb) h2 = tf.nn.relu(self.g_bn2( deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name='g_h2'), train=False)) h2 = conv_cond_concat(h2, yb) return tf.nn.sigmoid(deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name='g_h3')) def load_mnist(self): data_dir = os.path.join(\"./data\", self.dataset_name) fd = open(os.path.join(data_dir,'train-images-idx3-ubyte')) loaded = np.fromfile(file=fd,dtype=np.uint8) trX = loaded[16:].reshape((60000,28,28,1)).astype(np.float) fd = open(os.path.join(data_dir,'train-labels-idx1-ubyte')) loaded = np.fromfile(file=fd,dtype=np.uint8) trY = loaded[8:].reshape((60000)).astype(np.float) fd = open(os.path.join(data_dir,'t10k-images-idx3-ubyte')) loaded = np.fromfile(file=fd,dtype=np.uint8) teX = loaded[16:].reshape((10000,28,28,1)).astype(np.float) fd = open(os.path.join(data_dir,'t10k-labels-idx1-ubyte')) loaded = np.fromfile(file=fd,dtype=np.uint8) teY = loaded[8:].reshape((10000)).astype(np.float) trY = np.asarray(trY) teY = np.asarray(teY) X = np.concatenate((trX, teX), axis=0) y = np.concatenate((trY, teY), axis=0).astype(np.int) seed = 547 np.random.seed(seed) np.random.shuffle(X) np.random.seed(seed) np.random.shuffle(y) y_vec = np.zeros((len(y), self.y_dim), dtype=np.float) for i, label in enumerate(y): y_vec[i,y[i]] = 1.0 return X/255.,y_vec @property def model_dir(self): return \"{}_{}_{}_{}\".format( self.dataset_name, self.batch_size, self.output_height, self.output_width) def save(self, checkpoint_dir, step): model_name = \"DCGAN.model\" checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir) if not os.path.exists(checkpoint_dir): os.makedirs(checkpoint_dir) self.saver.save(self.sess, os.path.join(checkpoint_dir, model_name), global_step=step) def load(self, checkpoint_dir): print(\" [*] Reading checkpoints...\") checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir) ckpt = tf.train.get_checkpoint_state(checkpoint_dir) if ckpt and ckpt.model_checkpoint_path: ckpt_name = os.path.basename(ckpt.model_checkpoint_path) self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name)) print(\" [*] Success to read {}\".format(ckpt_name)) return True else: print(\" [*] Failed to find a checkpoint\") return False —|—","tags":["Python"],"categories":["深度学习"]},{"title":"DFS_BFS","path":"/2019/11/17/20191117-DFS-BFS/","content":"#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/11/17 10:28# @Author : caius# @Site :# @File : BFS.py# @Software: PyCharmgraph = { \"A\":{\"B\", \"C\"}, \"B\":{\"A\",\"C\",\"D\"}, \"C\":{\"A\",\"B\",\"D\",\"E\"}, \"D\":{\"B\",\"C\",\"E\",\"F\"}, \"E\":{\"C\",\"D\"}, \"F\":{\"D\"}}# 字典的基本用法# keys: A B C D E F# graph[\"E\"} \"c\". \"D\"def BFS(graph, s): # 队列先进先出 queue=[] queue.append(s) seen = set()# 代表这个东西是个set seen.add(s) parrent ={} parrent={s:None} while(len(queue)0): vertex = queue.pop(0) nodes = graph[vertex] for w in nodes: if w not in seen: queue.append(w) seen.add(w) parrent[w] = vertex print(vertex) return parrentdef DFS(graph, s): # 队列先进先出 stack=[] stack.append(s) seen = set()# 代表这个东西是个set seen.add(s) while(len(stack)0): vertex = stack.pop() nodes = graph[vertex] for w in nodes: if w not in seen: stack.append(w) seen.add(w) print(vertex)DFS(graph,\"E\")parrent = BFS(graph,'E')for key in parrent: print(key,parrent[key])v = \"B\"count=-1while v!= None: print(v) v = parrent[v] count+=1print(\"count: {} 次\".format(count))","tags":["算法"],"categories":["算法"]},{"title":"dynamic planning","path":"/2019/11/17/20191117-dynamic/","content":"arr= [1,2,4,1,7,8,3]def rec_opt(arr,i): if i == 0: return arr[0] elif i == 1: return max(arr[0],arr[1]) else: A = rec_opt(arr,i-2)+arr[i] B = rec_opt(arr,i-1) return max(A,B)print(rec_opt(arr,6)) 递归会产生很多的重叠的子问题，运算规模2^n#非递归的方法import numpy as npdef dp_opt(arr): opt = np.zeros(len(arr)) opt[0] = arr[0] opt[1] = max(arr[0], arr[1]) for i in range(2, len(arr)): A = opt[i-2] +arr[i] B = opt[i-1] opt[i] = max(A,B) return opt[len(arr)-1]dp_opt(arr) arr = [3,34,4,12,5,2]def rec_subset(arr, i,s): if s == 0: return True elif i == 0: return arr[0] == s elif arr[i]s: return rec_subset(arr,i-1,s) else: A= rec_subset(arr,i-1, s-arr[i]) B = rec_subset(arr, i-1,s) return A or Brec_subset(arr, len(arr)-1,9)~ # 非递归的方法，用二维数组来保存def dp_subset(arr,s): subset = np.zeros((len(arr),s+1),dtype=bool) subset[:,0] = True subset[0,:] = Fasle subset[0,arr[0]] = True for i in range(1,len(arr)): for ss in range(1,s+1): if arr[i] ss: subset[i,ss] = subset[i-1,ss] else: A= subset(i-1, s-arr[i]) B = subset(i-1,s) subset[i,s] = A or B r ,c =subset.shape return subset[r-1,c-1]rec_subset(arr, len(arr)-1,9)","tags":["算法"],"categories":["算法"]},{"title":"Duplicate","path":"/2019/11/17/20191117-Duplicate/","content":"是否存在相同元素，python3用字典的方式解决 class Solution(object): def containsDuplicate(self, nums): \"\"\" :type nums: List[int] :rtype: bool \"\"\" # 方法3：数字存字典 dic = {} for i in nums: dic[i] = dic.get(i, 0) + 1 if dic[i] 1: return True return False 最长回文子串 class Solution: def longestPalindrome(self, s: str) - str: # 两种判断条件 # DP 动态规划 # CABAC # B # ABA # CABAC palindrome = '' for i in range(len(s)): aa1 = self.getlongestpalindrome(s,i,i) len1 = len(aa1) if len1len(palindrome): palindrome = aa1 aa2 = self.getlongestpalindrome(s,i,i+1) len2 = len(aa2) if len2len(palindrome): palindrome = aa2 return palindrome def getlongestpalindrome(self, s, l, r): while l = 0 and rlen(s) and s[l]==s[r]: l -= 1 r += 1 return s[l+1:r]~ class Solution: def containsNearbyDuplicate(self, nums: List[int], k: int) - bool: # 用dictionary来做比较容易一点 lookup = {} for i , num in enumerate(nums): if num not in lookup: lookup[num] = i # 将num存到dic里面 else: if i-lookup[num]=k: return True lookup[num] = i return False~","tags":["算法"],"categories":["算法"]},{"title":"rnn","path":"/2019/11/17/20191117-rnn/","content":"RNN及RNN的几个变体序列形的数据不太好用原始的神经网络处理，为了建模序列问题，RNN引入了隐状态h（hidden）的概念，h可以对序列形的数据提取特征，接着转换为输出。 圆圈或方块表示的是向量 一个箭头就表示对该向量做一次变换。如上图中h0和x1分别有一个箭头连接，就表示对h0和x1各做了一次变换 Tips：U、W、b都是一样的，每个步骤的参数都是共享的，这是RNN的重要特点。 依次计算剩下来的（使用相同的参数U、W、b）：输出值的方法就是直接通过h进行计算：一个箭头就表示对对应的向量做一次类似于f(Wx+b)的变换，这里的这个箭头就表示对h1进行一次变换，得到输出y1输入是x1, x2, …..xn，输出为y1, y2, …yn，也就是说，输入和输出序列必须要是等长的。一些问题适合用经典的RNN结构建模，如： 计算视频中每一帧的分类标签。因为要对每一帧进行计算，因此输入和输出序列等长。 输入为字符，输出为下一个字符的概率。这就是著名的Char RNN（详细介绍请参考：The Unreasonable Effectiveness of Recurrent Neural Networks）。 N vs 1 多输入单输出这种结构通常用来处理序列分类问题。如输入一段文字判别它所属的类别，输入一个句子判断其情感倾向，输入一段视频并判断它的类别等等。 1 VS N 单输入多输出输入不是序列而输出为序列的情况怎么处理？我们可以只在序列开始进行输入计算：还有一种结构是把输入x作为每个阶段的输入：这种结构处理的问题： 从图像生成文字（image caption），此时输入的X就是图像的特征，而输出的y序列就是一段句子 从类别生成语音或音乐等 N vs MRNN最重要的一个变种：N vs M。这种结构又叫Encoder-Decoder模型，也可以称之为Seq2Seq模型。为此，Encoder-Decoder结构先将输入数据编码成一个上下文向量c：c有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给c，还可以对最后的隐状态做一个变换得到c，也可以对所有的隐状态做变换。 拿到c之后，就用另一个RNN网络对其进行解码，这部分RNN网络被称为Decoder。具体做法就是将c当做之前的初始状态h0输入到Decoder中：还有一种做法是将c当做每一步的输入：Encoder-Decoder结构不限制输入和输出的序列长度，因此应用的范围非常广泛，比如： 机器翻译。Encoder-Decoder的最经典应用，事实上这一结构就是在机器翻译领域最先提出的 文本摘要。输入是一段文本序列，输出是这段文本序列的摘要序列 阅读理解。将输入的文章和问题分别编码，再对其进行解码得到问题的答案。 语音识别。输入是语音信号序列，输出是文字序列。 ……… tensorflow 实现RNNRNNCellRNNCell 是TensorFlow中实现RNN的基本单元，每个RNNCell都有一个call方法，使用方式是：(output, next_state) = call(input, state)借助图片来说可能更容易理解。假设我们有一个初始状态h0，还有输入x1，调用call(x1, h0)后就可以得到(output1, h1)：再调用一次call(x2, h1)就可以得到(output2, h2)：也就是说，每调用一次RNNCell的call方法，就相当于在时间上“推进了一步”，这就是RNNCell的基本功能。RNNCell只是一个抽象类，其他的RNNcell都会继承该方法，然后具体实现其中的call()函数。主要有state_size和output_size两个属性，分别代表了隐藏层和输出层的维度。然后就是zero_state()和call()两个函数，分别用于初始化初始状态h0为全零向量和定义实际的RNNCell的操作（比如RNN就是一个激活，GRU的两个门，LSTM的三个门控等，不同的RNN的区别主要体现在这个函数）。 BasicRNNCell把state_size和output_size定义成相同，而且ht和output也是相同的最普通的RNN定义方式。也就是说output = new_state = f(W input + U state + B) GRU相比BasicRNNCell只改变了call函数部分，增加了重置门和更新门两部分，分别由r和u表示。然后c表示要更新的状态值。其对应的公式如如下所示： r = f(W1 input + U1 state + B1)r=f(W1∗input+U1∗state+B1)u = f(W2 input + U2 state + B2)u=f(W2∗input+U2∗state+B2)c = f(W3 input + U3 r _state + B3)c=f(W3∗input+U3∗r∗state+B3)h_new = u _ h + (1 - u) * chn​ew=u∗h+(1−u)∗c BasicLSTMCell相比GRU，LSTM又多了一个输出门，而且又新增添了一个C表示其内部状态，然后将h和c以tuple的形式返回作为LSTM内部的状态变量。 我们用的时候都是用的它的两个子类BasicRNNCell和BasicLSTMCell。顾名思义，前者是RNN的基础类，后者是LSTM的基础类。看下RNNCell、BasicRNNCell、BasicLSTMCell这三个类的注释部分，应该就可以理解它们的功能了。除了call方法外，对于RNNCell，还有两个类属性比较重要： state_size output_size前者是隐层的大小，后者是输出的大小。比如我们通常是将一个batch送入模型计算，设输入数据的形状为(batch_size, input_size)，那么计算时得到的隐层状态就是(batch_size, state_size)，输出就是(batch_size, output_size)。 In [2]: import numpy as npIn [3]: cell = tf.nn.rnn_cell.BasicRNNCell(num_units=128) # state_size = 128In [4]: print(cell.state_size) # 128 In [5]: inputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 是 batch_size In [6]: h0 = cell.zero_state(32, np.float32) # 通过zero_state得到一个全0的初始状态，形状为(batch_size, state_size) In [7]: output, h1 = cell.call(inputs, h0) #调用call函数 --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) ipython-input-7-378fe3b1c400 in module ---- 1 output, h1 = cell.call(inputs, h0) #调用call函数 ~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)```python 350 gate_inputs = math_ops.matmul( -- 351 array_ops.concat([inputs, state], 1), self._kernel) 352 gate_inputs = nn_ops.bias_add(gate_inputs, self._bias) 353 output = self._activation(gate_inputs) AttributeError: 'BasicRNNCell' object has no attribute '_kernel' In [8]: output, h1 = cell.__call__(inputs, h0) #调用call函数 In [9]: print(h1.shape) # (32, 128) (32, 128) In [10]: BasicLSTMCell In [10]: lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=128)In [11]: inputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 是 batch_sizeIn [12]: h0 = lstm_cell.zero_state(32, np.float32) # 通过zero_state得到一个全0的初始状态In [13]: output, h1 = lstm_cell.__call__(inputs, h0)In [14]: print(h1.h) # shape=(32, 128)Tensor(\"basic_lstm_cell/Mul_2:0\", shape=(32, 128), dtype=float32)In [15]: print(h1.c) # shape=(32, 128)Tensor(\"basic_lstm_cell/Add_1:0\", shape=(32, 128), dtype=float32) 参考链接：https://zhuanlan.zhihu.com/p/28054589https://zhuanlan.zhihu.com/p/28196873","tags":["深度学习"],"categories":["深度学习"]},{"title":"python_learn","path":"/2019/11/17/20191117-python-learn/","content":"查找图中两个节点的最小的距离这里面使用了python的优先队列，这里的队列按照后面的数值大小进行排序，而不是像普通的队列一样先进先出。后面的数值，是节点到出发节点的距离长度。 #!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/11/17 11:50# @Author : caius# @Site :# @File : find_min_bfs.py# @Software: PyCharmimport heapqimport math# pqueue = []# heapq.heappush(pqueue,(1,\"A\"))# heapq.heappush(pqueue,(7,\"B\"))# heapq.heappush(pqueue,(3,\"C\"))# heapq.heappush(pqueue,(6,\"D\"))# heapq.heappush(pqueue,(2,\"E\"))graph = { \"A\":{\"B\": 5, \"C\": 1}, \"B\":{\"A\": 5,\"C\": 2,\"D\": 1}, \"C\":{\"A\": 1,\"B\": 2,\"D\": 4,\"E\": 8}, \"D\":{\"B\": 1,\"C\": 4,\"E\": 3,\"F\": 6}, \"E\":{\"C\": 8,\"D\": 3}, \"F\":{\"D\": 6}}def init_distance(graph,s): distance = {s:0} for vertex in graph: if vertex != s: distance[vertex] = math.inf return distancedef dijkstra(graph, s): pqueue = [] heapq.heappush(pqueue,(0,s)) seen =set() parent ={s:None} distance = init_distance(graph,s) while(len(pqueue)0): pair = heapq.heappop(pqueue) # 拿到一对点，pair dist = pair[0] vertex = pair[1] seen.add(vertex) nodes = graph[vertex].keys() for w in nodes: if w not in seen: if dist+graph[vertex][w] distance[w]: heapq.heappush(pqueue,(dist+graph[vertex][w],w)) parent[w] = vertex distance[w] = dist+graph[vertex][w] return parent,distanceparent, distance = dijkstra(graph,\"A\")print(parent)print(distance) python 装饰器装饰器(Decorators)是 Python 的一个重要部分。简单地说：他们是修改其他函数的功能的函数。他们有助于让我们的代码更简短，也更Pythonic（Python范儿）。装饰器可以让你的代码更简洁。 #!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/11/17 9:59# @Author : caius# @Site :# @File : deco.py.py# @Software: PyCharmimport time# 装饰器def display_time(func): def wrapper(*args): t1 = time.time() result = func(*args) t2 = time.time() print(\"Total time: {:.4} s\".format(t2-t1)) return result return wrapper# 输出质数def is_prime(num): if num2: return False elif num==2: return True else: for i in range(2, num): if num%i ==0: return False return True@display_timedef prime_nums(): for i in range(2,10000): if is_prime(i): print(i)@display_timedef count_prime_nums(maxnum): count = 0 for i in range(2, maxnum): if is_prime(i): count += 1 return countcount = count_prime_nums(5000)print(count)~ 用turtle 画图#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/11/17 13:27# @Author : caius# @Site :# @File : draw.py# @Software: PyCharmfrom turtle import *import math# turtle 画图# forward(100)# left(90)# forward(100)# left(90)# forward(100)# left(90)# forward(100)# left(90)# exitonclick()# 不点击窗口的话就不会退出# # 画等边三角形# forward(100)# left(120)# forward(100)# left(120)# forward(100)# left(120)# exitonclick()# 不点击窗口的话就不会退出# 画五角星# forward(100)# right(180-36)# forward(100)# right(180-36)# forward(100)# right(180-36)# forward(100)# right(180-36)# forward(100)# right(180-36)# for i in range(5):# forward(100)# right(180-36)#angle = 360/8length = 100speed(0)for i in range(8): if i %2==0: color('yellow') else: color('red') begin_fill() forward(100) left(angle) forward(length) left(180-angle) forward(length) left(angle) forward(length) left(180-angle) end_fill() left(angle)forward(length)left(180-(180-angle)/2)alpha = angle*3.1415926536 /180step = 2*length*math.sin(alpha/2)color('blue')begin_fill()for i in range(8): forward(step) left(angle)end_fill()exitonclick()# 不点击窗口的话就不会退出 #!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/11/17 14:20# @Author : caius# @Site :# @File : Lsystem2.py# @Software: PyCharmfrom turtle import *length = 7angle = 60def split_path(path): i = 0 list = [] while i len(path): if path[i] == \"F\": list.append(path[i:i+2]) i = i+2 else: list.append(path[i]) i = i+1 return listdef apply_rule(path, rules): lst = split_path(path) for i in range(len(lst)): symbol = lst[i] if symbol in rules: lst[i] = rules[symbol] path =\"\".join(symbol for symbol in lst) return pathrules={ \"Fl\": \"Fr+Fl+Fr\", \"Fr\":\"Fl-Fr-Fl\"}def draw_patj(path): lst = split_path(path) for symbol in lst: if symbol ==\"Fl\" or symbol=='Fr': forward(length) elif symbol==\"-\": left(angle) elif symbol=='+': right(angle)speed(0)path = 'Fr'# speed(0)##lst = split_path(path)for i in range(6): path = apply_rule(path,rules)print(path)draw_patj(path)exitonclick()~ python 类的构造#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/11/17 14:52# @Author : caius# @Site :# @File : Bank.py# @Software: PyCharmclass BankAccount: # Constructor 构造器 def __init__(self,accountNumber, accountName, balance): self.accountNumber = accountNumber self.accountName = accountName self.balance = balance def __str__(self): return \"(name: {}, balance: {})\".format(self.accountName,self.balance)#!/usr/bin/env python# -*- coding: utf-8 -*-# @Time : 2019/11/17 14:56# @Author : caius# @Site :# @File : main.py# @Software: PyCharmfrom Bank import BankAccountb1 = BankAccount(\"56789\",\"Tony\", 100.0)print((b1))","tags":["Python"],"categories":["Python"]},{"title":"DFS_BFS","path":"/2019/11/17/DFS-BFS/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 | #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/17 10:28 # @Author : caius # @Site : # @File : BFS.py # @Software: PyCharm graph = { \"A\":{\"B\", \"C\"}, \"B\":{\"A\",\"C\",\"D\"}, \"C\":{\"A\",\"B\",\"D\",\"E\"}, \"D\":{\"B\",\"C\",\"E\",\"F\"}, \"E\":{\"C\",\"D\"}, \"F\":{\"D\"} } # 字典的基本用法 # keys: A B C D E F # graph[\"E\"} \"c\". \"D\" def BFS(graph, s): # 队列先进先出 queue=[] queue.append(s) seen = set()# 代表这个东西是个set seen.add(s) parrent ={} parrent={s:None} while(len(queue)0): vertex = queue.pop(0) nodes = graph[vertex] for w in nodes: if w not in seen: queue.append(w) seen.add(w) parrent[w] = vertex print(vertex) return parrent def DFS(graph, s): # 队列先进先出 stack=[] stack.append(s) seen = set()# 代表这个东西是个set seen.add(s) while(len(stack)0): vertex = stack.pop() nodes = graph[vertex] for w in nodes: if w not in seen: stack.append(w) seen.add(w) print(vertex) DFS(graph,\"E\") parrent = BFS(graph,'E') for key in parrent: print(key,parrent[key]) v = \"B\" count=-1 while v!= None: print(v) v = parrent[v] count+=1 print(\"count: {} 次\".format(count)) —|—","tags":["Python"],"categories":["算法"]},{"title":"Duplicate","path":"/2019/11/17/Duplicate/","content":"是否存在相同元素，python3用字典的方式解决 1 2 3 4 5 6 7 8 9 10 11 12 13 | class Solution(object): def containsDuplicate(self, nums): \"\"\" :type nums: List[int] :rtype: bool \"\"\" # 方法3：数字存字典 dic = {} for i in nums: dic[i] = dic.get(i, 0) + 1 if dic[i] 1: return True return False —|— 最长回文子串 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 | class Solution: def longestPalindrome(self, s: str) - str: # 两种判断条件 # DP 动态规划 # CABAC # B # ABA # CABAC palindrome = '' for i in range(len(s)): aa1 = self.getlongestpalindrome(s,i,i) len1 = len(aa1) if len1len(palindrome): palindrome = aa1 aa2 = self.getlongestpalindrome(s,i,i+1) len2 = len(aa2) if len2len(palindrome): palindrome = aa2 return palindrome def getlongestpalindrome(self, s, l, r): while l = 0 and rlen(s) and s[l]==s[r]: l -= 1 r += 1 return s[l+1:r] ~ —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | class Solution: def containsNearbyDuplicate(self, nums: List[int], k: int) - bool: # 用dictionary来做比较容易一点 lookup = {} for i , num in enumerate(nums): if num not in lookup: lookup[num] = i # 将num存到dic里面 else: if i-lookup[num]=k: return True lookup[num] = i return False ~ —|—","tags":["Python"],"categories":["算法"]},{"title":"dynamic planning","path":"/2019/11/17/dynamic/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 | arr= [1,2,4,1,7,8,3] def rec_opt(arr,i): if i == 0: return arr[0] elif i == 1: return max(arr[0],arr[1]) else: A = rec_opt(arr,i-2)+arr[i] B = rec_opt(arr,i-1) return max(A,B) print(rec_opt(arr,6)) —|— 递归会产生很多的重叠的子问题，运算规模2^n1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | #非递归的方法 import numpy as np def dp_opt(arr): opt = np.zeros(len(arr)) opt[0] = arr[0] opt[1] = max(arr[0], arr[1]) for i in range(2, len(arr)): A = opt[i-2] +arr[i] B = opt[i-1] opt[i] = max(A,B) return opt[len(arr)-1] dp_opt(arr) —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | arr = [3,34,4,12,5,2] def rec_subset(arr, i,s): if s == 0: return True elif i == 0: return arr[0] == s elif arr[i]s: return rec_subset(arr,i-1,s) else: A= rec_subset(arr,i-1, s-arr[i]) B = rec_subset(arr, i-1,s) return A or B rec_subset(arr, len(arr)-1,9) ~ —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 | # 非递归的方法，用二维数组来保存 def dp_subset(arr,s): subset = np.zeros((len(arr),s+1),dtype=bool) subset[:,0] = True subset[0,:] = Fasle subset[0,arr[0]] = True for i in range(1,len(arr)): for ss in range(1,s+1): if arr[i] ss: subset[i,ss] = subset[i-1,ss] else: A= subset(i-1, s-arr[i]) B = subset(i-1,s) subset[i,s] = A or B r ,c =subset.shape return subset[r-1,c-1] rec_subset(arr, len(arr)-1,9) —|—","tags":["Python"],"categories":["算法"]},{"title":"python_learn","path":"/2019/11/17/python-learn/","content":"查找图中两个节点的最小的距离这里面使用了python的优先队列，这里的队列按照后面的数值大小进行排序，而不是像普通的队列一样先进先出。后面的数值，是节点到出发节点的距离长度。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 | #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/17 11:50 # @Author : caius # @Site : # @File : find_min_bfs.py # @Software: PyCharm import heapq import math # pqueue = [] # heapq.heappush(pqueue,(1,\"A\")) # heapq.heappush(pqueue,(7,\"B\")) # heapq.heappush(pqueue,(3,\"C\")) # heapq.heappush(pqueue,(6,\"D\")) # heapq.heappush(pqueue,(2,\"E\")) graph = { \"A\":{\"B\": 5, \"C\": 1}, \"B\":{\"A\": 5,\"C\": 2,\"D\": 1}, \"C\":{\"A\": 1,\"B\": 2,\"D\": 4,\"E\": 8}, \"D\":{\"B\": 1,\"C\": 4,\"E\": 3,\"F\": 6}, \"E\":{\"C\": 8,\"D\": 3}, \"F\":{\"D\": 6} } def init_distance(graph,s): distance = {s:0} for vertex in graph: if vertex != s: distance[vertex] = math.inf return distance def dijkstra(graph, s): pqueue = [] heapq.heappush(pqueue,(0,s)) seen =set() parent ={s:None} distance = init_distance(graph,s) while(len(pqueue)0): pair = heapq.heappop(pqueue) # 拿到一对点，pair dist = pair[0] vertex = pair[1] seen.add(vertex) nodes = graph[vertex].keys() for w in nodes: if w not in seen: if dist+graph[vertex][w] distance[w]: heapq.heappush(pqueue,(dist+graph[vertex][w],w)) parent[w] = vertex distance[w] = dist+graph[vertex][w] return parent,distance parent, distance = dijkstra(graph,\"A\") print(parent) print(distance) —|— python 装饰器装饰器(Decorators)是 Python 的一个重要部分。简单地说：他们是修改其他函数的功能的函数。他们有助于让我们的代码更简短，也更Pythonic（Python范儿）。装饰器可以让你的代码更简洁。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 | #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/17 9:59 # @Author : caius # @Site : # @File : deco.py.py # @Software: PyCharm import time # 装饰器 def display_time(func): def wrapper(*args): t1 = time.time() result = func(*args) t2 = time.time() print(\"Total time: {:.4} s\".format(t2-t1)) return result return wrapper # 输出质数 def is_prime(num): if num2: return False elif num==2: return True else: for i in range(2, num): if num%i ==0: return False return True @display_time def prime_nums(): for i in range(2,10000): if is_prime(i): print(i) @display_time def count_prime_nums(maxnum): count = 0 for i in range(2, maxnum): if is_prime(i): count += 1 return count count = count_prime_nums(5000) print(count) ~ —|— 用turtle 画图1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 | #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/17 13:27 # @Author : caius # @Site : # @File : draw.py # @Software: PyCharm from turtle import * import math # turtle 画图 # forward(100) # left(90) # forward(100) # left(90) # forward(100) # left(90) # forward(100) # left(90) # exitonclick()# 不点击窗口的话就不会退出 # # 画等边三角形 # forward(100) # left(120) # forward(100) # left(120) # forward(100) # left(120) # exitonclick()# 不点击窗口的话就不会退出 # 画五角星 # forward(100) # right(180-36) # forward(100) # right(180-36) # forward(100) # right(180-36) # forward(100) # right(180-36) # forward(100) # right(180-36) # for i in range(5): # forward(100) # right(180-36) # angle = 360/8 length = 100 speed(0) for i in range(8): if i %2==0: color('yellow') else: color('red') begin_fill() forward(100) left(angle) forward(length) left(180-angle) forward(length) left(angle) forward(length) left(180-angle) end_fill() left(angle) forward(length) left(180-(180-angle)/2) alpha = angle*3.1415926536 /180 step = 2*length*math.sin(alpha/2) color('blue') begin_fill() for i in range(8): forward(step) left(angle) end_fill() exitonclick()# 不点击窗口的话就不会退出 —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 | #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/17 14:20 # @Author : caius # @Site : # @File : Lsystem2.py # @Software: PyCharm from turtle import * length = 7 angle = 60 def split_path(path): i = 0 list = [] while i len(path): if path[i] == \"F\": list.append(path[i:i+2]) i = i+2 else: list.append(path[i]) i = i+1 return list def apply_rule(path, rules): lst = split_path(path) for i in range(len(lst)): symbol = lst[i] if symbol in rules: lst[i] = rules[symbol] path =\"\".join(symbol for symbol in lst) return path rules={ \"Fl\": \"Fr+Fl+Fr\", \"Fr\":\"Fl-Fr-Fl\" } def draw_patj(path): lst = split_path(path) for symbol in lst: if symbol ==\"Fl\" or symbol=='Fr': forward(length) elif symbol==\"-\": left(angle) elif symbol=='+': right(angle) speed(0) path = 'Fr' # speed(0) # #lst = split_path(path) for i in range(6): path = apply_rule(path,rules) print(path) draw_patj(path) exitonclick() ~ —|— python 类的构造1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 | #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/17 14:52 # @Author : caius # @Site : # @File : Bank.py # @Software: PyCharm class BankAccount: # Constructor 构造器 def __init__(self,accountNumber, accountName, balance): self.accountNumber = accountNumber self.accountName = accountName self.balance = balance def __str__(self): return \"(name: {}, balance: {})\".format(self.accountName,self.balance) #!/usr/bin/env python # -*- coding: utf-8 -*- # @Time : 2019/11/17 14:56 # @Author : caius # @Site : # @File : main.py # @Software: PyCharm from Bank import BankAccount b1 = BankAccount(\"56789\",\"Tony\", 100.0) print((b1)) —|—","tags":["Python"],"categories":["深度学习"]},{"title":"rnn","path":"/2019/11/17/rnn/","content":"RNN及RNN的几个变体序列形的数据不太好用原始的神经网络处理，为了建模序列问题，RNN引入了隐状态h（hidden）的概念，h可以对序列形的数据提取特征，接着转换为输出。 圆圈或方块表示的是向量 一个箭头就表示对该向量做一次变换。如上图中h0和x1分别有一个箭头连接，就表示对h0和x1各做了一次变换 Tips：U、W、b都是一样的，每个步骤的参数都是共享的，这是RNN的重要特点。 依次计算剩下来的（使用相同的参数U、W、b）：输出值的方法就是直接通过h进行计算：一个箭头就表示对对应的向量做一次类似于f(Wx+b)的变换，这里的这个箭头就表示对h1进行一次变换，得到输出y1输入是x1, x2, …..xn，输出为y1, y2, …yn，也就是说，输入和输出序列必须要是等长的。一些问题适合用经典的RNN结构建模，如： 计算视频中每一帧的分类标签。因为要对每一帧进行计算，因此输入和输出序列等长。 输入为字符，输出为下一个字符的概率。这就是著名的Char RNN（详细介绍请参考：The Unreasonable Effectiveness of Recurrent Neural Networks）。 N vs 1 多输入单输出这种结构通常用来处理序列分类问题。如输入一段文字判别它所属的类别，输入一个句子判断其情感倾向，输入一段视频并判断它的类别等等。 1 VS N 单输入多输出输入不是序列而输出为序列的情况怎么处理？我们可以只在序列开始进行输入计算：还有一种结构是把输入x作为每个阶段的输入：这种结构处理的问题： 从图像生成文字（image caption），此时输入的X就是图像的特征，而输出的y序列就是一段句子 从类别生成语音或音乐等 N vs MRNN最重要的一个变种：N vs M。这种结构又叫Encoder-Decoder模型，也可以称之为Seq2Seq模型。为此，Encoder-Decoder结构先将输入数据编码成一个上下文向量c：c有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给c，还可以对最后的隐状态做一个变换得到c，也可以对所有的隐状态做变换。 拿到c之后，就用另一个RNN网络对其进行解码，这部分RNN网络被称为Decoder。具体做法就是将c当做之前的初始状态h0输入到Decoder中：还有一种做法是将c当做每一步的输入：Encoder-Decoder结构不限制输入和输出的序列长度，因此应用的范围非常广泛，比如： 机器翻译。Encoder-Decoder的最经典应用，事实上这一结构就是在机器翻译领域最先提出的 文本摘要。输入是一段文本序列，输出是这段文本序列的摘要序列 阅读理解。将输入的文章和问题分别编码，再对其进行解码得到问题的答案。 语音识别。输入是语音信号序列，输出是文字序列。 ……… tensorflow 实现RNNRNNCellRNNCell 是TensorFlow中实现RNN的基本单元，每个RNNCell都有一个call方法，使用方式是：(output, next_state) = call(input, state)借助图片来说可能更容易理解。假设我们有一个初始状态h0，还有输入x1，调用call(x1, h0)后就可以得到(output1, h1)：再调用一次call(x2, h1)就可以得到(output2, h2)：也就是说，每调用一次RNNCell的call方法，就相当于在时间上“推进了一步”，这就是RNNCell的基本功能。RNNCell只是一个抽象类，其他的RNNcell都会继承该方法，然后具体实现其中的call()函数。主要有state_size和output_size两个属性，分别代表了隐藏层和输出层的维度。然后就是zero_state()和call()两个函数，分别用于初始化初始状态h0为全零向量和定义实际的RNNCell的操作（比如RNN就是一个激活，GRU的两个门，LSTM的三个门控等，不同的RNN的区别主要体现在这个函数）。 BasicRNNCell把state_size和output_size定义成相同，而且ht和output也是相同的最普通的RNN定义方式。也就是说output = new_state = f(W input + U state + B) GRU相比BasicRNNCell只改变了call函数部分，增加了重置门和更新门两部分，分别由r和u表示。然后c表示要更新的状态值。其对应的公式如如下所示： r = f(W1 input + U1 state + B1)r=f(W1∗input+U1∗state+B1)u = f(W2 input + U2 state + B2)u=f(W2∗input+U2∗state+B2)c = f(W3 input + U3 r _state + B3)c=f(W3∗input+U3∗r∗state+B3)h_new = u _ h + (1 - u) * chn​ew=u∗h+(1−u)∗c BasicLSTMCell相比GRU，LSTM又多了一个输出门，而且又新增添了一个C表示其内部状态，然后将h和c以tuple的形式返回作为LSTM内部的状态变量。 我们用的时候都是用的它的两个子类BasicRNNCell和BasicLSTMCell。顾名思义，前者是RNN的基础类，后者是LSTM的基础类。看下RNNCell、BasicRNNCell、BasicLSTMCell这三个类的注释部分，应该就可以理解它们的功能了。除了call方法外，对于RNNCell，还有两个类属性比较重要： state_size output_size前者是隐层的大小，后者是输出的大小。比如我们通常是将一个batch送入模型计算，设输入数据的形状为(batch_size, input_size)，那么计算时得到的隐层状态就是(batch_size, state_size)，输出就是(batch_size, output_size)。 1234567891011121314151617181920212223242526272829303132 | In [2]: import numpy as np In [3]: cell = tf.nn.rnn_cell.BasicRNNCell(num_units=128) # state_size = 128 In [4]: print(cell.state_size) # 128 128 In [5]: inputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 是 batch_size In [6]: h0 = cell.zero_state(32, np.float32) # 通过zero_state得到一个全0的初始状态，形状为(batch_size, state_size) In [7]: output, h1 = cell.call(inputs, h0) #调用call函数 --------------------------------------------------------------------------- AttributeError Traceback (most recent call last) ipython-input-7-378fe3b1c400 in module ---- 1 output, h1 = cell.call(inputs, h0) #调用call函数 ~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state) 349 350 gate_inputs = math_ops.matmul( -- 351 array_ops.concat([inputs, state], 1), self._kernel) 352 gate_inputs = nn_ops.bias_add(gate_inputs, self._bias) 353 output = self._activation(gate_inputs) AttributeError: 'BasicRNNCell' object has no attribute '_kernel' In [8]: output, h1 = cell.__call__(inputs, h0) #调用call函数 In [9]: print(h1.shape) # (32, 128) (32, 128) In [10]: —|— BasicLSTMCell 1 2 3 4 5 6 7 8 9 10 11 12 13 | In [10]: lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=128) In [11]: inputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 是 batch_size In [12]: h0 = lstm_cell.zero_state(32, np.float32) # 通过zero_state得到一个全0的初始状态 In [13]: output, h1 = lstm_cell.__call__(inputs, h0) In [14]: print(h1.h) # shape=(32, 128) Tensor(\"basic_lstm_cell/Mul_2:0\", shape=(32, 128), dtype=float32) In [15]: print(h1.c) # shape=(32, 128) Tensor(\"basic_lstm_cell/Add_1:0\", shape=(32, 128), dtype=float32) —|— 参考链接：https://zhuanlan.zhihu.com/p/28054589https://zhuanlan.zhihu.com/p/28196873","tags":["TensorFlow"],"categories":["深度学习"]},{"title":"文本检测调研","path":"/2019/11/13/20191113-文本检测调研/","content":"文本检测问题，广义上来说可以看做是一个目标检测的问题，但是相当于目标检测要简单的多。因为目标检测往往除了背景还有其他的类，而文本检测，只需要检测背景和文本类两个问题。因此可以采用目标检测或者分割的方法来进行文本检测。而视频中的文本检测，也可以看做是视频中的目标检测中的一种，感觉应该也可以用视频中的目标检测+跟踪来做。 通常目标跟踪面临的极大难点：物体变形、亮度变化、快速移动、背景干扰覆盖。其中最主要的三个难题分别是目标背景的变化，物体本身的变化，光照强度的变化。 光流法帧间差分法背景差分法","tags":["计算机视觉"],"categories":["计算机视觉"]},{"title":"Python：logging模块","path":"/2019/11/04/20191104-Python：logging模块/","content":"In [12]: import logging ...: ...: logging.basicConfig(level=logging.DEBUG, ...: filename='output.log', ...: datefmt='%Y/%m/%d %H:%M:%S', ...: format='%(asctime)s - %(name)s - %(levelname)s - %(lineno)d - %(module)s - %(message)s') ...: logger = logging.getLogger(__name__) ...: ...: logger.info('This is a log info') ...: logger.debug('Debugging') ...: logger.warning('Warning exists') ...: logger.info('Finish') 2019-11-04 13:00:45,976 - main - INFO - This is a log info2019-11-04 13:00:45,977 - main - WARNING - Warning exists2019-11-04 13:00:45,977 - main - INFO - Finish 设置level等级，从而控制log输出的级别。 In [13]: import logging ...: ...: logging.basicConfig(level=logging.DEBUG, ...: filename='output.log', ...: datefmt='%Y/%m/%d %H:%M:%S', ...: format='%(asctime)s - %(name)s - %(levelname)s - %(lineno)d - %(module)s - %(message)s') ...: logger = logging.getLogger(__name__) ...: logger.setLevel(level=logging.DEBUG) ...: logger.info('This is a log info') ...: logger.debug('Debugging') ...: logger.warning('Warning exists') ...: logger.info('Finish') 如果不设置logger的Level的话， debug’的信息也不会被输出。 需要设置 logger.setLevel(level=logging.DEBUG)，然后信息就可以正常的显示出来了。 2019-11-04 13:10:01,634 - main - INFO - This is a log info2019-11-04 13:10:01,634 - main - DEBUG - Debugging2019-11-04 13:10:01,635 - main - WARNING - Warning exists2019-11-04 13:10:01,639 - main - INFO - Finish CSDN博客地址：https://blog.csdn.net/eilot_c/article/details/102894687","tags":["Python"],"categories":["Python"]},{"title":"Python：logging模块","path":"/2019/11/04/Python：logging模块/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 | In [12]: import logging ...: ...: logging.basicConfig(level=logging.DEBUG, ...: filename='output.log', ...: datefmt='%Y/%m/%d %H:%M:%S', ...: format='%(asctime)s - %(name)s - %(levelname)s - %(lineno)d - %(module)s - %(message)s') ...: logger = logging.getLogger(__name__) ...: ...: logger.info('This is a log info') ...: logger.debug('Debugging') ...: logger.warning('Warning exists') ...: logger.info('Finish') —|— 2019-11-04 13:00:45,976 - main - INFO - This is a log info2019-11-04 13:00:45,977 - main - WARNING - Warning exists2019-11-04 13:00:45,977 - main - INFO - Finish 设置level等级，从而控制log输出的级别。 1 2 3 4 5 6 7 8 9 10 11 12 | In [13]: import logging ...: ...: logging.basicConfig(level=logging.DEBUG, ...: filename='output.log', ...: datefmt='%Y/%m/%d %H:%M:%S', ...: format='%(asctime)s - %(name)s - %(levelname)s - %(lineno)d - %(module)s - %(message)s') ...: logger = logging.getLogger(__name__) ...: logger.setLevel(level=logging.DEBUG) ...: logger.info('This is a log info') ...: logger.debug('Debugging') ...: logger.warning('Warning exists') ...: logger.info('Finish') —|— 如果不设置logger的Level的话， debug’的信息也不会被输出。 需要设置 logger.setLevel(level=logging.DEBUG)，然后信息就可以正常的显示出来了。 2019-11-04 13:10:01,634 - main - INFO - This is a log info2019-11-04 13:10:01,634 - main - DEBUG - Debugging2019-11-04 13:10:01,635 - main - WARNING - Warning exists2019-11-04 13:10:01,639 - main - INFO - Finish CSDN博客地址：https://blog.csdn.net/eilot_c/article/details/102894687","tags":["Python"],"categories":["Python"]},{"title":"object_track_xmind","path":"/2019/10/31/20191031-pyobject-track-xmind/","content":"","tags":["技术"],"categories":["技术"]},{"title":"object_detection_xmind","path":"/2019/10/31/20191031-object-dectection-xmind/","content":"","tags":["目标检测"],"categories":["技术"]},{"title":"pytorch基础导图","path":"/2019/10/31/20191031-pytorchchch——-xmind/","content":"","tags":["深度学习"],"categories":["深度学习"]},{"title":"attention_mechanism_xmind","path":"/2019/10/31/20191031-attention-mechanism-xmind/","content":"","tags":["深度学习"],"categories":["深度学习"]},{"title":"attention_mechanism_xmind","path":"/2019/10/31/attention-mechanism-xmind/","content":"","tags":["Attention"],"categories":["其他"]},{"title":"object_detection_xmind","path":"/2019/10/31/object-dectection-xmind/","content":"","tags":["目标检测"],"categories":["计算机视觉"]},{"title":"object_track_xmind","path":"/2019/10/31/pyobject-track-xmind/","content":"","tags":["目标跟踪"],"categories":["计算机视觉"]},{"title":"pytorch基础导图","path":"/2019/10/31/pytorchchch——-xmind/","content":"","tags":["PyTorch"],"categories":["深度学习"]},{"title":"knn手写数字识别","path":"/2019/10/20/20191020-knn手写数字识别/","content":"K近邻算法算法的核心思想是，给定一个训练数据集，对于新的输入实例，在训练集中找到与该实例最近的K个实例，这K个实例的多数属于某个类，就把这个输入归为哪个类中。 # 1 重要# 2 KNN CNN 2种# 3 样本# 4 旧瓶装新酒 ：数字识别的不同# 4.1 网络 4。2 每一级 4.3 先原理 后代码# 本质：knn test 样本 K个 max4 3个1 -》1# 1 load Data 1.1 随机数 1.2 4组 训练 测试 （图片 和 标签）# 2 knn test train distance 5*500 = 2500 784=28*28# 3 knn k个最近的图片5 500 1-》500train （4）# 4 k个最近的图片- parse centent label# 5 label -》 数字 p9 测试图片-》数据# 6 检测概率统计import tensorflow as tfimport numpy as npimport randomfrom tensorflow.examples.tutorials.mnist import input_data# load data 2 one_hot : 1 0000 1 fileNamemnist = input_data.read_data_sets('MNIST_data',one_hot=True)# 属性设置trainNum = 55000testNum = 10000trainSize = 500testSize = 5k = 4# data 分解 1 trainSize 2范围0-trainNum 3 replace=FalsetrainIndex = np.random.choice(trainNum,trainSize,replace=False)testIndex = np.random.choice(testNum,testSize,replace=False)trainData = mnist.train.images[trainIndex]# 训练图片trainLabel = mnist.train.labels[trainIndex]# 训练标签testData = mnist.test.images[testIndex]testLabel = mnist.test.labels[testIndex]# 28*28 = 784print('trainData.shape=',trainData.shape)#500*784 1 图片个数 2 784?print('trainLabel.shape=',trainLabel.shape)#500*10print('testData.shape=',testData.shape)#5*784print('testLabel.shape=',testLabel.shape)#5*10print('testLabel=',testLabel)# 4 :testData [0] 3:testData[1] 6# tf input 784-imagetrainDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32)trainLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32)testDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32)testLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32)#knn distance 5*785. 5*1*784# 5 500 784 (3D) 2500*784f1 = tf.expand_dims(testDataInput,1) # 维度扩展 (?, 1, 784) # 0其实代表的第一维度，那么1代表第二维度，2代表第三维度。以此类推。print()f2 = tf.subtract(trainDataInput,f1)# 784 sum(784) 返回 x-y 的元素.f3 = tf.reduce_sum(tf.abs(f2),reduction_indices=2)# 完成数据累加 784 abs # axis=0时，按第一个维度求和，# 5*500f4 = tf.negative(f3)# 取反 取负值print('f1.shape=',f1.shape)#500*784 1 图片个数 2 784?print('f2.shape=',f2.shape)#500*10 (?, ?, 784)print('f3.shape=',f3.shape)#5*784 (?, ?)print('f4.shape=',f4.shape)#5*10 (?, ?)f5,f6 = tf.nn.top_k(f4,k=4) # 选取f4 最大的四个值 返回值和其索引位置# f3 最小的四个值# f6 index-trainLabelInputf7 = tf.gather(trainLabelInput,f6) # 根据索引抽取其中的张量# f8 num reduce_sum reduction_indices=1 '竖直'f8 = tf.reduce_sum(f7,reduction_indices=1)# tf.argmax 选取在某一个最大的值 indexf9 = tf.argmax(f8,dimension=1)# f9 - test5 image - 5 numwith tf.Session() as sess: # f1 - testData 5张图片 p1 = sess.run(f1,feed_dict={testDataInput:testData[0:5]}) print('p1=',p1.shape)# p1= (5, 1, 784) p2 = sess.run(f2,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print('p2=',p2.shape)#p2= (5, 500, 784) (1,100) p3 = sess.run(f3,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print('p3=',p3.shape)#p3= (5, 500) print('p3[0,0]=',p3[0,0]) #130.451 knn distance p3[0,0]= 155.812 p4 = sess.run(f4,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print('p4=',p4.shape) print('p4[0,0]',p4[0,0]) p5,p6 = sess.run((f5,f6),feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) #p5= (5, 4) 每一张测试图片（5张）分别对应4张最近训练图片 #p6= (5, 4) print('p5=',p5.shape) print('p6=',p6.shape) print('p5[0,0]',p5[0]) print('p6[0,0]',p6[0])# p6 index p7 = sess.run(f7,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print('p7=',p7.shape)#p7= (5, 4, 10) print('p7[]',p7) p8 = sess.run(f8,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print('p8=',p8.shape) print('p8[]=',p8) p9 = sess.run(f9,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print('p9=',p9.shape) print('p9[]=',p9) p10 = np.argmax(testLabel[0:5],axis=1) print('p10[]=',p10)j = 0for i in range(0,5): if p10[i] == p9[i]: j = j+1print('ac=',j*100/5) 参考链接：OpenCV+TensorFlow 入门人工智能图像处理 手写数字识别","tags":["机器学习"],"categories":["Python"]},{"title":"knn手写数字识别","path":"/2019/10/20/knn手写数字识别/","content":"K近邻算法算法的核心思想是，给定一个训练数据集，对于新的输入实例，在训练集中找到与该实例最近的K个实例，这K个实例的多数属于某个类，就把这个输入归为哪个类中。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 | # 1 重要 # 2 KNN CNN 2种 # 3 样本 # 4 旧瓶装新酒 ：数字识别的不同 # 4.1 网络 4。2 每一级 4.3 先原理 后代码 # 本质：knn test 样本 K个 max4 3个1 -》1 # 1 load Data 1.1 随机数 1.2 4组 训练 测试 （图片 和 标签） # 2 knn test train distance 5*500 = 2500 784=28*28 # 3 knn k个最近的图片5 500 1-》500train （4） # 4 k个最近的图片- parse centent label # 5 label -》 数字 p9 测试图片-》数据 # 6 检测概率统计 import tensorflow as tf import numpy as np import random from tensorflow.examples.tutorials.mnist import input_data # load data 2 one_hot : 1 0000 1 fileName mnist = input_data.read_data_sets('MNIST_data',one_hot=True) # 属性设置 trainNum = 55000 testNum = 10000 trainSize = 500 testSize = 5 k = 4 # data 分解 1 trainSize 2范围0-trainNum 3 replace=False trainIndex = np.random.choice(trainNum,trainSize,replace=False) testIndex = np.random.choice(testNum,testSize,replace=False) trainData = mnist.train.images[trainIndex]# 训练图片 trainLabel = mnist.train.labels[trainIndex]# 训练标签 testData = mnist.test.images[testIndex] testLabel = mnist.test.labels[testIndex] # 28*28 = 784 print('trainData.shape=',trainData.shape)#500*784 1 图片个数 2 784? print('trainLabel.shape=',trainLabel.shape)#500*10 print('testData.shape=',testData.shape)#5*784 print('testLabel.shape=',testLabel.shape)#5*10 print('testLabel=',testLabel)# 4 :testData [0] 3:testData[1] 6 # tf input 784-image trainDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32) trainLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32) testDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32) testLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32) #knn distance 5*785. 5*1*784 # 5 500 784 (3D) 2500*784 f1 = tf.expand_dims(testDataInput,1) # 维度扩展 (?, 1, 784) # 0其实代表的第一维度，那么1代表第二维度，2代表第三维度。以此类推。 print() f2 = tf.subtract(trainDataInput,f1)# 784 sum(784) 返回 x-y 的元素. f3 = tf.reduce_sum(tf.abs(f2),reduction_indices=2)# 完成数据累加 784 abs # axis=0时，按第一个维度求和， # 5*500 f4 = tf.negative(f3)# 取反 取负值 print('f1.shape=',f1.shape)#500*784 1 图片个数 2 784? print('f2.shape=',f2.shape)#500*10 (?, ?, 784) print('f3.shape=',f3.shape)#5*784 (?, ?) print('f4.shape=',f4.shape)#5*10 (?, ?) f5,f6 = tf.nn.top_k(f4,k=4) # 选取f4 最大的四个值 返回值和其索引位置 # f3 最小的四个值 # f6 index-trainLabelInput f7 = tf.gather(trainLabelInput,f6) # 根据索引抽取其中的张量 # f8 num reduce_sum reduction_indices=1 '竖直' f8 = tf.reduce_sum(f7,reduction_indices=1) # tf.argmax 选取在某一个最大的值 index f9 = tf.argmax(f8,dimension=1) # f9 - test5 image - 5 num with tf.Session() as sess: # f1 - testData 5张图片 p1 = sess.run(f1,feed_dict={testDataInput:testData[0:5]}) print('p1=',p1.shape)# p1= (5, 1, 784) p2 = sess.run(f2,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print('p2=',p2.shape)#p2= (5, 500, 784) (1,100) p3 = sess.run(f3,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print('p3=',p3.shape)#p3= (5, 500) print('p3[0,0]=',p3[0,0]) #130.451 knn distance p3[0,0]= 155.812 p4 = sess.run(f4,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) print('p4=',p4.shape) print('p4[0,0]',p4[0,0]) p5,p6 = sess.run((f5,f6),feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]}) #p5= (5, 4) 每一张测试图片（5张）分别对应4张最近训练图片 #p6= (5, 4) print('p5=',p5.shape) print('p6=',p6.shape) print('p5[0,0]',p5[0]) print('p6[0,0]',p6[0])# p6 index p7 = sess.run(f7,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print('p7=',p7.shape)#p7= (5, 4, 10) print('p7[]',p7) p8 = sess.run(f8,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print('p8=',p8.shape) print('p8[]=',p8) p9 = sess.run(f9,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel}) print('p9=',p9.shape) print('p9[]=',p9) p10 = np.argmax(testLabel[0:5],axis=1) print('p10[]=',p10) j = 0 for i in range(0,5): if p10[i] == p9[i]: j = j+1 print('ac=',j*100/5) —|— 参考链接：OpenCV+TensorFlow 入门人工智能图像处理 手写数字识别","tags":["TensorFlow"],"categories":["深度学习"]},{"title":"tensorflow自定义网络模型","path":"/2019/10/17/20191017-tensorflow自定义网络模型/","content":"SlimTF-Slim 模块是 TensorFlow 中最好用的 API 之一。尤其是里面引入的 arg_scope、model_variables、repeat、stack。TF-Slim 是 TensorFlow 中一个用来构建、训练、评估复杂模型的轻量化库。TF-Slim 模块可以和 TensorFlow 中其它API混合使用。 Slim模块的导入import tensorflow.contrib.slim as slim Slim 构建模型可以用 slim、variables、layers 和 scopes 来十分简洁地定义模型。下面对各个部分进行了详细描述： Slim变量（Variables）weights = slim.variable('weights', shape=[10, 10, 3 , 3], initializer=tf.truncated_normal_initializer(stddev=0.1), regularizer=slim.l2_regularizer(0.05), device='/CPU:0')~ Slim 层（Layers）使用基础（plain）的 TensorFlow 代码： input = ...with tf.name_scope('conv1_1') as scope: kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32, stddev=1e-1), name='weights') conv = tf.nn.conv2d(input, kernel, [1, 1, 1, 1], padding='SAME') biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32), trainable=True, name='biases') bias = tf.nn.bias_add(conv, biases) conv1 = tf.nn.relu(bias, name=scope) 为了避免代码的重复。Slim 提供了很多方便的神经网络 layers 的高层 op。例如：与上面的代码对应的 Slim 版的代码： input = ...net = slim.conv2d(input, 128, [3, 3], scope='conv1_1') slim.arg_scope（） 函数的使用这个函数的作用是给list_ops中的内容设置默认值。但是每个list_ops中的每个成员需要用@add_arg_scope修饰才行。所以使用slim.arg_scope（）有两个步骤： 使用@slim.add_arg_scope修饰目标函数 用 slim.arg_scope（）为目标函数设置默认参数.例如如下代码；首先用@slim.add_arg_scope修饰目标函数fun1（），然后利用slim.arg_scope（）为它设置默认参数。 ```pythonimport tensorflow as tfslim =tf.contrib.slim@slim.add_arg_scopedef fun1(a=0,b=0):return (a+b)with slim.arg_scope([fun1],a=10):x=fun1(b=30)print(x) 运行结果:40参考链接：https://blog.csdn.net/u013921430/article/details/80915696 其他用法见参考链接https://blog.csdn.net/wanttifa/article/details/90208398 查看ckpt中变量的几种方法查看ckpt中变量的方法有三种： 在有model的情况下，使用tf.train.Saver进行restore 使用tf.train.NewCheckpointReader直接读取ckpt文件，这种方法不需要model。 使用tools里的freeze_graph来读取ckptTips: 如果模型保存为.ckpt的文件，则使用该文件就可以查看.ckpt文件里的变量。ckpt路径为 model.ckpt 如果模型保存为.ckpt-xxx-data (图结构)、.ckpt-xxx.index (参数名)、.ckpt-xxx-meta (参数值)文件，则需要同时拥有这三个文件才行。并且ckpt的路径为 model.ckpt-xxx 1.基于model来读取ckpt文件里的变量1.首先建立起model2.从ckpt中恢复变量 1 2 3 4 5 6 7 8 9 10 with tf.Graph().as_default() as g: #建立model images, labels = cifar10.inputs(eval_data=eval_data) logits = cifar10.inference(images) top_k_op = tf.nn.in_top_k(logits, labels, 1) #从ckpt中恢复变量 sess = tf.Session() saver = tf.train.Saver() #saver = tf.train.Saver(…variables…) # 恢复部分变量时，只需要在Saver里指定要恢复的变量 save_path = ‘ckpt的路径’ saver.restore(sess, save_path) # 从ckpt中恢复变量 注意：基于model来读取ckpt中变量时，model和ckpt必须匹配。### 2.使用tf.train.NewCheckpointReader直接读取ckpt文件里的变量，使用tools.inspect_checkpoint里的print_tensors_in_checkpoint_file函数打印ckpt里的东西 ```python #使用NewCheckpointReader来读取ckpt里的变量 from tensorflow.python import pywrap_tensorflow checkpoint_path = os.path.join(model_dir, \"model.ckpt\") reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path) #tf.train.NewCheckpointReader var_to_shape_map = reader.get_variable_to_shape_map() for key in var_to_shape_map: print(\"tensor_name: \", key) #print(reader.get_tensor(key)) #使用print_tensors_in_checkpoint_file打印ckpt里的内容 from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file print_tensors_in_checkpoint_file(file_name, #ckpt文件名字 tensor_name, # 如果为None,则默认为ckpt里的所有变量 all_tensors, # bool 是否打印所有的tensor，这里打印出的是tensor的值，一般不推荐这里设置为False all_tensor_names) # bool 是否打印所有的tensor的name #上面的打印ckpt的内部使用的是pywrap_tensorflow.NewCheckpointReader所以要掌握NewCheckpointReader 3.使用tools里的freeze_graph来读取ckptfrom tensorflow.python.tools import freeze_graphfreeze_graph(input_graph, #=some_graph_def.pb input_saver, input_binary, input_checkpoint, #=model.ckpt output_node_names, #=softmax restore_op_name, filename_tensor_name, output_graph, #='./tmp/frozen_graph.pb' clear_devices, initializer_nodes, variable_names_whitelist='', variable_names_blacklist='', input_meta_graph=None, input_saved_model_dir=None, saved_model_tags='serve', checkpoint_version=2)#freeze_graph_test.py讲述了怎么使用freeze_grapg。 参考链接：https://www.jb51.net/article/142183.htm control_dependenciestf.control_dependencies(control_inputs)Wrapper for Graph.control_dependencies() using the default graph.See Graph.control_dependencies() for more details.此函数指定某些操作执行的依赖关系返回一个控制依赖的上下文管理器，使用 with 关键字可以让在这个上下文环境中的操作都在 control_inputs 执行 1 with tf.control_dependencies([a, b]):2 c = ....3 d = ... 在执行完 a，b 操作之后，才能执行 c，d 操作。意思就是 c，d 操作依赖 a，b 操作 1 with tf.control_dependencies([train_step, variable_averages_op]):2 train_op = tf.no_op(name='train') tf.no_op()表示执行完 train_step, variable_averages_op 操作之后什么都不做参考链接：http://www.tensorfly.cn/tfdoc/api_docs/python/framework.html#Graph.control_dependencies TensorBoard在TensorBoard中可视化图形构建您的网络，创建一个会话(session)，然后创建一个TensorFlow File Writer对象File Writer定义存储TensorBoard文件的路径，以及TensorFlow graph对象sess.graph是第二个参数。 writer = tf.summary.FileWriter(STORE_PATH, sess.graph) 当创建一个TensorFlow网络后，定义并运行File Writer时，就可以启动TensorBoard来可视化图形。要定义File Writer并将图形发送给它，运行以下命令: # start the sessionwith tf.Session() as sess:writer = tf.summary.FileWriter(STORE_PATH, sess.graph) 启动TensorBoardtensorboard --logdir=STORE_PATH 名称空间（Namespaces）名称空间是一种作用域，可以用它来包围图形组件，以便将它们组合在一起。通过这样的操作，名称空间中的细节将被折叠成TensorBoard计算图形可视化中的单个名称空间节点。要在TensorFlow中创建名称空间，可以使用Python with功能，如下所示： with tf.name_scope(\"layer_1\"):# now declare the weights connecting the input to the hidden layer W1 = tf.Variable(tf.random_normal([784, 300], stddev=0.01), name='W') b1 = tf.Variable(tf.random_normal([300]), name='b') hidden_logits = tf.add(tf.matmul(x_sc, W1), b1) hidden_out = tf.nn.sigmoid(hidden_logits) 还可以使用tf.variable_scope()代替tf.name_scope()。变量作用域是TensorFlow中的get_variable()变量共享机制的一部分。 标量总结（Scalar summaries）在网络中的任何位置，都可以记录标量(即单个实值)数量，以便在TensorBoard中显示。这对于跟踪诸如训练准确率的提高或损失函数的减少，或研究分布的标准差等方面都很有用。执行起来很容易。例如，下面的代码展示了如何在这个图中记录accuracy标量: # add a summary to store theaccuracytf.summary.scalar('acc_summary', accuracy) 第一个参数是要在TensorBoard可视化中给出标量的名称，第二个参数是要记录的操作(必须返回一个实值)。scalar()调用的输出是一个操作。在上面的代码中，我没有将这个操作分配给Python中的任何变量，但是如果用户愿意，可以这样做。然而，与TensorFlow中的其他操作一样，这些汇总操作在运行之前不会执行任何操作。根据开发人员想要观察的内容，在任何给定的图中通常都会运行许多可视化函数，因此有一个方便的助手函数merge_all()。这将把图中的所有函数调用合并在一起，这样您只需调用merge操作，它将为您收集所有其他函数操作并记录数据。它是这样的: merged = tf.summary.merge_all() 图像可视化# add summaryif reuse_variables is None: tf.summary.image('input', images) tf.summary.image('score_map', score_maps) tf.summary.image('score_map_pred', f_score * 255) tf.summary.image('geo_map_0', geo_maps[:, :, :, 0:1]) tf.summary.image('geo_map_0_pred', f_geometry[:, :, :, 0:1]) tf.summary.image('training_masks', training_masks) tf.summary.scalar('model_loss', model_loss) tf.summary.scalar('total_loss', total_loss) 文本检测模型EAST的搭建数据加载def load_annoataion(p): ''' load annotation from the text file :param p: :return: ''' text_polys = [] text_tags = [] if not os.path.exists(p): return np.array(text_polys, dtype=np.float32) with open(p, 'r') as f: reader = csv.reader(f) for line in reader: label = line[-1] # strip BOM. \\ufeff for python3, \\xef\\xbb\\bf for python2 line = [i.strip('\\ufeff').strip('\\xef\\xbb\\xbf') for i in line] x1, y1, x2, y2, x3, y3, x4, y4 = list(map(float, line[:8])) text_polys.append([[x1, y1], [x2, y2], [x3, y3], [x4, y4]]) if label == '*' or label == '###': text_tags.append(True) else: text_tags.append(False) return np.array(text_polys, dtype=np.float32), np.array(text_tags, dtype=np.bool)def generator(input_size=512, batch_size=32, background_ratio=3./8, random_scale=np.array([0.5, 1, 2.0, 3.0]), vis=False): image_list = np.array(get_images()) print('{} training images in {}'.format( image_list.shape[0], FLAGS.training_data_path)) index = np.arange(0, image_list.shape[0]) while True: np.random.shuffle(index) images = [] image_fns = [] score_maps = [] geo_maps = [] training_masks = [] for i in index: try: im_fn = image_list[i] im = cv2.imread(im_fn) # print im_fn h, w, _ = im.shape txt_fn = im_fn.replace(os.path.basename(im_fn).split('.')[1], 'txt') if not os.path.exists(txt_fn): print('text file {} does not exists'.format(txt_fn)) continue text_polys, text_tags = load_annoataion(txt_fn) text_polys, text_tags = check_and_validate_polys(text_polys, text_tags, (h, w)) # if text_polys.shape[0] == 0: # continue # random scale this image rd_scale = np.random.choice(random_scale) im = cv2.resize(im, dsize=None, fx=rd_scale, fy=rd_scale) text_polys *= rd_scale # print rd_scale # random crop a area from image if np.random.rand() background_ratio: # crop background im, text_polys, text_tags = crop_area(im, text_polys, text_tags, crop_background=True) if text_polys.shape[0] 0: # cannot find background continue # pad and resize image new_h, new_w, _ = im.shape max_h_w_i = np.max([new_h, new_w, input_size]) im_padded = np.zeros((max_h_w_i, max_h_w_i, 3), dtype=np.uint8) im_padded[:new_h, :new_w, :] = im.copy() im = cv2.resize(im_padded, dsize=(input_size, input_size)) score_map = np.zeros((input_size, input_size), dtype=np.uint8) geo_map_channels = 5 if FLAGS.geometry == 'RBOX' else 8 geo_map = np.zeros((input_size, input_size, geo_map_channels), dtype=np.float32) training_mask = np.ones((input_size, input_size), dtype=np.uint8) else: im, text_polys, text_tags = crop_area(im, text_polys, text_tags, crop_background=False) if text_polys.shape[0] == 0: continue h, w, _ = im.shape # pad the image to the training input size or the longer side of image new_h, new_w, _ = im.shape max_h_w_i = np.max([new_h, new_w, input_size]) im_padded = np.zeros((max_h_w_i, max_h_w_i, 3), dtype=np.uint8) im_padded[:new_h, :new_w, :] = im.copy() im = im_padded # resize the image to input size new_h, new_w, _ = im.shape resize_h = input_size resize_w = input_size im = cv2.resize(im, dsize=(resize_w, resize_h)) resize_ratio_3_x = resize_w/float(new_w) resize_ratio_3_y = resize_h/float(new_h) text_polys[:, :, 0] *= resize_ratio_3_x text_polys[:, :, 1] *= resize_ratio_3_y new_h, new_w, _ = im.shape score_map, geo_map, training_mask = generate_rbox((new_h, new_w), text_polys, text_tags) if vis: fig, axs = plt.subplots(3, 2, figsize=(20, 30)) # axs[0].imshow(im[:, :, ::-1]) # axs[0].set_xticks([]) # axs[0].set_yticks([]) # for poly in text_polys: # poly_h = min(abs(poly[3, 1] - poly[0, 1]), abs(poly[2, 1] - poly[1, 1])) # poly_w = min(abs(poly[1, 0] - poly[0, 0]), abs(poly[2, 0] - poly[3, 0])) # axs[0].add_artist(Patches.Polygon( # poly * 4, facecolor='none', edgecolor='green', linewidth=2, linestyle='-', fill=True)) # axs[0].text(poly[0, 0] * 4, poly[0, 1] * 4, '{:.0f}-{:.0f}'.format(poly_h * 4, poly_w * 4), # color='purple') # axs[1].imshow(score_map) # axs[1].set_xticks([]) # axs[1].set_yticks([]) axs[0, 0].imshow(im[:, :, ::-1]) axs[0, 0].set_xticks([]) axs[0, 0].set_yticks([]) for poly in text_polys: poly_h = min(abs(poly[3, 1] - poly[0, 1]), abs(poly[2, 1] - poly[1, 1])) poly_w = min(abs(poly[1, 0] - poly[0, 0]), abs(poly[2, 0] - poly[3, 0])) axs[0, 0].add_artist(Patches.Polygon( poly, facecolor='none', edgecolor='green', linewidth=2, linestyle='-', fill=True)) axs[0, 0].text(poly[0, 0], poly[0, 1], '{:.0f}-{:.0f}'.format(poly_h, poly_w), color='purple') axs[0, 1].imshow(score_map[::, ::]) axs[0, 1].set_xticks([]) axs[0, 1].set_yticks([]) axs[1, 0].imshow(geo_map[::, ::, 0]) axs[1, 0].set_xticks([]) axs[1, 0].set_yticks([]) axs[1, 1].imshow(geo_map[::, ::, 1]) axs[1, 1].set_xticks([]) axs[1, 1].set_yticks([]) axs[2, 0].imshow(geo_map[::, ::, 2]) axs[2, 0].set_xticks([]) axs[2, 0].set_yticks([]) axs[2, 1].imshow(training_mask[::, ::]) axs[2, 1].set_xticks([]) axs[2, 1].set_yticks([]) plt.tight_layout() plt.show() plt.close() images.append(im[:, :, ::-1].astype(np.float32)) image_fns.append(im_fn) score_maps.append(score_map[::4, ::4, np.newaxis].astype(np.float32)) geo_maps.append(geo_map[::4, ::4, :].astype(np.float32)) training_masks.append(training_mask[::4, ::4, np.newaxis].astype(np.float32)) if len(images) == batch_size: yield images, image_fns, score_maps, geo_maps, training_masks images = [] image_fns = [] score_maps = [] geo_maps = [] training_masks = [] except Exception as e: import traceback traceback.print_exc() continue 网络模型的搭建def model(images, weight_decay=1e-5, is_training=True): ''' define the model, we use slim's implemention of resnet ''' images = mean_image_subtraction(images) with slim.arg_scope(resnet_v1.resnet_arg_scope(weight_decay=weight_decay)): logits, end_points = resnet_v1.resnet_v1_50(images, is_training=is_training, scope='resnet_v1_50') with tf.variable_scope('feature_fusion', values=[end_points.values]): batch_norm_params = { 'decay': 0.997, 'epsilon': 1e-5, 'scale': True, 'is_training': is_training } with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu, normalizer_fn=slim.batch_norm, normalizer_params=batch_norm_params, weights_regularizer=slim.l2_regularizer(weight_decay)): f = [end_points['pool5'], end_points['pool4'], end_points['pool3'], end_points['pool2']] for i in range(4): print('Shape of f_{} {}'.format(i, f[i].shape)) g = [None, None, None, None] h = [None, None, None, None] num_outputs = [None, 128, 64, 32] for i in range(4): if i == 0: h[i] = f[i] else: c1_1 = slim.conv2d(tf.concat([g[i-1], f[i]], axis=-1), num_outputs[i], 1) h[i] = slim.conv2d(c1_1, num_outputs[i], 3) if i = 2: g[i] = unpool(h[i]) else: g[i] = slim.conv2d(h[i], num_outputs[i], 3) print('Shape of h_{} {}, g_{} {}'.format(i, h[i].shape, i, g[i].shape)) # here we use a slightly different way for regression part, # we first use a sigmoid to limit the regression range, and also # this is do with the angle map F_score = slim.conv2d(g[3], 1, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None) # 4 channel of axis aligned bbox and 1 channel rotation angle geo_map = slim.conv2d(g[3], 4, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None) * FLAGS.text_scale angle_map = (slim.conv2d(g[3], 1, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None) - 0.5) * np.pi/2 # angle is between [-45, 45] F_geometry = tf.concat([geo_map, angle_map], axis=-1) return F_score, F_geometry loss函数的设计def loss(y_true_cls, y_pred_cls, y_true_geo, y_pred_geo, training_mask): ''' define the loss used for training, contraning two part, the first part we use dice loss instead of weighted logloss, the second part is the iou loss defined in the paper :param y_true_cls: ground truth of text :param y_pred_cls: prediction os text :param y_true_geo: ground truth of geometry :param y_pred_geo: prediction of geometry :param training_mask: mask used in training, to ignore some text annotated by ### :return: ''' classification_loss = dice_coefficient(y_true_cls, y_pred_cls, training_mask) # scale classification loss to match the iou loss part classification_loss *= 0.01 # d1 - top, d2-right, d3-bottom, d4-left d1_gt, d2_gt, d3_gt, d4_gt, theta_gt = tf.split(value=y_true_geo, num_or_size_splits=5, axis=3) d1_pred, d2_pred, d3_pred, d4_pred, theta_pred = tf.split(value=y_pred_geo, num_or_size_splits=5, axis=3) area_gt = (d1_gt + d3_gt) * (d2_gt + d4_gt) area_pred = (d1_pred + d3_pred) * (d2_pred + d4_pred) w_union = tf.minimum(d2_gt, d2_pred) + tf.minimum(d4_gt, d4_pred) h_union = tf.minimum(d1_gt, d1_pred) + tf.minimum(d3_gt, d3_pred) area_intersect = w_union * h_union area_union = area_gt + area_pred - area_intersect L_AABB = -tf.log((area_intersect + 1.0)/(area_union + 1.0)) L_theta = 1 - tf.cos(theta_pred - theta_gt) tf.summary.scalar('geometry_AABB', tf.reduce_mean(L_AABB * y_true_cls * training_mask)) tf.summary.scalar('geometry_theta', tf.reduce_mean(L_theta * y_true_cls * training_mask)) L_g = L_AABB + 20 * L_theta return tf.reduce_mean(L_g * y_true_cls * training_mask) + classification_loss traindef main(argv=None): import os os.environ['CUDA_VISIBLE_DEVICES'] = FLAGS.gpu_list config = None config.batch_size = FLAGS.batch_size_per_gpu * FLAGS.num_gpus if not tf.gfile.Exists(FLAGS.checkpoint_path): tf.gfile.MkDir(FLAGS.checkpoint_path) else: if not FLAGS.restore: tf.gfile.DeleteRecursively(FLAGS.checkpoint_path) tf.gfile.MkDir(FLAGS.checkpoint_path) input_images = tf.placeholder(tf.float32, shape=[None, None, None, 3], name='input_images') input_score_maps = tf.placeholder(tf.float32, shape=[None, None, None, 1], name='input_score_maps') if FLAGS.geometry == 'RBOX': input_geo_maps = tf.placeholder(tf.float32, shape=[None, None, None, 5], name='input_geo_maps') else: input_geo_maps = tf.placeholder(tf.float32, shape=[None, None, None, 8], name='input_geo_maps') input_training_masks = tf.placeholder(tf.float32, shape=[None, None, None, 1], name='input_training_masks') global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False) learning_rate = tf.train.exponential_decay(FLAGS.learning_rate, global_step, decay_steps=10000, decay_rate=0.94, staircase=True) # add summary tf.summary.scalar('learning_rate', learning_rate) opt = tf.train.AdamOptimizer(learning_rate) # opt = tf.train.MomentumOptimizer(learning_rate, 0.9) # split input_images_split = tf.split(input_images, len(gpus)) input_score_maps_split = tf.split(input_score_maps, len(gpus)) input_geo_maps_split = tf.split(input_geo_maps, len(gpus)) input_training_masks_split = tf.split(input_training_masks, len(gpus)) tower_grads = [] reuse_variables = None for i, gpu_id in enumerate(gpus): with tf.device('/gpu:%d' % gpu_id): with tf.name_scope('model_%d' % gpu_id) as scope: iis = input_images_split[i] isms = input_score_maps_split[i] igms = input_geo_maps_split[i] itms = input_training_masks_split[i] total_loss, model_loss = tower_loss(iis, isms, igms, itms, reuse_variables) batch_norm_updates_op = tf.group(*tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope)) reuse_variables = True grads = opt.compute_gradients(total_loss) tower_grads.append(grads) grads = average_gradients(tower_grads) apply_gradient_op = opt.apply_gradients(grads, global_step=global_step) summary_op = tf.summary.merge_all() # save moving average variable_averages = tf.train.ExponentialMovingAverage( FLAGS.moving_average_decay, global_step) variables_averages_op = variable_averages.apply(tf.trainable_variables()) # batch norm updates with tf.control_dependencies([variables_averages_op, apply_gradient_op, batch_norm_updates_op]): train_op = tf.no_op(name='train_op') saver = tf.train.Saver(tf.global_variables()) summary_writer = tf.summary.FileWriter(FLAGS.checkpoint_path, tf.get_default_graph()) init = tf.global_variables_initializer() if FLAGS.pretrained_model_path is not None: variable_restore_op = slim.assign_from_checkpoint_fn(FLAGS.pretrained_model_path, slim.get_trainable_variables(), ignore_missing_vars=True) with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess: if FLAGS.restore: print('continue training from previous checkpoint') ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_path) saver.restore(sess, ckpt) else: sess.run(init) if FLAGS.pretrained_model_path is not None: variable_restore_op(sess) # data_generator = icdar.get_batch(num_workers=FLAGS.num_readers, # input_size=FLAGS.input_size, # batch_size=FLAGS.batch_size_per_gpu * len(gpus)) train_data_generator = icdar_single.get_batch_seq(num_workers=FLAGS.num_readers, config=config, is_training=True) start = time.time() for step in range(FLAGS.max_steps): data = next(train_data_generator) ml, tl, _ = sess.run([model_loss, total_loss, train_op], feed_dict={input_images: data[0], input_score_maps: data[2], input_geo_maps: data[3], input_training_masks: data[4]}) if np.isnan(tl): print('Loss diverged, stop training') break if step % 10 == 0: avg_time_per_step = (time.time() - start)/10 avg_examples_per_second = (10 * FLAGS.batch_size_per_gpu * len(gpus))/(time.time() - start) start = time.time() print('Step {:06d}, model loss {:.4f}, total loss {:.4f}, {:.2f} seconds/step, {:.2f} examples/second'.format( step, ml, tl, avg_time_per_step, avg_examples_per_second)) if step % FLAGS.save_checkpoint_steps == 0: saver.save(sess, FLAGS.checkpoint_path + 'model.ckpt', global_step=global_step) if step % FLAGS.save_summary_steps == 0: _, tl, summary_str = sess.run([train_op, total_loss, summary_op], feed_dict={input_images: data[0], input_score_maps: data[2], input_geo_maps: data[3], input_training_masks: data[4]}) summary_writer.add_summary(summary_str, global_step=step) 参考链接：https://github.com/argman/EAST","tags":["深度学习"],"categories":["深度学习"]},{"title":"tensorflow自定义网络模型","path":"/2019/10/17/tensorflow自定义网络模型/","content":"SlimTF-Slim 模块是 TensorFlow 中最好用的 API 之一。尤其是里面引入的 arg_scope、model_variables、repeat、stack。TF-Slim 是 TensorFlow 中一个用来构建、训练、评估复杂模型的轻量化库。TF-Slim 模块可以和 TensorFlow 中其它API混合使用。 Slim模块的导入1 | import tensorflow.contrib.slim as slim —|— Slim 构建模型可以用 slim、variables、layers 和 scopes 来十分简洁地定义模型。下面对各个部分进行了详细描述： Slim变量（Variables）1 2 3 4 5 6 | weights = slim.variable('weights', shape=[10, 10, 3 , 3], initializer=tf.truncated_normal_initializer(stddev=0.1), regularizer=slim.l2_regularizer(0.05), device='/CPU:0') ~ —|— Slim 层（Layers）使用基础（plain）的 TensorFlow 代码： 1 2 3 4 5 6 7 8 9 | input = ... with tf.name_scope('conv1_1') as scope: kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32, stddev=1e-1), name='weights') conv = tf.nn.conv2d(input, kernel, [1, 1, 1, 1], padding='SAME') biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32), trainable=True, name='biases') bias = tf.nn.bias_add(conv, biases) conv1 = tf.nn.relu(bias, name=scope) —|— 为了避免代码的重复。Slim 提供了很多方便的神经网络 layers 的高层 op。例如：与上面的代码对应的 Slim 版的代码： 1 2 | input = ... net = slim.conv2d(input, 128, [3, 3], scope='conv1_1') —|— slim.arg_scope（） 函数的使用这个函数的作用是给list_ops中的内容设置默认值。但是每个list_ops中的每个成员需要用@add_arg_scope修饰才行。所以使用slim.arg_scope（）有两个步骤： 使用@slim.add_arg_scope修饰目标函数 用 slim.arg_scope（）为目标函数设置默认参数.例如如下代码；首先用@slim.add_arg_scope修饰目标函数fun1（），然后利用slim.arg_scope（）为它设置默认参数。 1 2 3 4 5 6 7 8 9 10 | import tensorflow as tf slim =tf.contrib.slim @slim.add_arg_scope def fun1(a=0,b=0): return (a+b) with slim.arg_scope([fun1],a=10): x=fun1(b=30) print(x) —|— 运行结果:40参考链接：https://blog.csdn.net/u013921430/article/details/80915696 其他用法见参考链接https://blog.csdn.net/wanttifa/article/details/90208398 查看ckpt中变量的几种方法查看ckpt中变量的方法有三种： 在有model的情况下，使用tf.train.Saver进行restore 使用tf.train.NewCheckpointReader直接读取ckpt文件，这种方法不需要model。 使用tools里的freeze_graph来读取ckptTips: 如果模型保存为.ckpt的文件，则使用该文件就可以查看.ckpt文件里的变量。ckpt路径为 model.ckpt 如果模型保存为.ckpt-xxx-data (图结构)、.ckpt-xxx.index (参数名)、.ckpt-xxx-meta (参数值)文件，则需要同时拥有这三个文件才行。并且ckpt的路径为 model.ckpt-xxx 1.基于model来读取ckpt文件里的变量1.首先建立起model2.从ckpt中恢复变量 1 2 3 4 5 6 7 8 9 10 | with tf.Graph().as_default() as g: #建立model images, labels = cifar10.inputs(eval_data=eval_data) logits = cifar10.inference(images) top_k_op = tf.nn.in_top_k(logits, labels, 1) #从ckpt中恢复变量 sess = tf.Session() saver = tf.train.Saver() #saver = tf.train.Saver(...variables...) # 恢复部分变量时，只需要在Saver里指定要恢复的变量 save_path = 'ckpt的路径' saver.restore(sess, save_path) # 从ckpt中恢复变量 —|— 注意：基于model来读取ckpt中变量时，model和ckpt必须匹配。 2.使用tf.train.NewCheckpointReader直接读取ckpt文件里的变量，使用tools.inspect_checkpoint里的print_tensors_in_checkpoint_file函数打印ckpt里的东西1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 | #使用NewCheckpointReader来读取ckpt里的变量 from tensorflow.python import pywrap_tensorflow checkpoint_path = os.path.join(model_dir, \"model.ckpt\") reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path) #tf.train.NewCheckpointReader var_to_shape_map = reader.get_variable_to_shape_map() for key in var_to_shape_map: print(\"tensor_name: \", key) #print(reader.get_tensor(key)) #使用print_tensors_in_checkpoint_file打印ckpt里的内容 from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file print_tensors_in_checkpoint_file(file_name, #ckpt文件名字 tensor_name, # 如果为None,则默认为ckpt里的所有变量 all_tensors, # bool 是否打印所有的tensor，这里打印出的是tensor的值，一般不推荐这里设置为False all_tensor_names) # bool 是否打印所有的tensor的name #上面的打印ckpt的内部使用的是pywrap_tensorflow.NewCheckpointReader所以要掌握NewCheckpointReader —|— 3.使用tools里的freeze_graph来读取ckpt1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 | from tensorflow.python.tools import freeze_graph freeze_graph(input_graph, #=some_graph_def.pb input_saver, input_binary, input_checkpoint, #=model.ckpt output_node_names, #=softmax restore_op_name, filename_tensor_name, output_graph, #='./tmp/frozen_graph.pb' clear_devices, initializer_nodes, variable_names_whitelist='', variable_names_blacklist='', input_meta_graph=None, input_saved_model_dir=None, saved_model_tags='serve', checkpoint_version=2) #freeze_graph_test.py讲述了怎么使用freeze_grapg。 —|— 参考链接：https://www.jb51.net/article/142183.htm control_dependenciestf.control_dependencies(control_inputs)Wrapper for Graph.control_dependencies() using the default graph.See Graph.control_dependencies() for more details.此函数指定某些操作执行的依赖关系返回一个控制依赖的上下文管理器，使用 with 关键字可以让在这个上下文环境中的操作都在 control_inputs 执行 1 2 3 | 1 with tf.control_dependencies([a, b]): 2 c = .... 3 d = ... —|— 在执行完 a，b 操作之后，才能执行 c，d 操作。意思就是 c，d 操作依赖 a，b 操作 1 2 | 1 with tf.control_dependencies([train_step, variable_averages_op]): 2 train_op = tf.no_op(name='train') —|— tf.no_op()表示执行完 train_step, variable_averages_op 操作之后什么都不做参考链接：http://www.tensorfly.cn/tfdoc/api_docs/python/framework.html#Graph.control_dependencies TensorBoard在TensorBoard中可视化图形构建您的网络，创建一个会话(session)，然后创建一个TensorFlow File Writer对象File Writer定义存储TensorBoard文件的路径，以及TensorFlow graph对象sess.graph是第二个参数。 1 | writer = tf.summary.FileWriter(STORE_PATH, sess.graph) —|— 当创建一个TensorFlow网络后，定义并运行File Writer时，就可以启动TensorBoard来可视化图形。要定义File Writer并将图形发送给它，运行以下命令: 1 2 3 | # start the session with tf.Session() as sess: writer = tf.summary.FileWriter(STORE_PATH, sess.graph) —|— 启动TensorBoard1 | tensorboard --logdir=STORE_PATH —|— 名称空间（Namespaces）名称空间是一种作用域，可以用它来包围图形组件，以便将它们组合在一起。通过这样的操作，名称空间中的细节将被折叠成TensorBoard计算图形可视化中的单个名称空间节点。要在TensorFlow中创建名称空间，可以使用Python with功能，如下所示： 1 2 3 4 5 6 | with tf.name_scope(\"layer_1\"): # now declare the weights connecting the input to the hidden layer W1 = tf.Variable(tf.random_normal([784, 300], stddev=0.01), name='W') b1 = tf.Variable(tf.random_normal([300]), name='b') hidden_logits = tf.add(tf.matmul(x_sc, W1), b1) hidden_out = tf.nn.sigmoid(hidden_logits) —|— 还可以使用tf.variable_scope()代替tf.name_scope()。变量作用域是TensorFlow中的get_variable()变量共享机制的一部分。 标量总结（Scalar summaries）在网络中的任何位置，都可以记录标量(即单个实值)数量，以便在TensorBoard中显示。这对于跟踪诸如训练准确率的提高或损失函数的减少，或研究分布的标准差等方面都很有用。执行起来很容易。例如，下面的代码展示了如何在这个图中记录accuracy标量: 1 2 | # add a summary to store the accuracytf.summary.scalar('acc_summary', accuracy) —|— 第一个参数是要在TensorBoard可视化中给出标量的名称，第二个参数是要记录的操作(必须返回一个实值)。scalar()调用的输出是一个操作。在上面的代码中，我没有将这个操作分配给Python中的任何变量，但是如果用户愿意，可以这样做。然而，与TensorFlow中的其他操作一样，这些汇总操作在运行之前不会执行任何操作。根据开发人员想要观察的内容，在任何给定的图中通常都会运行许多可视化函数，因此有一个方便的助手函数merge_all()。这将把图中的所有函数调用合并在一起，这样您只需调用merge操作，它将为您收集所有其他函数操作并记录数据。它是这样的: 1 | merged = tf.summary.merge_all() —|— 图像可视化1 2 3 4 5 6 7 8 9 10 | # add summary if reuse_variables is None: tf.summary.image('input', images) tf.summary.image('score_map', score_maps) tf.summary.image('score_map_pred', f_score * 255) tf.summary.image('geo_map_0', geo_maps[:, :, :, 0:1]) tf.summary.image('geo_map_0_pred', f_geometry[:, :, :, 0:1]) tf.summary.image('training_masks', training_masks) tf.summary.scalar('model_loss', model_loss) tf.summary.scalar('total_loss', total_loss) —|— 文本检测模型EAST的搭建数据加载1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 | def load_annoataion(p): ''' load annotation from the text file :param p: :return: ''' text_polys = [] text_tags = [] if not os.path.exists(p): return np.array(text_polys, dtype=np.float32) with open(p, 'r') as f: reader = csv.reader(f) for line in reader: label = line[-1] # strip BOM. \\ufeff for python3, \\xef\\xbb\\bf for python2 line = [i.strip('\\ufeff').strip('\\xef\\xbb\\xbf') for i in line] x1, y1, x2, y2, x3, y3, x4, y4 = list(map(float, line[:8])) text_polys.append([[x1, y1], [x2, y2], [x3, y3], [x4, y4]]) if label == '*' or label == '###': text_tags.append(True) else: text_tags.append(False) return np.array(text_polys, dtype=np.float32), np.array(text_tags, dtype=np.bool) def generator(input_size=512, batch_size=32, background_ratio=3./8, random_scale=np.array([0.5, 1, 2.0, 3.0]), vis=False): image_list = np.array(get_images()) print('{} training images in {}'.format( image_list.shape[0], FLAGS.training_data_path)) index = np.arange(0, image_list.shape[0]) while True: np.random.shuffle(index) images = [] image_fns = [] score_maps = [] geo_maps = [] training_masks = [] for i in index: try: im_fn = image_list[i] im = cv2.imread(im_fn) # print im_fn h, w, _ = im.shape txt_fn = im_fn.replace(os.path.basename(im_fn).split('.')[1], 'txt') if not os.path.exists(txt_fn): print('text file {} does not exists'.format(txt_fn)) continue text_polys, text_tags = load_annoataion(txt_fn) text_polys, text_tags = check_and_validate_polys(text_polys, text_tags, (h, w)) # if text_polys.shape[0] == 0: # continue # random scale this image rd_scale = np.random.choice(random_scale) im = cv2.resize(im, dsize=None, fx=rd_scale, fy=rd_scale) text_polys *= rd_scale # print rd_scale # random crop a area from image if np.random.rand() background_ratio: # crop background im, text_polys, text_tags = crop_area(im, text_polys, text_tags, crop_background=True) if text_polys.shape[0] 0: # cannot find background continue # pad and resize image new_h, new_w, _ = im.shape max_h_w_i = np.max([new_h, new_w, input_size]) im_padded = np.zeros((max_h_w_i, max_h_w_i, 3), dtype=np.uint8) im_padded[:new_h, :new_w, :] = im.copy() im = cv2.resize(im_padded, dsize=(input_size, input_size)) score_map = np.zeros((input_size, input_size), dtype=np.uint8) geo_map_channels = 5 if FLAGS.geometry == 'RBOX' else 8 geo_map = np.zeros((input_size, input_size, geo_map_channels), dtype=np.float32) training_mask = np.ones((input_size, input_size), dtype=np.uint8) else: im, text_polys, text_tags = crop_area(im, text_polys, text_tags, crop_background=False) if text_polys.shape[0] == 0: continue h, w, _ = im.shape # pad the image to the training input size or the longer side of image new_h, new_w, _ = im.shape max_h_w_i = np.max([new_h, new_w, input_size]) im_padded = np.zeros((max_h_w_i, max_h_w_i, 3), dtype=np.uint8) im_padded[:new_h, :new_w, :] = im.copy() im = im_padded # resize the image to input size new_h, new_w, _ = im.shape resize_h = input_size resize_w = input_size im = cv2.resize(im, dsize=(resize_w, resize_h)) resize_ratio_3_x = resize_w/float(new_w) resize_ratio_3_y = resize_h/float(new_h) text_polys[:, :, 0] *= resize_ratio_3_x text_polys[:, :, 1] *= resize_ratio_3_y new_h, new_w, _ = im.shape score_map, geo_map, training_mask = generate_rbox((new_h, new_w), text_polys, text_tags) if vis: fig, axs = plt.subplots(3, 2, figsize=(20, 30)) # axs[0].imshow(im[:, :, ::-1]) # axs[0].set_xticks([]) # axs[0].set_yticks([]) # for poly in text_polys: # poly_h = min(abs(poly[3, 1] - poly[0, 1]), abs(poly[2, 1] - poly[1, 1])) # poly_w = min(abs(poly[1, 0] - poly[0, 0]), abs(poly[2, 0] - poly[3, 0])) # axs[0].add_artist(Patches.Polygon( # poly * 4, facecolor='none', edgecolor='green', linewidth=2, linestyle='-', fill=True)) # axs[0].text(poly[0, 0] * 4, poly[0, 1] * 4, '{:.0f}-{:.0f}'.format(poly_h * 4, poly_w * 4), # color='purple') # axs[1].imshow(score_map) # axs[1].set_xticks([]) # axs[1].set_yticks([]) axs[0, 0].imshow(im[:, :, ::-1]) axs[0, 0].set_xticks([]) axs[0, 0].set_yticks([]) for poly in text_polys: poly_h = min(abs(poly[3, 1] - poly[0, 1]), abs(poly[2, 1] - poly[1, 1])) poly_w = min(abs(poly[1, 0] - poly[0, 0]), abs(poly[2, 0] - poly[3, 0])) axs[0, 0].add_artist(Patches.Polygon( poly, facecolor='none', edgecolor='green', linewidth=2, linestyle='-', fill=True)) axs[0, 0].text(poly[0, 0], poly[0, 1], '{:.0f}-{:.0f}'.format(poly_h, poly_w), color='purple') axs[0, 1].imshow(score_map[::, ::]) axs[0, 1].set_xticks([]) axs[0, 1].set_yticks([]) axs[1, 0].imshow(geo_map[::, ::, 0]) axs[1, 0].set_xticks([]) axs[1, 0].set_yticks([]) axs[1, 1].imshow(geo_map[::, ::, 1]) axs[1, 1].set_xticks([]) axs[1, 1].set_yticks([]) axs[2, 0].imshow(geo_map[::, ::, 2]) axs[2, 0].set_xticks([]) axs[2, 0].set_yticks([]) axs[2, 1].imshow(training_mask[::, ::]) axs[2, 1].set_xticks([]) axs[2, 1].set_yticks([]) plt.tight_layout() plt.show() plt.close() images.append(im[:, :, ::-1].astype(np.float32)) image_fns.append(im_fn) score_maps.append(score_map[::4, ::4, np.newaxis].astype(np.float32)) geo_maps.append(geo_map[::4, ::4, :].astype(np.float32)) training_masks.append(training_mask[::4, ::4, np.newaxis].astype(np.float32)) if len(images) == batch_size: yield images, image_fns, score_maps, geo_maps, training_masks images = [] image_fns = [] score_maps = [] geo_maps = [] training_masks = [] except Exception as e: import traceback traceback.print_exc() continue —|— 网络模型的搭建1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 | def model(images, weight_decay=1e-5, is_training=True): ''' define the model, we use slim's implemention of resnet ''' images = mean_image_subtraction(images) with slim.arg_scope(resnet_v1.resnet_arg_scope(weight_decay=weight_decay)): logits, end_points = resnet_v1.resnet_v1_50(images, is_training=is_training, scope='resnet_v1_50') with tf.variable_scope('feature_fusion', values=[end_points.values]): batch_norm_params = { 'decay': 0.997, 'epsilon': 1e-5, 'scale': True, 'is_training': is_training } with slim.arg_scope([slim.conv2d], activation_fn=tf.nn.relu, normalizer_fn=slim.batch_norm, normalizer_params=batch_norm_params, weights_regularizer=slim.l2_regularizer(weight_decay)): f = [end_points['pool5'], end_points['pool4'], end_points['pool3'], end_points['pool2']] for i in range(4): print('Shape of f_{} {}'.format(i, f[i].shape)) g = [None, None, None, None] h = [None, None, None, None] num_outputs = [None, 128, 64, 32] for i in range(4): if i == 0: h[i] = f[i] else: c1_1 = slim.conv2d(tf.concat([g[i-1], f[i]], axis=-1), num_outputs[i], 1) h[i] = slim.conv2d(c1_1, num_outputs[i], 3) if i = 2: g[i] = unpool(h[i]) else: g[i] = slim.conv2d(h[i], num_outputs[i], 3) print('Shape of h_{} {}, g_{} {}'.format(i, h[i].shape, i, g[i].shape)) # here we use a slightly different way for regression part, # we first use a sigmoid to limit the regression range, and also # this is do with the angle map F_score = slim.conv2d(g[3], 1, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None) # 4 channel of axis aligned bbox and 1 channel rotation angle geo_map = slim.conv2d(g[3], 4, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None) * FLAGS.text_scale angle_map = (slim.conv2d(g[3], 1, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None) - 0.5) * np.pi/2 # angle is between [-45, 45] F_geometry = tf.concat([geo_map, angle_map], axis=-1) return F_score, F_geometry —|— loss函数的设计1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 | def loss(y_true_cls, y_pred_cls, y_true_geo, y_pred_geo, training_mask): ''' define the loss used for training, contraning two part, the first part we use dice loss instead of weighted logloss, the second part is the iou loss defined in the paper :param y_true_cls: ground truth of text :param y_pred_cls: prediction os text :param y_true_geo: ground truth of geometry :param y_pred_geo: prediction of geometry :param training_mask: mask used in training, to ignore some text annotated by ### :return: ''' classification_loss = dice_coefficient(y_true_cls, y_pred_cls, training_mask) # scale classification loss to match the iou loss part classification_loss *= 0.01 # d1 - top, d2-right, d3-bottom, d4-left d1_gt, d2_gt, d3_gt, d4_gt, theta_gt = tf.split(value=y_true_geo, num_or_size_splits=5, axis=3) d1_pred, d2_pred, d3_pred, d4_pred, theta_pred = tf.split(value=y_pred_geo, num_or_size_splits=5, axis=3) area_gt = (d1_gt + d3_gt) * (d2_gt + d4_gt) area_pred = (d1_pred + d3_pred) * (d2_pred + d4_pred) w_union = tf.minimum(d2_gt, d2_pred) + tf.minimum(d4_gt, d4_pred) h_union = tf.minimum(d1_gt, d1_pred) + tf.minimum(d3_gt, d3_pred) area_intersect = w_union * h_union area_union = area_gt + area_pred - area_intersect L_AABB = -tf.log((area_intersect + 1.0)/(area_union + 1.0)) L_theta = 1 - tf.cos(theta_pred - theta_gt) tf.summary.scalar('geometry_AABB', tf.reduce_mean(L_AABB * y_true_cls * training_mask)) tf.summary.scalar('geometry_theta', tf.reduce_mean(L_theta * y_true_cls * training_mask)) L_g = L_AABB + 20 * L_theta return tf.reduce_mean(L_g * y_true_cls * training_mask) + classification_loss —|— train1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 | def main(argv=None): import os os.environ['CUDA_VISIBLE_DEVICES'] = FLAGS.gpu_list config = None config.batch_size = FLAGS.batch_size_per_gpu * FLAGS.num_gpus if not tf.gfile.Exists(FLAGS.checkpoint_path): tf.gfile.MkDir(FLAGS.checkpoint_path) else: if not FLAGS.restore: tf.gfile.DeleteRecursively(FLAGS.checkpoint_path) tf.gfile.MkDir(FLAGS.checkpoint_path) input_images = tf.placeholder(tf.float32, shape=[None, None, None, 3], name='input_images') input_score_maps = tf.placeholder(tf.float32, shape=[None, None, None, 1], name='input_score_maps') if FLAGS.geometry == 'RBOX': input_geo_maps = tf.placeholder(tf.float32, shape=[None, None, None, 5], name='input_geo_maps') else: input_geo_maps = tf.placeholder(tf.float32, shape=[None, None, None, 8], name='input_geo_maps') input_training_masks = tf.placeholder(tf.float32, shape=[None, None, None, 1], name='input_training_masks') global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False) learning_rate = tf.train.exponential_decay(FLAGS.learning_rate, global_step, decay_steps=10000, decay_rate=0.94, staircase=True) # add summary tf.summary.scalar('learning_rate', learning_rate) opt = tf.train.AdamOptimizer(learning_rate) # opt = tf.train.MomentumOptimizer(learning_rate, 0.9) # split input_images_split = tf.split(input_images, len(gpus)) input_score_maps_split = tf.split(input_score_maps, len(gpus)) input_geo_maps_split = tf.split(input_geo_maps, len(gpus)) input_training_masks_split = tf.split(input_training_masks, len(gpus)) tower_grads = [] reuse_variables = None for i, gpu_id in enumerate(gpus): with tf.device('/gpu:%d' % gpu_id): with tf.name_scope('model_%d' % gpu_id) as scope: iis = input_images_split[i] isms = input_score_maps_split[i] igms = input_geo_maps_split[i] itms = input_training_masks_split[i] total_loss, model_loss = tower_loss(iis, isms, igms, itms, reuse_variables) batch_norm_updates_op = tf.group(*tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope)) reuse_variables = True grads = opt.compute_gradients(total_loss) tower_grads.append(grads) grads = average_gradients(tower_grads) apply_gradient_op = opt.apply_gradients(grads, global_step=global_step) summary_op = tf.summary.merge_all() # save moving average variable_averages = tf.train.ExponentialMovingAverage( FLAGS.moving_average_decay, global_step) variables_averages_op = variable_averages.apply(tf.trainable_variables()) # batch norm updates with tf.control_dependencies([variables_averages_op, apply_gradient_op, batch_norm_updates_op]): train_op = tf.no_op(name='train_op') saver = tf.train.Saver(tf.global_variables()) summary_writer = tf.summary.FileWriter(FLAGS.checkpoint_path, tf.get_default_graph()) init = tf.global_variables_initializer() if FLAGS.pretrained_model_path is not None: variable_restore_op = slim.assign_from_checkpoint_fn(FLAGS.pretrained_model_path, slim.get_trainable_variables(), ignore_missing_vars=True) with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess: if FLAGS.restore: print('continue training from previous checkpoint') ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_path) saver.restore(sess, ckpt) else: sess.run(init) if FLAGS.pretrained_model_path is not None: variable_restore_op(sess) # data_generator = icdar.get_batch(num_workers=FLAGS.num_readers, # input_size=FLAGS.input_size, # batch_size=FLAGS.batch_size_per_gpu * len(gpus)) train_data_generator = icdar_single.get_batch_seq(num_workers=FLAGS.num_readers, config=config, is_training=True) start = time.time() for step in range(FLAGS.max_steps): data = next(train_data_generator) ml, tl, _ = sess.run([model_loss, total_loss, train_op], feed_dict={input_images: data[0], input_score_maps: data[2], input_geo_maps: data[3], input_training_masks: data[4]}) if np.isnan(tl): print('Loss diverged, stop training') break if step % 10 == 0: avg_time_per_step = (time.time() - start)/10 avg_examples_per_second = (10 * FLAGS.batch_size_per_gpu * len(gpus))/(time.time() - start) start = time.time() print('Step {:06d}, model loss {:.4f}, total loss {:.4f}, {:.2f} seconds/step, {:.2f} examples/second'.format( step, ml, tl, avg_time_per_step, avg_examples_per_second)) if step % FLAGS.save_checkpoint_steps == 0: saver.save(sess, FLAGS.checkpoint_path + 'model.ckpt', global_step=global_step) if step % FLAGS.save_summary_steps == 0: _, tl, summary_str = sess.run([train_op, total_loss, summary_op], feed_dict={input_images: data[0], input_score_maps: data[2], input_geo_maps: data[3], input_training_masks: data[4]}) summary_writer.add_summary(summary_str, global_step=step) —|— 参考链接：https://github.com/argman/EAST","tags":["TensorFlow"],"categories":["深度学习"]},{"title":"图像增强常用函数","path":"/2019/10/14/20191014-图像增强常用函数/","content":"对目标检测一些常用的数据增强函数 # -*- coding: utf-8 -*-import cv2import numbersimport mathimport randomimport numpy as npfrom skimage.util import random_noise# 在原图上画出目标框def show_pic(img, bboxes=None, name='pic'): ''' 输入: img:图像array bboxes:图像的所有boudning box list, 格式为[[x_min, y_min, x_max, y_max]....] names:每个box对应的名称 ''' show_img = img.copy() if not isinstance(bboxes, np.ndarray): bboxes = np.array(bboxes) for point in bboxes.astype(np.int): cv2.line(show_img, tuple(point[0]), tuple(point[1]), (255, 0, 0), 2) # tuple 是在原有数据上加小括号 cv2.line(show_img, tuple(point[1]), tuple(point[2]), (255, 0, 0), 2) cv2.line(show_img, tuple(point[2]), tuple(point[3]), (255, 0, 0), 2) cv2.line(show_img, tuple(point[3]), tuple(point[0]), (255, 0, 0), 2) # cv2.namedWindow(name, 0) # 1表示原图 # cv2.moveWindow(name, 0, 0) # cv2.resizeWindow(name, 1200, 800) # 可视化的图片大小 cv2.imshow(name, show_img)# 图像均为cv2读取class DataAugment(): def __init__(self): pass def add_noise(self, im: np.ndarray): \"\"\" 对图片加噪声 :param img: 图像array :return: 加噪声后的图像array,由于输出的像素是在[0,1]之间,所以得乘以255 \"\"\" return (random_noise(im, mode='gaussian', clip=True) * 255).astype(im.dtype) def random_scale(self, im: np.ndarray, text_polys: np.ndarray, scales: np.ndarray or list) - tuple: \"\"\" 从scales中随机选择一个尺度，对图片和文本框进行缩放 :param im: 原图 :param text_polys: 文本框 :param scales: 尺度 :return: 经过缩放的图片和文本 \"\"\" tmp_text_polys = text_polys.copy() rd_scale = float(np.random.choice(scales)) im = cv2.resize(im, dsize=None, fx=rd_scale, fy=rd_scale) tmp_text_polys *= rd_scale return im, tmp_text_polys def random_rotate_img_bbox(self, img, text_polys, degrees: numbers.Number or list or tuple or np.ndarray, same_size=False): \"\"\" 从给定的角度中选择一个角度，对图片和文本框进行旋转 :param img: 图片 :param text_polys: 文本框 :param degrees: 角度，可以是一个数值或者list :param same_size: 是否保持和原图一样大 :return: 旋转后的图片和角度 \"\"\" if isinstance(degrees, numbers.Number): if degrees 0: raise ValueError(\"If degrees is a single number, it must be positive.\") degrees = (-degrees, degrees) elif isinstance(degrees, list) or isinstance(degrees, tuple) or isinstance(degrees, np.ndarray): if len(degrees) != 2: raise ValueError(\"If degrees is a sequence, it must be of len 2.\") degrees = degrees else: raise Exception('degrees must in Number or list or tuple or np.ndarray') # ---------------------- 旋转图像 ---------------------- w = img.shape[1] h = img.shape[0] angle = np.random.uniform(degrees[0], degrees[1]) if same_size: nw = w nh = h else: # 角度变弧度 rangle = np.deg2rad(angle) # 计算旋转之后图像的w, h nw = (abs(np.sin(rangle) * h) + abs(np.cos(rangle) * w)) nh = (abs(np.cos(rangle) * h) + abs(np.sin(rangle) * w)) # 构造仿射矩阵 rot_mat = cv2.getRotationMatrix2D((nw * 0.5, nh * 0.5), angle, 1) # 计算原图中心点到新图中心点的偏移量 rot_move = np.dot(rot_mat, np.array([(nw - w) * 0.5, (nh - h) * 0.5, 0])) # 更新仿射矩阵 rot_mat[0, 2] += rot_move[0] rot_mat[1, 2] += rot_move[1] # 仿射变换 rot_img = cv2.warpAffine(img, rot_mat, (int(math.ceil(nw)), int(math.ceil(nh))), flags=cv2.INTER_LANCZOS4) # ---------------------- 矫正bbox坐标 ---------------------- # rot_mat是最终的旋转矩阵 # 获取原始bbox的四个中点，然后将这四个点转换到旋转后的坐标系下 rot_text_polys = list() for bbox in text_polys: point1 = np.dot(rot_mat, np.array([bbox[0, 0], bbox[0, 1], 1])) point2 = np.dot(rot_mat, np.array([bbox[1, 0], bbox[1, 1], 1])) point3 = np.dot(rot_mat, np.array([bbox[2, 0], bbox[2, 1], 1])) point4 = np.dot(rot_mat, np.array([bbox[3, 0], bbox[3, 1], 1])) rot_text_polys.append([point1, point2, point3, point4]) return rot_img, np.array(rot_text_polys, dtype=np.float32) def random_crop(self, imgs, img_size): h, w = imgs[0].shape[0:2] th, tw = img_size if w == tw and h == th: return imgs # label中存在文本实例，并且按照概率进行裁剪 if np.max(imgs[1][:, :, -1]) 0 and random.random() 3.0 / 8.0: # 文本实例的top left点 tl = np.min(np.where(imgs[1][:, :, -1] 0), axis=1) - img_size tl[tl 0] = 0 # 文本实例的 bottom right 点 br = np.max(np.where(imgs[1][:, :, -1] 0), axis=1) - img_size br[br 0] = 0 # 保证选到右下角点是，有足够的距离进行crop br[0] = min(br[0], h - th) br[1] = min(br[1], w - tw) i = random.randint(tl[0], br[0]) j = random.randint(tl[1], br[1]) else: i = random.randint(0, h - th) j = random.randint(0, w - tw) # return i, j, th, tw for idx in range(len(imgs)): if len(imgs[idx].shape) == 3: imgs[idx] = imgs[idx][i:i + th, j:j + tw, :] else: imgs[idx] = imgs[idx][i:i + th, j:j + tw] return imgs def resize(self, im: np.ndarray, text_polys: np.ndarray, input_size: numbers.Number or list or tuple or np.ndarray, keep_ratio: bool = False) - tuple: \"\"\" 对图片和文本框进行resize :param im: 图片 :param text_polys: 文本框 :param input_size: resize尺寸,数字或者list的形式，如果为list形式，就是[w,h] :param keep_ratio: 是否保持长宽比 :return: resize后的图片和文本框 \"\"\" if isinstance(input_size, numbers.Number): if input_size 0: raise ValueError(\"If input_size is a single number, it must be positive.\") input_size = (input_size, input_size) elif isinstance(input_size, list) or isinstance(input_size, tuple) or isinstance(input_size, np.ndarray): if len(input_size) != 2: raise ValueError(\"If input_size is a sequence, it must be of len 2.\") input_size = (input_size[0], input_size[1]) else: raise Exception('input_size must in Number or list or tuple or np.ndarray') if keep_ratio: # 将图片短边pad到和长边一样 h, w, c = im.shape max_h = max(h, input_size[0]) max_w = max(w, input_size[1]) im_padded = np.zeros((max_h, max_w, c), dtype=np.uint8) im_padded[:h, :w] = im.copy() im = im_padded text_polys = text_polys.astype(np.float32) h, w, _ = im.shape im = cv2.resize(im, input_size) w_scale = input_size[0] / float(w) h_scale = input_size[1] / float(h) text_polys[:, :, 0] *= w_scale text_polys[:, :, 1] *= h_scale return im, text_polys def horizontal_flip(self, im: np.ndarray, text_polys: np.ndarray) - tuple: \"\"\" 对图片和文本框进行水平翻转 :param im: 图片 :param text_polys: 文本框 :return: 水平翻转之后的图片和文本框 \"\"\" flip_text_polys = text_polys.copy() flip_im = cv2.flip(im, 1) h, w, _ = flip_im.shape flip_text_polys[:, :, 0] = w - flip_text_polys[:, :, 0] return flip_im, flip_text_polys def vertical_flip(self, im: np.ndarray, text_polys: np.ndarray) - tuple: \"\"\" 对图片和文本框进行竖直翻转 :param im: 图片 :param text_polys: 文本框 :return: 竖直翻转之后的图片和文本框 \"\"\" flip_text_polys = text_polys.copy() flip_im = cv2.flip(im, 0) h, w, _ = flip_im.shape flip_text_polys[:, :, 1] = h - flip_text_polys[:, :, 1] return flip_im, flip_text_polys def test(self, im: np.ndarray, text_polys: np.ndarray): print('随机尺度缩放') t_im, t_text_polys = self.random_scale(im, text_polys, [0.5, 1, 2, 3]) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'random_scale') print('随机旋转') t_im, t_text_polys = self.random_rotate_img_bbox(im, text_polys, 10) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'random_rotate_img_bbox') print('随机裁剪') t_im, t_text_polys = self.random_crop_img_bboxes(im, text_polys) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'random_crop_img_bboxes') print('水平翻转') t_im, t_text_polys = self.horizontal_flip(im, text_polys) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'horizontal_flip') print('竖直翻转') t_im, t_text_polys = self.vertical_flip(im, text_polys) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'vertical_flip') show_pic(im, text_polys, 'vertical_flip_ori') print('加噪声') t_im = self.add_noise(im) print(t_im.shape) show_pic(t_im, text_polys, 'add_noise') show_pic(im, text_polys, 'add_noise_ori')","tags":["计算机视觉"],"categories":["计算机视觉"]},{"title":"图像增强常用函数","path":"/2019/10/14/图像增强常用函数/","content":"对目标检测一些常用的数据增强函数 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 | # -*- coding: utf-8 -*- import cv2 import numbers import math import random import numpy as np from skimage.util import random_noise # 在原图上画出目标框 def show_pic(img, bboxes=None, name='pic'): ''' 输入: img:图像array bboxes:图像的所有boudning box list, 格式为[[x_min, y_min, x_max, y_max]....] names:每个box对应的名称 ''' show_img = img.copy() if not isinstance(bboxes, np.ndarray): bboxes = np.array(bboxes) for point in bboxes.astype(np.int): cv2.line(show_img, tuple(point[0]), tuple(point[1]), (255, 0, 0), 2) # tuple 是在原有数据上加小括号 cv2.line(show_img, tuple(point[1]), tuple(point[2]), (255, 0, 0), 2) cv2.line(show_img, tuple(point[2]), tuple(point[3]), (255, 0, 0), 2) cv2.line(show_img, tuple(point[3]), tuple(point[0]), (255, 0, 0), 2) # cv2.namedWindow(name, 0) # 1表示原图 # cv2.moveWindow(name, 0, 0) # cv2.resizeWindow(name, 1200, 800) # 可视化的图片大小 cv2.imshow(name, show_img) # 图像均为cv2读取 class DataAugment(): def __init__(self): pass def add_noise(self, im: np.ndarray): \"\"\" 对图片加噪声 :param img: 图像array :return: 加噪声后的图像array,由于输出的像素是在[0,1]之间,所以得乘以255 \"\"\" return (random_noise(im, mode='gaussian', clip=True) * 255).astype(im.dtype) def random_scale(self, im: np.ndarray, text_polys: np.ndarray, scales: np.ndarray or list) - tuple: \"\"\" 从scales中随机选择一个尺度，对图片和文本框进行缩放 :param im: 原图 :param text_polys: 文本框 :param scales: 尺度 :return: 经过缩放的图片和文本 \"\"\" tmp_text_polys = text_polys.copy() rd_scale = float(np.random.choice(scales)) im = cv2.resize(im, dsize=None, fx=rd_scale, fy=rd_scale) tmp_text_polys *= rd_scale return im, tmp_text_polys def random_rotate_img_bbox(self, img, text_polys, degrees: numbers.Number or list or tuple or np.ndarray, same_size=False): \"\"\" 从给定的角度中选择一个角度，对图片和文本框进行旋转 :param img: 图片 :param text_polys: 文本框 :param degrees: 角度，可以是一个数值或者list :param same_size: 是否保持和原图一样大 :return: 旋转后的图片和角度 \"\"\" if isinstance(degrees, numbers.Number): if degrees 0: raise ValueError(\"If degrees is a single number, it must be positive.\") degrees = (-degrees, degrees) elif isinstance(degrees, list) or isinstance(degrees, tuple) or isinstance(degrees, np.ndarray): if len(degrees) != 2: raise ValueError(\"If degrees is a sequence, it must be of len 2.\") degrees = degrees else: raise Exception('degrees must in Number or list or tuple or np.ndarray') # ---------------------- 旋转图像 ---------------------- w = img.shape[1] h = img.shape[0] angle = np.random.uniform(degrees[0], degrees[1]) if same_size: nw = w nh = h else: # 角度变弧度 rangle = np.deg2rad(angle) # 计算旋转之后图像的w, h nw = (abs(np.sin(rangle) * h) + abs(np.cos(rangle) * w)) nh = (abs(np.cos(rangle) * h) + abs(np.sin(rangle) * w)) # 构造仿射矩阵 rot_mat = cv2.getRotationMatrix2D((nw * 0.5, nh * 0.5), angle, 1) # 计算原图中心点到新图中心点的偏移量 rot_move = np.dot(rot_mat, np.array([(nw - w) * 0.5, (nh - h) * 0.5, 0])) # 更新仿射矩阵 rot_mat[0, 2] += rot_move[0] rot_mat[1, 2] += rot_move[1] # 仿射变换 rot_img = cv2.warpAffine(img, rot_mat, (int(math.ceil(nw)), int(math.ceil(nh))), flags=cv2.INTER_LANCZOS4) # ---------------------- 矫正bbox坐标 ---------------------- # rot_mat是最终的旋转矩阵 # 获取原始bbox的四个中点，然后将这四个点转换到旋转后的坐标系下 rot_text_polys = list() for bbox in text_polys: point1 = np.dot(rot_mat, np.array([bbox[0, 0], bbox[0, 1], 1])) point2 = np.dot(rot_mat, np.array([bbox[1, 0], bbox[1, 1], 1])) point3 = np.dot(rot_mat, np.array([bbox[2, 0], bbox[2, 1], 1])) point4 = np.dot(rot_mat, np.array([bbox[3, 0], bbox[3, 1], 1])) rot_text_polys.append([point1, point2, point3, point4]) return rot_img, np.array(rot_text_polys, dtype=np.float32) def random_crop(self, imgs, img_size): h, w = imgs[0].shape[0:2] th, tw = img_size if w == tw and h == th: return imgs # label中存在文本实例，并且按照概率进行裁剪 if np.max(imgs[1][:, :, -1]) 0 and random.random() 3.0 / 8.0: # 文本实例的top left点 tl = np.min(np.where(imgs[1][:, :, -1] 0), axis=1) - img_size tl[tl 0] = 0 # 文本实例的 bottom right 点 br = np.max(np.where(imgs[1][:, :, -1] 0), axis=1) - img_size br[br 0] = 0 # 保证选到右下角点是，有足够的距离进行crop br[0] = min(br[0], h - th) br[1] = min(br[1], w - tw) i = random.randint(tl[0], br[0]) j = random.randint(tl[1], br[1]) else: i = random.randint(0, h - th) j = random.randint(0, w - tw) # return i, j, th, tw for idx in range(len(imgs)): if len(imgs[idx].shape) == 3: imgs[idx] = imgs[idx][i:i + th, j:j + tw, :] else: imgs[idx] = imgs[idx][i:i + th, j:j + tw] return imgs def resize(self, im: np.ndarray, text_polys: np.ndarray, input_size: numbers.Number or list or tuple or np.ndarray, keep_ratio: bool = False) - tuple: \"\"\" 对图片和文本框进行resize :param im: 图片 :param text_polys: 文本框 :param input_size: resize尺寸,数字或者list的形式，如果为list形式，就是[w,h] :param keep_ratio: 是否保持长宽比 :return: resize后的图片和文本框 \"\"\" if isinstance(input_size, numbers.Number): if input_size 0: raise ValueError(\"If input_size is a single number, it must be positive.\") input_size = (input_size, input_size) elif isinstance(input_size, list) or isinstance(input_size, tuple) or isinstance(input_size, np.ndarray): if len(input_size) != 2: raise ValueError(\"If input_size is a sequence, it must be of len 2.\") input_size = (input_size[0], input_size[1]) else: raise Exception('input_size must in Number or list or tuple or np.ndarray') if keep_ratio: # 将图片短边pad到和长边一样 h, w, c = im.shape max_h = max(h, input_size[0]) max_w = max(w, input_size[1]) im_padded = np.zeros((max_h, max_w, c), dtype=np.uint8) im_padded[:h, :w] = im.copy() im = im_padded text_polys = text_polys.astype(np.float32) h, w, _ = im.shape im = cv2.resize(im, input_size) w_scale = input_size[0] / float(w) h_scale = input_size[1] / float(h) text_polys[:, :, 0] *= w_scale text_polys[:, :, 1] *= h_scale return im, text_polys def horizontal_flip(self, im: np.ndarray, text_polys: np.ndarray) - tuple: \"\"\" 对图片和文本框进行水平翻转 :param im: 图片 :param text_polys: 文本框 :return: 水平翻转之后的图片和文本框 \"\"\" flip_text_polys = text_polys.copy() flip_im = cv2.flip(im, 1) h, w, _ = flip_im.shape flip_text_polys[:, :, 0] = w - flip_text_polys[:, :, 0] return flip_im, flip_text_polys def vertical_flip(self, im: np.ndarray, text_polys: np.ndarray) - tuple: \"\"\" 对图片和文本框进行竖直翻转 :param im: 图片 :param text_polys: 文本框 :return: 竖直翻转之后的图片和文本框 \"\"\" flip_text_polys = text_polys.copy() flip_im = cv2.flip(im, 0) h, w, _ = flip_im.shape flip_text_polys[:, :, 1] = h - flip_text_polys[:, :, 1] return flip_im, flip_text_polys def test(self, im: np.ndarray, text_polys: np.ndarray): print('随机尺度缩放') t_im, t_text_polys = self.random_scale(im, text_polys, [0.5, 1, 2, 3]) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'random_scale') print('随机旋转') t_im, t_text_polys = self.random_rotate_img_bbox(im, text_polys, 10) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'random_rotate_img_bbox') print('随机裁剪') t_im, t_text_polys = self.random_crop_img_bboxes(im, text_polys) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'random_crop_img_bboxes') print('水平翻转') t_im, t_text_polys = self.horizontal_flip(im, text_polys) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'horizontal_flip') print('竖直翻转') t_im, t_text_polys = self.vertical_flip(im, text_polys) print(t_im.shape, t_text_polys.dtype) show_pic(t_im, t_text_polys, 'vertical_flip') show_pic(im, text_polys, 'vertical_flip_ori') print('加噪声') t_im = self.add_noise(im) print(t_im.shape) show_pic(t_im, text_polys, 'add_noise') show_pic(im, text_polys, 'add_noise_ori') —|—","tags":["Python"],"categories":["计算机视觉"]},{"title":"park停车场项目实战","path":"/2019/10/13/20191013-park停车场项目实战/","content":"park 类对象# coding=utf-8import matplotlib.pyplot as pltimport cv2import os, globimport numpy as npclass Parking: # 显示图片 def show_images(self, images, cmap=None): cols = 2 rows = (len(images)+1)//cols plt.figure(figsize=(15,12)) for i, image in enumerate(images): plt.subplot(rows, cols, i+1) cmap = 'gray' if len(image.shape)==2 else cmap plt.imshow(image, cmap=cmap) plt.xticks([]) plt.yticks([]) plt.tight_layout(pad=0, h_pad=0, w_pad=0) plt.show() def cv_show(self, name, img): cv2.imshow(name,img) cv2.waitKey(0) cv2.destroyAllWindows() def select_rgb_white_yello(self, image): # 过滤掉背景 lower = np.uint8([120,120,120]) upper = np.uint8([255,255,255]) # lower_red和高于upper_red的部分分别变成0，lower_red～upper_red之间的值变成255,相当于过滤背景 white_mask = cv2.inRange(image,lower, upper) self.cv_show('white_mask', white_mask) masked = cv2.bitwise_and(image, image, mask = white_mask) self.cv_show('masked', masked) return masked def convert_gray_sacle(self,image): return cv2.cvtColor(image,cv2.COLOR_RGB2GRAY) # 检测边缘 def detect_edges(self, image, low_threshold=50, high_threshold=200): return cv2.Canny(image, low_threshold, high_threshold) def filter_region(self, image, vertices): \"\"\" 剔除不需要的地方 \"\"\" mask = np.zeros_like(image) if len(mask.shape) == 2: cv2.fillPoly(mask, vertices, 255) self.cv_show('mask', mask) return cv2.bitwise_and(image, mask) # 手动选择区域 def select_region(self,image): \"\"\" 手动选择区域 \"\"\" # first, define the polygon by vertices rows, cols = image.shape[:2] pt_1 = [cols*0.05, rows*0.90] pt_2 = [cols*0.05, rows*0.70] pt_3 = [cols*0.30, rows*0.55] pt_4 = [cols*0.6, rows*0.15] pt_5 = [cols*0.90, rows*0.15] pt_6 = [cols*0.90, rows*0.90] vertices = np.array([[pt_1, pt_2, pt_3, pt_4, pt_5, pt_6]], dtype=np.int32) point_img = image.copy() point_img = cv2.cvtColor(point_img, cv2.COLOR_GRAY2RGB) for point in vertices[0]: cv2.circle(point_img, (point[0],point[1]), 10, (0,0,255), 4) self.cv_show('point_img',point_img) return self.filter_region(image, vertices) def hough_line(self, image): # 输入的图像需要是边缘检测后的结果 # minLineLengh(线的最短长度，比这个短的都被忽略)和MaxLineCap（两条直线之间的最大间隔，小于此值，认为是一条直线） # rho距离精度,theta角度精度,threshod超过设定阈值才被检测出线段 return cv2.HoughLinesP(image, rho=0.1, theta=np.pi/10, threshold=15, minLineLength=9, maxLineGap=4) def draw_lines(self, image, lines, color=[255,0,0], thickness=2, make_copy=True): # 过滤霍夫变换检测得到直线 if make_copy: image = np.copy(image) cleaned = [] for line in lines: for x1, y1, x2, y2 in line: if abs(y2-y1) =1 and abs(x2-x1) =25 and abs(x2-x1) = 55: cleaned.append((x1,y1,x2,y2)) cv2.line(image, (x1, y1), (x2, y2), color, thickness) print('No lines detected: ', len(cleaned)) return image def identify_blocks(self, image, lines, make_copy=True): if make_copy: new_image = np.copy(image) # step 1: 过滤部分直线 cleaned = [] for line in lines: for x1,y1,x2,y2 in line: if abs(y2-y1) =1 and abs(x2-x1) =25 and abs(x2-x1) = 55: cleaned.append((x1,y1,x2,y2)) # step 2: 对直线按照x1进行排序 import operator list1 = sorted(cleaned, key=operator.itemgetter(0,1)) # b=operator.itemgetter(1,0) //定义函数b，获取对象的第1个域和第0个的值 # b(a) # (2, 1) # step 3: 找到多个列，相当于每列是一排车 clusters = {} dIndex = 0 clus_dist = 10 for i in range(len(list1)-1): distance = abs(list1[i+1][0] - list1[i][0]) if distance = clus_dist: if not dIndex in clusters.keys(): clusters[dIndex] = [] clusters[dIndex].append(list1[i]) clusters[dIndex].append(list1[i + 1]) else: dIndex += 1 # step 4: 得到坐标 rects = {} i = 0 for key in clusters: all_list = clusters[key] cleaned = list(set(all_list)) if len(cleaned) 5: cleaned = sorted(cleaned, key=lambda tup: tup[1]) avg_y1 = cleaned[0][1] avg_y2 = cleaned[-1][1] avg_x1 = 0 avg_x2 = 0 for tup in cleaned: avg_x1 += tup[0] avg_x2 += tup[2] avg_x1 = avg_x1/len(cleaned) avg_x2 = avg_x2/len(cleaned) rects[i] = (avg_x1, avg_y1, avg_x2, avg_y2) i += 1 print(\"Num Parking Lanes: \", len(rects)) # step 5: 把矩形画出来 buff = 7 for key in rects: tup_topLeft = (int(rects[key][0] - buff), int(rects[key][1])) tup_botRight = (int(rects[key][2] + buff), int(rects[key][3])) cv2.rectangle(new_image, tup_topLeft,tup_botRight,(0,255,0),3) return new_image, rects def draw_parking(self, image, rects, make_copy=True, color=[255,0,0],thickness=2, save=True): if make_copy: new_image = np.copy(image) gap = 15.5 spot_dict = {} # 字典：一个车位对应一个位置 tot_spots = 0 #微调 adj_y1 = {0: 20, 1:-10, 2:0, 3:-11, 4:28, 5:5, 6:-15, 7:-15, 8:-10, 9:-30, 10:9, 11:-32} adj_y2 = {0: 30, 1: 50, 2:15, 3:10, 4:-15, 5:15, 6:15, 7:-20, 8:15, 9:15, 10:0, 11:30} adj_x1 = {0: -8, 1:-15, 2:-15, 3:-15, 4:-15, 5:-15, 6:-15, 7:-15, 8:-10, 9:-10, 10:-10, 11:0} adj_x2 = {0: 0, 1: 15, 2:15, 3:15, 4:15, 5:15, 6:15, 7:15, 8:10, 9:10, 10:10, 11:0} for key in rects: tup = rects[key] x1 = int(tup[0]+ adj_x1[key]) x2 = int(tup[2]+ adj_x2[key]) y1 = int(tup[1] + adj_y1[key]) y2 = int(tup[3] + adj_y2[key]) cv2.rectangle(new_image, (x1, y1),(x2,y2),(0,255,0),2) num_splits = int(abs(y2-y1)//gap) for i in range(0, num_splits+1): y = int(y1 + i*gap) cv2.line(new_image, (x1, y), (x2, y), color, thickness) if key 0 and key len(rects) -1 : #竖直线 x = int((x1 + x2)/2) cv2.line(new_image, (x, y1), (x, y2), color, thickness) # 计算数量 if key == 0 or key == (len(rects) -1): tot_spots += num_splits +1 else: tot_spots += 2*(num_splits +1) # 字典对应好 if key == 0 or key == (len(rects) -1): for i in range(0, num_splits+1): cur_len = len(spot_dict) y = int(y1 + i*gap) spot_dict[(x1, y, x2, y+gap)] = cur_len +1 else: for i in range(0, num_splits+1): cur_len = len(spot_dict) y = int(y1 + i*gap) x = int((x1 + x2)/2) spot_dict[(x1, y, x, y+gap)] = cur_len +1 spot_dict[(x, y, x2, y+gap)] = cur_len +2 print(\"total parking spaces: \", tot_spots, cur_len) if save: filename = 'with_parking.jpg' cv2.imwrite(filename, new_image) return new_image, spot_dict def assign_spots_map(self,image, spot_dict, make_copy = True, color=[255, 0, 0], thickness=2): if make_copy: new_image = np.copy(image) for spot in spot_dict.keys(): (x1, y1, x2, y2) = spot cv2.rectangle(new_image, (int(x1),int(y1)), (int(x2),int(y2)), color, thickness) return new_image def save_images_for_cnn(self,image, spot_dict, folder_name ='cnn_data'): for spot in spot_dict.keys(): (x1, y1, x2, y2) = spot (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2)) #裁剪图像 spot_img = image[y1:y2, x1:x2] spot_img = cv2.resize(spot_img, (0,0), fx=2.0, fy=2.0) spot_id = spot_dict[spot] filename = 'spot' + str(spot_id) +'.jpg' print(spot_img.shape, filename, (x1,x2,y1,y2)) cv2.imwrite(os.path.join(folder_name, filename), spot_img) def make_prediction(self,image, model, class_dictionary): # 预处理 img = image/255. # 转换成4D tensor image = np.expand_dims(img,axis=0) # 用训练好的模型进行训练 class_predicted = model.predict(image) inID = np.argmax(class_predicted[0]) label = class_dictionary[inID] return label def predict_on_image(self, image, spot_dict, model, class_dictionary, make_copy=True, color = [0,255,0], alpha=0.5): if make_copy: new_image = np.copy(image) overlay = np.copy(image) self.cv_show('new_image',new_image) cnt_empty = 0 all_spots = 0 for spot in spot_dict.key(): all_spots += 1 (x1, y1, x2, y2) = spot (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2)) spot_img = image[y1:y2, x1:x2] spot_img = cv2.resize(spot_img, (48, 48)) label = self.make_prediction(spot_img,model,class_dictionary) if label == 'empty': cv2.rectangle(overlay, (int(x1),int(y1)), (int(x2),int(y2)), color, -1) cnt_empty += 1 cv2.addWeighted(overlay, alpha, new_image, 1 - alpha, 0, new_image) # 图像融合 cv2.putText(new_image, \"Available: %d spots\" %cnt_empty, (30, 95), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) cv2.putText(new_image, \"Total: %d spots\" %all_spots, (30, 125), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) save = False if save: filename = 'with_marking.jpg' cv2.imwrite(filename, new_image) self.cv_show('new_image',new_image) return new_image def predict_on_video(self,video_name,final_spot_dict, model,class_dictionary,ret=True): cap = cv2.VideoCapture(video_name) count = 0 while ret: ret, image = cap.read() count += 1 if count == 5: count = 0 new_image = np.copy(image) overlay = np.copy(image) cnt_empty = 0 all_spots = 0 color = [0, 255, 0] alpha=0.5 for spot in final_spot_dict.keys(): all_spots += 1 (x1, y1, x2, y2) = spot (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2)) spot_img = image[y1:y2, x1:x2] spot_img = cv2.resize(spot_img, (48,48)) label = self.make_prediction(spot_img,model,class_dictionary) if label == 'empty': cv2.rectangle(overlay, (int(x1),int(y1)), (int(x2),int(y2)), color, -1) cnt_empty += 1 cv2.addWeighted(overlay, alpha, new_image, 1 - alpha, 0, new_image) cv2.putText(new_image, \"Available: %d spots\" %cnt_empty, (30, 95), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) cv2.putText(new_image, \"Total: %d spots\" %all_spots, (30, 125), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) cv2.imshow('frame', new_image) if cv2.waitKey(10) 0xFF == ord('q'): break cv2.destroyAllWindows() cap.release() test模块from __future__ import divisionimport matplotlib.pyplot as pltimport cv2import os, globimport numpy as npfrom PIL import Imagefrom keras.applications.imagenet_utils import preprocess_inputfrom keras.models import load_modelfrom keras.preprocessing import imagefrom Parking import Parkingimport picklecwd = os.getcwd()def img_process(test_images,park): white_yellow_images = list(map(park.select_rgb_white_yellow, test_images)) park.show_images(white_yellow_images) gray_images = list(map(park.convert_gray_scale, white_yellow_images)) park.show_images(gray_images) edge_images = list(map(lambda image: park.detect_edges(image), gray_images)) park.show_images(edge_images) roi_images = list(map(park.select_region, edge_images)) park.show_images(roi_images) list_of_lines = list(map(park.hough_lines, roi_images)) line_images = [] for image, lines in zip(test_images, list_of_lines): line_images.append(park.draw_lines(image, lines)) park.show_images(line_images) rect_images = [] rect_coords = [] for image, lines in zip(test_images, list_of_lines): new_image, rects = park.identify_blocks(image, lines) rect_images.append(new_image) rect_coords.append(rects) park.show_images(rect_images) delineated = [] spot_pos = [] for image, rects in zip(test_images, rect_coords): new_image, spot_dict = park.draw_parking(image, rects) delineated.append(new_image) spot_pos.append(spot_dict) park.show_images(delineated) final_spot_dict = spot_pos[1] print(len(final_spot_dict)) with open('spot_dict.pickle', 'wb') as handle: pickle.dump(final_spot_dict, handle, protocol=pickle.HIGHEST_PROTOCOL) park.save_images_for_cnn(test_images[0],final_spot_dict) return final_spot_dictdef keras_model(weights_path): model = load_model(weights_path) return modeldef img_test(test_images,final_spot_dict,model,class_dictionary): for i in range (len(test_images)): predicted_images = park.predict_on_image(test_images[i],final_spot_dict,model,class_dictionary)def video_test(video_name,final_spot_dict,model,class_dictionary): name = video_name cap = cv2.VideoCapture(name) park.predict_on_video(name,final_spot_dict,model,class_dictionary,ret=True)if __name__ == '__main__': test_images = [plt.imread(path) for path in glob.glob('test_images/*.jpg')] weights_path = 'car1.h5' video_name = 'parking_video.mp4' class_dictionary = {} class_dictionary[0] = 'empty' class_dictionary[1] = 'occupied' park = Parking() park.show_images(test_images) final_spot_dict = img_process(test_images,park) model = keras_model(weights_path) img_test(test_images,final_spot_dict,model,class_dictionary) video_test(video_name,final_spot_dict,model,class_dictionary) 利用cnn训练出一个二分类网络import numpyimport osfrom keras import applicationsfrom keras.preprocessing.image import ImageDataGeneratorfrom keras import optimizersfrom keras.models import Sequential, Modelfrom keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2Dfrom keras import backend as kfrom keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStoppingfrom keras.models import Sequentialfrom keras.layers.normalization import BatchNormalizationfrom keras.layers.convolutional import Conv2Dfrom keras.layers.convolutional import MaxPooling2Dfrom keras.initializers import TruncatedNormalfrom keras.layers.core import Activationfrom keras.layers.core import Flattenfrom keras.layers.core import Dropoutfrom keras.layers.core import Densefiles_train = 0files_validation = 0cwd = os.getcwd()folder = 'train_data/train'for sub_folder in os.listdir(folder): path, dirs, files = next(os.walk(os.path.join(folder,sub_folder))) files_train += len(files)folder = 'train_data/test'for sub_folder in os.listdir(folder): path, dirs, files = next(os.walk(os.path.join(folder,sub_folder))) files_validation += len(files)print(files_train,files_validation)img_width, img_height = 48, 48train_data_dir = \"train_data/train\"validation_data_dir = \"train_data/test\"nb_train_samples = files_trainnb_validation_samples = files_validationbatch_size = 32epochs = 15num_classes = 2model = applications.VGG16(weights='imagenet', include_top=False, input_shape = (img_width, img_height, 3))for layer in model.layers[:10]: layer.trainable = Falsex = model.outputx = Flatten()(x)predictions = Dense(num_classes, activation=\"softmax\")(x)model_final = Model(input = model.input, output = predictions)model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"])train_datagen = ImageDataGenerator(rescale = 1./255,horizontal_flip = True,fill_mode = \"nearest\",zoom_range = 0.1,width_shift_range = 0.1,height_shift_range=0.1,rotation_range=5)test_datagen = ImageDataGenerator(rescale = 1./255,horizontal_flip = True,fill_mode = \"nearest\",zoom_range = 0.1,width_shift_range = 0.1,height_shift_range=0.1,rotation_range=5)train_generator = train_datagen.flow_from_directory(train_data_dir,target_size = (img_height, img_width),batch_size = batch_size,class_mode = \"categorical\")validation_generator = test_datagen.flow_from_directory(validation_data_dir,target_size = (img_height, img_width),class_mode = \"categorical\")checkpoint = ModelCheckpoint(\"car1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')history_object = model_final.fit_generator(train_generator,samples_per_epoch = nb_train_samples,epochs = epochs,validation_data = validation_generator,nb_val_samples = nb_validation_samples,callbacks = [checkpoint, early]) 参考资料： 唐宇迪 OpenCV计算机视觉实战(Python版)","tags":["计算机视觉"],"categories":["计算机视觉"]},{"title":"ocr透视变换","path":"/2019/10/13/20191013-ocr透视变换/","content":"# 导入工具包import numpy as npimport argparseimport cv2# 设置参数ap = argparse.ArgumentParser()ap.add_argument(\"-i\", \"--image\", required = False,\thelp = \"Path to the image to be scanned\",default='images/page.jpg')args = vars(ap.parse_args())def order_points(pts):\t# 一共4个坐标点\trect = np.zeros((4, 2), dtype = \"float32\")\t# 按顺序找到对应坐标0123分别是 左上，右上，右下，左下\t# 计算左上，右下\ts = pts.sum(axis = 1)\trect[0] = pts[np.argmin(s)]\trect[2] = pts[np.argmax(s)]\t# 计算右上和左下\tdiff = np.diff(pts, axis = 1)\trect[1] = pts[np.argmin(diff)]\trect[3] = pts[np.argmax(diff)]\treturn rectdef four_point_transform(image, pts):\t# 获取输入坐标点\trect = order_points(pts)\t(tl, tr, br, bl) = rect\t# 计算输入的w和h值\twidthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))\twidthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))\tmaxWidth = max(int(widthA), int(widthB))\theightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))\theightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))\tmaxHeight = max(int(heightA), int(heightB))\t# 变换后对应坐标位置\tdst = np.array([ [0, 0], [maxWidth - 1, 0], [maxWidth - 1, maxHeight - 1], [0, maxHeight - 1]], dtype = \"float32\")\t# 计算变换矩阵\tM = cv2.getPerspectiveTransform(rect, dst)\twarped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))\t# 返回变换后结果\treturn warpeddef resize(image, width=None, height=None, inter=cv2.INTER_AREA):\tdim = None\t(h, w) = image.shape[:2]\tif width is None and height is None: return image\tif width is None: r = height / float(h) dim = (int(w * r), height)\telse: r = width / float(w) dim = (width, int(h * r))\tresized = cv2.resize(image, dim, interpolation=inter)\treturn resized# 读取输入image = cv2.imread(args[\"image\"])#坐标也会相同变化ratio = image.shape[0] / 500.0orig = image.copy()image = resize(orig, height = 500)# 预处理gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)gray = cv2.GaussianBlur(gray, (5, 5), 0)edged = cv2.Canny(gray, 75, 200)# 展示预处理结果print(\"STEP 1: 边缘检测\")cv2.imshow(\"Image\", image)cv2.imshow(\"Edged\", edged)cv2.waitKey(0)cv2.destroyAllWindows()# 轮廓检测cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[1]cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]# 遍历轮廓for c in cnts:\t# 计算轮廓近似\tperi = cv2.arcLength(c, True)\t# C表示输入的点集\t# epsilon表示从原始轮廓到近似轮廓的最大距离，它是一个准确度参数\t# True表示封闭的\tapprox = cv2.approxPolyDP(c, 0.02 * peri, True)# 越小越精准，长度的百分之多少作为精度\t# 4个点的时候就拿出来\tif len(approx) == 4: screenCnt = approx break# 展示结果print(\"STEP 2: 获取轮廓\")cv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 2)cv2.imshow(\"Outline\", image)cv2.waitKey(0)cv2.destroyAllWindows()# 透视变换warped = four_point_transform(orig, screenCnt.reshape(4, 2) * ratio)# 二值处理warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)ref = cv2.threshold(warped, 100, 255, cv2.THRESH_BINARY)[1]cv2.imwrite('scan.jpg', ref)# 展示结果print(\"STEP 3: 变换\")cv2.imshow(\"Original\", resize(orig, height = 650))cv2.imshow(\"Scanned\", resize(ref, height = 650))cv2.waitKey(0) 参考资料： 唐宇迪 OpenCV计算机视觉实战(Python版)","tags":["计算机视觉"],"categories":["计算机视觉"]},{"title":"ocr答题卡识别","path":"/2019/10/13/20191013-ocr答题卡识别/","content":"import cv2def sort_contours(cnts, method=\"left-to-right\"): reverse = False i = 0 if method == \"right-to-left\" or method == \"bottom-to-top\": reverse = True if method == \"top-to-bottom\" or method == \"bottom-to-top\": i = 1 boundingBoxes = [cv2.boundingRect(c) for c in cnts] #用一个最小的矩形，把找到的形状包起来x,y,h,w (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes), key=lambda b: b[1][i], reverse=reverse)) return cnts, boundingBoxesdef resize(image, width=None, height=None, inter=cv2.INTER_AREA): dim = None (h, w) = image.shape[:2] if width is None and height is None: return image if width is None: r = height / float(h) dim = (int(w * r), height) else: r = width / float(w) dim = (width, int(h * r)) resized = cv2.resize(image, dim, interpolation=inter) return resized # 导入工具包from imutils import contoursimport numpy as npimport argparseimport cv2import myutils_beifen# # 设置参数# ap = argparse.ArgumentParser()# ap.add_argument(\"-i\", \"--image\", required=True,# help=\"path to input image\")# ap.add_argument(\"-t\", \"--template\", required=True,# help=\"path to template OCR-A image\")# args = vars(ap.parse_args())# 指定信用卡类型FIRST_NUMBER = {\t\"3\": \"American Express\",\t\"4\": \"Visa\",\t\"5\": \"MasterCard\",\t\"6\": \"Discover Card\"}# 绘图展示def cv_show(name,img):\tcv2.imshow(name, img)\tcv2.waitKey(0)\tcv2.destroyAllWindows()# 读取一个模板图像# img = cv2.imread(args[\"template\"])img = cv2.imread('images/ocr_a_reference.png')cv_show('img',img)# 灰度图ref = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)cv_show('ref',ref)# 二值图像ref = cv2.threshold(ref, 10, 255, cv2.THRESH_BINARY_INV)[1]cv_show('ref',ref)# 计算轮廓#cv2.findContours()函数接受的参数为二值图，即黑白的（不是灰度图）,cv2.RETR_EXTERNAL只检测外轮廓，cv2.CHAIN_APPROX_SIMPLE只保留终点坐标#返回的list中每个元素都是图像中的一个轮廓ref_, refCnts, hierarchy = cv2.findContours(ref.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)cv2.drawContours(img,refCnts,-1,(0,0,255),3) # -1表示画所有的轮廓cv_show('img',img)print (np.array(refCnts).shape)refCnts = myutils.sort_contours(refCnts, method=\"left-to-right\")[0] #排序，从左到右，从上到下digits = {}# 遍历每一个轮廓for (i, c) in enumerate(refCnts):\t# 计算外接矩形并且resize成合适大小\t(x, y, w, h) = cv2.boundingRect(c)\troi = ref[y:y + h, x:x + w]\troi = cv2.resize(roi, (57, 88))\t# 每一个数字对应每一个模板\tdigits[i] = roi# 初始化卷积核rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9, 3))sqKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))#读取输入图像，预处理# image = cv2.imread(args[\"image\"])image = cv2.imread('images/credit_card_02.png')cv_show('image',image)image = myutils.resize(image, width=300)gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)cv_show('gray',gray)#礼帽操作，突出更明亮的区域tophat = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, rectKernel)cv_show('tophat',tophat)#gradX = cv2.Sobel(tophat, ddepth=cv2.CV_32F, dx=1, dy=0, #ksize=-1相当于用3*3的\tksize=-1)gradX = np.absolute(gradX)(minVal, maxVal) = (np.min(gradX), np.max(gradX))gradX = (255 * ((gradX - minVal) / (maxVal - minVal)))gradX = gradX.astype(\"uint8\")print (np.array(gradX).shape)cv_show('gradX',gradX)#通过闭操作（先膨胀，再腐蚀）将数字连在一起gradX = cv2.morphologyEx(gradX, cv2.MORPH_CLOSE, rectKernel)cv_show('gradX',gradX)#THRESH_OTSU会自动寻找合适的阈值，适合双峰，需把阈值参数设置为0thresh = cv2.threshold(gradX, 0, 255,\tcv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]cv_show('thresh',thresh)#再来一个闭操作thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, sqKernel) #再来一个闭操作cv_show('thresh',thresh)# 计算轮廓thresh_, threshCnts, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,\tcv2.CHAIN_APPROX_SIMPLE)cnts = threshCntscur_img = image.copy()cv2.drawContours(cur_img,cnts,-1,(0,0,255),3)cv_show('img',cur_img)locs = []# 遍历轮廓for (i, c) in enumerate(cnts):\t# 计算矩形\t(x, y, w, h) = cv2.boundingRect(c)\tar = w / float(h)\t# 选择合适的区域，根据实际任务来，这里的基本都是四个数字一组\tif ar 2.5 and ar 4.0: if (w 40 and w 55) and (h 10 and h 20): #符合的留下来 locs.append((x, y, w, h))# 将符合的轮廓从左到右排序locs = sorted(locs, key=lambda x:x[0])output = []# 遍历每一个轮廓中的数字for (i, (gX, gY, gW, gH)) in enumerate(locs):\t# initialize the list of group digits\tgroupOutput = []\t# 根据坐标提取每一个组\tgroup = gray[gY - 5:gY + gH + 5, gX - 5:gX + gW + 5]\tcv_show('group',group)\t# 预处理\tgroup = cv2.threshold(group, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]\tcv_show('group',group)\t# 计算每一组的轮廓\tgroup_,digitCnts,hierarchy = cv2.findContours(group.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\tdigitCnts = contours.sort_contours(digitCnts, method=\"left-to-right\")[0]\t# 计算每一组中的每一个数值\tfor c in digitCnts: # 找到当前数值的轮廓，resize成合适的的大小 (x, y, w, h) = cv2.boundingRect(c) roi = group[y:y + h, x:x + w] roi = cv2.resize(roi, (57, 88)) # 模板匹配要求大小一致 cv_show('roi',roi) # 计算匹配得分 scores = [] # 在模板中计算每一个得分 for (digit, digitROI) in digits.items(): # 模板匹配 result = cv2.matchTemplate(roi, digitROI, cv2.TM_CCOEFF) (_, score, _, _) = cv2.minMaxLoc(result) scores.append(score) # 得到最合适的数字 groupOutput.append(str(np.argmax(scores)))\t# 画出来\tcv2.rectangle(image, (gX - 5, gY - 5), (gX + gW + 5, gY + gH + 5), (0, 0, 255), 1)\tcv2.putText(image, \"\".join(groupOutput), (gX, gY - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 0, 255), 2)\t# 得到结果\toutput.extend(groupOutput)# 打印结果print(\"Credit Card Type: {}\".format(FIRST_NUMBER[output[0]]))print(\"Credit Card #: {}\".format(\"\".join(output)))cv2.imshow(\"Image\", image)cv2.waitKey(0) 参考资料： 唐宇迪 OpenCV计算机视觉实战(Python版)","tags":["计算机视觉"],"categories":["计算机视觉"]},{"title":"图像处理-1","path":"/2019/10/13/20191013-图像处理-1/","content":"灰度图import cv2 #opencv读取的格式是BGRimport numpy as npimport matplotlib.pyplot as plt#Matplotlib是RGB%matplotlib inlineimg=cv2.imread('cat.jpg')img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)img_gray.shapecv2.imshow(\"img_gray\", img_gray)cv2.waitKey(0)cv2.destroyAllWindows() HSV H - 色调（主波长）。 S - 饱和度（纯度/颜色的阴影）。 V值（强度） hsv=cv2.cvtColor(img,cv2.COLOR_BGR2HSV)cv2.imshow(\"hsv\", hsv)cv2.waitKey(0)cv2.destroyAllWindows() 图像阈值ret, dst = cv2.threshold(src, thresh, maxval, type) src： 输入图，只能输入单通道图像，通常来说为灰度图 dst： 输出图 thresh： 阈值 maxval： 当像素值超过了阈值（或者小于阈值，根据type来决定），所赋予的值 type：二值化操作的类型，包含以下5种类型： cv2.THRESH_BINARY； cv2.THRESH_BINARY_INV； cv2.THRESH_TRUNC； cv2.THRESH_TOZERO； cv2.THRESH_TOZERO_INV cv2.THRESH_BINARY 超过阈值部分取maxval（最大值），否则取0 cv2.THRESH_BINARY_INV THRESH_BINARY的反转 cv2.THRESH_TRUNC 大于阈值部分设为阈值，否则不变 cv2.THRESH_TOZERO 大于阈值部分不改变，否则设为0 cv2.THRESH_TOZERO_INV THRESH_TOZERO的反转 | ret, thresh1 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_BINARY) ret, thresh2 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_BINARY_INV) # INV 反转 ret, thresh3 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_TRUNC) ret, thresh4 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_TOZERO) ret, thresh5 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_TOZERO_INV) titles = ['Original Image', 'BINARY', 'BINARY_INV', 'TRUNC', 'TOZERO', 'TOZERO_INV'] images = [img, thresh1, thresh2, thresh3, thresh4, thresh5] for i in range(6): plt.subplot(2, 3, i + 1), plt.imshow(images[i], 'gray') plt.title(titles[i]) plt.xticks([]), plt.yticks([]) plt.show() —|— 图像平滑img = cv2.imread('lenaNoise.png')cv2.imshow('img', img)cv2.waitKey(0)cv2.destroyAllWindows()# 均值滤波# 简单的平均卷积操作blur = cv2.blur(img, (3, 3))cv2.imshow('blur', blur)cv2.waitKey(0)cv2.destroyAllWindows()# 方框滤波# 基本和均值一样，可以选择归一化 ，-1表示颜色通道是一至的box = cv2.boxFilter(img,-1,(3,3), normalize=True)cv2.imshow('box', box)cv2.waitKey(0)cv2.destroyAllWindows()# 方框滤波# 基本和均值一样，可以选择归一化,容易越界 ，做归一化的结果跟均值滤波是一样的box = cv2.boxFilter(img,-1,(3,3), normalize=False)cv2.imshow('box', box)cv2.waitKey(0)cv2.destroyAllWindows()# 高斯滤波# 高斯模糊的卷积核里的数值是满足高斯分布，相当于更重视中间的aussian = cv2.GaussianBlur(img, (5, 5), 1)cv2.imshow('aussian', aussian)cv2.waitKey(0)cv2.destroyAllWindows()# 中值滤波# 相当于用中值代替median = cv2.medianBlur(img, 5) # 中值滤波cv2.imshow('median', median)cv2.waitKey(0)cv2.destroyAllWindows()# 展示所有的res = np.hstack((blur,aussian,median))#print (res)cv2.imshow('median vs average', res)cv2.waitKey(0)cv2.destroyAllWindows() 形态学-腐蚀操作img = cv2.imread('dige.png')cv2.imshow('img', img)cv2.waitKey(0)cv2.destroyAllWindows()kernel = np.ones((3,3),np.uint8)erosion = cv2.erode(img,kernel,iterations = 1) # erode函数cv2.imshow('erosion', erosion)cv2.waitKey(0)cv2.destroyAllWindows()pie = cv2.imread('pie.png')cv2.imshow('pie', pie)cv2.waitKey(0)cv2.destroyAllWindows()kernel = np.ones((30,30),np.uint8)erosion_1 = cv2.erode(pie,kernel,iterations = 1)erosion_2 = cv2.erode(pie,kernel,iterations = 2)erosion_3 = cv2.erode(pie,kernel,iterations = 3)res = np.hstack((erosion_1,erosion_2,erosion_3))cv2.imshow('res', res)cv2.waitKey(0)cv2.destroyAllWindows() 形态学-膨胀操作img = cv2.imread('dige.png')cv2.imshow('img', img)cv2.waitKey(0)cv2.destroyAllWindows()kernel = np.ones((3,3),np.uint8)dige_erosion = cv2.erode(img,kernel,iterations = 1)cv2.imshow('erosion', erosion)cv2.waitKey(0)cv2.destroyAllWindows()kernel = np.ones((3,3),np.uint8)dige_dilate = cv2.dilate(dige_erosion,kernel,iterations = 1)cv2.imshow('dilate', dige_dilate)cv2.waitKey(0)cv2.destroyAllWindows()pie = cv2.imread('pie.png')kernel = np.ones((30,30),np.uint8)dilate_1 = cv2.dilate(pie,kernel,iterations = 1)dilate_2 = cv2.dilate(pie,kernel,iterations = 2)dilate_3 = cv2.dilate(pie,kernel,iterations = 3)res = np.hstack((dilate_1,dilate_2,dilate_3))cv2.imshow('res', res)cv2.waitKey(0)cv2.destroyAllWindows() 开运算与闭运算# 开：先腐蚀，再膨胀img = cv2.imread('dige.png')kernel = np.ones((5,5),np.uint8)opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)cv2.imshow('opening', opening)cv2.waitKey(0)cv2.destroyAllWindows()# 闭：先膨胀，再腐蚀img = cv2.imread('dige.png')kernel = np.ones((5,5),np.uint8)closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)cv2.imshow('closing', closing)cv2.waitKey(0)cv2.destroyAllWindows() 梯度运算# 梯度=膨胀-腐蚀pie = cv2.imread('pie.png')kernel = np.ones((7,7),np.uint8)dilate = cv2.dilate(pie,kernel,iterations = 5)erosion = cv2.erode(pie,kernel,iterations = 5)res = np.hstack((dilate,erosion))cv2.imshow('res', res)cv2.waitKey(0)cv2.destroyAllWindows()gradient = cv2.morphologyEx(pie, cv2.MORPH_GRADIENT, kernel)cv2.imshow('gradient', gradient)cv2.waitKey(0)cv2.destroyAllWindows() 礼帽与黑帽 礼帽 = 原始输入-开运算结果 黑帽 = 闭运算-原始输入 | #礼帽 img = cv2.imread('dige.png') tophat = cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel) cv2.imshow('tophat', tophat) cv2.waitKey(0) cv2.destroyAllWindows() #黑帽 img = cv2.imread('dige.png') blackhat = cv2.morphologyEx(img,cv2.MORPH_BLACKHAT, kernel) cv2.imshow('blackhat ', blackhat ) cv2.waitKey(0) cv2.destroyAllWindows() —|— 参考资料： 唐宇迪 OpenCV计算机视觉实战(Python版)","tags":["计算机视觉"],"categories":["计算机视觉"]},{"title":"ocr答题卡识别","path":"/2019/10/13/ocr答题卡识别/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 | import cv2 def sort_contours(cnts, method=\"left-to-right\"): reverse = False i = 0 if method == \"right-to-left\" or method == \"bottom-to-top\": reverse = True if method == \"top-to-bottom\" or method == \"bottom-to-top\": i = 1 boundingBoxes = [cv2.boundingRect(c) for c in cnts] #用一个最小的矩形，把找到的形状包起来x,y,h,w (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes), key=lambda b: b[1][i], reverse=reverse)) return cnts, boundingBoxes def resize(image, width=None, height=None, inter=cv2.INTER_AREA): dim = None (h, w) = image.shape[:2] if width is None and height is None: return image if width is None: r = height / float(h) dim = (int(w * r), height) else: r = width / float(w) dim = (width, int(h * r)) resized = cv2.resize(image, dim, interpolation=inter) return resized —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 | # 导入工具包 from imutils import contours import numpy as np import argparse import cv2 import myutils_beifen # # 设置参数 # ap = argparse.ArgumentParser() # ap.add_argument(\"-i\", \"--image\", required=True, # help=\"path to input image\") # ap.add_argument(\"-t\", \"--template\", required=True, # help=\"path to template OCR-A image\") # args = vars(ap.parse_args()) # 指定信用卡类型 FIRST_NUMBER = { \"3\": \"American Express\", \"4\": \"Visa\", \"5\": \"MasterCard\", \"6\": \"Discover Card\" } # 绘图展示 def cv_show(name,img): cv2.imshow(name, img) cv2.waitKey(0) cv2.destroyAllWindows() # 读取一个模板图像 # img = cv2.imread(args[\"template\"]) img = cv2.imread('images/ocr_a_reference.png') cv_show('img',img) # 灰度图 ref = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY) cv_show('ref',ref) # 二值图像 ref = cv2.threshold(ref, 10, 255, cv2.THRESH_BINARY_INV)[1] cv_show('ref',ref) # 计算轮廓 #cv2.findContours()函数接受的参数为二值图，即黑白的（不是灰度图）,cv2.RETR_EXTERNAL只检测外轮廓，cv2.CHAIN_APPROX_SIMPLE只保留终点坐标 #返回的list中每个元素都是图像中的一个轮廓 ref_, refCnts, hierarchy = cv2.findContours(ref.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE) cv2.drawContours(img,refCnts,-1,(0,0,255),3) # -1表示画所有的轮廓 cv_show('img',img) print (np.array(refCnts).shape) refCnts = myutils.sort_contours(refCnts, method=\"left-to-right\")[0] #排序，从左到右，从上到下 digits = {} # 遍历每一个轮廓 for (i, c) in enumerate(refCnts): # 计算外接矩形并且resize成合适大小 (x, y, w, h) = cv2.boundingRect(c) roi = ref[y:y + h, x:x + w] roi = cv2.resize(roi, (57, 88)) # 每一个数字对应每一个模板 digits[i] = roi # 初始化卷积核 rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9, 3)) sqKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5)) #读取输入图像，预处理 # image = cv2.imread(args[\"image\"]) image = cv2.imread('images/credit_card_02.png') cv_show('image',image) image = myutils.resize(image, width=300) gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) cv_show('gray',gray) #礼帽操作，突出更明亮的区域 tophat = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, rectKernel) cv_show('tophat',tophat) # gradX = cv2.Sobel(tophat, ddepth=cv2.CV_32F, dx=1, dy=0, #ksize=-1相当于用3*3的 ksize=-1) gradX = np.absolute(gradX) (minVal, maxVal) = (np.min(gradX), np.max(gradX)) gradX = (255 * ((gradX - minVal) / (maxVal - minVal))) gradX = gradX.astype(\"uint8\") print (np.array(gradX).shape) cv_show('gradX',gradX) #通过闭操作（先膨胀，再腐蚀）将数字连在一起 gradX = cv2.morphologyEx(gradX, cv2.MORPH_CLOSE, rectKernel) cv_show('gradX',gradX) #THRESH_OTSU会自动寻找合适的阈值，适合双峰，需把阈值参数设置为0 thresh = cv2.threshold(gradX, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1] cv_show('thresh',thresh) #再来一个闭操作 thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, sqKernel) #再来一个闭操作 cv_show('thresh',thresh) # 计算轮廓 thresh_, threshCnts, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) cnts = threshCnts cur_img = image.copy() cv2.drawContours(cur_img,cnts,-1,(0,0,255),3) cv_show('img',cur_img) locs = [] # 遍历轮廓 for (i, c) in enumerate(cnts): # 计算矩形 (x, y, w, h) = cv2.boundingRect(c) ar = w / float(h) # 选择合适的区域，根据实际任务来，这里的基本都是四个数字一组 if ar 2.5 and ar 4.0: if (w 40 and w 55) and (h 10 and h 20): #符合的留下来 locs.append((x, y, w, h)) # 将符合的轮廓从左到右排序 locs = sorted(locs, key=lambda x:x[0]) output = [] # 遍历每一个轮廓中的数字 for (i, (gX, gY, gW, gH)) in enumerate(locs): # initialize the list of group digits groupOutput = [] # 根据坐标提取每一个组 group = gray[gY - 5:gY + gH + 5, gX - 5:gX + gW + 5] cv_show('group',group) # 预处理 group = cv2.threshold(group, 0, 255, cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1] cv_show('group',group) # 计算每一组的轮廓 group_,digitCnts,hierarchy = cv2.findContours(group.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE) digitCnts = contours.sort_contours(digitCnts, method=\"left-to-right\")[0] # 计算每一组中的每一个数值 for c in digitCnts: # 找到当前数值的轮廓，resize成合适的的大小 (x, y, w, h) = cv2.boundingRect(c) roi = group[y:y + h, x:x + w] roi = cv2.resize(roi, (57, 88)) # 模板匹配要求大小一致 cv_show('roi',roi) # 计算匹配得分 scores = [] # 在模板中计算每一个得分 for (digit, digitROI) in digits.items(): # 模板匹配 result = cv2.matchTemplate(roi, digitROI, cv2.TM_CCOEFF) (_, score, _, _) = cv2.minMaxLoc(result) scores.append(score) # 得到最合适的数字 groupOutput.append(str(np.argmax(scores))) # 画出来 cv2.rectangle(image, (gX - 5, gY - 5), (gX + gW + 5, gY + gH + 5), (0, 0, 255), 1) cv2.putText(image, \"\".join(groupOutput), (gX, gY - 15), cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 0, 255), 2) # 得到结果 output.extend(groupOutput) # 打印结果 print(\"Credit Card Type: {}\".format(FIRST_NUMBER[output[0]])) print(\"Credit Card #: {}\".format(\"\".join(output))) cv2.imshow(\"Image\", image) cv2.waitKey(0) —|— 参考资料： 唐宇迪 OpenCV计算机视觉实战(Python版)","tags":["OpenCV"],"categories":["计算机视觉"]},{"title":"ocr透视变换","path":"/2019/10/13/ocr透视变换/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 | # 导入工具包 import numpy as np import argparse import cv2 # 设置参数 ap = argparse.ArgumentParser() ap.add_argument(\"-i\", \"--image\", required = False, help = \"Path to the image to be scanned\",default='images/page.jpg') args = vars(ap.parse_args()) def order_points(pts): # 一共4个坐标点 rect = np.zeros((4, 2), dtype = \"float32\") # 按顺序找到对应坐标0123分别是 左上，右上，右下，左下 # 计算左上，右下 s = pts.sum(axis = 1) rect[0] = pts[np.argmin(s)] rect[2] = pts[np.argmax(s)] # 计算右上和左下 diff = np.diff(pts, axis = 1) rect[1] = pts[np.argmin(diff)] rect[3] = pts[np.argmax(diff)] return rect def four_point_transform(image, pts): # 获取输入坐标点 rect = order_points(pts) (tl, tr, br, bl) = rect # 计算输入的w和h值 widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2)) widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2)) maxWidth = max(int(widthA), int(widthB)) heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2)) heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2)) maxHeight = max(int(heightA), int(heightB)) # 变换后对应坐标位置 dst = np.array([ [0, 0], [maxWidth - 1, 0], [maxWidth - 1, maxHeight - 1], [0, maxHeight - 1]], dtype = \"float32\") # 计算变换矩阵 M = cv2.getPerspectiveTransform(rect, dst) warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight)) # 返回变换后结果 return warped def resize(image, width=None, height=None, inter=cv2.INTER_AREA): dim = None (h, w) = image.shape[:2] if width is None and height is None: return image if width is None: r = height / float(h) dim = (int(w * r), height) else: r = width / float(w) dim = (width, int(h * r)) resized = cv2.resize(image, dim, interpolation=inter) return resized # 读取输入 image = cv2.imread(args[\"image\"]) #坐标也会相同变化 ratio = image.shape[0] / 500.0 orig = image.copy() image = resize(orig, height = 500) # 预处理 gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) gray = cv2.GaussianBlur(gray, (5, 5), 0) edged = cv2.Canny(gray, 75, 200) # 展示预处理结果 print(\"STEP 1: 边缘检测\") cv2.imshow(\"Image\", image) cv2.imshow(\"Edged\", edged) cv2.waitKey(0) cv2.destroyAllWindows() # 轮廓检测 cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[1] cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5] # 遍历轮廓 for c in cnts: # 计算轮廓近似 peri = cv2.arcLength(c, True) # C表示输入的点集 # epsilon表示从原始轮廓到近似轮廓的最大距离，它是一个准确度参数 # True表示封闭的 approx = cv2.approxPolyDP(c, 0.02 * peri, True)# 越小越精准，长度的百分之多少作为精度 # 4个点的时候就拿出来 if len(approx) == 4: screenCnt = approx break # 展示结果 print(\"STEP 2: 获取轮廓\") cv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 2) cv2.imshow(\"Outline\", image) cv2.waitKey(0) cv2.destroyAllWindows() # 透视变换 warped = four_point_transform(orig, screenCnt.reshape(4, 2) * ratio) # 二值处理 warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY) ref = cv2.threshold(warped, 100, 255, cv2.THRESH_BINARY)[1] cv2.imwrite('scan.jpg', ref) # 展示结果 print(\"STEP 3: 变换\") cv2.imshow(\"Original\", resize(orig, height = 650)) cv2.imshow(\"Scanned\", resize(ref, height = 650)) cv2.waitKey(0) —|— 参考资料： 唐宇迪 OpenCV计算机视觉实战(Python版)","tags":["Python"],"categories":["计算机视觉"]},{"title":"park停车场项目实战","path":"/2019/10/13/park停车场项目实战/","content":"park 类对象1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 | # coding=utf-8 import matplotlib.pyplot as plt import cv2 import os, glob import numpy as np class Parking: # 显示图片 def show_images(self, images, cmap=None): cols = 2 rows = (len(images)+1)//cols plt.figure(figsize=(15,12)) for i, image in enumerate(images): plt.subplot(rows, cols, i+1) cmap = 'gray' if len(image.shape)==2 else cmap plt.imshow(image, cmap=cmap) plt.xticks([]) plt.yticks([]) plt.tight_layout(pad=0, h_pad=0, w_pad=0) plt.show() def cv_show(self, name, img): cv2.imshow(name,img) cv2.waitKey(0) cv2.destroyAllWindows() def select_rgb_white_yello(self, image): # 过滤掉背景 lower = np.uint8([120,120,120]) upper = np.uint8([255,255,255]) # lower_red和高于upper_red的部分分别变成0，lower_red～upper_red之间的值变成255,相当于过滤背景 white_mask = cv2.inRange(image,lower, upper) self.cv_show('white_mask', white_mask) masked = cv2.bitwise_and(image, image, mask = white_mask) self.cv_show('masked', masked) return masked def convert_gray_sacle(self,image): return cv2.cvtColor(image,cv2.COLOR_RGB2GRAY) # 检测边缘 def detect_edges(self, image, low_threshold=50, high_threshold=200): return cv2.Canny(image, low_threshold, high_threshold) def filter_region(self, image, vertices): \"\"\" 剔除不需要的地方 \"\"\" mask = np.zeros_like(image) if len(mask.shape) == 2: cv2.fillPoly(mask, vertices, 255) self.cv_show('mask', mask) return cv2.bitwise_and(image, mask) # 手动选择区域 def select_region(self,image): \"\"\" 手动选择区域 \"\"\" # first, define the polygon by vertices rows, cols = image.shape[:2] pt_1 = [cols*0.05, rows*0.90] pt_2 = [cols*0.05, rows*0.70] pt_3 = [cols*0.30, rows*0.55] pt_4 = [cols*0.6, rows*0.15] pt_5 = [cols*0.90, rows*0.15] pt_6 = [cols*0.90, rows*0.90] vertices = np.array([[pt_1, pt_2, pt_3, pt_4, pt_5, pt_6]], dtype=np.int32) point_img = image.copy() point_img = cv2.cvtColor(point_img, cv2.COLOR_GRAY2RGB) for point in vertices[0]: cv2.circle(point_img, (point[0],point[1]), 10, (0,0,255), 4) self.cv_show('point_img',point_img) return self.filter_region(image, vertices) def hough_line(self, image): # 输入的图像需要是边缘检测后的结果 # minLineLengh(线的最短长度，比这个短的都被忽略)和MaxLineCap（两条直线之间的最大间隔，小于此值，认为是一条直线） # rho距离精度,theta角度精度,threshod超过设定阈值才被检测出线段 return cv2.HoughLinesP(image, rho=0.1, theta=np.pi/10, threshold=15, minLineLength=9, maxLineGap=4) def draw_lines(self, image, lines, color=[255,0,0], thickness=2, make_copy=True): # 过滤霍夫变换检测得到直线 if make_copy: image = np.copy(image) cleaned = [] for line in lines: for x1, y1, x2, y2 in line: if abs(y2-y1) =1 and abs(x2-x1) =25 and abs(x2-x1) = 55: cleaned.append((x1,y1,x2,y2)) cv2.line(image, (x1, y1), (x2, y2), color, thickness) print('No lines detected: ', len(cleaned)) return image def identify_blocks(self, image, lines, make_copy=True): if make_copy: new_image = np.copy(image) # step 1: 过滤部分直线 cleaned = [] for line in lines: for x1,y1,x2,y2 in line: if abs(y2-y1) =1 and abs(x2-x1) =25 and abs(x2-x1) = 55: cleaned.append((x1,y1,x2,y2)) # step 2: 对直线按照x1进行排序 import operator list1 = sorted(cleaned, key=operator.itemgetter(0,1)) # b=operator.itemgetter(1,0) //定义函数b，获取对象的第1个域和第0个的值 # b(a) # (2, 1) # step 3: 找到多个列，相当于每列是一排车 clusters = {} dIndex = 0 clus_dist = 10 for i in range(len(list1)-1): distance = abs(list1[i+1][0] - list1[i][0]) if distance = clus_dist: if not dIndex in clusters.keys(): clusters[dIndex] = [] clusters[dIndex].append(list1[i]) clusters[dIndex].append(list1[i + 1]) else: dIndex += 1 # step 4: 得到坐标 rects = {} i = 0 for key in clusters: all_list = clusters[key] cleaned = list(set(all_list)) if len(cleaned) 5: cleaned = sorted(cleaned, key=lambda tup: tup[1]) avg_y1 = cleaned[0][1] avg_y2 = cleaned[-1][1] avg_x1 = 0 avg_x2 = 0 for tup in cleaned: avg_x1 += tup[0] avg_x2 += tup[2] avg_x1 = avg_x1/len(cleaned) avg_x2 = avg_x2/len(cleaned) rects[i] = (avg_x1, avg_y1, avg_x2, avg_y2) i += 1 print(\"Num Parking Lanes: \", len(rects)) # step 5: 把矩形画出来 buff = 7 for key in rects: tup_topLeft = (int(rects[key][0] - buff), int(rects[key][1])) tup_botRight = (int(rects[key][2] + buff), int(rects[key][3])) cv2.rectangle(new_image, tup_topLeft,tup_botRight,(0,255,0),3) return new_image, rects def draw_parking(self, image, rects, make_copy=True, color=[255,0,0],thickness=2, save=True): if make_copy: new_image = np.copy(image) gap = 15.5 spot_dict = {} # 字典：一个车位对应一个位置 tot_spots = 0 #微调 adj_y1 = {0: 20, 1:-10, 2:0, 3:-11, 4:28, 5:5, 6:-15, 7:-15, 8:-10, 9:-30, 10:9, 11:-32} adj_y2 = {0: 30, 1: 50, 2:15, 3:10, 4:-15, 5:15, 6:15, 7:-20, 8:15, 9:15, 10:0, 11:30} adj_x1 = {0: -8, 1:-15, 2:-15, 3:-15, 4:-15, 5:-15, 6:-15, 7:-15, 8:-10, 9:-10, 10:-10, 11:0} adj_x2 = {0: 0, 1: 15, 2:15, 3:15, 4:15, 5:15, 6:15, 7:15, 8:10, 9:10, 10:10, 11:0} for key in rects: tup = rects[key] x1 = int(tup[0]+ adj_x1[key]) x2 = int(tup[2]+ adj_x2[key]) y1 = int(tup[1] + adj_y1[key]) y2 = int(tup[3] + adj_y2[key]) cv2.rectangle(new_image, (x1, y1),(x2,y2),(0,255,0),2) num_splits = int(abs(y2-y1)//gap) for i in range(0, num_splits+1): y = int(y1 + i*gap) cv2.line(new_image, (x1, y), (x2, y), color, thickness) if key 0 and key len(rects) -1 : #竖直线 x = int((x1 + x2)/2) cv2.line(new_image, (x, y1), (x, y2), color, thickness) # 计算数量 if key == 0 or key == (len(rects) -1): tot_spots += num_splits +1 else: tot_spots += 2*(num_splits +1) # 字典对应好 if key == 0 or key == (len(rects) -1): for i in range(0, num_splits+1): cur_len = len(spot_dict) y = int(y1 + i*gap) spot_dict[(x1, y, x2, y+gap)] = cur_len +1 else: for i in range(0, num_splits+1): cur_len = len(spot_dict) y = int(y1 + i*gap) x = int((x1 + x2)/2) spot_dict[(x1, y, x, y+gap)] = cur_len +1 spot_dict[(x, y, x2, y+gap)] = cur_len +2 print(\"total parking spaces: \", tot_spots, cur_len) if save: filename = 'with_parking.jpg' cv2.imwrite(filename, new_image) return new_image, spot_dict def assign_spots_map(self,image, spot_dict, make_copy = True, color=[255, 0, 0], thickness=2): if make_copy: new_image = np.copy(image) for spot in spot_dict.keys(): (x1, y1, x2, y2) = spot cv2.rectangle(new_image, (int(x1),int(y1)), (int(x2),int(y2)), color, thickness) return new_image def save_images_for_cnn(self,image, spot_dict, folder_name ='cnn_data'): for spot in spot_dict.keys(): (x1, y1, x2, y2) = spot (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2)) #裁剪图像 spot_img = image[y1:y2, x1:x2] spot_img = cv2.resize(spot_img, (0,0), fx=2.0, fy=2.0) spot_id = spot_dict[spot] filename = 'spot' + str(spot_id) +'.jpg' print(spot_img.shape, filename, (x1,x2,y1,y2)) cv2.imwrite(os.path.join(folder_name, filename), spot_img) def make_prediction(self,image, model, class_dictionary): # 预处理 img = image/255. # 转换成4D tensor image = np.expand_dims(img,axis=0) # 用训练好的模型进行训练 class_predicted = model.predict(image) inID = np.argmax(class_predicted[0]) label = class_dictionary[inID] return label def predict_on_image(self, image, spot_dict, model, class_dictionary, make_copy=True, color = [0,255,0], alpha=0.5): if make_copy: new_image = np.copy(image) overlay = np.copy(image) self.cv_show('new_image',new_image) cnt_empty = 0 all_spots = 0 for spot in spot_dict.key(): all_spots += 1 (x1, y1, x2, y2) = spot (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2)) spot_img = image[y1:y2, x1:x2] spot_img = cv2.resize(spot_img, (48, 48)) label = self.make_prediction(spot_img,model,class_dictionary) if label == 'empty': cv2.rectangle(overlay, (int(x1),int(y1)), (int(x2),int(y2)), color, -1) cnt_empty += 1 cv2.addWeighted(overlay, alpha, new_image, 1 - alpha, 0, new_image) # 图像融合 cv2.putText(new_image, \"Available: %d spots\" %cnt_empty, (30, 95), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) cv2.putText(new_image, \"Total: %d spots\" %all_spots, (30, 125), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) save = False if save: filename = 'with_marking.jpg' cv2.imwrite(filename, new_image) self.cv_show('new_image',new_image) return new_image def predict_on_video(self,video_name,final_spot_dict, model,class_dictionary,ret=True): cap = cv2.VideoCapture(video_name) count = 0 while ret: ret, image = cap.read() count += 1 if count == 5: count = 0 new_image = np.copy(image) overlay = np.copy(image) cnt_empty = 0 all_spots = 0 color = [0, 255, 0] alpha=0.5 for spot in final_spot_dict.keys(): all_spots += 1 (x1, y1, x2, y2) = spot (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2)) spot_img = image[y1:y2, x1:x2] spot_img = cv2.resize(spot_img, (48,48)) label = self.make_prediction(spot_img,model,class_dictionary) if label == 'empty': cv2.rectangle(overlay, (int(x1),int(y1)), (int(x2),int(y2)), color, -1) cnt_empty += 1 cv2.addWeighted(overlay, alpha, new_image, 1 - alpha, 0, new_image) cv2.putText(new_image, \"Available: %d spots\" %cnt_empty, (30, 95), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) cv2.putText(new_image, \"Total: %d spots\" %all_spots, (30, 125), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2) cv2.imshow('frame', new_image) if cv2.waitKey(10) 0xFF == ord('q'): break cv2.destroyAllWindows() cap.release() —|— test模块1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 | from __future__ import division import matplotlib.pyplot as plt import cv2 import os, glob import numpy as np from PIL import Image from keras.applications.imagenet_utils import preprocess_input from keras.models import load_model from keras.preprocessing import image from Parking import Parking import pickle cwd = os.getcwd() def img_process(test_images,park): white_yellow_images = list(map(park.select_rgb_white_yellow, test_images)) park.show_images(white_yellow_images) gray_images = list(map(park.convert_gray_scale, white_yellow_images)) park.show_images(gray_images) edge_images = list(map(lambda image: park.detect_edges(image), gray_images)) park.show_images(edge_images) roi_images = list(map(park.select_region, edge_images)) park.show_images(roi_images) list_of_lines = list(map(park.hough_lines, roi_images)) line_images = [] for image, lines in zip(test_images, list_of_lines): line_images.append(park.draw_lines(image, lines)) park.show_images(line_images) rect_images = [] rect_coords = [] for image, lines in zip(test_images, list_of_lines): new_image, rects = park.identify_blocks(image, lines) rect_images.append(new_image) rect_coords.append(rects) park.show_images(rect_images) delineated = [] spot_pos = [] for image, rects in zip(test_images, rect_coords): new_image, spot_dict = park.draw_parking(image, rects) delineated.append(new_image) spot_pos.append(spot_dict) park.show_images(delineated) final_spot_dict = spot_pos[1] print(len(final_spot_dict)) with open('spot_dict.pickle', 'wb') as handle: pickle.dump(final_spot_dict, handle, protocol=pickle.HIGHEST_PROTOCOL) park.save_images_for_cnn(test_images[0],final_spot_dict) return final_spot_dict def keras_model(weights_path): model = load_model(weights_path) return model def img_test(test_images,final_spot_dict,model,class_dictionary): for i in range (len(test_images)): predicted_images = park.predict_on_image(test_images[i],final_spot_dict,model,class_dictionary) def video_test(video_name,final_spot_dict,model,class_dictionary): name = video_name cap = cv2.VideoCapture(name) park.predict_on_video(name,final_spot_dict,model,class_dictionary,ret=True) if __name__ == '__main__': test_images = [plt.imread(path) for path in glob.glob('test_images/*.jpg')] weights_path = 'car1.h5' video_name = 'parking_video.mp4' class_dictionary = {} class_dictionary[0] = 'empty' class_dictionary[1] = 'occupied' park = Parking() park.show_images(test_images) final_spot_dict = img_process(test_images,park) model = keras_model(weights_path) img_test(test_images,final_spot_dict,model,class_dictionary) video_test(video_name,final_spot_dict,model,class_dictionary) —|— 利用cnn训练出一个二分类网络1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 | import numpy import os from keras import applications from keras.preprocessing.image import ImageDataGenerator from keras import optimizers from keras.models import Sequential, Model from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D from keras import backend as k from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping from keras.models import Sequential from keras.layers.normalization import BatchNormalization from keras.layers.convolutional import Conv2D from keras.layers.convolutional import MaxPooling2D from keras.initializers import TruncatedNormal from keras.layers.core import Activation from keras.layers.core import Flatten from keras.layers.core import Dropout from keras.layers.core import Dense files_train = 0 files_validation = 0 cwd = os.getcwd() folder = 'train_data/train' for sub_folder in os.listdir(folder): path, dirs, files = next(os.walk(os.path.join(folder,sub_folder))) files_train += len(files) folder = 'train_data/test' for sub_folder in os.listdir(folder): path, dirs, files = next(os.walk(os.path.join(folder,sub_folder))) files_validation += len(files) print(files_train,files_validation) img_width, img_height = 48, 48 train_data_dir = \"train_data/train\" validation_data_dir = \"train_data/test\" nb_train_samples = files_train nb_validation_samples = files_validation batch_size = 32 epochs = 15 num_classes = 2 model = applications.VGG16(weights='imagenet', include_top=False, input_shape = (img_width, img_height, 3)) for layer in model.layers[:10]: layer.trainable = False x = model.output x = Flatten()(x) predictions = Dense(num_classes, activation=\"softmax\")(x) model_final = Model(input = model.input, output = predictions) model_final.compile(loss = \"categorical_crossentropy\", optimizer = optimizers.SGD(lr=0.0001, momentum=0.9), metrics=[\"accuracy\"]) train_datagen = ImageDataGenerator( rescale = 1./255, horizontal_flip = True, fill_mode = \"nearest\", zoom_range = 0.1, width_shift_range = 0.1, height_shift_range=0.1, rotation_range=5) test_datagen = ImageDataGenerator( rescale = 1./255, horizontal_flip = True, fill_mode = \"nearest\", zoom_range = 0.1, width_shift_range = 0.1, height_shift_range=0.1, rotation_range=5) train_generator = train_datagen.flow_from_directory( train_data_dir, target_size = (img_height, img_width), batch_size = batch_size, class_mode = \"categorical\") validation_generator = test_datagen.flow_from_directory( validation_data_dir, target_size = (img_height, img_width), class_mode = \"categorical\") checkpoint = ModelCheckpoint(\"car1.h5\", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1) early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto') history_object = model_final.fit_generator( train_generator, samples_per_epoch = nb_train_samples, epochs = epochs, validation_data = validation_generator, nb_val_samples = nb_validation_samples, callbacks = [checkpoint, early]) —|— 参考资料： 唐宇迪 OpenCV计算机视觉实战(Python版)","tags":["编程"],"categories":["其他"]},{"title":"图像处理-1","path":"/2019/10/13/图像处理-1/","content":"灰度图1 2 3 4 5 6 7 8 9 10 11 12 | import cv2 #opencv读取的格式是BGR import numpy as np import matplotlib.pyplot as plt#Matplotlib是RGB %matplotlib inline img=cv2.imread('cat.jpg') img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY) img_gray.shape cv2.imshow(\"img_gray\", img_gray) cv2.waitKey(0) cv2.destroyAllWindows() —|— HSV H - 色调（主波长）。 S - 饱和度（纯度/颜色的阴影）。 V值（强度） 1 2 3 4 5 | hsv=cv2.cvtColor(img,cv2.COLOR_BGR2HSV) cv2.imshow(\"hsv\", hsv) cv2.waitKey(0) cv2.destroyAllWindows() —|— 图像阈值ret, dst = cv2.threshold(src, thresh, maxval, type) src： 输入图，只能输入单通道图像，通常来说为灰度图 dst： 输出图 thresh： 阈值 maxval： 当像素值超过了阈值（或者小于阈值，根据type来决定），所赋予的值 type：二值化操作的类型，包含以下5种类型： cv2.THRESH_BINARY； cv2.THRESH_BINARY_INV； cv2.THRESH_TRUNC； cv2.THRESH_TOZERO； cv2.THRESH_TOZERO_INV cv2.THRESH_BINARY 超过阈值部分取maxval（最大值），否则取0 cv2.THRESH_BINARY_INV THRESH_BINARY的反转 cv2.THRESH_TRUNC 大于阈值部分设为阈值，否则不变 cv2.THRESH_TOZERO 大于阈值部分不改变，否则设为0 cv2.THRESH_TOZERO_INV THRESH_TOZERO的反转 1 2 3 4 5 6 7 8 9 10 11 12 13 14 | ret, thresh1 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_BINARY) ret, thresh2 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_BINARY_INV) # INV 反转 ret, thresh3 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_TRUNC) ret, thresh4 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_TOZERO) ret, thresh5 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_TOZERO_INV) titles = ['Original Image', 'BINARY', 'BINARY_INV', 'TRUNC', 'TOZERO', 'TOZERO_INV'] images = [img, thresh1, thresh2, thresh3, thresh4, thresh5] for i in range(6): plt.subplot(2, 3, i + 1), plt.imshow(images[i], 'gray') plt.title(titles[i]) plt.xticks([]), plt.yticks([]) plt.show() —|— 图像平滑1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 | img = cv2.imread('lenaNoise.png') cv2.imshow('img', img) cv2.waitKey(0) cv2.destroyAllWindows() # 均值滤波 # 简单的平均卷积操作 blur = cv2.blur(img, (3, 3)) cv2.imshow('blur', blur) cv2.waitKey(0) cv2.destroyAllWindows() # 方框滤波 # 基本和均值一样，可以选择归一化 ，-1表示颜色通道是一至的 box = cv2.boxFilter(img,-1,(3,3), normalize=True) cv2.imshow('box', box) cv2.waitKey(0) cv2.destroyAllWindows() # 方框滤波 # 基本和均值一样，可以选择归一化,容易越界 ，做归一化的结果跟均值滤波是一样的 box = cv2.boxFilter(img,-1,(3,3), normalize=False) cv2.imshow('box', box) cv2.waitKey(0) cv2.destroyAllWindows() # 高斯滤波 # 高斯模糊的卷积核里的数值是满足高斯分布，相当于更重视中间的 aussian = cv2.GaussianBlur(img, (5, 5), 1) cv2.imshow('aussian', aussian) cv2.waitKey(0) cv2.destroyAllWindows() # 中值滤波 # 相当于用中值代替 median = cv2.medianBlur(img, 5) # 中值滤波 cv2.imshow('median', median) cv2.waitKey(0) cv2.destroyAllWindows() # 展示所有的 res = np.hstack((blur,aussian,median)) #print (res) cv2.imshow('median vs average', res) cv2.waitKey(0) cv2.destroyAllWindows() —|— 形态学-腐蚀操作1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 | img = cv2.imread('dige.png') cv2.imshow('img', img) cv2.waitKey(0) cv2.destroyAllWindows() kernel = np.ones((3,3),np.uint8) erosion = cv2.erode(img,kernel,iterations = 1) # erode函数 cv2.imshow('erosion', erosion) cv2.waitKey(0) cv2.destroyAllWindows() pie = cv2.imread('pie.png') cv2.imshow('pie', pie) cv2.waitKey(0) cv2.destroyAllWindows() kernel = np.ones((30,30),np.uint8) erosion_1 = cv2.erode(pie,kernel,iterations = 1) erosion_2 = cv2.erode(pie,kernel,iterations = 2) erosion_3 = cv2.erode(pie,kernel,iterations = 3) res = np.hstack((erosion_1,erosion_2,erosion_3)) cv2.imshow('res', res) cv2.waitKey(0) cv2.destroyAllWindows() —|— 形态学-膨胀操作1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 | img = cv2.imread('dige.png') cv2.imshow('img', img) cv2.waitKey(0) cv2.destroyAllWindows() kernel = np.ones((3,3),np.uint8) dige_erosion = cv2.erode(img,kernel,iterations = 1) cv2.imshow('erosion', erosion) cv2.waitKey(0) cv2.destroyAllWindows() kernel = np.ones((3,3),np.uint8) dige_dilate = cv2.dilate(dige_erosion,kernel,iterations = 1) cv2.imshow('dilate', dige_dilate) cv2.waitKey(0) cv2.destroyAllWindows() pie = cv2.imread('pie.png') kernel = np.ones((30,30),np.uint8) dilate_1 = cv2.dilate(pie,kernel,iterations = 1) dilate_2 = cv2.dilate(pie,kernel,iterations = 2) dilate_3 = cv2.dilate(pie,kernel,iterations = 3) res = np.hstack((dilate_1,dilate_2,dilate_3)) cv2.imshow('res', res) cv2.waitKey(0) cv2.destroyAllWindows() —|— 开运算与闭运算1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 | # 开：先腐蚀，再膨胀 img = cv2.imread('dige.png') kernel = np.ones((5,5),np.uint8) opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel) cv2.imshow('opening', opening) cv2.waitKey(0) cv2.destroyAllWindows() # 闭：先膨胀，再腐蚀 img = cv2.imread('dige.png') kernel = np.ones((5,5),np.uint8) closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel) cv2.imshow('closing', closing) cv2.waitKey(0) cv2.destroyAllWindows() —|— 梯度运算1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 | # 梯度=膨胀-腐蚀 pie = cv2.imread('pie.png') kernel = np.ones((7,7),np.uint8) dilate = cv2.dilate(pie,kernel,iterations = 5) erosion = cv2.erode(pie,kernel,iterations = 5) res = np.hstack((dilate,erosion)) cv2.imshow('res', res) cv2.waitKey(0) cv2.destroyAllWindows() gradient = cv2.morphologyEx(pie, cv2.MORPH_GRADIENT, kernel) cv2.imshow('gradient', gradient) cv2.waitKey(0) cv2.destroyAllWindows() —|— 礼帽与黑帽 礼帽 = 原始输入-开运算结果 黑帽 = 闭运算-原始输入 1 2 3 4 5 6 7 8 9 10 11 12 | #礼帽 img = cv2.imread('dige.png') tophat = cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel) cv2.imshow('tophat', tophat) cv2.waitKey(0) cv2.destroyAllWindows() #黑帽 img = cv2.imread('dige.png') blackhat = cv2.morphologyEx(img,cv2.MORPH_BLACKHAT, kernel) cv2.imshow('blackhat ', blackhat ) cv2.waitKey(0) cv2.destroyAllWindows() —|— 参考资料： 唐宇迪 OpenCV计算机视觉实战(Python版)","tags":["Python"],"categories":["计算机视觉"]},{"title":"zhongjie","path":"/2019/09/30/20190930-zhongjie/","content":"一切终止于今天吧！","tags":["其他"],"categories":["其他"]},{"title":"zhongjie","path":"/2019/09/30/zhongjie/","content":"一切终止于今天吧！","tags":["编程"],"categories":["其他"]},{"title":"mosse","path":"/2019/09/14/20190914-mosse/","content":"MOSSEMOSSE(Minimum Output Sum of Squared Error) 是2010年 的CVPR，它的全名叫做Visual Object Tracking using Adaptive Correlation Filters。 MOSSE 是第一篇将correlation filter(CF) 引入object tracking 的论文，它也是CSK和KCF/DCF等算法的基础。 CF(相关滤波)相关一般分为自相关和互相关，这里我们一般指的是互相关，假设我们有两个信号f和g f∗表示f的共轭，互相关的直接解释就是衡量两个信号在某个时刻τ时的相似程度。假设f和g的形状一样，那么一定是f和g对齐的时候二者的相似程度最大，此时达到最大的输出响应，如下图所示：卷积计算和相关计算的关系 Two-dimensional correlation is equivalent to two-dimensional convolution with the filter matrix rotated 180 degrees. 论文解读 将CF应用在tracking方面最基本的思想就是，设计一个滤波模板，使得该模板与跟踪目标的ROI做卷积运算，得到最大的输出响应。 g表示输出响应 f表示输入原始图片的灰度图像 h表示滤波模板为了简化计算，将时域的卷积转化为频域的点乘积。时域公式表示：频域公式表示：所以目标H的计算为：在跟踪的光照等其他因素的影响下，为了提高滤波模板的鲁棒性，在文章中作者对GroundTruth进行随机仿射变换得到一系列的训练样本fi，gi是由高斯函数产生的并且其峰值位置是在fi的中心,我们同时考虑m帧作为参考，这就是MOSSE模型的思想，最终该模型的目标函数表示为：将目标函数最小化，对上式在频域进行求导（复数域不同于实数域），得到：在跟踪过程中，我们只需要将以上模板与当前帧与滤波模板做相关操作，在输出响应中找到最大值的位置，该位置就是目标在当前帧中的位置。本文的参数更新的策略为：其中，η是一个超参数，为经验值。 缺点： 输入的特征为单通道灰度图像，特征表达能力有限 没有尺度更新，对于尺度变化的跟踪目标不敏感 代码解析这里面主要做的就是 ，初始帧的输入与输出来求出Ai与Bi，从而求出初始的模板Hi，下面将初始的Hi与当前帧所在的上个位置进行卷积，频域也就是进行相乘。然后找到最大值的位置也就是当前目标的中心，由于宽高不变，所以在此基础上更新宽高就可以了，实现目标跟踪。 import numpy as npimport cv2import osfrom utils import linear_mapping, pre_process, random_warp\"\"\"This module implements the basic correlation filter based tracking algorithm -- MOSSEDate: 2018-05-28\"\"\"class mosse: def __init__(self, args, img_path): # get arguments.. self.args = args self.img_path = img_path # get the img lists... self.frame_lists = self._get_img_lists(self.img_path) self.frame_lists.sort() # start to do the object tracking... def start_tracking(self): # get the image of the first frame... (read as gray scale image...) init_img = cv2.imread(self.frame_lists[0]) init_frame = cv2.cvtColor(init_img, cv2.COLOR_BGR2GRAY) init_frame = init_frame.astype(np.float32) # get the init ground truth.. [x, y, width, height] init_gt = cv2.selectROI('demo', init_img, False, False) init_gt = np.array(init_gt).astype(np.int64) # start to draw the gaussian response... response_map = self._get_gauss_response(init_frame, init_gt) # start to create the training set ... # get the goal.. print(init_gt) g = response_map[init_gt[1]:init_gt[1]+init_gt[3], init_gt[0]:init_gt[0]+init_gt[2]] print(g) fi = init_frame[init_gt[1]:init_gt[1]+init_gt[3], init_gt[0]:init_gt[0]+init_gt[2]] G = np.fft.fft2(g) # start to do the pre-training... Ai, Bi = self._pre_training(fi, G) # start the tracking... for idx in range(len(self.frame_lists)): current_frame = cv2.imread(self.frame_lists[idx]) frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY) frame_gray = frame_gray.astype(np.float32) if idx == 0: Ai = self.args.lr * Ai Bi = self.args.lr * Bi pos = init_gt.copy() clip_pos = np.array([pos[0], pos[1], pos[0]+pos[2], pos[1]+pos[3]]).astype(np.int64) else: Hi = Ai / Bi fi = frame_gray[clip_pos[1]:clip_pos[3], clip_pos[0]:clip_pos[2]] fi = pre_process(cv2.resize(fi, (init_gt[2], init_gt[3]))) Gi = Hi * np.fft.fft2(fi) gi = linear_mapping(np.fft.ifft2(Gi)) # find the max pos... max_value = np.max(gi) max_pos = np.where(gi == max_value) dy = int(np.mean(max_pos[0]) - gi.shape[0] / 2) dx = int(np.mean(max_pos[1]) - gi.shape[1] / 2) # update the position... pos[0] = pos[0] + dx pos[1] = pos[1] + dy # trying to get the clipped position [xmin, ymin, xmax, ymax] clip_pos[0] = np.clip(pos[0], 0, current_frame.shape[1]) clip_pos[1] = np.clip(pos[1], 0, current_frame.shape[0]) clip_pos[2] = np.clip(pos[0]+pos[2], 0, current_frame.shape[1]) clip_pos[3] = np.clip(pos[1]+pos[3], 0, current_frame.shape[0]) clip_pos = clip_pos.astype(np.int64) # get the current fi.. fi = frame_gray[clip_pos[1]:clip_pos[3], clip_pos[0]:clip_pos[2]] fi = pre_process(cv2.resize(fi, (init_gt[2], init_gt[3]))) # online update... Ai = self.args.lr * (G * np.conjugate(np.fft.fft2(fi))) + (1 - self.args.lr) * Ai Bi = self.args.lr * (np.fft.fft2(fi) * np.conjugate(np.fft.fft2(fi))) + (1 - self.args.lr) * Bi # visualize the tracking process... cv2.rectangle(current_frame, (pos[0], pos[1]), (pos[0]+pos[2], pos[1]+pos[3]), (255, 0, 0), 2) cv2.imshow('demo', current_frame) cv2.waitKey(100) # if record... save the frames.. if self.args.record: frame_path = 'record_frames/' + self.img_path.split('/')[1] + '/' if not os.path.exists(frame_path): os.mkdir(frame_path) cv2.imwrite(frame_path + str(idx).zfill(5) + '.png', current_frame) # pre train the filter on the first frame... def _pre_training(self, init_frame, G): height, width = G.shape fi = cv2.resize(init_frame, (width, height)) # pre-process img.. fi = pre_process(fi) Ai = G * np.conjugate(np.fft.fft2(fi)) Bi = np.fft.fft2(init_frame) * np.conjugate(np.fft.fft2(init_frame)) for _ in range(self.args.num_pretrain): if self.args.rotate: fi = pre_process(random_warp(init_frame)) else: fi = pre_process(init_frame) Ai = Ai + G * np.conjugate(np.fft.fft2(fi)) Bi = Bi + np.fft.fft2(fi) * np.conjugate(np.fft.fft2(fi)) return Ai, Bi # get the ground-truth gaussian reponse... def _get_gauss_response(self, img, gt): # get the shape of the image.. height, width = img.shape # get the mesh grid... xx, yy = np.meshgrid(np.arange(width), np.arange(height)) # get the center of the object... center_x = gt[0] + 0.5 * gt[2] center_y = gt[1] + 0.5 * gt[3] # cal the distance... dist = (np.square(xx - center_x) + np.square(yy - center_y)) / (2 * self.args.sigma) # get the response map... response = np.exp(-dist) # normalize... response = linear_mapping(response) return response # it will extract the image list def _get_img_lists(self, img_path): frame_list = [] for frame in os.listdir(img_path): if os.path.splitext(frame)[1] == '.jpg': frame_list.append(os.path.join(img_path, frame)) return frame_list # it will get the first ground truth of the video.. def _get_init_ground_truth(self, img_path): gt_path = os.path.join(img_path, 'groundtruth.txt') with open(gt_path, 'r') as f: # just read the first frame... line = f.readline() gt_pos = line.split(',') return [float(element) for element in gt_pos] 参考链接：http://simtalk.cn/2017/07/03/Object-Tracking/https://github.com/TianhongDai/mosse-object-tracking","tags":["目标跟踪"],"categories":["深度学习"]},{"title":"树的子结构","path":"/2019/09/14/20190914-树的子结构/","content":"时间限制：1秒 空间限制：32768K 热度指数：829537本题知识点： 链表 题目描述输入一个链表，输出该链表中倒数第k个结点。 # -*- coding:utf-8 -*-# class TreeNode:# def __init__(self, x):# self.val = x# self.left = None# self.right = Noneclass Solution: def HasSubtree(self, pRoot1, pRoot2): # write code here if pRoot1 is None or pRoot2 is None: return False result = False if pRoot1.val == pRoot2.val: result = self.isSubtree(pRoot1,pRoot2) if result == False: result = self.HasSubtree(pRoot1.left, pRoot2) | self.HasSubtree(pRoot1.right, pRoot2) return result def isSubtree(self,root1,root2): if root2 is None: return True if root1 is None: return False if root1.val == root2.val: return self.isSubtree(root1.left,root2.left) self.isSubtree(root1.right,root2.right) return False 运行时间：24 ms占用内存：5860K","tags":["技术"],"categories":["技术"]},{"title":"合并两个排序的链表","path":"/2019/09/14/20190914-合并两个排序的链表/","content":"时间限制：1秒 空间限制：32768K 热度指数：587032本题知识点： 链表 题目描述输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 /*struct ListNode {\tint val;\tstruct ListNode *next;\tListNode(int x) : val(x), next(NULL) {\t}};*/class Solution {public: ListNode* Merge(ListNode* pHead1, ListNode* pHead2) { if(!pHead1)return pHead2; if(!pHead2)return pHead1; if(pHead1-val=pHead2-val){ pHead1-next= Merge(pHead1-next,pHead2); return pHead1; }else{ pHead2-next= Merge(pHead1,pHead2-next); return pHead2; } }}; # -*- coding:utf-8 -*-# class ListNode:# def __init__(self, x):# self.val = x# self.next = Noneclass Solution: # 返回合并后列表 def Merge(self, pHead1, pHead2): # write code here if pHead1 is None: return pHead2 if pHead2 is None: return pHead1 if pHead1.val pHead2.val: pHead1.next = self.Merge(pHead1.next,pHead2) return pHead1 else: pHead2.next = self.Merge(pHead1,pHead2.next) return pHead2 运行时间：3ms占用内存：492k","tags":["算法"],"categories":["算法"]},{"title":"mosse","path":"/2019/09/14/mosse/","content":"MOSSEMOSSE(Minimum Output Sum of Squared Error) 是2010年 的CVPR，它的全名叫做Visual Object Tracking using Adaptive Correlation Filters。 MOSSE 是第一篇将correlation filter(CF) 引入object tracking 的论文，它也是CSK和KCF/DCF等算法的基础。 CF(相关滤波)相关一般分为自相关和互相关，这里我们一般指的是互相关，假设我们有两个信号f和g f∗表示f的共轭，互相关的直接解释就是衡量两个信号在某个时刻τ时的相似程度。假设f和g的形状一样，那么一定是f和g对齐的时候二者的相似程度最大，此时达到最大的输出响应，如下图所示：卷积计算和相关计算的关系 Two-dimensional correlation is equivalent to two-dimensional convolution with the filter matrix rotated 180 degrees. 论文解读 将CF应用在tracking方面最基本的思想就是，设计一个滤波模板，使得该模板与跟踪目标的ROI做卷积运算，得到最大的输出响应。 g表示输出响应 f表示输入原始图片的灰度图像 h表示滤波模板为了简化计算，将时域的卷积转化为频域的点乘积。时域公式表示：频域公式表示：所以目标H的计算为：在跟踪的光照等其他因素的影响下，为了提高滤波模板的鲁棒性，在文章中作者对GroundTruth进行随机仿射变换得到一系列的训练样本fi，gi是由高斯函数产生的并且其峰值位置是在fi的中心,我们同时考虑m帧作为参考，这就是MOSSE模型的思想，最终该模型的目标函数表示为：将目标函数最小化，对上式在频域进行求导（复数域不同于实数域），得到：在跟踪过程中，我们只需要将以上模板与当前帧与滤波模板做相关操作，在输出响应中找到最大值的位置，该位置就是目标在当前帧中的位置。本文的参数更新的策略为：其中，η是一个超参数，为经验值。 缺点： 输入的特征为单通道灰度图像，特征表达能力有限 没有尺度更新，对于尺度变化的跟踪目标不敏感 代码解析这里面主要做的就是 ，初始帧的输入与输出来求出Ai与Bi，从而求出初始的模板Hi，下面将初始的Hi与当前帧所在的上个位置进行卷积，频域也就是进行相乘。然后找到最大值的位置也就是当前目标的中心，由于宽高不变，所以在此基础上更新宽高就可以了，实现目标跟踪。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 | import numpy as np import cv2 import os from utils import linear_mapping, pre_process, random_warp \"\"\" This module implements the basic correlation filter based tracking algorithm -- MOSSE Date: 2018-05-28 \"\"\" class mosse: def __init__(self, args, img_path): # get arguments.. self.args = args self.img_path = img_path # get the img lists... self.frame_lists = self._get_img_lists(self.img_path) self.frame_lists.sort() # start to do the object tracking... def start_tracking(self): # get the image of the first frame... (read as gray scale image...) init_img = cv2.imread(self.frame_lists[0]) init_frame = cv2.cvtColor(init_img, cv2.COLOR_BGR2GRAY) init_frame = init_frame.astype(np.float32) # get the init ground truth.. [x, y, width, height] init_gt = cv2.selectROI('demo', init_img, False, False) init_gt = np.array(init_gt).astype(np.int64) # start to draw the gaussian response... response_map = self._get_gauss_response(init_frame, init_gt) # start to create the training set ... # get the goal.. print(init_gt) g = response_map[init_gt[1]:init_gt[1]+init_gt[3], init_gt[0]:init_gt[0]+init_gt[2]] print(g) fi = init_frame[init_gt[1]:init_gt[1]+init_gt[3], init_gt[0]:init_gt[0]+init_gt[2]] G = np.fft.fft2(g) # start to do the pre-training... Ai, Bi = self._pre_training(fi, G) # start the tracking... for idx in range(len(self.frame_lists)): current_frame = cv2.imread(self.frame_lists[idx]) frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY) frame_gray = frame_gray.astype(np.float32) if idx == 0: Ai = self.args.lr * Ai Bi = self.args.lr * Bi pos = init_gt.copy() clip_pos = np.array([pos[0], pos[1], pos[0]+pos[2], pos[1]+pos[3]]).astype(np.int64) else: Hi = Ai / Bi fi = frame_gray[clip_pos[1]:clip_pos[3], clip_pos[0]:clip_pos[2]] fi = pre_process(cv2.resize(fi, (init_gt[2], init_gt[3]))) Gi = Hi * np.fft.fft2(fi) gi = linear_mapping(np.fft.ifft2(Gi)) # find the max pos... max_value = np.max(gi) max_pos = np.where(gi == max_value) dy = int(np.mean(max_pos[0]) - gi.shape[0] / 2) dx = int(np.mean(max_pos[1]) - gi.shape[1] / 2) # update the position... pos[0] = pos[0] + dx pos[1] = pos[1] + dy # trying to get the clipped position [xmin, ymin, xmax, ymax] clip_pos[0] = np.clip(pos[0], 0, current_frame.shape[1]) clip_pos[1] = np.clip(pos[1], 0, current_frame.shape[0]) clip_pos[2] = np.clip(pos[0]+pos[2], 0, current_frame.shape[1]) clip_pos[3] = np.clip(pos[1]+pos[3], 0, current_frame.shape[0]) clip_pos = clip_pos.astype(np.int64) # get the current fi.. fi = frame_gray[clip_pos[1]:clip_pos[3], clip_pos[0]:clip_pos[2]] fi = pre_process(cv2.resize(fi, (init_gt[2], init_gt[3]))) # online update... Ai = self.args.lr * (G * np.conjugate(np.fft.fft2(fi))) + (1 - self.args.lr) * Ai Bi = self.args.lr * (np.fft.fft2(fi) * np.conjugate(np.fft.fft2(fi))) + (1 - self.args.lr) * Bi # visualize the tracking process... cv2.rectangle(current_frame, (pos[0], pos[1]), (pos[0]+pos[2], pos[1]+pos[3]), (255, 0, 0), 2) cv2.imshow('demo', current_frame) cv2.waitKey(100) # if record... save the frames.. if self.args.record: frame_path = 'record_frames/' + self.img_path.split('/')[1] + '/' if not os.path.exists(frame_path): os.mkdir(frame_path) cv2.imwrite(frame_path + str(idx).zfill(5) + '.png', current_frame) # pre train the filter on the first frame... def _pre_training(self, init_frame, G): height, width = G.shape fi = cv2.resize(init_frame, (width, height)) # pre-process img.. fi = pre_process(fi) Ai = G * np.conjugate(np.fft.fft2(fi)) Bi = np.fft.fft2(init_frame) * np.conjugate(np.fft.fft2(init_frame)) for _ in range(self.args.num_pretrain): if self.args.rotate: fi = pre_process(random_warp(init_frame)) else: fi = pre_process(init_frame) Ai = Ai + G * np.conjugate(np.fft.fft2(fi)) Bi = Bi + np.fft.fft2(fi) * np.conjugate(np.fft.fft2(fi)) return Ai, Bi # get the ground-truth gaussian reponse... def _get_gauss_response(self, img, gt): # get the shape of the image.. height, width = img.shape # get the mesh grid... xx, yy = np.meshgrid(np.arange(width), np.arange(height)) # get the center of the object... center_x = gt[0] + 0.5 * gt[2] center_y = gt[1] + 0.5 * gt[3] # cal the distance... dist = (np.square(xx - center_x) + np.square(yy - center_y)) / (2 * self.args.sigma) # get the response map... response = np.exp(-dist) # normalize... response = linear_mapping(response) return response # it will extract the image list def _get_img_lists(self, img_path): frame_list = [] for frame in os.listdir(img_path): if os.path.splitext(frame)[1] == '.jpg': frame_list.append(os.path.join(img_path, frame)) return frame_list # it will get the first ground truth of the video.. def _get_init_ground_truth(self, img_path): gt_path = os.path.join(img_path, 'groundtruth.txt') with open(gt_path, 'r') as f: # just read the first frame... line = f.readline() gt_pos = line.split(',') return [float(element) for element in gt_pos] —|— 参考链接：http://simtalk.cn/2017/07/03/Object-Tracking/https://github.com/TianhongDai/mosse-object-tracking","tags":["Python"],"categories":["计算机视觉"]},{"title":"合并两个排序的链表","path":"/2019/09/14/合并两个排序的链表/","content":"时间限制：1秒 空间限制：32768K 热度指数：587032本题知识点： 链表 题目描述输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 | /* struct ListNode { int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) { } };*/ class Solution { public: ListNode* Merge(ListNode* pHead1, ListNode* pHead2) { if(!pHead1)return pHead2; if(!pHead2)return pHead1; if(pHead1-val=pHead2-val){ pHead1-next= Merge(pHead1-next,pHead2); return pHead1; }else{ pHead2-next= Merge(pHead1,pHead2-next); return pHead2; } } }; —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 | # -*- coding:utf-8 -*- # class ListNode: # def __init__(self, x): # self.val = x # self.next = None class Solution: # 返回合并后列表 def Merge(self, pHead1, pHead2): # write code here if pHead1 is None: return pHead2 if pHead2 is None: return pHead1 if pHead1.val pHead2.val: pHead1.next = self.Merge(pHead1.next,pHead2) return pHead1 else: pHead2.next = self.Merge(pHead1,pHead2.next) return pHead2 —|— 运行时间：3ms占用内存：492k","tags":["编程"],"categories":["算法"]},{"title":"树的子结构","path":"/2019/09/14/树的子结构/","content":"时间限制：1秒 空间限制：32768K 热度指数：829537本题知识点： 链表 题目描述输入一个链表，输出该链表中倒数第k个结点。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 | # -*- coding:utf-8 -*- # class TreeNode: # def __init__(self, x): # self.val = x # self.left = None # self.right = None class Solution: def HasSubtree(self, pRoot1, pRoot2): # write code here if pRoot1 is None or pRoot2 is None: return False result = False if pRoot1.val == pRoot2.val: result = self.isSubtree(pRoot1,pRoot2) if result == False: result = self.HasSubtree(pRoot1.left, pRoot2) | self.HasSubtree(pRoot1.right, pRoot2) return result def isSubtree(self,root1,root2): if root2 is None: return True if root1 is None: return False if root1.val == root2.val: return self.isSubtree(root1.left,root2.left) self.isSubtree(root1.right,root2.right) return False —|— 运行时间：24 ms占用内存：5860K","tags":["编程"],"categories":["算法"]},{"title":"反转链表","path":"/2019/09/12/20190912-反转链表/","content":"时间限制：1秒 空间限制：32768K 热度指数：606485本题知识点： 链表 题目描述输入一个链表，反转链表后，输出新链表的表头。 /*struct ListNode {\tint val;\tstruct ListNode *next;\tListNode(int x) : val(x), next(NULL) {\t}};*/class Solution {public: ListNode* ReverseList(ListNode* pHead) { ListNode *prev = NULL; ListNode *curr = pHead; ListNode *next = NULL; while(curr!=NULL) { next = curr-next; curr-next = prev; prev = curr; curr = next; } return prev; }}; 运行时间：4ms占用内存：488","tags":["算法"],"categories":["算法"]},{"title":"四边形按照顺时针排序","path":"/2019/09/12/20190912-四边形按照顺时针排序/","content":"做图像检测的时候处理数据经常遇到给出矩形的四个坐标点，要求找出左上角坐标并对乱序的坐标按顺时针或者逆时针进行排序 from scipy.spatial import distance as distimport numpy as npimport mathdef cos_dist(a, b): if len(a) != len(b): return None part_up = 0.0 a_sq = 0.0 b_sq = 0.0 print(a, b) print(zip(a, b)) for a1, b1 in zip(a, b): part_up += a1*b1 a_sq += a1**2 b_sq += b1**2 part_down = math.sqrt(a_sq*b_sq) if part_down == 0.0: return None else: return part_up / part_down# this function is confined to rectangledef order_points(pts): # sort the points based on their x-coordinates xSorted = pts[np.argsort(pts[:, 0]), :] # grab the left-most and right-most points from the sorted # x-roodinate points leftMost = xSorted[:2, :] rightMost = xSorted[2:, :] # now, sort the left-most coordinates according to their # y-coordinates so we can grab the top-left and bottom-left # points, respectively leftMost = leftMost[np.argsort(leftMost[:, 1]), :] (tl, bl) = leftMost # now that we have the top-left coordinate, use it as an # anchor to calculate the Euclidean distance between the # top-left and right-most points; by the Pythagorean # theorem, the point with the largest distance will be # our bottom-right point D = dist.cdist(tl[np.newaxis], rightMost, \"euclidean\")[0] (br, tr) = rightMost[np.argsort(D)[::-1], :] # return the coordinates in top-left, top-right, # bottom-right, and bottom-left order return np.array([tl, tr, br, bl], dtype=\"float32\")def order_points_quadrangle(pts): # sort the points based on their x-coordinates xSorted = pts[np.argsort(pts[:, 0]), :] # grab the left-most and right-most points from the sorted # x-roodinate points leftMost = xSorted[:2, :] rightMost = xSorted[2:, :] # now, sort the left-most coordinates according to their # y-coordinates so we can grab the top-left and bottom-left # points, respectively leftMost = leftMost[np.argsort(leftMost[:, 1]), :] (tl, bl) = leftMost # now that we have the top-left and bottom-left coordinate, use it as an # base vector to calculate the angles between the other two vectors vector_0 = np.array(bl-tl) vector_1 = np.array(rightMost[0]-tl) vector_2 = np.array(rightMost[1]-tl) angle = [np.arccos(cos_dist(vector_0, vector_1)), np.arccos(cos_dist(vector_0, vector_2))] (br, tr) = rightMost[np.argsort(angle), :] # return the coordinates in top-left, top-right, # bottom-right, and bottom-left order return np.array([tl, tr, br, bl], dtype=\"float32\")testdata =[1074,439,1078,424,991,427,991,411]points = numpy.array(array).reshape(4,2)poit = order_points_quadrangle(points)poitarray([[ 991., 411.], [1078., 424.], [1074., 439.], [ 991., 427.]], dtype=float32) 参考链接：http://www.bnee.net/article/821708.html","tags":["算法"],"categories":["算法"]},{"title":"四边形按照顺时针排序","path":"/2019/09/12/四边形按照顺时针排序/","content":"做图像检测的时候处理数据经常遇到给出矩形的四个坐标点，要求找出左上角坐标并对乱序的坐标按顺时针或者逆时针进行排序 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 | from scipy.spatial import distance as dist import numpy as np import math def cos_dist(a, b): if len(a) != len(b): return None part_up = 0.0 a_sq = 0.0 b_sq = 0.0 print(a, b) print(zip(a, b)) for a1, b1 in zip(a, b): part_up += a1*b1 a_sq += a1**2 b_sq += b1**2 part_down = math.sqrt(a_sq*b_sq) if part_down == 0.0: return None else: return part_up / part_down # this function is confined to rectangle def order_points(pts): # sort the points based on their x-coordinates xSorted = pts[np.argsort(pts[:, 0]), :] # grab the left-most and right-most points from the sorted # x-roodinate points leftMost = xSorted[:2, :] rightMost = xSorted[2:, :] # now, sort the left-most coordinates according to their # y-coordinates so we can grab the top-left and bottom-left # points, respectively leftMost = leftMost[np.argsort(leftMost[:, 1]), :] (tl, bl) = leftMost # now that we have the top-left coordinate, use it as an # anchor to calculate the Euclidean distance between the # top-left and right-most points; by the Pythagorean # theorem, the point with the largest distance will be # our bottom-right point D = dist.cdist(tl[np.newaxis], rightMost, \"euclidean\")[0] (br, tr) = rightMost[np.argsort(D)[::-1], :] # return the coordinates in top-left, top-right, # bottom-right, and bottom-left order return np.array([tl, tr, br, bl], dtype=\"float32\") def order_points_quadrangle(pts): # sort the points based on their x-coordinates xSorted = pts[np.argsort(pts[:, 0]), :] # grab the left-most and right-most points from the sorted # x-roodinate points leftMost = xSorted[:2, :] rightMost = xSorted[2:, :] # now, sort the left-most coordinates according to their # y-coordinates so we can grab the top-left and bottom-left # points, respectively leftMost = leftMost[np.argsort(leftMost[:, 1]), :] (tl, bl) = leftMost # now that we have the top-left and bottom-left coordinate, use it as an # base vector to calculate the angles between the other two vectors vector_0 = np.array(bl-tl) vector_1 = np.array(rightMost[0]-tl) vector_2 = np.array(rightMost[1]-tl) angle = [np.arccos(cos_dist(vector_0, vector_1)), np.arccos(cos_dist(vector_0, vector_2))] (br, tr) = rightMost[np.argsort(angle), :] # return the coordinates in top-left, top-right, # bottom-right, and bottom-left order return np.array([tl, tr, br, bl], dtype=\"float32\") testdata =[1074,439,1078,424,991,427,991,411] points = numpy.array(array).reshape(4,2) poit = order_points_quadrangle(points) poit array([[ 991., 411.], [1078., 424.], [1074., 439.], [ 991., 427.]], dtype=float32) —|— 参考链接：http://www.bnee.net/article/821708.html","tags":["Python"],"categories":["算法"]},{"title":"反转链表","path":"/2019/09/12/反转链表/","content":"时间限制：1秒 空间限制：32768K 热度指数：606485本题知识点： 链表 题目描述输入一个链表，反转链表后，输出新链表的表头。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 | /* struct ListNode { int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) { } };*/ class Solution { public: ListNode* ReverseList(ListNode* pHead) { ListNode *prev = NULL; ListNode *curr = pHead; ListNode *next = NULL; while(curr!=NULL) { next = curr-next; curr-next = prev; prev = curr; curr = next; } return prev; } }; —|— 运行时间：4ms占用内存：488","tags":["编程"],"categories":["算法"]},{"title":"链表中倒数第k个结点","path":"/2019/09/10/20190910-链表中倒数第k个结点/","content":"时间限制：1秒 空间限制：32768K 热度指数：829537本题知识点： 链表 题目描述输入一个链表，输出该链表中倒数第k个结点。 /*struct ListNode {\tint val;\tstruct ListNode *next;\tListNode(int x) : val(x), next(NULL) {\t}};*/class Solution {public: ListNode* FindKthToTail(ListNode* pListHead, unsigned int k) { if (k == 0) return NULL;//如果K为0，返回NULL queueListNode* que; ListNode *node = pListHead; while (node != NULL) { if (que.size() == k) { que.pop(); } que.push(node); node = node-next; } if (que.size() == k) return que.front(); else return NULL;//如果k大于链表的最大长度，返回NULL }}; 运行时间：3ms占用内存：472K","tags":["算法"],"categories":["算法"]},{"title":"链表中倒数第k个结点","path":"/2019/09/10/链表中倒数第k个结点/","content":"时间限制：1秒 空间限制：32768K 热度指数：829537本题知识点： 链表 题目描述输入一个链表，输出该链表中倒数第k个结点。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 | /* struct ListNode { int val; struct ListNode *next; ListNode(int x) : val(x), next(NULL) { } };*/ class Solution { public: ListNode* FindKthToTail(ListNode* pListHead, unsigned int k) { if (k == 0) return NULL;//如果K为0，返回NULL queueListNode* que; ListNode *node = pListHead; while (node != NULL) { if (que.size() == k) { que.pop(); } que.push(node); node = node-next; } if (que.size() == k) return que.front(); else return NULL;//如果k大于链表的最大长度，返回NULL } }; —|— 运行时间：3ms占用内存：472K","tags":["编程"],"categories":["算法"]},{"title":"二进制中1的个数","path":"/2019/09/08/20190908-二进制中1的个数/","content":"时间限制：1秒 空间限制：32768K 热度指数：489798本题知识点： 进制转化 补码 反码 原码 题目描述输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 class Solution {public: int NumberOf1(int n) { int count=0; while(n!=0) { count++; n = (n-1)n; } return count; }}; 运行时间：3ms占用内存：356k","tags":["算法"],"categories":["算法"]},{"title":"从csv中读取数据","path":"/2019/09/08/20190908-从csv中读取数据/","content":"import csvimport numpy as npdef loadDataSet(filename): # 读取数据 with open(filename) as f: dataMat = [] labelMat = [] f_csv = csv.reader(f) ## 用csv读取直接是个list headers = next(f_csv) for row in f_csv: dataMat.append([float(row[0]), float(row[1])]) labelMat.append(float(row[2])) f.close() return dataMat, labelMatdef loadData(filename): # 读取数据 dataMat=[] labelMat=[] fr=open(filename) next(fr) # 忽略第一行 for line in fr.readlines(): lineArr=line.strip().split(',') dataMat.append([lineArr[0],lineArr[1]]) labelMat.append(lineArr[2]) return dataMat,labelMat # 返回数据特征和数据类别if __name__ == '__main__': dataMat,labelMat = loadData('test_data.csv') print(dataMat,labelMat)","tags":["Python"],"categories":["Python"]},{"title":"支持向量机","path":"/2019/09/08/20190908-支持向量机/","content":"SVM的本质：寻找最大的间隔支持向量：距离超平面最近的那些点SMO算法的原理： 每次循环中选择两个alpha进行优化处理。一旦找到一对合适的alpha，那么就增大其中一个同时减小另一个。合适：条件一，两个alpha要在间隔边界之外；条件二，这两个alpha还没有进行过区间化处理或不在边界上 from numpy import *import csvdef loadDataSet(filename): # 读取数据 with open(filename) as f: dataMat = [] labelMat = [] f_csv = csv.reader(f) ## 用csv读取直接是个list headers = next(f_csv) for row in f_csv: dataMat.append([float(row[0]), float(row[1])]) labelMat.append(float(row[2])) f.close() return dataMat, labelMatdef selectJrand(i, m): # 在0-m中随机选择一个不是i的整数 j = i while (j == i): j = int(random.uniform(0, m)) return jdef clipAlpha(aj, H, L): # 保证a在L和H范围内（L = a = H） if aj H: aj = H if L aj: aj = L return ajdef kernelTrans(X, A, kTup): # 核函数，输入参数,X:支持向量的特征树；A：某一行特征数据；kTup：('lin',k1)核函数的类型和参数 m, n = shape(X) K = mat(zeros((m, 1))) print(\"A shape : \",A.shape) print(\"X shape : \",X.shape) if kTup[0] == 'lin': # 线性函数 K = X * A.T elif kTup[0] == 'rbf': # 径向基函数(radial bias function) for j in range(m): deltaRow = X[j, :] - A K[j] = deltaRow * deltaRow.T K = exp(K / (-1 * kTup[1] ** 2)) # 返回生成的结果 else: raise NameError('Houston We Have a Problem -- That Kernel is not recognized') return K# 定义类，方便存储数据class optStruct: def __init__(self, dataMatIn, classLabels, C, toler, kTup): # 存储各类参数 self.X = dataMatIn # 数据特征 self.labelMat = classLabels # 数据类别 self.C = C # 软间隔参数C，参数越大，非线性拟合能力越强 self.tol = toler # 停止阀值 self.m = shape(dataMatIn)[0] # 数据 b行数 self.alphas = mat(zeros((self.m, 1))) self.b = 0 # 初始设为0 self.eCache = mat(zeros((self.m, 2))) # 缓存 self.K = mat(zeros((self.m, self.m))) # 核函数的计算结果 for i in range(self.m): self.K[:, i] = kernelTrans(self.X, self.X[i, :], kTup)def calcEk(oS, k): # 计算Ek（参考《统计学习方法》p127公式7.105） fXk = float(multiply(oS.alphas, oS.labelMat).T * oS.K[:, k] + oS.b) Ek = fXk - float(oS.labelMat[k]) return Ek# 随机选取aj，并返回其E值def selectJ(i, oS, Ei): maxK = -1 maxDeltaE = 0 Ej = 0 oS.eCache[i] = [1, Ei] validEcacheList = nonzero(oS.eCache[:, 0].A)[0] # 返回矩阵中的非零位置的行数 if (len(validEcacheList)) 1: for k in validEcacheList: if k == i: continue Ek = calcEk(oS, k) deltaE = abs(Ei - Ek) if (deltaE maxDeltaE): # 返回步长最大的aj maxK = k maxDeltaE = deltaE Ej = Ek return maxK, Ej else: j = selectJrand(i, oS.m) Ej = calcEk(oS, j) return j, Ejdef updateEk(oS, k): # 更新os数据 Ek = calcEk(oS, k) oS.eCache[k] = [1, Ek]# 首先检验ai是否满足KKT条件，如果不满足，随机选择aj进行优化，更新ai,aj,b值def innerL(i, oS): # 输入参数i和所有参数数据 Ei = calcEk(oS, i) # 计算E值 if ((oS.labelMat[i] * Ei -oS.tol) and (oS.alphas[i] oS.C)) or ( (oS.labelMat[i] * Ei oS.tol) and (oS.alphas[i] 0)): # 检验这行数据是否符合KKT条件 参考《统计学习方法》p128公式7.111-113 j, Ej = selectJ(i, oS, Ei) # 随机选取aj，并返回其E值 alphaIold = oS.alphas[i].copy() alphaJold = oS.alphas[j].copy() if (oS.labelMat[i] != oS.labelMat[j]): # 以下代码的公式参考《统计学习方法》p126 L = max(0, oS.alphas[j] - oS.alphas[i]) H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i]) else: L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C) H = min(oS.C, oS.alphas[j] + oS.alphas[i]) if L == H: print(\"L==H\") return 0 eta = 2.0 * oS.K[i, j] - oS.K[i, i] - oS.K[j, j] # 参考《统计学习方法》p127公式7.107 if eta = 0: print(\"eta=0\") return 0 oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej) / eta # 参考《统计学习方法》p127公式7.106 oS.alphas[j] = clipAlpha(oS.alphas[j], H, L) # 参考《统计学习方法》p127公式7.108 updateEk(oS, j) if (abs(oS.alphas[j] - alphaJold) oS.tol): # alpha变化大小阀值（自己设定） print(\"j not moving enough\") return 0 oS.alphas[i] += oS.labelMat[j] * oS.labelMat[i] * (alphaJold - oS.alphas[j]) # 参考《统计学习方法》p127公式7.109 updateEk(oS, i) # 更新数据 # 以下求解b的过程，参考《统计学习方法》p129公式7.114-7.116 b1 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, i] - oS.labelMat[j] * ( oS.alphas[j] - alphaJold) * oS.K[i, j] b2 = oS.b - Ej - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, j] - oS.labelMat[j] * ( oS.alphas[j] - alphaJold) * oS.K[j, j] if (0 oS.alphas[i] oS.C): oS.b = b1 elif (0 oS.alphas[j] oS.C): oS.b = b2 else: oS.b = (b1 + b2) / 2.0 return 1 else: return 0# SMO函数，用于快速求解出alphadef smoP(dataMatIn, classLabels, C, toler, maxIter, kTup=('lin', 0)): # 输入参数：数据特征，数据类别，参数C，阀值toler，最大迭代次数，核函数（默认线性核） oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler, kTup) # dataArr, labelArr, 200, 0.0001, 10000, ('rbf', 1.3)) # 通过SMO算法得到b和alpha iter = 0 entireSet = True alphaPairsChanged = 0 while (iter maxIter) and ((alphaPairsChanged 0) or (entireSet)): alphaPairsChanged = 0 if entireSet: for i in range(oS.m): # 遍历所有数据 alphaPairsChanged += innerL(i, oS) print(\"fullSet, iter: %d i:%d, pairs changed %d\" % ( iter, i, alphaPairsChanged)) # 显示第多少次迭代，那行特征数据使alpha发生了改变，这次改变了多少次alpha iter += 1 else: nonBoundIs = nonzero((oS.alphas.A 0) * (oS.alphas.A C))[0] for i in nonBoundIs: # 遍历非边界的数据 alphaPairsChanged += innerL(i, oS) print(\"non-bound, iter: %d i:%d, pairs changed %d\" % (iter, i, alphaPairsChanged)) iter += 1 if entireSet: entireSet = False elif (alphaPairsChanged == 0): entireSet = True print(\"iteration number: %d\" % iter) return oS.b, oS.alphasdef train(data_train, data_test): dataArr, labelArr = loadDataSet(data_train) # 读取训练数据 b, alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, ('rbf', 1.3)) # 通过SMO算法得到b和alpha datMat = mat(dataArr) labelMat = mat(labelArr).transpose() svInd = nonzero(alphas)[0] # 选取不为0数据的行数（也就是支持向量） sVs = datMat[svInd] # 支持向量的特征数据 labelSV = labelMat[svInd] # 支持向量的类别（1或-1） print(\"there are %d Support Vectors\" % shape(sVs)[0]) # 打印出共有多少的支持向量 m, n = shape(datMat) # 训练数据的行列数 errorCount = 0 for i in range(m): kernelEval = kernelTrans(sVs, datMat[i, :], ('rbf', 1.3)) # 将支持向量转化为核函数 predict = kernelEval.T * multiply(labelSV, alphas[ svInd]) + b # 这一行的预测结果（代码来源于《统计学习方法》p133里面最后用于预测的公式）注意最后确定的分离平面只有那些支持向量决定。 if sign(predict) != sign(labelArr[i]): # sign函数 -1 if x 0, 0 if x==0, 1 if x 0 errorCount += 1 print(\"the training error rate is: %f\" % (float(errorCount) / m)) # 打印出错误率 dataArr_test, labelArr_test = loadDataSet(data_test) # 读取测试数据 errorCount_test = 0 datMat_test = mat(dataArr_test) labelMat = mat(labelArr_test).transpose() m, n = shape(datMat_test) for i in range(m): # 在测试数据上检验错误率 kernelEval = kernelTrans(sVs, datMat_test[i, :], ('rbf', 1.3)) predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b if sign(predict) != sign(labelArr_test[i]): errorCount_test += 1 print(\"the test error rate is: %f\" % (float(errorCount_test) / m))# 主程序def main(): filename_traindata = './train_data.csv' filename_testdata = './test_data.csv' train(filename_traindata, filename_testdata)if __name__ == '__main__': main() feature1,feature2,label-0.214824,0.662756,-1.000000-0.061569,-0.091875,1.0000000.406933,0.648055,-1.0000000.223650,0.130142,1.0000000.231317,0.766906,-1.000000-0.748800,-0.531637,-1.000000-0.557789,0.375797,-1.0000000.207123,-0.019463,1.0000000.286462,0.719470,-1.0000000.195300,-0.179039,1.000000-0.152696,-0.153030,1.0000000.384471,0.653336,-1.000000-0.117280,-0.153217,1.000000-0.238076,0.000583,1.000000-0.413576,0.145681,1.0000000.490767,-0.680029,-1.0000000.199894,-0.199381,1.000000-0.356048,0.537960,-1.000000-0.392868,-0.125261,1.0000000.353588,-0.070617,1.0000000.020984,0.925720,-1.000000-0.475167,-0.346247,-1.0000000.074952,0.042783,1.0000000.394164,-0.058217,1.0000000.663418,0.436525,-1.0000000.402158,0.577744,-1.000000-0.449349,-0.038074,1.0000000.619080,-0.088188,-1.0000000.268066,-0.071621,1.000000-0.015165,0.359326,1.0000000.539368,-0.374972,-1.000000-0.319153,0.629673,-1.0000000.694424,0.641180,-1.0000000.079522,0.193198,1.0000000.253289,-0.285861,1.000000-0.035558,-0.010086,1.000000-0.403483,0.474466,-1.000000-0.034312,0.995685,-1.000000-0.590657,0.438051,-1.000000-0.098871,-0.023953,1.000000-0.250001,0.141621,1.000000-0.012998,0.525985,-1.0000000.153738,0.491531,-1.0000000.388215,-0.656567,-1.0000000.049008,0.013499,1.0000000.068286,0.392741,1.0000000.747800,-0.066630,-1.0000000.004621,-0.042932,1.000000-0.701600,0.190983,-1.0000000.055413,-0.024380,1.0000000.035398,-0.333682,1.0000000.211795,0.024689,1.000000-0.045677,0.172907,1.0000000.595222,0.209570,-1.0000000.229465,0.250409,1.000000-0.089293,0.068198,1.0000000.384300,-0.176570,1.0000000.834912,-0.110321,-1.000000-0.307768,0.503038,-1.000000-0.777063,-0.348066,-1.0000000.017390,0.152441,1.000000-0.293382,-0.139778,1.000000-0.203272,0.286855,1.0000000.957812,-0.152444,-1.0000000.004609,-0.070617,1.000000-0.755431,0.096711,-1.000000-0.526487,0.547282,-1.000000-0.246873,0.833713,-1.0000000.185639,-0.066162,1.0000000.851934,0.456603,-1.000000-0.827912,0.117122,-1.0000000.233512,-0.106274,1.0000000.583671,-0.709033,-1.000000-0.487023,0.625140,-1.000000-0.448939,0.176725,1.0000000.155907,-0.166371,1.0000000.334204,0.381237,-1.0000000.081536,-0.106212,1.0000000.227222,0.527437,-1.0000000.759290,0.330720,-1.0000000.204177,-0.023516,1.0000000.577939,0.403784,-1.000000-0.568534,0.442948,-1.000000-0.011520,0.021165,1.0000000.875720,0.422476,-1.0000000.297885,-0.632874,-1.000000-0.015821,0.031226,1.0000000.541359,-0.205969,-1.000000-0.689946,-0.508674,-1.000000-0.343049,0.841653,-1.0000000.523902,-0.436156,-1.0000000.249281,-0.711840,-1.0000000.193449,0.574598,-1.000000-0.257542,-0.753885,-1.000000-0.021605,0.158080,1.0000000.601559,-0.727041,-1.000000-0.791603,0.095651,-1.000000-0.908298,-0.053376,-1.0000000.122020,0.850966,-1.000000-0.725568,-0.292022,-1.000000 test datafeature1,feature2,label0.676771,-0.486687,-1.0000000.008473,0.186070,1.000000-0.727789,0.594062,-1.0000000.112367,0.287852,1.0000000.383633,-0.038068,1.000000-0.927138,-0.032633,-1.000000-0.842803,-0.423115,-1.000000-0.003677,-0.367338,1.0000000.443211,-0.698469,-1.000000-0.473835,0.005233,1.0000000.616741,0.590841,-1.0000000.557463,-0.373461,-1.000000-0.498535,-0.223231,-1.000000-0.246744,0.276413,1.000000-0.761980,-0.244188,-1.0000000.641594,-0.479861,-1.000000-0.659140,0.529830,-1.000000-0.054873,-0.238900,1.000000-0.089644,-0.244683,1.000000-0.431576,-0.481538,-1.000000-0.099535,0.728679,-1.000000-0.188428,0.156443,1.0000000.267051,0.318101,1.0000000.222114,-0.528887,-1.0000000.030369,0.113317,1.0000000.392321,0.026089,1.0000000.298871,-0.915427,-1.000000-0.034581,-0.133887,1.0000000.405956,0.206980,1.0000000.144902,-0.605762,-1.0000000.274362,-0.401338,1.0000000.397998,-0.780144,-1.0000000.037863,0.155137,1.000000-0.010363,-0.004170,1.0000000.506519,0.486619,-1.0000000.000082,-0.020625,1.0000000.057761,-0.155140,1.0000000.027748,-0.553763,-1.000000-0.413363,-0.746830,-1.0000000.081500,-0.014264,1.0000000.047137,-0.491271,1.000000-0.267459,0.024770,1.000000-0.148288,-0.532471,-1.000000-0.225559,-0.201622,1.0000000.772360,-0.518986,-1.000000-0.440670,0.688739,-1.0000000.329064,-0.095349,1.0000000.970170,-0.010671,-1.000000-0.689447,-0.318722,-1.000000-0.465493,-0.227468,-1.000000-0.049370,0.405711,1.000000-0.166117,0.274807,1.0000000.054483,0.012643,1.0000000.021389,0.076125,1.000000-0.104404,-0.914042,-1.0000000.294487,0.440886,-1.0000000.107915,-0.493703,-1.0000000.076311,0.438860,1.0000000.370593,-0.728737,-1.0000000.409890,0.306851,-1.0000000.285445,0.474399,-1.000000-0.870134,-0.161685,-1.000000-0.654144,-0.675129,-1.0000000.285278,-0.767310,-1.0000000.049548,-0.000907,1.0000000.030014,-0.093265,1.000000-0.128859,0.278865,1.0000000.307463,0.085667,1.0000000.023440,0.298638,1.0000000.053920,0.235344,1.0000000.059675,0.533339,-1.0000000.817125,0.016536,-1.000000-0.108771,0.477254,1.000000-0.118106,0.017284,1.0000000.288339,0.195457,1.0000000.567309,-0.200203,-1.000000-0.202446,0.409387,1.000000-0.330769,-0.240797,1.000000-0.422377,0.480683,-1.000000-0.295269,0.326017,1.0000000.261132,0.046478,1.000000-0.492244,-0.319998,-1.000000-0.384419,0.099170,1.0000000.101882,-0.781145,-1.0000000.234592,-0.383446,1.000000-0.020478,-0.901833,-1.0000000.328449,0.186633,1.000000-0.150059,-0.409158,1.000000-0.155876,-0.843413,-1.000000-0.098134,-0.136786,1.0000000.110575,-0.197205,1.0000000.219021,0.054347,1.0000000.030152,0.251682,1.0000000.033447,-0.122824,1.000000-0.686225,-0.020779,-1.000000-0.911211,-0.262011,-1.0000000.572557,0.377526,-1.000000-0.073647,-0.519163,-1.000000-0.281830,-0.797236,-1.000000-0.555263,0.126232,-1.000000 参考链接：https://blog.csdn.net/csqazwsxedc/article/details/71513197https://blog.csdn.net/zouxy09/article/details/17291543","tags":["Python"],"categories":["Python"]},{"title":"从csv中读取数据","path":"/2019/09/08/从csv中读取数据/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 | import csv import numpy as np def loadDataSet(filename): # 读取数据 with open(filename) as f: dataMat = [] labelMat = [] f_csv = csv.reader(f) ## 用csv读取直接是个list headers = next(f_csv) for row in f_csv: dataMat.append([float(row[0]), float(row[1])]) labelMat.append(float(row[2])) f.close() return dataMat, labelMat def loadData(filename): # 读取数据 dataMat=[] labelMat=[] fr=open(filename) next(fr) # 忽略第一行 for line in fr.readlines(): lineArr=line.strip().split(',') dataMat.append([lineArr[0],lineArr[1]]) labelMat.append(lineArr[2]) return dataMat,labelMat # 返回数据特征和数据类别 if __name__ == '__main__': dataMat,labelMat = loadData('test_data.csv') print(dataMat,labelMat) —|—","tags":["Python"],"categories":["其他"]},{"title":"二进制中1的个数","path":"/2019/09/08/二进制中1的个数/","content":"时间限制：1秒 空间限制：32768K 热度指数：489798本题知识点： 进制转化 补码 反码 原码 题目描述输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。 1 2 3 4 5 6 7 8 9 10 11 12 | class Solution { public: int NumberOf1(int n) { int count=0; while(n!=0) { count++; n = (n-1)n; } return count; } }; —|— 运行时间：3ms占用内存：356k","tags":["编程"],"categories":["其他"]},{"title":"支持向量机","path":"/2019/09/08/支持向量机/","content":"SVM的本质：寻找最大的间隔支持向量：距离超平面最近的那些点SMO算法的原理： 每次循环中选择两个alpha进行优化处理。一旦找到一对合适的alpha，那么就增大其中一个同时减小另一个。合适：条件一，两个alpha要在间隔边界之外；条件二，这两个alpha还没有进行过区间化处理或不在边界上 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 | from numpy import * import csv def loadDataSet(filename): # 读取数据 with open(filename) as f: dataMat = [] labelMat = [] f_csv = csv.reader(f) ## 用csv读取直接是个list headers = next(f_csv) for row in f_csv: dataMat.append([float(row[0]), float(row[1])]) labelMat.append(float(row[2])) f.close() return dataMat, labelMat def selectJrand(i, m): # 在0-m中随机选择一个不是i的整数 j = i while (j == i): j = int(random.uniform(0, m)) return j def clipAlpha(aj, H, L): # 保证a在L和H范围内（L = a = H） if aj H: aj = H if L aj: aj = L return aj def kernelTrans(X, A, kTup): # 核函数，输入参数,X:支持向量的特征树；A：某一行特征数据；kTup：('lin',k1)核函数的类型和参数 m, n = shape(X) K = mat(zeros((m, 1))) print(\"A shape : \",A.shape) print(\"X shape : \",X.shape) if kTup[0] == 'lin': # 线性函数 K = X * A.T elif kTup[0] == 'rbf': # 径向基函数(radial bias function) for j in range(m): deltaRow = X[j, :] - A K[j] = deltaRow * deltaRow.T K = exp(K / (-1 * kTup[1] ** 2)) # 返回生成的结果 else: raise NameError('Houston We Have a Problem -- That Kernel is not recognized') return K # 定义类，方便存储数据 class optStruct: def __init__(self, dataMatIn, classLabels, C, toler, kTup): # 存储各类参数 self.X = dataMatIn # 数据特征 self.labelMat = classLabels # 数据类别 self.C = C # 软间隔参数C，参数越大，非线性拟合能力越强 self.tol = toler # 停止阀值 self.m = shape(dataMatIn)[0] # 数据 b行数 self.alphas = mat(zeros((self.m, 1))) self.b = 0 # 初始设为0 self.eCache = mat(zeros((self.m, 2))) # 缓存 self.K = mat(zeros((self.m, self.m))) # 核函数的计算结果 for i in range(self.m): self.K[:, i] = kernelTrans(self.X, self.X[i, :], kTup) def calcEk(oS, k): # 计算Ek（参考《统计学习方法》p127公式7.105） fXk = float(multiply(oS.alphas, oS.labelMat).T * oS.K[:, k] + oS.b) Ek = fXk - float(oS.labelMat[k]) return Ek # 随机选取aj，并返回其E值 def selectJ(i, oS, Ei): maxK = -1 maxDeltaE = 0 Ej = 0 oS.eCache[i] = [1, Ei] validEcacheList = nonzero(oS.eCache[:, 0].A)[0] # 返回矩阵中的非零位置的行数 if (len(validEcacheList)) 1: for k in validEcacheList: if k == i: continue Ek = calcEk(oS, k) deltaE = abs(Ei - Ek) if (deltaE maxDeltaE): # 返回步长最大的aj maxK = k maxDeltaE = deltaE Ej = Ek return maxK, Ej else: j = selectJrand(i, oS.m) Ej = calcEk(oS, j) return j, Ej def updateEk(oS, k): # 更新os数据 Ek = calcEk(oS, k) oS.eCache[k] = [1, Ek] # 首先检验ai是否满足KKT条件，如果不满足，随机选择aj进行优化，更新ai,aj,b值 def innerL(i, oS): # 输入参数i和所有参数数据 Ei = calcEk(oS, i) # 计算E值 if ((oS.labelMat[i] * Ei -oS.tol) and (oS.alphas[i] oS.C)) or ( (oS.labelMat[i] * Ei oS.tol) and (oS.alphas[i] 0)): # 检验这行数据是否符合KKT条件 参考《统计学习方法》p128公式7.111-113 j, Ej = selectJ(i, oS, Ei) # 随机选取aj，并返回其E值 alphaIold = oS.alphas[i].copy() alphaJold = oS.alphas[j].copy() if (oS.labelMat[i] != oS.labelMat[j]): # 以下代码的公式参考《统计学习方法》p126 L = max(0, oS.alphas[j] - oS.alphas[i]) H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i]) else: L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C) H = min(oS.C, oS.alphas[j] + oS.alphas[i]) if L == H: print(\"L==H\") return 0 eta = 2.0 * oS.K[i, j] - oS.K[i, i] - oS.K[j, j] # 参考《统计学习方法》p127公式7.107 if eta = 0: print(\"eta=0\") return 0 oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej) / eta # 参考《统计学习方法》p127公式7.106 oS.alphas[j] = clipAlpha(oS.alphas[j], H, L) # 参考《统计学习方法》p127公式7.108 updateEk(oS, j) if (abs(oS.alphas[j] - alphaJold) oS.tol): # alpha变化大小阀值（自己设定） print(\"j not moving enough\") return 0 oS.alphas[i] += oS.labelMat[j] * oS.labelMat[i] * (alphaJold - oS.alphas[j]) # 参考《统计学习方法》p127公式7.109 updateEk(oS, i) # 更新数据 # 以下求解b的过程，参考《统计学习方法》p129公式7.114-7.116 b1 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, i] - oS.labelMat[j] * ( oS.alphas[j] - alphaJold) * oS.K[i, j] b2 = oS.b - Ej - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, j] - oS.labelMat[j] * ( oS.alphas[j] - alphaJold) * oS.K[j, j] if (0 oS.alphas[i] oS.C): oS.b = b1 elif (0 oS.alphas[j] oS.C): oS.b = b2 else: oS.b = (b1 + b2) / 2.0 return 1 else: return 0 # SMO函数，用于快速求解出alpha def smoP(dataMatIn, classLabels, C, toler, maxIter, kTup=('lin', 0)): # 输入参数：数据特征，数据类别，参数C，阀值toler，最大迭代次数，核函数（默认线性核） oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler, kTup) # dataArr, labelArr, 200, 0.0001, 10000, ('rbf', 1.3)) # 通过SMO算法得到b和alpha iter = 0 entireSet = True alphaPairsChanged = 0 while (iter maxIter) and ((alphaPairsChanged 0) or (entireSet)): alphaPairsChanged = 0 if entireSet: for i in range(oS.m): # 遍历所有数据 alphaPairsChanged += innerL(i, oS) print(\"fullSet, iter: %d i:%d, pairs changed %d\" % ( iter, i, alphaPairsChanged)) # 显示第多少次迭代，那行特征数据使alpha发生了改变，这次改变了多少次alpha iter += 1 else: nonBoundIs = nonzero((oS.alphas.A 0) * (oS.alphas.A C))[0] for i in nonBoundIs: # 遍历非边界的数据 alphaPairsChanged += innerL(i, oS) print(\"non-bound, iter: %d i:%d, pairs changed %d\" % (iter, i, alphaPairsChanged)) iter += 1 if entireSet: entireSet = False elif (alphaPairsChanged == 0): entireSet = True print(\"iteration number: %d\" % iter) return oS.b, oS.alphas def train(data_train, data_test): dataArr, labelArr = loadDataSet(data_train) # 读取训练数据 b, alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, ('rbf', 1.3)) # 通过SMO算法得到b和alpha datMat = mat(dataArr) labelMat = mat(labelArr).transpose() svInd = nonzero(alphas)[0] # 选取不为0数据的行数（也就是支持向量） sVs = datMat[svInd] # 支持向量的特征数据 labelSV = labelMat[svInd] # 支持向量的类别（1或-1） print(\"there are %d Support Vectors\" % shape(sVs)[0]) # 打印出共有多少的支持向量 m, n = shape(datMat) # 训练数据的行列数 errorCount = 0 for i in range(m): kernelEval = kernelTrans(sVs, datMat[i, :], ('rbf', 1.3)) # 将支持向量转化为核函数 predict = kernelEval.T * multiply(labelSV, alphas[ svInd]) + b # 这一行的预测结果（代码来源于《统计学习方法》p133里面最后用于预测的公式）注意最后确定的分离平面只有那些支持向量决定。 if sign(predict) != sign(labelArr[i]): # sign函数 -1 if x 0, 0 if x==0, 1 if x 0 errorCount += 1 print(\"the training error rate is: %f\" % (float(errorCount) / m)) # 打印出错误率 dataArr_test, labelArr_test = loadDataSet(data_test) # 读取测试数据 errorCount_test = 0 datMat_test = mat(dataArr_test) labelMat = mat(labelArr_test).transpose() m, n = shape(datMat_test) for i in range(m): # 在测试数据上检验错误率 kernelEval = kernelTrans(sVs, datMat_test[i, :], ('rbf', 1.3)) predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b if sign(predict) != sign(labelArr_test[i]): errorCount_test += 1 print(\"the test error rate is: %f\" % (float(errorCount_test) / m)) # 主程序 def main(): filename_traindata = './train_data.csv' filename_testdata = './test_data.csv' train(filename_traindata, filename_testdata) if __name__ == '__main__': main() —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 | feature1,feature2,label -0.214824,0.662756,-1.000000 -0.061569,-0.091875,1.000000 0.406933,0.648055,-1.000000 0.223650,0.130142,1.000000 0.231317,0.766906,-1.000000 -0.748800,-0.531637,-1.000000 -0.557789,0.375797,-1.000000 0.207123,-0.019463,1.000000 0.286462,0.719470,-1.000000 0.195300,-0.179039,1.000000 -0.152696,-0.153030,1.000000 0.384471,0.653336,-1.000000 -0.117280,-0.153217,1.000000 -0.238076,0.000583,1.000000 -0.413576,0.145681,1.000000 0.490767,-0.680029,-1.000000 0.199894,-0.199381,1.000000 -0.356048,0.537960,-1.000000 -0.392868,-0.125261,1.000000 0.353588,-0.070617,1.000000 0.020984,0.925720,-1.000000 -0.475167,-0.346247,-1.000000 0.074952,0.042783,1.000000 0.394164,-0.058217,1.000000 0.663418,0.436525,-1.000000 0.402158,0.577744,-1.000000 -0.449349,-0.038074,1.000000 0.619080,-0.088188,-1.000000 0.268066,-0.071621,1.000000 -0.015165,0.359326,1.000000 0.539368,-0.374972,-1.000000 -0.319153,0.629673,-1.000000 0.694424,0.641180,-1.000000 0.079522,0.193198,1.000000 0.253289,-0.285861,1.000000 -0.035558,-0.010086,1.000000 -0.403483,0.474466,-1.000000 -0.034312,0.995685,-1.000000 -0.590657,0.438051,-1.000000 -0.098871,-0.023953,1.000000 -0.250001,0.141621,1.000000 -0.012998,0.525985,-1.000000 0.153738,0.491531,-1.000000 0.388215,-0.656567,-1.000000 0.049008,0.013499,1.000000 0.068286,0.392741,1.000000 0.747800,-0.066630,-1.000000 0.004621,-0.042932,1.000000 -0.701600,0.190983,-1.000000 0.055413,-0.024380,1.000000 0.035398,-0.333682,1.000000 0.211795,0.024689,1.000000 -0.045677,0.172907,1.000000 0.595222,0.209570,-1.000000 0.229465,0.250409,1.000000 -0.089293,0.068198,1.000000 0.384300,-0.176570,1.000000 0.834912,-0.110321,-1.000000 -0.307768,0.503038,-1.000000 -0.777063,-0.348066,-1.000000 0.017390,0.152441,1.000000 -0.293382,-0.139778,1.000000 -0.203272,0.286855,1.000000 0.957812,-0.152444,-1.000000 0.004609,-0.070617,1.000000 -0.755431,0.096711,-1.000000 -0.526487,0.547282,-1.000000 -0.246873,0.833713,-1.000000 0.185639,-0.066162,1.000000 0.851934,0.456603,-1.000000 -0.827912,0.117122,-1.000000 0.233512,-0.106274,1.000000 0.583671,-0.709033,-1.000000 -0.487023,0.625140,-1.000000 -0.448939,0.176725,1.000000 0.155907,-0.166371,1.000000 0.334204,0.381237,-1.000000 0.081536,-0.106212,1.000000 0.227222,0.527437,-1.000000 0.759290,0.330720,-1.000000 0.204177,-0.023516,1.000000 0.577939,0.403784,-1.000000 -0.568534,0.442948,-1.000000 -0.011520,0.021165,1.000000 0.875720,0.422476,-1.000000 0.297885,-0.632874,-1.000000 -0.015821,0.031226,1.000000 0.541359,-0.205969,-1.000000 -0.689946,-0.508674,-1.000000 -0.343049,0.841653,-1.000000 0.523902,-0.436156,-1.000000 0.249281,-0.711840,-1.000000 0.193449,0.574598,-1.000000 -0.257542,-0.753885,-1.000000 -0.021605,0.158080,1.000000 0.601559,-0.727041,-1.000000 -0.791603,0.095651,-1.000000 -0.908298,-0.053376,-1.000000 0.122020,0.850966,-1.000000 -0.725568,-0.292022,-1.000000 test data feature1,feature2,label 0.676771,-0.486687,-1.000000 0.008473,0.186070,1.000000 -0.727789,0.594062,-1.000000 0.112367,0.287852,1.000000 0.383633,-0.038068,1.000000 -0.927138,-0.032633,-1.000000 -0.842803,-0.423115,-1.000000 -0.003677,-0.367338,1.000000 0.443211,-0.698469,-1.000000 -0.473835,0.005233,1.000000 0.616741,0.590841,-1.000000 0.557463,-0.373461,-1.000000 -0.498535,-0.223231,-1.000000 -0.246744,0.276413,1.000000 -0.761980,-0.244188,-1.000000 0.641594,-0.479861,-1.000000 -0.659140,0.529830,-1.000000 -0.054873,-0.238900,1.000000 -0.089644,-0.244683,1.000000 -0.431576,-0.481538,-1.000000 -0.099535,0.728679,-1.000000 -0.188428,0.156443,1.000000 0.267051,0.318101,1.000000 0.222114,-0.528887,-1.000000 0.030369,0.113317,1.000000 0.392321,0.026089,1.000000 0.298871,-0.915427,-1.000000 -0.034581,-0.133887,1.000000 0.405956,0.206980,1.000000 0.144902,-0.605762,-1.000000 0.274362,-0.401338,1.000000 0.397998,-0.780144,-1.000000 0.037863,0.155137,1.000000 -0.010363,-0.004170,1.000000 0.506519,0.486619,-1.000000 0.000082,-0.020625,1.000000 0.057761,-0.155140,1.000000 0.027748,-0.553763,-1.000000 -0.413363,-0.746830,-1.000000 0.081500,-0.014264,1.000000 0.047137,-0.491271,1.000000 -0.267459,0.024770,1.000000 -0.148288,-0.532471,-1.000000 -0.225559,-0.201622,1.000000 0.772360,-0.518986,-1.000000 -0.440670,0.688739,-1.000000 0.329064,-0.095349,1.000000 0.970170,-0.010671,-1.000000 -0.689447,-0.318722,-1.000000 -0.465493,-0.227468,-1.000000 -0.049370,0.405711,1.000000 -0.166117,0.274807,1.000000 0.054483,0.012643,1.000000 0.021389,0.076125,1.000000 -0.104404,-0.914042,-1.000000 0.294487,0.440886,-1.000000 0.107915,-0.493703,-1.000000 0.076311,0.438860,1.000000 0.370593,-0.728737,-1.000000 0.409890,0.306851,-1.000000 0.285445,0.474399,-1.000000 -0.870134,-0.161685,-1.000000 -0.654144,-0.675129,-1.000000 0.285278,-0.767310,-1.000000 0.049548,-0.000907,1.000000 0.030014,-0.093265,1.000000 -0.128859,0.278865,1.000000 0.307463,0.085667,1.000000 0.023440,0.298638,1.000000 0.053920,0.235344,1.000000 0.059675,0.533339,-1.000000 0.817125,0.016536,-1.000000 -0.108771,0.477254,1.000000 -0.118106,0.017284,1.000000 0.288339,0.195457,1.000000 0.567309,-0.200203,-1.000000 -0.202446,0.409387,1.000000 -0.330769,-0.240797,1.000000 -0.422377,0.480683,-1.000000 -0.295269,0.326017,1.000000 0.261132,0.046478,1.000000 -0.492244,-0.319998,-1.000000 -0.384419,0.099170,1.000000 0.101882,-0.781145,-1.000000 0.234592,-0.383446,1.000000 -0.020478,-0.901833,-1.000000 0.328449,0.186633,1.000000 -0.150059,-0.409158,1.000000 -0.155876,-0.843413,-1.000000 -0.098134,-0.136786,1.000000 0.110575,-0.197205,1.000000 0.219021,0.054347,1.000000 0.030152,0.251682,1.000000 0.033447,-0.122824,1.000000 -0.686225,-0.020779,-1.000000 -0.911211,-0.262011,-1.000000 0.572557,0.377526,-1.000000 -0.073647,-0.519163,-1.000000 -0.281830,-0.797236,-1.000000 -0.555263,0.126232,-1.000000 —|— 参考链接：https://blog.csdn.net/csqazwsxedc/article/details/71513197https://blog.csdn.net/zouxy09/article/details/17291543","tags":["Python"],"categories":["深度学习"]},{"title":"PCA降维","path":"/2019/09/07/20190907-PCA降维/","content":"PCA（principal Component Analysis），主成分分析法。顾名思义，就是提取出数据中主要的成分，是一种数据压缩方法，常用于去除噪声、数据预处理，也是机器学习中常见的降维方法。 步骤 对所有样本进行中心化处理即将每个元素减去它的平均值，这样可以增加基向量的正交性。 计算协方差矩阵及特征值、特征向量 对特征值进行排序 # coding:utf-8from matplotlib import pylab as pltimport numpy as npdef read_file(filename):f = open(filename, 'r')d = f.readlines()f.close()return dclass PCA(object):\"\"\"使用PCA对高维数据进行降维处理\"\"\"def __init__(self):data = read_file('data.txt')self.n = len(data) # 数据的个数self.dim = 4 # 原始数据的维度self.x = np.zeros((self.n, self.dim), dtype='float64')for i in range(self.n):data_ = data[i].split(',')self.x[i][0] = data_[0]self.x[i][1] = data_[1]self.x[i][2] = data_[2]self.x[i][3] = data_[3]self.k = 2 # 降到二维self.mean_x = np.zeros((self.n, self.dim), dtype='float64') # 原始数据减去均值以后的xself.mean = np.zeros((self.dim, 1), dtype='float64') # x的均值self.cov = np.zeros((self.dim, self.dim), dtype='float64') # 协方差矩阵self.pre_x = np.zeros((self.n, self.dim), dtype='float64') # 预处理之后的数据self.eig_val = np.zeros((1, self.dim), dtype='float64') # 特征值self.eig_vec = np.zeros((self.dim, self.dim), dtype='float64') # 特征向量self.final_x = np.zeros((self.n, self.k), dtype='float64') # 投影后的数据self.pretreatment()self.pca()def pretreatment(self):\"\"\"预处理\"\"\"# 求均值for i in range(self.dim):self.mean[i] = np.mean(self.x[:, i])for i in range(self.n):self.mean_x[i] = self.x[i] - self.mean.T# 求协方差# self.cov = np.cov(self.mean_x, rowvar=0)# mean_x已经是x减去均值了，所以直接相乘就是方差self.cov = self.mean_x.T.dot(self.mean_x) / self.nfor i in range(self.dim):self.pre_x[:, i] = self.mean_x[:, i] / np.sqrt(self.cov[i][i]) # x的每个维度都处理一次def pca(self):\"\"\"pca的实现\"\"\"# 需要注意的是，在这里需要对预处理之后的数据重新计算协方差# 计算均值for i in range(self.dim):self.mean[i] = np.mean(self.pre_x[:, i])# 计算协方差for i in range(self.n):self.mean_x[i] = self.pre_x[i] - self.mean.Tself.cov = (self.mean_x.T.dot(self.mean_x)) / self.n# 求特征值self.eig_val, self.eig_vec = np.linalg.eig(np.mat(self.cov))# eig_vec的列向量是特征向量，所以用eig_v存储特征向量，以便后面排序eig_v = np.zeros((self.dim, self.dim), dtype='float64')for i in range(self.dim):eig_v[i] = self.eig_vec[:, i].T# 排序eig_list = zip(self.eig_val, eig_v)sorted(eig_list,key=lambda g: g[0], reverse=True)# 处理排序的结果for i in range(len(list(eig_list))):self.eig_val[i] = eig_list[i][0]self.eig_vec[i] = eig_list[i][1]# 最大的k个特征向量，降维f_vec = self.eig_vec[0:self.k, :]self.final_x = self.pre_x * f_vec.T# 显示降维后的数据点plt.plot(self.final_x[0:50, 0], self.final_x[0:50, 1], 'bo')plt.plot(self.final_x[50:100, 0], self.final_x[50:100, 1], 'go')plt.plot(self.final_x[100:150, 0], self.final_x[100:150, 1], 'ro')plt.show()if __name__ == '__main__':a = PCA() 5.1,3.5,1.4,0.2,Iris-setosa4.9,3.0,1.4,0.2,Iris-setosa4.7,3.2,1.3,0.2,Iris-setosa4.6,3.1,1.5,0.2,Iris-setosa5.0,3.6,1.4,0.2,Iris-setosa5.4,3.9,1.7,0.4,Iris-setosa4.6,3.4,1.4,0.3,Iris-setosa5.0,3.4,1.5,0.2,Iris-setosa4.4,2.9,1.4,0.2,Iris-setosa4.9,3.1,1.5,0.1,Iris-setosa5.4,3.7,1.5,0.2,Iris-setosa4.8,3.4,1.6,0.2,Iris-setosa4.8,3.0,1.4,0.1,Iris-setosa4.3,3.0,1.1,0.1,Iris-setosa5.8,4.0,1.2,0.2,Iris-setosa5.7,4.4,1.5,0.4,Iris-setosa5.4,3.9,1.3,0.4,Iris-setosa5.1,3.5,1.4,0.3,Iris-setosa5.7,3.8,1.7,0.3,Iris-setosa5.1,3.8,1.5,0.3,Iris-setosa5.4,3.4,1.7,0.2,Iris-setosa5.1,3.7,1.5,0.4,Iris-setosa4.6,3.6,1.0,0.2,Iris-setosa5.1,3.3,1.7,0.5,Iris-setosa4.8,3.4,1.9,0.2,Iris-setosa5.0,3.0,1.6,0.2,Iris-setosa5.0,3.4,1.6,0.4,Iris-setosa5.2,3.5,1.5,0.2,Iris-setosa5.2,3.4,1.4,0.2,Iris-setosa4.7,3.2,1.6,0.2,Iris-setosa4.8,3.1,1.6,0.2,Iris-setosa5.4,3.4,1.5,0.4,Iris-setosa5.2,4.1,1.5,0.1,Iris-setosa5.5,4.2,1.4,0.2,Iris-setosa4.9,3.1,1.5,0.1,Iris-setosa5.0,3.2,1.2,0.2,Iris-setosa5.5,3.5,1.3,0.2,Iris-setosa4.9,3.1,1.5,0.1,Iris-setosa4.4,3.0,1.3,0.2,Iris-setosa5.1,3.4,1.5,0.2,Iris-setosa5.0,3.5,1.3,0.3,Iris-setosa4.5,2.3,1.3,0.3,Iris-setosa4.4,3.2,1.3,0.2,Iris-setosa5.0,3.5,1.6,0.6,Iris-setosa5.1,3.8,1.9,0.4,Iris-setosa4.8,3.0,1.4,0.3,Iris-setosa5.1,3.8,1.6,0.2,Iris-setosa4.6,3.2,1.4,0.2,Iris-setosa5.3,3.7,1.5,0.2,Iris-setosa5.0,3.3,1.4,0.2,Iris-setosa7.0,3.2,4.7,1.4,Iris-versicolor6.4,3.2,4.5,1.5,Iris-versicolor6.9,3.1,4.9,1.5,Iris-versicolor5.5,2.3,4.0,1.3,Iris-versicolor6.5,2.8,4.6,1.5,Iris-versicolor5.7,2.8,4.5,1.3,Iris-versicolor6.3,3.3,4.7,1.6,Iris-versicolor4.9,2.4,3.3,1.0,Iris-versicolor6.6,2.9,4.6,1.3,Iris-versicolor5.2,2.7,3.9,1.4,Iris-versicolor5.0,2.0,3.5,1.0,Iris-versicolor5.9,3.0,4.2,1.5,Iris-versicolor6.0,2.2,4.0,1.0,Iris-versicolor6.1,2.9,4.7,1.4,Iris-versicolor5.6,2.9,3.6,1.3,Iris-versicolor6.7,3.1,4.4,1.4,Iris-versicolor5.6,3.0,4.5,1.5,Iris-versicolor5.8,2.7,4.1,1.0,Iris-versicolor6.2,2.2,4.5,1.5,Iris-versicolor5.6,2.5,3.9,1.1,Iris-versicolor5.9,3.2,4.8,1.8,Iris-versicolor6.1,2.8,4.0,1.3,Iris-versicolor6.3,2.5,4.9,1.5,Iris-versicolor6.1,2.8,4.7,1.2,Iris-versicolor6.4,2.9,4.3,1.3,Iris-versicolor6.6,3.0,4.4,1.4,Iris-versicolor6.8,2.8,4.8,1.4,Iris-versicolor6.7,3.0,5.0,1.7,Iris-versicolor6.0,2.9,4.5,1.5,Iris-versicolor5.7,2.6,3.5,1.0,Iris-versicolor5.5,2.4,3.8,1.1,Iris-versicolor5.5,2.4,3.7,1.0,Iris-versicolor5.8,2.7,3.9,1.2,Iris-versicolor6.0,2.7,5.1,1.6,Iris-versicolor5.4,3.0,4.5,1.5,Iris-versicolor6.0,3.4,4.5,1.6,Iris-versicolor6.7,3.1,4.7,1.5,Iris-versicolor6.3,2.3,4.4,1.3,Iris-versicolor5.6,3.0,4.1,1.3,Iris-versicolor5.5,2.5,4.0,1.3,Iris-versicolor5.5,2.6,4.4,1.2,Iris-versicolor6.1,3.0,4.6,1.4,Iris-versicolor5.8,2.6,4.0,1.2,Iris-versicolor5.0,2.3,3.3,1.0,Iris-versicolor5.6,2.7,4.2,1.3,Iris-versicolor5.7,3.0,4.2,1.2,Iris-versicolor5.7,2.9,4.2,1.3,Iris-versicolor6.2,2.9,4.3,1.3,Iris-versicolor5.1,2.5,3.0,1.1,Iris-versicolor5.7,2.8,4.1,1.3,Iris-versicolor6.3,3.3,6.0,2.5,Iris-virginica5.8,2.7,5.1,1.9,Iris-virginica7.1,3.0,5.9,2.1,Iris-virginica6.3,2.9,5.6,1.8,Iris-virginica6.5,3.0,5.8,2.2,Iris-virginica7.6,3.0,6.6,2.1,Iris-virginica4.9,2.5,4.5,1.7,Iris-virginica7.3,2.9,6.3,1.8,Iris-virginica6.7,2.5,5.8,1.8,Iris-virginica7.2,3.6,6.1,2.5,Iris-virginica6.5,3.2,5.1,2.0,Iris-virginica6.4,2.7,5.3,1.9,Iris-virginica6.8,3.0,5.5,2.1,Iris-virginica5.7,2.5,5.0,2.0,Iris-virginica5.8,2.8,5.1,2.4,Iris-virginica6.4,3.2,5.3,2.3,Iris-virginica6.5,3.0,5.5,1.8,Iris-virginica7.7,3.8,6.7,2.2,Iris-virginica7.7,2.6,6.9,2.3,Iris-virginica6.0,2.2,5.0,1.5,Iris-virginica6.9,3.2,5.7,2.3,Iris-virginica5.6,2.8,4.9,2.0,Iris-virginica7.7,2.8,6.7,2.0,Iris-virginica6.3,2.7,4.9,1.8,Iris-virginica6.7,3.3,5.7,2.1,Iris-virginica7.2,3.2,6.0,1.8,Iris-virginica6.2,2.8,4.8,1.8,Iris-virginica6.1,3.0,4.9,1.8,Iris-virginica6.4,2.8,5.6,2.1,Iris-virginica7.2,3.0,5.8,1.6,Iris-virginica7.4,2.8,6.1,1.9,Iris-virginica7.9,3.8,6.4,2.0,Iris-virginica6.4,2.8,5.6,2.2,Iris-virginica6.3,2.8,5.1,1.5,Iris-virginica6.1,2.6,5.6,1.4,Iris-virginica7.7,3.0,6.1,2.3,Iris-virginica6.3,3.4,5.6,2.4,Iris-virginica6.4,3.1,5.5,1.8,Iris-virginica6.0,3.0,4.8,1.8,Iris-virginica6.9,3.1,5.4,2.1,Iris-virginica6.7,3.1,5.6,2.4,Iris-virginica6.9,3.1,5.1,2.3,Iris-virginica5.8,2.7,5.1,1.9,Iris-virginica6.8,3.2,5.9,2.3,Iris-virginica6.7,3.3,5.7,2.5,Iris-virginica6.7,3.0,5.2,2.3,Iris-virginica6.3,2.5,5.0,1.9,Iris-virginica6.5,3.0,5.2,2.0,Iris-virginica6.2,3.4,5.4,2.3,Iris-virginica5.9,3.0,5.1,1.8,Iris-virginica 参考链接：https://github.com/eva-n27/PCA","tags":["机器学习"],"categories":["Python"]},{"title":"kmeans算法","path":"/2019/09/07/20190907-kmeans算法/","content":"K-Means的思想十分简单，首先随机指定类中心，根据样本与类中心的远近划分类簇，接着重新计算类中心，迭代直至收敛。但是其中迭代的过程并不是主观地想象得出，事实上，若将样本的类别看做为“隐变量”（latent variable），类中心看作样本的分布参数，这一过程正是通过EM算法的两步走策略而计算出，其根本的目的是为了最小化平方误差函数E。 kmeans算法的最大弱点：只能处理球形的簇（理论）kmeans 计算步骤 1.随机选取K个聚类中心，这里的k值可以自己设定 2.先设置一个聚类标志，用来保存当前的 样本与第几个聚类中心最近 3.计算每个样例与每个聚类中心的距离，保存最小距离的k以及距离 4.更新聚类中心，为当前类别所有样本的均值大小 from numpy import *import timeimport matplotlib.pyplot as plt# calculate Euclidean distancedef euclDistance(vector1, vector2):return sqrt(sum(power(vector2 - vector1, 2))) # 求这两个矩阵的距离， vector1, vector2 均为矩阵# init centroids with random samples# 在样本集中随机选取k个样本点作为初始质心def initCentroids(dataSet, k):numSamples, dim = dataSet.shape # 矩阵的行数、列数centroids = zeros((k, dim)) # 感觉要不要你都可以for i in range(k):index = int(random.uniform(0, numSamples)) # 随机产生一个浮点数，然后将其转化为int型centroids[i, :] = dataSet[index, :]return centroids# k-means cluster# dataSet为一个矩阵# k为将dataSet矩阵中的样本分成k个类def kmeans(dataSet, k):numSamples = dataSet.shape[0] # 读取矩阵dataSet的第一维度的长度,即获得有多少个样本数据# first column stores which cluster this sample belongs to,# second column stores the error between this sample and its centroidclusterAssment = mat(zeros((numSamples, 2))) # 得到一个N*2的零矩阵clusterChanged = True## step 1: init centroidscentroids = initCentroids(dataSet, k) # 在样本集中随机选取k个样本点作为初始质心while clusterChanged:clusterChanged = False## for each samplefor i in range(numSamples): # rangeminDist = 100000.0minIndex = 0## for each centroid## step 2: find the centroid who is closest# 计算每个样本点与质点之间的距离，将其归内到距离最小的那一簇for j in range(k):distance = euclDistance(centroids[j, :], dataSet[i, :])if distance minDist:minDist = distanceminIndex = j## step 3: update its cluster# k个簇里面与第i个样本距离最小的的标号和距离保存在clusterAssment中# 若所有的样本不在变化，则退出while循环if clusterAssment[i, 0] != minIndex:clusterChanged = TrueclusterAssment[i, :] = minIndex, minDist ** 2 # 两个**表示的是minDist的平方## step 4: update centroidsfor j in range(k):# clusterAssment[:,0].A==j是找出矩阵clusterAssment中第一列元素中等于j的行的下标，返回的是一个以array的列表，第一个array为等于j的下标pointsInCluster = dataSet[nonzero(clusterAssment[:, 0].A == j)[0]] # 将dataSet矩阵中相对应的样本提取出来centroids[j, :] = mean(pointsInCluster, axis=0) # 计算标注为j的所有样本的平均值print('Congratulations, cluster complete!')return centroids, clusterAssment# show your cluster only available with 2-D data# centroids为k个类别，其中保存着每个类别的质心# clusterAssment为样本的标记，第一列为此样本的类别号，第二列为到此类别质心的距离def showCluster(dataSet, k, centroids, clusterAssment):numSamples, dim = dataSet.shapeif dim != 2:print(\"Sorry! I can not draw because the dimension of your data is not 2!\")return 1mark = ['or', 'ob', 'og', 'ok', '^r', '+r', 'sr', 'dr', 'r', 'pr']if k len(mark):print(\"Sorry! Your k is too large! please contact wojiushimogui\")return 1# draw all samplesfor i in range(numSamples):markIndex = int(clusterAssment[i, 0]) # 为样本指定颜色plt.plot(dataSet[i, 0], dataSet[i, 1], mark[markIndex])mark = ['Dr', 'Db', 'Dg', 'Dk', '^b', '+b', 'sb', 'db', 'b', 'pb']# draw the centroidsfor i in range(k):plt.plot(centroids[i, 0], centroids[i, 1], mark[i], markersize=12)plt.show() from numpy import *import timeimport matplotlib.pyplot as pltimport kmeans## step 1: load dataprint(\"step 1: load data...\")dataSet = [] # 列表，用来表示，列表中的每个元素也是一个二维的列表；这个二维列表就是一个样本，样本中包含有我们的属性值和类别号。# 与我们所熟悉的矩阵类似，最终我们将获得N*2的矩阵，每行元素构成了我们的训练样本的属性值和类别号fileIn = open(\"./testSet.txt\") # 是正斜杠for line in fileIn.readlines(): temp = [] lineArr = line.strip().split('\\t') # line.strip()把末尾的' '去掉 temp.append(float(lineArr[0])) temp.append(float(lineArr[1])) dataSet.append(temp)# dataSet.append([float(lineArr[0]), float(lineArr[1])])fileIn.close()## step 2: clustering...print(\"step 2: clustering...\")dataSet = mat(dataSet) # mat()函数是Numpy中的库函数，将数组转化为矩阵k = 4centroids, clusterAssment = kmeans.kmeans(dataSet, k) # 调用KMeans文件中定义的kmeans方法。## step 3: show the resultprint(\"step 3: show the result...\")kmeans.showCluster(dataSet, k, centroids, clusterAssment) 1.658985\t4.285136-3.453687\t3.4243214.838138\t1.151539-5.379713\t-3.3621040.972564\t2.924086-3.567919\t1.5316110.450614\t-3.302219-3.487105\t-1.7244322.668759\t1.594842-3.156485\t3.1911373.165506\t-3.999838-2.786837\t-3.0993544.208187\t2.984927-2.123337\t2.9433660.704199\t-0.479481-0.392370\t-3.9637042.831667\t1.574018-0.790153\t3.3431442.943496\t-3.357075-3.195883\t-2.2839262.336445\t2.875106-1.786345\t2.5542482.190101\t-1.906020-3.403367\t-2.7782881.778124\t3.880832-1.688346\t2.2302672.592976\t-2.054368-4.007257\t-3.2070662.257734\t3.387564-2.679011\t0.7851190.939512\t-4.023563-3.674424\t-2.2610842.046259\t2.735279-3.189470\t1.7802694.372646\t-0.822248-2.579316\t-3.4975761.889034\t5.190400-0.798747\t2.1855882.836520\t-2.658556-3.837877\t-3.2538152.096701\t3.886007-2.709034\t2.9238873.367037\t-3.184789-2.121479\t-4.2325862.329546\t3.179764-3.284816\t3.2730993.091414\t-3.815232-3.762093\t-2.4321913.542056\t2.778832-1.736822\t4.2410412.127073\t-2.983680-4.323818\t-3.9381163.792121\t5.135768-4.786473\t3.3585472.624081\t-3.260715-4.009299\t-2.9781152.493525\t1.963710-2.513661\t2.6421621.864375\t-3.176309-3.171184\t-3.5724522.894220\t2.489128-2.562539\t2.8844383.491078\t-3.947487-2.565729\t-2.0121143.332948\t3.983102-1.616805\t3.5731882.280615\t-2.559444-2.651229\t-3.1031982.321395\t3.154987-1.685703\t2.9396973.031012\t-3.620252-4.599622\t-2.1858294.196223\t1.126677-2.133863\t3.0936864.668892\t-2.562705-2.793241\t-2.1497062.884105\t3.043438-2.967647\t2.8486964.479332\t-1.764772-4.905566\t-2.911070 参考链接：https://github.com/wojiushimogui/kmeans","tags":["机器学习"],"categories":["算法"]},{"title":"xml生成","path":"/2019/09/07/20190907-xml生成/","content":"先创建 root Element, 然后创建 SubElement, 最后将 root 传入 ElementTree(element), 创建 tree, 调用 tree.write() 方法写入文件,创建 XML 类型的数据文件 import xml.etree.ElementTree as ETdef subElement(root, tag, text): ele = ET.SubElement(root, tag) ele.text = text ele.tail = ' 'root = ET.Element(\"note\")to = root.makeelement(\"to\", {})to.text = \"peter\"to.tail = ' 'root.append(to)subElement(root, \"from\", \"marry\")subElement(root, \"heading\", \"Reminder\")subElement(root, \"body\", \"Don't forget the meeting!\")tree = ET.ElementTree(root)tree.write(\"note.xml\", encoding=\"utf-8\", xml_declaration=True) ?xml version='1.0' encoding='utf-8'?notetopeter/tofrommarry/fromheadingReminder/headingbodyDon't forget the meeting!/body/note","tags":["Python"],"categories":["Python"]},{"title":"xml解析","path":"/2019/09/07/20190907-xml解析/","content":"annotation\tfolderILSVRC2015_VID_train_0002/ILSVRC2015_train_00555002/folder\tfilename000000/filename\tsource databaseILSVRC_2015/database\t/source\tsize width1280/width height720/height\t/size\tobject trackid0/trackid namen02691156/name bndbox xmax659/xmax xmin592/xmin ymax375/ymax ymin334/ymin /bndbox occluded0/occluded generated0/generated\t/object/annotation ElementTree生来就是为了处理XML, 它在Python标准库中有两种实现：一种是纯Python实现的, 如xml.etree.ElementTree, 另一种是速度快一点的xml.etree.cElementTree. 注意：尽量使用C语言实现的那种, 因为它速度更快, 而且消耗的内存更少. a. 遍历根节点的下一层 b. 下标访问各个标签、属性、文本 c. 查找root下的指定标签 d. 遍历XML文件 e. 修改XML文件 ```pythonimport os, systry:import xml.etree.cElementTree as ETexcept:import xml.etree.ElementTree as ET 解析xml文件xmlFilePath = os.path.abspath('000000.xml')try: tree = ET.parse(xmlFilePath) # 或者 tree = ET.ElementTree(xmlFilePath) root = tree.getroot() # 获取根节点except Exception as e: print('parse xml failed!') sys.exit() 逐层遍历print(root.tag, root.attrib, root.text)for child in root: print(child.tag, child.attrib, child.text) 递归遍历全部:def traverseXml(element): if len(element) 0: # 叶节点的len为0 for child in element: print(child.tag, child.attrib) traverseXml(child)traverseXml(root) 根据签名查找需要的标签item_lists = root.findall('item') # 只能找到儿子, 不能找到孙子, 返回的是儿子们组成的列表item = root.find('item') # 返回的是单个的儿子print(root)print(item_lists)print(item) 获取叶子节点的值当访问到叶子节点时, 就可以利用 text 来得到相应的标签了obj_bbox_set =[]objects = root.findall('object')for obj in objects: obj_name = obj.find('name').text bbox = obj.find('bndbox') x1 = int(bbox.find('xmin').text) y1 = int(bbox.find('ymin').text) x2 = int(bbox.find('xmax').text) y2 = int(bbox.find('ymax').text) obj_bbox_set.append([x1, x2, y1, y2, obj_name])print(obj_bbox_set)","tags":["Python"],"categories":["Python"]},{"title":"矩形覆盖","path":"/2019/09/07/20190907-矩形覆盖/","content":"时间限制：1秒 空间限制：32768K 热度指数：363840本题知识点： 递归 题目描述我们可以用21的小矩形横着或者竖着去覆盖更大的矩形。请问用n个21的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？ class Solution {public: int rectCover(int number) { if(number = 0) return 0; if(number == 1 || number == 2) return number; return rectCover(number - 1) + rectCover(number - 2); }}; 运行时间：538ms占用内存：484k","tags":["算法"],"categories":["算法"]},{"title":"PCA降维","path":"/2019/09/07/PCA降维/","content":"PCA（principal Component Analysis），主成分分析法。顾名思义，就是提取出数据中主要的成分，是一种数据压缩方法，常用于去除噪声、数据预处理，也是机器学习中常见的降维方法。 步骤 对所有样本进行中心化处理即将每个元素减去它的平均值，这样可以增加基向量的正交性。 计算协方差矩阵及特征值、特征向量 对特征值进行排序 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 | # coding:utf-8 from matplotlib import pylab as plt import numpy as np def read_file(filename): f = open(filename, 'r') d = f.readlines() f.close() return d class PCA(object): \"\"\" 使用PCA对高维数据进行降维处理 \"\"\" def __init__(self): data = read_file('data.txt') self.n = len(data) # 数据的个数 self.dim = 4 # 原始数据的维度 self.x = np.zeros((self.n, self.dim), dtype='float64') for i in range(self.n): data_ = data[i].split(',') self.x[i][0] = data_[0] self.x[i][1] = data_[1] self.x[i][2] = data_[2] self.x[i][3] = data_[3] self.k = 2 # 降到二维 self.mean_x = np.zeros((self.n, self.dim), dtype='float64') # 原始数据减去均值以后的x self.mean = np.zeros((self.dim, 1), dtype='float64') # x的均值 self.cov = np.zeros((self.dim, self.dim), dtype='float64') # 协方差矩阵 self.pre_x = np.zeros((self.n, self.dim), dtype='float64') # 预处理之后的数据 self.eig_val = np.zeros((1, self.dim), dtype='float64') # 特征值 self.eig_vec = np.zeros((self.dim, self.dim), dtype='float64') # 特征向量 self.final_x = np.zeros((self.n, self.k), dtype='float64') # 投影后的数据 self.pretreatment() self.pca() def pretreatment(self): \"\"\" 预处理 \"\"\" # 求均值 for i in range(self.dim): self.mean[i] = np.mean(self.x[:, i]) for i in range(self.n): self.mean_x[i] = self.x[i] - self.mean.T # 求协方差 # self.cov = np.cov(self.mean_x, rowvar=0) # mean_x已经是x减去均值了，所以直接相乘就是方差 self.cov = self.mean_x.T.dot(self.mean_x) / self.n for i in range(self.dim): self.pre_x[:, i] = self.mean_x[:, i] / np.sqrt(self.cov[i][i]) # x的每个维度都处理一次 def pca(self): \"\"\" pca的实现 \"\"\" # 需要注意的是，在这里需要对预处理之后的数据重新计算协方差 # 计算均值 for i in range(self.dim): self.mean[i] = np.mean(self.pre_x[:, i]) # 计算协方差 for i in range(self.n): self.mean_x[i] = self.pre_x[i] - self.mean.T self.cov = (self.mean_x.T.dot(self.mean_x)) / self.n # 求特征值 self.eig_val, self.eig_vec = np.linalg.eig(np.mat(self.cov)) # eig_vec的列向量是特征向量，所以用eig_v存储特征向量，以便后面排序 eig_v = np.zeros((self.dim, self.dim), dtype='float64') for i in range(self.dim): eig_v[i] = self.eig_vec[:, i].T # 排序 eig_list = zip(self.eig_val, eig_v) sorted(eig_list,key=lambda g: g[0], reverse=True) # 处理排序的结果 for i in range(len(list(eig_list))): self.eig_val[i] = eig_list[i][0] self.eig_vec[i] = eig_list[i][1] # 最大的k个特征向量，降维 f_vec = self.eig_vec[0:self.k, :] self.final_x = self.pre_x * f_vec.T # 显示降维后的数据点 plt.plot(self.final_x[0:50, 0], self.final_x[0:50, 1], 'bo') plt.plot(self.final_x[50:100, 0], self.final_x[50:100, 1], 'go') plt.plot(self.final_x[100:150, 0], self.final_x[100:150, 1], 'ro') plt.show() if __name__ == '__main__': a = PCA() —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 | 5.1,3.5,1.4,0.2,Iris-setosa 4.9,3.0,1.4,0.2,Iris-setosa 4.7,3.2,1.3,0.2,Iris-setosa 4.6,3.1,1.5,0.2,Iris-setosa 5.0,3.6,1.4,0.2,Iris-setosa 5.4,3.9,1.7,0.4,Iris-setosa 4.6,3.4,1.4,0.3,Iris-setosa 5.0,3.4,1.5,0.2,Iris-setosa 4.4,2.9,1.4,0.2,Iris-setosa 4.9,3.1,1.5,0.1,Iris-setosa 5.4,3.7,1.5,0.2,Iris-setosa 4.8,3.4,1.6,0.2,Iris-setosa 4.8,3.0,1.4,0.1,Iris-setosa 4.3,3.0,1.1,0.1,Iris-setosa 5.8,4.0,1.2,0.2,Iris-setosa 5.7,4.4,1.5,0.4,Iris-setosa 5.4,3.9,1.3,0.4,Iris-setosa 5.1,3.5,1.4,0.3,Iris-setosa 5.7,3.8,1.7,0.3,Iris-setosa 5.1,3.8,1.5,0.3,Iris-setosa 5.4,3.4,1.7,0.2,Iris-setosa 5.1,3.7,1.5,0.4,Iris-setosa 4.6,3.6,1.0,0.2,Iris-setosa 5.1,3.3,1.7,0.5,Iris-setosa 4.8,3.4,1.9,0.2,Iris-setosa 5.0,3.0,1.6,0.2,Iris-setosa 5.0,3.4,1.6,0.4,Iris-setosa 5.2,3.5,1.5,0.2,Iris-setosa 5.2,3.4,1.4,0.2,Iris-setosa 4.7,3.2,1.6,0.2,Iris-setosa 4.8,3.1,1.6,0.2,Iris-setosa 5.4,3.4,1.5,0.4,Iris-setosa 5.2,4.1,1.5,0.1,Iris-setosa 5.5,4.2,1.4,0.2,Iris-setosa 4.9,3.1,1.5,0.1,Iris-setosa 5.0,3.2,1.2,0.2,Iris-setosa 5.5,3.5,1.3,0.2,Iris-setosa 4.9,3.1,1.5,0.1,Iris-setosa 4.4,3.0,1.3,0.2,Iris-setosa 5.1,3.4,1.5,0.2,Iris-setosa 5.0,3.5,1.3,0.3,Iris-setosa 4.5,2.3,1.3,0.3,Iris-setosa 4.4,3.2,1.3,0.2,Iris-setosa 5.0,3.5,1.6,0.6,Iris-setosa 5.1,3.8,1.9,0.4,Iris-setosa 4.8,3.0,1.4,0.3,Iris-setosa 5.1,3.8,1.6,0.2,Iris-setosa 4.6,3.2,1.4,0.2,Iris-setosa 5.3,3.7,1.5,0.2,Iris-setosa 5.0,3.3,1.4,0.2,Iris-setosa 7.0,3.2,4.7,1.4,Iris-versicolor 6.4,3.2,4.5,1.5,Iris-versicolor 6.9,3.1,4.9,1.5,Iris-versicolor 5.5,2.3,4.0,1.3,Iris-versicolor 6.5,2.8,4.6,1.5,Iris-versicolor 5.7,2.8,4.5,1.3,Iris-versicolor 6.3,3.3,4.7,1.6,Iris-versicolor 4.9,2.4,3.3,1.0,Iris-versicolor 6.6,2.9,4.6,1.3,Iris-versicolor 5.2,2.7,3.9,1.4,Iris-versicolor 5.0,2.0,3.5,1.0,Iris-versicolor 5.9,3.0,4.2,1.5,Iris-versicolor 6.0,2.2,4.0,1.0,Iris-versicolor 6.1,2.9,4.7,1.4,Iris-versicolor 5.6,2.9,3.6,1.3,Iris-versicolor 6.7,3.1,4.4,1.4,Iris-versicolor 5.6,3.0,4.5,1.5,Iris-versicolor 5.8,2.7,4.1,1.0,Iris-versicolor 6.2,2.2,4.5,1.5,Iris-versicolor 5.6,2.5,3.9,1.1,Iris-versicolor 5.9,3.2,4.8,1.8,Iris-versicolor 6.1,2.8,4.0,1.3,Iris-versicolor 6.3,2.5,4.9,1.5,Iris-versicolor 6.1,2.8,4.7,1.2,Iris-versicolor 6.4,2.9,4.3,1.3,Iris-versicolor 6.6,3.0,4.4,1.4,Iris-versicolor 6.8,2.8,4.8,1.4,Iris-versicolor 6.7,3.0,5.0,1.7,Iris-versicolor 6.0,2.9,4.5,1.5,Iris-versicolor 5.7,2.6,3.5,1.0,Iris-versicolor 5.5,2.4,3.8,1.1,Iris-versicolor 5.5,2.4,3.7,1.0,Iris-versicolor 5.8,2.7,3.9,1.2,Iris-versicolor 6.0,2.7,5.1,1.6,Iris-versicolor 5.4,3.0,4.5,1.5,Iris-versicolor 6.0,3.4,4.5,1.6,Iris-versicolor 6.7,3.1,4.7,1.5,Iris-versicolor 6.3,2.3,4.4,1.3,Iris-versicolor 5.6,3.0,4.1,1.3,Iris-versicolor 5.5,2.5,4.0,1.3,Iris-versicolor 5.5,2.6,4.4,1.2,Iris-versicolor 6.1,3.0,4.6,1.4,Iris-versicolor 5.8,2.6,4.0,1.2,Iris-versicolor 5.0,2.3,3.3,1.0,Iris-versicolor 5.6,2.7,4.2,1.3,Iris-versicolor 5.7,3.0,4.2,1.2,Iris-versicolor 5.7,2.9,4.2,1.3,Iris-versicolor 6.2,2.9,4.3,1.3,Iris-versicolor 5.1,2.5,3.0,1.1,Iris-versicolor 5.7,2.8,4.1,1.3,Iris-versicolor 6.3,3.3,6.0,2.5,Iris-virginica 5.8,2.7,5.1,1.9,Iris-virginica 7.1,3.0,5.9,2.1,Iris-virginica 6.3,2.9,5.6,1.8,Iris-virginica 6.5,3.0,5.8,2.2,Iris-virginica 7.6,3.0,6.6,2.1,Iris-virginica 4.9,2.5,4.5,1.7,Iris-virginica 7.3,2.9,6.3,1.8,Iris-virginica 6.7,2.5,5.8,1.8,Iris-virginica 7.2,3.6,6.1,2.5,Iris-virginica 6.5,3.2,5.1,2.0,Iris-virginica 6.4,2.7,5.3,1.9,Iris-virginica 6.8,3.0,5.5,2.1,Iris-virginica 5.7,2.5,5.0,2.0,Iris-virginica 5.8,2.8,5.1,2.4,Iris-virginica 6.4,3.2,5.3,2.3,Iris-virginica 6.5,3.0,5.5,1.8,Iris-virginica 7.7,3.8,6.7,2.2,Iris-virginica 7.7,2.6,6.9,2.3,Iris-virginica 6.0,2.2,5.0,1.5,Iris-virginica 6.9,3.2,5.7,2.3,Iris-virginica 5.6,2.8,4.9,2.0,Iris-virginica 7.7,2.8,6.7,2.0,Iris-virginica 6.3,2.7,4.9,1.8,Iris-virginica 6.7,3.3,5.7,2.1,Iris-virginica 7.2,3.2,6.0,1.8,Iris-virginica 6.2,2.8,4.8,1.8,Iris-virginica 6.1,3.0,4.9,1.8,Iris-virginica 6.4,2.8,5.6,2.1,Iris-virginica 7.2,3.0,5.8,1.6,Iris-virginica 7.4,2.8,6.1,1.9,Iris-virginica 7.9,3.8,6.4,2.0,Iris-virginica 6.4,2.8,5.6,2.2,Iris-virginica 6.3,2.8,5.1,1.5,Iris-virginica 6.1,2.6,5.6,1.4,Iris-virginica 7.7,3.0,6.1,2.3,Iris-virginica 6.3,3.4,5.6,2.4,Iris-virginica 6.4,3.1,5.5,1.8,Iris-virginica 6.0,3.0,4.8,1.8,Iris-virginica 6.9,3.1,5.4,2.1,Iris-virginica 6.7,3.1,5.6,2.4,Iris-virginica 6.9,3.1,5.1,2.3,Iris-virginica 5.8,2.7,5.1,1.9,Iris-virginica 6.8,3.2,5.9,2.3,Iris-virginica 6.7,3.3,5.7,2.5,Iris-virginica 6.7,3.0,5.2,2.3,Iris-virginica 6.3,2.5,5.0,1.9,Iris-virginica 6.5,3.0,5.2,2.0,Iris-virginica 6.2,3.4,5.4,2.3,Iris-virginica 5.9,3.0,5.1,1.8,Iris-virginica —|— 参考链接：https://github.com/eva-n27/PCA","tags":["Python"],"categories":["深度学习"]},{"title":"kmeans算法","path":"/2019/09/07/kmeans算法/","content":"K-Means的思想十分简单，首先随机指定类中心，根据样本与类中心的远近划分类簇，接着重新计算类中心，迭代直至收敛。但是其中迭代的过程并不是主观地想象得出，事实上，若将样本的类别看做为“隐变量”（latent variable），类中心看作样本的分布参数，这一过程正是通过EM算法的两步走策略而计算出，其根本的目的是为了最小化平方误差函数E。 kmeans算法的最大弱点：只能处理球形的簇（理论）kmeans 计算步骤 1.随机选取K个聚类中心，这里的k值可以自己设定 2.先设置一个聚类标志，用来保存当前的 样本与第几个聚类中心最近 3.计算每个样例与每个聚类中心的距离，保存最小距离的k以及距离 4.更新聚类中心，为当前类别所有样本的均值大小 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 | from numpy import * import time import matplotlib.pyplot as plt # calculate Euclidean distance def euclDistance(vector1, vector2): return sqrt(sum(power(vector2 - vector1, 2))) # 求这两个矩阵的距离， vector1, vector2 均为矩阵 # init centroids with random samples # 在样本集中随机选取k个样本点作为初始质心 def initCentroids(dataSet, k): numSamples, dim = dataSet.shape # 矩阵的行数、列数 centroids = zeros((k, dim)) # 感觉要不要你都可以 for i in range(k): index = int(random.uniform(0, numSamples)) # 随机产生一个浮点数，然后将其转化为int型 centroids[i, :] = dataSet[index, :] return centroids # k-means cluster # dataSet为一个矩阵 # k为将dataSet矩阵中的样本分成k个类 def kmeans(dataSet, k): numSamples = dataSet.shape[0] # 读取矩阵dataSet的第一维度的长度,即获得有多少个样本数据 # first column stores which cluster this sample belongs to, # second column stores the error between this sample and its centroid clusterAssment = mat(zeros((numSamples, 2))) # 得到一个N*2的零矩阵 clusterChanged = True ## step 1: init centroids centroids = initCentroids(dataSet, k) # 在样本集中随机选取k个样本点作为初始质心 while clusterChanged: clusterChanged = False ## for each sample for i in range(numSamples): # range minDist = 100000.0 minIndex = 0 ## for each centroid ## step 2: find the centroid who is closest # 计算每个样本点与质点之间的距离，将其归内到距离最小的那一簇 for j in range(k): distance = euclDistance(centroids[j, :], dataSet[i, :]) if distance minDist: minDist = distance minIndex = j ## step 3: update its cluster # k个簇里面与第i个样本距离最小的的标号和距离保存在clusterAssment中 # 若所有的样本不在变化，则退出while循环 if clusterAssment[i, 0] != minIndex: clusterChanged = True clusterAssment[i, :] = minIndex, minDist ** 2 # 两个**表示的是minDist的平方 ## step 4: update centroids for j in range(k): # clusterAssment[:,0].A==j是找出矩阵clusterAssment中第一列元素中等于j的行的下标，返回的是一个以array的列表，第一个array为等于j的下标 pointsInCluster = dataSet[nonzero(clusterAssment[:, 0].A == j)[0]] # 将dataSet矩阵中相对应的样本提取出来 centroids[j, :] = mean(pointsInCluster, axis=0) # 计算标注为j的所有样本的平均值 print('Congratulations, cluster complete!') return centroids, clusterAssment # show your cluster only available with 2-D data # centroids为k个类别，其中保存着每个类别的质心 # clusterAssment为样本的标记，第一列为此样本的类别号，第二列为到此类别质心的距离 def showCluster(dataSet, k, centroids, clusterAssment): numSamples, dim = dataSet.shape if dim != 2: print(\"Sorry! I can not draw because the dimension of your data is not 2!\") return 1 mark = ['or', 'ob', 'og', 'ok', '^r', '+r', 'sr', 'dr', 'r', 'pr'] if k len(mark): print(\"Sorry! Your k is too large! please contact wojiushimogui\") return 1 # draw all samples for i in range(numSamples): markIndex = int(clusterAssment[i, 0]) # 为样本指定颜色 plt.plot(dataSet[i, 0], dataSet[i, 1], mark[markIndex]) mark = ['Dr', 'Db', 'Dg', 'Dk', '^b', '+b', 'sb', 'db', 'b', 'pb'] # draw the centroids for i in range(k): plt.plot(centroids[i, 0], centroids[i, 1], mark[i], markersize=12) plt.show() —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 | from numpy import * import time import matplotlib.pyplot as plt import kmeans ## step 1: load data print(\"step 1: load data...\") dataSet = [] # 列表，用来表示，列表中的每个元素也是一个二维的列表；这个二维列表就是一个样本，样本中包含有我们的属性值和类别号。 # 与我们所熟悉的矩阵类似，最终我们将获得N*2的矩阵，每行元素构成了我们的训练样本的属性值和类别号 fileIn = open(\"./testSet.txt\") # 是正斜杠 for line in fileIn.readlines(): temp = [] lineArr = line.strip().split('\\t') # line.strip()把末尾的' '去掉 temp.append(float(lineArr[0])) temp.append(float(lineArr[1])) dataSet.append(temp) # dataSet.append([float(lineArr[0]), float(lineArr[1])]) fileIn.close() ## step 2: clustering... print(\"step 2: clustering...\") dataSet = mat(dataSet) # mat()函数是Numpy中的库函数，将数组转化为矩阵 k = 4 centroids, clusterAssment = kmeans.kmeans(dataSet, k) # 调用KMeans文件中定义的kmeans方法。 ## step 3: show the result print(\"step 3: show the result...\") kmeans.showCluster(dataSet, k, centroids, clusterAssment) —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 | 1.658985\t4.285136 -3.453687\t3.424321 4.838138\t1.151539 -5.379713\t-3.362104 0.972564\t2.924086 -3.567919\t1.531611 0.450614\t-3.302219 -3.487105\t-1.724432 2.668759\t1.594842 -3.156485\t3.191137 3.165506\t-3.999838 -2.786837\t-3.099354 4.208187\t2.984927 -2.123337\t2.943366 0.704199\t-0.479481 -0.392370\t-3.963704 2.831667\t1.574018 -0.790153\t3.343144 2.943496\t-3.357075 -3.195883\t-2.283926 2.336445\t2.875106 -1.786345\t2.554248 2.190101\t-1.906020 -3.403367\t-2.778288 1.778124\t3.880832 -1.688346\t2.230267 2.592976\t-2.054368 -4.007257\t-3.207066 2.257734\t3.387564 -2.679011\t0.785119 0.939512\t-4.023563 -3.674424\t-2.261084 2.046259\t2.735279 -3.189470\t1.780269 4.372646\t-0.822248 -2.579316\t-3.497576 1.889034\t5.190400 -0.798747\t2.185588 2.836520\t-2.658556 -3.837877\t-3.253815 2.096701\t3.886007 -2.709034\t2.923887 3.367037\t-3.184789 -2.121479\t-4.232586 2.329546\t3.179764 -3.284816\t3.273099 3.091414\t-3.815232 -3.762093\t-2.432191 3.542056\t2.778832 -1.736822\t4.241041 2.127073\t-2.983680 -4.323818\t-3.938116 3.792121\t5.135768 -4.786473\t3.358547 2.624081\t-3.260715 -4.009299\t-2.978115 2.493525\t1.963710 -2.513661\t2.642162 1.864375\t-3.176309 -3.171184\t-3.572452 2.894220\t2.489128 -2.562539\t2.884438 3.491078\t-3.947487 -2.565729\t-2.012114 3.332948\t3.983102 -1.616805\t3.573188 2.280615\t-2.559444 -2.651229\t-3.103198 2.321395\t3.154987 -1.685703\t2.939697 3.031012\t-3.620252 -4.599622\t-2.185829 4.196223\t1.126677 -2.133863\t3.093686 4.668892\t-2.562705 -2.793241\t-2.149706 2.884105\t3.043438 -2.967647\t2.848696 4.479332\t-1.764772 -4.905566\t-2.911070 —|— 参考链接：https://github.com/wojiushimogui/kmeans","tags":["Python"],"categories":["深度学习"]},{"title":"xml生成","path":"/2019/09/07/xml生成/","content":"先创建 root Element, 然后创建 SubElement, 最后将 root 传入 ElementTree(element), 创建 tree, 调用 tree.write() 方法写入文件,创建 XML 类型的数据文件 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | import xml.etree.ElementTree as ET def subElement(root, tag, text): ele = ET.SubElement(root, tag) ele.text = text ele.tail = ' ' root = ET.Element(\"note\") to = root.makeelement(\"to\", {}) to.text = \"peter\" to.tail = ' ' root.append(to) subElement(root, \"from\", \"marry\") subElement(root, \"heading\", \"Reminder\") subElement(root, \"body\", \"Don't forget the meeting!\") tree = ET.ElementTree(root) tree.write(\"note.xml\", encoding=\"utf-8\", xml_declaration=True) —|— 1 2 3 4 5 6 7 | ?xml version='1.0' encoding='utf-8'? note topeter/to frommarry/from headingReminder/heading bodyDon't forget the meeting!/body /note —|—","tags":["XML"],"categories":["工具"]},{"title":"xml解析","path":"/2019/09/07/xml解析/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 | annotation folderILSVRC2015_VID_train_0002/ILSVRC2015_train_00555002/folder filename000000/filename source databaseILSVRC_2015/database /source size width1280/width height720/height /size object trackid0/trackid namen02691156/name bndbox xmax659/xmax xmin592/xmin ymax375/ymax ymin334/ymin /bndbox occluded0/occluded generated0/generated /object /annotation —|— ElementTree生来就是为了处理XML, 它在Python标准库中有两种实现：一种是纯Python实现的, 如xml.etree.ElementTree, 另一种是速度快一点的xml.etree.cElementTree. 注意：尽量使用C语言实现的那种, 因为它速度更快, 而且消耗的内存更少. a. 遍历根节点的下一层 b. 下标访问各个标签、属性、文本 c. 查找root下的指定标签 d. 遍历XML文件 e. 修改XML文件 1 2 3 4 5 | import os, sys try: import xml.etree.cElementTree as ET except: import xml.etree.ElementTree as ET —|— 解析xml文件1 2 3 4 5 6 7 8 | xmlFilePath = os.path.abspath('000000.xml') try: tree = ET.parse(xmlFilePath) # 或者 tree = ET.ElementTree(xmlFilePath) root = tree.getroot() # 获取根节点 except Exception as e: print('parse xml failed!') sys.exit() —|— 逐层遍历1 2 3 | print(root.tag, root.attrib, root.text) for child in root: print(child.tag, child.attrib, child.text) —|— 递归遍历全部:1 2 3 4 5 6 7 | def traverseXml(element): if len(element) 0: # 叶节点的len为0 for child in element: print(child.tag, child.attrib) traverseXml(child) traverseXml(root) —|— 根据签名查找需要的标签1 2 3 4 5 | item_lists = root.findall('item') # 只能找到儿子, 不能找到孙子, 返回的是儿子们组成的列表 item = root.find('item') # 返回的是单个的儿子 print(root) print(item_lists) print(item) —|— 获取叶子节点的值当访问到叶子节点时, 就可以利用 text 来得到相应的标签了1 2 3 4 5 6 7 8 9 10 11 | obj_bbox_set =[] objects = root.findall('object') for obj in objects: obj_name = obj.find('name').text bbox = obj.find('bndbox') x1 = int(bbox.find('xmin').text) y1 = int(bbox.find('ymin').text) x2 = int(bbox.find('xmax').text) y2 = int(bbox.find('ymax').text) obj_bbox_set.append([x1, x2, y1, y2, obj_name]) print(obj_bbox_set) —|—","tags":["Python"],"categories":["算法"]},{"title":"矩形覆盖","path":"/2019/09/07/矩形覆盖/","content":"时间限制：1秒 空间限制：32768K 热度指数：363840本题知识点： 递归 题目描述我们可以用21的小矩形横着或者竖着去覆盖更大的矩形。请问用n个21的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？ 1 2 3 4 5 6 7 8 | class Solution { public: int rectCover(int number) { if(number = 0) return 0; if(number == 1 || number == 2) return number; return rectCover(number - 1) + rectCover(number - 2); } }; —|— 运行时间：538ms占用内存：484k","tags":["编程"],"categories":["算法"]},{"title":"docker使用笔记","path":"/2019/09/06/20190906-docker使用笔记/","content":"0. 查看需要的Docker列表https://hub.docker.com/r/nvidia/cuda/列出已经存在的镜像列表 1. 创建容器(没有镜像, 会自动下载)docker run -it -v /media/data2/dh:/media/data2/dh —name=cloud —runtime=nvidia nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04 /bin/bash(已有镜像)docker run -it -v /media/data2/dh:/media/data2/dh —name=cloud —runtime=nvidia afc5ab1e9a0d /bin/bash 2. 启动容器dockerstart cloud（容器名） 3. 进入容器docker exec -it cloud /bin/bash 4. 离开容器exit 5. 删除容器A. 停止容器docker stop CONTAINER_ID B. 删除容器docker rm CONTAINER_ID 启动 systemctl start docker 守护进程重启 sudo systemctl daemon-reload 重启docker服务 systemctl restart docker 重启docker服务 sudo service docker restart 关闭docker service docker stop 关闭docker systemctl stop docker","tags":["工具"],"categories":["工具"]},{"title":"mx-maskrcnn环境搭建","path":"/2019/09/06/20190906-mx-maskrcnn环境搭建/","content":"从https://hub.docker.com/ 选取所需的镜像 下载caffe2 镜像docker pull caffe2/caffe2 :snapshot-py2-cuda8.0-cudnn7-ubuntu16.04Cannot open your terminal ‘/dev/pts/0’错误原因解决可以使用script命令来记录这个终端会话,执行script /dev/nullscreen -S caiusdocker 分配http://www.cnblogs.com/codeaaa/p/9041533.htmlhttps://blog.csdn.net/u013948858/article/details/78429954（有效）docker run -it -v /media/:/media/ —name=mxcaius —runtime=nvidia 89f57a4ade86 /bin/bashdocker ubuntu源卡主，解决措施：mv source改完之后改回去mv sources.list.d.odd sources.list.d需要BLAS库，可以安装ATLAS、OpenBLAS、MKL，我安装的是atlassudo apt-get install libatlas-base-dev安装opencv库pip install opencv-pythonsudo apt-get install libopencv-dev安装Python包cd python;python setup.py installapt-get install python-numpyodules/imgproc/src/resize.cpp:3596: error: (-215:Assertion failed) func != 0 in function ‘resize’ numpy 1.14setuptools和numpy(sudo apt-get install python-numpy) git clone —recursive https://github.com/apache/incubator-mxnet.git incubator-mxnet —branch 0.11.0cp rcnn/CXX_OP/* incubator-mxnet/src/operator/cd incubator-mxnetmake -j USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1cd ..makebash scripts/train_alternate.shmakecaius@sugon:~$ echo -n “cvlab1205” |md5sumd0599e86d6134fee87bcf017ddca1990 然后我们使用docker ps查看到该容器信息，接下来就使用docker attach进入该容器可形变卷积 IndexError: list index out of range self.class_id = [0, 1] imdb = eval(dataset)(image_set, root_path, dataset_path) [‘train’]imagesetIcdar2015/media/data1/caius/dataset[‘train’]imagesetIcdar2015/media/data1/caius/dataset icdar2015_train gt roidb loaded from model/res50-fpn/icdar2015/alternate/cache/icdar2015_train_gt_roidb.pklOpenCV Error: Assertion failed (func != 0) in resize, file /io/opencv/modules/imgproc/src/imgwarp.cpp, line 3370Traceback (most recent call last):File “train_alternate_mask_fpn.py”, line 118, inmain()File “train_alternate_mask_fpn.py”, line 115, in mainargs.rcnn_epoch, args.rcnn_lr, args.rcnn_lr_step)File “train_alternate_mask_fpn.py”, line 61, in alternate_trainvis=False, shuffle=False, thresh=0)File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/tools/test_rpn.py”, line 63, in test_rpnimdb_boxes = generate_proposals(predictor, test_data, imdb, vis=vis, thresh=thresh)File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/core/tester.py”, line 61, in generate_proposalsfor im_info, data_batch in test_data:File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/core/loader.py”, line 60, in nextself.get_batch()File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/core/loader.py”, line 83, in get_batchdata, label, im_info = get_rpn_testbatch(roidb)File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/io/rpn.py”, line 32, in get_rpn_testbatchimgs, roidb,masks = get_image(roidb)File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/io/image.py”, line 99, in get_imagemask, _ = resize(mask, target_size, max_size)File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/io/image.py”, line 138, in resizeim = cv2.resize(im, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR)cv2.error: /io/opencv/modules/imgproc/src/imgwarp.cpp:3370: error: (-215) func != 0 in function resize root@d59236d7a683:/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss#","tags":["工具"],"categories":["工具"]},{"title":"变态跳台阶","path":"/2019/09/06/20190906-变态跳台阶/","content":"时间限制：1秒 空间限制：32768K 热度指数：396047本题知识点： 递归 题目描述一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 解析f(n) = f(n-1)+f(n-2)+…+f(1)f(n-1) = f(n-2)+f(n-3)+…f(1) f(n) = 2*f(n-1) class Solution {public: int jumpFloorII(int number) { if(number == 1) return 1; else return 2*jumpFloorII(number-1); }}; 运行时间：4ms占用内存：480k","tags":["算法"],"categories":["算法"]},{"title":"docker使用笔记","path":"/2019/09/06/docker使用笔记/","content":"0. 查看需要的Docker列表https://hub.docker.com/r/nvidia/cuda/列出已经存在的镜像列表 1. 创建容器(没有镜像, 会自动下载)docker run -it -v /media/data2/dh:/media/data2/dh —name=cloud —runtime=nvidia nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04 /bin/bash(已有镜像)docker run -it -v /media/data2/dh:/media/data2/dh —name=cloud —runtime=nvidia afc5ab1e9a0d /bin/bash 2. 启动容器dockerstart cloud（容器名） 3. 进入容器docker exec -it cloud /bin/bash 4. 离开容器exit 5. 删除容器A. 停止容器docker stop CONTAINER_ID B. 删除容器docker rm CONTAINER_ID 启动 systemctl start docker 守护进程重启 sudo systemctl daemon-reload 重启docker服务 systemctl restart docker 重启docker服务 sudo service docker restart 关闭docker service docker stop 关闭docker systemctl stop docker","tags":["Docker"],"categories":["C++"]},{"title":"mx-maskrcnn环境搭建","path":"/2019/09/06/mx-maskrcnn环境搭建/","content":"从https://hub.docker.com/ 选取所需的镜像 下载caffe2 镜像docker pull caffe2/caffe2 :snapshot-py2-cuda8.0-cudnn7-ubuntu16.04Cannot open your terminal ‘/dev/pts/0’错误原因解决可以使用script命令来记录这个终端会话,执行script /dev/nullscreen -S caiusdocker 分配http://www.cnblogs.com/codeaaa/p/9041533.htmlhttps://blog.csdn.net/u013948858/article/details/78429954（有效）docker run -it -v /media/:/media/ —name=mxcaius —runtime=nvidia 89f57a4ade86 /bin/bashdocker ubuntu源卡主，解决措施：mv source改完之后改回去mv sources.list.d.odd sources.list.d需要BLAS库，可以安装ATLAS、OpenBLAS、MKL，我安装的是atlassudo apt-get install libatlas-base-dev安装opencv库pip install opencv-pythonsudo apt-get install libopencv-dev安装Python包cd python;python setup.py installapt-get install python-numpyodules/imgproc/src/resize.cpp:3596: error: (-215:Assertion failed) func != 0 in function ‘resize’ numpy 1.14setuptools和numpy(sudo apt-get install python-numpy) git clone —recursive https://github.com/apache/incubator-mxnet.git incubator-mxnet —branch 0.11.0cp rcnn/CXX_OP/* incubator-mxnet/src/operator/cd incubator-mxnetmake -j USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1cd ..makebash scripts/train_alternate.shmakecaius@sugon:~$ echo -n “cvlab1205” |md5sumd0599e86d6134fee87bcf017ddca1990 然后我们使用docker ps查看到该容器信息，接下来就使用docker attach进入该容器可形变卷积 IndexError: list index out of range self.class_id = [0, 1] imdb = eval(dataset)(image_set, root_path, dataset_path) [‘train’]imagesetIcdar2015/media/data1/caius/dataset[‘train’]imagesetIcdar2015/media/data1/caius/dataset icdar2015_train gt roidb loaded from model/res50-fpn/icdar2015/alternate/cache/icdar2015_train_gt_roidb.pklOpenCV Error: Assertion failed (func != 0) in resize, file /io/opencv/modules/imgproc/src/imgwarp.cpp, line 3370Traceback (most recent call last):File “train_alternate_mask_fpn.py”, line 118, inmain()File “train_alternate_mask_fpn.py”, line 115, in mainargs.rcnn_epoch, args.rcnn_lr, args.rcnn_lr_step)File “train_alternate_mask_fpn.py”, line 61, in alternate_trainvis=False, shuffle=False, thresh=0)File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/tools/test_rpn.py”, line 63, in test_rpnimdb_boxes = generate_proposals(predictor, test_data, imdb, vis=vis, thresh=thresh)File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/core/tester.py”, line 61, in generate_proposalsfor im_info, data_batch in test_data:File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/core/loader.py”, line 60, in nextself.get_batch()File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/core/loader.py”, line 83, in get_batchdata, label, im_info = get_rpn_testbatch(roidb)File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/io/rpn.py”, line 32, in get_rpn_testbatchimgs, roidb,masks = get_image(roidb)File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/io/image.py”, line 99, in get_imagemask, _ = resize(mask, target_size, max_size)File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/io/image.py”, line 138, in resizeim = cv2.resize(im, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR)cv2.error: /io/opencv/modules/imgproc/src/imgwarp.cpp:3370: error: (-215) func != 0 in function resize root@d59236d7a683:/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss#","tags":["Python"],"categories":["深度学习"]},{"title":"变态跳台阶","path":"/2019/09/06/变态跳台阶/","content":"时间限制：1秒 空间限制：32768K 热度指数：396047本题知识点： 递归 题目描述一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。 解析f(n) = f(n-1)+f(n-2)+…+f(1)f(n-1) = f(n-2)+f(n-3)+…f(1) f(n) = 2*f(n-1) 1 2 3 4 5 6 7 8 9 | class Solution { public: int jumpFloorII(int number) { if(number == 1) return 1; else return 2*jumpFloorII(number-1); } }; —|— 运行时间：4ms占用内存：480k","tags":["编程"],"categories":["算法"]},{"title":"Terminal 手册","path":"/2019/09/05/20190905-cmd手册/","content":"查看磁盘空间df -hl 查看磁盘剩余空间df -h 查看每个根路径的分区大小 查看文件/文件夹大小查看指定文件/文件夹大小：du -hs 文件名或文件夹名查看当前文件夹下所有文件大小（包括子文件夹）：du -sh 查看文件数量统计当前目录下文件的个数（不包括目录）ls -l | grep “^-“ | wc -l ls 命令ls -a 显示全部的文件及文件夹，包括隐藏的文件或文件夹ls -l 显示较全的文件信息，包括权限、用户、用户组等。ls —color 显示文件及文件夹，并标有不同的颜色。 tab键tab command 用于当你的命令记不全时，输入一部再按一下进行补全，如果有多个前面部分相同命令，则按两次tab键 cmpcmp /bin/ls /bin/dir 用于比较两个文件是否是完全相同的。 cpcp /bin/ls /bin/a 用于复制文件的命令。这时就复制了一个命令文件，就可以运行a命令，与ls用法相同cp命令与操作文件一样是用来复制的，带r表示将其子目录一起复制。 mv格式：mv /home/user1/桌面/ruijie/xrgsu /usr/share/local/bin/xrgsu chmod用于改为用户对于文件的操作权限。chmod 0+r 添加读的权限。sudo chmod 0-r filename取消读的权限。 cat cat 命令就是用于查看ubuntu中文本文件的内容的命令。 cat /proc/cpuinfo 用于查看计算机的cpu信息。 cat /proc/meminfo 用于查看计算机的内在信息。 cat /etc/issue 查看ubuntu的版本信息。 查看cuda 版本 cat /usr/local/cuda/version.txt 查看cudnn 版本 cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 touch格式：touch test1 test2 test3touch命令用于创建文件，可以同一时间创建多个文件。 pwdpwd命令是用来指出当前所在的路径。是print working directory的缩写","tags":["工具"],"categories":["工具"]},{"title":"跳台阶","path":"/2019/09/05/20190905-跳台阶/","content":"时间限制：1秒 空间限制：32768K 热度指数：489122本题知识点： 递归 题目描述一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。 class Solution {public: int jumpFloor(int number) { if(number == 1) return 1; else if(number==2) return 2; else return jumpFloor(number-1)+jumpFloor(number-2); }}; 运行时间：519 ms占用内存：456K","tags":["算法"],"categories":["算法"]},{"title":"Terminal 手册","path":"/2019/09/05/cmd手册/","content":"查看磁盘空间df -hl 查看磁盘剩余空间df -h 查看每个根路径的分区大小 查看文件/文件夹大小查看指定文件/文件夹大小：du -hs 文件名或文件夹名查看当前文件夹下所有文件大小（包括子文件夹）：du -sh 查看文件数量统计当前目录下文件的个数（不包括目录）ls -l | grep “^-“ | wc -l ls 命令ls -a 显示全部的文件及文件夹，包括隐藏的文件或文件夹ls -l 显示较全的文件信息，包括权限、用户、用户组等。ls —color 显示文件及文件夹，并标有不同的颜色。 tab键tab command 用于当你的命令记不全时，输入一部再按一下进行补全，如果有多个前面部分相同命令，则按两次tab键 cmpcmp /bin/ls /bin/dir 用于比较两个文件是否是完全相同的。 cpcp /bin/ls /bin/a 用于复制文件的命令。这时就复制了一个命令文件，就可以运行a命令，与ls用法相同cp命令与操作文件一样是用来复制的，带r表示将其子目录一起复制。 mv格式：mv /home/user1/桌面/ruijie/xrgsu /usr/share/local/bin/xrgsu chmod用于改为用户对于文件的操作权限。chmod 0+r 添加读的权限。sudo chmod 0-r filename取消读的权限。 cat cat 命令就是用于查看ubuntu中文本文件的内容的命令。 cat /proc/cpuinfo 用于查看计算机的cpu信息。 cat /proc/meminfo 用于查看计算机的内在信息。 cat /etc/issue 查看ubuntu的版本信息。 查看cuda 版本 cat /usr/local/cuda/version.txt 查看cudnn 版本 cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2 touch格式：touch test1 test2 test3touch命令用于创建文件，可以同一时间创建多个文件。 pwdpwd命令是用来指出当前所在的路径。是print working directory的缩写","tags":["编程"],"categories":["其他"]},{"title":"跳台阶","path":"/2019/09/05/跳台阶/","content":"时间限制：1秒 空间限制：32768K 热度指数：489122本题知识点： 递归 题目描述一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。 1 2 3 4 5 6 7 8 9 10 11 | class Solution { public: int jumpFloor(int number) { if(number == 1) return 1; else if(number==2) return 2; else return jumpFloor(number-1)+jumpFloor(number-2); } }; —|— 运行时间：519 ms占用内存：456K","tags":["编程"],"categories":["算法"]},{"title":"采坑记录","path":"/2019/09/04/20190904-采坑记录/","content":"os.getcwd()该函数不需要传递参数，它返回当前的目录。需要说明的是，当前目录并不是指脚本所在的目录，而是所运行脚本的目录。就是说如果你在home的终端运行 os.listdir(path)其参数path 为要获得内容目录的路径","tags":["其他"],"categories":["其他"]},{"title":"斐波那契数列","path":"/2019/09/04/20190904-斐波那契数列/","content":"时间限制：1秒 空间限制：32768K 热度指数：640784本题知识点： 递归 题目描述大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项（从0开始，第0项为0）。n=39 class Solution {public: int Fibonacci(int n) { if(n = 0) return 0; if(n==1 || n==2) return 1; return Fibonacci(n-1)+Fibonacci(n-2); }}; 运行时间：694 ms占用内存：484K","tags":["算法"],"categories":["算法"]},{"title":"斐波那契数列","path":"/2019/09/04/斐波那契数列/","content":"时间限制：1秒 空间限制：32768K 热度指数：640784本题知识点： 递归 题目描述大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项（从0开始，第0项为0）。n=39 1 2 3 4 5 6 7 8 | class Solution { public: int Fibonacci(int n) { if(n = 0) return 0; if(n==1 || n==2) return 1; return Fibonacci(n-1)+Fibonacci(n-2); } }; —|— 运行时间：694 ms占用内存：484K","tags":["编程"],"categories":["算法"]},{"title":"采坑记录","path":"/2019/09/04/采坑记录/","content":"os.getcwd()该函数不需要传递参数，它返回当前的目录。需要说明的是，当前目录并不是指脚本所在的目录，而是所运行脚本的目录。就是说如果你在home的终端运行 os.listdir(path)其参数path 为要获得内容目录的路径","tags":["编程"],"categories":["其他"]},{"title":"遍历文件夹图片","path":"/2019/09/03/20190903-遍历文件夹图片/","content":"题目要求要求从下图所示的文件夹遍历子文件夹，并将所在的图片路径保存到txt文件中 代码如下import os, globimport randomroot = '/media/data1/lesson/pokemon/pokeman'f_w = open(os.path.join('../', 'train2.txt'), 'w', encoding='utf8')images = []for name in sorted(os.listdir(os.path.join((root)))): # 获取子文件夹 if not os.path.isdir(os.path.join(root,name)): continue images += glob.glob(os.path.join(root, name, '*.png')) images += glob.glob(os.path.join(root, name, '*.jpg')) images += glob.glob(os.path.join(root, name, '*.jpeg'))random.shuffle(images)for line in images: f_w.write(line + ' ') # f_w.write(line)f_w.close()print('Write Done!') # 咖喱的代码import osimport globimport os.path as osp dataset_dir = os.getcwd() bulbasaur_dir = osp.join(dataset_dir, 'pokeman/bulbasaur') charmander_dir = osp.join(dataset_dir, 'pokeman/charmander') mewtwo_dir = osp.join(dataset_dir, 'pokeman/mewtwo') pikachu_dir = osp.join(dataset_dir, 'pokeman/pikachu') squirtle_dir = osp.join(dataset_dir, 'pokeman/squirtle')def process(dir_path, save_path): img_paths = glob.glob(osp.join(dir_path, '*.jpg')) f= open(save_path, 'w') for img_path in img_paths: f.write(img_path) f.write(' ') f.close()dataset_dir = os.getcwd()bulbasaur_dir = osp.join(dataset_dir, 'pokeman/bulbasaur')charmander_dir = osp.join(dataset_dir, 'pokeman/charmander')mewtwo_dir = osp.join(dataset_dir, 'pokeman/mewtwo')pikachu_dir = osp.join(dataset_dir, 'pokeman/pikachu')squirtle_dir = osp.join(dataset_dir, 'pokeman/squirtle')save_dir = osp.join(dataset_dir, 'imgpath.txt')process(test_dir, save_dir)","tags":["技术"],"categories":["技术"]},{"title":"遍历文件夹图片","path":"/2019/09/03/遍历文件夹图片/","content":"题目要求要求从下图所示的文件夹遍历子文件夹，并将所在的图片路径保存到txt文件中 代码如下1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | import os, glob import random root = '/media/data1/lesson/pokemon/pokeman' f_w = open(os.path.join('../', 'train2.txt'), 'w', encoding='utf8') images = [] for name in sorted(os.listdir(os.path.join((root)))): # 获取子文件夹 if not os.path.isdir(os.path.join(root,name)): continue images += glob.glob(os.path.join(root, name, '*.png')) images += glob.glob(os.path.join(root, name, '*.jpg')) images += glob.glob(os.path.join(root, name, '*.jpeg')) random.shuffle(images) for line in images: f_w.write(line + ' ') # f_w.write(line) f_w.close() print('Write Done!') —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 | # 咖喱的代码 import os import glob import os.path as osp dataset_dir = os.getcwd() bulbasaur_dir = osp.join(dataset_dir, 'pokeman/bulbasaur') charmander_dir = osp.join(dataset_dir, 'pokeman/charmander') mewtwo_dir = osp.join(dataset_dir, 'pokeman/mewtwo') pikachu_dir = osp.join(dataset_dir, 'pokeman/pikachu') squirtle_dir = osp.join(dataset_dir, 'pokeman/squirtle') def process(dir_path, save_path): img_paths = glob.glob(osp.join(dir_path, '*.jpg')) f= open(save_path, 'w') for img_path in img_paths: f.write(img_path) f.write(' ') f.close() dataset_dir = os.getcwd() bulbasaur_dir = osp.join(dataset_dir, 'pokeman/bulbasaur') charmander_dir = osp.join(dataset_dir, 'pokeman/charmander') mewtwo_dir = osp.join(dataset_dir, 'pokeman/mewtwo') pikachu_dir = osp.join(dataset_dir, 'pokeman/pikachu') squirtle_dir = osp.join(dataset_dir, 'pokeman/squirtle') save_dir = osp.join(dataset_dir, 'imgpath.txt') process(test_dir, save_dir) —|—","tags":["编程"],"categories":["其他"]},{"title":"用两个栈实现队列","path":"/2019/09/02/20190902-用两个栈实现队列/","content":"时间限制：1秒 空间限制：32768K 热度指数：499320本题知识点： 队列 栈 题目描述用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。 class Solution{public: void push(int node) { while(!stack2.empty())//入队时要保证 Stack2为空 { stack1.push(stack2.top()); stack2.pop(); } stack1.push(node); } int pop() { while(!stack1.empty())//入队时要保证 Stack1为空 { stack2.push(stack1.top()); stack1.pop(); } int temp = stack2.top(); stack2.pop(); return temp; }private: stackint stack1; stackint stack2;}; 运行时间：3ms占用内存：460K","tags":["算法"],"categories":["算法"]},{"title":"用两个栈实现队列","path":"/2019/09/02/用两个栈实现队列/","content":"时间限制：1秒 空间限制：32768K 热度指数：499320本题知识点： 队列 栈 题目描述用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 | class Solution { public: void push(int node) { while(!stack2.empty())//入队时要保证 Stack2为空 { stack1.push(stack2.top()); stack2.pop(); } stack1.push(node); } int pop() { while(!stack1.empty())//入队时要保证 Stack1为空 { stack2.push(stack1.top()); stack1.pop(); } int temp = stack2.top(); stack2.pop(); return temp; } private: stackint stack1; stackint stack2; }; —|— 运行时间：3ms占用内存：460K","tags":["编程"],"categories":["C++"]},{"title":"从尾到头打印链表","path":"/2019/08/29/20190829-从尾到头打印链表/","content":"时间限制：1秒 空间限制：32768K 热度指数：1022710本题知识点： 链表 题目描述输入一个链表，按链表从尾到头的顺序返回一个ArrayList。 /*** struct ListNode {* int val;* struct ListNode *next;* ListNode(int x) :* val(x), next(NULL) {* }* };*/class Solution {public: vectorint printListFromTailToHead(ListNode* head) { vectorint value; ListNode *p=NULL; p=head; stackint stk; while(p!=NULL){ stk.push(p-val); p=p-next; } while(!stk.empty()){ value.push_back(stk.top()); stk.pop(); } return value; }}; 运行时间：3ms占用内存：480K","tags":["算法"],"categories":["算法"]},{"title":"替换空格","path":"/2019/08/29/20190829-替换空格/","content":"时间限制：1秒 空间限制：32768K 热度指数：1142927本题知识点： 字符串 题目描述请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。 代码class Solution {public:\tvoid replaceSpace(char *str,int length) { if(str==NULL) return ; int CountOfBlanks=0; int Originallength=0; for(int i=0;str[i]!='\\0';i++) { Originallength++; if(str[i]==' ') ++CountOfBlanks; } int len =Originallength+2*CountOfBlanks; if(len+1length) return ; /* int pOrignallength=orignallength; int pNewlength=newlength; while(pOrignallength=0 pNewlengthpOrignallength) { if(str[pOrignallength]==' ') { str[pNewlength--]='0'; str[pNewlength--]='2'; str[pNewlength--]='%'; } else { str[pNewlength--]=str[pOrignallength]; } pOrignallength--; } */ char*pStr1=str+Originallength;//复制结束符‘\\0’ char*pStr2=str+len; while(pStr1pStr2) { if(*pStr1==' ') { *pStr2--='0'; *pStr2--='2'; *pStr2--='%'; } else { *pStr2--=*pStr1; } --pStr1; } }};","tags":["技术"],"categories":["技术"]},{"title":"从尾到头打印链表","path":"/2019/08/29/从尾到头打印链表/","content":"时间限制：1秒 空间限制：32768K 热度指数：1022710本题知识点： 链表 题目描述输入一个链表，按链表从尾到头的顺序返回一个ArrayList。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 | /** * struct ListNode { * int val; * struct ListNode *next; * ListNode(int x) : * val(x), next(NULL) { * } * }; */ class Solution { public: vectorint printListFromTailToHead(ListNode* head) { vectorint value; ListNode *p=NULL; p=head; stackint stk; while(p!=NULL){ stk.push(p-val); p=p-next; } while(!stk.empty()){ value.push_back(stk.top()); stk.pop(); } return value; } }; —|— 运行时间：3ms占用内存：480K","tags":["编程"],"categories":["算法"]},{"title":"替换空格","path":"/2019/08/29/替换空格/","content":"时间限制：1秒 空间限制：32768K 热度指数：1142927本题知识点： 字符串 题目描述请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。 代码1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 | class Solution { public: void replaceSpace(char *str,int length) { if(str==NULL) return ; int CountOfBlanks=0; int Originallength=0; for(int i=0;str[i]!='\\0';i++) { Originallength++; if(str[i]==' ') ++CountOfBlanks; } int len =Originallength+2*CountOfBlanks; if(len+1length) return ; /* int pOrignallength=orignallength; int pNewlength=newlength; while(pOrignallength=0 pNewlengthpOrignallength) { if(str[pOrignallength]==' ') { str[pNewlength--]='0'; str[pNewlength--]='2'; str[pNewlength--]='%'; } else { str[pNewlength--]=str[pOrignallength]; } pOrignallength--; } */ char*pStr1=str+Originallength;//复制结束符‘\\0’ char*pStr2=str+len; while(pStr1pStr2) { if(*pStr1==' ') { *pStr2--='0'; *pStr2--='2'; *pStr2--='%'; } else { *pStr2--=*pStr1; } --pStr1; } } }; —|—","tags":["编程"],"categories":["其他"]},{"title":"PIL模块","path":"/2019/08/28/20190828-PIL模块/","content":"导入from PIL import Image 读取img = Image.open(filepath) 显示img.show() 与 numpy 数组的互相转换PIL Image 转 numpy 数组 img_to_array = np.array(img) numpy 数组转 PIL Image (注意要确保数组内的值符合 PIL 的要求) array_to_img = Image.fromarray(img_to_array) PIL 与 cv2 格式互相转换PIL.Image读入的图片数据类型不是 numpy 数组, 它的size属性为 (w, h), 利用np.array转换成 numpy 数组后, 它的通道顺序为 (r, g, b) from PIL import Imageimport numpy as np# PIL to cv2pil_img = Image.open(img_path)print(pil_img.size) # (w, h)np_img = np.array(pil_img)cv2_img = np_img[:, :, ::-1] # 交换通道# cv2 to PILpil_img = Image.fromarray(cv2_img[:, :, ::-1])","tags":["计算机视觉"],"categories":["计算机视觉"]},{"title":"Pytorch trick","path":"/2019/08/28/20190828-Pytorch-trick/","content":"目录： 指定GPU编号 查看模型每层输出详情 梯度裁剪 扩展单张图片维度 独热编码 防止验证模型时爆显存 学习率衰减 冻结某些层的参数 对不同层使用不同学习率 1. 指定GPU编号 设置当前使用的GPU设备仅为0号设备，设备名称为 /gpu:0：os.environ[“CUDA_VISIBLE_DEVICES”] = “0” 设置当前使用的GPU设备为0,1号两个设备，名称依次为 /gpu:0、/gpu:1： os.environ[“CUDA_VISIBLE_DEVICES”] = “0,1” ，根据顺序表示优先使用0号设备,然后使用1号设备。指定GPU的命令需要放在和神经网络相关的一系列操作的前面。 2.查看模型每层输出详情Keras有一个简洁的API来查看模型的每一层输出尺寸，这在调试网络时非常有用。现在在PyTorch中也可以实现这个功能。 使用很简单，如下用法： from torchsummary import summarysummary(your_model, input_size=(channels, H, W)) input_size 是根据你自己的网络模型的输入尺寸进行设置。 3.梯度裁剪（Gradient Clipping）import torch.nn as nnoutputs = model(data)loss= loss_fn(outputs, target)optimizer.zero_grad()loss.backward()nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)optimizer.step() nn.utils.clip_grad_norm_ 的参数： parameters – 一个基于变量的迭代器，会进行梯度归一化 max_norm – 梯度的最大范数 max_norm – 梯度的最大范数知乎用户 不椭的椭圆 提出：梯度裁剪在某些任务上会额外消耗大量的计算时间，可移步评论区查看详情。 4、扩展单张图片维度因为在训练时的数据维度一般都是 (batch_size, c, h, w)，而在测试时只输入一张图片，所以需要扩展维度，扩展维度有多个方法： import cv2import torchimage = cv2.imread(img_path)image = torch.tensor(image)print(image.size())img = image.view(1, *image.size())print(img.size())# output:# torch.Size([h, w, c])# torch.Size([1, h, w, c]) 或者 import cv2import numpy as npimage = cv2.imread(img_path)print(image.shape)img = image[np.newaxis, :, :, :]print(img.shape)# output:# (h, w, c)# (1, h, w, c) 或者 import cv2import torchimage = cv2.imread(img_path)image = torch.tensor(image)print(image.size())img = image.unsqueeze(dim=0)print(img.size())img = img.squeeze(dim=0)print(img.size())# output:# torch.Size([(h, w, c)])# torch.Size([1, h, w, c])# torch.Size([h, w, c]) tensor.unsqueeze(dim)：扩展维度，dim指定扩展哪个维度。 tensor.squeeze(dim)：去除dim指定的且size为1的维度，维度大于1时，squeeze()不起作用，不指定dim时，去除所有size为1的维度。 5.独热编码在PyTorch中使用交叉熵损失函数的时候会自动把label转化成onehot，所以不用手动转化，而使用MSE需要手动转化成onehot编码。 import torchclass_num = 8batch_size = 4def one_hot(label): \"\"\" 将一维列表转换为独热编码 \"\"\" label = label.resize_(batch_size, 1) m_zeros = torch.zeros(batch_size, class_num) # 从 value 中取值，然后根据 dim 和 index 给相应位置赋值 onehot = m_zeros.scatter_(1, label, 1) # (dim,index,value) return onehot.numpy() # Tensor - Numpylabel = torch.LongTensor(batch_size).random_() % class_num # 对随机数取余print(one_hot(label))# output:[[0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0.]] 6. 防止验证模型时爆显存验证模型时不需要求导，即不需要梯度计算，关闭autograd，可以提高速度，节约内存。如果不关闭可能会爆显存。 with torch.no_grad(): # 使用model进行预测的代码 pass 感谢知乎用户zhaz 的提醒，我把 torch.cuda.empty_cache() 的使用原因更新一下。 这是原回答： Pytorch 训练时无用的临时变量可能会越来越多，导致 out of memory ，可以使用下面语句来清理这些不需要的变量。 官网 上的解释为： Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.torch.cuda.empty_cache() 意思就是PyTorch的缓存分配器会事先分配一些固定的显存，即使实际上tensors并没有使用完这些显存，这些显存也不能被其他应用使用。这个分配过程由第一次CUDA内存访问触发的。 而 torch.cuda.empty_cache() 的作用就是释放缓存分配器当前持有的且未占用的缓存显存，以便这些显存可以被其他GPU应用程序中使用，并且通过 nvidia-smi命令可见。注意使用此命令不会释放tensors占用的显存。 对于不用的数据变量，Pytorch 可以自动进行回收从而释放相应的显存。 7. 学习率衰减import torch.optim as optimfrom torch.optim import lr_scheduler# 训练前的初始化optimizer = optim.Adam(net.parameters(), lr=0.001)scheduler = lr_scheduler.StepLR(optimizer, 10, 0.1) # # 每过10个epoch，学习率乘以0.1# 训练过程中for n in n_epoch: scheduler.step() ... 8. 冻结某些层的参数在加载预训练模型的时候，我们有时想冻结前面几层，使其参数在训练过程中不发生变化。 我们需要先知道每一层的名字，通过如下代码打印： net = Network() # 获取自定义网络结构for name, value in net.named_parameters(): print('name: {0}, grad: {1}'.format(name, value.requires_grad)) 假设前几层信息如下： name: cnn.VGG_16.convolution1_1.weight, grad: Truename: cnn.VGG_16.convolution1_1.bias, grad: Truename: cnn.VGG_16.convolution1_2.weight, grad: Truename: cnn.VGG_16.convolution1_2.bias, grad: Truename: cnn.VGG_16.convolution2_1.weight, grad: Truename: cnn.VGG_16.convolution2_1.bias, grad: Truename: cnn.VGG_16.convolution2_2.weight, grad: Truename: cnn.VGG_16.convolution2_2.bias, grad: True 后面的True表示该层的参数可训练，然后我们定义一个要冻结的层的列表： no_grad = [ 'cnn.VGG_16.convolution1_1.weight', 'cnn.VGG_16.convolution1_1.bias', 'cnn.VGG_16.convolution1_2.weight', 'cnn.VGG_16.convolution1_2.bias'] 冻结方法如下： net = Net.CTPN() # 获取网络结构for name, value in net.named_parameters(): if name in no_grad: value.requires_grad = False else: value.requires_grad = True 冻结后我们再打印每层的信息： name: cnn.VGG_16.convolution1_1.weight, grad: Falsename: cnn.VGG_16.convolution1_1.bias, grad: Falsename: cnn.VGG_16.convolution1_2.weight, grad: Falsename: cnn.VGG_16.convolution1_2.bias, grad: Falsename: cnn.VGG_16.convolution2_1.weight, grad: Truename: cnn.VGG_16.convolution2_1.bias, grad: Truename: cnn.VGG_16.convolution2_2.weight, grad: Truename: cnn.VGG_16.convolution2_2.bias, grad: True 可以看到前两层的weight和bias的requires_grad都为False，表示它们不可训练。最后在定义优化器时，只对requires_grad为True的层的参数进行更新。 optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.01) 9. 对不同层使用不同学习率我们对模型的不同层使用不同的学习率。 还是使用这个模型作为例子： net = Network() # 获取自定义网络结构for name, value in net.named_parameters(): print('name: {}'.format(name))# 输出：# name: cnn.VGG_16.convolution1_1.weight# name: cnn.VGG_16.convolution1_1.bias# name: cnn.VGG_16.convolution1_2.weight# name: cnn.VGG_16.convolution1_2.bias# name: cnn.VGG_16.convolution2_1.weight# name: cnn.VGG_16.convolution2_1.bias# name: cnn.VGG_16.convolution2_2.weight# name: cnn.VGG_16.convolution2_2.bias 对 convolution1 和 convolution2 设置不同的学习率，首先将它们分开，即放到不同的列表里： conv1_params = []conv2_params = []for name, parms in net.named_parameters(): if \"convolution1\" in name: conv1_params += [parms] else: conv2_params += [parms]# 然后在优化器中进行如下操作：optimizer = optim.Adam( [ {\"params\": conv1_params, 'lr': 0.01}, {\"params\": conv2_params, 'lr': 0.001}, ], weight_decay=1e-3,) 我们将模型划分为两部分，存放到一个列表里，每部分就对应上面的一个字典，在字典里设置不同的学习率。当这两部分有相同的其他参数时，就将该参数放到列表外面作为全局参数，如上面的weight_decay。 也可以在列表外设置一个全局学习率，当各部分字典里设置了局部学习率时，就使用该学习率，否则就使用列表外的全局学习率。 显示训练时间for epoch in range(start_epoch, config.epochs): start = time.time() train_loss, lr = train_epoch(model, optimizer, scheduler, train_loader, device, criterion, epoch, all_step, writer, logger) logger.info('[{}/{}], train_loss: {:.4f}, time: {:.4f}, lr: {}'.format( epoch, config.epochs, train_loss, time.time() - start, lr)) 参考：https://mp.weixin.qq.com/s?__biz=MzU3NjE4NjQ4MA==mid=2247485953idx=2sn=3ae788b7d643541254ba311f7a7facedchksm=fd16fb1eca61720870bc58c1a465a346cf2c6a7e8bea39e4b3d582474b595021f3a5b635086dmpshare=1scene=1srcid=sharer_sharetime=1566885137387sharer_shareid=285785c5623899db73795495779fe8be#rd","tags":["深度学习"],"categories":["深度学习"]},{"title":"广度/宽度优先搜索(BFS)","path":"/2019/08/28/20190828-BFS/","content":"前言广度优先搜索 是最简单的图搜索算法之一， 也是许多重要的图算法的原型。Prime的最小生成树算法和Dijkstra的单源最短路径算法都使用了类似广度优先搜索的思想。给定图G=(V,E) 和一个可以识别的源 节点 s，广度优先搜索对图G中的边进行系统性的探索来发现可以从源节点，到达所有的节点。该算法能够计算从源结点s到每个可到达的节点的距离(最小的边数)，同时生成一棵“广度优先搜索树”。该树以源结点s为根节点，包括所有可以从s到达的结点。对于每个从源结点s可以到达的结点v，在广度优先搜索树里从结点s到结点v的简单路径所对应的的就是图G中从结点s到结点v的“最短路径”，即包含最少边数的路径，该算法既可以用于有向图也可以用于无向图。广度优先算法之所以如此得名是因为该算法始终是将已经发现的结点和未发现结点之间的边界，沿其广度方向向外扩展。也就是说，算法需要在发现所有距离源结点s为k的所有结点之后，才会发现距离源结点s为k+1的 其他结点。 图的概念 图(graph) 是一种$\\textcolor{Blue}{网状数据} $结构， 图是由非空的顶点集合和一个描述顶点之间的关系的集合组成。 图由顶点和边组成，顶点表示对象，边表示对象之间的连接关系。 边也可以带权值，称为带权值图。 无向图术语 两个顶点之间如果有边连接，视为两个顶点相邻 相邻顶点间的序列称为路径 起点和终点重合的路径称为圈 顶点连接的边数叫做这个顶点的度 没有圈的连通图，就是树 没有圈的非连通图，就是森林 一棵树的边数等于顶点数-1 边数等于顶点数-1 的连通图，就是树 BFS(G,s) \\\\for each vertex u \\in G.V -{s}\\\\\tu.color = WHITE\\\\\tu.d = \\infty\\\\\tu.\\pi = NIL\\\\s.color = GRAY\\\\s.d = 0\\\\s.\\pi = NIL\\\\Q = \\emptyset\\\\ENQUEUE(Q,s)\\\\while Q eq = \\emptyset\\\\\tu = DEQUEUE(Q)\\\\\tfor each v \\in G.Adj[u]\\\\ if v.color == WHITE\\\\ v.color = GRAY\\\\ v.d = u.d+1\\\\ v.\\pi = u\\\\ ENQUEUE(Q,v)\\\\\tu.color = BLACK\\\\ 广度优先搜索的流程图 实例POJ3984《迷宫问题》定义一个二维数组：int maze[5][5] = {0, 1, 0, 0, 0,0, 1, 0, 1, 0,0, 0, 0, 0, 0,0, 1, 1, 1, 0,0, 0, 0, 1, 0,};它表示一个迷宫，其中的1表示墙壁，0表示可以走的路，只能横着走或竖着走，不能斜着走，要求编程序找出从左上角到右下角的最短路线。 解题思路队列是先进后出，后进先出。 对应于题目的输入数组：0, 1, 0, 0, 0,0, 1, 0, 1, 0,0, 0, 0, 0, 0,0, 1, 1, 1, 0,0, 0, 0, 1, 0,把节点定义为(x,y)，(x,y)表示数组maze的项maze[x][y]。于是起点就是(0,0)，终点是(4,4)。按照刚刚的思路，手工梳理一遍：初始条件：起点Vs为(0,0)终点Vd为(4,4)灰色节点集合Q={}初始化所有节点为白色节点下面开始广度优先搜索：1.起始节点Vs变成灰色，加入队列Q，Q={(0,0)}2.取出队列Q的头一个节点Vn，Vn={0,0}，Q={}3.把Vn={0,0}染成黑色，取出Vn所有相邻的白色节点{(1,0)}4.不包含终点(4,4)，染成灰色，加入队列Q，Q={(1,0)}5.取出队列Q的头一个节点Vn，Vn={1,0}，Q={}6.把Vn={1,0}染成黑色，取出Vn所有相邻的白色节点{(2,0)}7.不包含终点(4,4)，染成灰色，加入队列Q，Q={(2,0)}8.取出队列Q的头一个节点Vn，Vn={2,0}，Q={}9.把Vn={2,0}染成黑色，取出Vn所有相邻的白色节点{(2,1), (3,0)}10.不包含终点(4,4)，染成灰色，加入队列Q，Q={(2,1), (3,0)}11.取出队列Q的头一个节点Vn，Vn={2,1}，Q={(3,0)}12.把Vn={2,1}染成黑色，取出Vn所有相邻的白色节点{(2,2)}13.不包含终点(4,4)，染成灰色，加入队列Q，Q={(3,0), (2,2)}14.持续下去，知道Vn的所有相邻的白色节点中包含了(4,4)……15.此时获得了答案 // BFS.cpp : 定义控制台应用程序的入口点。//#include \"stdafx.h\"#include iostreamusing namespace std;int map[5][5];//相邻四个节点int borderUponX[4] = { 0, 0, 1, -1 };int borderUponY[4] = { 1, -1, 0, 0 };int front = 0, rear = 1;struct node {\tint pre;\tint x;\tint y;} path[100];void print(int i) {//当前节点\tif (path[i].pre != -1) {//找到前面那个节点 print(path[i].pre); cout \"(\" path[i].x \",\" path[i].y \")\" endl;\t}\telse {//最前面的那个节点 cout \"(\" path[i].x \",\" path[i].y \")\" endl;\t}}void bfsSearch(int x, int y) {\t//开始节点（出发），前面没有节点了\tpath[front].x = x;\tpath[front].y = y;\tpath[front].pre = -1;\t//当front == rear的时候说明已经走完了所以“相邻”节点\t//且都不通\twhile (front rear) { for (int i = 0; i != 4; i++) { //相邻节点坐标 int pathX = path[front].x + borderUponX[i]; int pathY = path[front].y + borderUponY[i]; //不符合的节点（遇到边界或已经走过了） if (pathY 0 || pathX 0 || pathX 4 || pathY 4 || map[pathX][pathY]) continue; else {//将front的相邻的可以过去的并且是还没有走过的节点加到路径里面 map[pathX][pathY] = 1; path[rear].x = pathX; path[rear].y = pathY; path[rear].pre = front; rear++; } if (pathX == 4 pathY == 4) { //找到了一条路径，又是第一次找到 //那么就是最短路径了 print(rear - 1); break; } } front++;\t}}int main(int argc, char const *argv[]){\tfor (int i = 0; i 5; i++) for (int j = 0; j 5; j++) cin map[i][j];\tbfsSearch(0, 0);\tsystem(\"pause\");\treturn 0;}","tags":["算法"],"categories":["算法"]},{"title":"glob模块","path":"/2019/08/28/20190828-glob模块/","content":"glob模块是Python最简单的模块之一, 内容非常少, 用它可以查找符合特定规则的文件路径名, 查找文件时只会用到三个匹配符: : 匹配0个或多个字符 ? : 匹配单个字符 [] : 匹配指定范围内的字符, 如[0-9]匹配数字 glob.glob()参数:_(str): 文件路径的正则表达式 返回值:_(list): 符合正则表达式的文件路径列表 备注:返回所有匹配的文件路径列表, 它只有一个参数pathname, 定义了文件路径匹配的规则, 这里可以是绝对路径或者相对路径: import globpathes_list = glob.glob(\"~/Pictures/*.jpg\")# 获取Pictures下的所有图片relative_pathes_list = glob.glob(\"../*.py\")# 获取上级目录中的所有.py文件 在 linux, osx 系统中, 通配符的匹配是大小写区分的, 也就是需要特别指定大小写: extensions = ['jpg', 'JPG', 'jpeg', 'JPEG'] 但是在 windows 当中, 通配符的匹配是不区分大小写的, 因此只需要指定大小写中的一个即可, 两个都指定的话, 会出现重复的情况 extensions = ['jpg', 'jpeg'] glob.iglob获取一个可遍历的对象, 使用它可以逐个获取匹配的文件路径名. 与glob.glob()的区别是: glob.glob()会同时获取到所有的匹配路径, 而glob.iglob()一次只获取一个匹配路径. f = glob.iglob(\"../*.py\")print f # generator object iglob at 0x00B9FF80for py in f: print(py)","tags":["Python"],"categories":["Python"]},{"title":"logging模块","path":"/2019/08/28/20190828-logging模块/","content":"简单使用import logginglogging.debug(\"debug msg\")logging.info(\"info msg\")logging.warn(\"warn msg\")logging.error(\"error msg\")logging.critical(\"critical msg\") 默认情况下, logging模块将日志打印到屏幕上, 只有日志级别高于WARNING的日志信息才回输出","tags":["Python"],"categories":["Python"]},{"title":"Pytorch手册","path":"/2019/08/28/20190828-Pytorch手册/","content":"PyTorch 主要提供以下两大特色： 支持强力GPU加速的Tensor计算能力 基于tape的具有自动微分求导能力的深度神经网络框架 PyTorch 主要包含以下组成要素: 组成要素 描述说明 torch 一个类似于numpy的tensor哭, 提供强力的GPU支持 torch.autograd 一个基于tape的具有自动微分求导能力的库, 可以支持几乎所有的tesnor operatioin torch.nn 一个神经网络库, 与autograd深度整合, 可以提供最大限度的灵活性 torch.multiprocessing Python的多线程处理, 可以提供torch Tensors之间的内存共享, 对于加载数据和Hogwild training来说十分有用 torch.utils 一些功能类和函数, 如DataLoader, Trainer等等 torch.legacy(.nn/.optim) 为了兼容性而存在的一些代码和实现 Pytorch通常可以作为以下用途使用: 为了使用GPUs性能的numpy替代品 可以提供强大灵活力和速度优势的深度学习平台. torchbackends.cudnntorch.backends.cudnn.benchmark = True 上述设置可以让内置的cudnn的auto-tuner自动寻找最合适当前配置的搞笑算法, 来达到优化运行效率的目标, 在使用时, 应该遵循以下两个准则: 如果网络的输入数据维度或类型上变化不大, 则该设置可以增加运行效率 如果网络的输入数据在每次的iteration中都变化的话, 会导致cudnn每次都寻找一遍最优配置, 这样反而 会降低 运行效率. torch.cat()torch.cat(seq, dim=0, out=None) # 返回连接后的tensor 将给定的 tensor 序列 seq 按照维度连接起来. 默认维度为0, 说明会将其在第 0 个维度上进行拼接.(最后的结果是第 0 维度增大, 例如三个2行3列的 tensor 按照第0维度拼接, 最后得到的 tensor 维度为6行3列) clamp()/clamp_()torch.clamp(input, min, max, out=None) - Tensor 将input里面元素全部划分到[min,max]区间内, 小于min的置为min, 大于max的置为max. 如果不指定min或者max,则认为无下界或上界其他调用形式: torch.Tensor(min, max) # 调用tensor为input, 返回值为out device()device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") gather()torch.gather(input, dim, index, out=None) - Tensor 沿着dim指定的轴按着index指定的值重新组合成一个新的tensor. out[i][j][k] = input[index[i][j][k]][j][k] # if dim == 0out[i][j][k] = input[i][index[i][j][k]][k] # if dim == 1out[i][j][k] = input[i][j][index[i][j][k]] # if dim == 2 即假设input是一个 n 维的tensor, 其 size 为 (x0,x1,…,xi−1,xi,xi+1,…,xn−1), 若dim=i, 则 index 必须也是一个 n 维的tensor, 其 size 为 (x0,x1,…,xi−1,y,xi+1,…,xn−1), 其中 y≥1, 而返回的 tensor out 的 size 和 index 的 size 相同.一句来说 gather 的作用就是, 在指定的维度上筛选给给定下标index指示的值, 其他值舍弃.一个例子说明:scores是一个计算出来的分数，类型为[torch.FloatTensor of size 5x1000]而y_var是正确分数的索引，类型为[torch.LongTensor of size 5]容易知道，这里有1000个类别，有5个输入图像，每个图像得出的分数中只有一个是正确的，正确的索引就在y_var中，这里要做的是将正确分数根据索引标号提取出来。 scores = model(X_var) # 分数scores = scores.gather(1, y_var.view(-1, 1)).squeeze() #进行提取 提取后的scores格式也为[torch.FloatTensor of size 5]这里讲一下变化过程： 首先要知道之前的scores的size为[5,1000]，而y_var的size为[5]，scores为2维，y_var为1维不匹配，所以先用view将其展开为[5,1]的size，这样维数n就与scroes匹配了。 接下来进行gather，gather函数中第一个参数为1，意思是在第二维进行汇聚，也就是说通过y_var中的五个值来在scroes中第二维的5个1000中进行一一挑选，挑选出来后的size也为[5,1]，然后再通过squeeze将那个一维去掉，最后结果为[5].Tensor形式 torch.Tensor.gather(dim, index) - Tensor torch.ge()torch.gt()torch.gt(input, other, out=None) # - Tensor 根据 input 和 other 的值返回一个二值 tensor, 如果满足大于条件则为1, 不满足则为0.other 可以是能够转换成 input size 的tensor, 也可以是一个 float 标量. torch.index_select()torch.index_select(input, dim, index, out=None) # - Tensor 返回在 dim 维度上的 index 指明的下标组成的 tensor.返回的 tensor 的维度的数量和 input 是相同的, 但是第 dim 维度的 size 会和 index size大小相同. 其他维度的 size 保持不变. torch.le()torch.le(input, other, out=None) # -Tensor 按元素计算 input≤other. max()torch.max(input) # 返回一个Tensor, 代表所有元素中的最大值torch.max(input,dim,keepdim=False,out=None) # 返回一个元组:(Tensor, LongTensor) 第二种形式会返回一个元组, 元组内元素类型为: (Tensor, LongTensor), 其中, 前者代表对应 dim 上 reduce 后的最大值, 后者代表最大值在维度 dim 中对应的下标.如果keepdim=True, 则输出的 tensor 的 size 会和输入的相同, 只不过对应 dim 维度上的size为1. 否则, 对应 dim 维度会被 squeeze/reduce, 使得输出的维度比输入的维度少1. a = torch.randn(4, 4) atensor([[-1.2360, -0.2942, -0.1222, 0.8475], [ 1.1949, -1.1127, -2.2379, -0.6702], [ 1.5717, -0.9207, 0.1297, -1.8768], [-0.6172, 1.0036, -0.6060, -0.2432]]) torch.max(a, 1)(tensor([ 0.8475, 1.1949, 1.5717, 1.0036]), tensor([ 3, 0, 0, 1])) mm()注意, 没有torch.mm_版本 torch.mm(mat1, mat2, out=None) # 返回值为Tensor, 也可以使用out记录返回值 两矩阵相乘, 矩阵的size需要满足乘法规则其他调用形式: torch.Tensor(mat2) # 调用者为mat1 norm()返回输入tensor的p-norm标量 torch.norm(input, p=2) # 返回一个标量tensor numel()torch.numel(input) #返回一个int值 返回 inpput tensor 中的元素的总个数 a = torch.randn(1,2,3,4,5)print(torch.numel(a)) # 120 ones()randn()标准正太分布随机基础, 传入参数为维度信息 torch.sort()torch.sort(input, dim=None, descending=False, out=None) # 返回 (Tensor, LongTensor) sum()torch.sum(input, dtype=None) # 返回求和后的Tensor(只有一个元素)torch.sum(input, dim, keepdim=False, dtype=None) # 返回在dim上reduce的sum和, 如果dim包含多个维度, 则都进行reduce求和.# reduce这个词很形象, 因为返回的Tensor的维度刚好没有了dim指示的那些维度 其他形式: torch.Tensor.sum() torch.t()torch.t(input) # 返回转置后的Tensor 其他形式: torch.Tensor.t() unsqueeze()在指定维度上插入一个 singleton 维度(一般用于将单一数据处理用 batch 的形式) torch.unsqueeze(input, dim, out=None) # - Tensor 返回的tensor与input tensor 共享数据 dim 的取值范围在 [-input.dim()-1, input.dim()+1] 之间, 如果为负值, 则相当于 dim = dim + input.dim() + 1. zeros()torch.cudatorch.cuda.empty_cache()释放所有未使用的 GPU 内存, 使用这些内存可以被其他 GPU 应用使用, 并且可以被 nvidia-smi 查到.empty_cache() 并不会强制提升供 PyTorch 使用的显卡内存的大小, 查看Memory management torch.Tensortorch.Tensor 是默认类型 torch.FloatTensor 的别名, 使用 torch.Tenosr 的构造函数创建 tensor 变量时, 传入的是维度信息(注意与 torch.tensor() 的区别): t = torch.Tensor(2,3,4) # 里面的数值未初始化, 是随机的print(t.size()) # torch.Size([2,3,4]) torch.LongTesnor 使用方法相似, 只不过数据类型是长整型. troch.tensor()创建tensor torch.tensor(data, dtype=None, device=None, requires_grad=False) 可以利用torch.tensor从python的list数据或者其他序列数据中创建tensor对象 torch.tensor([[1,-1],[1,-1]])torch.tensor(np.array([[1,2,3],[4,5,6]])) 注意, torch.tensor()函数总是会对数据进行复制操作, 因此, 如果你仅仅是想将数据的requires_grad标志改变, 那么就应该使用required_grad_()或者detach()函数来避免复制. 同时, 对numpy数组使用torch.as_tensor()将其转换成tensor而无需复制 torch.Tensor.cpu()torch.Tensor.cpu()z = x.cpu() 将tensor移动到cpu上, 注意返回值z是cpu上的数据, tensor x 本身的device属性不变 torch.Tensor.cuda()torch.Tensor.cuda()z = x.cuda() torch.Tensor.dim()torch.Tensor.dim() - int 返回 tensor 的维度的个数. torch.Tensor.max()torch.Tensor.max(dim=None, keepdim=False) - Tensor or (Tensor, Tensor) 详情见 torch.max() torch.Tensor.numel()torch.Tensor.numel() 详见 torch.numel() torch.Tensor.to()torch.Tensor.to(*args, *kwargs) 返回一个转移后的tensor, 而自身维持不变 t = torch.randn(2,3)t.to(torch.float64)t.to(device)t.to(\"cuda:0\") 将tensor移动到gpu上, 注意返回值 z 是gpu 上的数据, tensor x 本身的 device 属性不变 torch.Tensor.numpy()tensor与numpy数组的转换 torch.Tensor.numpy() # 返回tensor对应的numpy数组torch.from_numpy(ndarray) # 将numpy数组ndarray转换成对应的tensor并返回. torch.Tensor 实际上是 torch.FloatFensor 的别名 torch.Tensor.permute()重新排列tensor的维度 torch.Tensor.permute(*dims) # 返回一个重新排列维度后的 tensor torch.Tensor.unsqueeze()详细可见torch.unsqueeze torch.Tensor.expand()torch.Tensor.expand(*sizes) # 返回 tensor 将 tensor 中的 singleton 维度扩展到一个更大的 size.参数 -1 意味着不改变原始的维度新增的维度的元素被被添加到前头, size不能设置为-1.expand 并没有申请新的内存, 而仅仅是在当前已经存在的 tensor 上面创建了新的视图(view), 使得 singleton 维度被扩展成了一个更大的尺寸.Any dimension of size 1 can be expanded to an arbitrary value without new memory. x = torch.tensor([1],[2],[3])print(x.size()) # torch.Size([3,1])print(x.expand(3,4)) # torch.Size([3,4]) # 将维度为1的扩展到任意尺寸print(x.expand(-1,4)) # torch.Size([3,4]) # -1 代表不改变维度 注意, 只能对 singleton 的维度进行扩展, 如果强行对其他维度扩展, 则会报错. torch.Tensor.expand_as()torch.Tensor.expand_as(other) # 返回 tensor 将当前 tensor 扩展到和 other 一样的size.self.expand_as(other) 与 self.expand(other.size()) 等价. torch.Tensor.index_fill_()torch.Tensor.index_fill_(dim, index, val) # 返回tensor 在给定的维度 dim 上, 用 val 将该维度上的 index 坐标的值填充. x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)index = torch.tensor([0, 2])x.index_fill_(1, index, -1)print(x)#tensor([[-1., 2., -1.],# [-1., 5., -1.],# [-1., 8., -1.]]) torch.Tensor.contiguous()返回一个连续的tensor, 数据内容不变 torch.Tensor.contiguous() # 如果tensor本身就是连续的, 那么就会返回tensor本身 这里的 contiguous 指的是内存上的连续, 由于在 PyTorch 中, view 只能用在 contiguous 的 tensor 上面, 而如果在 view 之前使用了 transpose, permute 等操作后, 就需要使用 contiguous 来返回一个 contiguous tensor.在 PyTorch 0.4 版本以后, 增加了 torch.reshape(), 这与 numpy.reshape() 的功能类似, 它大致相当于 tensor.contiguous().view() ? torch.Tensor.item()当Tensor中只包含一个元素时, 可以利用该函数返回这个元素的标量 torch.Tensor.tolist()可以将Tensor转换成列表 torch.Tensor.zero_()torch.Tensor.zero_() 将当前的 tensor 变量全部置为0(原地) torch.autogradset_grad_enabled()class torch.autograd.set_grad_enabled(mode) 用来控制梯度计算的开关(依据bool类型参数mode决定), 可以当做上下文管理器使用, 也可以当做函数使用 # 当做上下文管理器with torch.set_grad_enabled(is_train): # 注意, 这里省略了autograd loss.backward() optimizer.step()# 当做函数使用w1 = torch.Tensor([1], requires=True)torch.set_grad_enabled(True)print(w1.requires_grad) # Truetorch.set_grad_enabled(False)print(w1.requires_grad) # False no_grad()class torch.autograd.no_grad 用于禁用梯度计算的上下文管理器.在测试阶段, 当你确信你不会调用Tensor.backward()时,禁用梯度计算十分有用. 这会降低计算使用内存消耗. x = torch.tensor([1.0], requires_grad=True)with torch.no_grad(): # 省略了autograd print(x.requires_grad) # True, 虽然为True, 但在该上下文中, 会无视掉requires_grad参数, 一律做False处理 y = x*2 print(y.requires_grad) # False, 在当前上下文产生的tensor的requires_grad属性为Falseprint(x.requires_grad) # True torch.autograd.Functionclass torch.autograd.Function 为可微分的 ops 记录 operation history, 同时定义计算公式. 每一个作用在 tensor 上的 operatin 都会创建一个新的 function 对象, 它会执行计算过程并记录相关信息. 这些信息可以从一个由 functions 组成的有向图中获得. 当 backward() 方法被调用时, 就会利用这些信息在 function 上进行反向传播, 并将梯度传给下一个 Funtion.通常情况下, 当用于需要自定义可自动求导的 ops 时, 可以实现一个 Function 的子类. # Exampleclass Exp(Function): @staticmethod def forward(ctx, i): result = i.exp() ctx.save_for_backward(result) @staticmethod def backward(ctx, grad_output): result, = ctx.saved_tensors return grad_output*result static forward(ctx, args, kwargs):*定义前向计算的逻辑. static backward(ctx, *grad_outputs):定义反向传导的逻辑, 如果确定不会使用到反向传播, 则可以不实现该函数. torch.nnModuleclass torch.nn.Module 所有神经网络Module的基类, 自定义的模型也应该是它的子类.Modules可以包含其他Module(如Linear, Conv2d等等). parameters()for param in model.parameters(): print(param.data, param.size()) state_dict: torch.nn.Module.state_dict(destination=None,prefix=\"\",keep_vars=False) 以字典形式返回整个module的状态 traintorch.nn.Module.train(mode=True) 将module的模式设置为train, 这只对部分module有效, 如Dropout, BatchNorm等, 详细请查看官网.返回值: torch.nn.Module trainingtorch.nn.Module.training # 属性, 返回一个bool值, 指示当前的模式是否为train evaltorch.nn.Module.eval() # 注意, 和train不同, eval为无参函数 将module的mode设置为evaluation, 同样, 只对部分module起效. Lineartorch.nn.Linear(in_features, out_features, bias=True) 全连接层的实现. 输入的shape为 (N,…,infeatures), 输出的shape为 (N,…,outfeatures), 可以看出, 除了最后一维不同外, 其他维度都相同. (通常在使用Linear之前, 会将输入变成二维的矩阵, 其中第一维为batch size, 第二维为特征向量).in_features 和 out_features 可以当做属性用.来获取. Conv2dclass torch.nn.Conv2的(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) in_channels(int): out_channels(int): kernel_size(intortuple): stride(intortuple, optional): MaxPool2dSoftmax()```pythonclass torch.nn.Softmax(dim=None) dim指明了需要进行 softmax 的维度, 在这个维度上的值, 加起来和为1. ReLUtorch.nn.ReLU(inplace=False) 输入输出的shape是相同的, 执行relu函数 torch.nn.Sequentialclass torch.nn.Sequential(*args) torch.nn.MSELossclass torch.nn.MSELoss(size_average=None, reduce=None, reduction=\"elementwise_mean\") size_average(bool, optional): 弃用(见reduction参数). 默认情况下, loss会计算在每个样本上的平均误差. 如果将size_average置为False, 则计算平方误差总和. 当reduce参数为False时, 忽视该参数 reduce(bool, optional): 弃用(见reduction参数). reduce参数顾名思义, 就是是否让MSELoss函数返回值的维度减少, 默认为True, 即会将任意维度的输入计算loss后, 返回一个标量(平均or总和取决于size_average), 如果为False, 则说明返回值维度不应该发生变化, 故而返回值就是对每个元素单独进行平方损失计算.12345678910y = torch.tensor([1,2,3,4], dtype=torch.float)pred_y = torch.tensor([1,1,1,1], dtype=torch.float)loss_fn1 = torch.nn.MSELoss()loss1 = loss_fn1(y, pred_y)loss_fn2 = torch.nn.MSELoss(size_average=False)loss2 = loss_fn2(y, pred_y)loss_fn3 = torch.nn.MSELoss(reduce=False)loss3 = loss_fn3(y, pred_y)print(loss1,loss2,loss3)# tensor(3.5000) tensor(14.) tensor([0., 1., 4., 9.]) 3. reduction(string, optional): 用字符串来替代上面两个参数的作用: “elementwise_mean”(默认) | “sum” | “none” (不进行reduce).### torch.nn.functional#### conv1d()#### conv2d()#### relu() ```python torch.nn.functional.relu(input, inplace=True) # 返回 一个 Tenosr #### relu_()```python torch.nn.functional.relu_(input) # relu() 的原地版本 torch.optimlr_schedulerStepLRclass torch.optim.lr_schedulr.StepLR(optimizer,step_size,gamma=0.1,last_epoch=-1) 每经过step_size次epoch之后, lr就会衰减gamma倍(new_lr=lr×gamma), 初始的lr来自于optimizer中的lr参数. # Observe that all parameters are being optimizedoptimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)# Decay LR by a factor of 0.1 every 7 epochsexp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) ExponentialLRclass torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma,last_epoch=-1) CosineAnnealingLR## Adamclass torch.optim.Adam(params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False) conv2dtorch.utils.dataDataLoaderclass torch.utils.data.DataLoader(dataset,batch_size=1,shuffle=False,sampler=None,batch_sampler=None,num_workers=0,collate_fn=function default_collate,pin_memory=False,drop_last=False,timeout=0,worker_init_fn=None) 数据加载器, 将数据集和采样器结合起来, 并且提供单/多线程的迭代器. dataset(utils.data.Dataset): batch_size(int,optional): batch中的样本个数 shuffle(bool,optional) num_worker(int,optional): 加载数据的线程个数, 0意味着只有一个主线程.方法： iter(self): 可以当做迭代器使用, 如inputs,class_ids=next(iter(dataloaders)), 其中, input的shape为 (N,C,H,W), class_ids的shape为 (N). len(self): 返回数据集的类别数目 torchvisiontorchvision.utilsmake_gridtorchvision.utils.make_grid(tensor,nrow=8,padding=2,normalize=False,range=None,scale_each=False,pad_value=0) 制作一个关于image的grid, 返回值依然是一个tensor, 只不过尺度变成了3D, 相当于把多个图片拼接在一起了, 直接通过plt.imshow(grid)即可输出网格化以后的图片. tensor(Tensor/list): 4D的 mini-batch Tensor, Shape为 (N×C×H×W), 或者是同维度的list. torchvision.transformstorchvision.transforms.Compose 1 2 3 4 5 6 7 class torchvision.transforms.Compose(transforms)# 使用trans.Compose([transforms.CenterCrop(10),transforms.ToTensor(),]) 将多个transforms操作组合起来, 注意参数是列表形式 Transforms on PIL Image# cv2 image to PIL Image# skimage to PIL Image 注意, 以下操作作用在PIL Image上的 CenterCropclass torchvision.transform.CenterCrop(size) size参数表示输出的图谱的大小, 如果只传入了一个数字, 则该数字既表示高度, 又表示宽度. Resizeclass torchvision.transforms.Resize(size, interpolation=2) size: 期望的输出size. interpolation: 插值方法, 默认为双线性插值 ToTensor 1 class torchvision.transforms.ToTensor 将一个PIL Image或者numpy.ndarray (H×W×C,[0, 255])转换成torch.FloatTensor (C×H×W, [0.0, 1.0]). RandomHorizontalFliptransforms.RandomHorizontalFlip(p=0.5) 在给定概率下对PIL Image随机执行水平翻转操作 RandomResizedCroptorch.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333), interpolation=2) 对PIL Image随机执行剪裁操作(按照scale和ratio的区间剪裁), 然后将剪裁后的图片放缩都期望的尺寸(默认插值为双线性插值) size: 期望得到的尺寸 scale: 剪裁的面积比例(相对于原始图) ratio: 剪裁的宽高比 interpolation: 默认为:PIL.Image.BILINEAR Transforms on torch.*Tensor注意, 以下操作是作用在tensor上的 Normalize 1 class torchvision.transforms.Normalize(mean, std) 将图片tensor按照均值mean和标准差std进行归一化, 对于n个channels, 有 mean=(M1, …, Mn), std=(S1,…,Sn).注意, 这个归一化操作是原地进行的 torchvision.datasetsImageFolderclass torchvision.datasets.ImageFolder(root, transform=None, target_transform=None, loader=function default_loader) 一个一般化的数据加载器, 主要针对如下数据排列格式: root/dog/x.pngroot/dog/y.pngroot/dog/z.png...root/cat/123.pngroot/cat/nsdf3.pngroot/cat/asd932_.png root: 根目录路径 transform(callable,optional): 对图片要做的变换操作 target_transform(callable,optional): 对target要做的变换操作 loader: 用于加载给定路径图片的函数属性： classes(list): 返回类别的名字列表 class_names class_to_idx(dict): 以字典的形式返回(class_name, class_index) imgs(list): 返回元组列表: (image path, class_index)方法： getitem(index): 根据index返回(sample,target)元组. 可以使用 len(imagefolder) 返回类别数量 sort()sort(dim=None, descending=False) # 默认为升序, 返回(Tensor, LongTensor) 详见 torch.sort() torch.distributedtorch.distributed.reduce()inspect 模块inspect.signature() # 查看函数签名, python3.6以上inspect.getargspec() # 查看函数签名, python3.6以上inspect.getsource() # 获取模型的codeinspect.getabsfile() # 获取模块的路径 un normalizemean = torch.tensor([1, 2, 3], dtype=torch.float32)std = torch.tensor([2, 2, 2], dtype=torch.float32)normalize = T.Normalize(mean.tolist(), std.tolist())unnormalize = T.Normalize((-mean / std).tolist(), (1.0 / std).tolist())​~~~s","tags":["深度学习"],"categories":["深度学习"]},{"title":"numpy实现神经网络","path":"/2019/08/28/20190828-numpy实现神经网络/","content":"import numpy as np# N 为batch size, D_in 为输入维度# H 为隐藏层的维度, D_out 为输出的维度N, D_in, H, D_out = 64, 1000, 100, 10# 创建随机的输入和输出数据x = np.random.randn(N, D_in) # N × D_in 的矩阵y = np.random.randn(N, D_out) # N × D_out 的矩阵# 对两个隐藏层w1,w2进行初始化w1 = np.random.randn(D_in, H)w2 = np.random.randn(H, D_out)# 设置学习率learning_rate = 1e-6for t in range(500): # 前向传播: 计算预测结果 y_pred h = x.dot(w1) # x维度为64 × 1000, w1维度为 1000 × 100, 计算完以后, h维度为 64 × 100 h_relu = np.maximum(h,0) y_pred = h_relu.dot(w2) # h_relu维度为 64×100, w2维度为100×10, y的维度为64×10 # 计算损失 loss = np.square(y_pred - y).sum() print(t, loss) # 反向传播根据loss更新w1和w2的值 grad_y_pred = 2.0*(y_pred - y) # 对y_pred求导 grad_w2 = h_relu.T.dot(grad_y_pred) # 对w2求导, 微分矩阵应该与w2的size相同 grad_h_relu = grad_y_pred.dot(w2.T) # 对h_relu求导 grad_h = grad_h_relu.copy() grad_h[h 0] = 0 # 经过relu, 将小于0的梯度归0 grad_w1 = x.T.dot(grad_h) # Update weights w1 = w1 - learning_rate * grad_w1 w2 = w2 - learning_rate * grad_w2","tags":["深度学习"],"categories":["深度学习"]},{"title":"python手册","path":"/2019/08/28/20190828-python手册/","content":"最近在学习查找资料的过程之中，看到了大佬的博客，觉得写得很好，也是我现在所欠缺的，所以下面先对大佬的博客进行复现。以供自己日后复习，查找，完善成自己的东西。 字符串固定字数，不足的空格补齐str.ljust(10) # 左对齐 字符串长10位rjust，ljust和center三个方法来给字符串补全空格rjust，向右对其，在左边补空格ljust，向左对其，在右边补空格center，让字符串居中，在左右补空格 排序sorted: 返回一个新的 listlist.sort(): 改变 list 自身的值reverse 参数: 默认为 False, 升序, True 时变为降序 列表循环删除列表元素常见错误: 直接删除, 或者正序删除 正确做法:1.使用 pop, 倒序删除 for i in range(len(list)): list.pop() 2.使用切片, 遍历拷贝列表, 操作原始列表, 用 remove 删除, remove 会操作首个遇到的匹配元素, 相等元素删除, 删除哪个都一样 for x in enumerate(a[::]): a.remove(x)for x in enumerate(a[::-1]): a.remove(x) 遍历列表:zz_list = ['a', 'b', 'c', 'd']for index in list: print(index) # 0 # 1 # 2 # 3for index in range(len(list)): print(index) # 0 # 1 # 2 # 3for index, val in enumerate(list): print(index, val) # 0 a # 1 b # 2 c # 3 d# 设置遍历的开始序号, val的输出不变for i, val in enumerate(list, 2): print(index, val) # 2 a # 3 b # 4 c # 5 d append() 方法追加单个元素 extend() 方法extend()函数用于在列表末尾一次性追加另一个序列中的多个值(用新列表扩展原来的列表).该方法没有返回值, 会直接在已经存在的列表中添加新的列表内容, extend和+=的作用差不多 a= [[1,2,3],[4,5,6]]b= [['a','b','c'],['d','e','f']]a.extend(b)print(a)# [[1, 2, 3], [4, 5, 6], ['a', 'b', 'c'], ['d', 'e', 'f']] 序列切片(双冒号)Python序列切片地址可以写为 [开始(包含) : 结束(不包含) : 步长]. 当开始省略的时候, 默认从第0项开始, 当结尾省略的时候, 默认到数组最后, 当步长省略的时候, 默认为1. 步长可以为负数, 代表从右向左取数. a = range(10) # a = [0, 1, 2, 3, 4, 5, 6, 7, 8 ,9]a[0:9:1] # [0, 1, 2, 3, 4, 5, 6, 7, 8] 包含开始下标, 不包含结束下标a[1::2] # [1, 3, 5, 7, 9]a[::3] # [0, 3, 6, 9]a[::-1] # [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]a[::-2] # [9, 7, 5, 3, 1] update() 方法dict.update(dict2) 将 dict2 中的键值更新到 dict 中, 对于存在的则覆盖原值, 对于不存在的则添加新的键值. 实例#!/usr/bin/pythondict = {'Name': 'Zara', 'Age': 7}dict2 = {'Sex': 'female' }dict.update(dict2)print \"Value : %s\" % dict 以上实例输出结果为： Value : {'Age': 7, 'Name': 'Zara', 'Sex': 'female'} 字典遍历字典: zz_dict = {'x': 1, 'y':2, 'z':3} 遍历keys: # 输出均为: x y zfor key in zz_dict: print(key)for key in zz_dict.iterkeys(): print(key)for key in zz_dict.keys(): print(key) 遍历values: # 输出均为 1 2 3for value in zz_dict.itervalues(): print(value)for value in zz_dict.values(): print(value) 遍历keys和values # 输出为: x corresponds to 1 (其余两个也一样)for key, value in zz_dict.iteritems(): # python3 没有iteritems print(key, \"corresponds to\", value)for key, value in zz_dict.items(): print(key, \"corresponds to\", value) 字符串判断字符串是否为字母或者数字str.isalnum() 字母或数字str.isalpha() 字母str.isdigit() 数字str.isspace() 空白符, \\t, , \\r isdigit() 和 isnumeric() 的区别 num = \"1\" #unicodenum.isdigit() # Truenum.isdecimal() # Truenum.isnumeric() # Truenum = \"1\" # 全角num.isdigit() # Truenum.isdecimal() # Truenum.isnumeric() # Truenum = b\"1\" # bytenum.isdigit() # Truenum.isdecimal() # AttributeError 'bytes' object has no attribute 'isdecimal'num.isnumeric() # AttributeError 'bytes' object has no attribute 'isnumeric'num = \"IV\" # 罗马数字num.isdigit() # Truenum.isdecimal() # Falsenum.isnumeric() # Truenum = \"四\" # 汉字num.isdigit() # Falsenum.isdecimal() # Falsenum.isnumeric() # True isdigit()True: Unicode数字，byte数字（单字节），全角数字（双字节），罗马数字False: 汉字数字Error: 无 isdecimal()True: Unicode数字，，全角数字（双字节）False: 罗马数字，汉字数字Error: byte数字（单字节） isnumeric()True: Unicode数字，全角数字（双字节），罗马数字，汉字数字False: 无Error: byte数字（单字节） str.rstrip()参数:chars: 指定删除的字符(默认为空格或换行符) 返回值:返回删除指定字符后的新字符串 备注:删除字符串末尾的指定字符(默认为空格或换行符) str.rstrip([chars]) str.strip()参数chars — 移除字符串头尾指定的字符序列。返回值返回移除字符串头尾指定的字符生成的新字符串。备注: str.strip([chars]) str.split()参数 str — 分隔符，默认为所有的空字符，包括空格、换行( )、制表符(\\t)等。 num — 分割次数。默认为 -1, 即分隔所有。返回值 返回分割后的字符串列表。 str.split(str=\"\", num=string.count(str)). 文件reduce() 函数reduce() 函数会对参数序列中元素进行累积。函数将一个数据集合（链表，元组等）中的所有数据进行下列操作：用传给 reduce 中的函数 function（有两个参数）先对集合中的第 1、2 个元素进行操作，得到的结果再与第三个数据用 function 函数运算，最后得到一个结果。reduce() 函数语法： reduce(function, iterable[, initializer]) 参数 function — 函数，有两个参数 iterable — 可迭代对象 initializer — 可选，初始参数返回值 返回函数计算结果实例 ```pythondef add(x, y) : # 两数相加... return x + y... reduce(add, [1,2,3,4,5]) # 计算列表和：1+2+3+4+515 reduce(lambda x, y: x+y, [1,2,3,4,5]) # 使用 lambda 匿名函数15 zip() 函数zip() 函数用于将可迭代的对象作为参数, 将对象中对应的元素打包成一个个 元组 ,然后返回有这些元组组成的 对象. ( 相比于python2中返回列表的方式, 这样做的好处是节约了不少的内存 )可以用list()转换或者dict()转换将对象转换成相应的数据类型如果各个迭代器的元素个数不一致, 则返回列表长度与最短的对象相同, 多出来的部分会被舍弃, 利用*号操作符, 可以将元组解压成列表. a = [1,2,3]b = [4,5,6]c = ['a','b','c','d','e','f']zip_ab = zip(a,b)print(zip_ab) # zip object at 0x104605348print(dict(zip_ab)) # {1: 4, 2: 5, 3: 6}# !!!注意, 一旦将zip_ab转换成dict以后, zip_ab内部就为空了!! 例如, 再次调用上面的语句:print(dict(zip_ab)) # {}# 但是zip_ab对象本身不会消失, 地址仍然不变print(zip_ab) # zip object at 0x104605348zip_abc = zip(a,b,c) # 注意, 三个元素的zip是不能转换成dict类型的print(zip_abc) # zip object at 0x1046054c8print(list(zip_abc)) # [(1, 4, 'a'), (2, 5, 'b'), (3, 6, 'c')]zip_abc = zip(a,b,c)z_a, z_b, z_c = zip(*zip_abc) # 利用zip(*)可以将zip对象重新解压, 返回类型是元组print(z_a) # (1,2,3)print(z_b) # (4,5,6)print(z_c) # ('a','b','c') getattr() 函数getattr()函数用于返回一个对象的属性值, 语法如下 getattr(object, name[, default]) 参数： object: 对象 name: 字符串, 对象属性 default: 默认返回值, 如果不提供该参数, 在没有对应属性时, 将触发Attributerror实例class A(object):... bar = 1... a = A() getattr(a, 'bar') # 获取属性 bar 值1 getattr(a, 'bar2') # 属性 bar2 不存在，触发异常Traceback (most recent call last):File \"stdin\", line 1, in moduleAttributeError: 'A' object has no attribute 'bar2' getattr(a, 'bar2', 3) # 属性 bar2 不存在，但设置了默认值3 dir() 函数可以查看某个类的所有方法和属性 members = [attr for attr in dir(classA)] _var: 在一个模块中以单下划线开头的变量和函数会被默认当做内部函数, 在使用from a_module import * 导入时, 这部分变量和函数不会被导入. 不过如果使用import a_module导入模块时, 仍然可以用a_module._var的形式访问该变量或函数 var_: 有时候, 一个变量的最适合的名称已经被另一个关键字所占用. 在这种情况下, 可以在名称的末尾附加一个下划线来解决冲突. __var: 双下划线前缀会导致Python解释器重写属性名称, 以避免子类中的命名冲突. 举例来说, 如果在class Test中有一个成员__x, 那么当利用内置函数dir(Test)来查看类的属性时, 会发现__x被解释器重命名为_Test__x. 双下划线的名称修饰同样也适用于方法名称. var: 双下划线开头和结尾的是一些 Python 的特殊对象, 如类成员的 init, del, name, call 等. Python 官方推荐永远不要讲这样的命名方式应用于自己的变量或函数. 有一种说法是说双下划线建议为类的私有成员, 但是 PEP8 当前的官方版本中并没有明说. _: 有时候我们会用一个独立的下划线作为一个名字, 这通常是用来指示某个变量时临时的或者无关紧要的. 类的特殊方法call()在 Python 中, 函数实际上也是一个对象: 1 2 3 f = abs print(f.name) # ‘abs’ print(f(-123)) # 123 从上面可以看出, 函数是一个对象, 当它赋给另一个变量时, 该变量也是一个函数对象, 可以起到与原函数相同的效果. 在 Python 中, 一个类实例也可以变成一个可调用对象, 只需要实现一个特殊方法 __call__() 即可. 下面我们举例把 Person 类变成一个可调用对象: ```python class Person(object): def __init__(self, name, gender): self.name = name self.gender = gender def __call__(self, friend): print(\"name:\", self.name) print(\"friend:\", friend) 接下来我们就可以将 Person 类的实例对象当做一个函数来使用, 如下所示: p = Person('Bob', 'male')p('Tim')# name: Bob# friend: Tim getitem()凡是在类中定义了 getitem() 方法, 那么它的实例对象就是可以通过 [] 操作符来访问指定的成员或进行特定的行为, 大多数情况下会将该方法实现成通过索引来方法元素的形式. class DataBase(object): def __init__(self): super(DataBase, self).__init__() self.vals = [1,2,3,4,5] def __getitem__(self, key): return self.vals[key] setitem()使得可以通过 A[3] = 4, B[“a”] = 5 等方式来对类中的元素进行赋值 file()查看模块的路径 len()使得类对象可以使用 Python 的内建方法 len(), 返回你自定义的数值. class DictDemo: def __init__(self,key,value): self.dict = {} self.dict[key] = value def __getitem__(self,key): return self.dict[key] def __setitem__(self,key,value): self.dict[key] = value def __len__(self): return len(self.dict)dictDemo = DictDemo('key0','value0')print(dictDemo['key0']) #value0dictDemo['key1'] = 'value1'print(dictDemo['key1']) #value1print(len(dictDemo)) #2 repr()class Test(object): def __init__(self, value='hello, world!'): self.data = value t = Test() t__main__.Test at 0x7fa91c307190 print t__main__.Test object at 0x7fa91c307190# 看到了么？上面打印类对象并不是很友好，显示的是对象的内存地址# 下面我们重构下该类的__repr__以及__str__，看看它们俩有啥区别# 重构__repr__class TestRepr(Test): def __repr__(self): return 'TestRepr(%s)' % self.data tr = TestRepr() trTestRepr(hello, world!) print trTestRepr(hello, world!)# 重构__repr__方法后，不管直接输出对象还是通过print打印的信息都按我们__repr__方法中定义的格式进行显示了# 重构__str__calss TestStr(Test): def __str__(self): return '[Value: %s]' % self.data ts = TestStr() ts__main__.TestStr at 0x7fa91c314e50 print ts[Value: hello, world!]# 你会发现，直接输出对象ts时并没有按我们__str__方法中定义的格式进行输出，而用print输出的信息却改变了 str()参见 repr() 代码示例 星号 **: 乘法**: 乘幂 用于函数参数单星号: 将所有参数以 元组(tuple) 的形式导入 def foo(param1, *param2): print(param1) print(param2)foo(1,2,3,4,5)# 1# (2,3,4,5) 双星号: 将所有参数以 字典 的形式导入 def bar(param1, **param2): print(param1) print(param2)bar(1, a=2, b=3)# 1# {'a': 2, 'b': 3} 当然这两个用法可以同时出现在一个函数中: def fun(a, b=10, *args, **kwargs): print(a) print(b) print(args) print(kwargs)fun(1,2,3,4,e=5,f=6)# 1# 2# (3,4)# {'e': 5, 'f': 6} globals() 函数该函数会以字典类型返回当前位置的全部全局变量 stripe()readlines()lambda 函数3.6新功能 f string包的导入机制模块和包的定义模块(module): 用来从逻辑上组织 Python 代码(变量, 函数, 类), 通常是一个.py文件.包(package): 定义了一个由模块和子包组成的 Python 应用程序执行环境, 本质上就是一个有层次的文件目录结果(必须带有一个__init__.py文件) import 的搜索路径 在当前目录下搜索 在环境变量PYTHONPATH中指定的路径列表中搜索 在 Python 安装路径的lib库中搜索Python 所有加载的模型信息都存放在sys.modules结构中, 当import一个模块时, 会按如下步骤来进行: 如果import A, 检查sys.modules中是否已经有A, 如果有则不加载, 如果没有则为A创建module对象, 并加载A; 如果是from A import B, 先为A创建module对象, 再解析A(此时会加载并执行A中的所有代码), 从中寻找B并填充到A的__dict__中.在导入模块的时候, 模块所在文件夹会自动生成一个__pycache__/module_name.cpython-35.pyc的文件. ```pythonimport module_name的本质是将module_name.py中的全部代码加载到内存中, 并将其赋值给与模块同名的变量, 这个变量的类型是classmodule.from module_name import name的本质是将指定的变量或者方法导入到当前的文件中import package_name的本质是执行该包下的__init__.py文件, 在执行文件后, 会在package_name目录下生成一个__pycache__/__init__cpython-35.pyc文件.from package_name import *的本质是导入__init__.py文件中的__all__列表(eg. __all__ = ['L2Norm', 'MultiBoxLoss']). 相对导入和绝对导入绝对导入: import A.Bfrom A import B 相对导入: from . import B # . 代表当前路径from ..A import B # .. 代表上层路径, ... 代表上上层路径. 在没有明确指定包结构的情况下, Python 是根据__name__来决定一个模块在包中的结构的, 如果是__main__, 则它本身就是顶层模块, 没有包结构, 如果是A.B.C结构, 则A是顶层模块. Python 的导入方式的不同具有不同的规则:1.如果是绝对导入, 一个模块只能导入自身的子模块或者和它的顶层模块同级别的模块及其子模块.2.如果是相对导入, 一个模块必须有包结构且只能导入它的顶层模块内部的模块.如果一个模块被直接运行, 则它自己为顶层模块, 不存在层次结构, 所以也找不到上层(..)的相对路径Python2.x 默认为相对路径导入, 而 Python3.x 默认为绝对路径导入, 这样可以避免导入的子包覆盖掉标准库模块. 通常, 在 Python2.x 中, 我们利用下面的语句来使其导入规则遵循 Python3.x from __future__ import absolute_import absolute_import的意思并不是将所有的导入都视为绝对导入, 而是指禁用隐式相对导入(implicit relative import), 关于隐式的显示的具体区别, 可以看下面的例子, 假设有如下的包结构: thing└── __init__.py├── books│ ├── __init__.py│ ├── adventure.py│ ├── history.py│ ├── horror.py│ └── lovestory.py├── furniture│ ├── __init__.py│ ├── armchair.py│ ├── bench.py│ ├── screen.py│ └── stool.py 那么如果想在stool.py中导入bench模块, 则有如下几种方式: import bench # 隐式相对导入from . import bench # 显式相对导入from furniture import bench # 绝对导入 隐式相对导入没有告诉解释器相对于谁进行导入, 默认相对于当前模块; 而显式相对导入则明确告诉了解释器相对于谁来导入. 以上导入方式的第三种是官方推荐的, 第一种是官方强烈不推荐的, Python3 中第一种导入方式只能用于导入sys.path中的模块.**注意, 还有相对导入的模块不能被直接运行, 会提示如下错误: Traceback (most recent call last): File \"test.py\", line 8, in module from .ssd import SSDModuleNotFoundError: No module named '__main__.ssd'; '__main__' is not a package 另外存在一种情况就是: 假如有两个模块a.py和b.py放在同一个目录下, 则可以直接在a.py中使用import b来导入模块b. 这是为什么呢? 我们上面说了在 Python3.x 中不能使用这种隐式相对导入, 但是这里却可以成功导入, 这是因为此时我们是直接运行a.py, 所以a.py和b.py的目录没有被当做一个包来处理, 因此不涉及相对导入和绝对导入的概念. 因此相对导入和绝对导入仅仅是针对于包而言的. 综合距离存在目录结构如下所示: dirRoot└── __init__.py├── file1.py├── file2.py├── dirA│ ├── __init__.py│ ├── a1.py│ └── a2.py├── dirB│ ├── __init__.py│ ├── b1.py│ └── b2.py 直接运行a1.py, 并希望导入a2模块: # a1.pyimport a2 # 正确, 此时并未将 dirA 当做包来处理, a1.py 和 a2.py 相当于两个独立的模块from a2 import func_a2 # 正确from .a2 import func_a2 # 错误, 当进行相对导入时, 不能直接运行 直接运行file1.py, 并希望导入a1模块, 同时a1模块中需要导入a2模块: # file1.pyfrom dirA import a1a1.func_a1() # a1.py 中的函数a1.func_a2() # a1.py 中导入了 a2.py 的函数, 可以直接使用# a1.pyimport a2 # 错误, 此时由于 dirA 中有 __init__.py 文件, 因此会将 dirA 当做包来处理,# 由于 Python3.x 不允许使用隐式的相对导入, 因此该语句非法from a2 import func_a2 # 错误, 原因同上from .a2 import func_a2 # 正确, 当进行相对导入时, 需要使用显式的相对导入 直接运行file1.py, 并希望导入a1模块, 同时a1模块中需要导入dirB/b1模块(跨文件夹导入): # file1.pyfrom dirA import a1a1.func_a1() # a1.py 中的函数a1.func_a2() # a2.py 中的函数a1.func_b1() # b1.py 中的函数# a1.pyfrom .a2 import func_a2 # 推荐使用绝对导入 from dirA.a1 import func_a2from dirB import b1 # 由于运行的是 file1.py 文件, 因此顶层目录是 dirRootfrom dirB.b1 import func_b1 # 所以可以直接使用 dirB 包 直接运行a1.py, 并希望跨目录的导入dirB/b1模块. 由于这种跨目录的导入超越了顶层路径的限制, 因此必须使用sys.path.append()方法来额外添加搜索路径, 否则无法正常导 # a1.pyimport syssys.path.append(\"../\") # 将 dirA 的上一次目录添加到搜索路径中from dirB import b1 # 正确, 注意必须先添加 path, 然后再导入from dirB.b1 import func_b1 # 正确from .a2 import func_a2 # 这里是错误的, 当直接执行 a1.py 时, a1.py 中不能包含显式相对导入 获取 python 版本:print(sys.version_info) 获取包的安装位置print(cv2) 解析 xml 文件导入: import sysif sys.version_info[0] == 2: import xml.etree.cElementTree as ETelse: import xml.etree.ElementTree as ET 解析: xmlfile = ET.parse(xmlfile_path)root = xmlfile.getroot() # 获取根节点root.tag # 标签root.attrib # 属性字典for child in root: # 迭代访问子节点 print(child.tag, child.attrib)# 可以通过索引访问嵌套节点的内容root[0][1].textElement.findall() #Element.find() # python 中 == 和 is 的区别== 只用于判断值是否相等is 用于判断两个对象是否为同一个实例小整数对象池: Python 为了优化速度，使用了小整数对象池，避免为整数频繁申请和销毁内存空间。而Python 对小整数的定义是 [-5, 257)，只有数字在-5到256之间它们的id才会相等，超过了这个范围就不行了，同样的道理，字符串对象也有一个类似的缓冲池，超过区间范围内自然不会相等了 队列 queue在 Python3 中, 原来的Queue模块被重命名为queue, 该模块包含以下三类数据结构: queue.Queue(maxsize=0): FIFO queue, 先进先出队列, 代表普通队列 queue.LifoQueue(maxsize=0): LIFO queue, 后进先出队列, 类似栈的作用 queue.PriorityQueue(maxsize=0): 优先级队列, 类似堆的作用. 默认为小顶堆, 常用形式为元组:(priority_number, data)上面的 maxsize 表明了队列中最大可以容纳的元素数量, 如果超过, 则无法插入. 当 maxsize = 0 时, 代表元素数量无限制.公有方法(以上三个通用): qsize(): 返回 approximate size, qsize() 0 不保证get()一定 work, 同理, qsize() maxsize 不保证put()一定 work. empty(): 如果队列为空, 返回 True. 和qsize()一样, 不提供保证性. full(): 如果队列满, 返回 True. 不提供保证性 put(item[, block[, timeout]]) put_nowait(item): 等价于put(item, False) get([block[, timeout]]) get_nowait(): 等价于get(False) task_done(): join(): 堆 heapqheapq 模块只有最小堆的功能, 要实现最大堆, 需要在入堆和出堆的时候取反, 并且 heapq 模块只能作用于数值型类型.最大堆: _heapify_max(), _heappop_max()给定一组数据, 创建堆, 两种方式(二者等价): import heapqdata = [1,3,6,2,8,5]heap = []for d in data: heapq.heappush(heap, n) # 方法一 逐个构建heapq.heapify(data) # 方法二 原地构建, 效率更高 小顶堆: heap = [1,3,6,2,8,5]heapq.heapify(heap)heapq.heappop(heap) # 返回并删除堆顶heapq.heapreplace(heap, 10) # 删除堆顶并添加新值heapq.heappushpop(heap, 10) # 先将新值加入堆中, 然后立刻弹出堆顶print(heap[0]) # 查看堆顶 大顶堆: # 方法一: 取负值heap = [-1,-3,-6,-2,-8,-5]# 方法二: 内置方法heap = [1,3,6,2,8,5]heapq._heapify_max(heap) # max_heapprint(heap[0]) # 查看堆顶, 8heapq._heappop_max(heap) # po from maxheapprint(heap[0]) # 6heapq._heapreplace_max(heap, 10)print(heap[0]) # 10# heapq._heappushpop_max(heap, 10) # 注意, 没有 _heappushpop_max 函数 Python 刷题常用队列: import Queuebase_queue = Queue.Queue() # 基本队列, 先进先出base_queue.put(x)base_queue.get()lifo_queue = Queue.LifoQueue() # 先进后出, 类似栈lifo_queue.put(x)lifo_queue.get()prio_queue = Queue.PriorityQueue() # 优先队列, 与C++中priority_queue类似, 可实现堆的功能prio_queue.put(x)prio_queue.get() numpy 中vstack, hstack, concatenate 和 stack 之间的区别和联系concatenatenumpy.concatenate((a1, a2, ...), axis=0, out=None) concatenate 的作用就是将多个数组序列按照axis指定的维度连接起来, 这些数组序列 a1, a2, … 必须保证 除了 axis 指定维度之外的其他维度具有相同的 shape.注意: 这里的维度指的是a1, a2的维度, 而不是(a1, a2)的维度从维度角度来更好理解 concatenate 的作用concatenate 执行后的 shape 特定是: axis 指定的维度是多个数组序列对应维度的数值和, 而其他维度保持不变. 也就是说不会增加新的维度, 这是 concatenate 与 stack 之间的一个重要的区别.如下所示: import numpy as npa1 = np.array([[1, 1], [2, 2], [3, 3]]) # shape = 3x2a2 = np.array([[1, 1], [2, 2]]) # shape = 2 x 2print(a1.shape, a2.shape)concat1 = np.concatenate((a1, a2), axis=0)print(concat1.shape) # shape 为 [5, 2], 在 0 维度上为 3+2, 其他维度保持不变print(concat1) # a1, a2 维度 0 不同, 一个为 3, 一个为 2, 其他维度相同, 均为 2#[[1 1]# [2 2]# [3 3]# [1 1]# [2 2]]#print(np.concatenate((a1, a2), axis=1)) # 由于维度 0 二者不同, 无法保持不变, 因此报错a1 = np.array([[1, 2, 3]]) # shape = 1x3a2 = np.array([[1, 2]]) # shape = 1x2print(a1.shape, a2.shape)concat2 = np.concatenate((a1, a2), axis=1)print(concat2.shape) # shape 为 [1, 5]在 1 维度上为 3 + 2, 0 维度上保持 1 不变print(concat2)# [[1 2 3 1 2]]# print(np.concatenate((a1, a2), axis=0)) # 维度 1 不同, 报错 有时候, concatenate的第一个参数只会传送一个一个数组序列, 这时候, 等价于将这个数组序列的第一维的元素看做是多个数组序列作为concatenate的参数进行传递. 如下所示: a = [[1, 2, 3], [1, 2, 3]]print(np.concatenate(a, axis=0)) # 该行与下一行等价print(np.concatenate((a[0], a[1]), axis=0))a = [[1, 2, 3], [1, 2]]print(np.concatenate(a, axis=0)) # 可以看出, 虽然 a 的第一维度为 2, 第二维度为 3 和 2# 但是, 我们要将其拆分, 拆分后, a[0], a[1] 的第一维度3和2, 其他维度相同, 因此可以在第一维度上进行连接print(np.concatenate((a[0], a[1]), axis=0)) stacknumpy.stack(arrays, axis=0, out=None)numpy.stack((a1, a2, ...), axis=0, out=None) stack 的作用就是将多个数组序列按照axis指定的维度 堆叠 起来, 这些数组序列 a1, a2, … 必须保证 所有维度都相同, 注意这里与 concatenate 的区别.要更好的理解stack, 可以借助 维度 的概念进行理解, 对于 shape 相同的 k 个数组序列来说, stack 的作用相当于新插入一个维度, 维度的大小为 k, 插入的位置为axis指定的位置. 如下所示: a1 = [[1, 1], [2, 2], [3, 3]] # shape = 3x2a2 = [[4, 4], [5, 5], [6, 6]] # shape = 3x2a3 = [[7, 7], [8, 8], [9, 9]] # shape = 3x2a4 = [[0, 0], [0, 0], [0, 0]] # shape = 3x2stack1 = np.stack((a1, a2, a3, a4), axis=0) # 新插入维度大小为 4, 位置为第 0 维print(stack1.shape) # shape 为 (4, 3, 2)print('### ', stack1) # 先将 shape 画好, 然后进行填充, 在第 0 维上进行堆叠, 因此 stack1[*][*] = a1[0], a1[1], ..., a4[2]stack2 = np.stack((a1, a2, a3, a4), axis=1) # 新插入维度大小为 4, 位置为第 1 维print(stack2.shape) # shape 为 (3, 4, 2)print('### ', stack2) # 在第 1 维上进行堆叠, 因此 stack2[*][*] = a1[0], a2[0], a3[0], a1[1], ...stack3 = np.stack((a1, a2, a3, a4), axis=2) # 新插入维度大小为 4, 位置为第 2 维print(stack3.shape) # shape 为 (3, 2, 4)print('### ', stack3) # 在第 2 维上进行堆叠, 因此 stack2[*][*] = [1 4 7 0], [1 4 7 0], [2 5 8 0], ... hstack 和 vstackhstack 和 vstack 虽然名字中都带有 stack, 但是实际上, 它们和np.stack的关系并不大, 一个明显的区别就是np.stack要求进行堆叠的多个数组序列需要保证 shape 完全相同, 并且堆叠后会新增加一个由axis指定的维度. 实际上, hstack 和 vstack 可以看做是特殊的 concatenate, 它们在某些情况下可以用 concatenate 来代替既然 hstack 和 vstack 是特殊的 concatenate, 也就是说, 它们所接受的多个数组序列在axis指定的维度上可以不同, 而在其他维度上必须相同.vstack: 在垂直方向上将多个数组序列进行堆叠, 相当于在axis=0维度上执行concatenatehstack: 在水平方向上将多个数组序列进行堆叠, 相当于在axis=1维度上执行concatenate a = [[1, 1], [2, 2], [3, 3]] # shape = 3x2b = [[4, 4], [5, 5], [6, 6]] # shape = 3x2c = [[7, 7], [8, 8], [9, 9]] # shape = 3x2d = [[0, 0], [0, 0], [0, 0]] # shape = 3x2v = np.vstack((a, b, c, d))print(v.shape) # (12, 2)print(v)x = np.concatenate((a, b, c, d), axis = 0) # 等价于 vstackprint(x.shape) # 12, 2print(x)h = np.hstack((a, b, c, d))print(h.shape) # (3, 8)print(h)x = np.concatenate((a, b, c, d), axis = 1) # 等价于 hstackprint(x.shape) # 3, 8print(x) 需要特别注意, 当多个数组序列是一维数组时, 应该先将一维数组转换成二维数组, 然后才能与相应的 concatenate 进行等价. 这是因为, 在数组序列是一维数组时, concatenate 是无法使用axis=1的, 因此此时的 hstack 相当于是在axis=0上进行 concatenate, 而 vstack 则需要先将数组的 shape 从 (N,) 转换成 (1, N) 后才相当于是在axis=1上进行 concatenate a = np.array([1, 2, 3, 4, 5]) # 当面对的是一维数组时,b = np.array([6, 7, 8, 9, 10])h = np.hstack((a, b))print(h.shape)print(h)con = np.concatenate((a, b), axis=0) # 当 a, b 是一维数组时, hstack 相当于在 axis=0 上进行连接print(con.shape)print(con)v = np.vstack((a, b))print(v.shape)print(v)con = np.concatenate(([a], [b]), axis=0) # 当 a, b 是一维数组时, vstack 相当于将 a, b 先转换成二维 (1, N), 然后在 axis=0 上进行连接print(con.shape)print(con) set 去重对于二维列表, 由于 list 的元素也是 list, 在内存中存储的是首元素地址, 无法直接使用 set, 因此需要先将内部的元素全部全换成 tuple 后, 才能使用 list 去重. 如下所示 a = list()a.append([1,2,3])a.append([1,2,3])a.append([1,2,3])a.append([4, 5, 6])# b = set(a) # 报错b = set(map(tuple, a))print(b) # {(4, 5, 6), (1, 2, 3)} os.sep用法ython是跨平台的。在Windows上，文件的路径分隔符是’\\’，在Linux上是’/‘。为了让代码在不同的平台上都能运行，那么路径应该写’\\’还是’/‘呢？使用os.sep的话，就不用考虑这个了，os.sep根据你所处的平台，自动采用相应的分隔符号。举例Linux下一个路径，/usr/share/python,那么上面的os.sep就是‘/’windows下一个路径，C：\\Users\\Public\\Desktop,那么上面的os.sep就是‘\\’. data_dir = os.sep.join(['hello', 'world']) Python3 元组Python元组包含了以下内置函数 len(tuple) 计算元组元素个数。1234 tuple1 = (‘Google’, ‘Runoob’, ‘Taobao’) len(tuple1)3 * max(tuple) 返回元组中元素最大值。 tuple2 = ('5', '4', '8') max(tuple2) '8' * min(tuple) 返回元组中元素最小值。 tuple2 = ('5', '4', '8') min(tuple2) '4' * tuple(seq) 将列表转换为元组。 list1= ['Google', 'Taobao', 'Runoob', 'Baidu'] tuple1=tuple(list1) tuple1 ('Google', 'Taobao', 'Runoob', 'Baidu') ## 序列化Python对象你需要将一个Python对象序列化为一个字节流，以便将它保存到一个文件、存储到数据库或者通过网络传输它。 对于序列化最普遍的做法就是使用 pickle 模块。为了将一个对象保存到一个文件中，可以这样做pickle 对于大型的数据结构比如使用 array 或 numpy 模块创建的二进制数组效率并不是一个高效的编码方式。 如果你需要移动大量的数组数据，你最好是先在一个文件中将其保存为数组数据块或使用更高级的标准编码方式如HDF5 (需要第三方库的支持)。 ```python In [1]: import pickle In [2]: obj = 123,\"abcdef\", [\"ac\", 123], {\"key\": \"value\", \"key1\": \"value1\"} In [3]: print(obj) (123, 'abcdef', ['ac', 123], {'key': 'value', 'key1': 'value1'}) In [4]: # 序列化到文件 In [5]: with open(r'./a.pickle','wb') as f: ...: pickle.dump(obj,f) ...: In [6]: with open(r'./a.pickle','rb') as f: ...: aa= pickle.load(f) ...: print(aa) ...: ...: (123, 'abcdef', ['ac', 123], {'key': 'value', 'key1': 'value1'}) 参考链接：https://hellozhaozheng.github.iohttps://www.runoob.com/python/python-tutorial.html","tags":["Python"],"categories":["Python"]},{"title":"shutil模块","path":"/2019/08/28/20190828-shutil模块/","content":"shutil.copyfile(\"old\",\"new\") # 复制文件，都只能是文件shutil.copytree(\"old\",\"new\") # 复制文件夹，都只能是目录，且new必须不存在shutil.copy(\"old\",\"new\") # 复制文件/文件夹，复制 old 为 new（new是文件，若不存在，即新建），复制 old 为至 new 文件夹（文件夹已存在）shutil.move(\"old\",\"new\") # 移动文件/文件夹至 new 文件夹中","tags":["Python"],"categories":["Python"]},{"title":"opencv模块","path":"/2019/08/28/20190828-opencv模块/","content":"opencv 基础知识cv2.imread 读入的图片, 其shape为(h, w, c), 颜色通道顺序为 (b, g, r) 常用颜色读取图片img = cv2.imread(img_path) 保存图片cv2.imwrite(save_path, img) 文本(startX, startY) 为左上角坐标 cv2.putText(img, \"text test\", (startX, startY), cv2.FONT_HERSHEY_SIMPLEX, font_size, (B,G,R), thickness) 画框(x,y) 为左上角坐标(x+h,y+w) 为右下角坐标 cv2.rectangle(img,(x,y), (x+h,y+w), (0,255,0), thickness) waitKey()keypress = cv2.waitKey(200) # 200为当前图片的显示持续时间if keypress == ord('c') # keypress为按键的整数形式, 所以需要用ord将字符类型转换if cv2.waitKey(200) == 27: # Decimal 27 = Esc opencv与numpyopencv的基础类型为numpy.ndarray, 因此可以直接使用 ndarray 的一些属性的方法 import cv2img = cv2.imread('./test.jpg')print(type(img)) # class 'numpy.ndarray'print(img.shape) # (500, 1069, 3) (高, 宽, 通道) 利用 cv2.merge 方法将 numpy.ndarray 数据转换成opencv的图片数据: # 图片的分辨率为300*200(宽*高)，这里b, g, r设为随机值，注意dtype属性b = np.random.randint(0, 255, (200, 300), dtype=np.uint8)g = np.random.randint(0, 255, (200, 300), dtype=np.uint8)r = np.random.randint(0, 255, (200, 300), dtype=np.uint8)# 合并通道，形成图片img = cv2.merge([b, g, r]) # opencv的通道是b在最前,r在最后# 显示图片cv2.imshow('test', img)cv2.waitKey(0)cv2.destroyWindow('test') 通道的拆分与合并拆分: cv2.split合并: cv2.merge # 图片的分辨率为800*200(宽*高)，这里b, g, r设为随机值，注意dtype属性b = np.random.randint(0, 255, (200, 800), dtype=np.uint8)g = np.random.randint(0, 255, (200, 800), dtype=np.uint8)r = np.random.randint(0, 255, (200, 800), dtype=np.uint8)# 合并通道，形成图片img = cv2.merge([b, g, r]) # opencv的通道是b在最前,r在最后# 显示图片cv2.imshow('test', img)cv2.waitKey(0)cv2.destroyWindow('test')# 拆分通道, 每个通道都变成了单通道数组[blue, green, red] = cv2.split(img) 将 BGR 转换成 RGB 通道顺序# 方法一:rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)# 方法二:rgb_img = img[:, :, [2, 1, 0]] # img[h,w,v]rgb_img = img[:, :, ::-1] PIL 与 cv2 格式互相转换PIL.Image读入的图片数据类型不是 numpy 数组, 它的size属性为 (w, h), 利用np.array转换成 numpy 数组后, 它的通道顺序为 (r, g, b) from PIL import Imageimport numpy as np# PIL to cv2pil_img = Image.open(img_path)print(pil_img.size) # (w, h)np_img = np.array(pil_img)cv2_img = np_img[:, :, ::-1] # 交换通道# cv2 to PILpil_img = Image.fromarray(cv2_img[:, :, ::-1]) 用matplotlib显示图像b,g,r=cv2.split(img)img2=cv2.merge([r,g,b])plt.imshow(img2)plt.show() 截取子图# 已知子图左上角坐标 (x1, y1), 右下角坐标(x2, y2)crop_img = img[y1:y2, x1:x2, :] opencv 核心算法cv2import cv2image_path = './test.jpg'src_image = cv2.imread(image_path) # 读取图片size = src_image.shape # 获取图片的尺寸, 返回一个元组: (height, width, depth)copy_image = src_image.copy() # 复制图片cv2.imwrite('./dst_test.jpg', copy_image) # 保存图片cv2.imshow('image', src_image) # 显示图片# 利用下标访问指定像素for x in range(src_image.shape[0]): # 以行为主, 行数=图片height for y in range(src_image.shape[1]): # 列数 = 图片width src_image[x,y] = (255,0,255) # (blue, green, red) 值越高表示对应颜色越显著, 全0为黑, 全255为白","tags":["计算机视觉"],"categories":["计算机视觉"]},{"title":"skimage模块","path":"/2019/08/28/20190828-skimage模块/","content":"比opencv的速度要慢很多, 但是使用起来更加简单, 真的对速度要求很高的话, 一般都会C++和opecv使用. 所以一般情况下, 首先看skimage能否实现, 不行的话再转用opencv import skimagefrom skimage import io # IO is a submodule. Submodules need to be imported from the parent module explicitly.img = io.imread(\"1.jpg\")","tags":["计算机视觉"],"categories":["计算机视觉"]},{"title":"从上往下打印二叉树","path":"/2019/08/28/20190828-从上往下打印二叉树/","content":"时间限制：1秒 空间限制：32768K 热度指数：420679本题知识点： 队列 树 题目描述从上往下打印出二叉树的每个节点，同层节点从左至右打印。 /*struct TreeNode {\tint val;\tstruct TreeNode *left;\tstruct TreeNode *right;\tTreeNode(int x) : val(x), left(NULL), right(NULL) {\t}};*/class Solution {public: vectorint PrintFromTopToBottom(TreeNode* root) { //队列是先进先出 queueTreeNode* que; vectorint vec; que.push(root);//先将整个二叉树放入队列 while(!que.empty()) //当队列非空进行循环 { TreeNode* p; p = que.front();//先读取队列的首元素 que.pop();//弹出队列的首元素 if(p == NULL) continue;//所有元素存入vec后，由于队列中存放着空指针，依然进入循环，但此时p的值为NULL，不执行下面的操作，跳出循环结束 que.push(p-left); que.push(p-right); vec.push_back(p-val); } return vec; }}; 运行时间：3ms占用内存：464k","tags":["算法"],"categories":["算法"]},{"title":"广度/宽度优先搜索(BFS)","path":"/2019/08/28/BFS/","content":"前言广度优先搜索 是最简单的图搜索算法之一， 也是许多重要的图算法的原型。Prime的最小生成树算法和Dijkstra的单源最短路径算法都使用了类似广度优先搜索的思想。给定图G=(V,E) 和一个可以识别的源 节点 s，广度优先搜索对图G中的边进行系统性的探索来发现可以从源节点，到达所有的节点。该算法能够计算从源结点s到每个可到达的节点的距离(最小的边数)，同时生成一棵“广度优先搜索树”。该树以源结点s为根节点，包括所有可以从s到达的结点。对于每个从源结点s可以到达的结点v，在广度优先搜索树里从结点s到结点v的简单路径所对应的的就是图G中从结点s到结点v的“最短路径”，即包含最少边数的路径，该算法既可以用于有向图也可以用于无向图。广度优先算法之所以如此得名是因为该算法始终是将已经发现的结点和未发现结点之间的边界，沿其广度方向向外扩展。也就是说，算法需要在发现所有距离源结点s为k的所有结点之后，才会发现距离源结点s为k+1的 其他结点。 图的概念 图(graph) 是一种$\\textcolor{Blue}{网状数据} $结构， 图是由非空的顶点集合和一个描述顶点之间的关系的集合组成。 图由顶点和边组成，顶点表示对象，边表示对象之间的连接关系。 边也可以带权值，称为带权值图。 无向图术语 两个顶点之间如果有边连接，视为两个顶点相邻 相邻顶点间的序列称为路径 起点和终点重合的路径称为圈 顶点连接的边数叫做这个顶点的度 没有圈的连通图，就是树 没有圈的非连通图，就是森林 一棵树的边数等于顶点数-1 边数等于顶点数-1 的连通图，就是树 12345678910111213141516171819 | BFS(G,s) \\\\ for each vertex u \\in G.V -{s}\\\\ u.color = WHITE\\\\ u.d = \\infty\\\\ u.\\pi = NIL\\\\ s.color = GRAY\\\\ s.d = 0\\\\ s.\\pi = NIL\\\\ Q = \\emptyset\\\\ ENQUEUE(Q,s)\\\\ while Q eq = \\emptyset\\\\ u = DEQUEUE(Q)\\\\ for each v \\in G.Adj[u]\\\\ if v.color == WHITE\\\\ v.color = GRAY\\\\ v.d = u.d+1\\\\ v.\\pi = u\\\\ ENQUEUE(Q,v)\\\\ u.color = BLACK\\\\ —|— 广度优先搜索的流程图 实例POJ3984《迷宫问题》定义一个二维数组：int maze[5][5] = {0, 1, 0, 0, 0,0, 1, 0, 1, 0,0, 0, 0, 0, 0,0, 1, 1, 1, 0,0, 0, 0, 1, 0,};它表示一个迷宫，其中的1表示墙壁，0表示可以走的路，只能横着走或竖着走，不能斜着走，要求编程序找出从左上角到右下角的最短路线。 解题思路队列是先进后出，后进先出。 对应于题目的输入数组：0, 1, 0, 0, 0,0, 1, 0, 1, 0,0, 0, 0, 0, 0,0, 1, 1, 1, 0,0, 0, 0, 1, 0,把节点定义为(x,y)，(x,y)表示数组maze的项maze[x][y]。于是起点就是(0,0)，终点是(4,4)。按照刚刚的思路，手工梳理一遍：初始条件：起点Vs为(0,0)终点Vd为(4,4)灰色节点集合Q={}初始化所有节点为白色节点下面开始广度优先搜索：1.起始节点Vs变成灰色，加入队列Q，Q={(0,0)}2.取出队列Q的头一个节点Vn，Vn={0,0}，Q={}3.把Vn={0,0}染成黑色，取出Vn所有相邻的白色节点{(1,0)}4.不包含终点(4,4)，染成灰色，加入队列Q，Q={(1,0)}5.取出队列Q的头一个节点Vn，Vn={1,0}，Q={}6.把Vn={1,0}染成黑色，取出Vn所有相邻的白色节点{(2,0)}7.不包含终点(4,4)，染成灰色，加入队列Q，Q={(2,0)}8.取出队列Q的头一个节点Vn，Vn={2,0}，Q={}9.把Vn={2,0}染成黑色，取出Vn所有相邻的白色节点{(2,1), (3,0)}10.不包含终点(4,4)，染成灰色，加入队列Q，Q={(2,1), (3,0)}11.取出队列Q的头一个节点Vn，Vn={2,1}，Q={(3,0)}12.把Vn={2,1}染成黑色，取出Vn所有相邻的白色节点{(2,2)}13.不包含终点(4,4)，染成灰色，加入队列Q，Q={(3,0), (2,2)}14.持续下去，知道Vn的所有相邻的白色节点中包含了(4,4)……15.此时获得了答案 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 | // BFS.cpp : 定义控制台应用程序的入口点。 // #include \"stdafx.h\" #include iostream using namespace std; int map[5][5]; //相邻四个节点 int borderUponX[4] = { 0, 0, 1, -1 }; int borderUponY[4] = { 1, -1, 0, 0 }; int front = 0, rear = 1; struct node { int pre; int x; int y; } path[100]; void print(int i) {//当前节点 if (path[i].pre != -1) {//找到前面那个节点 print(path[i].pre); cout \"(\" path[i].x \",\" path[i].y \")\" endl; } else {//最前面的那个节点 cout \"(\" path[i].x \",\" path[i].y \")\" endl; } } void bfsSearch(int x, int y) { //开始节点（出发），前面没有节点了 path[front].x = x; path[front].y = y; path[front].pre = -1; //当front == rear的时候说明已经走完了所以“相邻”节点 //且都不通 while (front rear) { for (int i = 0; i != 4; i++) { //相邻节点坐标 int pathX = path[front].x + borderUponX[i]; int pathY = path[front].y + borderUponY[i]; //不符合的节点（遇到边界或已经走过了） if (pathY 0 || pathX 0 || pathX 4 || pathY 4 || map[pathX][pathY]) continue; else {//将front的相邻的可以过去的并且是还没有走过的节点加到路径里面 map[pathX][pathY] = 1; path[rear].x = pathX; path[rear].y = pathY; path[rear].pre = front; rear++; } if (pathX == 4 pathY == 4) { //找到了一条路径，又是第一次找到 //那么就是最短路径了 print(rear - 1); break; } } front++; } } int main(int argc, char const *argv[]) { for (int i = 0; i 5; i++) for (int j = 0; j 5; j++) cin map[i][j]; bfsSearch(0, 0); system(\"pause\"); return 0; } —|—","tags":["BFS"],"categories":["算法"]},{"title":"PIL模块","path":"/2019/08/28/PIL模块/","content":"导入1 | from PIL import Image —|— 读取1 | img = Image.open(filepath) —|— 显示1 | img.show() —|— 与 numpy 数组的互相转换PIL Image 转 numpy 数组 1 | img_to_array = np.array(img) —|— numpy 数组转 PIL Image (注意要确保数组内的值符合 PIL 的要求) 1 | array_to_img = Image.fromarray(img_to_array) —|— PIL 与 cv2 格式互相转换PIL.Image读入的图片数据类型不是 numpy 数组, 它的size属性为 (w, h), 利用np.array转换成 numpy 数组后, 它的通道顺序为 (r, g, b) 1 2 3 4 5 6 7 8 9 10 11 | from PIL import Image import numpy as np # PIL to cv2 pil_img = Image.open(img_path) print(pil_img.size) # (w, h) np_img = np.array(pil_img) cv2_img = np_img[:, :, ::-1] # 交换通道 # cv2 to PIL pil_img = Image.fromarray(cv2_img[:, :, ::-1]) —|—","tags":["Python"],"categories":["计算机视觉"]},{"title":"Pytorch trick","path":"/2019/08/28/Pytorch-trick/","content":"目录： 指定GPU编号 查看模型每层输出详情 梯度裁剪 扩展单张图片维度 独热编码 防止验证模型时爆显存 学习率衰减 冻结某些层的参数 对不同层使用不同学习率 1. 指定GPU编号 设置当前使用的GPU设备仅为0号设备，设备名称为 /gpu:0：os.environ[“CUDA_VISIBLE_DEVICES”] = “0” 设置当前使用的GPU设备为0,1号两个设备，名称依次为 /gpu:0、/gpu:1： os.environ[“CUDA_VISIBLE_DEVICES”] = “0,1” ，根据顺序表示优先使用0号设备,然后使用1号设备。指定GPU的命令需要放在和神经网络相关的一系列操作的前面。 2.查看模型每层输出详情Keras有一个简洁的API来查看模型的每一层输出尺寸，这在调试网络时非常有用。现在在PyTorch中也可以实现这个功能。 使用很简单，如下用法： 1 2 | from torchsummary import summary summary(your_model, input_size=(channels, H, W)) —|— input_size 是根据你自己的网络模型的输入尺寸进行设置。 3.梯度裁剪（Gradient Clipping）1 2 3 4 5 6 7 8 | import torch.nn as nn outputs = model(data) loss= loss_fn(outputs, target) optimizer.zero_grad() loss.backward() nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2) optimizer.step() —|— nn.utils.clip_grad_norm_ 的参数： parameters – 一个基于变量的迭代器，会进行梯度归一化 max_norm – 梯度的最大范数 max_norm – 梯度的最大范数知乎用户 不椭的椭圆 提出：梯度裁剪在某些任务上会额外消耗大量的计算时间，可移步评论区查看详情。 4、扩展单张图片维度因为在训练时的数据维度一般都是 (batch_size, c, h, w)，而在测试时只输入一张图片，所以需要扩展维度，扩展维度有多个方法： 1 2 3 4 5 6 7 8 9 10 11 12 13 | import cv2 import torch image = cv2.imread(img_path) image = torch.tensor(image) print(image.size()) img = image.view(1, *image.size()) print(img.size()) # output: # torch.Size([h, w, c]) # torch.Size([1, h, w, c]) —|— 或者 1 2 3 4 5 6 7 8 9 10 11 | import cv2 import numpy as np image = cv2.imread(img_path) print(image.shape) img = image[np.newaxis, :, :, :] print(img.shape) # output: # (h, w, c) # (1, h, w, c) —|— 或者 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | import cv2 import torch image = cv2.imread(img_path) image = torch.tensor(image) print(image.size()) img = image.unsqueeze(dim=0) print(img.size()) img = img.squeeze(dim=0) print(img.size()) # output: # torch.Size([(h, w, c)]) # torch.Size([1, h, w, c]) # torch.Size([h, w, c]) —|— tensor.unsqueeze(dim)：扩展维度，dim指定扩展哪个维度。 tensor.squeeze(dim)：去除dim指定的且size为1的维度，维度大于1时，squeeze()不起作用，不指定dim时，去除所有size为1的维度。 5.独热编码在PyTorch中使用交叉熵损失函数的时候会自动把label转化成onehot，所以不用手动转化，而使用MSE需要手动转化成onehot编码。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 | import torch class_num = 8 batch_size = 4 def one_hot(label): \"\"\" 将一维列表转换为独热编码 \"\"\" label = label.resize_(batch_size, 1) m_zeros = torch.zeros(batch_size, class_num) # 从 value 中取值，然后根据 dim 和 index 给相应位置赋值 onehot = m_zeros.scatter_(1, label, 1) # (dim,index,value) return onehot.numpy() # Tensor - Numpy label = torch.LongTensor(batch_size).random_() % class_num # 对随机数取余 print(one_hot(label)) # output: [[0. 0. 0. 1. 0. 0. 0. 0.] [0. 0. 0. 0. 1. 0. 0. 0.] [0. 0. 1. 0. 0. 0. 0. 0.] [0. 1. 0. 0. 0. 0. 0. 0.]] —|— 6. 防止验证模型时爆显存验证模型时不需要求导，即不需要梯度计算，关闭autograd，可以提高速度，节约内存。如果不关闭可能会爆显存。 1 2 3 | with torch.no_grad(): # 使用model进行预测的代码 pass —|— 感谢知乎用户zhaz 的提醒，我把 torch.cuda.empty_cache() 的使用原因更新一下。 这是原回答： Pytorch 训练时无用的临时变量可能会越来越多，导致 out of memory ，可以使用下面语句来清理这些不需要的变量。 官网 上的解释为： Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.torch.cuda.empty_cache() 意思就是PyTorch的缓存分配器会事先分配一些固定的显存，即使实际上tensors并没有使用完这些显存，这些显存也不能被其他应用使用。这个分配过程由第一次CUDA内存访问触发的。 而 torch.cuda.empty_cache() 的作用就是释放缓存分配器当前持有的且未占用的缓存显存，以便这些显存可以被其他GPU应用程序中使用，并且通过 nvidia-smi命令可见。注意使用此命令不会释放tensors占用的显存。 对于不用的数据变量，Pytorch 可以自动进行回收从而释放相应的显存。 7. 学习率衰减1 2 3 4 5 6 7 8 9 10 11 | import torch.optim as optim from torch.optim import lr_scheduler # 训练前的初始化 optimizer = optim.Adam(net.parameters(), lr=0.001) scheduler = lr_scheduler.StepLR(optimizer, 10, 0.1) # # 每过10个epoch，学习率乘以0.1 # 训练过程中 for n in n_epoch: scheduler.step() ... —|— 8. 冻结某些层的参数在加载预训练模型的时候，我们有时想冻结前面几层，使其参数在训练过程中不发生变化。 我们需要先知道每一层的名字，通过如下代码打印： 1 2 3 | net = Network() # 获取自定义网络结构 for name, value in net.named_parameters(): print('name: {0}, grad: {1}'.format(name, value.requires_grad)) —|— 假设前几层信息如下： 1 2 3 4 5 6 7 8 | name: cnn.VGG_16.convolution1_1.weight, grad: True name: cnn.VGG_16.convolution1_1.bias, grad: True name: cnn.VGG_16.convolution1_2.weight, grad: True name: cnn.VGG_16.convolution1_2.bias, grad: True name: cnn.VGG_16.convolution2_1.weight, grad: True name: cnn.VGG_16.convolution2_1.bias, grad: True name: cnn.VGG_16.convolution2_2.weight, grad: True name: cnn.VGG_16.convolution2_2.bias, grad: True —|— 后面的True表示该层的参数可训练，然后我们定义一个要冻结的层的列表： 1 2 3 4 5 6 | no_grad = [ 'cnn.VGG_16.convolution1_1.weight', 'cnn.VGG_16.convolution1_1.bias', 'cnn.VGG_16.convolution1_2.weight', 'cnn.VGG_16.convolution1_2.bias' ] —|— 冻结方法如下： 1 2 3 4 5 6 | net = Net.CTPN() # 获取网络结构 for name, value in net.named_parameters(): if name in no_grad: value.requires_grad = False else: value.requires_grad = True —|— 冻结后我们再打印每层的信息： 1 2 3 4 5 6 7 8 | name: cnn.VGG_16.convolution1_1.weight, grad: False name: cnn.VGG_16.convolution1_1.bias, grad: False name: cnn.VGG_16.convolution1_2.weight, grad: False name: cnn.VGG_16.convolution1_2.bias, grad: False name: cnn.VGG_16.convolution2_1.weight, grad: True name: cnn.VGG_16.convolution2_1.bias, grad: True name: cnn.VGG_16.convolution2_2.weight, grad: True name: cnn.VGG_16.convolution2_2.bias, grad: True —|— 可以看到前两层的weight和bias的requires_grad都为False，表示它们不可训练。最后在定义优化器时，只对requires_grad为True的层的参数进行更新。 1 | optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.01) —|— 9. 对不同层使用不同学习率我们对模型的不同层使用不同的学习率。 还是使用这个模型作为例子： 1 2 3 4 5 6 7 8 9 10 11 12 13 | net = Network() # 获取自定义网络结构 for name, value in net.named_parameters(): print('name: {}'.format(name)) # 输出： # name: cnn.VGG_16.convolution1_1.weight # name: cnn.VGG_16.convolution1_1.bias # name: cnn.VGG_16.convolution1_2.weight # name: cnn.VGG_16.convolution1_2.bias # name: cnn.VGG_16.convolution2_1.weight # name: cnn.VGG_16.convolution2_1.bias # name: cnn.VGG_16.convolution2_2.weight # name: cnn.VGG_16.convolution2_2.bias —|— 对 convolution1 和 convolution2 设置不同的学习率，首先将它们分开，即放到不同的列表里： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | conv1_params = [] conv2_params = [] for name, parms in net.named_parameters(): if \"convolution1\" in name: conv1_params += [parms] else: conv2_params += [parms] # 然后在优化器中进行如下操作： optimizer = optim.Adam( [ {\"params\": conv1_params, 'lr': 0.01}, {\"params\": conv2_params, 'lr': 0.001}, ], weight_decay=1e-3, ) —|— 我们将模型划分为两部分，存放到一个列表里，每部分就对应上面的一个字典，在字典里设置不同的学习率。当这两部分有相同的其他参数时，就将该参数放到列表外面作为全局参数，如上面的weight_decay。 也可以在列表外设置一个全局学习率，当各部分字典里设置了局部学习率时，就使用该学习率，否则就使用列表外的全局学习率。 显示训练时间1 2 3 4 5 6 | for epoch in range(start_epoch, config.epochs): start = time.time() train_loss, lr = train_epoch(model, optimizer, scheduler, train_loader, device, criterion, epoch, all_step, writer, logger) logger.info('[{}/{}], train_loss: {:.4f}, time: {:.4f}, lr: {}'.format( epoch, config.epochs, train_loss, time.time() - start, lr)) —|— 参考：https://mp.weixin.qq.com/s?__biz=MzU3NjE4NjQ4MA==mid=2247485953idx=2sn=3ae788b7d643541254ba311f7a7facedchksm=fd16fb1eca61720870bc58c1a465a346cf2c6a7e8bea39e4b3d582474b595021f3a5b635086dmpshare=1scene=1srcid=sharer_sharetime=1566885137387sharer_shareid=285785c5623899db73795495779fe8be#rd","tags":["Python"],"categories":["深度学习"]},{"title":"Pytorch手册","path":"/2019/08/28/Pytorch手册/","content":"PyTorch 主要提供以下两大特色： 支持强力GPU加速的Tensor计算能力 基于tape的具有自动微分求导能力的深度神经网络框架 PyTorch 主要包含以下组成要素: 组成要素 描述说明 torch 一个类似于numpy的tensor哭, 提供强力的GPU支持 torch.autograd 一个基于tape的具有自动微分求导能力的库, 可以支持几乎所有的tesnor operatioin torch.nn 一个神经网络库, 与autograd深度整合, 可以提供最大限度的灵活性 torch.multiprocessing Python的多线程处理, 可以提供torch Tensors之间的内存共享, 对于加载数据和Hogwild training来说十分有用 torch.utils 一些功能类和函数, 如DataLoader, Trainer等等 torch.legacy(.nn/.optim) 为了兼容性而存在的一些代码和实现 Pytorch通常可以作为以下用途使用: 为了使用GPUs性能的numpy替代品 可以提供强大灵活力和速度优势的深度学习平台. torchbackends.cudnn 1 | torch.backends.cudnn.benchmark = True —|— 上述设置可以让内置的cudnn的auto-tuner自动寻找最合适当前配置的搞笑算法, 来达到优化运行效率的目标, 在使用时, 应该遵循以下两个准则: 如果网络的输入数据维度或类型上变化不大, 则该设置可以增加运行效率 如果网络的输入数据在每次的iteration中都变化的话, 会导致cudnn每次都寻找一遍最优配置, 这样反而 会降低 运行效率. torch.cat() 1 | torch.cat(seq, dim=0, out=None) # 返回连接后的tensor —|— 将给定的 tensor 序列 seq 按照维度连接起来. 默认维度为0, 说明会将其在第 0 个维度上进行拼接.(最后的结果是第 0 维度增大, 例如三个2行3列的 tensor 按照第0维度拼接, 最后得到的 tensor 维度为6行3列) clamp()/clamp_()1 | torch.clamp(input, min, max, out=None) - Tensor —|— 将input里面元素全部划分到[min,max]区间内, 小于min的置为min, 大于max的置为max. 如果不指定min或者max,则认为无下界或上界其他调用形式: 1 | torch.Tensor(min, max) # 调用tensor为input, 返回值为out —|— device()1 | device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") —|— gather()1 | torch.gather(input, dim, index, out=None) - Tensor —|— 沿着dim指定的轴按着index指定的值重新组合成一个新的tensor. 1 2 3 | out[i][j][k] = input[index[i][j][k]][j][k] # if dim == 0 out[i][j][k] = input[i][index[i][j][k]][k] # if dim == 1 out[i][j][k] = input[i][j][index[i][j][k]] # if dim == 2 —|— 即假设input是一个 n 维的tensor, 其 size 为 (x0,x1,…,xi−1,xi,xi+1,…,xn−1), 若dim=i, 则 index 必须也是一个 n 维的tensor, 其 size 为 (x0,x1,…,xi−1,y,xi+1,…,xn−1), 其中 y≥1, 而返回的 tensor out 的 size 和 index 的 size 相同.一句来说 gather 的作用就是, 在指定的维度上筛选给给定下标index指示的值, 其他值舍弃.一个例子说明:scores是一个计算出来的分数，类型为[torch.FloatTensor of size 5x1000]而y_var是正确分数的索引，类型为[torch.LongTensor of size 5]容易知道，这里有1000个类别，有5个输入图像，每个图像得出的分数中只有一个是正确的，正确的索引就在y_var中，这里要做的是将正确分数根据索引标号提取出来。 1 2 | scores = model(X_var) # 分数 scores = scores.gather(1, y_var.view(-1, 1)).squeeze() #进行提取 —|— 提取后的scores格式也为[torch.FloatTensor of size 5]这里讲一下变化过程： 首先要知道之前的scores的size为[5,1000]，而y_var的size为[5]，scores为2维，y_var为1维不匹配，所以先用view将其展开为[5,1]的size，这样维数n就与scroes匹配了。 接下来进行gather，gather函数中第一个参数为1，意思是在第二维进行汇聚，也就是说通过y_var中的五个值来在scroes中第二维的5个1000中进行一一挑选，挑选出来后的size也为[5,1]，然后再通过squeeze将那个一维去掉，最后结果为[5].Tensor形式 1 | torch.Tensor.gather(dim, index) - Tensor —|— torch.ge()torch.gt()1 | torch.gt(input, other, out=None) # - Tensor —|— 根据 input 和 other 的值返回一个二值 tensor, 如果满足大于条件则为1, 不满足则为0.other 可以是能够转换成 input size 的tensor, 也可以是一个 float 标量. torch.index_select()1 | torch.index_select(input, dim, index, out=None) # - Tensor —|— 返回在 dim 维度上的 index 指明的下标组成的 tensor.返回的 tensor 的维度的数量和 input 是相同的, 但是第 dim 维度的 size 会和 index size大小相同. 其他维度的 size 保持不变. torch.le()1 | torch.le(input, other, out=None) # -Tensor —|— 按元素计算 input≤other. max()1 2 3 | torch.max(input) # 返回一个Tensor, 代表所有元素中的最大值 torch.max(input,dim,keepdim=False,out=None) # 返回一个元组:(Tensor, LongTensor) —|— 第二种形式会返回一个元组, 元组内元素类型为: (Tensor, LongTensor), 其中, 前者代表对应 dim 上 reduce 后的最大值, 后者代表最大值在维度 dim 中对应的下标.如果keepdim=True, 则输出的 tensor 的 size 会和输入的相同, 只不过对应 dim 维度上的size为1. 否则, 对应 dim 维度会被 squeeze/reduce, 使得输出的维度比输入的维度少1. 1 2 3 4 5 6 7 8 | a = torch.randn(4, 4) a tensor([[-1.2360, -0.2942, -0.1222, 0.8475], [ 1.1949, -1.1127, -2.2379, -0.6702], [ 1.5717, -0.9207, 0.1297, -1.8768], [-0.6172, 1.0036, -0.6060, -0.2432]]) torch.max(a, 1) (tensor([ 0.8475, 1.1949, 1.5717, 1.0036]), tensor([ 3, 0, 0, 1])) —|— mm()注意, 没有torch.mm_版本 1 | torch.mm(mat1, mat2, out=None) # 返回值为Tensor, 也可以使用out记录返回值 —|— 两矩阵相乘, 矩阵的size需要满足乘法规则其他调用形式: 1 | torch.Tensor(mat2) # 调用者为mat1 —|— norm()返回输入tensor的p-norm标量 1 | torch.norm(input, p=2) # 返回一个标量tensor —|— numel()1 | torch.numel(input) #返回一个int值 —|— 返回 inpput tensor 中的元素的总个数 1 2 | a = torch.randn(1,2,3,4,5) print(torch.numel(a)) # 120 —|— ones()randn()标准正太分布随机基础, 传入参数为维度信息 torch.sort()1 | torch.sort(input, dim=None, descending=False, out=None) # 返回 (Tensor, LongTensor) —|— sum()1 2 3 4 | torch.sum(input, dtype=None) # 返回求和后的Tensor(只有一个元素) torch.sum(input, dim, keepdim=False, dtype=None) # 返回在dim上reduce的sum和, 如果dim包含多个维度, 则都进行reduce求和. # reduce这个词很形象, 因为返回的Tensor的维度刚好没有了dim指示的那些维度 —|— 其他形式: 1 | torch.Tensor.sum() —|— torch.t()1 | torch.t(input) # 返回转置后的Tensor —|— 其他形式: 1 | torch.Tensor.t() —|— unsqueeze()在指定维度上插入一个 singleton 维度(一般用于将单一数据处理用 batch 的形式) 1 | torch.unsqueeze(input, dim, out=None) # - Tensor —|— 返回的tensor与input tensor 共享数据 dim 的取值范围在 [-input.dim()-1, input.dim()+1] 之间, 如果为负值, 则相当于 dim = dim + input.dim() + 1. zeros()torch.cudatorch.cuda.empty_cache()释放所有未使用的 GPU 内存, 使用这些内存可以被其他 GPU 应用使用, 并且可以被 nvidia-smi 查到.empty_cache() 并不会强制提升供 PyTorch 使用的显卡内存的大小, 查看Memory management torch.Tensortorch.Tensor 是默认类型 torch.FloatTensor 的别名, 使用 torch.Tenosr 的构造函数创建 tensor 变量时, 传入的是维度信息(注意与 torch.tensor() 的区别): 1 2 | t = torch.Tensor(2,3,4) # 里面的数值未初始化, 是随机的 print(t.size()) # torch.Size([2,3,4]) —|— torch.LongTesnor 使用方法相似, 只不过数据类型是长整型. troch.tensor()创建tensor 1 | torch.tensor(data, dtype=None, device=None, requires_grad=False) —|— 可以利用torch.tensor从python的list数据或者其他序列数据中创建tensor对象 1 2 | torch.tensor([[1,-1],[1,-1]]) torch.tensor(np.array([[1,2,3],[4,5,6]])) —|— 注意, torch.tensor()函数总是会对数据进行复制操作, 因此, 如果你仅仅是想将数据的requires_grad标志改变, 那么就应该使用required_grad_()或者detach()函数来避免复制. 同时, 对numpy数组使用torch.as_tensor()将其转换成tensor而无需复制 torch.Tensor.cpu()1 2 | torch.Tensor.cpu() z = x.cpu() —|— 将tensor移动到cpu上, 注意返回值z是cpu上的数据, tensor x 本身的device属性不变 torch.Tensor.cuda()1 2 | torch.Tensor.cuda() z = x.cuda() —|— torch.Tensor.dim()1 | torch.Tensor.dim() - int —|— 返回 tensor 的维度的个数. torch.Tensor.max()1 | torch.Tensor.max(dim=None, keepdim=False) - Tensor or (Tensor, Tensor) —|— 详情见 torch.max() torch.Tensor.numel()1 | torch.Tensor.numel() —|— 详见 torch.numel() torch.Tensor.to()1 | torch.Tensor.to(*args, *kwargs) —|— 返回一个转移后的tensor, 而自身维持不变 1 2 3 4 | t = torch.randn(2,3) t.to(torch.float64) t.to(device) t.to(\"cuda:0\") —|— 将tensor移动到gpu上, 注意返回值 z 是gpu 上的数据, tensor x 本身的 device 属性不变 torch.Tensor.numpy()tensor与numpy数组的转换 1 2 3 | torch.Tensor.numpy() # 返回tensor对应的numpy数组 torch.from_numpy(ndarray) # 将numpy数组ndarray转换成对应的tensor并返回. —|— torch.Tensor 实际上是 torch.FloatFensor 的别名 torch.Tensor.permute()重新排列tensor的维度 1 | torch.Tensor.permute(*dims) # 返回一个重新排列维度后的 tensor —|— torch.Tensor.unsqueeze()详细可见torch.unsqueeze torch.Tensor.expand()1 | torch.Tensor.expand(*sizes) # 返回 tensor —|— 将 tensor 中的 singleton 维度扩展到一个更大的 size.参数 -1 意味着不改变原始的维度新增的维度的元素被被添加到前头, size不能设置为-1.expand 并没有申请新的内存, 而仅仅是在当前已经存在的 tensor 上面创建了新的视图(view), 使得 singleton 维度被扩展成了一个更大的尺寸.Any dimension of size 1 can be expanded to an arbitrary value without new memory. 1 2 3 4 | x = torch.tensor([1],[2],[3]) print(x.size()) # torch.Size([3,1]) print(x.expand(3,4)) # torch.Size([3,4]) # 将维度为1的扩展到任意尺寸 print(x.expand(-1,4)) # torch.Size([3,4]) # -1 代表不改变维度 —|— 注意, 只能对 singleton 的维度进行扩展, 如果强行对其他维度扩展, 则会报错. torch.Tensor.expand_as()1 | torch.Tensor.expand_as(other) # 返回 tensor —|— 将当前 tensor 扩展到和 other 一样的size.self.expand_as(other) 与 self.expand(other.size()) 等价. torch.Tensor.index_fill_()1 | torch.Tensor.index_fill_(dim, index, val) # 返回tensor —|— 在给定的维度 dim 上, 用 val 将该维度上的 index 坐标的值填充. 1 2 3 4 5 6 7 | x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float) index = torch.tensor([0, 2]) x.index_fill_(1, index, -1) print(x) #tensor([[-1., 2., -1.], # [-1., 5., -1.], # [-1., 8., -1.]]) —|— torch.Tensor.contiguous()返回一个连续的tensor, 数据内容不变 1 | torch.Tensor.contiguous() # 如果tensor本身就是连续的, 那么就会返回tensor本身 —|— 这里的 contiguous 指的是内存上的连续, 由于在 PyTorch 中, view 只能用在 contiguous 的 tensor 上面, 而如果在 view 之前使用了 transpose, permute 等操作后, 就需要使用 contiguous 来返回一个 contiguous tensor.在 PyTorch 0.4 版本以后, 增加了 torch.reshape(), 这与 numpy.reshape() 的功能类似, 它大致相当于 tensor.contiguous().view() ? torch.Tensor.item()当Tensor中只包含一个元素时, 可以利用该函数返回这个元素的标量 torch.Tensor.tolist()可以将Tensor转换成列表 torch.Tensor.zero_()1 | torch.Tensor.zero_() —|— 将当前的 tensor 变量全部置为0(原地) torch.autogradset_grad_enabled()1 | class torch.autograd.set_grad_enabled(mode) —|— 用来控制梯度计算的开关(依据bool类型参数mode决定), 可以当做上下文管理器使用, 也可以当做函数使用 1 2 3 4 5 6 7 8 9 10 11 | # 当做上下文管理器 with torch.set_grad_enabled(is_train): # 注意, 这里省略了autograd loss.backward() optimizer.step() # 当做函数使用 w1 = torch.Tensor([1], requires=True) torch.set_grad_enabled(True) print(w1.requires_grad) # True torch.set_grad_enabled(False) print(w1.requires_grad) # False —|— no_grad()1 | class torch.autograd.no_grad —|— 用于禁用梯度计算的上下文管理器.在测试阶段, 当你确信你不会调用Tensor.backward()时,禁用梯度计算十分有用. 这会降低计算使用内存消耗. 1 2 3 4 5 6 | x = torch.tensor([1.0], requires_grad=True) with torch.no_grad(): # 省略了autograd print(x.requires_grad) # True, 虽然为True, 但在该上下文中, 会无视掉requires_grad参数, 一律做False处理 y = x*2 print(y.requires_grad) # False, 在当前上下文产生的tensor的requires_grad属性为False print(x.requires_grad) # True —|— torch.autograd.Function1 | class torch.autograd.Function —|— 为可微分的 ops 记录 operation history, 同时定义计算公式. 每一个作用在 tensor 上的 operatin 都会创建一个新的 function 对象, 它会执行计算过程并记录相关信息. 这些信息可以从一个由 functions 组成的有向图中获得. 当 backward() 方法被调用时, 就会利用这些信息在 function 上进行反向传播, 并将梯度传给下一个 Funtion.通常情况下, 当用于需要自定义可自动求导的 ops 时, 可以实现一个 Function 的子类. 1 2 3 4 5 6 7 8 9 10 11 12 | # Example class Exp(Function): @staticmethod def forward(ctx, i): result = i.exp() ctx.save_for_backward(result) @staticmethod def backward(ctx, grad_output): result, = ctx.saved_tensors return grad_output*result —|— static forward(ctx, args, kwargs):*定义前向计算的逻辑. static backward(ctx, *grad_outputs):定义反向传导的逻辑, 如果确定不会使用到反向传播, 则可以不实现该函数. torch.nnModule1 | class torch.nn.Module —|— 所有神经网络Module的基类, 自定义的模型也应该是它的子类.Modules可以包含其他Module(如Linear, Conv2d等等). parameters()1 2 | for param in model.parameters(): print(param.data, param.size()) —|— state_dict: 1 | torch.nn.Module.state_dict(destination=None,prefix=\"\",keep_vars=False) —|— 以字典形式返回整个module的状态 train1 | torch.nn.Module.train(mode=True) —|— 将module的模式设置为train, 这只对部分module有效, 如Dropout, BatchNorm等, 详细请查看官网.返回值: torch.nn.Module training1 | torch.nn.Module.training # 属性, 返回一个bool值, 指示当前的模式是否为train —|— eval1 | torch.nn.Module.eval() # 注意, 和train不同, eval为无参函数 —|— 将module的mode设置为evaluation, 同样, 只对部分module起效. Linear1 | torch.nn.Linear(in_features, out_features, bias=True) —|— 全连接层的实现. 输入的shape为 (N,…,infeatures), 输出的shape为 (N,…,outfeatures), 可以看出, 除了最后一维不同外, 其他维度都相同. (通常在使用Linear之前, 会将输入变成二维的矩阵, 其中第一维为batch size, 第二维为特征向量).in_features 和 out_features 可以当做属性用.来获取. Conv2d1 | class torch.nn.Conv2的(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True) —|— in_channels(int): out_channels(int): kernel_size(intortuple): stride(intortuple, optional): MaxPool2dSoftmax() 1 | class torch.nn.Softmax(dim=None) —|— dim指明了需要进行 softmax 的维度, 在这个维度上的值, 加起来和为1. ReLU1 | torch.nn.ReLU(inplace=False) —|— 输入输出的shape是相同的, 执行relu函数 torch.nn.Sequential1 | class torch.nn.Sequential(*args) —|— torch.nn.MSELoss1 | class torch.nn.MSELoss(size_average=None, reduce=None, reduction=\"elementwise_mean\") —|— size_average(bool, optional): 弃用(见reduction参数). 默认情况下, loss会计算在每个样本上的平均误差. 如果将size_average置为False, 则计算平方误差总和. 当reduce参数为False时, 忽视该参数 reduce(bool, optional): 弃用(见reduction参数). reduce参数顾名思义, 就是是否让MSELoss函数返回值的维度减少, 默认为True, 即会将任意维度的输入计算loss后, 返回一个标量(平均or总和取决于size_average), 如果为False, 则说明返回值维度不应该发生变化, 故而返回值就是对每个元素单独进行平方损失计算. 1 2 3 4 5 6 7 8 9 10 | y = torch.tensor([1,2,3,4], dtype=torch.float) pred_y = torch.tensor([1,1,1,1], dtype=torch.float) loss_fn1 = torch.nn.MSELoss() loss1 = loss_fn1(y, pred_y) loss_fn2 = torch.nn.MSELoss(size_average=False) loss2 = loss_fn2(y, pred_y) loss_fn3 = torch.nn.MSELoss(reduce=False) loss3 = loss_fn3(y, pred_y) print(loss1,loss2,loss3) # tensor(3.5000) tensor(14.) tensor([0., 1., 4., 9.]) —|— 3. reduction(string, optional): 用字符串来替代上面两个参数的作用: “elementwise_mean”(默认) | “sum” | “none” (不进行reduce). torch.nn.functionalconv1d()conv2d()relu() 1 | torch.nn.functional.relu(input, inplace=True) # 返回 一个 Tenosr —|— relu_()1 | torch.nn.functional.relu_(input) # relu() 的原地版本 —|— torch.optimlr_schedulerStepLR1 | class torch.optim.lr_schedulr.StepLR(optimizer,step_size,gamma=0.1,last_epoch=-1) —|— 每经过step_size次epoch之后, lr就会衰减gamma倍(new_lr=lr×gamma), 初始的lr来自于optimizer中的lr参数. 1 2 3 4 5 | # Observe that all parameters are being optimized optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9) # Decay LR by a factor of 0.1 every 7 epochs exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1) —|— ExponentialLR1 | class torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma,last_epoch=-1) —|— CosineAnnealingLR1 2 | ## Adam class torch.optim.Adam(params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False) —|— conv2dtorch.utils.dataDataLoader1 | class torch.utils.data.DataLoader(dataset,batch_size=1,shuffle=False,sampler=None,batch_sampler=None,num_workers=0,collate_fn=function default_collate,pin_memory=False,drop_last=False,timeout=0,worker_init_fn=None) —|— 数据加载器, 将数据集和采样器结合起来, 并且提供单/多线程的迭代器. dataset(utils.data.Dataset): batch_size(int,optional): batch中的样本个数 shuffle(bool,optional) num_worker(int,optional): 加载数据的线程个数, 0意味着只有一个主线程.方法： iter(self): 可以当做迭代器使用, 如inputs,class_ids=next(iter(dataloaders)), 其中, input的shape为 (N,C,H,W), class_ids的shape为 (N). len(self): 返回数据集的类别数目 torchvisiontorchvision.utilsmake_grid1 | torchvision.utils.make_grid(tensor,nrow=8,padding=2,normalize=False,range=None,scale_each=False,pad_value=0) —|— 制作一个关于image的grid, 返回值依然是一个tensor, 只不过尺度变成了3D, 相当于把多个图片拼接在一起了, 直接通过plt.imshow(grid)即可输出网格化以后的图片. tensor(Tensor/list): 4D的 mini-batch Tensor, Shape为 (N×C×H×W), 或者是同维度的list. torchvision.transformstorchvision.transforms.Compose 1 2 3 4 5 6 7 | class torchvision.transforms.Compose(transforms) # 使用 trans.Compose([ transforms.CenterCrop(10), transforms.ToTensor(), ]) —|— 将多个transforms操作组合起来, 注意参数是列表形式 Transforms on PIL Image1 2 3 | # cv2 image to PIL Image # skimage to PIL Image —|— 注意, 以下操作作用在PIL Image上的 CenterCrop1 | class torchvision.transform.CenterCrop(size) —|— size参数表示输出的图谱的大小, 如果只传入了一个数字, 则该数字既表示高度, 又表示宽度. Resize1 | class torchvision.transforms.Resize(size, interpolation=2) —|— size: 期望的输出size. interpolation: 插值方法, 默认为双线性插值 ToTensor 1 | class torchvision.transforms.ToTensor —|— 将一个PIL Image或者numpy.ndarray (H×W×C,[0, 255])转换成torch.FloatTensor (C×H×W, [0.0, 1.0]). RandomHorizontalFlip1 | transforms.RandomHorizontalFlip(p=0.5) —|— 在给定概率下对PIL Image随机执行水平翻转操作 RandomResizedCrop1 | torch.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333), interpolation=2) —|— 对PIL Image随机执行剪裁操作(按照scale和ratio的区间剪裁), 然后将剪裁后的图片放缩都期望的尺寸(默认插值为双线性插值) size: 期望得到的尺寸 scale: 剪裁的面积比例(相对于原始图) ratio: 剪裁的宽高比 interpolation: 默认为:PIL.Image.BILINEAR Transforms on torch.*Tensor注意, 以下操作是作用在tensor上的 Normalize 1 | class torchvision.transforms.Normalize(mean, std) —|— 将图片tensor按照均值mean和标准差std进行归一化, 对于n个channels, 有 mean=(M1, …, Mn), std=(S1,…,Sn).注意, 这个归一化操作是原地进行的 torchvision.datasetsImageFolder1 | class torchvision.datasets.ImageFolder(root, transform=None, target_transform=None, loader=function default_loader) —|— 一个一般化的数据加载器, 主要针对如下数据排列格式: 1 2 3 4 5 6 7 | root/dog/x.png root/dog/y.png root/dog/z.png ... root/cat/123.png root/cat/nsdf3.png root/cat/asd932_.png —|— root: 根目录路径 transform(callable,optional): 对图片要做的变换操作 target_transform(callable,optional): 对target要做的变换操作 loader: 用于加载给定路径图片的函数属性： classes(list): 返回类别的名字列表 class_names class_to_idx(dict): 以字典的形式返回(class_name, class_index) imgs(list): 返回元组列表: (image path, class_index)方法： getitem(index): 根据index返回(sample,target)元组. 可以使用 len(imagefolder) 返回类别数量 sort()1 | sort(dim=None, descending=False) # 默认为升序, 返回(Tensor, LongTensor) —|— 详见 torch.sort() torch.distributedtorch.distributed.reduce()inspect 模块1 2 3 4 | inspect.signature() # 查看函数签名, python3.6以上 inspect.getargspec() # 查看函数签名, python3.6以上 inspect.getsource() # 获取模型的code inspect.getabsfile() # 获取模块的路径 —|— un normalize1 2 3 4 5 6 7 | mean = torch.tensor([1, 2, 3], dtype=torch.float32) std = torch.tensor([2, 2, 2], dtype=torch.float32) normalize = T.Normalize(mean.tolist(), std.tolist()) unnormalize = T.Normalize((-mean / std).tolist(), (1.0 / std).tolist()) ​~~~s —|—","tags":["Python"],"categories":["深度学习"]},{"title":"glob模块","path":"/2019/08/28/glob模块/","content":"glob模块是Python最简单的模块之一, 内容非常少, 用它可以查找符合特定规则的文件路径名, 查找文件时只会用到三个匹配符: : 匹配0个或多个字符 ? : 匹配单个字符 [] : 匹配指定范围内的字符, 如[0-9]匹配数字 glob.glob()参数:_(str): 文件路径的正则表达式 返回值:_(list): 符合正则表达式的文件路径列表 备注:返回所有匹配的文件路径列表, 它只有一个参数pathname, 定义了文件路径匹配的规则, 这里可以是绝对路径或者相对路径: 1 2 3 4 5 6 | import glob pathes_list = glob.glob(\"~/Pictures/*.jpg\") # 获取Pictures下的所有图片 relative_pathes_list = glob.glob(\"../*.py\") # 获取上级目录中的所有.py文件 —|— 在 linux, osx 系统中, 通配符的匹配是大小写区分的, 也就是需要特别指定大小写: 1 | extensions = ['jpg', 'JPG', 'jpeg', 'JPEG'] —|— 但是在 windows 当中, 通配符的匹配是不区分大小写的, 因此只需要指定大小写中的一个即可, 两个都指定的话, 会出现重复的情况 1 | extensions = ['jpg', 'jpeg'] —|— glob.iglob获取一个可遍历的对象, 使用它可以逐个获取匹配的文件路径名. 与glob.glob()的区别是: glob.glob()会同时获取到所有的匹配路径, 而glob.iglob()一次只获取一个匹配路径. 1 2 3 4 | f = glob.iglob(\"../*.py\") print f # generator object iglob at 0x00B9FF80 for py in f: print(py) —|—","tags":["Python"],"categories":["Python"]},{"title":"logging模块","path":"/2019/08/28/logging模块/","content":"简单使用1 2 3 4 5 6 7 | import logging logging.debug(\"debug msg\") logging.info(\"info msg\") logging.warn(\"warn msg\") logging.error(\"error msg\") logging.critical(\"critical msg\") —|— 默认情况下, logging模块将日志打印到屏幕上, 只有日志级别高于WARNING的日志信息才回输出","tags":["编程"],"categories":["Python"]},{"title":"numpy实现神经网络","path":"/2019/08/28/numpy实现神经网络/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 | import numpy as np # N 为batch size, D_in 为输入维度 # H 为隐藏层的维度, D_out 为输出的维度 N, D_in, H, D_out = 64, 1000, 100, 10 # 创建随机的输入和输出数据 x = np.random.randn(N, D_in) # N × D_in 的矩阵 y = np.random.randn(N, D_out) # N × D_out 的矩阵 # 对两个隐藏层w1,w2进行初始化 w1 = np.random.randn(D_in, H) w2 = np.random.randn(H, D_out) # 设置学习率 learning_rate = 1e-6 for t in range(500): # 前向传播: 计算预测结果 y_pred h = x.dot(w1) # x维度为64 × 1000, w1维度为 1000 × 100, 计算完以后, h维度为 64 × 100 h_relu = np.maximum(h,0) y_pred = h_relu.dot(w2) # h_relu维度为 64×100, w2维度为100×10, y的维度为64×10 # 计算损失 loss = np.square(y_pred - y).sum() print(t, loss) # 反向传播根据loss更新w1和w2的值 grad_y_pred = 2.0*(y_pred - y) # 对y_pred求导 grad_w2 = h_relu.T.dot(grad_y_pred) # 对w2求导, 微分矩阵应该与w2的size相同 grad_h_relu = grad_y_pred.dot(w2.T) # 对h_relu求导 grad_h = grad_h_relu.copy() grad_h[h 0] = 0 # 经过relu, 将小于0的梯度归0 grad_w1 = x.T.dot(grad_h) # Update weights w1 = w1 - learning_rate * grad_w1 w2 = w2 - learning_rate * grad_w2 —|—","tags":["Python"],"categories":["Python"]},{"title":"opencv模块","path":"/2019/08/28/opencv模块/","content":"opencv 基础知识cv2.imread 读入的图片, 其shape为(h, w, c), 颜色通道顺序为 (b, g, r) 常用颜色读取图片1 | img = cv2.imread(img_path) —|— 保存图片1 | cv2.imwrite(save_path, img) —|— 文本(startX, startY) 为左上角坐标 1 | cv2.putText(img, \"text test\", (startX, startY), cv2.FONT_HERSHEY_SIMPLEX, font_size, (B,G,R), thickness) —|— 画框(x,y) 为左上角坐标(x+h,y+w) 为右下角坐标 1 | cv2.rectangle(img,(x,y), (x+h,y+w), (0,255,0), thickness) —|— waitKey()1 2 3 4 5 | keypress = cv2.waitKey(200) # 200为当前图片的显示持续时间 if keypress == ord('c') # keypress为按键的整数形式, 所以需要用ord将字符类型转换 if cv2.waitKey(200) == 27: # Decimal 27 = Esc —|— opencv与numpyopencv的基础类型为numpy.ndarray, 因此可以直接使用 ndarray 的一些属性的方法 1 2 3 4 | import cv2 img = cv2.imread('./test.jpg') print(type(img)) # class 'numpy.ndarray' print(img.shape) # (500, 1069, 3) (高, 宽, 通道) —|— 利用 cv2.merge 方法将 numpy.ndarray 数据转换成opencv的图片数据: 1 2 3 4 5 6 7 8 9 10 | # 图片的分辨率为300*200(宽*高)，这里b, g, r设为随机值，注意dtype属性 b = np.random.randint(0, 255, (200, 300), dtype=np.uint8) g = np.random.randint(0, 255, (200, 300), dtype=np.uint8) r = np.random.randint(0, 255, (200, 300), dtype=np.uint8) # 合并通道，形成图片 img = cv2.merge([b, g, r]) # opencv的通道是b在最前,r在最后 # 显示图片 cv2.imshow('test', img) cv2.waitKey(0) cv2.destroyWindow('test') —|— 通道的拆分与合并拆分: cv2.split合并: cv2.merge 1 2 3 4 5 6 7 8 9 10 11 12 | # 图片的分辨率为800*200(宽*高)，这里b, g, r设为随机值，注意dtype属性 b = np.random.randint(0, 255, (200, 800), dtype=np.uint8) g = np.random.randint(0, 255, (200, 800), dtype=np.uint8) r = np.random.randint(0, 255, (200, 800), dtype=np.uint8) # 合并通道，形成图片 img = cv2.merge([b, g, r]) # opencv的通道是b在最前,r在最后 # 显示图片 cv2.imshow('test', img) cv2.waitKey(0) cv2.destroyWindow('test') # 拆分通道, 每个通道都变成了单通道数组 [blue, green, red] = cv2.split(img) —|— 将 BGR 转换成 RGB 通道顺序1 2 3 4 5 | # 方法一: rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) # 方法二: rgb_img = img[:, :, [2, 1, 0]] # img[h,w,v] rgb_img = img[:, :, ::-1] —|— PIL 与 cv2 格式互相转换PIL.Image读入的图片数据类型不是 numpy 数组, 它的size属性为 (w, h), 利用np.array转换成 numpy 数组后, 它的通道顺序为 (r, g, b) 1 2 3 4 5 6 7 8 9 10 11 | from PIL import Image import numpy as np # PIL to cv2 pil_img = Image.open(img_path) print(pil_img.size) # (w, h) np_img = np.array(pil_img) cv2_img = np_img[:, :, ::-1] # 交换通道 # cv2 to PIL pil_img = Image.fromarray(cv2_img[:, :, ::-1]) —|— 用matplotlib显示图像1 2 3 4 | b,g,r=cv2.split(img) img2=cv2.merge([r,g,b]) plt.imshow(img2) plt.show() —|— 截取子图1 2 | # 已知子图左上角坐标 (x1, y1), 右下角坐标(x2, y2) crop_img = img[y1:y2, x1:x2, :] —|— opencv 核心算法cv21 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 | import cv2 image_path = './test.jpg' src_image = cv2.imread(image_path) # 读取图片 size = src_image.shape # 获取图片的尺寸, 返回一个元组: (height, width, depth) copy_image = src_image.copy() # 复制图片 cv2.imwrite('./dst_test.jpg', copy_image) # 保存图片 cv2.imshow('image', src_image) # 显示图片 # 利用下标访问指定像素 for x in range(src_image.shape[0]): # 以行为主, 行数=图片height for y in range(src_image.shape[1]): # 列数 = 图片width src_image[x,y] = (255,0,255) # (blue, green, red) 值越高表示对应颜色越显著, 全0为黑, 全255为白 —|—","tags":["Python"],"categories":["计算机视觉"]},{"title":"python手册","path":"/2019/08/28/python手册/","content":"最近在学习查找资料的过程之中，看到了大佬的博客，觉得写得很好，也是我现在所欠缺的，所以下面先对大佬的博客进行复现。以供自己日后复习，查找，完善成自己的东西。 字符串固定字数，不足的空格补齐1 2 3 4 5 | str.ljust(10) # 左对齐 字符串长10位 rjust，ljust和center三个方法来给字符串补全空格 rjust，向右对其，在左边补空格 ljust，向左对其，在右边补空格 center，让字符串居中，在左右补空格 —|— 排序sorted: 返回一个新的 listlist.sort(): 改变 list 自身的值reverse 参数: 默认为 False, 升序, True 时变为降序 列表循环删除列表元素常见错误: 直接删除, 或者正序删除 正确做法:1.使用 pop, 倒序删除 1 2 | for i in range(len(list)): list.pop() —|— 2.使用切片, 遍历拷贝列表, 操作原始列表, 用 remove 删除, remove 会操作首个遇到的匹配元素, 相等元素删除, 删除哪个都一样 1 2 3 4 5 | for x in enumerate(a[::]): a.remove(x) for x in enumerate(a[::-1]): a.remove(x) —|— 遍历列表:1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 | zz_list = ['a', 'b', 'c', 'd'] for index in list: print(index) # 0 # 1 # 2 # 3 for index in range(len(list)): print(index) # 0 # 1 # 2 # 3 for index, val in enumerate(list): print(index, val) # 0 a # 1 b # 2 c # 3 d # 设置遍历的开始序号, val的输出不变 for i, val in enumerate(list, 2): print(index, val) # 2 a # 3 b # 4 c # 5 d —|— append() 方法追加单个元素 extend() 方法extend()函数用于在列表末尾一次性追加另一个序列中的多个值(用新列表扩展原来的列表).该方法没有返回值, 会直接在已经存在的列表中添加新的列表内容, extend和+=的作用差不多 1 2 3 4 5 | a= [[1,2,3],[4,5,6]] b= [['a','b','c'],['d','e','f']] a.extend(b) print(a) # [[1, 2, 3], [4, 5, 6], ['a', 'b', 'c'], ['d', 'e', 'f']] —|— 序列切片(双冒号)Python序列切片地址可以写为 [开始(包含) : 结束(不包含) : 步长]. 当开始省略的时候, 默认从第0项开始, 当结尾省略的时候, 默认到数组最后, 当步长省略的时候, 默认为1. 步长可以为负数, 代表从右向左取数. 1 2 3 4 5 6 | a = range(10) # a = [0, 1, 2, 3, 4, 5, 6, 7, 8 ,9] a[0:9:1] # [0, 1, 2, 3, 4, 5, 6, 7, 8] 包含开始下标, 不包含结束下标 a[1::2] # [1, 3, 5, 7, 9] a[::3] # [0, 3, 6, 9] a[::-1] # [9, 8, 7, 6, 5, 4, 3, 2, 1, 0] a[::-2] # [9, 7, 5, 3, 1] —|— update() 方法1 | dict.update(dict2) —|— 将 dict2 中的键值更新到 dict 中, 对于存在的则覆盖原值, 对于不存在的则添加新的键值. 实例1 2 3 4 5 6 7 | #!/usr/bin/python dict = {'Name': 'Zara', 'Age': 7} dict2 = {'Sex': 'female' } dict.update(dict2) print \"Value : %s\" % dict —|— 以上实例输出结果为： 1 | Value : {'Age': 7, 'Name': 'Zara', 'Sex': 'female'} —|— 字典遍历字典: 1 | zz_dict = {'x': 1, 'y':2, 'z':3} —|— 遍历keys: 1 2 3 4 5 6 7 8 9 | # 输出均为: x y z for key in zz_dict: print(key) for key in zz_dict.iterkeys(): print(key) for key in zz_dict.keys(): print(key) —|— 遍历values: 1 2 3 4 5 6 | # 输出均为 1 2 3 for value in zz_dict.itervalues(): print(value) for value in zz_dict.values(): print(value) —|— 遍历keys和values 1 2 3 4 5 6 | # 输出为: x corresponds to 1 (其余两个也一样) for key, value in zz_dict.iteritems(): # python3 没有iteritems print(key, \"corresponds to\", value) for key, value in zz_dict.items(): print(key, \"corresponds to\", value) —|— 字符串判断字符串是否为字母或者数字str.isalnum() 字母或数字str.isalpha() 字母str.isdigit() 数字str.isspace() 空白符, \\t, , \\r isdigit() 和 isnumeric() 的区别 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 | num = \"1\" #unicode num.isdigit() # True num.isdecimal() # True num.isnumeric() # True num = \"1\" # 全角 num.isdigit() # True num.isdecimal() # True num.isnumeric() # True num = b\"1\" # byte num.isdigit() # True num.isdecimal() # AttributeError 'bytes' object has no attribute 'isdecimal' num.isnumeric() # AttributeError 'bytes' object has no attribute 'isnumeric' num = \"IV\" # 罗马数字 num.isdigit() # True num.isdecimal() # False num.isnumeric() # True num = \"四\" # 汉字 num.isdigit() # False num.isdecimal() # False num.isnumeric() # True —|— isdigit()True: Unicode数字，byte数字（单字节），全角数字（双字节），罗马数字False: 汉字数字Error: 无 isdecimal()True: Unicode数字，，全角数字（双字节）False: 罗马数字，汉字数字Error: byte数字（单字节） isnumeric()True: Unicode数字，全角数字（双字节），罗马数字，汉字数字False: 无Error: byte数字（单字节） str.rstrip()参数:chars: 指定删除的字符(默认为空格或换行符) 返回值:返回删除指定字符后的新字符串 备注:删除字符串末尾的指定字符(默认为空格或换行符) 1 | str.rstrip([chars]) —|— str.strip()参数chars — 移除字符串头尾指定的字符序列。返回值返回移除字符串头尾指定的字符生成的新字符串。备注: 1 | str.strip([chars]) —|— str.split()参数 str — 分隔符，默认为所有的空字符，包括空格、换行( )、制表符(\\t)等。 num — 分割次数。默认为 -1, 即分隔所有。返回值 返回分割后的字符串列表。 1 | str.split(str=\"\", num=string.count(str)). —|— 文件reduce() 函数reduce() 函数会对参数序列中元素进行累积。函数将一个数据集合（链表，元组等）中的所有数据进行下列操作：用传给 reduce 中的函数 function（有两个参数）先对集合中的第 1、2 个元素进行操作，得到的结果再与第三个数据用 function 函数运算，最后得到一个结果。reduce() 函数语法： 1 | reduce(function, iterable[, initializer]) —|— 参数 function — 函数，有两个参数 iterable — 可迭代对象 initializer — 可选，初始参数返回值 返回函数计算结果实例 1 2 3 4 5 6 7 | def add(x, y) : # 两数相加 ... return x + y ... reduce(add, [1,2,3,4,5]) # 计算列表和：1+2+3+4+5 15 reduce(lambda x, y: x+y, [1,2,3,4,5]) # 使用 lambda 匿名函数 15 —|— zip() 函数zip() 函数用于将可迭代的对象作为参数, 将对象中对应的元素打包成一个个 元组 ,然后返回有这些元组组成的 对象. ( 相比于python2中返回列表的方式, 这样做的好处是节约了不少的内存 )可以用list()转换或者dict()转换将对象转换成相应的数据类型如果各个迭代器的元素个数不一致, 则返回列表长度与最短的对象相同, 多出来的部分会被舍弃, 利用*号操作符, 可以将元组解压成列表. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 | a = [1,2,3] b = [4,5,6] c = ['a','b','c','d','e','f'] zip_ab = zip(a,b) print(zip_ab) # zip object at 0x104605348 print(dict(zip_ab)) # {1: 4, 2: 5, 3: 6} # !!!注意, 一旦将zip_ab转换成dict以后, zip_ab内部就为空了!! 例如, 再次调用上面的语句: print(dict(zip_ab)) # {} # 但是zip_ab对象本身不会消失, 地址仍然不变 print(zip_ab) # zip object at 0x104605348 zip_abc = zip(a,b,c) # 注意, 三个元素的zip是不能转换成dict类型的 print(zip_abc) # zip object at 0x1046054c8 print(list(zip_abc)) # [(1, 4, 'a'), (2, 5, 'b'), (3, 6, 'c')] zip_abc = zip(a,b,c) z_a, z_b, z_c = zip(*zip_abc) # 利用zip(*)可以将zip对象重新解压, 返回类型是元组 print(z_a) # (1,2,3) print(z_b) # (4,5,6) print(z_c) # ('a','b','c') —|— getattr() 函数getattr()函数用于返回一个对象的属性值, 语法如下 1 | getattr(object, name[, default]) —|— 参数： object: 对象 name: 字符串, 对象属性 default: 默认返回值, 如果不提供该参数, 在没有对应属性时, 将触发Attributerror实例 1 2 3 4 5 6 7 8 9 10 11 12 13 | class A(object): ... bar = 1 ... a = A() getattr(a, 'bar') # 获取属性 bar 值 1 getattr(a, 'bar2') # 属性 bar2 不存在，触发异常 Traceback (most recent call last): File \"stdin\", line 1, in module AttributeError: 'A' object has no attribute 'bar2' getattr(a, 'bar2', 3) # 属性 bar2 不存在，但设置了默认值 3 —|— dir() 函数可以查看某个类的所有方法和属性 1 | members = [attr for attr in dir(classA)] —|— _var: 在一个模块中以单下划线开头的变量和函数会被默认当做内部函数, 在使用from a_module import * 导入时, 这部分变量和函数不会被导入. 不过如果使用import a_module导入模块时, 仍然可以用a_module._var的形式访问该变量或函数 var_: 有时候, 一个变量的最适合的名称已经被另一个关键字所占用. 在这种情况下, 可以在名称的末尾附加一个下划线来解决冲突. __var: 双下划线前缀会导致Python解释器重写属性名称, 以避免子类中的命名冲突. 举例来说, 如果在class Test中有一个成员__x, 那么当利用内置函数dir(Test)来查看类的属性时, 会发现__x被解释器重命名为_Test__x. 双下划线的名称修饰同样也适用于方法名称. var: 双下划线开头和结尾的是一些 Python 的特殊对象, 如类成员的 init, del, name, call 等. Python 官方推荐永远不要讲这样的命名方式应用于自己的变量或函数. 有一种说法是说双下划线建议为类的私有成员, 但是 PEP8 当前的官方版本中并没有明说. _: 有时候我们会用一个独立的下划线作为一个名字, 这通常是用来指示某个变量时临时的或者无关紧要的. 类的特殊方法call()在 Python 中, 函数实际上也是一个对象: 1 2 3 | f = abs print(f.__name__) # 'abs' print(f(-123)) # 123 —|— 从上面可以看出, 函数是一个对象, 当它赋给另一个变量时, 该变量也是一个函数对象, 可以起到与原函数相同的效果. 在 Python 中, 一个类实例也可以变成一个可调用对象, 只需要实现一个特殊方法 call() 即可. 下面我们举例把 Person 类变成一个可调用对象: 1 2 3 4 5 6 7 8 9 | class Person(object): def __init__(self, name, gender): self.name = name self.gender = gender def __call__(self, friend): print(\"name:\", self.name) print(\"friend:\", friend) —|— 接下来我们就可以将 Person 类的实例对象当做一个函数来使用, 如下所示: 1 2 3 4 | p = Person('Bob', 'male') p('Tim') # name: Bob # friend: Tim —|— getitem()凡是在类中定义了 getitem() 方法, 那么它的实例对象就是可以通过 [] 操作符来访问指定的成员或进行特定的行为, 大多数情况下会将该方法实现成通过索引来方法元素的形式. 1 2 3 4 5 6 7 | class DataBase(object): def __init__(self): super(DataBase, self).__init__() self.vals = [1,2,3,4,5] def __getitem__(self, key): return self.vals[key] —|— setitem()使得可以通过 A[3] = 4, B[“a”] = 5 等方式来对类中的元素进行赋值 file()查看模块的路径 len()使得类对象可以使用 Python 的内建方法 len(), 返回你自定义的数值. 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | class DictDemo: def __init__(self,key,value): self.dict = {} self.dict[key] = value def __getitem__(self,key): return self.dict[key] def __setitem__(self,key,value): self.dict[key] = value def __len__(self): return len(self.dict) dictDemo = DictDemo('key0','value0') print(dictDemo['key0']) #value0 dictDemo['key1'] = 'value1' print(dictDemo['key1']) #value1 print(len(dictDemo)) #2 —|— repr()1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 | class Test(object): def __init__(self, value='hello, world!'): self.data = value t = Test() t __main__.Test at 0x7fa91c307190 print t __main__.Test object at 0x7fa91c307190 # 看到了么？上面打印类对象并不是很友好，显示的是对象的内存地址 # 下面我们重构下该类的__repr__以及__str__，看看它们俩有啥区别 # 重构__repr__ class TestRepr(Test): def __repr__(self): return 'TestRepr(%s)' % self.data tr = TestRepr() tr TestRepr(hello, world!) print tr TestRepr(hello, world!) # 重构__repr__方法后，不管直接输出对象还是通过print打印的信息都按我们__repr__方法中定义的格式进行显示了 # 重构__str__ calss TestStr(Test): def __str__(self): return '[Value: %s]' % self.data ts = TestStr() ts __main__.TestStr at 0x7fa91c314e50 print ts [Value: hello, world!] # 你会发现，直接输出对象ts时并没有按我们__str__方法中定义的格式进行输出，而用print输出的信息却改变了 —|— str()参见 repr() 代码示例 星号 **: 乘法**: 乘幂 用于函数参数单星号: 将所有参数以 元组(tuple) 的形式导入 1 2 3 4 5 6 | def foo(param1, *param2): print(param1) print(param2) foo(1,2,3,4,5) # 1 # (2,3,4,5) —|— 双星号: 将所有参数以 字典 的形式导入 1 2 3 4 5 6 | def bar(param1, **param2): print(param1) print(param2) bar(1, a=2, b=3) # 1 # {'a': 2, 'b': 3} —|— 当然这两个用法可以同时出现在一个函数中: 1 2 3 4 5 6 7 8 9 10 | def fun(a, b=10, *args, **kwargs): print(a) print(b) print(args) print(kwargs) fun(1,2,3,4,e=5,f=6) # 1 # 2 # (3,4) # {'e': 5, 'f': 6} —|— globals() 函数该函数会以字典类型返回当前位置的全部全局变量 stripe()readlines()lambda 函数3.6新功能 f string包的导入机制模块和包的定义模块(module): 用来从逻辑上组织 Python 代码(变量, 函数, 类), 通常是一个.py文件.包(package): 定义了一个由模块和子包组成的 Python 应用程序执行环境, 本质上就是一个有层次的文件目录结果(必须带有一个__init__.py文件) import 的搜索路径 在当前目录下搜索 在环境变量PYTHONPATH中指定的路径列表中搜索 在 Python 安装路径的lib库中搜索Python 所有加载的模型信息都存放在sys.modules结构中, 当import一个模块时, 会按如下步骤来进行: 如果import A, 检查sys.modules中是否已经有A, 如果有则不加载, 如果没有则为A创建module对象, 并加载A; 如果是from A import B, 先为A创建module对象, 再解析A(此时会加载并执行A中的所有代码), 从中寻找B并填充到A的__dict__中.在导入模块的时候, 模块所在文件夹会自动生成一个__pycache__/module_name.cpython-35.pyc的文件. 1 2 3 4 | import module_name的本质是将module_name.py中的全部代码加载到内存中, 并将其赋值给与模块同名的变量, 这个变量的类型是classmodule. from module_name import name的本质是将指定的变量或者方法导入到当前的文件中 import package_name的本质是执行该包下的__init__.py文件, 在执行文件后, 会在package_name目录下生成一个__pycache__/__init__cpython-35.pyc文件. from package_name import *的本质是导入__init__.py文件中的__all__列表(eg. __all__ = ['L2Norm', 'MultiBoxLoss']). —|— 相对导入和绝对导入绝对导入: 1 2 | import A.B from A import B —|— 相对导入: 1 2 | from . import B # . 代表当前路径 from ..A import B # .. 代表上层路径, ... 代表上上层路径. —|— 在没有明确指定包结构的情况下, Python 是根据__name__来决定一个模块在包中的结构的, 如果是__main__, 则它本身就是顶层模块, 没有包结构, 如果是A.B.C结构, 则A是顶层模块. Python 的导入方式的不同具有不同的规则: 1.如果是绝对导入, 一个模块只能导入自身的子模块或者和它的顶层模块同级别的模块及其子模块.2.如果是相对导入, 一个模块必须有包结构且只能导入它的顶层模块内部的模块. 如果一个模块被直接运行, 则它自己为顶层模块, 不存在层次结构, 所以也找不到上层(..)的相对路径Python2.x 默认为相对路径导入, 而 Python3.x 默认为绝对路径导入, 这样可以避免导入的子包覆盖掉标准库模块. 通常, 在 Python2.x 中, 我们利用下面的语句来使其导入规则遵循 Python3.x 1 | from __future__ import absolute_import —|— absolute_import的意思并不是将所有的导入都视为绝对导入, 而是指禁用隐式相对导入(implicit relative import), 关于隐式的显示的具体区别, 可以看下面的例子, 假设有如下的包结构: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 | thing └── __init__.py ├── books │ ├── __init__.py │ ├── adventure.py │ ├── history.py │ ├── horror.py │ └── lovestory.py ├── furniture │ ├── __init__.py │ ├── armchair.py │ ├── bench.py │ ├── screen.py │ └── stool.py —|— 那么如果想在stool.py中导入bench模块, 则有如下几种方式: 1 2 3 | import bench # 隐式相对导入 from . import bench # 显式相对导入 from furniture import bench # 绝对导入 —|— 隐式相对导入没有告诉解释器相对于谁进行导入, 默认相对于当前模块; 而显式相对导入则明确告诉了解释器相对于谁来导入. 以上导入方式的第三种是官方推荐的, 第一种是官方强烈不推荐的, Python3 中第一种导入方式只能用于导入sys.path中的模块.**注意, 还有相对导入的模块不能被直接运行, 会提示如下错误: 1 2 3 4 | Traceback (most recent call last): File \"test.py\", line 8, in module from .ssd import SSD ModuleNotFoundError: No module named '__main__.ssd'; '__main__' is not a package —|— 另外存在一种情况就是: 假如有两个模块a.py和b.py放在同一个目录下, 则可以直接在a.py中使用import b来导入模块b. 这是为什么呢? 我们上面说了在 Python3.x 中不能使用这种隐式相对导入, 但是这里却可以成功导入, 这是因为此时我们是直接运行a.py, 所以a.py和b.py的目录没有被当做一个包来处理, 因此不涉及相对导入和绝对导入的概念. 因此相对导入和绝对导入仅仅是针对于包而言的. 综合距离存在目录结构如下所示: 1 2 3 4 5 6 7 8 9 10 11 12 | dirRoot └── __init__.py ├── file1.py ├── file2.py ├── dirA │ ├── __init__.py │ ├── a1.py │ └── a2.py ├── dirB │ ├── __init__.py │ ├── b1.py │ └── b2.py —|— 直接运行a1.py, 并希望导入a2模块: 1 2 3 4 | # a1.py import a2 # 正确, 此时并未将 dirA 当做包来处理, a1.py 和 a2.py 相当于两个独立的模块 from a2 import func_a2 # 正确 from .a2 import func_a2 # 错误, 当进行相对导入时, 不能直接运行 —|— 直接运行file1.py, 并希望导入a1模块, 同时a1模块中需要导入a2模块: 1 2 3 4 5 6 7 8 9 10 | # file1.py from dirA import a1 a1.func_a1() # a1.py 中的函数 a1.func_a2() # a1.py 中导入了 a2.py 的函数, 可以直接使用 # a1.py import a2 # 错误, 此时由于 dirA 中有 __init__.py 文件, 因此会将 dirA 当做包来处理, # 由于 Python3.x 不允许使用隐式的相对导入, 因此该语句非法 from a2 import func_a2 # 错误, 原因同上 from .a2 import func_a2 # 正确, 当进行相对导入时, 需要使用显式的相对导入 —|— 直接运行file1.py, 并希望导入a1模块, 同时a1模块中需要导入dirB/b1模块(跨文件夹导入): 1 2 3 4 5 6 7 8 9 10 | # file1.py from dirA import a1 a1.func_a1() # a1.py 中的函数 a1.func_a2() # a2.py 中的函数 a1.func_b1() # b1.py 中的函数 # a1.py from .a2 import func_a2 # 推荐使用绝对导入 from dirA.a1 import func_a2 from dirB import b1 # 由于运行的是 file1.py 文件, 因此顶层目录是 dirRoot from dirB.b1 import func_b1 # 所以可以直接使用 dirB 包 —|— 直接运行a1.py, 并希望跨目录的导入dirB/b1模块. 由于这种跨目录的导入超越了顶层路径的限制, 因此必须使用sys.path.append()方法来额外添加搜索路径, 否则无法正常导 1 2 3 4 5 6 | # a1.py import sys sys.path.append(\"../\") # 将 dirA 的上一次目录添加到搜索路径中 from dirB import b1 # 正确, 注意必须先添加 path, 然后再导入 from dirB.b1 import func_b1 # 正确 from .a2 import func_a2 # 这里是错误的, 当直接执行 a1.py 时, a1.py 中不能包含显式相对导入 —|— 获取 python 版本:1 | print(sys.version_info) —|— 获取包的安装位置1 | print(cv2) —|— 解析 xml 文件导入: 1 2 3 4 5 | import sys if sys.version_info[0] == 2: import xml.etree.cElementTree as ET else: import xml.etree.ElementTree as ET —|— 解析: 1 2 3 4 5 6 7 8 9 10 11 12 13 | xmlfile = ET.parse(xmlfile_path) root = xmlfile.getroot() # 获取根节点 root.tag # 标签 root.attrib # 属性字典 for child in root: # 迭代访问子节点 print(child.tag, child.attrib) # 可以通过索引访问嵌套节点的内容 root[0][1].text Element.findall() # Element.find() # —|— python 中 == 和 is 的区别== 只用于判断值是否相等is 用于判断两个对象是否为同一个实例小整数对象池: Python 为了优化速度，使用了小整数对象池，避免为整数频繁申请和销毁内存空间。而Python 对小整数的定义是 [-5, 257)，只有数字在-5到256之间它们的id才会相等，超过了这个范围就不行了，同样的道理，字符串对象也有一个类似的缓冲池，超过区间范围内自然不会相等了 队列 queue在 Python3 中, 原来的Queue模块被重命名为queue, 该模块包含以下三类数据结构: queue.Queue(maxsize=0): FIFO queue, 先进先出队列, 代表普通队列 queue.LifoQueue(maxsize=0): LIFO queue, 后进先出队列, 类似栈的作用 queue.PriorityQueue(maxsize=0): 优先级队列, 类似堆的作用. 默认为小顶堆, 常用形式为元组:(priority_number, data)上面的 maxsize 表明了队列中最大可以容纳的元素数量, 如果超过, 则无法插入. 当 maxsize = 0 时, 代表元素数量无限制.公有方法(以上三个通用): qsize(): 返回 approximate size, qsize() 0 不保证get()一定 work, 同理, qsize() maxsize 不保证put()一定 work. empty(): 如果队列为空, 返回 True. 和qsize()一样, 不提供保证性. full(): 如果队列满, 返回 True. 不提供保证性 put(item[, block[, timeout]]) put_nowait(item): 等价于put(item, False) get([block[, timeout]]) get_nowait(): 等价于get(False) task_done(): join(): 堆 heapqheapq 模块只有最小堆的功能, 要实现最大堆, 需要在入堆和出堆的时候取反, 并且 heapq 模块只能作用于数值型类型.最大堆: _heapify_max(), _heappop_max() 给定一组数据, 创建堆, 两种方式(二者等价): 1 2 3 4 5 6 7 | import heapq data = [1,3,6,2,8,5] heap = [] for d in data: heapq.heappush(heap, n) # 方法一 逐个构建 heapq.heapify(data) # 方法二 原地构建, 效率更高 —|— 小顶堆: 1 2 3 4 5 6 | heap = [1,3,6,2,8,5] heapq.heapify(heap) heapq.heappop(heap) # 返回并删除堆顶 heapq.heapreplace(heap, 10) # 删除堆顶并添加新值 heapq.heappushpop(heap, 10) # 先将新值加入堆中, 然后立刻弹出堆顶 print(heap[0]) # 查看堆顶 —|— 大顶堆: 1 2 3 4 5 6 7 8 9 10 11 12 | # 方法一: 取负值 heap = [-1,-3,-6,-2,-8,-5] # 方法二: 内置方法 heap = [1,3,6,2,8,5] heapq._heapify_max(heap) # max_heap print(heap[0]) # 查看堆顶, 8 heapq._heappop_max(heap) # po from maxheap print(heap[0]) # 6 heapq._heapreplace_max(heap, 10) print(heap[0]) # 10 # heapq._heappushpop_max(heap, 10) # 注意, 没有 _heappushpop_max 函数 —|— Python 刷题常用队列: 1 2 3 4 5 6 7 8 9 10 11 12 | import Queue base_queue = Queue.Queue() # 基本队列, 先进先出 base_queue.put(x) base_queue.get() lifo_queue = Queue.LifoQueue() # 先进后出, 类似栈 lifo_queue.put(x) lifo_queue.get() prio_queue = Queue.PriorityQueue() # 优先队列, 与C++中priority_queue类似, 可实现堆的功能 prio_queue.put(x) prio_queue.get() —|— numpy 中vstack, hstack, concatenate 和 stack 之间的区别和联系concatenate1 | numpy.concatenate((a1, a2, ...), axis=0, out=None) —|— concatenate 的作用就是将多个数组序列按照axis指定的维度连接起来, 这些数组序列 a1, a2, … 必须保证 除了 axis 指定维度之外的其他维度具有相同的 shape. 注意: 这里的维度指的是a1, a2的维度, 而不是(a1, a2)的维度 从维度角度来更好理解 concatenate 的作用concatenate 执行后的 shape 特定是: axis 指定的维度是多个数组序列对应维度的数值和, 而其他维度保持不变. 也就是说不会增加新的维度, 这是 concatenate 与 stack 之间的一个重要的区别. 如下所示: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 | import numpy as np a1 = np.array([[1, 1], [2, 2], [3, 3]]) # shape = 3x2 a2 = np.array([[1, 1], [2, 2]]) # shape = 2 x 2 print(a1.shape, a2.shape) concat1 = np.concatenate((a1, a2), axis=0) print(concat1.shape) # shape 为 [5, 2], 在 0 维度上为 3+2, 其他维度保持不变 print(concat1) # a1, a2 维度 0 不同, 一个为 3, 一个为 2, 其他维度相同, 均为 2 #[[1 1] # [2 2] # [3 3] # [1 1] # [2 2]] #print(np.concatenate((a1, a2), axis=1)) # 由于维度 0 二者不同, 无法保持不变, 因此报错 a1 = np.array([[1, 2, 3]]) # shape = 1x3 a2 = np.array([[1, 2]]) # shape = 1x2 print(a1.shape, a2.shape) concat2 = np.concatenate((a1, a2), axis=1) print(concat2.shape) # shape 为 [1, 5]在 1 维度上为 3 + 2, 0 维度上保持 1 不变 print(concat2) # [[1 2 3 1 2]] # print(np.concatenate((a1, a2), axis=0)) # 维度 1 不同, 报错 —|— 有时候, concatenate的第一个参数只会传送一个一个数组序列, 这时候, 等价于将这个数组序列的第一维的元素看做是多个数组序列作为concatenate的参数进行传递. 如下所示: 1 2 3 4 5 6 7 8 9 | a = [[1, 2, 3], [1, 2, 3]] print(np.concatenate(a, axis=0)) # 该行与下一行等价 print(np.concatenate((a[0], a[1]), axis=0)) a = [[1, 2, 3], [1, 2]] print(np.concatenate(a, axis=0)) # 可以看出, 虽然 a 的第一维度为 2, 第二维度为 3 和 2 # 但是, 我们要将其拆分, 拆分后, a[0], a[1] 的第一维度3和2, 其他维度相同, 因此可以在第一维度上进行连接 print(np.concatenate((a[0], a[1]), axis=0)) —|— stack1 2 | numpy.stack(arrays, axis=0, out=None) numpy.stack((a1, a2, ...), axis=0, out=None) —|— stack 的作用就是将多个数组序列按照axis指定的维度 堆叠 起来, 这些数组序列 a1, a2, … 必须保证 所有维度都相同, 注意这里与 concatenate 的区别. 要更好的理解stack, 可以借助 维度 的概念进行理解, 对于 shape 相同的 k 个数组序列来说, stack 的作用相当于新插入一个维度, 维度的大小为 k, 插入的位置为axis指定的位置. 如下所示: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | a1 = [[1, 1], [2, 2], [3, 3]] # shape = 3x2 a2 = [[4, 4], [5, 5], [6, 6]] # shape = 3x2 a3 = [[7, 7], [8, 8], [9, 9]] # shape = 3x2 a4 = [[0, 0], [0, 0], [0, 0]] # shape = 3x2 stack1 = np.stack((a1, a2, a3, a4), axis=0) # 新插入维度大小为 4, 位置为第 0 维 print(stack1.shape) # shape 为 (4, 3, 2) print('### ', stack1) # 先将 shape 画好, 然后进行填充, 在第 0 维上进行堆叠, 因此 stack1[*][*] = a1[0], a1[1], ..., a4[2] stack2 = np.stack((a1, a2, a3, a4), axis=1) # 新插入维度大小为 4, 位置为第 1 维 print(stack2.shape) # shape 为 (3, 4, 2) print('### ', stack2) # 在第 1 维上进行堆叠, 因此 stack2[*][*] = a1[0], a2[0], a3[0], a1[1], ... stack3 = np.stack((a1, a2, a3, a4), axis=2) # 新插入维度大小为 4, 位置为第 2 维 print(stack3.shape) # shape 为 (3, 2, 4) print('### ', stack3) # 在第 2 维上进行堆叠, 因此 stack2[*][*] = [1 4 7 0], [1 4 7 0], [2 5 8 0], ... —|— hstack 和 vstackhstack 和 vstack 虽然名字中都带有 stack, 但是实际上, 它们和np.stack的关系并不大, 一个明显的区别就是np.stack要求进行堆叠的多个数组序列需要保证 shape 完全相同, 并且堆叠后会新增加一个由axis指定的维度. 实际上, hstack 和 vstack 可以看做是特殊的 concatenate, 它们在某些情况下可以用 concatenate 来代替 既然 hstack 和 vstack 是特殊的 concatenate, 也就是说, 它们所接受的多个数组序列在axis指定的维度上可以不同, 而在其他维度上必须相同. vstack: 在垂直方向上将多个数组序列进行堆叠, 相当于在axis=0维度上执行concatenatehstack: 在水平方向上将多个数组序列进行堆叠, 相当于在axis=1维度上执行concatenate 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 | a = [[1, 1], [2, 2], [3, 3]] # shape = 3x2 b = [[4, 4], [5, 5], [6, 6]] # shape = 3x2 c = [[7, 7], [8, 8], [9, 9]] # shape = 3x2 d = [[0, 0], [0, 0], [0, 0]] # shape = 3x2 v = np.vstack((a, b, c, d)) print(v.shape) # (12, 2) print(v) x = np.concatenate((a, b, c, d), axis = 0) # 等价于 vstack print(x.shape) # 12, 2 print(x) h = np.hstack((a, b, c, d)) print(h.shape) # (3, 8) print(h) x = np.concatenate((a, b, c, d), axis = 1) # 等价于 hstack print(x.shape) # 3, 8 print(x) —|— 需要特别注意, 当多个数组序列是一维数组时, 应该先将一维数组转换成二维数组, 然后才能与相应的 concatenate 进行等价. 这是因为, 在数组序列是一维数组时, concatenate 是无法使用axis=1的, 因此此时的 hstack 相当于是在axis=0上进行 concatenate, 而 vstack 则需要先将数组的 shape 从 (N,) 转换成 (1, N) 后才相当于是在axis=1上进行 concatenate 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 | a = np.array([1, 2, 3, 4, 5]) # 当面对的是一维数组时, b = np.array([6, 7, 8, 9, 10]) h = np.hstack((a, b)) print(h.shape) print(h) con = np.concatenate((a, b), axis=0) # 当 a, b 是一维数组时, hstack 相当于在 axis=0 上进行连接 print(con.shape) print(con) v = np.vstack((a, b)) print(v.shape) print(v) con = np.concatenate(([a], [b]), axis=0) # 当 a, b 是一维数组时, vstack 相当于将 a, b 先转换成二维 (1, N), 然后在 axis=0 上进行连接 print(con.shape) print(con) —|— set 去重对于二维列表, 由于 list 的元素也是 list, 在内存中存储的是首元素地址, 无法直接使用 set, 因此需要先将内部的元素全部全换成 tuple 后, 才能使用 list 去重. 如下所示 1 2 3 4 5 6 7 8 | a = list() a.append([1,2,3]) a.append([1,2,3]) a.append([1,2,3]) a.append([4, 5, 6]) # b = set(a) # 报错 b = set(map(tuple, a)) print(b) # {(4, 5, 6), (1, 2, 3)} —|— os.sep用法ython是跨平台的。在Windows上，文件的路径分隔符是’\\’，在Linux上是’/‘。 为了让代码在不同的平台上都能运行，那么路径应该写’\\’还是’/‘呢？ 使用os.sep的话，就不用考虑这个了，os.sep根据你所处的平台，自动采用相应的分隔符号。 举例 Linux下一个路径，/usr/share/python,那么上面的os.sep就是‘/’windows下一个路径，C：\\Users\\Public\\Desktop,那么上面的os.sep就是‘\\’. 1 | data_dir = os.sep.join(['hello', 'world']) —|— Python3 元组Python元组包含了以下内置函数 len(tuple) 计算元组元素个数。 1 2 3 4 | tuple1 = ('Google', 'Runoob', 'Taobao') len(tuple1) 3 —|— max(tuple) 返回元组中元素最大值。 1 2 3 4 | tuple2 = ('5', '4', '8') max(tuple2) '8' —|— min(tuple) 返回元组中元素最小值。 1 2 3 4 | tuple2 = ('5', '4', '8') min(tuple2) '4' —|— tuple(seq) 将列表转换为元组。 1 2 3 4 | list1= ['Google', 'Taobao', 'Runoob', 'Baidu'] tuple1=tuple(list1) tuple1 ('Google', 'Taobao', 'Runoob', 'Baidu') —|— 序列化Python对象你需要将一个Python对象序列化为一个字节流，以便将它保存到一个文件、存储到数据库或者通过网络传输它。对于序列化最普遍的做法就是使用 pickle 模块。为了将一个对象保存到一个文件中，可以这样做 pickle 对于大型的数据结构比如使用 array 或 numpy 模块创建的二进制数组效率并不是一个高效的编码方式。 如果你需要移动大量的数组数据，你最好是先在一个文件中将其保存为数组数据块或使用更高级的标准编码方式如HDF5 (需要第三方库的支持)。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 | In [1]: import pickle In [2]: obj = 123,\"abcdef\", [\"ac\", 123], {\"key\": \"value\", \"key1\": \"value1\"} In [3]: print(obj) (123, 'abcdef', ['ac', 123], {'key': 'value', 'key1': 'value1'}) In [4]: # 序列化到文件 In [5]: with open(r'./a.pickle','wb') as f: ...: pickle.dump(obj,f) ...: In [6]: with open(r'./a.pickle','rb') as f: ...: aa= pickle.load(f) ...: print(aa) ...: ...: (123, 'abcdef', ['ac', 123], {'key': 'value', 'key1': 'value1'}) —|— 参考链接：https://hellozhaozheng.github.iohttps://www.runoob.com/python/python-tutorial.html","tags":["Python"],"categories":["算法"]},{"title":"shutil模块","path":"/2019/08/28/shutil模块/","content":"1 2 3 4 5 6 7 | shutil.copyfile(\"old\",\"new\") # 复制文件，都只能是文件 shutil.copytree(\"old\",\"new\") # 复制文件夹，都只能是目录，且new必须不存在 shutil.copy(\"old\",\"new\") # 复制文件/文件夹，复制 old 为 new（new是文件，若不存在，即新建），复制 old 为至 new 文件夹（文件夹已存在） shutil.move(\"old\",\"new\") # 移动文件/文件夹至 new 文件夹中 —|—","tags":["编程"],"categories":["Python"]},{"title":"skimage模块","path":"/2019/08/28/skimage模块/","content":"比opencv的速度要慢很多, 但是使用起来更加简单, 真的对速度要求很高的话, 一般都会C++和opecv使用. 所以一般情况下, 首先看skimage能否实现, 不行的话再转用opencv 1 2 3 | import skimage from skimage import io # IO is a submodule. Submodules need to be imported from the parent module explicitly. img = io.imread(\"1.jpg\") —|—","tags":["OpenCV"],"categories":["计算机视觉"]},{"title":"从上往下打印二叉树","path":"/2019/08/28/从上往下打印二叉树/","content":"时间限制：1秒 空间限制：32768K 热度指数：420679本题知识点： 队列 树 题目描述从上往下打印出二叉树的每个节点，同层节点从左至右打印。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 | /* struct TreeNode { int val; struct TreeNode *left; struct TreeNode *right; TreeNode(int x) : val(x), left(NULL), right(NULL) { } };*/ class Solution { public: vectorint PrintFromTopToBottom(TreeNode* root) { //队列是先进先出 queueTreeNode* que; vectorint vec; que.push(root);//先将整个二叉树放入队列 while(!que.empty()) //当队列非空进行循环 { TreeNode* p; p = que.front();//先读取队列的首元素 que.pop();//弹出队列的首元素 if(p == NULL) continue;//所有元素存入vec后，由于队列中存放着空指针，依然进入循环，但此时p的值为NULL，不执行下面的操作，跳出循环结束 que.push(p-left); que.push(p-right); vec.push_back(p-val); } return vec; } }; —|— 运行时间：3ms占用内存：464k","tags":["编程"],"categories":["算法"]},{"title":"二维数组中的查找","path":"/2019/08/27/20190827-二维数组中的查找/","content":"题目描述在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 时间限制：1秒 空间限制：32768K 热度指数：1323359 题目解析已知从左到右、从上导线都呈递增关系，令行row为0，令列col为第一行最后一位，然后将target与第一行最后一个数进行比较，若大于这个数则，行数++，若小于这个数，则列数向前递减。题目的要求是 当目标数target既不大于也不小于array[row][col]，认为target==array[row][col]，返回true。其他的情况则返回false。注： 在 c++中获取vector数组的行数和列数的代码与java不一样 c++:int col = array[0].size()-1;int row = array.size();java:int col = array[0].length - 1;int array_len = array.length; c++代码 如下： class Solution {public: //行数： //example.length // 列数： //example[0].length //第0行的列数 bool Find(int target, vectorvectorint array) { if(array.size()==0) { return false; } int row = 0; int col = array[0].size()-1; int array_len = array.size(); while(row array_len col=0 ) { if(target array[row][col]) row++; else if(target array[row][col]) col--; else return true; } return false; }}; 运行时间：11ms占用内存：1500k","tags":["算法"],"categories":["算法"]},{"title":"二维数组中的查找","path":"/2019/08/27/二维数组中的查找/","content":"题目描述在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。 时间限制：1秒 空间限制：32768K 热度指数：1323359 题目解析已知从左到右、从上导线都呈递增关系，令行row为0，令列col为第一行最后一位，然后将target与第一行最后一个数进行比较，若大于这个数则，行数++，若小于这个数，则列数向前递减。题目的要求是 当目标数target既不大于也不小于array[row][col]，认为target==array[row][col]，返回true。其他的情况则返回false。注： 在 c++中获取vector数组的行数和列数的代码与java不一样 1 2 3 4 5 6 | c++: int col = array[0].size()-1; int row = array.size(); java: int col = array[0].length - 1; int array_len = array.length; —|— c++代码 如下： 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 | class Solution { public: //行数： //example.length // 列数： //example[0].length //第0行的列数 bool Find(int target, vectorvectorint array) { if(array.size()==0) { return false; } int row = 0; int col = array[0].size()-1; int array_len = array.size(); while(row array_len col=0 ) { if(target array[row][col]) row++; else if(target array[row][col]) col--; else return true; } return false; } }; —|— 运行时间：11ms占用内存：1500k","tags":["C++"],"categories":["算法"]},{"title":"仿射变换","path":"/2019/08/26/20190826-仿射变换/","content":"欧式变换包括平移和旋转变换 什么是仿射变换？ 简单的来说“仿射变换” 就是：“线性变换”+平移尺度变换包括相似变换，当x与y变换的尺度相等的时候，叫做相似变换什么是线性变换？ 1 线性变换 线性变换从几何直观有三个 要点： 变换前是直线的，变换后依旧是直线 直线比例保持不变 变换前是原点，变换后依然是原点 2 仿射变换仿射变换从几何直观只有两个要点： 变换前是直线的，变换后依然是直线 直线比例保持不变 与线性变换相比少了原点保持不变这一条","tags":["计算机视觉"],"categories":["计算机视觉"]},{"title":"仿射变换","path":"/2019/08/26/仿射变换/","content":"欧式变换包括平移和旋转变换 什么是仿射变换？ 简单的来说“仿射变换” 就是：“线性变换”+平移尺度变换包括相似变换，当x与y变换的尺度相等的时候，叫做相似变换什么是线性变换？ 1 线性变换 线性变换从几何直观有三个 要点： 变换前是直线的，变换后依旧是直线 直线比例保持不变 变换前是原点，变换后依然是原点 2 仿射变换仿射变换从几何直观只有两个要点： 变换前是直线的，变换后依然是直线 直线比例保持不变 与线性变换相比少了原点保持不变这一条","tags":["编程"],"categories":["计算机视觉"]},{"title":"Matplotlib绘制六种可视化图表","path":"/2019/08/24/20190824-pythonplot/","content":"01 折线图import numpy as npimport matplotlib.pyplot as pltx = np.linspace(0,2,100)plt.plot(x, x, label='linear')plt.plot(x, x**2, label='quadratic')plt.plot(x, x**3, label='cubic')plt.xlabel('x label')plt.ylabel('y label')plt.title(\"Simple Plot\")plt.legend()plt.show() 02 散点图import numpy as npimport matplotlib.pyplot as pltx = np.arange(0., 5., 0.2)# 红色破折号, 蓝色方块 ，绿色三角块plt.plot(x, x, 'r--', x, x**2, 'bs', x, x**3, 'g^')plt.show() 03 直方图import numpy as npimport matplotlib.pyplot as pltnp.random.seed(19680801)mu1, sigma1 = 100, 15mu2, sigma2 = 80, 15x1 = mu1 + sigma1 * np.random.randn(10000)x2 = mu2 + sigma2 * np.random.randn(10000)# the histogram of the data# 50：将数据分成50组# facecolor：颜色；alpha：透明度# density：是密度而不是具体数值n1, bins1, patches1 = plt.hist(x1, 50, density=True, facecolor='g', alpha=1)n2, bins2, patches2 = plt.hist(x2, 50, density=True, facecolor='r', alpha=0.2)# n：概率值；bins：具体数值；patches：直方图对象。plt.xlabel('Smarts')plt.ylabel('Probability')plt.title('Histogram of IQ')plt.text(110, .025, r'$\\mu=100,\\ \\sigma=15$')plt.text(50, .025, r'$\\mu=80,\\ \\sigma=15$')# 设置x，y轴的具体范围plt.axis([40, 160, 0, 0.03])plt.grid(True)plt.show() 04 柱状图4.1 并列柱状图import numpy as npimport matplotlib.pyplot as pltsize = 5a = np.random.random(size)b = np.random.random(size)c = np.random.random(size)x = np.arange(size)# 有多少个类型，只需更改n即可total_width, n = 0.8, 3width = total_width / n# 重新拟定x的坐标x = x - (total_width - width) / 2# 这里使用的是偏移plt.bar(x, a, width=width, label='a')plt.bar(x + width, b, width=width, label='b')plt.bar(x + 2 * width, c, width=width, label='c')plt.legend()plt.show() 4.2 叠加柱状图import numpy as npimport matplotlib.pyplot as pltsize = 5a = np.random.random(size)b = np.random.random(size)c = np.random.random(size)x = np.arange(size)# 这里使用的是偏移plt.bar(x, a, width=0.5, label='a',fc='r')plt.bar(x, b, bottom=a, width=0.5, label='b', fc='g')plt.bar(x, c, bottom=a+b, width=0.5, label='c', fc='b')plt.ylim(0, 2.5)plt.legend()plt.grid(True)plt.show() 05 饼图5.1 普通饼图import matplotlib.pyplot as pltlabels = 'Frogs', 'Hogs', 'Dogs', 'Logs'sizes = [15, 30, 45, 10]# 设置分离的距离，0表示不分离explode = (0, 0.1, 0, 0)plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90)# Equal aspect ratio 保证画出的图是正圆形plt.axis('equal')plt.show() 5.2 嵌套饼图import numpy as npimport matplotlib.pyplot as plt# 设置每环的宽度size = 0.3vals = np.array([[60., 32.], [37., 40.], [29., 10.]])# 通过get_cmap随机获取颜色cmap = plt.get_cmap(\"tab20c\")outer_colors = cmap(np.arange(3)*4)inner_colors = cmap(np.array([1, 2, 5, 6, 9, 10]))print(vals.sum(axis=1))# [92. 77. 39.]plt.pie(vals.sum(axis=1), radius=1, colors=outer_colors, wedgeprops=dict(width=size, edgecolor='w'))print(vals.flatten())# [60. 32. 37. 40. 29. 10.]plt.pie(vals.flatten(), radius=1-size, colors=inner_colors, wedgeprops=dict(width=size, edgecolor='w'))# equal 使得为正圆plt.axis('equal')plt.show() 5.3 极轴饼图import numpy as npimport matplotlib.pyplot as pltnp.random.seed(19680801)N = 10theta = np.linspace(0.0, 2 * np.pi, N, endpoint=False)radii = 10 * np.random.rand(N)width = np.pi / 4 * np.random.rand(N)ax = plt.subplot(111, projection='polar')bars = ax.bar(theta, radii, width=width, bottom=0.0)# left表示从哪开始，# radii表示从中心点向边缘绘制的长度（半径）# width表示末端的弧长# 自定义颜色和不透明度for r, bar in zip(radii, bars): bar.set_facecolor(plt.cm.viridis(r / 10.)) bar.set_alpha(0.5)plt.show() 06 三维图6.1 绘制三维散点图import numpy as npimport matplotlib.pyplot as pltfrom mpl_toolkits.mplot3d import Axes3Ddata = np.random.randint(0, 255, size=[40, 40, 40])x, y, z = data[0], data[1], data[2]ax = plt.subplot(111, projection='3d') # 创建一个三维的绘图工程# 将数据点分成三部分画，在颜色上有区分度ax.scatter(x[:10], y[:10], z[:10], c='y') # 绘制数据点ax.scatter(x[10:20], y[10:20], z[10:20], c='r')ax.scatter(x[30:40], y[30:40], z[30:40], c='g')ax.set_zlabel('Z') # 坐标轴ax.set_ylabel('Y')ax.set_xlabel('X')plt.show() 6.2 绘制三维平面图from matplotlib import pyplot as pltimport numpy as npfrom mpl_toolkits.mplot3d import Axes3Dfig = plt.figure()ax = Axes3D(fig)X = np.arange(-4, 4, 0.25)Y = np.arange(-4, 4, 0.25)X, Y = np.meshgrid(X, Y)R = np.sqrt(X**2 + Y**2)Z = np.sin(R)# 具体函数方法可用 help(function) 查看，如：help(ax.plot_surface)ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='rainbow')plt.show() 参考链接： https://mp.weixin.qq.com/s/bMvrle-FRvli0pRNi83waQ","tags":["Python"],"categories":["Python"]},{"title":"SSD源码解析","path":"/2019/08/24/20190824-ssd/","content":"论文地址: SSD: Single Shot MultiBox Detector 非官方代码: pytorch 介绍SSD，全称Single Shot MultiBox Detector，是一种One-Stage的方法，它由Wei Liu在ECCV 2016上提出，SSD具有如下主要特点： 从YOLO中继承了将detection转化为regression的思路，同时一次即可完成网络训练 基于Faster RCNN中的anchor，提出了相似的prior box 加入基于特征金字塔（Pyramidal Feature Hierarchy）的检测方式，相当于半个FPN思路 SSD的 网络结构 由上图可以看出，SSD的基础网络结构由基础网络VGG16组成，在VGG16基础网络之后接了一个3x3的卷积和一个1x1的卷积做特征融合，然后增加了一个Extra Feature Layers 层，这个层由八个卷积层构成。SSD在前面的基础网络去conv_4_3之后的relu层输出，以及倒数第二层的conv_7_1的relu再加上Extra Feature Layers层的第1,3,5,7层 共有6个 featuremap层，在此基础上对box进行预测。但感觉要提升效果的话可以对基础网络进行更改，增加特征融合等等。 空洞卷积(Dilation Conv)ssd网络里还使用了空洞卷积(Dilation Conv),采用VGG16做基础模型，首先VGG16是在ILSVRC CLS-LOC数据集预训练。然后借鉴了DeepLab-LargeFOV，分别将VGG16的全连接层fc6和fc7转换成 3×3卷积层 conv6和 1×1 卷积层conv7，同时将池化层 pool5 由原来的 stride=2 的 2×2 变成 stride=1 的(猜想是不想reduce特征图大小)，为了配合这种变化，采用了一种 Atrous Algorithm，其实就是conv6采用扩展卷积或带孔卷积（Dilation Conv），其在不增加参数与模型复杂度的条件下指数级扩大卷积的视野，其使用扩张率(dilation rate)参数，来表示扩张的大小，如下图所示，(a)是普通的 3×3 卷积，其视野就是 3×3 ，(b)是扩张率为 1，此时视野变成 7×7 ，(c)扩张率为3时，视野扩大为 15×15 ，但是视野的特征更稀疏了。Conv6采用 3×3 大小但dilation rate=6的扩展卷积。 Prior BoxSSD中有着类似anchor机制的Prior Box机制，用于来生成先验框，后面将这些先验框与真实的gt进行匹配，然后与预测的进行回归。从而得到物体真实的 位置。SSD的prior Box 按照如下规则生成： 以feature map上每个点的中点为中心（offset=0.5），生成一些列同心的prior box（然后中心点的坐标会乘以step，相当于从feature map位置映射回原图位置） 正方形prior box最小边长为’’’pash $min_size$’’’，最大边长为：\\sqrt{min_size*max_size}E=mc^2 根据相应的aspect ratio，会生成不同个数的长方形 ，长宽为：$ \\sqrt{aspect_ratio}min_size$ 和 $1/ \\sqrt{aspect_ratio} min_size$ $f(x)=ax+b$ 最终网络生成固定数量的Prior Box 每个feature map 对应prior box的min_size 和max_size 由以下的公式决定，公式中的m是使用feature map的数量(m=6)第一层feature map对应的min_size=S1，max_size=S2；第二层min_size=S2，max_size=S3；其他类推。在原文中，Smin=0.2，Smax=0.9 | min_size | max_size | def.boxes num—|—|—|—conv4_3 | 30 | 60 | 4fc7 | 60 | 111 | 6conv6_2 | 111 | 162 | 6fc7 | 162 | 213 | 6conv4_3 | 213 | 264 | 4fc7 | 264 | 315 | 4 训练策略正负样本给定输入图像以及每个物体的Ground Truth,首先找到每个Ground True box对应的default box中IOU最大的最为正样本。然后,在剩下的default box中寻找与Ground Truth 的IOU大于0.5的default box作为正样本。一个Ground Truth可能对应多个正样本default box.其他的default box作为负样本。,为了保证样本尽量平衡,SSD采用了hard nagative mining,即对负样本进行抽样,抽样时按照置信度误差(预测背景的置信度越小,误差越大)进行奖序排列,选取误差较大的top-k作为训练的负样本,保证正负样本比例接近1:3。 目标函数目标函数为训练过程中的优化标准,目标函数也称损失函数,主要包括位置误差(localization loss,loc) 与置信度误差(confidence loss,conf,分类损失)的加权和,定义为： 代码解析基础模型定义# vgg([64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',# 512, 512, 512], 3)# This function is derived from torchvision VGG make_layers()# https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.pydef vgg(cfg, i, batch_norm=False): layers = [] in_channels = i for v in cfg: if v == 'M': layers += [nn.MaxPool2d(kernel_size=2, stride=2)] elif v == 'C': layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)] else: conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1) if batch_norm: layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)] else: layers += [conv2d, nn.ReLU(inplace=True)] in_channels = v pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6) conv7 = nn.Conv2d(1024, 1024, kernel_size=1) layers += [pool5, conv6, nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)] return layers base = { '300': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M', 512, 512, 512], '512': [],}extras = { '300': [256, 'S', 512, 128, 'S', 256, 128, 256, 128, 256], '512': [],}mbox = { '300': [4, 6, 6, 6, 4, 4], # number of boxes per feature map location '512': [],}def add_extras(cfg, i, batch_norm=False): # Extra layers added to VGG for feature scaling layers =[] in_channel = i flag = False for k,v in enumerate(cfg): if in_channel !=\"S\": if v == \"S\": layers += [nn.Conv2d(in_channel,cfg[k+1], kernel_size=(1,3)[flag], stride=2,padding=1)] # flag 来控制卷积核是1 还是3 else: layers += [nn.Conv2d(in_channel, v, kernel_size=(1,3)[flag])] flag = not flag in_channel = v return layers 模型Head部分的生成# cfg [4, 6, 6, 6, 4, 4], # number of boxes per feature map locationdef multibox(vgg, extra_layers, cfg, num_classes): loc_layers = [] conf_layers = [] vgg_source = [21, -2] for k, v in enumerate(vgg_source): loc_layers += [nn.Conv2d(vgg[v].out_channels, cfg[k] * 4, kernel_size=3, padding=1)] conf_layers += [nn.Conv2d(vgg[v].out_channels, cfg[k] * num_classes, kernel_size=3, padding=1)] # 对extra_layers中的（Conv2d-2_1、Conv2d-4_1、Conv2d-6_1、Conv2d-8_1）层通过卷积提取特征 for k, v in enumerate(extra_layers[1::2], 2): loc_layers += [nn.Conv2d(v.out_channels, cfg[k] * 4, kernel_size=3, padding=1)] conf_layers += [nn.Conv2d(v.out_channels, cfg[k] * num_classes, kernel_size=3, padding=1)] return vgg, extra_layers, (loc_layers, conf_layers) 模型先验框的生成class PriorBox(object): \"\"\"Compute priorbox coordinates in center-offset form for each source feature map. \"\"\" def __init__(self, cfg): super(PriorBox, self).__init__() self.image_size = cfg['min_dim'] # number of priors for feature map location (either 4 or 6) self.num_priors = len(cfg['aspect_ratios']) self.variance = cfg['variance'] or [0.1] self.feature_maps = cfg['feature_maps'] self.min_sizes = cfg['min_sizes'] self.max_sizes = cfg['max_sizes'] self.steps = cfg['steps'] self.aspect_ratios = cfg['aspect_ratios'] self.clip = cfg['clip'] self.version = cfg['name'] for v in self.variance: if v = 0: raise ValueError('Variances must be greater than 0') def forward(self): mean = [] # 'steps': [8, 16, 32, 64, 100, 300], # 'feature_maps': [38, 19, 10, 5, 3, 1], # 'min_sizes': [21, 45, 99, 153, 207, 261], # 'max_sizes': [45, 99, 153, 207, 261, 315], for k, f in enumerate(self.feature_maps): for i, j in product(range(f), repeat=2): f_k = self.image_size / self.steps[k] # unit center x,y cx = (j + 0.5) / f_k cy = (i + 0.5) / f_k # aspect_ratio: 1 # rel size: min_size s_k = self.min_sizes[k]/self.image_size mean += [cx, cy, s_k, s_k] # aspect_ratio: 1 # rel size: sqrt(s_k * s_(k+1)) s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size)) mean += [cx, cy, s_k_prime, s_k_prime] # rest of aspect ratios for ar in self.aspect_ratios[k]: mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)] mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)] # back to torch land output = torch.Tensor(mean).view(-1, 4) if self.clip: output.clamp_(max=1, min=0) return output MultiBox 损失函数class MultiBoxLoss(nn.Module): \"\"\"SSD Weighted Loss Function Compute Targets: 1) Produce Confidence Target Indices by matching ground truth boxes with (default) 'priorboxes' that have jaccard index threshold parameter (default threshold: 0.5). 2) Produce localization target by 'encoding' variance into offsets of ground truth boxes and their matched 'priorboxes'. 3) Hard negative mining to filter the excessive number of negative examples that comes with using a large number of default bounding boxes. (default negative:positive ratio 3:1) Objective Loss: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss weighted by α which is set to 1 by cross val. Args: c: class confidences, l: predicted boxes, g: ground truth boxes N: number of matched default boxes See: https://arxiv.org/pdf/1512.02325.pdf for more details. # 计算目标: # 输出那些与真实框的iou大于一定阈值的框的下标. # 根据与真实框的偏移量输出localization目标 # 用难样例挖掘算法去除大量负样本(默认正负样本比例为1:3) # 目标损失: # L(x,c,l,g) = (Lconf(x,c) + αLloc(x,l,g)) / N # 参数: # c: 类别置信度(class confidences) # l: 预测的框(predicted boxes) # g: 真实框(ground truth boxes) # N: 匹配到的框的数量(number of matched default boxes) \"\"\"# MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5, False, args.cuda) def __init__(self, num_classes, overlap_thresh, prior_for_matching, bkg_label, neg_mining, neg_pos, neg_overlap, encode_target, use_gpu=True): super(MultiBoxLoss, self).__init__() self.use_gpu = use_gpu self.num_classes = num_classes # 列表数 21 self.threshold = overlap_thresh # 交并比阈值, 0.5 self.background_label = bkg_label # 背景标签, 0 self.encode_target = encode_target # True 没卵用 self.use_prior_for_matching = prior_for_matching # True, 没卵用 self.do_neg_mining = neg_mining # 负样本和正样本的比例, 3:1 self.negpos_ratio = neg_pos # 0.5 判定负样本的阈值. self.neg_overlap = neg_overlap # False 没卵用 self.variance = cfg['variance'] def forward(self, predictions, targets): \"\"\"Multibox Loss Args: predictions (tuple): A tuple containing loc preds, conf preds, and prior boxes from SSD net. conf shape: torch.size(batch_size,num_priors,num_classes) loc shape: torch.size(batch_size,num_priors,4) priors shape: torch.size(num_priors,4) targets (tensor): Ground truth boxes and labels for a batch, shape: [batch_size,num_objs,5] (last idx is the label). \"\"\" loc_data, conf_data, priors = predictions # loc_data: [batch_size, 8732, 4] # conf_data: [batch_size, 8732, 21] # priors: [8732, 4] default box 对于任意的图片, 都是相同的, 因此无需带有 batch 维度 num = loc_data.size(0) # num = batch_size priors = priors[:loc_data.size(1), :] # loc_data.size(1) = 8732, 因此 priors 维持不变 num_priors = (priors.size(0)) # num_priors = 8732 num_classes = self.num_classes # num_classes = 21 (默认为voc数据集) # match priors (default boxes) and ground truth boxes # 将priors(default boxes)和ground truth boxes匹配 loc_t = torch.Tensor(num, num_priors, 4) # shape:[batch_size, 8732, 4] conf_t = torch.LongTensor(num, num_priors) # shape:[batch_size, 8732] for idx in range(num): # targets是列表, 列表的长度为batch_size, 列表中每个元素为一个 tensor, # 其 shape 为 [num_objs, 5], 其中 num_objs 为当前图片中物体的数量, 第二维前4个元素为边框坐标, 最后一个元素为类别编号(1~20) truths = targets[idx][:, :-1].data # [num_objs, 4] labels = targets[idx][:, -1].data # [num_objs] 使用的是 -1, 而不是 -1:, 因此, 返回的维度变少了 defaults = priors.data # [8732, 4] # from ..box_utils import match # 关键函数, 实现候选框与真实框之间的匹配, 注意是候选框而不是预测结果框! 这个函数实现较为复杂, 会在后面着重讲解 match(self.threshold, truths, defaults, self.variance, labels, loc_t, conf_t, ) if self.use_gpu: loc_t = loc_t.cuda() conf_t = conf_t.cuda() # wrap targets # 用Variable封装loc_t, 新版本的 PyTorch 无需这么做, 只需要将 requires_grad 属性设置为 True 就行了 loc_t = Variable(loc_t, requires_grad=False) conf_t = Variable(conf_t, requires_grad=False) pos = conf_t 0 # 筛选出 0 的box下标(大部分都是=0的) num_pos = pos.sum(dim=1, keepdim=True) # 求和, 取得满足条件的box的数量, [batch_size, num_gt_threshold] # Localization Loss (Smooth L1) # Shape: [batch,num_priors,4] # 位置(localization)损失函数, 使用 Smooth L1 函数求损失 # loc_data:[batch, num_priors, 4] # pos: [batch, num_priors] # pos_idx: [batch, num_priors, 4], 复制下标成坐标格式, 以便获取坐标值 pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data) loc_p = loc_data[pos_idx].view(-1, 4) # 获取预测结果值 loc_t = loc_t[pos_idx].view(-1, 4) # 获取gt值 loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False) # 计算损失 # Compute max conf across batch for hard negative mining # 计算最大的置信度, 以进行难负样本挖掘 # conf_data: [batch, num_priors, num_classes] # batch_conf: [batch, num_priors, num_classes] batch_conf = conf_data.view(-1, self.num_classes) # conf_t: [batch, num_priors] # loss_c: [batch*num_priors, 1], 计算每个priorbox预测后的损失 loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1)) # 难负样本挖掘, 按照loss进行排序, 取loss最大的负样本参与更新 # Hard Negative Mining loss_c[pos] = 0 # filter out pos boxes for now # 将所有的pos下标的box的loss置为0(pos指示的是正样本的下标) # 将 loss_c 的shape 从 [batch*num_priors, 1] 转换成 [batch, num_priors] loss_c = loss_c.view(num, -1) # reshape # 进行降序排序, 并获取到排序的下标 _, loss_idx = loss_c.sort(1, descending=True) # 将下标进行升序排序, 并获取到下标的下标 _, idx_rank = loss_idx.sort(1) # num_pos: [batch, 1], 统计每个样本中的obj个数 num_pos = pos.long().sum(1, keepdim=True) # 根据obj的个数, 确定负样本的个数(正样本的3倍) num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1) # 获取到负样本的下标 neg = idx_rank num_neg.expand_as(idx_rank) # 计算包括正样本和负样本的置信度损失 # pos: [batch, num_priors] # pos_idx: [batch, num_priors, num_classes] pos_idx = pos.unsqueeze(2).expand_as(conf_data) # neg: [batch, num_priors] # neg_idx: [batch, num_priors, num_classes] neg_idx = neg.unsqueeze(2).expand_as(conf_data) # 按照pos_idx和neg_idx指示的下标筛选参与计算损失的预测数据 conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes) # 按照pos_idx和neg_idx筛选目标数据 targets_weighted = conf_t[(pos+neg).gt(0)] # 计算二者的交叉熵 loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False) # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N # 将损失函数归一化后返回 N = num_pos.data.sum() loss_l /= N loss_c /= N return loss_l, loss_c match函数的解析def match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx): # threshold: (float) 确定是否匹配的交并比阈值 # truths: (tensor: [num_obj, 4]) 存储真实 box 的边框坐标 # priors: (tensor: [num_priors, 4], 即[8732, 4]), 存储推荐框的坐标, 注意, 此时的框是 default box, 而不是 SSD 网络预测出来的框的坐标, 预测的结果存储在 loc_data中, 其 shape 为[num_obj, 8732, 4]. # variances: cfg['variance'], [0.1, 0.2], 用于将坐标转换成方便训练的形式(参考RCNN系列对边框坐标的处理) # labels: (tensor: [num_obj]), 代表了每个真实 box 对应的类别的编号 # loc_t: (tensor: [batches, 8732, 4]), # conf_t: (tensor: [batches, 8732]), # idx: batches 中图片的序号, 标识当前正在处理的 image 在 batches 中的序号 # jaccard index overlaps = jaccard( # [A, B], 返回任意两个box之间的交并比, overlaps[i][j] 代表box_a中的第i个box与box_b中的第j个box之间的交并比. truths, point_form(priors) ) # 二部图匹配(Bipartite Matching) # [num_objs,1], 得到对于每个 gt box 来说的匹配度最高的 prior box, 前者存储交并比, 后者存储prior box在num_priors中的位置 best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True) # keepdim=True, 因此shape为[num_objs,1] # [1,num_priors] best ground truth for each prior # [1, num_priors], 即[1,8732], 同理, 得到对于每个 prior box 来说的匹配度最高的 gt box best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True) best_truth_idx.squeeze_(0) # 上面特意保留了维度(keepdim=True), 这里又都把维度 squeeze/reduce 了, 实际上只需用默认的 keepdim=False 就可以自动 squeeze/reduce 维度. best_truth_overlap.squeeze_(0) best_prior_idx.squeeze_(1) best_prior_overlap.squeeze_(1) best_truth_overlap.index_fill_(0, best_prior_idx, 2) # ensure best prior # 维度压缩后变为[num_priors], best_prior_idx 维度为[num_objs], # 该语句会将与gt box匹配度最好的prior box 的交并比置为 2, 确保其最大, 以免防止某些 gtbox 没有匹配的 priorbox. # 假想一种极端情况, 所有的priorbox与某个gtbox(标记为G)的交并比为1, 而其他gtbox分别有一个交并比 # 最高的priorbox, 但是肯定小于1(因为其他的gtbox与G的交并比肯定小于1), 这样一来, 就会使得所有 # 的priorbox都与G匹配, 为了防止这种情况, 我们将那些对gtbox来说, 具有最高交并比的priorbox, # 强制进行互相匹配, 即令best_truth_idx[best_prior_idx[j]] = j, 详细见下面的for循环 # TODO refactor: index best_prior_idx with long tensor # ensure every gt matches with its prior of max overlap # 注意!!: 因为 gt box 的数量要远远少于 prior box 的数量, 因此, 同一个 gt box 会与多个 prior box 匹配. for j in range(best_prior_idx.size(0)): best_truth_idx[best_prior_idx[j]] = j # best_prior_idx[j] 代表与box_a的第j个box交并比最高的 prior box 的下标, 将与该 gtbox # 匹配度最好的 prior box 的下标改为j, 由此,完成了该 gtbox 与第j个 prior box 的匹配. # 这里的循环只会进行num_obj次, 剩余的匹配为 best_truth_idx 中原本的值. # 这里处理的情况是, priorbox中第i个box与gtbox中第k个box的交并比最高, # 即 best_truth_idx[i]= k # 但是对于best_prior_idx[k]来说, 它却与priorbox的第l个box有着最高的交并比, # 即best_prior_idx[k]=l # 而对于gtbox的另一个边框gtbox[j]来说, 它与priorbox[i]的交并比最大, # 即但是对于best_prior_idx[j] = i. # 那么, 此时, 我们就应该将best_truth_idx[i]= k 修改成 best_truth_idx[i]= j. # 即令 priorbox[i] 与 gtbox[j]对应. # 这样做的原因: 防止某个gtbox没有匹配的 prior box. matches = truths[best_truth_idx] # Shape: [num_priors,4] # truths 的shape 为[num_objs, 4], 而best_truth_idx是一个指示下标的列表, 列表长度为 8732, # 列表中的下标范围为0~num_objs-1, 代表的是与每个priorbox匹配的gtbox的下标 # 上面的表达式会返回一个shape为 [num_priors, 4], 即 [8732, 4] 的tensor, 代表的就是与每个priorbox匹配的gtbox的坐标值. conf = labels[best_truth_idx] + 1 # 与上面的语句道理差不多, 这里得到的是每个prior box匹配的类别编号, shape 为[8732] conf[best_truth_overlap threshold] = 0 # 将与gtbox的交并比小于阈值的置为0 , 即认为是非物体框 loc = encode(matches, priors, variances) # 返回编码后的中心坐标和宽高. loc_t[idx] = loc # [num_priors,4] encoded offsets to learn # 设置第idx张图片的gt编码坐标信息 conf_t[idx] = conf # [num_priors] top class label for each prior 设置第idx张图片的编号信息.(大于0即为物体编号, 认为有物体, 小于0认为是背景) 参考文章：https://blog.csdn.net/happyday_d/article/details/86021993https://hellozhaozheng.github.io/z_post/PyTorch-SSD","tags":["目标检测"],"categories":["深度学习"]},{"title":"Matplotlib绘制六种可视化图表","path":"/2019/08/24/pythonplot/","content":"01 折线图1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | import numpy as np import matplotlib.pyplot as plt x = np.linspace(0,2,100) plt.plot(x, x, label='linear') plt.plot(x, x**2, label='quadratic') plt.plot(x, x**3, label='cubic') plt.xlabel('x label') plt.ylabel('y label') plt.title(\"Simple Plot\") plt.legend() plt.show() —|— 02 散点图1 2 3 4 5 6 7 8 | import numpy as np import matplotlib.pyplot as plt x = np.arange(0., 5., 0.2) # 红色破折号, 蓝色方块 ，绿色三角块 plt.plot(x, x, 'r--', x, x**2, 'bs', x, x**3, 'g^') plt.show() —|— 03 直方图1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 | import numpy as np import matplotlib.pyplot as plt np.random.seed(19680801) mu1, sigma1 = 100, 15 mu2, sigma2 = 80, 15 x1 = mu1 + sigma1 * np.random.randn(10000) x2 = mu2 + sigma2 * np.random.randn(10000) # the histogram of the data # 50：将数据分成50组 # facecolor：颜色；alpha：透明度 # density：是密度而不是具体数值 n1, bins1, patches1 = plt.hist(x1, 50, density=True, facecolor='g', alpha=1) n2, bins2, patches2 = plt.hist(x2, 50, density=True, facecolor='r', alpha=0.2) # n：概率值；bins：具体数值；patches：直方图对象。 plt.xlabel('Smarts') plt.ylabel('Probability') plt.title('Histogram of IQ') plt.text(110, .025, r'$\\mu=100,\\ \\sigma=15$') plt.text(50, .025, r'$\\mu=80,\\ \\sigma=15$') # 设置x，y轴的具体范围 plt.axis([40, 160, 0, 0.03]) plt.grid(True) plt.show() —|— 04 柱状图4.1 并列柱状图1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 | import numpy as np import matplotlib.pyplot as plt size = 5 a = np.random.random(size) b = np.random.random(size) c = np.random.random(size) x = np.arange(size) # 有多少个类型，只需更改n即可 total_width, n = 0.8, 3 width = total_width / n # 重新拟定x的坐标 x = x - (total_width - width) / 2 # 这里使用的是偏移 plt.bar(x, a, width=width, label='a') plt.bar(x + width, b, width=width, label='b') plt.bar(x + 2 * width, c, width=width, label='c') plt.legend() plt.show() —|— 4.2 叠加柱状图1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 | import numpy as np import matplotlib.pyplot as plt size = 5 a = np.random.random(size) b = np.random.random(size) c = np.random.random(size) x = np.arange(size) # 这里使用的是偏移 plt.bar(x, a, width=0.5, label='a',fc='r') plt.bar(x, b, bottom=a, width=0.5, label='b', fc='g') plt.bar(x, c, bottom=a+b, width=0.5, label='c', fc='b') plt.ylim(0, 2.5) plt.legend() plt.grid(True) plt.show() —|— 05 饼图5.1 普通饼图1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 | import matplotlib.pyplot as plt labels = 'Frogs', 'Hogs', 'Dogs', 'Logs' sizes = [15, 30, 45, 10] # 设置分离的距离，0表示不分离 explode = (0, 0.1, 0, 0) plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%', shadow=True, startangle=90) # Equal aspect ratio 保证画出的图是正圆形 plt.axis('equal') plt.show() —|— 5.2 嵌套饼图1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 | import numpy as np import matplotlib.pyplot as plt # 设置每环的宽度 size = 0.3 vals = np.array([[60., 32.], [37., 40.], [29., 10.]]) # 通过get_cmap随机获取颜色 cmap = plt.get_cmap(\"tab20c\") outer_colors = cmap(np.arange(3)*4) inner_colors = cmap(np.array([1, 2, 5, 6, 9, 10])) print(vals.sum(axis=1)) # [92. 77. 39.] plt.pie(vals.sum(axis=1), radius=1, colors=outer_colors, wedgeprops=dict(width=size, edgecolor='w')) print(vals.flatten()) # [60. 32. 37. 40. 29. 10.] plt.pie(vals.flatten(), radius=1-size, colors=inner_colors, wedgeprops=dict(width=size, edgecolor='w')) # equal 使得为正圆 plt.axis('equal') plt.show() —|— 5.3 极轴饼图1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 | import numpy as np import matplotlib.pyplot as plt np.random.seed(19680801) N = 10 theta = np.linspace(0.0, 2 * np.pi, N, endpoint=False) radii = 10 * np.random.rand(N) width = np.pi / 4 * np.random.rand(N) ax = plt.subplot(111, projection='polar') bars = ax.bar(theta, radii, width=width, bottom=0.0) # left表示从哪开始， # radii表示从中心点向边缘绘制的长度（半径） # width表示末端的弧长 # 自定义颜色和不透明度 for r, bar in zip(radii, bars): bar.set_facecolor(plt.cm.viridis(r / 10.)) bar.set_alpha(0.5) plt.show() —|— 06 三维图6.1 绘制三维散点图1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 | import numpy as np import matplotlib.pyplot as plt from mpl_toolkits.mplot3d import Axes3D data = np.random.randint(0, 255, size=[40, 40, 40]) x, y, z = data[0], data[1], data[2] ax = plt.subplot(111, projection='3d') # 创建一个三维的绘图工程 # 将数据点分成三部分画，在颜色上有区分度 ax.scatter(x[:10], y[:10], z[:10], c='y') # 绘制数据点 ax.scatter(x[10:20], y[10:20], z[10:20], c='r') ax.scatter(x[30:40], y[30:40], z[30:40], c='g') ax.set_zlabel('Z') # 坐标轴 ax.set_ylabel('Y') ax.set_xlabel('X') plt.show() —|— 6.2 绘制三维平面图1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 | from matplotlib import pyplot as plt import numpy as np from mpl_toolkits.mplot3d import Axes3D fig = plt.figure() ax = Axes3D(fig) X = np.arange(-4, 4, 0.25) Y = np.arange(-4, 4, 0.25) X, Y = np.meshgrid(X, Y) R = np.sqrt(X**2 + Y**2) Z = np.sin(R) # 具体函数方法可用 help(function) 查看，如：help(ax.plot_surface) ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='rainbow') plt.show() —|— 参考链接： https://mp.weixin.qq.com/s/bMvrle-FRvli0pRNi83waQ","tags":["Python"],"categories":["Python"]},{"title":"SSD源码解析","path":"/2019/08/24/ssd/","content":"论文地址: SSD: Single Shot MultiBox Detector 非官方代码: pytorch 介绍SSD，全称Single Shot MultiBox Detector，是一种One-Stage的方法，它由Wei Liu在ECCV 2016上提出，SSD具有如下主要特点： 从YOLO中继承了将detection转化为regression的思路，同时一次即可完成网络训练 基于Faster RCNN中的anchor，提出了相似的prior box 加入基于特征金字塔（Pyramidal Feature Hierarchy）的检测方式，相当于半个FPN思路 SSD的 网络结构 由上图可以看出，SSD的基础网络结构由基础网络VGG16组成，在VGG16基础网络之后接了一个3x3的卷积和一个1x1的卷积做特征融合，然后增加了一个Extra Feature Layers 层，这个层由八个卷积层构成。SSD在前面的基础网络去conv_4_3之后的relu层输出，以及倒数第二层的conv_7_1的relu再加上Extra Feature Layers层的第1,3,5,7层 共有6个 featuremap层，在此基础上对box进行预测。但感觉要提升效果的话可以对基础网络进行更改，增加特征融合等等。 空洞卷积(Dilation Conv)ssd网络里还使用了空洞卷积(Dilation Conv),采用VGG16做基础模型，首先VGG16是在ILSVRC CLS-LOC数据集预训练。然后借鉴了DeepLab-LargeFOV，分别将VGG16的全连接层fc6和fc7转换成 3×3卷积层 conv6和 1×1 卷积层conv7，同时将池化层 pool5 由原来的 stride=2 的 2×2 变成 stride=1 的(猜想是不想reduce特征图大小)，为了配合这种变化，采用了一种 Atrous Algorithm，其实就是conv6采用扩展卷积或带孔卷积（Dilation Conv），其在不增加参数与模型复杂度的条件下指数级扩大卷积的视野，其使用扩张率(dilation rate)参数，来表示扩张的大小，如下图所示，(a)是普通的 3×3 卷积，其视野就是 3×3 ，(b)是扩张率为 1，此时视野变成 7×7 ，(c)扩张率为3时，视野扩大为 15×15 ，但是视野的特征更稀疏了。Conv6采用 3×3 大小但dilation rate=6的扩展卷积。 Prior BoxSSD中有着类似anchor机制的Prior Box机制，用于来生成先验框，后面将这些先验框与真实的gt进行匹配，然后与预测的进行回归。从而得到物体真实的 位置。SSD的prior Box 按照如下规则生成： 以feature map上每个点的中点为中心（offset=0.5），生成一些列同心的prior box（然后中心点的坐标会乘以step，相当于从feature map位置映射回原图位置） 正方形prior box最小边长为’’’pash $min_size$’’’，最大边长为：\\sqrt{min_size*max_size}E=mc^2 根据相应的aspect ratio，会生成不同个数的长方形 ，长宽为：$ \\sqrt{aspect_ratio}min_size$ 和 $1/ \\sqrt{aspect_ratio} min_size$ $f(x)=ax+b$ 最终网络生成固定数量的Prior Box 每个feature map 对应prior box的min_size 和max_size 由以下的公式决定，公式中的m是使用feature map的数量(m=6)第一层feature map对应的min_size=S1，max_size=S2；第二层min_size=S2，max_size=S3；其他类推。在原文中，Smin=0.2，Smax=0.9 | min_size | max_size | def.boxes num—|—|—|—conv4_3 | 30 | 60 | 4fc7 | 60 | 111 | 6conv6_2 | 111 | 162 | 6fc7 | 162 | 213 | 6conv4_3 | 213 | 264 | 4fc7 | 264 | 315 | 4 训练策略正负样本给定输入图像以及每个物体的Ground Truth,首先找到每个Ground True box对应的default box中IOU最大的最为正样本。然后,在剩下的default box中寻找与Ground Truth 的IOU大于0.5的default box作为正样本。一个Ground Truth可能对应多个正样本default box.其他的default box作为负样本。,为了保证样本尽量平衡,SSD采用了hard nagative mining,即对负样本进行抽样,抽样时按照置信度误差(预测背景的置信度越小,误差越大)进行奖序排列,选取误差较大的top-k作为训练的负样本,保证正负样本比例接近1:3。 目标函数目标函数为训练过程中的优化标准,目标函数也称损失函数,主要包括位置误差(localization loss,loc) 与置信度误差(confidence loss,conf,分类损失)的加权和,定义为： 代码解析基础模型定义1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 | # vgg([64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M', # 512, 512, 512], 3) # This function is derived from torchvision VGG make_layers() # https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py def vgg(cfg, i, batch_norm=False): layers = [] in_channels = i for v in cfg: if v == 'M': layers += [nn.MaxPool2d(kernel_size=2, stride=2)] elif v == 'C': layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)] else: conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1) if batch_norm: layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)] else: layers += [conv2d, nn.ReLU(inplace=True)] in_channels = v pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1) conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6) conv7 = nn.Conv2d(1024, 1024, kernel_size=1) layers += [pool5, conv6, nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)] return layers —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 | base = { '300': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M', 512, 512, 512], '512': [], } extras = { '300': [256, 'S', 512, 128, 'S', 256, 128, 256, 128, 256], '512': [], } mbox = { '300': [4, 6, 6, 6, 4, 4], # number of boxes per feature map location '512': [], } def add_extras(cfg, i, batch_norm=False): # Extra layers added to VGG for feature scaling layers =[] in_channel = i flag = False for k,v in enumerate(cfg): if in_channel !=\"S\": if v == \"S\": layers += [nn.Conv2d(in_channel,cfg[k+1], kernel_size=(1,3)[flag], stride=2,padding=1)] # flag 来控制卷积核是1 还是3 else: layers += [nn.Conv2d(in_channel, v, kernel_size=(1,3)[flag])] flag = not flag in_channel = v return layers —|— 模型Head部分的生成1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 | # cfg [4, 6, 6, 6, 4, 4], # number of boxes per feature map location def multibox(vgg, extra_layers, cfg, num_classes): loc_layers = [] conf_layers = [] vgg_source = [21, -2] for k, v in enumerate(vgg_source): loc_layers += [nn.Conv2d(vgg[v].out_channels, cfg[k] * 4, kernel_size=3, padding=1)] conf_layers += [nn.Conv2d(vgg[v].out_channels, cfg[k] * num_classes, kernel_size=3, padding=1)] # 对extra_layers中的（Conv2d-2_1、Conv2d-4_1、Conv2d-6_1、Conv2d-8_1）层通过卷积提取特征 for k, v in enumerate(extra_layers[1::2], 2): loc_layers += [nn.Conv2d(v.out_channels, cfg[k] * 4, kernel_size=3, padding=1)] conf_layers += [nn.Conv2d(v.out_channels, cfg[k] * num_classes, kernel_size=3, padding=1)] return vgg, extra_layers, (loc_layers, conf_layers) —|— 模型先验框的生成1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 | class PriorBox(object): \"\"\"Compute priorbox coordinates in center-offset form for each source feature map. \"\"\" def __init__(self, cfg): super(PriorBox, self).__init__() self.image_size = cfg['min_dim'] # number of priors for feature map location (either 4 or 6) self.num_priors = len(cfg['aspect_ratios']) self.variance = cfg['variance'] or [0.1] self.feature_maps = cfg['feature_maps'] self.min_sizes = cfg['min_sizes'] self.max_sizes = cfg['max_sizes'] self.steps = cfg['steps'] self.aspect_ratios = cfg['aspect_ratios'] self.clip = cfg['clip'] self.version = cfg['name'] for v in self.variance: if v = 0: raise ValueError('Variances must be greater than 0') def forward(self): mean = [] # 'steps': [8, 16, 32, 64, 100, 300], # 'feature_maps': [38, 19, 10, 5, 3, 1], # 'min_sizes': [21, 45, 99, 153, 207, 261], # 'max_sizes': [45, 99, 153, 207, 261, 315], for k, f in enumerate(self.feature_maps): for i, j in product(range(f), repeat=2): f_k = self.image_size / self.steps[k] # unit center x,y cx = (j + 0.5) / f_k cy = (i + 0.5) / f_k # aspect_ratio: 1 # rel size: min_size s_k = self.min_sizes[k]/self.image_size mean += [cx, cy, s_k, s_k] # aspect_ratio: 1 # rel size: sqrt(s_k * s_(k+1)) s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size)) mean += [cx, cy, s_k_prime, s_k_prime] # rest of aspect ratios for ar in self.aspect_ratios[k]: mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)] mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)] # back to torch land output = torch.Tensor(mean).view(-1, 4) if self.clip: output.clamp_(max=1, min=0) return output —|— MultiBox 损失函数1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 | class MultiBoxLoss(nn.Module): \"\"\"SSD Weighted Loss Function Compute Targets: 1) Produce Confidence Target Indices by matching ground truth boxes with (default) 'priorboxes' that have jaccard index threshold parameter (default threshold: 0.5). 2) Produce localization target by 'encoding' variance into offsets of ground truth boxes and their matched 'priorboxes'. 3) Hard negative mining to filter the excessive number of negative examples that comes with using a large number of default bounding boxes. (default negative:positive ratio 3:1) Objective Loss: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss weighted by α which is set to 1 by cross val. Args: c: class confidences, l: predicted boxes, g: ground truth boxes N: number of matched default boxes See: https://arxiv.org/pdf/1512.02325.pdf for more details. # 计算目标: # 输出那些与真实框的iou大于一定阈值的框的下标. # 根据与真实框的偏移量输出localization目标 # 用难样例挖掘算法去除大量负样本(默认正负样本比例为1:3) # 目标损失: # L(x,c,l,g) = (Lconf(x,c) + αLloc(x,l,g)) / N # 参数: # c: 类别置信度(class confidences) # l: 预测的框(predicted boxes) # g: 真实框(ground truth boxes) # N: 匹配到的框的数量(number of matched default boxes) \"\"\" # MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5, False, args.cuda) def __init__(self, num_classes, overlap_thresh, prior_for_matching, bkg_label, neg_mining, neg_pos, neg_overlap, encode_target, use_gpu=True): super(MultiBoxLoss, self).__init__() self.use_gpu = use_gpu self.num_classes = num_classes # 列表数 21 self.threshold = overlap_thresh # 交并比阈值, 0.5 self.background_label = bkg_label # 背景标签, 0 self.encode_target = encode_target # True 没卵用 self.use_prior_for_matching = prior_for_matching # True, 没卵用 self.do_neg_mining = neg_mining # 负样本和正样本的比例, 3:1 self.negpos_ratio = neg_pos # 0.5 判定负样本的阈值. self.neg_overlap = neg_overlap # False 没卵用 self.variance = cfg['variance'] def forward(self, predictions, targets): \"\"\"Multibox Loss Args: predictions (tuple): A tuple containing loc preds, conf preds, and prior boxes from SSD net. conf shape: torch.size(batch_size,num_priors,num_classes) loc shape: torch.size(batch_size,num_priors,4) priors shape: torch.size(num_priors,4) targets (tensor): Ground truth boxes and labels for a batch, shape: [batch_size,num_objs,5] (last idx is the label). \"\"\" loc_data, conf_data, priors = predictions # loc_data: [batch_size, 8732, 4] # conf_data: [batch_size, 8732, 21] # priors: [8732, 4] default box 对于任意的图片, 都是相同的, 因此无需带有 batch 维度 num = loc_data.size(0) # num = batch_size priors = priors[:loc_data.size(1), :] # loc_data.size(1) = 8732, 因此 priors 维持不变 num_priors = (priors.size(0)) # num_priors = 8732 num_classes = self.num_classes # num_classes = 21 (默认为voc数据集) # match priors (default boxes) and ground truth boxes # 将priors(default boxes)和ground truth boxes匹配 loc_t = torch.Tensor(num, num_priors, 4) # shape:[batch_size, 8732, 4] conf_t = torch.LongTensor(num, num_priors) # shape:[batch_size, 8732] for idx in range(num): # targets是列表, 列表的长度为batch_size, 列表中每个元素为一个 tensor, # 其 shape 为 [num_objs, 5], 其中 num_objs 为当前图片中物体的数量, 第二维前4个元素为边框坐标, 最后一个元素为类别编号(1~20) truths = targets[idx][:, :-1].data # [num_objs, 4] labels = targets[idx][:, -1].data # [num_objs] 使用的是 -1, 而不是 -1:, 因此, 返回的维度变少了 defaults = priors.data # [8732, 4] # from ..box_utils import match # 关键函数, 实现候选框与真实框之间的匹配, 注意是候选框而不是预测结果框! 这个函数实现较为复杂, 会在后面着重讲解 match(self.threshold, truths, defaults, self.variance, labels, loc_t, conf_t, ) if self.use_gpu: loc_t = loc_t.cuda() conf_t = conf_t.cuda() # wrap targets # 用Variable封装loc_t, 新版本的 PyTorch 无需这么做, 只需要将 requires_grad 属性设置为 True 就行了 loc_t = Variable(loc_t, requires_grad=False) conf_t = Variable(conf_t, requires_grad=False) pos = conf_t 0 # 筛选出 0 的box下标(大部分都是=0的) num_pos = pos.sum(dim=1, keepdim=True) # 求和, 取得满足条件的box的数量, [batch_size, num_gt_threshold] # Localization Loss (Smooth L1) # Shape: [batch,num_priors,4] # 位置(localization)损失函数, 使用 Smooth L1 函数求损失 # loc_data:[batch, num_priors, 4] # pos: [batch, num_priors] # pos_idx: [batch, num_priors, 4], 复制下标成坐标格式, 以便获取坐标值 pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data) loc_p = loc_data[pos_idx].view(-1, 4) # 获取预测结果值 loc_t = loc_t[pos_idx].view(-1, 4) # 获取gt值 loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False) # 计算损失 # Compute max conf across batch for hard negative mining # 计算最大的置信度, 以进行难负样本挖掘 # conf_data: [batch, num_priors, num_classes] # batch_conf: [batch, num_priors, num_classes] batch_conf = conf_data.view(-1, self.num_classes) # conf_t: [batch, num_priors] # loss_c: [batch*num_priors, 1], 计算每个priorbox预测后的损失 loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1)) # 难负样本挖掘, 按照loss进行排序, 取loss最大的负样本参与更新 # Hard Negative Mining loss_c[pos] = 0 # filter out pos boxes for now # 将所有的pos下标的box的loss置为0(pos指示的是正样本的下标) # 将 loss_c 的shape 从 [batch*num_priors, 1] 转换成 [batch, num_priors] loss_c = loss_c.view(num, -1) # reshape # 进行降序排序, 并获取到排序的下标 _, loss_idx = loss_c.sort(1, descending=True) # 将下标进行升序排序, 并获取到下标的下标 _, idx_rank = loss_idx.sort(1) # num_pos: [batch, 1], 统计每个样本中的obj个数 num_pos = pos.long().sum(1, keepdim=True) # 根据obj的个数, 确定负样本的个数(正样本的3倍) num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1) # 获取到负样本的下标 neg = idx_rank num_neg.expand_as(idx_rank) # 计算包括正样本和负样本的置信度损失 # pos: [batch, num_priors] # pos_idx: [batch, num_priors, num_classes] pos_idx = pos.unsqueeze(2).expand_as(conf_data) # neg: [batch, num_priors] # neg_idx: [batch, num_priors, num_classes] neg_idx = neg.unsqueeze(2).expand_as(conf_data) # 按照pos_idx和neg_idx指示的下标筛选参与计算损失的预测数据 conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes) # 按照pos_idx和neg_idx筛选目标数据 targets_weighted = conf_t[(pos+neg).gt(0)] # 计算二者的交叉熵 loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False) # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N # 将损失函数归一化后返回 N = num_pos.data.sum() loss_l /= N loss_c /= N return loss_l, loss_c —|— match函数的解析1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 | def match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx): # threshold: (float) 确定是否匹配的交并比阈值 # truths: (tensor: [num_obj, 4]) 存储真实 box 的边框坐标 # priors: (tensor: [num_priors, 4], 即[8732, 4]), 存储推荐框的坐标, 注意, 此时的框是 default box, 而不是 SSD 网络预测出来的框的坐标, 预测的结果存储在 loc_data中, 其 shape 为[num_obj, 8732, 4]. # variances: cfg['variance'], [0.1, 0.2], 用于将坐标转换成方便训练的形式(参考RCNN系列对边框坐标的处理) # labels: (tensor: [num_obj]), 代表了每个真实 box 对应的类别的编号 # loc_t: (tensor: [batches, 8732, 4]), # conf_t: (tensor: [batches, 8732]), # idx: batches 中图片的序号, 标识当前正在处理的 image 在 batches 中的序号 # jaccard index overlaps = jaccard( # [A, B], 返回任意两个box之间的交并比, overlaps[i][j] 代表box_a中的第i个box与box_b中的第j个box之间的交并比. truths, point_form(priors) ) # 二部图匹配(Bipartite Matching) # [num_objs,1], 得到对于每个 gt box 来说的匹配度最高的 prior box, 前者存储交并比, 后者存储prior box在num_priors中的位置 best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True) # keepdim=True, 因此shape为[num_objs,1] # [1,num_priors] best ground truth for each prior # [1, num_priors], 即[1,8732], 同理, 得到对于每个 prior box 来说的匹配度最高的 gt box best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True) best_truth_idx.squeeze_(0) # 上面特意保留了维度(keepdim=True), 这里又都把维度 squeeze/reduce 了, 实际上只需用默认的 keepdim=False 就可以自动 squeeze/reduce 维度. best_truth_overlap.squeeze_(0) best_prior_idx.squeeze_(1) best_prior_overlap.squeeze_(1) best_truth_overlap.index_fill_(0, best_prior_idx, 2) # ensure best prior # 维度压缩后变为[num_priors], best_prior_idx 维度为[num_objs], # 该语句会将与gt box匹配度最好的prior box 的交并比置为 2, 确保其最大, 以免防止某些 gtbox 没有匹配的 priorbox. # 假想一种极端情况, 所有的priorbox与某个gtbox(标记为G)的交并比为1, 而其他gtbox分别有一个交并比 # 最高的priorbox, 但是肯定小于1(因为其他的gtbox与G的交并比肯定小于1), 这样一来, 就会使得所有 # 的priorbox都与G匹配, 为了防止这种情况, 我们将那些对gtbox来说, 具有最高交并比的priorbox, # 强制进行互相匹配, 即令best_truth_idx[best_prior_idx[j]] = j, 详细见下面的for循环 # TODO refactor: index best_prior_idx with long tensor # ensure every gt matches with its prior of max overlap # 注意!!: 因为 gt box 的数量要远远少于 prior box 的数量, 因此, 同一个 gt box 会与多个 prior box 匹配. for j in range(best_prior_idx.size(0)): best_truth_idx[best_prior_idx[j]] = j # best_prior_idx[j] 代表与box_a的第j个box交并比最高的 prior box 的下标, 将与该 gtbox # 匹配度最好的 prior box 的下标改为j, 由此,完成了该 gtbox 与第j个 prior box 的匹配. # 这里的循环只会进行num_obj次, 剩余的匹配为 best_truth_idx 中原本的值. # 这里处理的情况是, priorbox中第i个box与gtbox中第k个box的交并比最高, # 即 best_truth_idx[i]= k # 但是对于best_prior_idx[k]来说, 它却与priorbox的第l个box有着最高的交并比, # 即best_prior_idx[k]=l # 而对于gtbox的另一个边框gtbox[j]来说, 它与priorbox[i]的交并比最大, # 即但是对于best_prior_idx[j] = i. # 那么, 此时, 我们就应该将best_truth_idx[i]= k 修改成 best_truth_idx[i]= j. # 即令 priorbox[i] 与 gtbox[j]对应. # 这样做的原因: 防止某个gtbox没有匹配的 prior box. matches = truths[best_truth_idx] # Shape: [num_priors,4] # truths 的shape 为[num_objs, 4], 而best_truth_idx是一个指示下标的列表, 列表长度为 8732, # 列表中的下标范围为0~num_objs-1, 代表的是与每个priorbox匹配的gtbox的下标 # 上面的表达式会返回一个shape为 [num_priors, 4], 即 [8732, 4] 的tensor, 代表的就是与每个priorbox匹配的gtbox的坐标值. conf = labels[best_truth_idx] + 1 # 与上面的语句道理差不多, 这里得到的是每个prior box匹配的类别编号, shape 为[8732] conf[best_truth_overlap threshold] = 0 # 将与gtbox的交并比小于阈值的置为0 , 即认为是非物体框 loc = encode(matches, priors, variances) # 返回编码后的中心坐标和宽高. loc_t[idx] = loc # [num_priors,4] encoded offsets to learn # 设置第idx张图片的gt编码坐标信息 conf_t[idx] = conf # [num_priors] top class label for each prior 设置第idx张图片的编号信息.(大于0即为物体编号, 认为有物体, 小于0认为是背景) —|— 参考文章：https://blog.csdn.net/happyday_d/article/details/86021993https://hellozhaozheng.github.io/z_post/PyTorch-SSD","tags":["PyTorch"],"categories":["深度学习"]},{"title":"SqueezeNet","path":"/2019/08/20/20190820-squeezenet/","content":"论文地址: SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and 0.5MB model size非官方代码: pytorch 介绍这篇文章是DeepScal，加州大学伯克利分校，以及斯坦福大学在ICLR 2017发表的一篇文章。文章的主要目的是为了压缩模型，提高运行速度。这篇文章主要提出了SqueezeNet: 使用少量参数保持精度。 结构设计策略这篇文章的首要目标是在保持准确率 的同时，有几个参数的CNN架构。这篇文章在设计CNN架构的时候采取了三个主要策略。这篇文章的主要模块 是Fire模块。 用1x1的卷积核代替3x3的卷积核，从而减少参数量。1x1 卷积的参数比3x3的卷积核少了 9X. 减少3x3 卷积输入通道的数量。假设有一个卷积层, 它完全由3x3 卷积组成。此层中参数的总数量为：(输入通道数) (过滤器数) (3 * 3)。要在squeeze层中将输入的通道数减少。 在网络中减少下采样(maxpooling)实现, 以便卷积层具有较大的特征图。 Fire ModuleFire Module是将原来一层conv层变成两层：squeeze层+expand层，各自带上Relu激活层。在squeeze层里面全是1x1的卷积kernel，数量记为S11；在expand层里面有1x1和3x3的卷积kernel，expand层之后将1x1和3x3的卷积output feature maps在channel维度cat。 自己手推的一张图，字比较丑，也没时间重现写一下。 fire moudle的pytorch代码很奇怪的是论文中用的是3个1x1，以及expand用的是4个1x1的卷积核和4个 3x3的卷积核，但是pytroch版本的代码并没有体现出来。 class fire(nn.Module): def __init__(self, inplanes,squeeze_planes, expand_planes): super(fire,self).__init__() self.conv1 = nn.Conv2d(inplanes,squeeze_planes, kernel_size=1, stride=1) self.bn1 = nn.BatchNorm2d(squeeze_planes) self.relu1 = nn.ReLU(inplace=True) self.conv2 = nn.Conv2d(squeeze_planes, expand_planes, kernel_size=1, stride=1) self.bn2 = nn.BatchNorm2d(expand_planes) self.conv3 = nn.Conv2d(squeeze_planes,expand_planes,kernel_size=3, stride=1,padding=1) self.bn3 = nn.BatchNorm2d(expand_planes) self.relu2 = nn.ReLU(inplace=True) # using MSR initialization for m in self.modules(): if isinstance(m,nn.Conv2d): n = m.kernel_size[0]*m.kernel_size[1]*m.in_channels m.weight.data.normal_(0,math.sqrt(2./n)) def forward(self,x): x = self.conv1(x) x = self.bn1(x) x = self.relu1(x) out1 = self.conv2(x) out1 = self.bn2(out1) out2 = self.conv3(x) out2 = self.bn3(out2) out = torch.cat([out1,out2],1) out = self.relu2(out) return out SqueezeNet的具体网络结构 实验结果imagenet数据上比较了alexnet，可以看到准确率差不多的情况下，squeezeNet模型参数数量显著降低了（下表倒数第三行），参数减少50X；如果再加上deep compression技术，压缩比可以达到461X！还是不错的结果。 参考文章：https://blog.csdn.net/xbinworld/article/details/50897870","tags":["深度学习"],"categories":["深度学习"]},{"title":"SqueezeNet","path":"/2019/08/20/squeezenet/","content":"论文地址: SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and 0.5MB model size非官方代码: pytorch 介绍这篇文章是DeepScal，加州大学伯克利分校，以及斯坦福大学在ICLR 2017发表的一篇文章。文章的主要目的是为了压缩模型，提高运行速度。这篇文章主要提出了SqueezeNet: 使用少量参数保持精度。 结构设计策略这篇文章的首要目标是在保持准确率 的同时，有几个参数的CNN架构。这篇文章在设计CNN架构的时候采取了三个主要策略。这篇文章的主要模块 是Fire模块。 用1x1的卷积核代替3x3的卷积核，从而减少参数量。1x1 卷积的参数比3x3的卷积核少了 9X. 减少3x3 卷积输入通道的数量。假设有一个卷积层, 它完全由3x3 卷积组成。此层中参数的总数量为：(输入通道数) (过滤器数) (3 * 3)。要在squeeze层中将输入的通道数减少。 在网络中减少下采样(maxpooling)实现, 以便卷积层具有较大的特征图。 Fire ModuleFire Module是将原来一层conv层变成两层：squeeze层+expand层，各自带上Relu激活层。在squeeze层里面全是1x1的卷积kernel，数量记为S11；在expand层里面有1x1和3x3的卷积kernel，expand层之后将1x1和3x3的卷积output feature maps在channel维度cat。 自己手推的一张图，字比较丑，也没时间重现写一下。 fire moudle的pytorch代码很奇怪的是论文中用的是3个1x1，以及expand用的是4个1x1的卷积核和4个 3x3的卷积核，但是pytroch版本的代码并没有体现出来。 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 | class fire(nn.Module): def __init__(self, inplanes,squeeze_planes, expand_planes): super(fire,self).__init__() self.conv1 = nn.Conv2d(inplanes,squeeze_planes, kernel_size=1, stride=1) self.bn1 = nn.BatchNorm2d(squeeze_planes) self.relu1 = nn.ReLU(inplace=True) self.conv2 = nn.Conv2d(squeeze_planes, expand_planes, kernel_size=1, stride=1) self.bn2 = nn.BatchNorm2d(expand_planes) self.conv3 = nn.Conv2d(squeeze_planes,expand_planes,kernel_size=3, stride=1,padding=1) self.bn3 = nn.BatchNorm2d(expand_planes) self.relu2 = nn.ReLU(inplace=True) # using MSR initialization for m in self.modules(): if isinstance(m,nn.Conv2d): n = m.kernel_size[0]*m.kernel_size[1]*m.in_channels m.weight.data.normal_(0,math.sqrt(2./n)) def forward(self,x): x = self.conv1(x) x = self.bn1(x) x = self.relu1(x) out1 = self.conv2(x) out1 = self.bn2(out1) out2 = self.conv3(x) out2 = self.bn3(out2) out = torch.cat([out1,out2],1) out = self.relu2(out) return out —|— SqueezeNet的具体网络结构 实验结果imagenet数据上比较了alexnet，可以看到准确率差不多的情况下，squeezeNet模型参数数量显著降低了（下表倒数第三行），参数减少50X；如果再加上deep compression技术，压缩比可以达到461X！还是不错的结果。 参考文章：https://blog.csdn.net/xbinworld/article/details/50897870","tags":["PyTorch"],"categories":["深度学习"]},{"title":"多层感知机的反向传播","path":"/2019/08/19/20190819-MLP-Back-Propagation/","content":"全连接神经网络是形式上最简单的神经网络，反向传播算法是一种常用的训练神经网络的算法，理解全连接神经网络中的反向传播算法是理解其他更加复杂网络中反向传播算法的重要基础。 参考链接： https://zhuanlan.zhihu.com/p/61863634","tags":["深度学习"],"categories":["深度学习"]},{"title":"分治算法","path":"/2019/08/19/20190819-分治算法/","content":"问题： 给定一组数，要求从中找出第k小的元素。分析：这里通过快速排序算法来解决次问题。记一趟快速排序后，左子集中的元素个数为nleft，则选择问题，可能是一下几种情况之一： nleft等于k-1，则枢纽值即为所求； nleft大于k-1,则继续在左子树中找； nleft小于k-1,则继续在右子集中找C++代码实现 // t5.cpp : 定义控制台应用程序的入口点。//#include \"stdafx.h\"#include \"stdafx.h\"#includestdio.h#includestdlib.h#includetime.h#includemath.h#includestring.hint quickSelect(int a[], int l, int r, int k){int p = rand() % (r - l + 1) + l;int pivot = a[p];{int t = a[p]; a[p] = a[r]; a[r] = t; }int i = l,j = r;while (i j){while (i ja[i] pivot) i++;if (i j) {a[j] = a[i];j--;}while (i ja[i] pivot) j--;if (i j) {a[i] = a[j];i++;}}a[i] = pivot;p = i;if (i - l + 1 == k) return a[i];// j + 1, right, k - (j - left + 1)1)if (i - l + 1 k) return quickSelect(a, i+1, r, k-i+l-1);else return quickSelect(a, l, i-1, k);}int main(){int a[] = { 1,4,54,8,3,7,45,58,27,8,25,26,21,12 };printf_s(\"%d \", quickSelect(a, 0, 14, 6));system(\"pause\");return 0;}","tags":["算法"],"categories":["算法"]},{"title":"MobileNetsV2","path":"/2019/08/19/20190819-MobileNetsV2/","content":"论文地址:MobileNetV2: Inverted Residuals and Linear Bottlenecks 非官方代码:pytorch 介绍这篇文章是谷歌在2019提出来的文章在MobileNets 基础上做的改进。 深度可分离卷积示例 首先在Xception 中被广泛使用 好处： 理论上可以成倍的减少卷积层的时间复杂度和空间复杂度 文章内容与MobileNets 的对比 相同点 都采用 Depth-wise (DW) 卷积搭配 Point-wise (PW) 卷积的方式来提特征 不同点 Linear Bottleneck V2 在 DW 卷积之前新加了一个 PW 卷积。 DW卷积由于本身的计算特性不能改变通道数的能力。若通道数很少的话，DW在提取地低纬特征，效果可能并不会好。 在每个DW之前，增加了PW用于升维，这样DW可以更好的提取特征 V2 去掉了第二个 PW 的激活函数 激活函数在高维空间能够有效的增加非线性，而在低维空间时则会破坏特征 第二个 PW 的主要功能就是降维 与ResNet的对比 相同点 MobileNet V2 借鉴 ResNet，都采用了 1x1-3x3-1x1的模式 MobileNet V2 借鉴 ResNet，同样使用 Shortcut 将输出与输入相加 不同点 Inverted Residual Block ResNet 使用 标准卷积 提特征，MobileNet 始终使用 DW卷积 提特征 ResNet 先降维 (0.25倍)、卷积、再升维，而 MobileNet V2 则是 先升维 (6倍)、卷积、再降维。直观的形象上来看，ResNet 的微结构是沙漏形，而 MobileNet V2 则是纺锤形，刚好相反。因此论文作者将 MobileNet V2 的结构称为 Inverted Residual Block。使用DW卷积而作的适配，特征提取能够在高维进行 结论 MobileNets 与MobileNets V2在模型结构上的对比 MobileNetsV2 的卷积层数比V1要多，但是时间复杂度，以及空间复杂度，以及在cpu上的推理时间要远远优于MobileNets 参考文章：https://zhuanlan.zhihu.com/p/33075914","tags":["深度学习"],"categories":["深度学习"]},{"title":"MobileNets","path":"/2019/08/19/20190819-MobileNets/","content":"论文地址:MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications 非官方代码:pytorch/models 前言这篇文章是谷歌在2017针对手机等嵌入式设备提出的一种轻量级深层网络，这篇论文主要的贡献点在于提出了一种深度可分离卷积。 主要解决的问题是注重优化延迟，同时也兼顾了模型的大小，不像有些模型虽然参数量比较小，但是速度也是慢的可以。 MobileNets使用了大量的3 × 3的卷积核，极大地减少了计算量（1/8到1/9之间），同时准确率下降的很少，相比其他的方法确有优势。 深度可分离卷积示例 模型结构和训练MobileNets结构建立在上述深度可分解卷积中（只有第一层是标准卷积）。该网络允许我们探索网络拓扑，找到一个适合的良好网络。其具体架构在表1说明。除了最后的全连接层，所有层后面跟了batchnorm和ReLU，最终输入到softmax进行分类。图3对比了标准卷积和分解卷积的结构，二者都附带了BN和ReLU层。按照作者的计算方法，MobileNets总共28层（1 + 2 × 13 + 1 = 28）","tags":["深度学习"],"categories":["深度学习"]},{"title":"多层感知机的反向传播","path":"/2019/08/19/MLP-Back-Propagation/","content":"全连接神经网络是形式上最简单的神经网络，反向传播算法是一种常用的训练神经网络的算法，理解全连接神经网络中的反向传播算法是理解其他更加复杂网络中反向传播算法的重要基础。 参考链接： https://zhuanlan.zhihu.com/p/61863634","tags":["编程"],"categories":["深度学习"]},{"title":"MobileNetsV2","path":"/2019/08/19/MobileNetsV2/","content":"论文地址:MobileNetV2: Inverted Residuals and Linear Bottlenecks 非官方代码:pytorch 介绍这篇文章是谷歌在2019提出来的文章在MobileNets 基础上做的改进。 深度可分离卷积示例 首先在Xception 中被广泛使用 好处： 理论上可以成倍的减少卷积层的时间复杂度和空间复杂度 文章内容与MobileNets 的对比 相同点 都采用 Depth-wise (DW) 卷积搭配 Point-wise (PW) 卷积的方式来提特征 不同点 Linear Bottleneck V2 在 DW 卷积之前新加了一个 PW 卷积。 DW卷积由于本身的计算特性不能改变通道数的能力。若通道数很少的话，DW在提取地低纬特征，效果可能并不会好。 在每个DW之前，增加了PW用于升维，这样DW可以更好的提取特征 V2 去掉了第二个 PW 的激活函数 激活函数在高维空间能够有效的增加非线性，而在低维空间时则会破坏特征 第二个 PW 的主要功能就是降维 与ResNet的对比 相同点 MobileNet V2 借鉴 ResNet，都采用了 1x1-3x3-1x1的模式 MobileNet V2 借鉴 ResNet，同样使用 Shortcut 将输出与输入相加 不同点 Inverted Residual Block ResNet 使用 标准卷积 提特征，MobileNet 始终使用 DW卷积 提特征 ResNet 先降维 (0.25倍)、卷积、再升维，而 MobileNet V2 则是 先升维 (6倍)、卷积、再降维。直观的形象上来看，ResNet 的微结构是沙漏形，而 MobileNet V2 则是纺锤形，刚好相反。因此论文作者将 MobileNet V2 的结构称为 Inverted Residual Block。使用DW卷积而作的适配，特征提取能够在高维进行 结论 MobileNets 与MobileNets V2在模型结构上的对比 MobileNetsV2 的卷积层数比V1要多，但是时间复杂度，以及空间复杂度，以及在cpu上的推理时间要远远优于MobileNets 参考文章：https://zhuanlan.zhihu.com/p/33075914","tags":["PyTorch"],"categories":["深度学习"]},{"title":"MobileNets","path":"/2019/08/19/MobileNets/","content":"论文地址:MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications 非官方代码:pytorch/models 前言这篇文章是谷歌在2017针对手机等嵌入式设备提出的一种轻量级深层网络，这篇论文主要的贡献点在于提出了一种深度可分离卷积。 主要解决的问题是注重优化延迟，同时也兼顾了模型的大小，不像有些模型虽然参数量比较小，但是速度也是慢的可以。 MobileNets使用了大量的3 × 3的卷积核，极大地减少了计算量（1/8到1/9之间），同时准确率下降的很少，相比其他的方法确有优势。 深度可分离卷积示例 模型结构和训练MobileNets结构建立在上述深度可分解卷积中（只有第一层是标准卷积）。该网络允许我们探索网络拓扑，找到一个适合的良好网络。其具体架构在表1说明。除了最后的全连接层，所有层后面跟了batchnorm和ReLU，最终输入到softmax进行分类。图3对比了标准卷积和分解卷积的结构，二者都附带了BN和ReLU层。按照作者的计算方法，MobileNets总共28层（1 + 2 × 13 + 1 = 28）","tags":["PyTorch"],"categories":["深度学习"]},{"title":"分治算法","path":"/2019/08/19/分治算法/","content":"问题： 给定一组数，要求从中找出第k小的元素。分析：这里通过快速排序算法来解决次问题。记一趟快速排序后，左子集中的元素个数为nleft，则选择问题，可能是一下几种情况之一： nleft等于k-1，则枢纽值即为所求； nleft大于k-1,则继续在左子树中找； nleft小于k-1,则继续在右子集中找C++代码实现 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 | // t5.cpp : 定义控制台应用程序的入口点。 // #include \"stdafx.h\" #include \"stdafx.h\" #includestdio.h #includestdlib.h #includetime.h #includemath.h #includestring.h int quickSelect(int a[], int l, int r, int k) { int p = rand() % (r - l + 1) + l; int pivot = a[p]; {int t = a[p]; a[p] = a[r]; a[r] = t; } int i = l,j = r; while (i j) { while (i ja[i] pivot) i++; if (i j) { a[j] = a[i]; j--; } while (i ja[i] pivot) j--; if (i j) { a[i] = a[j]; i++; } } a[i] = pivot; p = i; if (i - l + 1 == k) return a[i]; // j + 1, right, k - (j - left + 1)1) if (i - l + 1 k) return quickSelect(a, i+1, r, k-i+l-1); else return quickSelect(a, l, i-1, k); } int main() { int a[] = { 1,4,54,8,3,7,45,58,27,8,25,26,21,12 }; printf_s(\"%d \", quickSelect(a, 0, 14, 6)); system(\"pause\"); return 0; } —|—","tags":["C++"],"categories":["算法"]},{"title":"算法竞赛入门经典第四章","path":"/2019/08/18/20190818-algorithm4/","content":"// t4.cpp : 定义控制台应用程序的入口点。//#include \"stdafx.h\"//素数判断方法2#includestdio.h#includestdlib.h#includetime.h#includemath.h#includestring.h#define Max 1000000#define maxn 100int left, chance; //还需要left位置，错chance之后就会输char s[maxn], s2[maxn]; //答案是字符串s, 玩家猜的字母序列是s2int win, lose; // win=1 表示已经赢了; lose=1 表示已经输了long fac(int n){\tif (0 == n || 1 == n)\t{ return 1;\t}\telse\t{ return fac(n - 1)*n;\t}}// 非递归的阶乘方法long fact(int n){\tlong iRes = 1;\tfor (int i = 1; i = n; i++)\t{ iRes *= i;\t}\treturn iRes;}int combination(int m, int n){\tlong iRes = fac(n) / (fac(m)*fac(n - m));\t// long iRes = fact(n) /(fac(m)*fac(n-m));\tprintf_s(\"%ld %.2lf \", iRes, (double)clock() / CLOCKS_PER_SEC);\treturn iRes;}// 刽子手游戏---guess函数void guess(char ch){\tint bad = 1;\tfor (int i = 0; i strlen(s); i++) if (s[i] == ch) { left--; s[i] = ' '; bad = 0; }\tif (bad) --chance;\tif (!chance) lose = 1;\tif (!left) win = 1;}int main(){\t/*\t关键:\t1 用素数筛选法先预处理，默认刚开始全为素数，然后对素数的倍数标记为非素数， for(int j = i*i ; j = 10000 ; j += i){iPrimeArr[j] = 1;}\t2 通过开根号判断素数时，可以用floor,注意浮点数加上0.5，int iRadical = floor(sqrt(n*1.0) + 0.5);\t3 可以用assert()对输入的合法性进行校验，assert(n = 5 n = 10000);void assert(int exp),如果表达式的值为0则退出。*/\t//int sum = 1;\t//for (int i = 3; i = Max; i += 2)\t//{\t//\t//因为偶数除了2 都不是质数\t//\tint j;\t//\tfor (j = 2; j = (int)sqrt(i); j++)//利用上述结论判断\t// if (i%j == 0) break;\t//\tif (j (int)sqrt(i))\t// sum++;\t//}\t//printf_s(\"Time used = %0.2f s \", (double)clock() / CLOCKS_PER_SEC);\t//printf_s(\"%d \", sum);\t//int a, b;\t//scanf_s(\"%d %d\", a, b);\t//combination(a, b);\t//刽子手游戏\t/*\t游戏规则是这样的：计算机想一个单词让你猜，你每次可以猜一个字母。 如果单词里有那个字母，所有该字母会显示出来；如果没有那个字母，则计算机会在一幅“刽子手”画上填一笔。 这幅画一共需要7笔就能完成，因此你最多只能错6次。 注意，猜一个已经猜过的字母也算错。\t在本题中，你的任务是编写一个“裁判”程序，输入单词和玩家的猜测，判断玩家赢了（You win.）、 输了（You lose.）还是放弃了（You chickened out.）。 每组数据包含3行，第1行是游戏编号（-1为输入结束标记），第2行是计算机想的单词，第3行是玩家的猜测。 后两行保证只含小写字母。\t*/\t//char ans[21];\t//char gus[28];\t//int times, yes;\t//int chances = 7;\t//int win = 0;\t//int lose = 0;\t//scanf_s(\"%d\", times);\t//while (times != -1)\t//{\t//\tprintf_s(\"Round %d \", times);\t//\tscanf_s(\"%s %s\", ans, gus);\t//\tprintf_s(\"Round %s s \", ans, gus);\t//\tyes = 0;\t//\twin = 0, lose = 0, chances = 7;\t//\tfor (int i = 0; i strlen(gus); i++)\t//\t{\t// int flag = 0;\t// for (int j = 0; j strlen(ans); j++)\t// {\t// if (ans[j] == gus[i])\t// {\t// yes++;\t// flag = 1;//找到之后不退出，因为有一个有相同的字母\t// }\t// if (flag == 0)\t// {\t// chances--;\t// }\t// if (chances == 0)\t// {\t// lose = 1;\t// printf_s(\"You lose. \");\t// }\t// else if (yes == strlen(ans))\t// {\t// win = 1;\t// printf_s(\"You win. \");\t// break;\t// }\t// if (win != 1 lose != 1)\t// {\t// printf(\"You chickened out. \");\t// }\t// scanf_s(\"%d\", times);\t// }\t//\t}\t//}\tint rnd;\twhile (scanf_s(\"%d%s%s\", rnd, s, s2) == 3 rnd != -1)\t{ printf_s(\"%Round %d \", rnd); win = lose = 0; left = strlen(s); chance = 7; for (int i = 0; i strlen(s2); i++) { guess(s2[i]); //猜一个字母 if (win || lose) break; //检查状态 } //根据结果进行输出 if (win) printf_s(\"You win. \"); else if (lose) printf_s(\"You lose. \"); else printf_s(\"You chickened out. \");\t}\tsystem(\"pause\"); return 0;}","tags":["算法"],"categories":["算法"]},{"title":"算法竞赛入门经典第四章","path":"/2019/08/18/algorithm4/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 | // t4.cpp : 定义控制台应用程序的入口点。 // #include \"stdafx.h\" //素数判断方法2 #includestdio.h #includestdlib.h #includetime.h #includemath.h #includestring.h #define Max 1000000 #define maxn 100 int left, chance; //还需要left位置，错chance之后就会输 char s[maxn], s2[maxn]; //答案是字符串s, 玩家猜的字母序列是s2 int win, lose; // win=1 表示已经赢了; lose=1 表示已经输了 long fac(int n) { if (0 == n || 1 == n) { return 1; } else { return fac(n - 1)*n; } } // 非递归的阶乘方法 long fact(int n) { long iRes = 1; for (int i = 1; i = n; i++) { iRes *= i; } return iRes; } int combination(int m, int n) { long iRes = fac(n) / (fac(m)*fac(n - m)); // long iRes = fact(n) /(fac(m)*fac(n-m)); printf_s(\"%ld %.2lf \", iRes, (double)clock() / CLOCKS_PER_SEC); return iRes; } // 刽子手游戏---guess函数 void guess(char ch) { int bad = 1; for (int i = 0; i strlen(s); i++) if (s[i] == ch) { left--; s[i] = ' '; bad = 0; } if (bad) --chance; if (!chance) lose = 1; if (!left) win = 1; } int main() { /* 关键: 1 用素数筛选法先预处理，默认刚开始全为素数，然后对素数的倍数标记为非素数， for(int j = i*i ; j = 10000 ; j += i){iPrimeArr[j] = 1;} 2 通过开根号判断素数时，可以用floor,注意浮点数加上0.5，int iRadical = floor(sqrt(n*1.0) + 0.5); 3 可以用assert()对输入的合法性进行校验，assert(n = 5 n = 10000);void assert(int exp),如果表达式的值为0则退出。*/ //int sum = 1; //for (int i = 3; i = Max; i += 2) //{ //\t//因为偶数除了2 都不是质数 //\tint j; //\tfor (j = 2; j = (int)sqrt(i); j++)//利用上述结论判断 // if (i%j == 0) break; //\tif (j (int)sqrt(i)) // sum++; //} //printf_s(\"Time used = %0.2f s \", (double)clock() / CLOCKS_PER_SEC); //printf_s(\"%d \", sum); //int a, b; //scanf_s(\"%d %d\", a, b); //combination(a, b); //刽子手游戏 /* 游戏规则是这样的：计算机想一个单词让你猜，你每次可以猜一个字母。 如果单词里有那个字母，所有该字母会显示出来；如果没有那个字母，则计算机会在一幅“刽子手”画上填一笔。 这幅画一共需要7笔就能完成，因此你最多只能错6次。 注意，猜一个已经猜过的字母也算错。 在本题中，你的任务是编写一个“裁判”程序，输入单词和玩家的猜测，判断玩家赢了（You win.）、 输了（You lose.）还是放弃了（You chickened out.）。 每组数据包含3行，第1行是游戏编号（-1为输入结束标记），第2行是计算机想的单词，第3行是玩家的猜测。 后两行保证只含小写字母。 */ //char ans[21]; //char gus[28]; //int times, yes; //int chances = 7; //int win = 0; //int lose = 0; //scanf_s(\"%d\", times); //while (times != -1) //{ //\tprintf_s(\"Round %d \", times); //\tscanf_s(\"%s %s\", ans, gus); //\tprintf_s(\"Round %s s \", ans, gus); //\tyes = 0; //\twin = 0, lose = 0, chances = 7; //\tfor (int i = 0; i strlen(gus); i++) //\t{ // int flag = 0; // for (int j = 0; j strlen(ans); j++) // { // if (ans[j] == gus[i]) // { // yes++; // flag = 1;//找到之后不退出，因为有一个有相同的字母 // } // if (flag == 0) // { // chances--; // } // if (chances == 0) // { // lose = 1; // printf_s(\"You lose. \"); // } // else if (yes == strlen(ans)) // { // win = 1; // printf_s(\"You win. \"); // break; // } // if (win != 1 lose != 1) // { // printf(\"You chickened out. \"); // } // scanf_s(\"%d\", times); // } //\t} //} int rnd; while (scanf_s(\"%d%s%s\", rnd, s, s2) == 3 rnd != -1) { printf_s(\"%Round %d \", rnd); win = lose = 0; left = strlen(s); chance = 7; for (int i = 0; i strlen(s2); i++) { guess(s2[i]); //猜一个字母 if (win || lose) break; //检查状态 } //根据结果进行输出 if (win) printf_s(\"You win. \"); else if (lose) printf_s(\"You lose. \"); else printf_s(\"You chickened out. \"); } system(\"pause\"); return 0; } —|—","tags":["C++"],"categories":["算法"]},{"title":"LNMS","path":"/2019/08/16/20190816-LNMS/","content":"locality NMSLNMS是在EAST文本检测中提出的．主要原因：文本检测面临的是成千上万个几何体，如果用普通的NMS，其计算复杂度，n是几何体的个数，这是不可接受的．对上述时间复杂度问题，EAST提出了基于行合并几何体的方法，当然这是基于邻近几个几何体是高度相关的假设．注意：这里合并的四边形坐标是通过两个给定四边形的得分进行加权平均的，也就是说这里是“平均”而不是”选择”几何体*,目的是减少计算量．基本步骤1.先对所有的output box集合结合相应的阈值（大于阈值则进行合并，小于阈值则不和并），依次遍历进行加权合并，得到合并后的bbox集合；2.对合并后的bbox集合进行标准的NMS操作 def detect(score_map, geo_map, timer, score_map_thresh=1e-5, box_thresh=1e-8, nms_thres=0.1): ''' restore text boxes from score map and geo map :param score_map: bs* 128 * 128 * 1 :param geo_map: ## geo_map = bs * 128 * 128 * 5 :param timer: :param score_map_thresh: threshhold for score map :param box_thresh: threshhold for boxes :param nms_thres: threshold for nms :return: ''' if len(score_map.shape) == 4: score_map = score_map[0, :, :, 0] geo_map = geo_map[0, :, :, ] # filter the score map xy_text = np.argwhere(score_map score_map_thresh) # sort the text boxes via the y axis xy_text = xy_text[np.argsort(xy_text[:, 0])] # restore start = time.time() text_box_restored = restore_rectangle(xy_text[:, ::-1]*4, geo_map[xy_text[:, 0], xy_text[:, 1], :]) # N*4*2 #print('{} text boxes before nms'.format(text_box_restored.shape[0])) boxes = np.zeros((text_box_restored.shape[0], 9), dtype=np.float32) boxes[:, :8] = text_box_restored.reshape((-1, 8)) boxes[:, 8] = score_map[xy_text[:, 0], xy_text[:, 1]] timer['restore'] = time.time() - start # 得到box 的坐标以及分数 # nms part start = time.time() # boxes = nms_locality.nms_locality(boxes.astype(np.float64), nms_thres) boxes = lanms.merge_quadrangle_n9(boxes.astype('float32'), nms_thres) timer['nms'] = time.time() - start if boxes.shape[0] == 0: return None, timer # here we filter some low score boxes by the average score map, this is different from the orginal paper for i, box in enumerate(boxes): mask = np.zeros_like(score_map, dtype=np.uint8) cv2.fillPoly(mask, box[:8].reshape((-1, 4, 2)).astype(np.int32) // 4, 1) boxes[i, 8] = cv2.mean(score_map, mask)[0] boxes = boxes[boxes[:, 8] box_thresh] return boxes, timer import numpy as npfrom shapely.geometry import Polygondef intersection(g, p): #取g,p中的几何体信息组成多边形 g = Polygon(g[:8].reshape((4, 2))) p = Polygon(p[:8].reshape((4, 2))) # 判断g,p是否为有效的多边形几何体 if not g.is_valid or not p.is_valid: return 0 # 取两个几何体的交集和并集 inter = Polygon(g).intersection(Polygon(p)).area union = g.area + p.area - inter if union == 0: return 0 else: return inter/uniondef weighted_merge(g, p): # 取g,p两个几何体的加权（权重根据对应的检测得分计算得到） g[:8] = (g[8] * g[:8] + p[8] * p[:8])/(g[8] + p[8]) #合并后的几何体的得分为两个几何体得分的总和 g[8] = (g[8] + p[8]) return gdef standard_nms(S, thres): #标准NMS order = np.argsort(S[:, 8])[::-1] keep = [] while order.size 0: i = order[0] keep.append(i) ovr = np.array([intersection(S[i], S[t]) for t in order[1:]]) inds = np.where(ovr = thres)[0] order = order[inds+1] return S[keep]def nms_locality(polys, thres=0.3): ''' locality aware nms of EAST :param polys: a N*9 numpy array. first 8 coordinates, then prob :return: boxes after nms ''' S = [] #合并后的几何体集合 p = None #合并后的几何体 for g in polys: if p is not None and intersection(g, p) thres: #若两个几何体的相交面积大于指定的阈值，则进行合并 p = weighted_merge(g, p) else: #反之，则保留当前的几何体 if p is not None: S.append(p) p = g if p is not None: S.append(p) if len(S) == 0: return np.array([]) return standard_nms(np.array(S), thres)if __name__ == '__main__': # 343,350,448,135,474,143,369,359 print(Polygon(np.array([[343, 350], [448, 135], [474, 143], [369, 359]])).area) 参考博客： https://www.jianshu.com/p/4934875f7eb6","tags":["计算机视觉"],"categories":["计算机视觉"]},{"title":"算法竞赛入门经典第三章","path":"/2019/08/16/20190816-algorithm3/","content":"今天心情不好，所以就敲了这一点代码，希望明天的状态能好一点！ // t3.cpp : 定义控制台应用程序的入口点。//#include \"stdafx.h\"#include \"stdio.h\"#include stdlib.h#includemath.h#includetime.h#define maxn 10000000 +10char s[maxn];int main(){\t//题目1 统计个数\t/*int count =0;\tint temp;\twhile (~scanf_s(\"%d\", temp))\t{ count++;\t}\tprintf_s(\"%d\", count);*/ //显示不出来\t//输入一些数，求最大值、最小值和平均数\t//int min, max, n, sum,count = 0;\t//float avg;\t//scanf_s(\"%d\", n);\t//max = n;\t//min = n;\t//sum = n;\t//count++;\t//while (scanf_s(\"%d\", n)!=EOF)\t//{\t//\tcount++;\t//\tsum += n;\t//\tif (n max)\t// max = n;\t//\tif (n min)\t// min = n;\t//}\t//avg = sum*1.0 / count;\t//printf_s(\"%d %d %f \", max,min,avg);\t//5.输入一些数 ，求出他们的方差\t/*double ave, sum = 0, varance, psum = 0;\tint n.a[110], count = 0;\twhile (scanf_s(\"%d\", n) != EOF)\t{ a[count++] = n; sum += n;\t}\tavg = sum*1.0 / count;\tfor (int i = 0; i count; i++) psum += (a[i] - ave)*(a[i] - ave);\tvariance = psum / count;\tprint(\"%lf\", variance);*/\tscanf_s(\"%s\", s);\tint tot = 0;\tfor (int i = 0;s[i]; i++) { if (s[i] == '1') tot++;\t}\tprintf_s(\"%d \", tot);\tsystem(\"pause\"); return 0;}","tags":["算法"],"categories":["算法"]},{"title":"LNMS","path":"/2019/08/16/LNMS/","content":"locality NMSLNMS是在EAST文本检测中提出的．主要原因：文本检测面临的是成千上万个几何体，如果用普通的NMS，其计算复杂度，n是几何体的个数，这是不可接受的．对上述时间复杂度问题，EAST提出了基于行合并几何体的方法，当然这是基于邻近几个几何体是高度相关的假设．注意：这里合并的四边形坐标是通过两个给定四边形的得分进行加权平均的，也就是说这里是“平均”而不是”选择”几何体*,目的是减少计算量．基本步骤1.先对所有的output box集合结合相应的阈值（大于阈值则进行合并，小于阈值则不和并），依次遍历进行加权合并，得到合并后的bbox集合；2.对合并后的bbox集合进行标准的NMS操作 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 | def detect(score_map, geo_map, timer, score_map_thresh=1e-5, box_thresh=1e-8, nms_thres=0.1): ''' restore text boxes from score map and geo map :param score_map: bs* 128 * 128 * 1 :param geo_map: ## geo_map = bs * 128 * 128 * 5 :param timer: :param score_map_thresh: threshhold for score map :param box_thresh: threshhold for boxes :param nms_thres: threshold for nms :return: ''' if len(score_map.shape) == 4: score_map = score_map[0, :, :, 0] geo_map = geo_map[0, :, :, ] # filter the score map xy_text = np.argwhere(score_map score_map_thresh) # sort the text boxes via the y axis xy_text = xy_text[np.argsort(xy_text[:, 0])] # restore start = time.time() text_box_restored = restore_rectangle(xy_text[:, ::-1]*4, geo_map[xy_text[:, 0], xy_text[:, 1], :]) # N*4*2 #print('{} text boxes before nms'.format(text_box_restored.shape[0])) boxes = np.zeros((text_box_restored.shape[0], 9), dtype=np.float32) boxes[:, :8] = text_box_restored.reshape((-1, 8)) boxes[:, 8] = score_map[xy_text[:, 0], xy_text[:, 1]] timer['restore'] = time.time() - start # 得到box 的坐标以及分数 # nms part start = time.time() # boxes = nms_locality.nms_locality(boxes.astype(np.float64), nms_thres) boxes = lanms.merge_quadrangle_n9(boxes.astype('float32'), nms_thres) timer['nms'] = time.time() - start if boxes.shape[0] == 0: return None, timer # here we filter some low score boxes by the average score map, this is different from the orginal paper for i, box in enumerate(boxes): mask = np.zeros_like(score_map, dtype=np.uint8) cv2.fillPoly(mask, box[:8].reshape((-1, 4, 2)).astype(np.int32) // 4, 1) boxes[i, 8] = cv2.mean(score_map, mask)[0] boxes = boxes[boxes[:, 8] box_thresh] return boxes, timer —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 | import numpy as np from shapely.geometry import Polygon def intersection(g, p): #取g,p中的几何体信息组成多边形 g = Polygon(g[:8].reshape((4, 2))) p = Polygon(p[:8].reshape((4, 2))) # 判断g,p是否为有效的多边形几何体 if not g.is_valid or not p.is_valid: return 0 # 取两个几何体的交集和并集 inter = Polygon(g).intersection(Polygon(p)).area union = g.area + p.area - inter if union == 0: return 0 else: return inter/union def weighted_merge(g, p): # 取g,p两个几何体的加权（权重根据对应的检测得分计算得到） g[:8] = (g[8] * g[:8] + p[8] * p[:8])/(g[8] + p[8]) #合并后的几何体的得分为两个几何体得分的总和 g[8] = (g[8] + p[8]) return g def standard_nms(S, thres): #标准NMS order = np.argsort(S[:, 8])[::-1] keep = [] while order.size 0: i = order[0] keep.append(i) ovr = np.array([intersection(S[i], S[t]) for t in order[1:]]) inds = np.where(ovr = thres)[0] order = order[inds+1] return S[keep] def nms_locality(polys, thres=0.3): ''' locality aware nms of EAST :param polys: a N*9 numpy array. first 8 coordinates, then prob :return: boxes after nms ''' S = [] #合并后的几何体集合 p = None #合并后的几何体 for g in polys: if p is not None and intersection(g, p) thres: #若两个几何体的相交面积大于指定的阈值，则进行合并 p = weighted_merge(g, p) else: #反之，则保留当前的几何体 if p is not None: S.append(p) p = g if p is not None: S.append(p) if len(S) == 0: return np.array([]) return standard_nms(np.array(S), thres) if __name__ == '__main__': # 343,350,448,135,474,143,369,359 print(Polygon(np.array([[343, 350], [448, 135], [474, 143], [369, 359]])).area) —|— 参考博客： https://www.jianshu.com/p/4934875f7eb6","tags":["OpenCV"],"categories":["深度学习"]},{"title":"算法竞赛入门经典第三章","path":"/2019/08/16/algorithm3/","content":"今天心情不好，所以就敲了这一点代码，希望明天的状态能好一点！ 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 | // t3.cpp : 定义控制台应用程序的入口点。 // #include \"stdafx.h\" #include \"stdio.h\" #include stdlib.h #includemath.h #includetime.h #define maxn 10000000 +10 char s[maxn]; int main() { //题目1 统计个数 /*int count =0; int temp; while (~scanf_s(\"%d\", temp)) { count++; } printf_s(\"%d\", count);*/ //显示不出来 //输入一些数，求最大值、最小值和平均数 //int min, max, n, sum,count = 0; //float avg; //scanf_s(\"%d\", n); //max = n; //min = n; //sum = n; //count++; //while (scanf_s(\"%d\", n)!=EOF) //{ //\tcount++; //\tsum += n; //\tif (n max) // max = n; //\tif (n min) // min = n; //} //avg = sum*1.0 / count; //printf_s(\"%d %d %f \", max,min,avg); //5.输入一些数 ，求出他们的方差 /*double ave, sum = 0, varance, psum = 0; int n.a[110], count = 0; while (scanf_s(\"%d\", n) != EOF) { a[count++] = n; sum += n; } avg = sum*1.0 / count; for (int i = 0; i count; i++) psum += (a[i] - ave)*(a[i] - ave); variance = psum / count; print(\"%lf\", variance);*/ scanf_s(\"%s\", s); int tot = 0; for (int i = 0;s[i]; i++) { if (s[i] == '1') tot++; } printf_s(\"%d \", tot); system(\"pause\"); return 0; } —|—","tags":["C++"],"categories":["其他"]},{"title":"算法竞赛入门经典第二章","path":"/2019/08/15/20190815-algorithm2/","content":"// t2.cpp : 定义控制台应用程序的入口点。//#include \"stdafx.h\"#include \"stdio.h\"#include stdlib.h#includemath.h#includetime.hint main(){\t////3n+1问题、\t//int n, count = 0; //当n过大的时候 2*n溢出\t//\t//scanf_s(\"%d\", n);\t//long long n2 = n;\t//while (n2 1)\t//{\t//\tif (n2 % 2 == 1) n2 = n2 * 3 + 1;\t//\telse n2 /= 2;\t//\tcount++;\t//}\t//printf_s(\"%d \", count);\t//近似计算\t/*double sum = 0;\tfor (int i = 0;; i++) { double term = 1.0 / (i * 2 + 1); if (i % 2 == 0) sum += term; else sum -= term; if (term 1e-6) break;\t}\tprintf_s(\"%.6f \", sum);*/\t// 阶乘之和 只保存后六位\t//int n, S = 0;\t//scanf_s(\"%d\", n);\t//for (int i = 1; i = n; i++)\t//{\t//\tint factorial = 1;\t//\tfor (int j = 1; j = i; j++)\t// factorial *= j;\t//\tS += factorial;\t//}\t//printf_s(\"%d \", S % 1000000);\t// 阶乘之和2, 优化版本\t//int n, S = 0;\t//const int MOD = 1000000;\t//scanf_s(\"%d\", n);\t//for (int i = 1; i = n; i++)\t//{\t//\tint factorial = 1;\t//\tfor (int j = 1; j = i; j++)\t// factorial = (factorial *j)%MOD;\t//\tS += factorial;\t//}\t//printf_s(\"%d \", S %MOD);\t//printf_s(\"Time used = %.2f \", (double)clock() / CLOCKS_PER_SEC); // 得到程序运行的时间 单位：秒\t// 数据统计 //习题2.1 水仙花数目\t/*int sum;\tfor (int i = 1; i 10; i++) for (int k = 0; k 10; k++) for (int j = 0; j 10; j++) { sum = i * 100 + 10 * k + j; if(sum == i*i*i+j*j*j + k*k*k) printf_s(\"%d \", sum); }*/\t//习题2.2 韩信点兵 相传韩信才智过人，从不直接清点自己军队的人数，只要让士兵先后以三人一排、五人一排、七人一排地变换队形，\t//而他每次只掠一眼队伍的排尾就知道总人数了。输入包含多组数据，每组数据包含3个非负整数a，b，c，表示每种队形排尾的人数（a＜3，b＜5，c＜7），\t//输出总人数的最小值（或报告无解）。已知总人数不小于10，不超过100。输入到文件结束为止。\t//int i, a, b, c;\t//scanf_s(\"%d%d%d\", a, b, c);\t//for (i = 0; i = 100; i++) {\t//\tif (i % 3 == a i % 5 == b i % 7 == c)\t// printf_s(\"%d \", i);\t//\t//}\t//if (i % 3 != a i % 5 != b i % 7 != c i100)\t// printf_s(\"No answer \"); //习题2.3 倒三角形\t//int n, /* 输出n行; n=20 */\t//\ti, /* 打印第i行 */\t//\tj;\t//scanf_s(\"%d\", n);\t//for (i = 1; i = n; i = i + 1) {\t//\t/* 在第i行，打印(i-1)个空格 */\t//\tfor (j = 1; j = i - 1; j = j + 1) printf_s(\" \");\t//\t/* 在第i行，打印(2*n-2*i+1)个# */\t//\tfor (j = 1; j = (2 * n - 2 * i + 1); j = j + 1) printf_s(\"#\");\t//\tprintf_s(\" \"); /* 输出结束后换行，否则所有的#号在同一行输出 */\t//}\t//习题2.4 子序列的和 输入两个正整数n＜m＜10 6 ，输出 ，保留5位小数。输入包含多组数据， 注：陷阱就是在n特别大时如果直接n*n就会溢出，所以只能连除两次\t//int count = 0;\t//while (1) {\t//\tint n = 0;\t//\tint m = 0;\t//\tscanf_s(\"%d\", n);\t//\tscanf_s(\"%d\", m);\t//\tif (n == mn == 0) {\t// break;\t//\t}\t//\tcount++;\t//\tdouble sum = 0;\t//\tfor (int i = n; i = m; i++) {\t// sum += 1.0 / i / i;\t//\t}\t//\tprintf_s(\"Case %d:%.5f \", count, sum);\t//} //习题2.5 分数化小数（decimal） //输入正整数a，b，c，输出a / b的小数形式，精确到小数点后c位。a，b≤10 ^ 6，c≤100。输入包含多组数据，结束标记为a＝b＝c＝0。 // int count = 0;\t//while (1) {\t//\tint a, b, c;\t//\tint k, d, i;\t//\tscanf_s(\"%d\", a);\t//\tscanf_s(\"%d\", b);\t//\tscanf_s(\"%d\", c);\t//\tif (a == 0b == 0 c==0 ) {\t// break;\t//\t}\t//\tcount++;\t//\tfor (i = 0; ic - 1; i++)\t//\t{\t// /*机智地把余数放大十倍，使之除以b并取模*/\t// k = (k%b) * 10;\t// printf_s(\"Case %d:%.5f \", count, k / b);\t//\t}\t//\tk = (k%b) * 10;\t//\td = (k%b) * 10 / b;\t//\tif (d = 5)//判断第c+1位小数是否大于等于5，if yes,第c位小数要进1\t//\t{\t// printf_s(\"Case %d:%.5f \", count, k / b + 1);\t//\t}\t//\telse\t//\t{\t// printf_s(\"Case %d:%.5f \", count, k / b);\t//\t}\t//\t//} int abc,def,ghi; int a[10],count=0; memset(a,0,sizeof(a)); // 将a数组中的值全部设置为0 for (abc = 123;abc 333;abc ++) { // 基本可以确定abc的最小值和最大值 def = 2 * abc; ghi = 3 * abc; // 设置数组中所有对应的9位数字位置的值1 a[abc/100] = 1; // a a[abc/10%10] = 1; // b a[abc%10] = 1; // c a[def/100] = 1; // d a[def/10%10] = 1; // e a[def%10] = 1; // f a[ghi/100] = 1; // g a[ghi/10%10] = 1; // h a[ghi%10] = 1; // i int i; for (i=1;i=9;i++) { count += a[i]; } if (count == 9) { printf_s(\"%d %d %d \",abc,def,ghi); } // 重置count 和a数组 count = 0; memset(a,0,sizeof(a)); }\tsystem(\"pause\"); return 0;}","tags":["算法"],"categories":["算法"]},{"title":"算法竞赛入门经典第二章","path":"/2019/08/15/algorithm2/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 | // t2.cpp : 定义控制台应用程序的入口点。 // #include \"stdafx.h\" #include \"stdio.h\" #include stdlib.h #includemath.h #includetime.h int main() { ////3n+1问题、 //int n, count = 0; //当n过大的时候 2*n溢出 // //scanf_s(\"%d\", n); //long long n2 = n; //while (n2 1) //{ //\tif (n2 % 2 == 1) n2 = n2 * 3 + 1; //\telse n2 /= 2; //\tcount++; //} //printf_s(\"%d \", count); //近似计算 /*double sum = 0; for (int i = 0;; i++) { double term = 1.0 / (i * 2 + 1); if (i % 2 == 0) sum += term; else sum -= term; if (term 1e-6) break; } printf_s(\"%.6f \", sum);*/ // 阶乘之和 只保存后六位 //int n, S = 0; //scanf_s(\"%d\", n); //for (int i = 1; i = n; i++) //{ //\tint factorial = 1; //\tfor (int j = 1; j = i; j++) // factorial *= j; //\tS += factorial; //} //printf_s(\"%d \", S % 1000000); // 阶乘之和2, 优化版本 //int n, S = 0; //const int MOD = 1000000; //scanf_s(\"%d\", n); //for (int i = 1; i = n; i++) //{ //\tint factorial = 1; //\tfor (int j = 1; j = i; j++) // factorial = (factorial *j)%MOD; //\tS += factorial; //} //printf_s(\"%d \", S %MOD); //printf_s(\"Time used = %.2f \", (double)clock() / CLOCKS_PER_SEC); // 得到程序运行的时间 单位：秒 // 数据统计 //习题2.1 水仙花数目 /*int sum; for (int i = 1; i 10; i++) for (int k = 0; k 10; k++) for (int j = 0; j 10; j++) { sum = i * 100 + 10 * k + j; if(sum == i*i*i+j*j*j + k*k*k) printf_s(\"%d \", sum); }*/ //习题2.2 韩信点兵 相传韩信才智过人，从不直接清点自己军队的人数，只要让士兵先后以三人一排、五人一排、七人一排地变换队形， //而他每次只掠一眼队伍的排尾就知道总人数了。输入包含多组数据，每组数据包含3个非负整数a，b，c，表示每种队形排尾的人数（a＜3，b＜5，c＜7）， //输出总人数的最小值（或报告无解）。已知总人数不小于10，不超过100。输入到文件结束为止。 //int i, a, b, c; //scanf_s(\"%d%d%d\", a, b, c); //for (i = 0; i = 100; i++) { //\tif (i % 3 == a i % 5 == b i % 7 == c) // printf_s(\"%d \", i); // //} //if (i % 3 != a i % 5 != b i % 7 != c i100) // printf_s(\"No answer \"); //习题2.3 倒三角形 //int n, /* 输出n行; n=20 */ //\ti, /* 打印第i行 */ //\tj; //scanf_s(\"%d\", n); //for (i = 1; i = n; i = i + 1) { //\t/* 在第i行，打印(i-1)个空格 */ //\tfor (j = 1; j = i - 1; j = j + 1) printf_s(\" \"); //\t/* 在第i行，打印(2*n-2*i+1)个# */ //\tfor (j = 1; j = (2 * n - 2 * i + 1); j = j + 1) printf_s(\"#\"); //\tprintf_s(\" \"); /* 输出结束后换行，否则所有的#号在同一行输出 */ //} //习题2.4 子序列的和 输入两个正整数n＜m＜10 6 ，输出 ，保留5位小数。输入包含多组数据， 注：陷阱就是在n特别大时如果直接n*n就会溢出，所以只能连除两次 //int count = 0; //while (1) { //\tint n = 0; //\tint m = 0; //\tscanf_s(\"%d\", n); //\tscanf_s(\"%d\", m); //\tif (n == mn == 0) { // break; //\t} //\tcount++; //\tdouble sum = 0; //\tfor (int i = n; i = m; i++) { // sum += 1.0 / i / i; //\t} //\tprintf_s(\"Case %d:%.5f \", count, sum); //} //习题2.5 分数化小数（decimal） //输入正整数a，b，c，输出a / b的小数形式，精确到小数点后c位。a，b≤10 ^ 6，c≤100。输入包含多组数据，结束标记为a＝b＝c＝0。 // int count = 0; //while (1) { //\tint a, b, c; //\tint k, d, i; //\tscanf_s(\"%d\", a); //\tscanf_s(\"%d\", b); //\tscanf_s(\"%d\", c); //\tif (a == 0b == 0 c==0 ) { // break; //\t} //\tcount++; //\tfor (i = 0; ic - 1; i++) //\t{ // /*机智地把余数放大十倍，使之除以b并取模*/ // k = (k%b) * 10; // printf_s(\"Case %d:%.5f \", count, k / b); //\t} //\tk = (k%b) * 10; //\td = (k%b) * 10 / b; //\tif (d = 5)//判断第c+1位小数是否大于等于5，if yes,第c位小数要进1 //\t{ // printf_s(\"Case %d:%.5f \", count, k / b + 1); //\t} //\telse //\t{ // printf_s(\"Case %d:%.5f \", count, k / b); //\t} // //} int abc,def,ghi; int a[10],count=0; memset(a,0,sizeof(a)); // 将a数组中的值全部设置为0 for (abc = 123;abc 333;abc ++) { // 基本可以确定abc的最小值和最大值 def = 2 * abc; ghi = 3 * abc; // 设置数组中所有对应的9位数字位置的值1 a[abc/100] = 1; // a a[abc/10%10] = 1; // b a[abc%10] = 1; // c a[def/100] = 1; // d a[def/10%10] = 1; // e a[def%10] = 1; // f a[ghi/100] = 1; // g a[ghi/10%10] = 1; // h a[ghi%10] = 1; // i int i; for (i=1;i=9;i++) { count += a[i]; } if (count == 9) { printf_s(\"%d %d %d \",abc,def,ghi); } // 重置count 和a数组 count = 0; memset(a,0,sizeof(a)); } system(\"pause\"); return 0; } —|—","tags":["C++"],"categories":["其他"]},{"title":"算法竞赛入门经典第一章","path":"/2019/08/14/20190814-algorithm1/","content":"// t1.cpp : 定义控制台应用程序的入口点。//#include \"stdafx.h\"#include \"stdio.h\"#include stdlib.h#includemath.hint main(){\t//printf(\"%.1f \", 8.0 / 5.0); // %点后面是保留几位浮点小数1.6\t//printf(\"%.1f \", 8 / 5); // %点后面是保留几位浮点小数 结果为0\t//printf(\"%d \", 8.0 / 5.0); // 整数值用%d输出，实数用%f输出 # int 是2字节 ，float是4字节 溢出\t// //整数/整数等与整数，浮点数除以浮点数等于浮点数\t//printf(\"%.8f \", 1 + 2 * sqrt(3) / (5 - 0.1));\t//int a, b;\t//scanf_s(\"%d%d\", a, b); //scanf_s 中的占位符要和变量的数据类型一一对应，要在变量前面加上“”符号\t//printf(\"%d \", a+b);\t//const double pi = acos(-1.0);\t//double r, h, s1, s2, s;\t//scanf_s(\"%1f%1f\", r, h);\t//s1 = pi*r*r;\t//s2 = 2 * pi*r*h;\t//s = s1*2.0 + s2;\t//printf_s(\"Area = %.3f \", s);\t//交换变量\t//int a, b, m, n;\t//scanf_s(\"%d%d\", a, b);\t///*a = a + b;\t//b = a - b;\t//a = a - b;*/\t//printf_s(\"%d %d \", b, a);\t//int a, b, m, n;\t//scanf_s(\"%d%d\", n, m);\t///*a = a + b;\t//b = a - b;\t//a = a - b;*/\t//a = (4*n-m) / 2;\t//b = n - a;\t//if (m % 2 == 1 || a 0 || b 0)\t//\tprintf_s(\"No answer! \");\t//else\t//\tprintf_s(\"Answer is chicken %d rabbit %d \", a, b);\t//exp1 输入三个整数，求出他们的平均数，保留三位小数\t//int a, b, c;\t//scanf_s(\"%d%d%d\", a, b, c);\t//printf_s(\"%.3f\", (a + b + c) / 3.0);\t//exp2 输入华氏温度f,输出对应的摄氏温度c,保留3位小数.提示 : c = 5(f-32)/9.\t//float f, c;\t//scanf_s(\"%f\", f);\t//c = 5*(f - 32) / 9.0;\t//printf_s(\"摄制温度为： %.3f\", c);\t//exp3\t//int n;\t//scanf_s(\"%d\", n);\t//printf_s(\"%d\", n*(n + 1) / 2);\t//exp4 输入正整数《360 ，输出正弦余弦函数值。提示：使用数学函数\t//int n; // #define PI 3.1415926\t//scanf_s(\"%d\", n);\t//if (n 0 || n360)\t//\tprintf_s(\"error\");\t//else\t//{\t//\tprintf_s(\"%.2f \", cos(n*PI/180));\t//\tprintf_s(\"%.2f\", sin(n*PI / 180));\t//}\t// // exp5 一件衣服95元，若消费满300元可以打85折，输入购买衣服的简书，输出需要支付的金额。保留两位小数\t//int n,money;\t//scanf_s(\"%d\", n);\t//money = n * 95;\t//if (money 300)\t//\tprintf_s(\"%.2f\", money* 0.85);\t//else\t//\tprintf_s(\"%.2f\", money);\t//\t//exp6 判断三个边是否能构成直角三角形 边长均为正整数\t//int a, b, c;\t//scanf_s(\"%d%d%d\", a, b, c);\t//if (((a*a + b*b)== c*c) || ((a*a + c*c) == b*b) || ((c*c + b*b) == a*a))\t//\tprintf_s(\"yes \");\t//else\t//\tprintf_s(\"not a triangle \");\t//exp7 判断一个年是不是闰年 能被4整除但是不能被100整除的年份\t//int n;\t//scanf_s(\"%d\", n);\t//if (n % 4 == 0 n % 100 != 0)\t//\tprintf_s(\"yes \");\t//else\t//\tprintf_s(\"no \");\t//\t//q1: int类型的最大值和最小值是多少 2^(n - 1) - 1 -2^(n - 1)\t//q2; double浮点数能精确到多少位小数？ 遵循IEEE标准的8字节（64位）的double能表示的有效数字的位数是：15 ~ 16\t//q3: double类型的最大正数值和最小正数值\t//q4: 逻辑运算符号的优先级\tsystem(\"pause\"); return 0;} 思考题解析： https://blog.csdn.net/panderang/article/details/54096426","tags":["算法"],"categories":["算法"]},{"title":"算法竞赛入门经典第一章","path":"/2019/08/14/algorithm1/","content":"1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 | // t1.cpp : 定义控制台应用程序的入口点。 // #include \"stdafx.h\" #include \"stdio.h\" #include stdlib.h #includemath.h int main() { //printf(\"%.1f \", 8.0 / 5.0); // %点后面是保留几位浮点小数1.6 //printf(\"%.1f \", 8 / 5); // %点后面是保留几位浮点小数 结果为0 //printf(\"%d \", 8.0 / 5.0); // 整数值用%d输出，实数用%f输出 # int 是2字节 ，float是4字节 溢出 // //整数/整数等与整数，浮点数除以浮点数等于浮点数 //printf(\"%.8f \", 1 + 2 * sqrt(3) / (5 - 0.1)); //int a, b; //scanf_s(\"%d%d\", a, b); //scanf_s 中的占位符要和变量的数据类型一一对应，要在变量前面加上“”符号 //printf(\"%d \", a+b); //const double pi = acos(-1.0); //double r, h, s1, s2, s; //scanf_s(\"%1f%1f\", r, h); //s1 = pi*r*r; //s2 = 2 * pi*r*h; //s = s1*2.0 + s2; //printf_s(\"Area = %.3f \", s); //交换变量 //int a, b, m, n; //scanf_s(\"%d%d\", a, b); ///*a = a + b; //b = a - b; //a = a - b;*/ //printf_s(\"%d %d \", b, a); //int a, b, m, n; //scanf_s(\"%d%d\", n, m); ///*a = a + b; //b = a - b; //a = a - b;*/ //a = (4*n-m) / 2; //b = n - a; //if (m % 2 == 1 || a 0 || b 0) //\tprintf_s(\"No answer! \"); //else //\tprintf_s(\"Answer is chicken %d rabbit %d \", a, b); //exp1 输入三个整数，求出他们的平均数，保留三位小数 //int a, b, c; //scanf_s(\"%d%d%d\", a, b, c); //printf_s(\"%.3f\", (a + b + c) / 3.0); //exp2 输入华氏温度f,输出对应的摄氏温度c,保留3位小数.提示 : c = 5(f-32)/9. //float f, c; //scanf_s(\"%f\", f); //c = 5*(f - 32) / 9.0; //printf_s(\"摄制温度为： %.3f\", c); //exp3 //int n; //scanf_s(\"%d\", n); //printf_s(\"%d\", n*(n + 1) / 2); //exp4 输入正整数《360 ，输出正弦余弦函数值。提示：使用数学函数 //int n; // #define PI 3.1415926 //scanf_s(\"%d\", n); //if (n 0 || n360) //\tprintf_s(\"error\"); //else //{ //\tprintf_s(\"%.2f \", cos(n*PI/180)); //\tprintf_s(\"%.2f\", sin(n*PI / 180)); //} // // exp5 一件衣服95元，若消费满300元可以打85折，输入购买衣服的简书，输出需要支付的金额。保留两位小数 //int n,money; //scanf_s(\"%d\", n); //money = n * 95; //if (money 300) //\tprintf_s(\"%.2f\", money* 0.85); //else //\tprintf_s(\"%.2f\", money); // //exp6 判断三个边是否能构成直角三角形 边长均为正整数 //int a, b, c; //scanf_s(\"%d%d%d\", a, b, c); //if (((a*a + b*b)== c*c) || ((a*a + c*c) == b*b) || ((c*c + b*b) == a*a)) //\tprintf_s(\"yes \"); //else //\tprintf_s(\"not a triangle \"); //exp7 判断一个年是不是闰年 能被4整除但是不能被100整除的年份 //int n; //scanf_s(\"%d\", n); //if (n % 4 == 0 n % 100 != 0) //\tprintf_s(\"yes \"); //else //\tprintf_s(\"no \"); // //q1: int类型的最大值和最小值是多少 2^(n - 1) - 1 -2^(n - 1) //q2; double浮点数能精确到多少位小数？ 遵循IEEE标准的8字节（64位）的double能表示的有效数字的位数是：15 ~ 16 //q3: double类型的最大正数值和最小正数值 //q4: 逻辑运算符号的优先级 system(\"pause\"); return 0; } —|— 思考题解析： https://blog.csdn.net/panderang/article/details/54096426","tags":["C++"],"categories":["其他"]},{"title":"文本检测调研","path":"/2019/07/29/20190729-文本检测调研/","content":"文本检测问题，广义上来说可以看做是一个目标检测的问题，但是相当于目标检测要简单的多。因为目标检测往往除了背景还有其他的类，而文本检测，只需要检测背景和文本类两个问题。因此可以采用目标检测或者分割的方法来进行文本检测。而视频中的文本检测，也可以看做是视频中的目标检测中的一种，感觉应该也可以用视频中的目标检测+跟踪来做。 通常目标跟踪面临的极大难点：物体变形、亮度变化、快速移动、背景干扰覆盖。其中最主要的三个难题分别是目标背景的变化，物体本身的变化，光照强度的变化。 光流法帧间差分法背景差分法","tags":["计算机视觉"],"categories":["计算机视觉"]},{"title":"文本检测调研","path":"/2019/07/29/文本检测调研/","content":"文本检测问题，广义上来说可以看做是一个目标检测的问题，但是相当于目标检测要简单的多。因为目标检测往往除了背景还有其他的类，而文本检测，只需要检测背景和文本类两个问题。因此可以采用目标检测或者分割的方法来进行文本检测。而视频中的文本检测，也可以看做是视频中的目标检测中的一种，感觉应该也可以用视频中的目标检测+跟踪来做。 通常目标跟踪面临的极大难点：物体变形、亮度变化、快速移动、背景干扰覆盖。其中最主要的三个难题分别是目标背景的变化，物体本身的变化，光照强度的变化。 光流法帧间差分法背景差分法","tags":["目标检测"],"categories":["计算机视觉"]},{"title":"c++中级教程  STL queue","path":"/2019/07/26/20190726-STLqueue/","content":"STL queue 队列： FIFO 先进先出 自适应容器（容器适配器） 栈适配器 STL queue queueint, dequeint q;queueint, listint q;q.empty()q.size()q.front()q.back()q.pop()q.push(item) 可以用list和deque做queue先进先出，后进后出 // queue.cpp : 定义控制台应用程序的入口点。//#include \"stdafx.h\"#includeiostream#includequeue#includelist#includedequeusing namespace std;int main(){\tqueueint, dequeint a;\tqueueint, listint b;\t//queueint, vectorint c; 不可以，因为vector不能进行两端操作\t//队列有什么用途？？？\tqueueint q;\tq.push(10);\tq.push(5);\tq.push(-1);\tq.push(20);\tcout \"现在队列里有\" q.size() \"个数据 \" endl;\tcout \"队首的数据：\" q.front() endl;\tcout \"队尾的数据：\" q.back() endl;\tq.pop();\tcout \"新的队首的数据：\" q.front() endl;\twhile (q.size() != 0)\t{ cout \" 删除\" a.front() endl; q.pop();\t}\tif (q.empty())\t{ cout \"队列为空！\"endl;\t}\tsystem(\"pause\"); return 0;} 优先级队列 priority_queue 自适应容器（容器适配器）：不能使用list 最大值优先级队列、最小值优先级队列(值越大，优先级越高，值越小优先级越高) 优先级队列适配器 STL priority_queue priority_queueint, dequeint pg1;//对队列里的数据进行随机操作，所以不能使用listpriority_queueint, vectorint pg2; //vector是默认的 //谓词priority_queueint, vectorint, greaterint pg2; //vector是默认的，最小优先队列pg.empty()pg.size()pg.top()pg.pop()pg.push(item)~~~~~~~~~~ // priority_queue.cpp : 定义控制台应用程序的入口点。//#include \"stdafx.h\"#includeiostream#includedeque#includevector#includequeueusing namespace std;int main(){\tpriority_queueint, dequeint pg1; //对队列里的数据进行随机操作，所以不能使用list\tpriority_queueint, vectorint pg2; //vector是默认的 //谓词\tpriority_queueint, vectorint, greaterint pg2; //vector是默认的，最小优先队列\tpg2.push(10);\tpg2.push(5);\tpg2.push(-1);\tpg2.push(20);\tcout \"优先级队列一共有： \" pg2.size() \"个数据\" endl;\tcout pg2.top() endl;\twhile (!pg2.empty())\t{ cout \"从优先级队列里删除： \" pg2.top() endl; pg2.pop();\t}\tsystem(\"pause\"); return 0;}优先级队列一共有： 4个数据 从优先级队列里删除： 20从优先级队列里删除： 10从优先级队列里删除： 5从优先级队列里删除： -1请按任意键继续. . .~~","tags":["C++"],"categories":["C++"]},{"title":"c++中级教程 顺序容器的定义","path":"/2019/07/26/20190726-顺序容器的定义/","content":"顺序容器的定义 顺序容器 vector list deque 顺序容器适配器 stack queue priority_queue","tags":["C++"],"categories":["C++"]},{"title":"c++中级教程  STL queue","path":"/2019/07/26/STLqueue/","content":"STL queue 队列： FIFO 先进先出 自适应容器（容器适配器） 栈适配器 STL queue 1 2 3 4 5 6 7 8 | queueint, dequeint q; queueint, listint q; q.empty() q.size() q.front() q.back() q.pop() q.push(item) —|— 可以用list和deque做queue先进先出，后进后出 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 | // queue.cpp : 定义控制台应用程序的入口点。 // #include \"stdafx.h\" #includeiostream #includequeue #includelist #includedeque using namespace std; int main() { queueint, dequeint a; queueint, listint b; //queueint, vectorint c; 不可以，因为vector不能进行两端操作 //队列有什么用途？？？ queueint q; q.push(10); q.push(5); q.push(-1); q.push(20); cout \"现在队列里有\" q.size() \"个数据 \" endl; cout \"队首的数据：\" q.front() endl; cout \"队尾的数据：\" q.back() endl; q.pop(); cout \"新的队首的数据：\" q.front() endl; while (q.size() != 0) { cout \" 删除\" a.front() endl; q.pop(); } if (q.empty()) { cout \"队列为空！\"endl; } system(\"pause\"); return 0; } —|— 优先级队列 priority_queue 自适应容器（容器适配器）：不能使用list 最大值优先级队列、最小值优先级队列(值越大，优先级越高，值越小优先级越高) 优先级队列适配器 STL priority_queue 1 2 3 4 5 6 7 8 9 10 11 | priority_queueint, dequeint pg1; //对队列里的数据进行随机操作，所以不能使用list priority_queueint, vectorint pg2; //vector是默认的 //谓词 priority_queueint, vectorint, greaterint pg2; //vector是默认的，最小优先队列 pg.empty() pg.size() pg.top() pg.pop() pg.push(item) ~~~~~~~~~~ —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 | // priority_queue.cpp : 定义控制台应用程序的入口点。 // #include \"stdafx.h\" #includeiostream #includedeque #includevector #includequeue using namespace std; int main() { priority_queueint, dequeint pg1; //对队列里的数据进行随机操作，所以不能使用list priority_queueint, vectorint pg2; //vector是默认的 //谓词 priority_queueint, vectorint, greaterint pg2; //vector是默认的，最小优先队列 pg2.push(10); pg2.push(5); pg2.push(-1); pg2.push(20); cout \"优先级队列一共有： \" pg2.size() \"个数据\" endl; cout pg2.top() endl; while (!pg2.empty()) { cout \"从优先级队列里删除： \" pg2.top() endl; pg2.pop(); } system(\"pause\"); return 0; } 优先级队列一共有： 4个数据 20 从优先级队列里删除： 20 从优先级队列里删除： 10 从优先级队列里删除： 5 从优先级队列里删除： -1 请按任意键继续. . . ~~ —|—","tags":["C++"],"categories":["C++"]},{"title":"c++中级教程 顺序容器的定义","path":"/2019/07/26/顺序容器的定义/","content":"顺序容器的定义 顺序容器 vector list deque 顺序容器适配器 stack queue priority_queue","tags":["C++"],"categories":["C++"]},{"title":"c++中级教程  STL stack","path":"/2019/07/25/20190725-STL-stack/","content":"STL stack (堆) 栈： LIFO 后进先出 自适应容器（容器适配器） 栈适配器 STL stack stackint, dequeint s;stackint, vectorint s;stackint, listint s;s.empty()s.size()s.pop()s.push(item) // stack1.cpp : 定义控制台应用程序的入口点。//#include \"stdafx.h\"#include iostream#include vector#include list#include stackusing namespace std;int main(){\tstackint, dequeint a;\tstackint, vectorint b;\tstackint, listint c;\tstackint d; //默认用deque\t//什么是堆栈？ 先进后出，后进先出\td.push(25);\td.push(10);\td.push(1);\td.push(5);\tint x = 0;\tcout \"现在栈里一共有：\" d.size() \"个数据。\" endl;\twhile (d.empty() == false)\t{ x = d.top(); //查看数据并且返回 d.pop();//删除，不返回 cout x endl;\t}\t//x = d.top(); //查看数据并且返回\t//d.pop();//删除，不返回\t//cout x endl;\tcout \"现在栈里一共有：\" d.size() \"个数据。\" endl;\tsystem(\"pause\"); return 0;}现在栈里一共有：4个数据。 现在栈里一共有：0个数据。请按任意键继续. . .","tags":["C++"],"categories":["算法"]},{"title":"c++中级教程(一)","path":"/2019/07/25/20190725-顺序容器-STL-deque-类/","content":"顺序容器 STL deque 类 deque是一个动态数组 deque与vector 非常类似 deque可以在数组开头和末尾插入和删除数据 // demo3.cpp : 定义控制台应用程序的入口点。//#include \"stdafx.h\"#include iostream#include deque#include algorithmusing namespace std;int main(){dequeint a;a.push_back(3);a.push_back(4);a.push_back(5);a.push_back(6);//vector只能push_backa.push_front(2);a.push_front(1);a.push_front(9);a.push_front(8);for (size_t nCount = 0; nCount a.size(); ++nCount){cout \"a[\"nCount\"]\" \"= \" a[nCount] endl;}cout endlendl;a.pop_front();// 前面删除a.pop_back();// 后面删除cout \"删除之后：\" endl;/*for (size_t nCount = 0; nCount a.size(); ++nCount){cout \"a[\" nCount \"]\" a[nCount] endl;}*/dequeint::iterator iElementLocater; //这边使用了迭代器 distence 可以计算当前for (iElementLocater = a.begin();iElementLocater != a.end();++iElementLocater){size_t nOffset = distance(a.begin(), iElementLocater);//distence 可以计算当前下标与begin开始的，距离正好是下标cout \"a[\" nOffset \"]\" \"= \"*iElementLocater endl;}system(\"pause\");return 0;}a[0]= 8a[1]= 9a[2]= 1a[3]= 2a[4]= 3a[5]= 4a[6]= 5a[7]= 6删除之后：a[0]= 9a[1]= 1a[2]= 2a[3]= 3a[4]= 4a[5]= 5请按任意键继续. . . 順序容器 STL list 類 实例化std::list对象 在list开头插入元素 在list末尾插入元素 在list中间插入元素 删除list中的元素 对list中的元素进行反转和排序 // list.cpp : 定义控制台应用程序的入口点。//#include \"stdafx.h\"#includeiostream#includelistusing namespace std;void PrintListContent(const listint listInput);int main(){listint a;listint b;b.push_back(100);b.push_back(200);b.push_back(300);b.push_back(400);b.push_back(500);PrintListContent(b);cout endl;a.push_front(4);a.push_front(3);a.push_front(2);a.push_front(1);a.push_back(5);//使用链表数据，不能使用下标，只能使用迭代器listint::iterator iter;iter = a.begin();a.insert(iter, 10);// 在begin前面插入10，第一个参数迭代器，指定插入的位置a.insert(a.end(),10);PrintListContent(a);//将b插入到a之中a.insert(a.begin(), b.begin(), b.end());a.insert(iter,++b.begin(),--b.end())PrintListContent(a);system(\"pause\");return 0;}void PrintListContent(const listint listInput){//会是一个底层 const，即其所指对象可以改变，但不能改变其所指对象的值。listint::const_iterator iter;for (iter = listInput.begin(); iter != listInput.end(); ++iter){cout *iter endl;}} // listdelet.cpp : 定义控制台应用程序的入口点。//#include \"stdafx.h\"#includeiostream#includelistusing namespace std;void PrintListContent(const listint listInput);int main(){\tlistint a;\ta.push_front(4);\ta.push_front(3);\tlistint::iterator iElementValueTwo;\tiElementValueTwo = a.insert(a.begin(),2); // inset 才返回迭代器迭代器指向这个位置\ta.push_front(1);\ta.push_back(0);\tcout \"删除之前\" endl;\tPrintListContent(a);\t// 删除2\tcout \"删除之后\" endl;\ta.erase(iElementValueTwo);\t//a.erase(a.beigin(),iElementValueTwo); 删除从第一个迭代器到第二个迭代器所有的数据\tPrintListContent(a);\tsystem(\"pause\"); return 0;}void PrintListContent(const listint listInput){\t//会是一个底层 const，即其所指对象可以改变，但不能改变其所指对象的值。\tcout \"{\";\tlistint::const_iterator iter;\tfor (iter = listInput.begin(); iter != listInput.end(); ++iter)\t{ cout *iter \" \";\t}\tcout \"}\" endl;}删除之前{1 2 3 4 0 }删除之后{1 3 4 0 }请按任意键继续. . . // list3.cpp : 定义控制台应用程序的入口点。//#include \"stdafx.h\"#includeiostream#includelistusing namespace std;void PrintListContent(const listint listInput);int main(){\tlistint a;\ta.push_front(4);\ta.push_front(3);\ta.push_front(2);\ta.push_front(1);\tPrintListContent(a);\tcout \"反转之后的数据：\" endl;\ta.reverse();\tPrintListContent(a);\tlistint b;\tb.push_front(4);\tb.push_front(53);\tb.push_front(24);\tb.push_front(132);\tPrintListContent(b);\tcout \"排序之后的数据：\" endl;\tb.sort();\tPrintListContent(b);\tsystem(\"pause\"); return 0;}void PrintListContent(const listint listInput){\t//会是一个底层 const，即其所指对象可以改变，但不能改变其所指对象的值。\tcout \"{\";\tlistint::const_iterator iter;\tfor (iter = listInput.begin(); iter != listInput.end(); ++iter)\t{ cout *iter \" \";\t}\tcout \"}\" endl;}{1 2 3 4 }反转之后的数据：{4 3 2 1 }{132 24 53 4 }排序之后的数据：{4 24 53 132 }请按任意键继续. . .","tags":["C++"],"categories":["C++"]},{"title":"firstblog","path":"/2019/07/25/20190725-firstblog/","content":"以上是摘要 Hello World","tags":["其他"],"categories":["其他"]},{"title":"c++中级教程  STL stack","path":"/2019/07/25/STL-stack/","content":"STL stack (堆) 栈： LIFO 后进先出 自适应容器（容器适配器） 栈适配器 STL stack 1 2 3 4 5 6 7 | stackint, dequeint s; stackint, vectorint s; stackint, listint s; s.empty() s.size() s.pop() s.push(item) —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 | // stack1.cpp : 定义控制台应用程序的入口点。 // #include \"stdafx.h\" #include iostream #include vector #include list #include stack using namespace std; int main() { stackint, dequeint a; stackint, vectorint b; stackint, listint c; stackint d; //默认用deque //什么是堆栈？ 先进后出，后进先出 d.push(25); d.push(10); d.push(1); d.push(5); int x = 0; cout \"现在栈里一共有：\" d.size() \"个数据。\" endl; while (d.empty() == false) { x = d.top(); //查看数据并且返回 d.pop();//删除，不返回 cout x endl; } //x = d.top(); //查看数据并且返回 //d.pop();//删除，不返回 //cout x endl; cout \"现在栈里一共有：\" d.size() \"个数据。\" endl; system(\"pause\"); return 0; } 现在栈里一共有：4个数据。 5 1 10 25 现在栈里一共有：0个数据。 请按任意键继续. . . —|—","tags":["C++"],"categories":["C++"]},{"title":"firstblog","path":"/2019/07/25/firstblog/","content":"以上是摘要 Hello World","tags":["编程"],"categories":["其他"]},{"title":"c++中级教程(一)","path":"/2019/07/25/顺序容器-STL-deque-类/","content":"顺序容器 STL deque 类 deque是一个动态数组 deque与vector 非常类似 deque可以在数组开头和末尾插入和删除数据 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 | // demo3.cpp : 定义控制台应用程序的入口点。 // #include \"stdafx.h\" #include iostream #include deque #include algorithm using namespace std; int main() { dequeint a; a.push_back(3); a.push_back(4); a.push_back(5); a.push_back(6); //vector只能push_back a.push_front(2); a.push_front(1); a.push_front(9); a.push_front(8); for (size_t nCount = 0; nCount a.size(); ++nCount) { cout \"a[\"nCount\"]\" \"= \" a[nCount] endl; } cout endlendl; a.pop_front();// 前面删除 a.pop_back();// 后面删除 cout \"删除之后：\" endl; /*for (size_t nCount = 0; nCount a.size(); ++nCount) { cout \"a[\" nCount \"]\" a[nCount] endl; }*/ dequeint::iterator iElementLocater; //这边使用了迭代器 distence 可以计算当前 for (iElementLocater = a.begin(); iElementLocater != a.end(); ++iElementLocater) { size_t nOffset = distance(a.begin(), iElementLocater);//distence 可以计算当前下标与begin开始的，距离正好是下标 cout \"a[\" nOffset \"]\" \"= \"*iElementLocater endl; } system(\"pause\"); return 0; } a[0]= 8 a[1]= 9 a[2]= 1 a[3]= 2 a[4]= 3 a[5]= 4 a[6]= 5 a[7]= 6 删除之后： a[0]= 9 a[1]= 1 a[2]= 2 a[3]= 3 a[4]= 4 a[5]= 5 请按任意键继续. . . —|— 順序容器 STL list 類 实例化std::list对象 在list开头插入元素 在list末尾插入元素 在list中间插入元素 删除list中的元素 对list中的元素进行反转和排序 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 | // list.cpp : 定义控制台应用程序的入口点。 // #include \"stdafx.h\" #includeiostream #includelist using namespace std; void PrintListContent(const listint listInput); int main() { listint a; listint b; b.push_back(100); b.push_back(200); b.push_back(300); b.push_back(400); b.push_back(500); PrintListContent(b); cout endl; a.push_front(4); a.push_front(3); a.push_front(2); a.push_front(1); a.push_back(5); //使用链表数据，不能使用下标，只能使用迭代器 listint::iterator iter; iter = a.begin(); a.insert(iter, 10);// 在begin前面插入10，第一个参数迭代器，指定插入的位置 a.insert(a.end(),10); PrintListContent(a); //将b插入到a之中 a.insert(a.begin(), b.begin(), b.end()); a.insert(iter,++b.begin(),--b.end()) PrintListContent(a); system(\"pause\"); return 0; } void PrintListContent(const listint listInput) { //会是一个底层 const，即其所指对象可以改变，但不能改变其所指对象的值。 listint::const_iterator iter; for (iter = listInput.begin(); iter != listInput.end(); ++iter) { cout *iter endl; } } —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 | // listdelet.cpp : 定义控制台应用程序的入口点。 // #include \"stdafx.h\" #includeiostream #includelist using namespace std; void PrintListContent(const listint listInput); int main() { listint a; a.push_front(4); a.push_front(3); listint::iterator iElementValueTwo; iElementValueTwo = a.insert(a.begin(),2); // inset 才返回迭代器迭代器指向这个位置 a.push_front(1); a.push_back(0); cout \"删除之前\" endl; PrintListContent(a); // 删除2 cout \"删除之后\" endl; a.erase(iElementValueTwo); //a.erase(a.beigin(),iElementValueTwo); 删除从第一个迭代器到第二个迭代器所有的数据 PrintListContent(a); system(\"pause\"); return 0; } void PrintListContent(const listint listInput) { //会是一个底层 const，即其所指对象可以改变，但不能改变其所指对象的值。 cout \"{\"; listint::const_iterator iter; for (iter = listInput.begin(); iter != listInput.end(); ++iter) { cout *iter \" \"; } cout \"}\" endl; } 删除之前 {1 2 3 4 0 } 删除之后 {1 3 4 0 } 请按任意键继续. . . —|— 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 | // list3.cpp : 定义控制台应用程序的入口点。 // #include \"stdafx.h\" #includeiostream #includelist using namespace std; void PrintListContent(const listint listInput); int main() { listint a; a.push_front(4); a.push_front(3); a.push_front(2); a.push_front(1); PrintListContent(a); cout \"反转之后的数据：\" endl; a.reverse(); PrintListContent(a); listint b; b.push_front(4); b.push_front(53); b.push_front(24); b.push_front(132); PrintListContent(b); cout \"排序之后的数据：\" endl; b.sort(); PrintListContent(b); system(\"pause\"); return 0; } void PrintListContent(const listint listInput) { //会是一个底层 const，即其所指对象可以改变，但不能改变其所指对象的值。 cout \"{\"; listint::const_iterator iter; for (iter = listInput.begin(); iter != listInput.end(); ++iter) { cout *iter \" \"; } cout \"}\" endl; } {1 2 3 4 } 反转之后的数据： {4 3 2 1 } {132 24 53 4 } 排序之后的数据： {4 24 53 132 } 请按任意键继续. . . —|—","tags":["C++"],"categories":["算法"]},{"title":"关于我","path":"/about/index.html","content":"👋 Hi, I’m Caius Lu 计算机视觉研究者 | 大模型初学者 | AI 探索者 | 技术博主 🎯 关于我我是一名专注于计算机视觉和深度学习领域的技术研究者。同时我也是一名大模型(LLM)初学者，正在积极探索这一前沿领域。 热衷于探索 AI 技术的无限可能，尤其在以下方向有深入研究： 🎯 目标检测与跟踪 - 多相机行人跟踪、ReID 📐 几何视觉 - 单应性矩阵、相机标定、SLAM 🧠 深度学习 - 神经网络架构、模型优化 🤖 大模型 (LLM) - 参数高效微调 (PEFT/LoRA)、RAG 🚗 智慧交通 - 交通场景感知与分析 🔗 联系方式GitHub 👉 点击访问 GitHub 主页 (github.com/caiusy) 访问我的 GitHub 主页，查看开源项目和代码。 博客 👉 点击访问博客首页 这里记录我的技术笔记和学习心得。 📊 GitHub 统计 🛠️ 技术栈语言 框架 工具 📝 博客内容本博客主要分享以下内容： 论文阅读笔记 - 计算机视觉领域前沿论文解读 大模型探索 - LLM 核心原理、LoRA 微调实战 技术教程 - 深度学习、图像处理实践教程 项目实践 - 真实项目经验分享 学习心得 - 技术成长路上的思考 📬 留言如果你有任何问题或想法，欢迎在下方留言交流！"}]