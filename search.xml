<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>动态规划完全指南：从入门到精通</title>
      <link href="/2026/02/20/20250220-dynamic-programming-complete-guide/"/>
      <url>/2026/02/20/20250220-dynamic-programming-complete-guide/</url>
      
        <content type="html"><![CDATA[<h1 id="动态规划完全指南：从入门到精通"><a href="#动态规划完全指南：从入门到精通" class="headerlink" title="动态规划完全指南：从入门到精通"></a>动态规划完全指南：从入门到精通</h1><blockquote><p>用费曼学习法，彻底搞懂动态规划的本质</p></blockquote><h2 id="一、什么是动态规划？"><a href="#一、什么是动态规划？" class="headerlink" title="一、什么是动态规划？"></a>一、什么是动态规划？</h2><h3 id="1-1-一句话定义"><a href="#1-1-一句话定义" class="headerlink" title="1.1 一句话定义"></a>1.1 一句话定义</h3><p><strong>动态规划 (Dynamic Programming, DP)</strong> = <strong>记住已经解决过的子问题的答案，避免重复计算</strong>。</p><p>就这么简单。</p><h3 id="1-2-为什么叫”动态规划”？"><a href="#1-2-为什么叫”动态规划”？" class="headerlink" title="1.2 为什么叫”动态规划”？"></a>1.2 为什么叫”动态规划”？</h3><p>这个名字其实很有误导性。Richard Bellman 在 1950 年代发明这个方法时，故意起了个听起来很高大上的名字，因为他的老板不喜欢”数学研究”。”Dynamic” 听起来很酷，”Programming” 在当时指的是”规划/优化”，不是写代码。</p><p>所以别被名字吓到，它的本质就是：<strong>用空间换时间，记住中间结果</strong>。</p><h3 id="1-3-核心直觉：斐波那契数列"><a href="#1-3-核心直觉：斐波那契数列" class="headerlink" title="1.3 核心直觉：斐波那契数列"></a>1.3 核心直觉：斐波那契数列</h3><p>让我们从最经典的例子开始：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">fib(0) = 0</span><br><span class="line">fib(1) = 1</span><br><span class="line">fib(n) = fib(n-1) + fib(n-2)</span><br></pre></td></tr></tbody></table></figure><p><strong>暴力递归写法：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fib</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    <span class="keyword">return</span> fib(n-<span class="number">1</span>) + fib(n-<span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure><p>看起来很简洁对吧？但是有个致命问题：</p><p><img src="/images/dp/01_dp_vs_recursion.png" alt="暴力递归 vs 动态规划"></p><p>看到了吗？<code>fib(3)</code> 被计算了 2 次，<code>fib(2)</code> 被计算了 3 次！当 n=50 时，这个递归树会有 2^50 个节点，你的电脑会直接卡死。</p><p><strong>时间复杂度：O(2^n)</strong> —— 指数级爆炸！</p><h3 id="1-4-DP-的解决方案"><a href="#1-4-DP-的解决方案" class="headerlink" title="1.4 DP 的解决方案"></a>1.4 DP 的解决方案</h3><p><strong>方案一：记忆化递归 (Top-Down)</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fib</span>(<span class="params">n, memo={}</span>):</span><br><span class="line">    <span class="keyword">if</span> n <span class="keyword">in</span> memo:</span><br><span class="line">        <span class="keyword">return</span> memo[n]</span><br><span class="line">    <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    memo[n] = fib(n-<span class="number">1</span>, memo) + fib(n-<span class="number">2</span>, memo)</span><br><span class="line">    <span class="keyword">return</span> memo[n]</span><br></pre></td></tr></tbody></table></figure><p><strong>方案二：迭代填表 (Bottom-Up)</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fib</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    dp = [<span class="number">0</span>] * (n + <span class="number">1</span>)</span><br><span class="line">    dp[<span class="number">0</span>], dp[<span class="number">1</span>] = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n + <span class="number">1</span>):</span><br><span class="line">        dp[i] = dp[i-<span class="number">1</span>] + dp[i-<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">return</span> dp[n]</span><br></pre></td></tr></tbody></table></figure><p><strong>方案三：空间优化</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fib</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    prev, curr = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n + <span class="number">1</span>):</span><br><span class="line">        prev, curr = curr, prev + curr</span><br><span class="line">    <span class="keyword">return</span> curr</span><br></pre></td></tr></tbody></table></figure><table><thead><tr><th>方案</th><th>时间复杂度</th><th>空间复杂度</th></tr></thead><tbody><tr><td>暴力递归</td><td>O(2^n)</td><td>O(n) 栈空间</td></tr><tr><td>记忆化递归</td><td>O(n)</td><td>O(n)</td></tr><tr><td>迭代填表</td><td>O(n)</td><td>O(n)</td></tr><tr><td>空间优化</td><td>O(n)</td><td>O(1)</td></tr></tbody></table><p><strong>这就是 DP 的威力：从指数级降到线性级！</strong></p><hr><h2 id="二、DP-的三大核心要素"><a href="#二、DP-的三大核心要素" class="headerlink" title="二、DP 的三大核心要素"></a>二、DP 的三大核心要素</h2><p><img src="/images/dp/02_dp_three_elements.png" alt="DP三要素"></p><h3 id="2-1-状态定义-State-Definition"><a href="#2-1-状态定义-State-Definition" class="headerlink" title="2.1 状态定义 (State Definition)"></a>2.1 状态定义 (State Definition)</h3><p><strong>这是最关键的一步！</strong> 状态定义错了，后面全白搭。</p><p><strong>问自己：<code>dp[i]</code> 或 <code>dp[i][j]</code> 代表什么？</strong></p><p>常见的状态定义模式：</p><table><thead><tr><th>问题类型</th><th>状态定义</th><th>例子</th></tr></thead><tbody><tr><td>线性序列</td><td><code>dp[i]</code> = 以第i个元素结尾的xxx</td><td>最长递增子序列</td></tr><tr><td>线性序列</td><td><code>dp[i]</code> = 前i个元素的xxx</td><td>打家劫舍</td></tr><tr><td>双序列</td><td><code>dp[i][j]</code> = s1前i个和s2前j个的xxx</td><td>LCS, 编辑距离</td></tr><tr><td>背包</td><td><code>dp[i][w]</code> = 前i个物品、容量w的xxx</td><td>0-1背包</td></tr><tr><td>区间</td><td><code>dp[i][j]</code> = 区间[i,j]的xxx</td><td>戳气球</td></tr></tbody></table><h3 id="2-2-状态转移方程-Transition"><a href="#2-2-状态转移方程-Transition" class="headerlink" title="2.2 状态转移方程 (Transition)"></a>2.2 状态转移方程 (Transition)</h3><p><strong>问自己：当前状态和哪些之前的状态有关？</strong></p><p>这是 DP 的”递推公式”，决定了如何从小问题推导出大问题。</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dp[i] = f(dp[i-1], dp[i-2], ..., dp[0])</span><br></pre></td></tr></tbody></table></figure><p><strong>技巧：画图！</strong> 把状态之间的依赖关系画出来，转移方程自然就出来了。</p><h3 id="2-3-边界条件-Base-Case"><a href="#2-3-边界条件-Base-Case" class="headerlink" title="2.3 边界条件 (Base Case)"></a>2.3 边界条件 (Base Case)</h3><p><strong>问自己：最小的子问题，答案是什么？</strong></p><p>边界条件就是递推的起点。没有正确的边界条件，整个 DP 就会崩溃。</p><p>常见边界：</p><ul><li><code>dp[0] = 0</code> 或 <code>dp[0] = 1</code></li><li><code>dp[0][j] = xxx</code>, <code>dp[i][0] = xxx</code></li><li>空集、空串的情况</li></ul><hr><h2 id="三、两种实现方式对比"><a href="#三、两种实现方式对比" class="headerlink" title="三、两种实现方式对比"></a>三、两种实现方式对比</h2><p><img src="/images/dp/05_topdown_vs_bottomup.png" alt="Top-Down vs Bottom-Up"></p><h3 id="3-1-Top-Down：记忆化递归"><a href="#3-1-Top-Down：记忆化递归" class="headerlink" title="3.1 Top-Down：记忆化递归"></a>3.1 Top-Down：记忆化递归</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">solve</span>(<span class="params">n, memo={}</span>):</span><br><span class="line">    <span class="comment"># 1. 检查缓存</span></span><br><span class="line">    <span class="keyword">if</span> n <span class="keyword">in</span> memo:</span><br><span class="line">        <span class="keyword">return</span> memo[n]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. Base case</span></span><br><span class="line">    <span class="keyword">if</span> n &lt;= <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> base_value</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. 递归 + 记忆化</span></span><br><span class="line">    memo[n] = f(solve(n-<span class="number">1</span>), solve(n-<span class="number">2</span>), ...)</span><br><span class="line">    <span class="keyword">return</span> memo[n]</span><br></pre></td></tr></tbody></table></figure><p><strong>优点：</strong></p><ul><li>思路自然，从目标出发</li><li>只计算需要的子问题</li><li>代码更接近数学定义</li></ul><p><strong>缺点：</strong></p><ul><li>递归栈开销</li><li>Python 默认递归深度限制 (1000)</li></ul><h3 id="3-2-Bottom-Up：迭代填表"><a href="#3-2-Bottom-Up：迭代填表" class="headerlink" title="3.2 Bottom-Up：迭代填表"></a>3.2 Bottom-Up：迭代填表</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">solve</span>(<span class="params">n</span>):</span><br><span class="line">    <span class="comment"># 1. 初始化 DP 数组</span></span><br><span class="line">    dp = [<span class="number">0</span>] * (n + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. Base case</span></span><br><span class="line">    dp[<span class="number">0</span>] = base_value</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. 按顺序填表</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        dp[i] = f(dp[i-<span class="number">1</span>], dp[i-<span class="number">2</span>], ...)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 4. 返回结果</span></span><br><span class="line">    <span class="keyword">return</span> dp[n]</span><br></pre></td></tr></tbody></table></figure><p><strong>优点：</strong></p><ul><li>无递归栈开销</li><li>更容易做空间优化</li><li>通常更快（无函数调用开销）</li></ul><p><strong>缺点：</strong></p><ul><li>需要想清楚遍历顺序</li><li>可能计算不需要的子问题</li></ul><h3 id="3-3-如何选择？"><a href="#3-3-如何选择？" class="headerlink" title="3.3 如何选择？"></a>3.3 如何选择？</h3><table><thead><tr><th>场景</th><th>推荐方式</th></tr></thead><tbody><tr><td>面试时快速写出</td><td>Top-Down（更直观）</td></tr><tr><td>追求性能</td><td>Bottom-Up</td></tr><tr><td>需要空间优化</td><td>Bottom-Up</td></tr><tr><td>状态转移复杂</td><td>Top-Down（更容易调试）</td></tr></tbody></table><hr><h2 id="四、空间优化技巧"><a href="#四、空间优化技巧" class="headerlink" title="四、空间优化技巧"></a>四、空间优化技巧</h2><p><img src="/images/dp/06_space_optimization.png" alt="空间优化"></p><h3 id="4-1-滚动数组"><a href="#4-1-滚动数组" class="headerlink" title="4.1 滚动数组"></a>4.1 滚动数组</h3><p>当 <code>dp[i]</code> 只依赖 <code>dp[i-1]</code> 时，不需要保存整个数组：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 优化前：O(n) 空间</span></span><br><span class="line">dp = [<span class="number">0</span>] * n</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">    dp[i] = dp[i-<span class="number">1</span>] + something</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化后：O(1) 空间</span></span><br><span class="line">prev = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">    curr = prev + something</span><br><span class="line">    prev = curr</span><br></pre></td></tr></tbody></table></figure><h3 id="4-2-二维降一维"><a href="#4-2-二维降一维" class="headerlink" title="4.2 二维降一维"></a>4.2 二维降一维</h3><p>当 <code>dp[i][j]</code> 只依赖 <code>dp[i-1][...]</code> 时：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 优化前：O(m*n) 空间</span></span><br><span class="line">dp = [[<span class="number">0</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m)]</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">        dp[i][j] = dp[i-<span class="number">1</span>][j] + dp[i][j-<span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 优化后：O(n) 空间</span></span><br><span class="line">dp = [<span class="number">0</span>] * n</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m):</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">        dp[j] = dp[j] + dp[j-<span class="number">1</span>]  <span class="comment"># dp[j] 就是原来的 dp[i-1][j]</span></span><br></pre></td></tr></tbody></table></figure><p><strong>注意遍历顺序！</strong> 如果依赖左上角，需要从右往左遍历。</p><hr><h2 id="五、经典问题详解"><a href="#五、经典问题详解" class="headerlink" title="五、经典问题详解"></a>五、经典问题详解</h2><h3 id="5-1-爬楼梯-LeetCode-70"><a href="#5-1-爬楼梯-LeetCode-70" class="headerlink" title="5.1 爬楼梯 (LeetCode 70)"></a>5.1 爬楼梯 (LeetCode 70)</h3><p><img src="/images/dp/03_classic_problems.png" alt="爬楼梯"></p><p><strong>问题：</strong> 每次可以爬 1 或 2 个台阶，爬到第 n 阶有多少种方法？</p><p><strong>状态定义：</strong> <code>dp[i]</code> = 爬到第 i 阶的方法数</p><p><strong>转移方程：</strong> <code>dp[i] = dp[i-1] + dp[i-2]</code></p><ul><li>从第 i-1 阶爬 1 步上来</li><li>从第 i-2 阶爬 2 步上来</li></ul><p><strong>边界条件：</strong> <code>dp[0] = 1, dp[1] = 1</code></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">climbStairs</span>(<span class="params">n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">if</span> n &lt;= <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    prev, curr = <span class="number">1</span>, <span class="number">2</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>, n + <span class="number">1</span>):</span><br><span class="line">        prev, curr = curr, prev + curr</span><br><span class="line">    <span class="keyword">return</span> curr</span><br></pre></td></tr></tbody></table></figure><p><strong>复杂度：</strong> 时间 O(n)，空间 O(1)</p><h3 id="5-2-打家劫舍-LeetCode-198"><a href="#5-2-打家劫舍-LeetCode-198" class="headerlink" title="5.2 打家劫舍 (LeetCode 198)"></a>5.2 打家劫舍 (LeetCode 198)</h3><p><strong>问题：</strong> 不能偷相邻的房子，求能偷到的最大金额。</p><p><strong>状态定义：</strong> <code>dp[i]</code> = 考虑前 i 个房子能偷到的最大金额</p><p><strong>转移方程：</strong> <code>dp[i] = max(dp[i-1], dp[i-2] + nums[i])</code></p><ul><li>不偷第 i 个：<code>dp[i-1]</code></li><li>偷第 i 个：<code>dp[i-2] + nums[i]</code>（不能偷 i-1）</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rob</span>(<span class="params">nums: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(nums) &lt;= <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(nums) <span class="keyword">if</span> nums <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">    prev2, prev1 = nums[<span class="number">0</span>], <span class="built_in">max</span>(nums[<span class="number">0</span>], nums[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">        prev2, prev1 = prev1, <span class="built_in">max</span>(prev1, prev2 + nums[i])</span><br><span class="line">    <span class="keyword">return</span> prev1</span><br></pre></td></tr></tbody></table></figure><p><strong>复杂度：</strong> 时间 O(n)，空间 O(1)</p><h3 id="5-3-最大子数组和-LeetCode-53"><a href="#5-3-最大子数组和-LeetCode-53" class="headerlink" title="5.3 最大子数组和 (LeetCode 53)"></a>5.3 最大子数组和 (LeetCode 53)</h3><p><img src="/images/dp/08_kadane.png" alt="Kadane算法"></p><p><strong>问题：</strong> 找出和最大的连续子数组。</p><p><strong>状态定义：</strong> <code>dp[i]</code> = 以 nums[i] 结尾的最大子数组和</p><p><strong>转移方程：</strong> <code>dp[i] = max(nums[i], dp[i-1] + nums[i])</code></p><ul><li>要么从 nums[i] 重新开始</li><li>要么接上前面的子数组</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">maxSubArray</span>(<span class="params">nums: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    max_sum = curr_sum = nums[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> nums[<span class="number">1</span>:]:</span><br><span class="line">        curr_sum = <span class="built_in">max</span>(num, curr_sum + num)</span><br><span class="line">        max_sum = <span class="built_in">max</span>(max_sum, curr_sum)</span><br><span class="line">    <span class="keyword">return</span> max_sum</span><br></pre></td></tr></tbody></table></figure><p><strong>复杂度：</strong> 时间 O(n)，空间 O(1)</p><h3 id="5-4-最长递增子序列-LIS-LeetCode-300"><a href="#5-4-最长递增子序列-LIS-LeetCode-300" class="headerlink" title="5.4 最长递增子序列 LIS (LeetCode 300)"></a>5.4 最长递增子序列 LIS (LeetCode 300)</h3><p><strong>问题：</strong> 找出最长严格递增子序列的长度。</p><p><strong>状态定义：</strong> <code>dp[i]</code> = 以 nums[i] 结尾的 LIS 长度</p><p><strong>转移方程：</strong> <code>dp[i] = max(dp[j] + 1)</code> for all j &lt; i where nums[j] &lt; nums[i]</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lengthOfLIS</span>(<span class="params">nums: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    n = <span class="built_in">len</span>(nums)</span><br><span class="line">    dp = [<span class="number">1</span>] * n</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i):</span><br><span class="line">            <span class="keyword">if</span> nums[j] &lt; nums[i]:</span><br><span class="line">                dp[i] = <span class="built_in">max</span>(dp[i], dp[j] + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(dp)</span><br></pre></td></tr></tbody></table></figure><p><strong>复杂度：</strong> 时间 O(n²)，空间 O(n)</p><p><strong>优化版本（二分查找）：</strong> O(n log n)</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> bisect</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lengthOfLIS</span>(<span class="params">nums: <span class="built_in">list</span>[<span class="built_in">int</span>]</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    tails = []</span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">        pos = bisect.bisect_left(tails, num)</span><br><span class="line">        <span class="keyword">if</span> pos == <span class="built_in">len</span>(tails):</span><br><span class="line">            tails.append(num)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tails[pos] = num</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(tails)</span><br></pre></td></tr></tbody></table></figure><h3 id="5-5-最长公共子序列-LCS-LeetCode-1143"><a href="#5-5-最长公共子序列-LCS-LeetCode-1143" class="headerlink" title="5.5 最长公共子序列 LCS (LeetCode 1143)"></a>5.5 最长公共子序列 LCS (LeetCode 1143)</h3><p><img src="/images/dp/04_lcs.png" alt="LCS"></p><p><strong>问题：</strong> 两个字符串的最长公共子序列长度。</p><p><strong>状态定义：</strong> <code>dp[i][j]</code> = s1 前 i 个字符和 s2 前 j 个字符的 LCS 长度</p><p><strong>转移方程：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">if s1[i-1] == s2[j-1]:</span><br><span class="line">    dp[i][j] = dp[i-1][j-1] + 1</span><br><span class="line">else:</span><br><span class="line">    dp[i][j] = max(dp[i-1][j], dp[i][j-1])</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">longestCommonSubsequence</span>(<span class="params">text1: <span class="built_in">str</span>, text2: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    m, n = <span class="built_in">len</span>(text1), <span class="built_in">len</span>(text2)</span><br><span class="line">    dp = [[<span class="number">0</span>] * (n + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m + <span class="number">1</span>)]</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> text1[i-<span class="number">1</span>] == text2[j-<span class="number">1</span>]:</span><br><span class="line">                dp[i][j] = dp[i-<span class="number">1</span>][j-<span class="number">1</span>] + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dp[i][j] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>][j], dp[i][j-<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> dp[m][n]</span><br></pre></td></tr></tbody></table></figure><p><strong>复杂度：</strong> 时间 O(mn)，空间 O(mn)，可优化到 O(n)</p><h3 id="5-6-编辑距离-LeetCode-72"><a href="#5-6-编辑距离-LeetCode-72" class="headerlink" title="5.6 编辑距离 (LeetCode 72)"></a>5.6 编辑距离 (LeetCode 72)</h3><p><img src="/images/dp/09_edit_distance.png" alt="编辑距离"></p><p><strong>问题：</strong> 将 word1 转换成 word2 所需的最少操作数（插入、删除、替换）。</p><p><strong>状态定义：</strong> <code>dp[i][j]</code> = word1 前 i 个字符转换成 word2 前 j 个字符的最少操作数</p><p><strong>转移方程：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">if word1[i-1] == word2[j-1]:</span><br><span class="line">    dp[i][j] = dp[i-1][j-1]  # 不需要操作</span><br><span class="line">else:</span><br><span class="line">    dp[i][j] = 1 + min(</span><br><span class="line">        dp[i-1][j-1],  # 替换</span><br><span class="line">        dp[i-1][j],    # 删除</span><br><span class="line">        dp[i][j-1]     # 插入</span><br><span class="line">    )</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">minDistance</span>(<span class="params">word1: <span class="built_in">str</span>, word2: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    m, n = <span class="built_in">len</span>(word1), <span class="built_in">len</span>(word2)</span><br><span class="line">    dp = [[<span class="number">0</span>] * (n + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m + <span class="number">1</span>)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 边界条件</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m + <span class="number">1</span>):</span><br><span class="line">        dp[i][<span class="number">0</span>] = i</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>):</span><br><span class="line">        dp[<span class="number">0</span>][j] = j</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> word1[i-<span class="number">1</span>] == word2[j-<span class="number">1</span>]:</span><br><span class="line">                dp[i][j] = dp[i-<span class="number">1</span>][j-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dp[i][j] = <span class="number">1</span> + <span class="built_in">min</span>(dp[i-<span class="number">1</span>][j-<span class="number">1</span>], dp[i-<span class="number">1</span>][j], dp[i][j-<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dp[m][n]</span><br></pre></td></tr></tbody></table></figure><p><strong>复杂度：</strong> 时间 O(mn)，空间 O(mn)</p><h3 id="5-7-0-1-背包问题"><a href="#5-7-0-1-背包问题" class="headerlink" title="5.7 0-1 背包问题"></a>5.7 0-1 背包问题</h3><p><strong>问题：</strong> n 个物品，每个有重量 w[i] 和价值 v[i]，背包容量 W，求最大价值。</p><p><strong>状态定义：</strong> <code>dp[i][w]</code> = 前 i 个物品、容量 w 时的最大价值</p><p><strong>转移方程：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">dp[i][w] = max(</span><br><span class="line">    dp[i-1][w],           # 不选第 i 个</span><br><span class="line">    dp[i-1][w-w[i]] + v[i]  # 选第 i 个（如果装得下）</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">knapsack</span>(<span class="params">weights, values, W</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(weights)</span><br><span class="line">    dp = [[<span class="number">0</span>] * (W + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>)]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> <span class="built_in">range</span>(W + <span class="number">1</span>):</span><br><span class="line">            dp[i][w] = dp[i-<span class="number">1</span>][w]  <span class="comment"># 不选</span></span><br><span class="line">            <span class="keyword">if</span> w &gt;= weights[i-<span class="number">1</span>]:</span><br><span class="line">                dp[i][w] = <span class="built_in">max</span>(dp[i][w], dp[i-<span class="number">1</span>][w-weights[i-<span class="number">1</span>]] + values[i-<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dp[n][W]</span><br></pre></td></tr></tbody></table></figure><p><strong>空间优化版本：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">knapsack</span>(<span class="params">weights, values, W</span>):</span><br><span class="line">    dp = [<span class="number">0</span>] * (W + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(weights)):</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> <span class="built_in">range</span>(W, weights[i] - <span class="number">1</span>, -<span class="number">1</span>):  <span class="comment"># 从右往左！</span></span><br><span class="line">            dp[w] = <span class="built_in">max</span>(dp[w], dp[w - weights[i]] + values[i])</span><br><span class="line">    <span class="keyword">return</span> dp[W]</span><br></pre></td></tr></tbody></table></figure><p><strong>为什么要从右往左？</strong> 因为 <code>dp[w]</code> 依赖 <code>dp[w-weights[i]]</code>，如果从左往右，<code>dp[w-weights[i]]</code> 已经被更新过了，相当于同一个物品被选了多次（变成完全背包了）。</p><p><strong>复杂度：</strong> 时间 O(nW)，空间 O(W)</p><h3 id="5-8-完全背包问题"><a href="#5-8-完全背包问题" class="headerlink" title="5.8 完全背包问题"></a>5.8 完全背包问题</h3><p><strong>问题：</strong> 每个物品可以选无限次。</p><p><strong>和 0-1 背包的区别：</strong> 遍历顺序从左往右！</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">unboundedKnapsack</span>(<span class="params">weights, values, W</span>):</span><br><span class="line">    dp = [<span class="number">0</span>] * (W + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(weights)):</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> <span class="built_in">range</span>(weights[i], W + <span class="number">1</span>):  <span class="comment"># 从左往右！</span></span><br><span class="line">            dp[w] = <span class="built_in">max</span>(dp[w], dp[w - weights[i]] + values[i])</span><br><span class="line">    <span class="keyword">return</span> dp[W]</span><br></pre></td></tr></tbody></table></figure><h3 id="5-9-零钱兑换-LeetCode-322"><a href="#5-9-零钱兑换-LeetCode-322" class="headerlink" title="5.9 零钱兑换 (LeetCode 322)"></a>5.9 零钱兑换 (LeetCode 322)</h3><p><strong>问题：</strong> 用最少的硬币凑出金额 amount。</p><p><strong>状态定义：</strong> <code>dp[i]</code> = 凑出金额 i 需要的最少硬币数</p><p><strong>转移方程：</strong> <code>dp[i] = min(dp[i - coin] + 1)</code> for all coins</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">coinChange</span>(<span class="params">coins: <span class="built_in">list</span>[<span class="built_in">int</span>], amount: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    dp = [<span class="built_in">float</span>(<span class="string">'inf'</span>)] * (amount + <span class="number">1</span>)</span><br><span class="line">    dp[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, amount + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> coin <span class="keyword">in</span> coins:</span><br><span class="line">            <span class="keyword">if</span> coin &lt;= i <span class="keyword">and</span> dp[i - coin] != <span class="built_in">float</span>(<span class="string">'inf'</span>):</span><br><span class="line">                dp[i] = <span class="built_in">min</span>(dp[i], dp[i - coin] + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> dp[amount] <span class="keyword">if</span> dp[amount] != <span class="built_in">float</span>(<span class="string">'inf'</span>) <span class="keyword">else</span> -<span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><p><strong>复杂度：</strong> 时间 O(amount × n)，空间 O(amount)</p><h3 id="5-10-不同路径-LeetCode-62"><a href="#5-10-不同路径-LeetCode-62" class="headerlink" title="5.10 不同路径 (LeetCode 62)"></a>5.10 不同路径 (LeetCode 62)</h3><p><strong>问题：</strong> m×n 网格，从左上到右下，只能向右或向下，有多少条路径？</p><p><strong>状态定义：</strong> <code>dp[i][j]</code> = 到达 (i,j) 的路径数</p><p><strong>转移方程：</strong> <code>dp[i][j] = dp[i-1][j] + dp[i][j-1]</code></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">uniquePaths</span>(<span class="params">m: <span class="built_in">int</span>, n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    dp = [<span class="number">1</span>] * n</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">            dp[j] += dp[j-<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> dp[n-<span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure><p><strong>复杂度：</strong> 时间 O(mn)，空间 O(n)</p><hr><h2 id="六、DP-问题分类"><a href="#六、DP-问题分类" class="headerlink" title="六、DP 问题分类"></a>六、DP 问题分类</h2><p><img src="/images/dp/07_dp_categories.png" alt="DP分类"></p><h3 id="6-1-线性-DP"><a href="#6-1-线性-DP" class="headerlink" title="6.1 线性 DP"></a>6.1 线性 DP</h3><p>特点：状态沿着一个维度线性递推</p><table><thead><tr><th>题目</th><th>状态定义</th><th>转移方程</th></tr></thead><tbody><tr><td>爬楼梯</td><td>dp[i]=到第i阶方法数</td><td>dp[i]=dp[i-1]+dp[i-2]</td></tr><tr><td>打家劫舍</td><td>dp[i]=前i家最大金额</td><td>dp[i]=max(dp[i-1], dp[i-2]+nums[i])</td></tr><tr><td>最大子数组和</td><td>dp[i]=以i结尾的最大和</td><td>dp[i]=max(nums[i], dp[i-1]+nums[i])</td></tr><tr><td>LIS</td><td>dp[i]=以i结尾的LIS长度</td><td>dp[i]=max(dp[j]+1) for j&lt;i</td></tr></tbody></table><h3 id="6-2-序列-DP（双序列）"><a href="#6-2-序列-DP（双序列）" class="headerlink" title="6.2 序列 DP（双序列）"></a>6.2 序列 DP（双序列）</h3><p>特点：两个序列之间的关系</p><table><thead><tr><th>题目</th><th>状态定义</th><th>关键点</th></tr></thead><tbody><tr><td>LCS</td><td>dp[i][j]=s1前i和s2前j的LCS</td><td>字符相等时+1</td></tr><tr><td>编辑距离</td><td>dp[i][j]=最少操作数</td><td>三种操作取min</td></tr><tr><td>不同子序列</td><td>dp[i][j]=s中t出现次数</td><td>选或不选当前字符</td></tr></tbody></table><h3 id="6-3-背包-DP"><a href="#6-3-背包-DP" class="headerlink" title="6.3 背包 DP"></a>6.3 背包 DP</h3><p>特点：选择物品，满足约束，优化目标</p><table><thead><tr><th>类型</th><th>特点</th><th>遍历顺序</th></tr></thead><tbody><tr><td>0-1背包</td><td>每个物品最多选1次</td><td>容量从大到小</td></tr><tr><td>完全背包</td><td>每个物品可选无限次</td><td>容量从小到大</td></tr><tr><td>多重背包</td><td>每个物品有数量限制</td><td>二进制优化</td></tr></tbody></table><h3 id="6-4-区间-DP"><a href="#6-4-区间-DP" class="headerlink" title="6.4 区间 DP"></a>6.4 区间 DP</h3><p>特点：在区间上进行决策</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 区间DP模板</span></span><br><span class="line"><span class="keyword">for</span> length <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n + <span class="number">1</span>):      <span class="comment"># 枚举区间长度</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - length + <span class="number">1</span>):  <span class="comment"># 枚举起点</span></span><br><span class="line">        j = i + length - <span class="number">1</span>           <span class="comment"># 计算终点</span></span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(i, j):        <span class="comment"># 枚举分割点</span></span><br><span class="line">            dp[i][j] = <span class="built_in">min</span>(dp[i][j], dp[i][k] + dp[k+<span class="number">1</span>][j] + cost)</span><br></pre></td></tr></tbody></table></figure><p>经典题目：戳气球、石子合并、矩阵链乘法</p><h3 id="6-5-树形-DP"><a href="#6-5-树形-DP" class="headerlink" title="6.5 树形 DP"></a>6.5 树形 DP</h3><p>特点：在树结构上进行 DP</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">node</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> node:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    left = dfs(node.left)</span><br><span class="line">    right = dfs(node.right)</span><br><span class="line">    <span class="comment"># 根据子树结果计算当前节点</span></span><br><span class="line">    <span class="keyword">return</span> f(left, right, node.val)</span><br></pre></td></tr></tbody></table></figure><p>经典题目：打家劫舍III、二叉树最大路径和</p><h3 id="6-6-状态压缩-DP"><a href="#6-6-状态压缩-DP" class="headerlink" title="6.6 状态压缩 DP"></a>6.6 状态压缩 DP</h3><p>特点：用二进制表示状态集合</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 枚举所有子集</span></span><br><span class="line"><span class="keyword">for</span> mask <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span> &lt;&lt; n):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">if</span> mask &amp; (<span class="number">1</span> &lt;&lt; i):  <span class="comment"># 第i位是否为1</span></span><br><span class="line">            <span class="comment"># 处理</span></span><br></pre></td></tr></tbody></table></figure><p>经典题目：旅行商问题 TSP</p><hr><h2 id="七、时间空间复杂度分析"><a href="#七、时间空间复杂度分析" class="headerlink" title="七、时间空间复杂度分析"></a>七、时间空间复杂度分析</h2><h3 id="7-1-时间复杂度"><a href="#7-1-时间复杂度" class="headerlink" title="7.1 时间复杂度"></a>7.1 时间复杂度</h3><p><strong>公式：状态数量 × 每个状态的转移代价</strong></p><table><thead><tr><th>问题</th><th>状态数</th><th>转移代价</th><th>总复杂度</th></tr></thead><tbody><tr><td>斐波那契</td><td>O(n)</td><td>O(1)</td><td>O(n)</td></tr><tr><td>LIS (朴素)</td><td>O(n)</td><td>O(n)</td><td>O(n²)</td></tr><tr><td>LCS</td><td>O(mn)</td><td>O(1)</td><td>O(mn)</td></tr><tr><td>0-1背包</td><td>O(nW)</td><td>O(1)</td><td>O(nW)</td></tr><tr><td>区间DP</td><td>O(n²)</td><td>O(n)</td><td>O(n³)</td></tr></tbody></table><h3 id="7-2-空间复杂度"><a href="#7-2-空间复杂度" class="headerlink" title="7.2 空间复杂度"></a>7.2 空间复杂度</h3><p><strong>基本空间 = DP 数组大小</strong></p><p>优化技巧：</p><ol><li><strong>滚动数组</strong>：O(n) → O(1)</li><li><strong>降维</strong>：O(mn) → O(n)</li><li><strong>只保留必要状态</strong></li></ol><hr><h2 id="八、DP-解题模板"><a href="#八、DP-解题模板" class="headerlink" title="八、DP 解题模板"></a>八、DP 解题模板</h2><p><img src="/images/dp/10_dp_template.png" alt="解题模板"></p><h3 id="8-1-五步法"><a href="#8-1-五步法" class="headerlink" title="8.1 五步法"></a>8.1 五步法</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dp_template</span>(<span class="params"><span class="built_in">input</span></span>):</span><br><span class="line">    <span class="comment"># Step 1: 定义状态</span></span><br><span class="line">    <span class="comment"># dp[i] = ???</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: 初始化（边界条件）</span></span><br><span class="line">    dp = [<span class="number">0</span>] * n</span><br><span class="line">    dp[<span class="number">0</span>] = base_case</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 3: 确定遍历顺序</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">        <span class="comment"># Step 4: 状态转移</span></span><br><span class="line">        dp[i] = f(dp[i-<span class="number">1</span>], ...)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 5: 返回结果</span></span><br><span class="line">    <span class="keyword">return</span> dp[n-<span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure><h3 id="8-2-Debug-技巧"><a href="#8-2-Debug-技巧" class="headerlink" title="8.2 Debug 技巧"></a>8.2 Debug 技巧</h3><ol><li><strong>打印 DP 数组</strong>：看中间状态是否正确</li><li><strong>手算小例子</strong>：n=3,4 时手动验证</li><li><strong>检查边界</strong>：i=0, j=0 的情况</li><li><strong>检查遍历顺序</strong>：依赖的状态是否已计算</li></ol><hr><h2 id="九、LeetCode-经典题目练习"><a href="#九、LeetCode-经典题目练习" class="headerlink" title="九、LeetCode 经典题目练习"></a>九、LeetCode 经典题目练习</h2><h3 id="9-1-入门级（Easy）"><a href="#9-1-入门级（Easy）" class="headerlink" title="9.1 入门级（Easy）"></a>9.1 入门级（Easy）</h3><table><thead><tr><th>题号</th><th>题目</th><th>核心思路</th></tr></thead><tbody><tr><td>70</td><td>爬楼梯</td><td>dp[i]=dp[i-1]+dp[i-2]</td></tr><tr><td>121</td><td>买卖股票最佳时机</td><td>记录最小价格，更新最大利润</td></tr><tr><td>53</td><td>最大子数组和</td><td>dp[i]=max(nums[i], dp[i-1]+nums[i])</td></tr><tr><td>746</td><td>使用最小花费爬楼梯</td><td>dp[i]=min(dp[i-1]+cost[i-1], dp[i-2]+cost[i-2])</td></tr></tbody></table><h3 id="9-2-中等级（Medium）"><a href="#9-2-中等级（Medium）" class="headerlink" title="9.2 中等级（Medium）"></a>9.2 中等级（Medium）</h3><table><thead><tr><th>题号</th><th>题目</th><th>核心思路</th></tr></thead><tbody><tr><td>198</td><td>打家劫舍</td><td>dp[i]=max(dp[i-1], dp[i-2]+nums[i])</td></tr><tr><td>300</td><td>最长递增子序列</td><td>dp[i]=max(dp[j]+1)</td></tr><tr><td>322</td><td>零钱兑换</td><td>完全背包变形</td></tr><tr><td>62</td><td>不同路径</td><td>dp[i][j]=dp[i-1][j]+dp[i][j-1]</td></tr><tr><td>64</td><td>最小路径和</td><td>同上，取min</td></tr><tr><td>139</td><td>单词拆分</td><td>dp[i]=any(dp[j] and s[j:i] in dict)</td></tr><tr><td>152</td><td>乘积最大子数组</td><td>同时维护最大和最小</td></tr><tr><td>1143</td><td>最长公共子序列</td><td>双序列DP经典</td></tr></tbody></table><h3 id="9-3-困难级（Hard）"><a href="#9-3-困难级（Hard）" class="headerlink" title="9.3 困难级（Hard）"></a>9.3 困难级（Hard）</h3><table><thead><tr><th>题号</th><th>题目</th><th>核心思路</th></tr></thead><tbody><tr><td>72</td><td>编辑距离</td><td>三种操作取min</td></tr><tr><td>312</td><td>戳气球</td><td>区间DP</td></tr><tr><td>10</td><td>正则表达式匹配</td><td>双序列DP + 特殊字符处理</td></tr><tr><td>32</td><td>最长有效括号</td><td>dp[i]=以i结尾的最长有效长度</td></tr><tr><td>115</td><td>不同的子序列</td><td>dp[i][j]=s前i个中t前j个出现次数</td></tr></tbody></table><hr><h2 id="十、费曼总结：用一句话解释-DP"><a href="#十、费曼总结：用一句话解释-DP" class="headerlink" title="十、费曼总结：用一句话解释 DP"></a>十、费曼总结：用一句话解释 DP</h2><h3 id="10-1-给小学生解释"><a href="#10-1-给小学生解释" class="headerlink" title="10.1 给小学生解释"></a>10.1 给小学生解释</h3><p><strong>“做作业时，把算过的题目答案记在草稿纸上，下次遇到一样的题直接抄答案，不用重新算。”</strong></p><h3 id="10-2-给程序员解释"><a href="#10-2-给程序员解释" class="headerlink" title="10.2 给程序员解释"></a>10.2 给程序员解释</h3><p><strong>“用哈希表缓存递归的中间结果，或者用数组从小到大迭代填表。”</strong></p><h3 id="10-3-给面试官解释"><a href="#10-3-给面试官解释" class="headerlink" title="10.3 给面试官解释"></a>10.3 给面试官解释</h3><p><strong>“DP 是一种通过将问题分解为重叠子问题，并存储子问题的解来避免重复计算的优化技术。它适用于具有最优子结构和重叠子问题性质的问题。”</strong></p><h3 id="10-4-核心公式"><a href="#10-4-核心公式" class="headerlink" title="10.4 核心公式"></a>10.4 核心公式</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">DP = 递归 + 记忆化 = 分治 + 缓存</span><br></pre></td></tr></tbody></table></figure><h3 id="10-5-判断是否能用-DP"><a href="#10-5-判断是否能用-DP" class="headerlink" title="10.5 判断是否能用 DP"></a>10.5 判断是否能用 DP</h3><p>问自己两个问题：</p><ol><li><strong>最优子结构</strong>：大问题的最优解能否由小问题的最优解推导出来？</li><li><strong>重叠子问题</strong>：在求解过程中，是否会重复计算相同的子问题？</li></ol><p>如果两个都是 Yes，就可以用 DP！</p><hr><h2 id="十一、常见错误与陷阱"><a href="#十一、常见错误与陷阱" class="headerlink" title="十一、常见错误与陷阱"></a>十一、常见错误与陷阱</h2><h3 id="11-1-状态定义不清"><a href="#11-1-状态定义不清" class="headerlink" title="11.1 状态定义不清"></a>11.1 状态定义不清</h3><p><strong>错误：</strong> 模糊地定义 <code>dp[i]</code> 是”第 i 个的答案”</p><p><strong>正确：</strong> 明确是”以第 i 个结尾”还是”前 i 个”</p><h3 id="11-2-边界条件遗漏"><a href="#11-2-边界条件遗漏" class="headerlink" title="11.2 边界条件遗漏"></a>11.2 边界条件遗漏</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 错误：忘记处理空数组</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">maxSubArray</span>(<span class="params">nums</span>):</span><br><span class="line">    dp = [<span class="number">0</span>] * <span class="built_in">len</span>(nums)  <span class="comment"># 如果 nums 为空会报错</span></span><br><span class="line">    ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 正确：先检查边界</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">maxSubArray</span>(<span class="params">nums</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> nums:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    ...</span><br></pre></td></tr></tbody></table></figure><h3 id="11-3-遍历顺序错误"><a href="#11-3-遍历顺序错误" class="headerlink" title="11.3 遍历顺序错误"></a>11.3 遍历顺序错误</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0-1背包：必须从右往左</span></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> <span class="built_in">range</span>(W, weight - <span class="number">1</span>, -<span class="number">1</span>):  <span class="comment"># 正确</span></span><br><span class="line"><span class="keyword">for</span> w <span class="keyword">in</span> <span class="built_in">range</span>(weight, W + <span class="number">1</span>):       <span class="comment"># 错误！变成完全背包了</span></span><br></pre></td></tr></tbody></table></figure><h3 id="11-4-返回值搞错"><a href="#11-4-返回值搞错" class="headerlink" title="11.4 返回值搞错"></a>11.4 返回值搞错</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LIS：返回 max(dp)，不是 dp[n-1]</span></span><br><span class="line"><span class="keyword">return</span> <span class="built_in">max</span>(dp)  <span class="comment"># 正确</span></span><br><span class="line"><span class="keyword">return</span> dp[n-<span class="number">1</span>]  <span class="comment"># 错误！</span></span><br></pre></td></tr></tbody></table></figure><hr><h2 id="十二、进阶技巧"><a href="#十二、进阶技巧" class="headerlink" title="十二、进阶技巧"></a>十二、进阶技巧</h2><h3 id="12-1-状态压缩"><a href="#12-1-状态压缩" class="headerlink" title="12.1 状态压缩"></a>12.1 状态压缩</h3><p>当状态是一个集合时，用二进制数表示：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 表示选了哪些元素</span></span><br><span class="line">mask = <span class="number">0b1011</span>  <span class="comment"># 选了第0、1、3个元素</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 检查第i位</span></span><br><span class="line"><span class="keyword">if</span> mask &amp; (<span class="number">1</span> &lt;&lt; i): ...</span><br><span class="line"></span><br><span class="line"><span class="comment"># 设置第i位</span></span><br><span class="line">mask |= (<span class="number">1</span> &lt;&lt; i)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 清除第i位</span></span><br><span class="line">mask &amp;= ~(<span class="number">1</span> &lt;&lt; i)</span><br></pre></td></tr></tbody></table></figure><h3 id="12-2-单调队列优化"><a href="#12-2-单调队列优化" class="headerlink" title="12.2 单调队列优化"></a>12.2 单调队列优化</h3><p>当转移方程形如 <code>dp[i] = max(dp[j]) + cost</code> 且 j 在滑动窗口内时：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">solve</span>(<span class="params">nums, k</span>):</span><br><span class="line">    q = deque()  <span class="comment"># 存储下标，保持单调递减</span></span><br><span class="line">    dp = [<span class="number">0</span>] * n</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">while</span> q <span class="keyword">and</span> q[<span class="number">0</span>] &lt; i - k:</span><br><span class="line">            q.popleft()</span><br><span class="line">        dp[i] = nums[q[<span class="number">0</span>]] + cost <span class="keyword">if</span> q <span class="keyword">else</span> cost</span><br><span class="line">        <span class="keyword">while</span> q <span class="keyword">and</span> nums[q[-<span class="number">1</span>]] &lt;= nums[i]:</span><br><span class="line">            q.pop()</span><br><span class="line">        q.append(i)</span><br></pre></td></tr></tbody></table></figure><h3 id="12-3-斜率优化"><a href="#12-3-斜率优化" class="headerlink" title="12.3 斜率优化"></a>12.3 斜率优化</h3><p>当转移方程形如 <code>dp[i] = min(dp[j] + f(i,j))</code> 且 f 可以分离变量时，用凸包优化到 O(n)。</p><hr><h2 id="十三、实战代码模板汇总"><a href="#十三、实战代码模板汇总" class="headerlink" title="十三、实战代码模板汇总"></a>十三、实战代码模板汇总</h2><h3 id="13-1-线性-DP-模板"><a href="#13-1-线性-DP-模板" class="headerlink" title="13.1 线性 DP 模板"></a>13.1 线性 DP 模板</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">linear_dp</span>(<span class="params">nums</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(nums)</span><br><span class="line">    <span class="keyword">if</span> n == <span class="number">0</span>: <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    dp = [<span class="number">0</span>] * n</span><br><span class="line">    dp[<span class="number">0</span>] = nums[<span class="number">0</span>]  <span class="comment"># base case</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">        dp[i] = f(dp[i-<span class="number">1</span>], nums[i])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dp[n-<span class="number">1</span>]  <span class="comment"># 或 max(dp)</span></span><br></pre></td></tr></tbody></table></figure><h3 id="13-2-双序列-DP-模板"><a href="#13-2-双序列-DP-模板" class="headerlink" title="13.2 双序列 DP 模板"></a>13.2 双序列 DP 模板</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">two_seq_dp</span>(<span class="params">s1, s2</span>):</span><br><span class="line">    m, n = <span class="built_in">len</span>(s1), <span class="built_in">len</span>(s2)</span><br><span class="line">    dp = [[<span class="number">0</span>] * (n+<span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m+<span class="number">1</span>)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 边界初始化</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m+<span class="number">1</span>): dp[i][<span class="number">0</span>] = init_val</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n+<span class="number">1</span>): dp[<span class="number">0</span>][j] = init_val</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> s1[i-<span class="number">1</span>] == s2[j-<span class="number">1</span>]:</span><br><span class="line">                dp[i][j] = dp[i-<span class="number">1</span>][j-<span class="number">1</span>] + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dp[i][j] = <span class="built_in">max</span>(dp[i-<span class="number">1</span>][j], dp[i][j-<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dp[m][n]</span><br></pre></td></tr></tbody></table></figure><h3 id="13-3-背包-DP-模板"><a href="#13-3-背包-DP-模板" class="headerlink" title="13.3 背包 DP 模板"></a>13.3 背包 DP 模板</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 0-1 背包</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">knapsack_01</span>(<span class="params">weights, values, W</span>):</span><br><span class="line">    dp = [<span class="number">0</span>] * (W + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(weights)):</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> <span class="built_in">range</span>(W, weights[i]-<span class="number">1</span>, -<span class="number">1</span>):  <span class="comment"># 从右往左</span></span><br><span class="line">            dp[w] = <span class="built_in">max</span>(dp[w], dp[w-weights[i]] + values[i])</span><br><span class="line">    <span class="keyword">return</span> dp[W]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 完全背包</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">knapsack_complete</span>(<span class="params">weights, values, W</span>):</span><br><span class="line">    dp = [<span class="number">0</span>] * (W + <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(weights)):</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> <span class="built_in">range</span>(weights[i], W+<span class="number">1</span>):  <span class="comment"># 从左往右</span></span><br><span class="line">            dp[w] = <span class="built_in">max</span>(dp[w], dp[w-weights[i]] + values[i])</span><br><span class="line">    <span class="keyword">return</span> dp[W]</span><br></pre></td></tr></tbody></table></figure><h3 id="13-4-区间-DP-模板"><a href="#13-4-区间-DP-模板" class="headerlink" title="13.4 区间 DP 模板"></a>13.4 区间 DP 模板</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">interval_dp</span>(<span class="params">nums</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(nums)</span><br><span class="line">    dp = [[<span class="number">0</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 长度从小到大</span></span><br><span class="line">    <span class="keyword">for</span> length <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n+<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - length + <span class="number">1</span>):</span><br><span class="line">            j = i + length - <span class="number">1</span></span><br><span class="line">            dp[i][j] = <span class="built_in">float</span>(<span class="string">'inf'</span>)</span><br><span class="line">            <span class="keyword">for</span> k <span class="keyword">in</span> <span class="built_in">range</span>(i, j):</span><br><span class="line">                dp[i][j] = <span class="built_in">min</span>(dp[i][j], dp[i][k] + dp[k+<span class="number">1</span>][j] + cost(i,j))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dp[<span class="number">0</span>][n-<span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="十四、面试高频问题速查"><a href="#十四、面试高频问题速查" class="headerlink" title="十四、面试高频问题速查"></a>十四、面试高频问题速查</h2><h3 id="14-1-股票买卖系列"><a href="#14-1-股票买卖系列" class="headerlink" title="14.1 股票买卖系列"></a>14.1 股票买卖系列</h3><table><thead><tr><th>题号</th><th>限制</th><th>状态定义</th></tr></thead><tbody><tr><td>121</td><td>只能买卖1次</td><td>记录最小价格</td></tr><tr><td>122</td><td>无限次</td><td>贪心：所有上涨都吃</td></tr><tr><td>123</td><td>最多2次</td><td>dp[i][k][0/1]</td></tr><tr><td>188</td><td>最多k次</td><td>同上</td></tr><tr><td>309</td><td>有冷冻期</td><td>增加冷冻状态</td></tr><tr><td>714</td><td>有手续费</td><td>卖出时减fee</td></tr></tbody></table><h3 id="14-2-子序列系列"><a href="#14-2-子序列系列" class="headerlink" title="14.2 子序列系列"></a>14.2 子序列系列</h3><table><thead><tr><th>题目</th><th>关键区别</th></tr></thead><tbody><tr><td>子序列</td><td>可以不连续</td></tr><tr><td>子数组</td><td>必须连续</td></tr><tr><td>子串</td><td>必须连续（字符串版）</td></tr></tbody></table><h3 id="14-3-路径系列"><a href="#14-3-路径系列" class="headerlink" title="14.3 路径系列"></a>14.3 路径系列</h3><table><thead><tr><th>题号</th><th>变形</th></tr></thead><tbody><tr><td>62</td><td>基础版</td></tr><tr><td>63</td><td>有障碍物</td></tr><tr><td>64</td><td>带权重（最小路径和）</td></tr><tr><td>120</td><td>三角形</td></tr><tr><td>931</td><td>下降路径最小和</td></tr></tbody></table><hr><h2 id="十五、总结思维导图"><a href="#十五、总结思维导图" class="headerlink" title="十五、总结思维导图"></a>十五、总结思维导图</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">动态规划</span><br><span class="line">├── 核心思想</span><br><span class="line">│   ├── 最优子结构</span><br><span class="line">│   ├── 重叠子问题</span><br><span class="line">│   └── 空间换时间</span><br><span class="line">├── 三要素</span><br><span class="line">│   ├── 状态定义 dp[i] = ?</span><br><span class="line">│   ├── 转移方程 dp[i] = f(dp[...])</span><br><span class="line">│   └── 边界条件 dp[0] = ?</span><br><span class="line">├── 实现方式</span><br><span class="line">│   ├── Top-Down 记忆化递归</span><br><span class="line">│   └── Bottom-Up 迭代填表</span><br><span class="line">├── 优化技巧</span><br><span class="line">│   ├── 滚动数组</span><br><span class="line">│   ├── 降维</span><br><span class="line">│   └── 单调队列/斜率优化</span><br><span class="line">└── 问题分类</span><br><span class="line">    ├── 线性DP</span><br><span class="line">    ├── 序列DP</span><br><span class="line">    ├── 背包DP</span><br><span class="line">    ├── 区间DP</span><br><span class="line">    ├── 树形DP</span><br><span class="line">    └── 状压DP</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li>《算法导论》第15章 - 动态规划</li><li>LeetCode 动态规划专题</li><li><a href="https://oi-wiki.org/dp/">OI Wiki - 动态规划</a></li></ol><hr><blockquote><p>💡 <strong>费曼学习法核心</strong>：如果你能把 DP 解释给一个完全不懂编程的人听，你就真正理解了它。</p><p>记住：<strong>DP 不是背模板，而是理解”如何把大问题拆成小问题，并记住小问题的答案”。</strong></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 动态规划 </tag>
            
            <tag> DP </tag>
            
            <tag> LeetCode </tag>
            
            <tag> 算法面试 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单调栈完全指南：从 O(n²) 到 O(n) 的优雅跃迁</title>
      <link href="/2026/02/19/monotonic-stack-guide/"/>
      <url>/2026/02/19/monotonic-stack-guide/</url>
      
        <content type="html"><![CDATA[<h1 id="单调栈完全指南：从-O-n²-到-O-n-的优雅跃迁"><a href="#单调栈完全指南：从-O-n²-到-O-n-的优雅跃迁" class="headerlink" title="单调栈完全指南：从 O(n²) 到 O(n) 的优雅跃迁"></a>单调栈完全指南：从 O(n²) 到 O(n) 的优雅跃迁</h1><h2 id="🎯-一句话本质"><a href="#🎯-一句话本质" class="headerlink" title="🎯 一句话本质"></a>🎯 一句话本质</h2><blockquote><p><strong>单调栈通过维护单调性来及时淘汰无用候选元素，将”寻找下一个更大/更小元素”问题从 O(n²) 优化到 O(n)。</strong></p></blockquote><p>它不是一种数据结构，而是一种<strong>遍历策略</strong> —— 通过栈的单调性保证”不漏掉答案”的同时”跳过不可能的区域”。</p><hr><h2 id="🤔-为什么需要单调栈？"><a href="#🤔-为什么需要单调栈？" class="headerlink" title="🤔 为什么需要单调栈？"></a>🤔 为什么需要单调栈？</h2><h3 id="问题场景"><a href="#问题场景" class="headerlink" title="问题场景"></a>问题场景</h3><p>给定数组 <code>[2, 1, 5, 6, 2, 3]</code>，对于每个元素，找到它右边第一个比它大的元素。</p><p><strong>暴力解法</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">next_greater_brute</span>(<span class="params">nums</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(nums)</span><br><span class="line">    result = [-<span class="number">1</span>] * n</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i+<span class="number">1</span>, n):  <span class="comment"># 向右扫描</span></span><br><span class="line">            <span class="keyword">if</span> nums[j] &gt; nums[i]:</span><br><span class="line">                result[i] = nums[j]</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><ul><li><strong>时间复杂度</strong>：O(n²) - 每个元素都要向右扫描</li><li><strong>问题</strong>：大量重复计算</li></ul><h3 id="💡-关键洞察"><a href="#💡-关键洞察" class="headerlink" title="💡 关键洞察"></a>💡 关键洞察</h3><p><strong>如果 <code>nums[i] &lt; nums[j]</code>（i &lt; j），那么对于 j 右边的元素来说，i 永远不可能是答案！</strong></p><p><strong>为什么？</strong> 因为 j 更大且更靠右，它会”遮挡”住 i。</p><p>这就是单调栈的核心：<strong>维护一个递减的候选序列，及时淘汰无用元素</strong>。</p><hr><h2 id="🔬-工作原理：数据流与状态转移"><a href="#🔬-工作原理：数据流与状态转移" class="headerlink" title="🔬 工作原理：数据流与状态转移"></a>🔬 工作原理：数据流与状态转移</h2><h3 id="单调栈的处理流程"><a href="#单调栈的处理流程" class="headerlink" title="单调栈的处理流程"></a>单调栈的处理流程</h3><p>以 <code>[2, 1, 5, 6, 2, 3]</code> 为例，维护一个<strong>单调递减栈</strong>（栈底到栈顶递减）：</p><table><thead><tr><th>步骤</th><th>当前元素</th><th>栈状态（存索引）</th><th>操作</th><th>发现的答案</th></tr></thead><tbody><tr><td>1</td><td>2 (i=0)</td><td>[0]</td><td>直接入栈</td><td>-</td></tr><tr><td>2</td><td>1 (i=1)</td><td>[0, 1]</td><td>1 &lt; 2，直接入栈</td><td>-</td></tr><tr><td>3</td><td>5 (i=2)</td><td>[2]</td><td>5 &gt; 1，弹出1；5 &gt; 2，弹出2；5入栈</td><td>nums[1]=5, nums[0]=5</td></tr><tr><td>4</td><td>6 (i=3)</td><td>[3]</td><td>6 &gt; 5，弹出5；6入栈</td><td>nums[2]=6</td></tr><tr><td>5</td><td>2 (i=4)</td><td>[3, 4]</td><td>2 &lt; 6，直接入栈</td><td>-</td></tr><tr><td>6</td><td>3 (i=5)</td><td>[3, 5]</td><td>3 &gt; 2，弹出2；3 &lt; 6，3入栈</td><td>nums[4]=3</td></tr></tbody></table><p><strong>最终结果</strong>：<code>[5, 5, 6, -1, 3, -1]</code></p><h3 id="📊-可视化：栈的动态演变"><a href="#📊-可视化：栈的动态演变" class="headerlink" title="📊 可视化：栈的动态演变"></a>📊 可视化：栈的动态演变</h3><p><img src="/images/monotonic-stack/monotonic_stack_states.png" alt="单调栈状态变化"></p><p>上图展示了处理数组的6个关键步骤，每个步骤包含：</p><ul><li><strong>输入数组</strong>：当前处理的元素用红色高亮</li><li><strong>栈状态</strong>：绿色方块表示栈中的元素</li><li><strong>结果数组</strong>：黄色表示已找到答案，灰色表示未找到</li></ul><h3 id="🎬-动画演示"><a href="#🎬-动画演示" class="headerlink" title="🎬 动画演示"></a>🎬 动画演示</h3><p><img src="/images/monotonic-stack/monotonic_stack_animation.gif" alt="单调栈动画"></p><p>动画展示了完整的处理流程，可以清晰看到：</p><ul><li>元素逐个进入处理</li><li>栈的动态变化（入栈/出栈）</li><li>结果数组的实时更新</li></ul><h3 id="💻-核心代码实现"><a href="#💻-核心代码实现" class="headerlink" title="💻 核心代码实现"></a>💻 核心代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">next_greater_element</span>(<span class="params">nums</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(nums)</span><br><span class="line">    result = [-<span class="number">1</span>] * n  <span class="comment"># 初始化答案数组</span></span><br><span class="line">    stack = []  <span class="comment"># 单调栈（存储索引）</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        <span class="comment"># 当前元素 &gt; 栈顶元素时，找到了栈顶的答案</span></span><br><span class="line">        <span class="keyword">while</span> stack <span class="keyword">and</span> nums[i] &gt; nums[stack[-<span class="number">1</span>]]:</span><br><span class="line">            idx = stack.pop()</span><br><span class="line">            result[idx] = nums[i]  <span class="comment"># 记录答案</span></span><br><span class="line">        stack.append(i)  <span class="comment"># 当前索引入栈</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><p><strong>数据流向</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">输入数组 → 逐个处理 → 单调栈（维护候选） → 输出答案数组</span><br><span class="line">         ↑                    ↓</span><br><span class="line">         └────── 弹出时记录答案 ──┘</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="⏱️-时间复杂度证明"><a href="#⏱️-时间复杂度证明" class="headerlink" title="⏱️ 时间复杂度证明"></a>⏱️ 时间复杂度证明</h2><p><img src="/images/monotonic-stack/monotonic_stack_complexity.png" alt="时间复杂度可视化"></p><h3 id="为什么是-O-n-？"><a href="#为什么是-O-n-？" class="headerlink" title="为什么是 O(n)？"></a>为什么是 O(n)？</h3><p><strong>关键洞察</strong>：每个元素最多入栈一次，出栈一次。</p><p><strong>证明</strong>：</p><ul><li>外层循环：遍历 n 个元素 → O(n)</li><li>内层 while 循环：看似嵌套，但总共最多弹出 n 次<ul><li>每个元素入栈 1 次</li><li>每个元素出栈 ≤ 1 次</li><li>总操作次数 ≤ 2n</li></ul></li></ul><p><strong>均摊分析</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">总入栈次数 = n</span><br><span class="line">总出栈次数 ≤ n</span><br><span class="line">总操作 = n + n = 2n = O(n)</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="🎯-经典问题实战"><a href="#🎯-经典问题实战" class="headerlink" title="🎯 经典问题实战"></a>🎯 经典问题实战</h2><h3 id="问题-1：下一个更大元素-II（循环数组）"><a href="#问题-1：下一个更大元素-II（循环数组）" class="headerlink" title="问题 1：下一个更大元素 II（循环数组）"></a>问题 1：下一个更大元素 II（循环数组）</h3><p><strong>问题</strong>：数组是循环的，如 <code>[1, 2, 1]</code> 中，最后一个 1 的答案是 2。</p><p><strong>解法</strong>：将数组复制一遍，模拟循环。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">next_greater_circular</span>(<span class="params">nums</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(nums)</span><br><span class="line">    result = [-<span class="number">1</span>] * n</span><br><span class="line">    stack = []</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 遍历两遍数组（模拟循环）</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span> * n):</span><br><span class="line">        idx = i % n  <span class="comment"># 实际索引</span></span><br><span class="line">        <span class="keyword">while</span> stack <span class="keyword">and</span> nums[idx] &gt; nums[stack[-<span class="number">1</span>]]:</span><br><span class="line">            result[stack.pop()] = nums[idx]</span><br><span class="line">        <span class="keyword">if</span> i &lt; n:  <span class="comment"># 只在第一遍时入栈</span></span><br><span class="line">            stack.append(idx)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><h3 id="问题-2：柱状图中最大的矩形"><a href="#问题-2：柱状图中最大的矩形" class="headerlink" title="问题 2：柱状图中最大的矩形"></a>问题 2：柱状图中最大的矩形</h3><p><strong>问题</strong>：给定柱状图高度 <code>[2, 1, 5, 6, 2, 3]</code>，找最大矩形面积。</p><p><strong>核心思路</strong>：对于每个柱子，找到它左右两边第一个比它矮的柱子，计算以它为高的矩形面积。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">largest_rectangle_area</span>(<span class="params">heights</span>):</span><br><span class="line">    stack = []</span><br><span class="line">    max_area = <span class="number">0</span></span><br><span class="line">    heights = [<span class="number">0</span>] + heights + [<span class="number">0</span>]  <span class="comment"># 哨兵技巧</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, h <span class="keyword">in</span> <span class="built_in">enumerate</span>(heights):</span><br><span class="line">        <span class="keyword">while</span> stack <span class="keyword">and</span> h &lt; heights[stack[-<span class="number">1</span>]]:</span><br><span class="line">            height_idx = stack.pop()</span><br><span class="line">            height = heights[height_idx]</span><br><span class="line">            width = i - stack[-<span class="number">1</span>] - <span class="number">1</span>  <span class="comment"># 左右边界</span></span><br><span class="line">            max_area = <span class="built_in">max</span>(max_area, height * width)</span><br><span class="line">        stack.append(i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> max_area</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/monotonic-stack/monotonic_stack_rectangle.png" alt="柱状图最大矩形"></p><p>上图展示了柱状图中最大矩形的可视化，红色虚线框标出了面积为10的最大矩形。</p><h3 id="问题-3：每日温度"><a href="#问题-3：每日温度" class="headerlink" title="问题 3：每日温度"></a>问题 3：每日温度</h3><p><strong>问题</strong>：给定温度数组 <code>[73, 74, 75, 71, 69, 72, 76, 73]</code>，计算每天需要等几天才能等到更暖和的温度。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">daily_temperatures</span>(<span class="params">temperatures</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(temperatures)</span><br><span class="line">    result = [<span class="number">0</span>] * n</span><br><span class="line">    stack = []</span><br><span class="line"></span><br><span class="line">    <span class="keyword">for</span> i, temp <span class="keyword">in</span> <span class="built_in">enumerate</span>(temperatures):</span><br><span class="line">        <span class="keyword">while</span> stack <span class="keyword">and</span> temp &gt; temperatures[stack[-<span class="number">1</span>]]:</span><br><span class="line">            idx = stack.pop()</span><br><span class="line">            result[idx] = i - idx  <span class="comment"># 天数差</span></span><br><span class="line">        stack.append(i)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><p><strong>输出</strong>：<code>[1, 1, 4, 2, 1, 1, 0, 0]</code></p><hr><h2 id="🧠-费曼总结：用简单的话解释单调栈"><a href="#🧠-费曼总结：用简单的话解释单调栈" class="headerlink" title="🧠 费曼总结：用简单的话解释单调栈"></a>🧠 费曼总结：用简单的话解释单调栈</h2><h3 id="给10岁小孩的解释"><a href="#给10岁小孩的解释" class="headerlink" title="给10岁小孩的解释"></a>给10岁小孩的解释</h3><p>想象你在排队买票，队伍里每个人都有一个身高牌。你的任务是：<strong>告诉每个人，他后面第一个比他高的人是谁</strong>。</p><p><strong>笨办法</strong>：每个人都要回头看一遍后面所有人 → 很慢（O(n²)）</p><p><strong>聪明办法（单调栈）</strong>：</p><ol><li>维护一个”候选队列”，队列里的人从前到后越来越矮</li><li>新人来了，如果他比队尾的人高：<ul><li>队尾的人找到答案了！（就是这个新人）</li><li>把队尾的人踢出去（他已经没用了）</li><li>继续比较，直到新人不比队尾高</li></ul></li><li>新人加入队尾</li></ol><p><strong>为什么这样快？</strong> 每个人最多进队一次、出队一次 → O(n)</p><h3 id="核心要点"><a href="#核心要点" class="headerlink" title="核心要点"></a>核心要点</h3><ol><li><p><strong>什么时候用单调栈？</strong></p><ul><li>需要找”下一个更大/更小元素”</li><li>需要找”左右边界”</li><li>暴力解法需要嵌套循环</li></ul></li><li><p><strong>单调栈的本质</strong></p><ul><li>维护一个候选序列</li><li>新元素到来时，淘汰被”遮挡”的元素</li><li>弹出时记录答案</li></ul></li><li><p><strong>为什么是 O(n)？</strong></p><ul><li>每个元素最多入栈一次，出栈一次</li><li>总操作次数 ≤ 2n</li></ul></li><li><p><strong>单调递增 vs 单调递减</strong></p><ul><li>单调递增栈：找下一个更小元素</li><li>单调递减栈：找下一个更大元素</li></ul></li></ol><h3 id="记忆口诀"><a href="#记忆口诀" class="headerlink" title="记忆口诀"></a>记忆口诀</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">遇到"下一个更大"，单调栈来帮忙</span><br><span class="line">栈顶小于当前值，弹出记录答案忙</span><br><span class="line">每个元素进出一次，时间复杂度 O(n) 强</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="🧠-费曼总结：用简单的话解释单调栈-1"><a href="#🧠-费曼总结：用简单的话解释单调栈-1" class="headerlink" title="🧠 费曼总结：用简单的话解释单调栈"></a>🧠 费曼总结：用简单的话解释单调栈</h2><h3 id="给10岁小孩解释"><a href="#给10岁小孩解释" class="headerlink" title="给10岁小孩解释"></a>给10岁小孩解释</h3><p>想象你在排队买冰淇淋，队伍里每个人都有不同的身高。</p><p><strong>问题</strong>：每个人都想知道，自己后面第一个比自己高的人是谁？</p><p><strong>笨办法</strong>：每个人都回头看，一个一个找 → 很慢（O(n²)）</p><p><strong>聪明办法（单调栈）</strong>：</p><ol><li>维护一个”候选队列”，队列里的人从前到后越来越矮</li><li>新人来了：<ul><li>如果比队尾的人矮 → 直接加入队列</li><li>如果比队尾的人高 → 队尾的人找到答案了！把他踢出去，继续比较</li></ul></li><li>每个人最多进出队列一次 → 很快（O(n)）</li></ol><h3 id="核心要点-1"><a href="#核心要点-1" class="headerlink" title="核心要点"></a>核心要点</h3><ol><li><p><strong>什么时候用单调栈？</strong></p><ul><li>需要找”下一个更大/更小元素”</li><li>需要找”左右边界”</li><li>暴力解法需要嵌套循环</li></ul></li><li><p><strong>单调栈的本质</strong></p><ul><li>维护一个候选序列</li><li>新元素到来时，淘汰被”遮挡”的元素</li><li>弹出时记录答案</li></ul></li><li><p><strong>为什么是 O(n)？</strong></p><ul><li>每个元素最多入栈一次，出栈一次</li><li>总操作次数 ≤ 2n</li></ul></li><li><p><strong>单调递增 vs 单调递减</strong></p><ul><li>单调递增栈：找下一个更小元素</li><li>单调递减栈：找下一个更大元素</li></ul></li></ol><h3 id="记忆口诀-1"><a href="#记忆口诀-1" class="headerlink" title="记忆口诀"></a>记忆口诀</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">遇到"下一个更大"，单调栈来帮忙</span><br><span class="line">栈顶小于当前值，弹出记录答案忙</span><br><span class="line">每个元素进出一次，时间复杂度 O(n) 强</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="📝-代码模板"><a href="#📝-代码模板" class="headerlink" title="📝 代码模板"></a>📝 代码模板</h2><h3 id="找下一个更大元素（单调递减栈）"><a href="#找下一个更大元素（单调递减栈）" class="headerlink" title="找下一个更大元素（单调递减栈）"></a>找下一个更大元素（单调递减栈）</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">next_greater</span>(<span class="params">nums</span>):</span><br><span class="line">    result = [-<span class="number">1</span>] * <span class="built_in">len</span>(nums)</span><br><span class="line">    stack = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">        <span class="keyword">while</span> stack <span class="keyword">and</span> nums[i] &gt; nums[stack[-<span class="number">1</span>]]:</span><br><span class="line">            result[stack.pop()] = nums[i]</span><br><span class="line">        stack.append(i)</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><h3 id="找下一个更小元素（单调递增栈）"><a href="#找下一个更小元素（单调递增栈）" class="headerlink" title="找下一个更小元素（单调递增栈）"></a>找下一个更小元素（单调递增栈）</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">next_smaller</span>(<span class="params">nums</span>):</span><br><span class="line">    result = [-<span class="number">1</span>] * <span class="built_in">len</span>(nums)</span><br><span class="line">    stack = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">        <span class="keyword">while</span> stack <span class="keyword">and</span> nums[i] &lt; nums[stack[-<span class="number">1</span>]]:</span><br><span class="line">            result[stack.pop()] = nums[i]</span><br><span class="line">        stack.append(i)</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="📚-LeetCode-题单"><a href="#📚-LeetCode-题单" class="headerlink" title="📚 LeetCode 题单"></a>📚 LeetCode 题单</h2><h3 id="基础题（Easy）"><a href="#基础题（Easy）" class="headerlink" title="基础题（Easy）"></a>基础题（Easy）</h3><ul><li><a href="https://leetcode.com/problems/next-greater-element-i/">496. 下一个更大元素 I</a></li><li><a href="https://leetcode.com/problems/daily-temperatures/">739. 每日温度</a></li></ul><h3 id="进阶题（Medium）"><a href="#进阶题（Medium）" class="headerlink" title="进阶题（Medium）"></a>进阶题（Medium）</h3><ul><li><a href="https://leetcode.com/problems/next-greater-element-ii/">503. 下一个更大元素 II</a></li><li><a href="https://leetcode.com/problems/online-stock-span/">901. 股票价格跨度</a></li></ul><h3 id="困难题（Hard）"><a href="#困难题（Hard）" class="headerlink" title="困难题（Hard）"></a>困难题（Hard）</h3><ul><li><a href="https://leetcode.com/problems/largest-rectangle-in-histogram/">84. 柱状图中最大的矩形</a></li><li><a href="https://leetcode.com/problems/trapping-rain-water/">42. 接雨水</a></li><li><a href="https://leetcode.com/problems/maximal-rectangle/">85. 最大矩形</a></li></ul><hr><h2 id="🎓-进阶思考"><a href="#🎓-进阶思考" class="headerlink" title="🎓 进阶思考"></a>🎓 进阶思考</h2><h3 id="1-单调队列-vs-单调栈"><a href="#1-单调队列-vs-单调栈" class="headerlink" title="1. 单调队列 vs 单调栈"></a>1. 单调队列 vs 单调栈</h3><ul><li><strong>单调队列</strong>：支持队首队尾操作，用于滑动窗口最大值</li><li><strong>单调栈</strong>：只支持栈顶操作，用于寻找边界</li></ul><h3 id="2-双单调栈"><a href="#2-双单调栈" class="headerlink" title="2. 双单调栈"></a>2. 双单调栈</h3><p>某些问题需要同时维护左右边界，可以用两个单调栈分别处理。</p><h3 id="3-单调栈-DP"><a href="#3-单调栈-DP" class="headerlink" title="3. 单调栈 + DP"></a>3. 单调栈 + DP</h3><p>在某些 DP 问题中，单调栈可以优化状态转移。</p><hr><h2 id="🔑-关键要点总结"><a href="#🔑-关键要点总结" class="headerlink" title="🔑 关键要点总结"></a>🔑 关键要点总结</h2><table><thead><tr><th>特性</th><th>说明</th></tr></thead><tbody><tr><td><strong>适用场景</strong></td><td>找下一个更大/更小元素、左右边界</td></tr><tr><td><strong>时间复杂度</strong></td><td>O(n)</td></tr><tr><td><strong>空间复杂度</strong></td><td>O(n)</td></tr><tr><td><strong>核心思想</strong></td><td>维护单调性，及时淘汰无用元素</td></tr><tr><td><strong>存储内容</strong></td><td>通常存索引而非值</td></tr></tbody></table><hr><p><strong>创建时间</strong>：2026-02-19<br><strong>标签</strong>：#算法 #单调栈 #数据结构 #优化技巧</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>双指针（Two Pointers）Feynman 风格教程</title>
      <link href="/2026/02/19/20260219-%E5%8F%8C%E6%8C%87%E9%92%88%E6%95%99%E7%A8%8B/"/>
      <url>/2026/02/19/20260219-%E5%8F%8C%E6%8C%87%E9%92%88%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="双指针（Two-Pointers）—-Feynman-风格教程"><a href="#双指针（Two-Pointers）—-Feynman-风格教程" class="headerlink" title="双指针（Two Pointers）— Feynman 风格教程"></a>双指针（Two Pointers）— Feynman 风格教程</h1><blockquote><p>目标：把“两个指针协作”的机制讲清楚，给出可运行的题型模板、具体 I/O 示例，并配合图像理解。</p></blockquote><h2 id="1-核心直觉：两个指针-x3D-两个同步的“游标”"><a href="#1-核心直觉：两个指针-x3D-两个同步的“游标”" class="headerlink" title="1. 核心直觉：两个指针 = 两个同步的“游标”"></a>1. 核心直觉：两个指针 = 两个同步的“游标”</h2><p>把数组当成一条直线，两个指针就是两个游标。它们用“局部信息”决定下一步动作，从而减少搜索空间。</p><p><strong>关键机制</strong>：</p><ul><li><strong>有序信息</strong>（单调性）让我们能“确定方向”。</li><li><strong>约束信息</strong>（例如目标和、去重规则）让我们能“确定动作”。</li></ul><p>图示：</p><p><img src="/images/20260219-%E5%8F%8C%E6%8C%87%E9%92%88%E6%95%99%E7%A8%8B/fig1.png" alt="双指针基本概念"></p><p><img src="/images/20260219-%E5%8F%8C%E6%8C%87%E9%92%88%E6%95%99%E7%A8%8B/fig2.png" alt="双指针常见类型"></p><h2 id="2-对撞指针：有序数组里的“夹逼”"><a href="#2-对撞指针：有序数组里的“夹逼”" class="headerlink" title="2. 对撞指针：有序数组里的“夹逼”"></a>2. 对撞指针：有序数组里的“夹逼”</h2><h3 id="2-1-机制直观"><a href="#2-1-机制直观" class="headerlink" title="2.1 机制直观"></a>2.1 机制直观</h3><p>左右指针从两端向中间移动。若当前和太大，右指针左移；太小，左指针右移；刚好命中就返回。</p><p>图示：</p><p><img src="/images/20260219-%E5%8F%8C%E6%8C%87%E9%92%88%E6%95%99%E7%A8%8B/fig3.png" alt="对撞指针流程图"></p><p><img src="/images/20260219-%E5%8F%8C%E6%8C%87%E9%92%88%E6%95%99%E7%A8%8B/gif1.gif" alt="对撞指针动画"></p><h3 id="2-2-具体-I-x2F-O-示例"><a href="#2-2-具体-I-x2F-O-示例" class="headerlink" title="2.2 具体 I/O 示例"></a>2.2 具体 I/O 示例</h3><p><strong>输入</strong>：</p><ul><li><code>nums = [2, 5, 8, 10, 15, 18, 22, 25]</code>（长度 <code>n=8</code>）</li><li><code>target = 27</code></li></ul><p><strong>过程</strong>：</p><ul><li><code>left=0 (2), right=7 (25) =&gt; sum=27</code>，命中。</li></ul><p><strong>输出</strong>：</p><ul><li><code>return (0, 7)</code> 或 <code>return [2, 25]</code>（依题目要求）</li></ul><h3 id="2-3-代码模板（Python）"><a href="#2-3-代码模板（Python）" class="headerlink" title="2.3 代码模板（Python）"></a>2.3 代码模板（Python）</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">two_sum_sorted</span>(<span class="params">nums, target</span>):</span><br><span class="line">    left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        s = nums[left] + nums[right]</span><br><span class="line">        <span class="keyword">if</span> s == target:</span><br><span class="line">            <span class="keyword">return</span> left, right</span><br><span class="line">        <span class="keyword">if</span> s &gt; target:</span><br><span class="line">            right -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">None</span></span><br></pre></td></tr></tbody></table></figure><h3 id="2-4-复杂度"><a href="#2-4-复杂度" class="headerlink" title="2.4 复杂度"></a>2.4 复杂度</h3><ul><li>时间：O(n)</li><li>空间：O(1)</li></ul><hr><h2 id="3-快慢指针：速度差制造“相遇信息”"><a href="#3-快慢指针：速度差制造“相遇信息”" class="headerlink" title="3. 快慢指针：速度差制造“相遇信息”"></a>3. 快慢指针：速度差制造“相遇信息”</h2><h3 id="3-1-机制直观"><a href="#3-1-机制直观" class="headerlink" title="3.1 机制直观"></a>3.1 机制直观</h3><p>慢指针每次走一步，快指针每次走两步。</p><ul><li>如果存在环，快慢一定相遇。</li><li>相遇后可推导出入环点。</li></ul><p>图示：</p><p><img src="/images/20260219-%E5%8F%8C%E6%8C%87%E9%92%88%E6%95%99%E7%A8%8B/fig4.png" alt="快慢指针示意"></p><p><img src="/images/20260219-%E5%8F%8C%E6%8C%87%E9%92%88%E6%95%99%E7%A8%8B/gif2.gif" alt="快慢指针动画"></p><h3 id="3-2-具体-I-x2F-O-示例（环检测）"><a href="#3-2-具体-I-x2F-O-示例（环检测）" class="headerlink" title="3.2 具体 I/O 示例（环检测）"></a>3.2 具体 I/O 示例（环检测）</h3><p><strong>输入</strong>：</p><ul><li>链表：<code>1 -&gt; 2 -&gt; 3 -&gt; 4 -&gt; 5 -&gt; 3 ...</code>（3 是入环点）</li></ul><p><strong>过程</strong>：</p><ul><li>slow: 1-&gt;2-&gt;3-&gt;4…</li><li>fast: 1-&gt;3-&gt;5-&gt;4…</li><li>相遇则说明有环。</li></ul><p><strong>输出</strong>：</p><ul><li><code>True</code>（存在环）</li></ul><h3 id="3-3-代码模板（Python）"><a href="#3-3-代码模板（Python）" class="headerlink" title="3.3 代码模板（Python）"></a>3.3 代码模板（Python）</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">has_cycle</span>(<span class="params">head</span>):</span><br><span class="line">    slow, fast = head, head</span><br><span class="line">    <span class="keyword">while</span> fast <span class="keyword">and</span> fast.<span class="built_in">next</span>:</span><br><span class="line">        slow = slow.<span class="built_in">next</span></span><br><span class="line">        fast = fast.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">if</span> slow == fast:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><h3 id="3-4-复杂度"><a href="#3-4-复杂度" class="headerlink" title="3.4 复杂度"></a>3.4 复杂度</h3><ul><li>时间：O(n)</li><li>空间：O(1)</li></ul><hr><h2 id="4-同向指针：稳定“过滤-写回”"><a href="#4-同向指针：稳定“过滤-写回”" class="headerlink" title="4. 同向指针：稳定“过滤 + 写回”"></a>4. 同向指针：稳定“过滤 + 写回”</h2><h3 id="4-1-机制直观"><a href="#4-1-机制直观" class="headerlink" title="4.1 机制直观"></a>4.1 机制直观</h3><p>两个指针同向移动：</p><ul><li><code>fast</code> 负责扫描</li><li><code>slow</code> 负责写入有效元素</li></ul><p>图示：</p><p><img src="/images/20260219-%E5%8F%8C%E6%8C%87%E9%92%88%E6%95%99%E7%A8%8B/fig5.png" alt="去重过程示意"></p><h3 id="4-2-具体-I-x2F-O-示例（去重）"><a href="#4-2-具体-I-x2F-O-示例（去重）" class="headerlink" title="4.2 具体 I/O 示例（去重）"></a>4.2 具体 I/O 示例（去重）</h3><p><strong>输入</strong>：</p><ul><li><code>nums = [1,1,2,2,3,4,4,5]</code>（长度 <code>8</code>）</li></ul><p><strong>过程</strong>：</p><ul><li><code>slow</code> 指向最后一个有效位置</li><li>遇到新值时，写入 <code>nums[slow+1]</code></li></ul><p><strong>输出</strong>：</p><ul><li>数组前 <code>k</code> 位为 <code>[1,2,3,4,5]</code>，<code>k=5</code></li></ul><h3 id="4-3-代码模板（Python）"><a href="#4-3-代码模板（Python）" class="headerlink" title="4.3 代码模板（Python）"></a>4.3 代码模板（Python）</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">remove_duplicates</span>(<span class="params">nums</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> nums:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    slow = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> fast <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">        <span class="keyword">if</span> nums[fast] != nums[slow]:</span><br><span class="line">            slow += <span class="number">1</span></span><br><span class="line">            nums[slow] = nums[fast]</span><br><span class="line">    <span class="keyword">return</span> slow + <span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><h3 id="4-4-复杂度"><a href="#4-4-复杂度" class="headerlink" title="4.4 复杂度"></a>4.4 复杂度</h3><ul><li>时间：O(n)</li><li>空间：O(1)</li></ul><hr><h2 id="5-训练式思维（迁移到“可学习”的直觉）"><a href="#5-训练式思维（迁移到“可学习”的直觉）" class="headerlink" title="5. 训练式思维（迁移到“可学习”的直觉）"></a>5. 训练式思维（迁移到“可学习”的直觉）</h2><p>虽然双指针是算法题，但你可以把它当作一种“闭环决策过程”：</p><ul><li><strong>输入</strong>：当前指针位置 + 局部值</li><li><strong>损失</strong>：偏离目标（如 sum-target 的误差）</li><li><strong>更新</strong>：移动指针（left++/right–/fast++）</li></ul><p>这种机制类似一个“有约束的贪心优化”。本质是用局部信息做出最有可能减少误差的动作。</p><hr><h2 id="6-小结"><a href="#6-小结" class="headerlink" title="6. 小结"></a>6. 小结</h2><ul><li>对撞指针：利用有序性，缩小区间</li><li>快慢指针：利用速度差，制造相遇</li><li>同向指针：扫描 + 写回，稳定过滤</li></ul><hr><h2 id="7-费曼总结"><a href="#7-费曼总结" class="headerlink" title="7. 费曼总结"></a>7. 费曼总结</h2><p>一句话：双指针不是技巧堆叠，而是“用两条视线把搜索空间压扁”。</p><ul><li><strong>先找结构</strong>：数组有序/值域单调/可过滤 ⇒ 才能决定指针移动方向。</li><li><strong>每步有理由</strong>：指针移动是“减少误差”的动作，不是拍脑袋。</li><li><strong>写回=压缩信息</strong>：同向指针通过覆盖写回，把“保留什么”变成可验证的过程。</li><li><strong>遇到新题先问</strong>：我能否用两个游标，把未知区域变成更小的已知区域？</li></ul>]]></content>
      
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>CLIP (Contrastive Language-Image Pre-Training) 深度解析：从原理到大模型应用</title>
      <link href="/2026/02/16/CLIP%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/"/>
      <url>/2026/02/16/CLIP%E6%B7%B1%E5%BA%A6%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<h1 id="CLIP-Contrastive-Language-Image-Pre-Training-深度解析"><a href="#CLIP-Contrastive-Language-Image-Pre-Training-深度解析" class="headerlink" title="CLIP (Contrastive Language-Image Pre-Training) 深度解析"></a>CLIP (Contrastive Language-Image Pre-Training) 深度解析</h1><blockquote><p><strong>一句话本质</strong>：CLIP 是连接文本与图像的”罗塞塔石碑”。它通过对比学习，强行把图像和文本拉到同一个向量空间，让计算机视觉终于”读懂了语言”。</p></blockquote><hr><h2 id="一、STAR-框架：CLIP-的诞生与革命"><a href="#一、STAR-框架：CLIP-的诞生与革命" class="headerlink" title="一、STAR 框架：CLIP 的诞生与革命"></a>一、STAR 框架：CLIP 的诞生与革命</h2><h3 id="1-Situation-背景与痛点"><a href="#1-Situation-背景与痛点" class="headerlink" title="1. Situation (背景与痛点)"></a>1. Situation (背景与痛点)</h3><p>在 CLIP (2021) 之前，计算机视觉（CV）和自然语言处理（NLP）是两个平行的世界：</p><ul><li><strong>CV 的困境</strong>：严重依赖人工标注（ImageNet 的 1000 个类），模型只能识别训练过的类别（Closed-Set），换个场景就要重新训练。</li><li><strong>NLP 的突破</strong>：GPT 系列证明了”从海量无标注文本中自监督学习”是通往通用的钥匙。</li></ul><p><strong>核心痛点</strong>：如何让 CV 模型像 GPT 一样，从互联网海量数据中学习，不再需要人工打标签？</p><h3 id="2-Task-任务目标"><a href="#2-Task-任务目标" class="headerlink" title="2. Task (任务目标)"></a>2. Task (任务目标)</h3><p>OpenAI 的目标很明确：</p><blockquote><p><strong>训练一个能直接通过”自然语言”指挥的视觉模型，实现 Zero-Shot（零样本）迁移。</strong></p></blockquote><p>即：不用微调，直接告诉模型”找一张猫的照片”，它就能从图库里找出猫。</p><h3 id="3-Action-核心方法"><a href="#3-Action-核心方法" class="headerlink" title="3. Action (核心方法)"></a>3. Action (核心方法)</h3><p>CLIP 摒弃了传统的”分类”范式，采用了 **对比学习 (Contrastive Learning)**：</p><ul><li><strong>数据</strong>：收集了 4 亿对 (图片, 文本) 数据 (WebImageText)。</li><li><strong>机制</strong>：训练一个图像编码器和一个文本编码器，让配对的图文向量<strong>相似度最大化</strong>，不配对的<strong>最小化</strong>。</li><li><strong>规模</strong>：使用了超大的 Batch Size (32,768)，这是成功的关键之一。</li></ul><h3 id="4-Result-结果与影响"><a href="#4-Result-结果与影响" class="headerlink" title="4. Result (结果与影响)"></a>4. Result (结果与影响)</h3><ul><li><strong>性能</strong>：在 ImageNet 上，Zero-Shot 的 CLIP 达到了 ResNet-50 的水平（76.2%），但鲁棒性远超后者。</li><li><strong>影响</strong>：它成为了 AI 绘画（Stable Diffusion）、多模态大模型（LLaVA、GPT-4V）的基石。没有 CLIP，就没有现在的 AIGC 热潮。</li></ul><hr><h2 id="二、深度原理：从-Tensor-流向到梯度回传"><a href="#二、深度原理：从-Tensor-流向到梯度回传" class="headerlink" title="二、深度原理：从 Tensor 流向到梯度回传"></a>二、深度原理：从 Tensor 流向到梯度回传</h2><h3 id="2-1-架构与数据流-The-Flow"><a href="#2-1-架构与数据流-The-Flow" class="headerlink" title="2.1 架构与数据流 (The Flow)"></a>2.1 架构与数据流 (The Flow)</h3><p>CLIP 是典型的**双塔架构 (Two-Tower Architecture)**。</p><p><img src="/images/clip/fig1_arch.png" alt="CLIP Architecture"><br><em>(图1：CLIP 完整架构与 Tensor 维度变化图)</em></p><h4 id="数据流拆解-以-ViT-B-x2F-32-为例"><a href="#数据流拆解-以-ViT-B-x2F-32-为例" class="headerlink" title="数据流拆解 (以 ViT-B/32 为例)"></a><strong>数据流拆解 (以 ViT-B/32 为例)</strong></h4><p>假设 Batch Size $N=4$：</p><ol><li><p><strong>Image Branch (左侧)</strong>:</p><ul><li>Input: <code>[4, 3, 224, 224]</code> (图片)</li><li>Patch Embedding: 切成 7x7=49 个 patch $<br>ightarrow$ <code>[4, 49, 768]</code></li><li>Transformer: 加上 <code>[CLS]</code> token $<br>ightarrow$ <code>[4, 50, 768]</code></li><li><strong>Output</strong>: 取 <code>[CLS]</code> token，投影到 512 维 $<br>ightarrow$ <strong><code>I_e [4, 512]</code></strong></li></ul></li><li><p><strong>Text Branch (右侧)</strong>:</p><ul><li>Input: <code>[4, 77]</code> (文本 Token ID)</li><li>Transformer: 经过 12 层处理 $<br>ightarrow$ <code>[4, 77, 512]</code></li><li><strong>Output</strong>: 取 <code>[EOS]</code> token，投影到 512 维 $<br>ightarrow$ <strong><code>T_e [4, 512]</code></strong></li></ul></li><li><p><strong>Interaction (交互)</strong>:</p><ul><li><strong>矩阵乘法</strong>: <code>Logits = I_e @ T_e.T</code> $<br>ightarrow$ <strong><code>[4, 4]</code></strong></li><li>这就得到了一个相似度矩阵！</li></ul></li></ol><hr><h3 id="2-2-核心机制：对比学习与-InfoNCE-Loss"><a href="#2-2-核心机制：对比学习与-InfoNCE-Loss" class="headerlink" title="2.2 核心机制：对比学习与 InfoNCE Loss"></a>2.2 核心机制：对比学习与 InfoNCE Loss</h3><p>CLIP 不做生成（不画图），也不做分类（不预测 Label），它只做<strong>判断题</strong>：</p><blockquote><p>“这张图和这段字，是不是一对？”</p></blockquote><p><img src="/images/clip/fig2_loss.png" alt="Contrastive Loss"><br><em>(图2：对比损失矩阵计算与梯度回传)</em></p><h4 id="InfoNCE-Loss-详解"><a href="#InfoNCE-Loss-详解" class="headerlink" title="InfoNCE Loss 详解"></a><strong>InfoNCE Loss 详解</strong></h4><p>对于 Batch 中的第 $i$ 张图，它与第 $i$ 段文本是正样本，与其他所有文本是负样本。</p><p>$$ L_i = -\log rac{xp(ext{sim}(I_i, T_i)/au)}{\sum_{j=1}^N xp(ext{sim}(I_i, T_j)/au)} $$</p><ul><li><strong>分子</strong>：正样本的相似度（我们要最大化它）。</li><li><strong>分母</strong>：所有样本的相似度总和（我们要通过最大化分子，间接压低分母中其他负样本的比重）。</li><li>**$au$ (Temperature)**：温度系数。$au$ 越小，分布越尖锐，模型越关注最难区分的负样本。CLIP 中 $au$ 是可学习的。</li></ul><h4 id="费曼直觉：为什么要用对比学习？"><a href="#费曼直觉：为什么要用对比学习？" class="headerlink" title="费曼直觉：为什么要用对比学习？"></a><strong>费曼直觉：为什么要用对比学习？</strong></h4><p>想象你在教小孩认动物：</p><ul><li>**生成式 (Generative)**：让小孩画一只猫。（太难了，还要学画画）</li><li>**分类式 (Classification)**：给小孩看图，让他背这是”类别ID 283”。（死记硬背，不懂含义）</li><li><strong>对比式 (Contrastive)<strong>：给小孩看一张猫图和”猫”字卡片，再看一张狗图和”车”字卡片，让他</strong>配对</strong>。（简单、高效，懂语义）</li></ul><hr><h2 id="三、Zero-Shot-机制：如何”听懂人话”？"><a href="#三、Zero-Shot-机制：如何”听懂人话”？" class="headerlink" title="三、Zero-Shot 机制：如何”听懂人话”？"></a>三、Zero-Shot 机制：如何”听懂人话”？</h2><p>这是 CLIP 最骚的操作。它把<strong>分类问题</strong>变成了<strong>检索问题</strong>。</p><p><img src="/images/clip/fig3_zeroshot.png" alt="Zero-Shot Mechanism"><br><em>(图3：Zero-Shot 推理与动态权重生成)</em></p><h3 id="3-1-动态分类器-Dynamic-Classifier"><a href="#3-1-动态分类器-Dynamic-Classifier" class="headerlink" title="3.1 动态分类器 (Dynamic Classifier)"></a>3.1 动态分类器 (Dynamic Classifier)</h3><p>传统的分类器，最后一层权重 $W$ 是固定的（比如 ImageNet 的 1000 类）。<br>CLIP 的权重是<strong>动态生成</strong>的：</p><ol><li>你给它一组类别词：<code>["dog", "cat", "plane"]</code>。</li><li>它把这些词变成向量：<code>[v_dog, v_cat, v_plane]</code>。</li><li>这三个向量，就构成了临时的分类器权重 $W’$！</li><li>图片向量 $I$ 与 $W’$ 做点积，谁大就是谁。</li></ol><h3 id="3-2-Prompt-Engineering-的起源"><a href="#3-2-Prompt-Engineering-的起源" class="headerlink" title="3.2 Prompt Engineering 的起源"></a>3.2 Prompt Engineering 的起源</h3><p>论文发现，直接用单词 <code>"dog"</code> 效果一般。如果改成 <code>"a photo of a dog"</code>，效果提升 1.3%。<br>如果用 <strong>Ensemble（集成）</strong>：</p><ul><li><code>"a photo of a big {label}"</code></li><li><code>"a drawing of a {label}"</code></li><li><code>"it is a {label}"</code><br>把 80 种句子的向量取平均，效果提升 3.5%！</li></ul><p>这告诉我们：<strong>多角度描述一个事物，特征更稳。</strong></p><hr><h2 id="四、大模型应用：CLIP-是-AI-的”视觉接口”"><a href="#四、大模型应用：CLIP-是-AI-的”视觉接口”" class="headerlink" title="四、大模型应用：CLIP 是 AI 的”视觉接口”"></a>四、大模型应用：CLIP 是 AI 的”视觉接口”</h2><p>CLIP 最大的贡献不是它自己，而是它<strong>成全了</strong>别人。</p><p><img src="/images/clip/fig4_apps.png" alt="Applications"><br><em>(图4：CLIP 在 Stable Diffusion 和 LLaVA 中的核心地位)</em></p><h3 id="4-1-Stable-Diffusion-AI-绘画"><a href="#4-1-Stable-Diffusion-AI-绘画" class="headerlink" title="4.1 Stable Diffusion (AI 绘画)"></a>4.1 Stable Diffusion (AI 绘画)</h3><ul><li><strong>角色</strong>：CLIP Text Encoder 是 SD 的”理解中枢”。</li><li><strong>流程</strong>：<ol><li>用户输入：”一只赛博朋克风格的猫”。</li><li><strong>CLIP Text Encoder</strong> 把这句话变成向量。</li><li>U-Net 根据这个向量，从噪声中”雕刻”出图像。</li></ol></li><li><strong>本质</strong>：SD 画得好，是因为 CLIP <strong>懂</strong>文本对应的视觉特征长什么样。</li></ul><h3 id="4-2-LLaVA-x2F-GPT-4V-多模态大模型"><a href="#4-2-LLaVA-x2F-GPT-4V-多模态大模型" class="headerlink" title="4.2 LLaVA / GPT-4V (多模态大模型)"></a>4.2 LLaVA / GPT-4V (多模态大模型)</h3><ul><li><strong>角色</strong>：CLIP Vision Encoder 是大模型的”眼睛”。</li><li><strong>流程</strong>：<ol><li>输入一张图。</li><li><strong>CLIP Image Encoder</strong> 提取图像特征。</li><li>通过一个投影层 (Projection)，把图像特征伪装成”词向量”。</li><li>LLM (如 Vicuna/Llama) 以为自己看到了文字，实际上是看到了图像特征。</li></ol></li><li><strong>本质</strong>：CLIP 把像素变成了 LLM 能读懂的语义。</li></ul><hr><h2 id="五、核心代码实现-PyTorch"><a href="#五、核心代码实现-PyTorch" class="headerlink" title="五、核心代码实现 (PyTorch)"></a>五、核心代码实现 (PyTorch)</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CLIP</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, embed_dim=<span class="number">512</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.visual = VisionTransformer() <span class="comment"># Image Encoder</span></span><br><span class="line">        self.text = TextTransformer()     <span class="comment"># Text Encoder</span></span><br><span class="line">        self.logit_scale = nn.Parameter(torch.ones([]) * np.log(<span class="number">1</span> / <span class="number">0.07</span>))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, image, text</span>):</span><br><span class="line">        <span class="comment"># 1. 提取特征</span></span><br><span class="line">        I_e = self.visual(image)  <span class="comment"># [N, 512]</span></span><br><span class="line">        T_e = self.text(text)     <span class="comment"># [N, 512]</span></span><br><span class="line"></span><br><span class="line">        <span class="comment"># 2. 归一化 (关键！否则点积无上界)</span></span><br><span class="line">        I_e = I_e / I_e.norm(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        T_e = T_e / T_e.norm(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 3. 计算相似度矩阵</span></span><br><span class="line">        <span class="comment"># exp(t) * (I @ T.T)</span></span><br><span class="line">        logit_scale = self.logit_scale.exp()</span><br><span class="line">        logits_per_image = logit_scale * I_e @ T_e.t()</span><br><span class="line">        logits_per_text = logits_per_image.t()</span><br><span class="line"></span><br><span class="line">        <span class="comment"># 4. 构造标签 (对角线是正样本)</span></span><br><span class="line">        labels = torch.arange(<span class="built_in">len</span>(image)).to(image.device)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 5. 计算双向 Loss</span></span><br><span class="line">        loss_i = nn.CrossEntropyLoss()(logits_per_image, labels)</span><br><span class="line">        loss_t = nn.CrossEntropyLoss()(logits_per_text, labels)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> (loss_i + loss_t) / <span class="number">2</span></span><br></pre></td></tr></tbody></table></figure><hr><h2 id="六、局限性与思考"><a href="#六、局限性与思考" class="headerlink" title="六、局限性与思考"></a>六、局限性与思考</h2><p>虽然 CLIP 很强，但它不是万能的：</p><ol><li><strong>不擅长细粒度任务</strong>：它很难区分”波音747”和”波音777”，或者数清图里有几只鸟。因为对比学习更关注整体语义匹配。</li><li><strong>OCR 能力弱</strong>：它能认出”Apple”这个Logo，但如果你手写一个”Sony”贴在苹果上，它可能会困惑。</li><li><strong>非生成式</strong>：CLIP 自己不能生成图像，它只能<strong>评价</strong>图像。它需要配合 GAN 或 Diffusion 才能搞创作。</li></ol><h2 id="七、总结"><a href="#七、总结" class="headerlink" title="七、总结"></a>七、总结</h2><p>CLIP 是 AI 历史上的一个转折点。它打破了 Vision 和 Language 的界限，用<strong>4亿对图文数据</strong>暴力美学地证明了：<strong>语言是理解视觉的最佳监督信号。</strong></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MoE 深度解析：Switch Transformer 与 Mixtral 的稀疏之道</title>
      <link href="/2026/02/08/MoE-Switch-Transformer-Mixtral-Deep-Dive/"/>
      <url>/2026/02/08/MoE-Switch-Transformer-Mixtral-Deep-Dive/</url>
      
        <content type="html"><![CDATA[<h1 id="MoE-深度解析：Switch-Transformer-与-Mixtral-的稀疏之道"><a href="#MoE-深度解析：Switch-Transformer-与-Mixtral-的稀疏之道" class="headerlink" title="MoE 深度解析：Switch Transformer 与 Mixtral 的稀疏之道"></a>MoE 深度解析：Switch Transformer 与 Mixtral 的稀疏之道</h1><blockquote><p>📅 <strong>创建时间</strong>：2026-02-08<br>🏷️ <strong>标签</strong>：#MoE #SwitchTransformer #Mixtral #稀疏计算 #大模型<br>📚 <strong>学习方法</strong>：费曼式讲解 + 数学推导 + 代码实现<br>📖 <strong>前置知识</strong>：Transformer FFN 层, Softmax, PyTorch 基础</p></blockquote><hr><h2 id="🎯-一句话理解-MoE"><a href="#🎯-一句话理解-MoE" class="headerlink" title="🎯 一句话理解 MoE"></a>🎯 一句话理解 MoE</h2><div class="feynman-box"><h4>🧠 费曼式理解</h4><p><strong>MoE 就像一家拥有 128 个专科医生的医院。</strong></p><p>普通医院：所有病人都找同一个全科医生（稠密 FFN）→ 医生累死，效率低下。</p><p>MoE 医院：前台护士（Router）快速判断病情，把心脏病人分给心脏专家，骨折病人分给骨科专家。每个病人只见 1-2 个专家，但医院总共有 128 个专家随时待命。</p><p><strong>结果</strong>：医院容量提升 128 倍，但每个病人的等待时间几乎不变！</p></div><hr><h2 id="📋-目录"><a href="#📋-目录" class="headerlink" title="📋 目录"></a>📋 目录</h2><ol><li><a href="#why-moe">为什么需要 MoE？</a></li><li><a href="#mechanism">核心机制：Router + Expert</a></li><li><a href="#tensor-flow">Tensor 维度变化全图解</a></li><li><a href="#training">Training Loop：梯度如何流动？</a></li><li><a href="#comparison">Switch vs Mixtral：关键差异</a></li><li><a href="#code">完整代码实现</a></li><li><a href="#quiz">费曼自测题</a></li></ol><hr><p><a id="why-moe"></a></p><h2 id="1-为什么需要-MoE？"><a href="#1-为什么需要-MoE？" class="headerlink" title="1. 为什么需要 MoE？"></a>1. 为什么需要 MoE？</h2><h3 id="1-1-稠密模型的困境"><a href="#1-1-稠密模型的困境" class="headerlink" title="1.1 稠密模型的困境"></a>1.1 稠密模型的困境</h3><p>传统 Transformer 的 FFN 层占据了 <strong>2/3 的参数量</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DenseFFN</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model=<span class="number">1024</span>, d_ff=<span class="number">4096</span></span>):</span><br><span class="line">        self.W1 = nn.Linear(d_model, d_ff)   <span class="comment"># [1024, 4096] = 4.2M 参数</span></span><br><span class="line">        self.W2 = nn.Linear(d_ff, d_model)   <span class="comment"># [4096, 1024] = 4.2M 参数</span></span><br><span class="line">        <span class="comment"># 总计 8.4M 参数，每个 token 都要过一遍</span></span><br></pre></td></tr></tbody></table></figure><p><strong>问题</strong>：</p><ul><li>GPT-3 (175B) 每次推理都激活 <strong>100% 参数</strong></li><li>推理成本 ∝ 参数量 → 参数越多，推理越慢</li><li><strong>扩展悖论</strong>：想要更聪明，就必须更慢？</li></ul><h3 id="1-2-MoE-的核心洞察"><a href="#1-2-MoE-的核心洞察" class="headerlink" title="1.2 MoE 的核心洞察"></a>1.2 MoE 的核心洞察</h3><div class="insight-box"><strong>💡 关键洞察</strong>：不同的 token 需要不同的"知识"来处理。<ul><li>"The capital of France is" → 需要<strong>地理知识</strong></li><li>"def quicksort(arr):" → 需要<strong>编程知识</strong></li><li>"I feel so happy" → 需要<strong>情感理解</strong></li></ul><p>为什么要让同一个 FFN 处理所有这些？让专家各司其职！</p></div><p><strong>MoE 的解决方案</strong>：</p><table><thead><tr><th>对比项</th><th>稠密 FFN</th><th>MoE (128 Experts)</th></tr></thead><tbody><tr><td>总参数量</td><td>8.4M</td><td>8.4M × 128 = <strong>1.075B</strong></td></tr><tr><td>每次激活参数</td><td>8.4M (100%)</td><td>8.4M (<strong>0.78%</strong>)</td></tr><tr><td>模型容量</td><td>1×</td><td><strong>128×</strong></td></tr><tr><td>推理成本</td><td>1×</td><td><strong>≈ 1×</strong></td></tr></tbody></table><p><strong>这就是 MoE 的魔法</strong>：参数量暴涨 128 倍，推理成本几乎不变！</p><hr><p><a id="mechanism"></a></p><h2 id="2-核心机制：Router-Expert"><a href="#2-核心机制：Router-Expert" class="headerlink" title="2. 核心机制：Router + Expert"></a>2. 核心机制：Router + Expert</h2><h3 id="2-1-架构总览"><a href="#2-1-架构总览" class="headerlink" title="2.1 架构总览"></a>2.1 架构总览</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">输入 Token [B, L, D]</span><br><span class="line">       ↓</span><br><span class="line">   ┌─────────┐</span><br><span class="line">   │ Router  │  ← 一个简单的 Linear 层</span><br><span class="line">   └────┬────┘</span><br><span class="line">        ↓ Softmax + Top-K</span><br><span class="line">   ┌────┴────┐</span><br><span class="line">   ↓    ↓    ↓</span><br><span class="line">┌───┐ ┌───┐ ┌───┐</span><br><span class="line">│E_0│ │E_1│ │...│ │E_127│  ← 128 个并行的 FFN</span><br><span class="line">└─┬─┘ └─┬─┘ └───┘</span><br><span class="line">  │     │</span><br><span class="line">  └──┬──┘</span><br><span class="line">     ↓ 加权求和</span><br><span class="line">输出 [B, L, D]</span><br></pre></td></tr></tbody></table></figure><h3 id="2-2-Router：交通调度员"><a href="#2-2-Router：交通调度员" class="headerlink" title="2.2 Router：交通调度员"></a>2.2 Router：交通调度员</h3><p>Router 就是一个 <strong>Linear 层</strong>，把每个 token 映射到 N 个 Expert 的概率分布：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Router 的全部代码</span></span><br><span class="line">self.router = nn.Linear(d_model, num_experts, bias=<span class="literal">False</span>)  <span class="comment"># [1024, 128]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 前向传播</span></span><br><span class="line">router_logits = self.router(x)           <span class="comment"># [B, L, 128]</span></span><br><span class="line">router_probs = F.softmax(router_logits, dim=-<span class="number">1</span>)  <span class="comment"># 概率分布</span></span><br></pre></td></tr></tbody></table></figure><p><strong>具体例子</strong>：</p><p>假设一个 token 的 embedding 是 <code>x = [0.1, 0.2, ..., 0.5]</code>（1024 维）</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Router 计算</span></span><br><span class="line">router_logits = W_router @ x  <span class="comment"># [128]</span></span><br><span class="line"><span class="comment"># 假设结果是 [2.1, 0.5, 3.2, 0.1, ..., 0.8]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Softmax 归一化</span></span><br><span class="line">router_probs = softmax(router_logits)</span><br><span class="line"><span class="comment"># [0.15, 0.03, 0.45, 0.01, ..., 0.02]</span></span><br><span class="line"><span class="comment">#   ↑                ↑</span></span><br><span class="line"><span class="comment"># Expert 0: 15%   Expert 2: 45% ← 选这个！</span></span><br></pre></td></tr></tbody></table></figure><h3 id="2-3-Top-K-选择"><a href="#2-3-Top-K-选择" class="headerlink" title="2.3 Top-K 选择"></a>2.3 Top-K 选择</h3><p>**Switch Transformer (Top-1)**：每个 token 只选 <strong>1 个</strong> Expert</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">topk_probs, topk_indices = torch.topk(router_probs, k=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># topk_indices = [2]  ← 选择 Expert 2</span></span><br><span class="line"><span class="comment"># topk_probs = [0.45] ← 权重 0.45</span></span><br></pre></td></tr></tbody></table></figure><p>**Mixtral (Top-2)**：每个 token 选 <strong>2 个</strong> Expert</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">topk_probs, topk_indices = torch.topk(router_probs, k=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># topk_indices = [2, 0]  ← 选择 Expert 2 和 Expert 0</span></span><br><span class="line"><span class="comment"># topk_probs = [0.45, 0.15] → 归一化 → [0.75, 0.25]</span></span><br></pre></td></tr></tbody></table></figure><h3 id="2-4-稀疏计算的实现"><a href="#2-4-稀疏计算的实现" class="headerlink" title="2.4 稀疏计算的实现"></a>2.4 稀疏计算的实现</h3><div class="warning-box"><strong>⚠️ 关键问题</strong>：128 个 Expert，每个 token 只用 1 个，怎么高效计算？</div><p><strong>朴素实现</strong>（低效）：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> expert_id <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">128</span>):</span><br><span class="line">    mask = (topk_indices == expert_id)</span><br><span class="line">    <span class="keyword">if</span> mask.<span class="built_in">any</span>():</span><br><span class="line">        output[mask] = experts[expert_id](x[mask])</span><br></pre></td></tr></tbody></table></figure><p><strong>优化实现</strong>（实际使用）：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 按 Expert ID 重排所有 token</span></span><br><span class="line">sorted_indices = topk_indices.argsort()</span><br><span class="line">sorted_x = x[sorted_indices]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 批量处理每个 Expert 的 token</span></span><br><span class="line">expert_outputs = []</span><br><span class="line"><span class="keyword">for</span> expert_id, expert <span class="keyword">in</span> <span class="built_in">enumerate</span>(experts):</span><br><span class="line">    start, end = expert_boundaries[expert_id]</span><br><span class="line">    <span class="keyword">if</span> start &lt; end:</span><br><span class="line">        expert_outputs.append(expert(sorted_x[start:end]))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 还原到原始顺序</span></span><br><span class="line">output = torch.cat(expert_outputs)[inverse_indices]</span><br></pre></td></tr></tbody></table></figure><hr><p><a id="tensor-flow"></a></p><h2 id="3-Tensor-维度变化全图解"><a href="#3-Tensor-维度变化全图解" class="headerlink" title="3. Tensor 维度变化全图解"></a>3. Tensor 维度变化全图解</h2><div class="feynman-box"><h4>🎨 可视化是建立直觉的最快方式</h4><p>让我们跟踪一个具体的例子，看 Tensor 如何流动。</p></div><h3 id="3-1-配置参数"><a href="#3-1-配置参数" class="headerlink" title="3.1 配置参数"></a>3.1 配置参数</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">batch_size = <span class="number">8</span></span><br><span class="line">seq_len = <span class="number">512</span></span><br><span class="line">d_model = <span class="number">1024</span></span><br><span class="line">num_experts = <span class="number">128</span></span><br><span class="line">d_ff = <span class="number">4096</span>  <span class="comment"># 每个 Expert 的 FFN 隐藏层</span></span><br><span class="line">top_k = <span class="number">1</span>    <span class="comment"># Switch Transformer</span></span><br></pre></td></tr></tbody></table></figure><h3 id="3-2-完整数据流"><a href="#3-2-完整数据流" class="headerlink" title="3.2 完整数据流"></a>3.2 完整数据流</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│ Step 0: Input                                               │</span><br><span class="line">│ x: [8, 512, 1024]                                          │</span><br><span class="line">│ 含义: 8 个样本，每个 512 个 token，每个 token 1024 维        │</span><br><span class="line">└───────────────────────────┬─────────────────────────────────┘</span><br><span class="line">                            ↓</span><br><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│ Step 1: Router Logits                                       │</span><br><span class="line">│ router_logits = W_r @ x                                     │</span><br><span class="line">│ W_r: [1024, 128]                                           │</span><br><span class="line">│ router_logits: [8, 512, 128]                               │</span><br><span class="line">│ 含义: 每个 token 对 128 个 Expert 的"打分"                  │</span><br><span class="line">└───────────────────────────┬─────────────────────────────────┘</span><br><span class="line">                            ↓ Softmax</span><br><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│ Step 2: Router Probabilities                                │</span><br><span class="line">│ router_probs = softmax(router_logits, dim=-1)              │</span><br><span class="line">│ router_probs: [8, 512, 128]                                │</span><br><span class="line">│ 含义: 每个 token 选择各 Expert 的概率                        │</span><br><span class="line">│ 每行和 = 1.0                                                │</span><br><span class="line">└───────────────────────────┬─────────────────────────────────┘</span><br><span class="line">                            ↓ Top-1</span><br><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│ Step 3: Top-K Selection                                     │</span><br><span class="line">│ topk_probs: [8, 512, 1]   ← 最大概率值                      │</span><br><span class="line">│ topk_indices: [8, 512, 1] ← Expert ID (0-127)              │</span><br><span class="line">│                                                             │</span><br><span class="line">│ 例: Token[0,0] → Expert 42, 权重 0.67                       │</span><br><span class="line">│     Token[0,1] → Expert 7,  权重 0.81                       │</span><br><span class="line">│     Token[0,2] → Expert 42, 权重 0.55  ← 同一个 Expert!     │</span><br><span class="line">└───────────────────────────┬─────────────────────────────────┘</span><br><span class="line">                            ↓ Dispatch</span><br><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│ Step 4: Expert Processing (稀疏!)                           │</span><br><span class="line">│                                                             │</span><br><span class="line">│ Expert 7:  收到 32 个 token  → [32, 1024]                   │</span><br><span class="line">│ Expert 42: 收到 48 个 token  → [48, 1024]  ← 负载不均!      │</span><br><span class="line">│ Expert 99: 收到 0 个 token   → 空闲                         │</span><br><span class="line">│ ...                                                         │</span><br><span class="line">│                                                             │</span><br><span class="line">│ 每个 Expert 内部:                                            │</span><br><span class="line">│ [N_i, 1024] → W1 → [N_i, 4096] → GELU → W2 → [N_i, 1024]   │</span><br><span class="line">└───────────────────────────┬─────────────────────────────────┘</span><br><span class="line">                            ↓ Weighted Combine</span><br><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│ Step 5: Output                                              │</span><br><span class="line">│ output[i] = topk_probs[i] × expert_output[i]               │</span><br><span class="line">│ output: [8, 512, 1024]                                     │</span><br><span class="line">│ 维度与输入完全一致!                                          │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><h3 id="3-3-可视化：Router-决策热力图"><a href="#3-3-可视化：Router-决策热力图" class="headerlink" title="3.3 可视化：Router 决策热力图"></a>3.3 可视化：Router 决策热力图</h3><p><img src="/images/moe/01_router_decision_load.png" alt="Router 决策与负载分布"><br><em>图 3.1: Router 决策热力图 (左)、Top-1 分配 (中)、负载分布 (右)</em></p><p><strong>解读</strong>：</p><ul><li><strong>左图</strong>：每列是一个 token 对 16 个 Expert 的概率（颜色越深 = 概率越高）</li><li><strong>中图</strong>：每个 token 最终选择的 Expert（绿色 = 被选中）</li><li><strong>右图</strong>：Expert 负载分布<ul><li>🔴 红色柱子 = 过载（如 Expert 2 处理 12 个 token）</li><li>🟠 橙色柱子 = 空闲（如 Expert 13 只处理 1 个）</li><li>🟢 绿色虚线 = 理想均衡值</li></ul></li></ul><p><strong>这就是负载不均衡问题的根源！</strong></p><hr><p><a id="training"></a></p><h2 id="4-Training-Loop：梯度如何流动？"><a href="#4-Training-Loop：梯度如何流动？" class="headerlink" title="4. Training Loop：梯度如何流动？"></a>4. Training Loop：梯度如何流动？</h2><h3 id="4-1-完整的-Loss-函数"><a href="#4-1-完整的-Loss-函数" class="headerlink" title="4.1 完整的 Loss 函数"></a>4.1 完整的 Loss 函数</h3>$$\mathcal{L}_{\text{total}} = \underbrace{\mathcal{L}_{\text{task}}}_{\text{Task Loss}} + \alpha \cdot \underbrace{\mathcal{L}_{\text{balance}}}_{\text{Load Balance}}$$<p>其中 $\alpha = 0.01$（Switch Transformer 推荐值）</p><h3 id="4-2-Load-Balance-Loss-推导"><a href="#4-2-Load-Balance-Loss-推导" class="headerlink" title="4.2 Load Balance Loss 推导"></a>4.2 Load Balance Loss 推导</h3><div class="feynman-box"><h4>🧠 费曼式理解 Load Balance Loss</h4><p>想象你是医院管理者，要让 128 个专家的工作量均衡：</p><ul><li><strong>f_i</strong> = Expert i 实际接诊的病人比例（频率）</li><li><strong>P_i</strong> = 护士给 Expert i 的平均推荐概率</li></ul><p>如果某个专家既<strong>实际接诊多</strong>（f 大），又<strong>被推荐概率高</strong>（P 大），说明系统在"偏袒"这个专家。惩罚它！</p></div><p><strong>数学定义</strong>：</p>$$f_i = \frac{1}{B \cdot L} \sum_{b,l} \mathbb{1}[\text{Top1}(x_{b,l}) = i]$$$$P_i = \frac{1}{B \cdot L} \sum_{b,l} \text{Router}(x_{b,l})_i$$$$\mathcal{L}_{\text{balance}} = N \cdot \sum_{i=1}^{N} f_i \cdot P_i$$<p><strong>具体计算例子</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 假设 4 个 Expert，8 个 token</span></span><br><span class="line">router_probs = torch.tensor([</span><br><span class="line">    [<span class="number">0.7</span>, <span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>],  <span class="comment"># Token 0 → Expert 0 (p=0.7)</span></span><br><span class="line">    [<span class="number">0.6</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">0.1</span>],  <span class="comment"># Token 1 → Expert 0 (p=0.6)</span></span><br><span class="line">    [<span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.1</span>, <span class="number">0.1</span>],  <span class="comment"># Token 2 → Expert 0 (p=0.5)</span></span><br><span class="line">    [<span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.1</span>, <span class="number">0.1</span>],  <span class="comment"># Token 3 → Expert 0 (p=0.4)</span></span><br><span class="line">    [<span class="number">0.1</span>, <span class="number">0.6</span>, <span class="number">0.2</span>, <span class="number">0.1</span>],  <span class="comment"># Token 4 → Expert 1</span></span><br><span class="line">    [<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.7</span>, <span class="number">0.1</span>],  <span class="comment"># Token 5 → Expert 2</span></span><br><span class="line">    [<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.7</span>],  <span class="comment"># Token 6 → Expert 3</span></span><br><span class="line">    [<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.7</span>],  <span class="comment"># Token 7 → Expert 3</span></span><br><span class="line">])</span><br><span class="line"></span><br><span class="line"><span class="comment"># Top-1 选择结果: [0, 0, 0, 0, 1, 2, 3, 3]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 f (频率)</span></span><br><span class="line">f = [<span class="number">4</span>/<span class="number">8</span>, <span class="number">1</span>/<span class="number">8</span>, <span class="number">1</span>/<span class="number">8</span>, <span class="number">2</span>/<span class="number">8</span>]  <span class="comment"># = [0.5, 0.125, 0.125, 0.25]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 P (平均概率)</span></span><br><span class="line">P = router_probs.mean(dim=<span class="number">0</span>)  <span class="comment"># = [0.325, 0.2375, 0.1625, 0.2375]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Load Balance Loss</span></span><br><span class="line">N = <span class="number">4</span></span><br><span class="line">L_balance = N * <span class="built_in">sum</span>(f[i] * P[i] <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>))</span><br><span class="line"><span class="comment"># = 4 * (0.5×0.325 + 0.125×0.2375 + 0.125×0.1625 + 0.25×0.2375)</span></span><br><span class="line"><span class="comment"># = 4 * 0.2719 = 1.0876</span></span><br></pre></td></tr></tbody></table></figure><p><strong>梯度分析</strong>：</p>$$\frac{\partial \mathcal{L}_{\text{balance}}}{\partial P_i} = N \cdot f_i$$<ul><li>Expert 0: $\nabla P_0 = 4 \times 0.5 = 2.0$ ← 梯度最大，会被惩罚！</li><li>Expert 1: $\nabla P_1 = 4 \times 0.125 = 0.5$ ← 梯度较小</li></ul><p>→ 训练会<strong>降低 Expert 0 的概率，提升其他 Expert 的概率</strong></p><h3 id="4-3-梯度流动图解"><a href="#4-3-梯度流动图解" class="headerlink" title="4.3 梯度流动图解"></a>4.3 梯度流动图解</h3><p><img src="/images/moe/03_gradient_flow.png" alt="梯度流动路径"><br><em>图 4.1: MoE 反向传播的完整梯度流动路径</em></p><p><strong>关键点</strong>：</p><ol><li><p><strong>Expert 参数的梯度</strong>：</p><ul><li>✅ 被选中的 Expert 收到梯度</li><li>❌ 未被选中的 Expert 梯度为 0</li></ul></li><li><p><strong>Router 参数收到两种梯度</strong>：</p><ul><li>🔵 来自主任务 Loss（通过 Gating Weight）</li><li>🟠 来自 Load Balance Loss（直接作用于 softmax）</li></ul></li><li><p><strong>梯度冲突</strong>：</p><ul><li>主任务想让 Router <strong>选择最好的 Expert</strong></li><li>LB Loss 想让 Router <strong>均匀分配</strong></li><li>$\alpha = 0.01$ 平衡这两个目标</li></ul></li></ol><h3 id="4-4-训练稳定性技巧"><a href="#4-4-训练稳定性技巧" class="headerlink" title="4.4 训练稳定性技巧"></a>4.4 训练稳定性技巧</h3><table><thead><tr><th>技巧</th><th>原因</th><th>代码</th></tr></thead><tbody><tr><td><strong>小初始化</strong></td><td>防止 Router 一开始就偏向某些 Expert</td><td><code>nn.init.normal_(router.weight, std=0.01)</code></td></tr><tr><td><strong>Capacity Factor</strong></td><td>限制每个 Expert 最多处理的 token 数</td><td><code>capacity = (B*L/N) * 1.25</code></td></tr><tr><td><strong>BF16 训练</strong></td><td>防止 softmax 上溢/下溢</td><td><code>model.to(torch.bfloat16)</code></td></tr><tr><td><strong>梯度裁剪</strong></td><td>防止梯度爆炸</td><td><code>clip_grad_norm_(params, 1.0)</code></td></tr></tbody></table><hr><p><a id="comparison"></a></p><h2 id="5-Switch-vs-Mixtral：关键差异"><a href="#5-Switch-vs-Mixtral：关键差异" class="headerlink" title="5. Switch vs Mixtral：关键差异"></a>5. Switch vs Mixtral：关键差异</h2><table><thead><tr><th>特性</th><th>Switch Transformer (2021)</th><th>Mixtral 8x7B (2023)</th></tr></thead><tbody><tr><td><strong>Top-K</strong></td><td>Top-1（极致稀疏）</td><td>Top-2（平衡性能）</td></tr><tr><td><strong>Expert 数量</strong></td><td>2048（极端多）</td><td>8（适合单机）</td></tr><tr><td><strong>每 token 计算量</strong></td><td>1 个 Expert</td><td>2 个 Expert</td></tr><tr><td><strong>负载均衡</strong></td><td>Auxiliary Loss</td><td>Token Choice + Expert Choice</td></tr><tr><td><strong>适用场景</strong></td><td>预训练超大模型</td><td>指令微调 + 推理部署</td></tr></tbody></table><p><img src="/images/moe/05_switch_vs_mixtral.png" alt="Switch vs Mixtral 架构对比"><br><em>图 5.1: Switch Transformer (Top-1) vs Mixtral (Top-2) 架构对比</em></p><h3 id="5-1-Mixtral-的-Top-2-优势"><a href="#5-1-Mixtral-的-Top-2-优势" class="headerlink" title="5.1 Mixtral 的 Top-2 优势"></a>5.1 Mixtral 的 Top-2 优势</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Top-2 路由</span></span><br><span class="line">topk_probs, topk_indices = torch.topk(router_probs, k=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># 归一化</span></span><br><span class="line">topk_probs = topk_probs / topk_probs.<span class="built_in">sum</span>(dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出 = w1 * Expert1(x) + w2 * Expert2(x)</span></span><br><span class="line">output = topk_probs[:, <span class="number">0</span>] * expert1_out + topk_probs[:, <span class="number">1</span>] * expert2_out</span><br></pre></td></tr></tbody></table></figure><p><strong>优势</strong>：</p><ul><li>更好的<strong>容错性</strong>（如果 Top-1 过载，Top-2 补充）</li><li>负载<strong>自然更均衡</strong></li><li>训练<strong>更稳定</strong></li></ul><p><strong>代价</strong>：</p><ul><li>推理成本 ×2（但仍远低于稠密模型：8 个 Expert 只激活 2 个 = 25%）</li></ul><hr><p><a id="code"></a></p><h2 id="6-完整代码实现"><a href="#6-完整代码实现" class="headerlink" title="6. 完整代码实现"></a>6. 完整代码实现</h2><h3 id="6-1-Switch-MoE-Layer"><a href="#6-1-Switch-MoE-Layer" class="headerlink" title="6.1 Switch MoE Layer"></a>6.1 Switch MoE Layer</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SwitchMoELayer</span>(nn.Module):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Switch Transformer 的 MoE Layer 实现</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        d_model: 输入/输出维度</span></span><br><span class="line"><span class="string">        num_experts: Expert 数量</span></span><br><span class="line"><span class="string">        d_ff: FFN 隐藏层维度</span></span><br><span class="line"><span class="string">        capacity_factor: 容量因子（默认 1.25）</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model=<span class="number">1024</span>, num_experts=<span class="number">128</span>, d_ff=<span class="number">4096</span>, </span></span><br><span class="line"><span class="params">                 capacity_factor=<span class="number">1.25</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.num_experts = num_experts</span><br><span class="line">        self.capacity_factor = capacity_factor</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Router: 简单的 Linear 层</span></span><br><span class="line">        self.router = nn.Linear(d_model, num_experts, bias=<span class="literal">False</span>)</span><br><span class="line">        nn.init.normal_(self.router.weight, std=<span class="number">0.01</span>)  <span class="comment"># 关键：小初始化</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Experts: N 个独立的 FFN</span></span><br><span class="line">        self.experts = nn.ModuleList([</span><br><span class="line">            nn.Sequential(</span><br><span class="line">                nn.Linear(d_model, d_ff),</span><br><span class="line">                nn.GELU(),</span><br><span class="line">                nn.Linear(d_ff, d_model)</span><br><span class="line">            ) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(num_experts)</span><br><span class="line">        ])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        B, L, D = x.shape</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 1: Router 计算</span></span><br><span class="line">        router_logits = self.router(x)  <span class="comment"># [B, L, N]</span></span><br><span class="line">        router_probs = F.softmax(router_logits, dim=-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 2: Top-1 选择</span></span><br><span class="line">        topk_probs, topk_indices = torch.topk(router_probs, k=<span class="number">1</span>, dim=-<span class="number">1</span>)</span><br><span class="line">        topk_probs = topk_probs.squeeze(-<span class="number">1</span>)      <span class="comment"># [B, L]</span></span><br><span class="line">        topk_indices = topk_indices.squeeze(-<span class="number">1</span>)  <span class="comment"># [B, L]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 3: 容量限制</span></span><br><span class="line">        capacity = <span class="built_in">int</span>((B * L / self.num_experts) * self.capacity_factor)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 4: Expert 计算</span></span><br><span class="line">        output = torch.zeros_like(x)</span><br><span class="line">        expert_counts = torch.zeros(self.num_experts, device=x.device)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> expert_id <span class="keyword">in</span> <span class="built_in">range</span>(self.num_experts):</span><br><span class="line">            mask = (topk_indices == expert_id)</span><br><span class="line">            num_tokens = mask.<span class="built_in">sum</span>().item()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> num_tokens == <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 容量限制：只选概率最高的 token</span></span><br><span class="line">            <span class="keyword">if</span> num_tokens &gt; capacity:</span><br><span class="line">                masked_probs = torch.where(</span><br><span class="line">                    mask, topk_probs, </span><br><span class="line">                    torch.tensor(-<span class="number">1e9</span>, device=x.device)</span><br><span class="line">                )</span><br><span class="line">                _, top_indices = torch.topk(masked_probs.flatten(), k=capacity)</span><br><span class="line">                new_mask = torch.zeros_like(mask.flatten(), dtype=torch.<span class="built_in">bool</span>)</span><br><span class="line">                new_mask[top_indices] = <span class="literal">True</span></span><br><span class="line">                mask = new_mask.view(B, L)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Expert 前向传播</span></span><br><span class="line">            selected_x = x[mask]</span><br><span class="line">            expert_out = self.experts[expert_id](selected_x)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 加权输出</span></span><br><span class="line">            weights = topk_probs[mask].unsqueeze(-<span class="number">1</span>)</span><br><span class="line">            output[mask] = expert_out * weights</span><br><span class="line">            expert_counts[expert_id] = mask.<span class="built_in">sum</span>().item()</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Step 5: 计算 Load Balance Loss</span></span><br><span class="line">        f = expert_counts / (B * L)</span><br><span class="line">        P = router_probs.mean(dim=[<span class="number">0</span>, <span class="number">1</span>])</span><br><span class="line">        load_balance_loss = self.num_experts * (f * P).<span class="built_in">sum</span>()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, load_balance_loss</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># ===== 测试代码 =====</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    moe = SwitchMoELayer(d_model=<span class="number">512</span>, num_experts=<span class="number">8</span>, d_ff=<span class="number">2048</span>)</span><br><span class="line">    x = torch.randn(<span class="number">2</span>, <span class="number">16</span>, <span class="number">512</span>)</span><br><span class="line">    </span><br><span class="line">    output, lb_loss = moe(x)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Input:  <span class="subst">{x.shape}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Output: <span class="subst">{output.shape}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"LB Loss: <span class="subst">{lb_loss.item():<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反向传播测试</span></span><br><span class="line">    total_loss = output.<span class="built_in">sum</span>() + <span class="number">0.01</span> * lb_loss</span><br><span class="line">    total_loss.backward()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Router grad norm: <span class="subst">{moe.router.weight.grad.norm():<span class="number">.4</span>f}</span>"</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="6-2-完整训练循环"><a href="#6-2-完整训练循环" class="headerlink" title="6.2 完整训练循环"></a>6.2 完整训练循环</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">model = SwitchMoELayer(d_model=<span class="number">512</span>, num_experts=<span class="number">16</span>, d_ff=<span class="number">2048</span>)</span><br><span class="line">optimizer = optim.AdamW(model.parameters(), lr=<span class="number">1e-4</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">10</span>):</span><br><span class="line">    x = torch.randn(<span class="number">8</span>, <span class="number">64</span>, <span class="number">512</span>)</span><br><span class="line">    target = torch.randn(<span class="number">8</span>, <span class="number">64</span>, <span class="number">512</span>)</span><br><span class="line">    </span><br><span class="line">    output, lb_loss = model(x)</span><br><span class="line">    main_loss = F.mse_loss(output, target)</span><br><span class="line">    total_loss = main_loss + <span class="number">0.01</span> * lb_loss</span><br><span class="line">    </span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    total_loss.backward()</span><br><span class="line">    torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">1.0</span>)</span><br><span class="line">    optimizer.step()</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Epoch <span class="subst">{epoch}</span>: Main=<span class="subst">{main_loss:<span class="number">.4</span>f}</span>, LB=<span class="subst">{lb_loss:<span class="number">.4</span>f}</span>"</span>)</span><br></pre></td></tr></tbody></table></figure><p><strong>输出示例</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Epoch 0: Main=1.0234, LB=1.4521</span><br><span class="line">Epoch 1: Main=0.9876, LB=1.3214</span><br><span class="line">...</span><br><span class="line">Epoch 9: Main=0.7821, LB=1.0234  ← LB Loss 下降，负载更均衡！</span><br></pre></td></tr></tbody></table></figure><hr><p><a id="quiz"></a></p><h2 id="7-费曼自测题"><a href="#7-费曼自测题" class="headerlink" title="7. 费曼自测题"></a>7. 费曼自测题</h2><div class="quiz-box"><h4>🧪 检验你是否真正理解了 MoE</h4><p><strong>Q1</strong>: 如果 MoE 有 128 个 Expert，每个 Expert 的 FFN 隐藏层是 4096，那么：</p><ul><li>总参数量是多少？</li><li>每次推理激活多少参数？（Top-1）</li></ul><details><summary>点击查看答案</summary><ul><li>总参数：$2 \times 1024 \times 4096 \times 128 = 1.07B$</li><li>激活参数：$2 \times 1024 \times 4096 = 8.4M$（仅 0.78%）</li></ul></details><hr><p><strong>Q2</strong>: Load Balance Loss 中，$f_i \cdot P_i$ 乘积的直觉含义是什么？</p><details><summary>点击查看答案</summary><ul><li>$f_i$ 是 Expert i 实际被选中的频率</li><li>$P_i$ 是 Router 给 Expert i 的平均概率</li><li><strong>乘积大</strong>说明这个 Expert 既被选中多，又被推荐概率高 → 系统在”偏袒”它</li><li>Loss 惩罚这种情况，迫使 Router 更均匀地分配</li></ul></details><hr><p><strong>Q3</strong>: 为什么 Mixtral 选择 Top-2 而不是 Top-1？</p><details><summary>点击查看答案</summary><ol><li><strong>容错性</strong>：如果 Top-1 Expert 过载，Top-2 补充</li><li><strong>训练稳定性</strong>：负载自然更均衡</li><li><strong>性能</strong>：两个 Expert 的知识互补</li></ol><p>代价：推理成本 ×2，但仍远低于稠密模型</p></details><hr><p><strong>Q4</strong>: 如果不使用 Load Balance Loss，会发生什么？</p><details><summary>点击查看答案</summary><ul><li>部分 Expert 会被大量选择（马太效应）</li><li>部分 Expert 几乎不被使用 → 参数浪费</li><li>梯度更新不均 → 某些 Expert 过度训练，某些几乎不更新</li><li>极端情况：模型退化为只用 1-2 个 Expert 的”伪 MoE”</li></ul></details></div><hr><h2 id="🔗-延伸阅读"><a href="#🔗-延伸阅读" class="headerlink" title="🔗 延伸阅读"></a>🔗 延伸阅读</h2><ol><li><p><strong>论文原文</strong>：</p><ul><li><a href="https://arxiv.org/abs/2101.03961">Switch Transformers (2021)</a></li><li><a href="https://arxiv.org/abs/2401.04088">Mixtral of Experts (2023)</a></li><li><a href="https://arxiv.org/abs/2006.16668">GShard (2020)</a> - 分布式 MoE</li></ul></li><li><p><strong>开源实现</strong>：</p><ul><li><a href="https://github.com/huggingface/transformers">Hugging Face Transformers</a></li><li><a href="https://github.com/facebookresearch/fairseq">Fairseq MoE</a></li></ul></li><li><p><strong>进阶话题</strong>：</p><ul><li>Expert Parallelism vs Tensor Parallelism</li><li>MoE + LoRA 稀疏微调</li><li>Dynamic Expert Selection</li></ul></li></ol><hr><h2 id="📊-总结"><a href="#📊-总结" class="headerlink" title="📊 总结"></a>📊 总结</h2><div class="feynman-box"><h4>🎯 一图总结 MoE</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">                 ┌─────────────────────────────────────┐</span><br><span class="line">                 │           MoE 核心公式              │</span><br><span class="line">                 │                                     │</span><br><span class="line">                 │  y = Σ Router(x)_i × Expert_i(x)   │</span><br><span class="line">                 │      i∈Top-K                       │</span><br><span class="line">                 └─────────────────────────────────────┘</span><br><span class="line">                                  ↓</span><br><span class="line">     ┌────────────────────────────┼────────────────────────────┐</span><br><span class="line">     │                            │                            │</span><br><span class="line">┌────┴────┐                 ┌─────┴─────┐                ┌─────┴─────┐</span><br><span class="line">│ Router  │                 │  Experts  │                │   Loss    │</span><br><span class="line">│ (调度)  │                 │  (执行)   │                │  (均衡)   │</span><br><span class="line">└────┬────┘                 └─────┬─────┘                └─────┬─────┘</span><br><span class="line">     │                            │                            │</span><br><span class="line">Softmax+TopK               并行 FFN 层                   f·P 惩罚项</span><br><span class="line">选择 K 个专家              稀疏激活                      防止偏袒</span><br></pre></td></tr></tbody></table></figure><p><strong>记住三个数字</strong>：</p><ul><li><strong>128×</strong> 参数量提升</li><li><strong>1%</strong> 激活率（Top-1 with 128 Experts）</li><li><strong>0.01</strong> Load Balance Loss 权重</li></ul></div><hr><p><em>Created: 2026-02-08 by Caius</em><br><em>Tags: #MoE #SwitchTransformer #Mixtral #DeepLearning #LLM</em></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>KL散度完全图解教程：从零到精通</title>
      <link href="/2026/02/08/KL%E6%95%A3%E5%BA%A6%E5%AE%8C%E5%85%A8%E5%9B%BE%E8%A7%A3%E6%95%99%E7%A8%8B/"/>
      <url>/2026/02/08/KL%E6%95%A3%E5%BA%A6%E5%AE%8C%E5%85%A8%E5%9B%BE%E8%A7%A3%E6%95%99%E7%A8%8B/</url>
      
        <content type="html"><![CDATA[<blockquote><p>🎯 <strong>学习目标</strong><br>假设你完全不懂信息论，本教程将带你从零开始，用最通俗的语言、最生动的例子、最直观的图示，彻底理解 KL 散度的本质。</p></blockquote><blockquote><p>📊 <strong>配套资源</strong>  </p><ul><li>5张高清可视化图表（Jensen不等式、KL非负性证明、前向vs反向KL对比等）</li><li>Python可视化代码</li><li>练习题及参考答案</li></ul></blockquote><span id="more"></span><hr><h2 id="📑-目录"><a href="#📑-目录" class="headerlink" title="📑 目录"></a>📑 目录</h2><ol><li><a href="#%E7%AC%AC%E4%B8%80%E7%AB%A0%E4%BF%A1%E6%81%AF%E7%9A%84%E4%BB%B7%E5%80%BC">第一章：信息的价值 - 为什么需要信息论？</a></li><li><a href="#%E7%AC%AC%E4%BA%8C%E7%AB%A0%E4%BF%A1%E6%81%AF%E9%87%8F">第二章：信息量 - 惊讶度的数学</a></li><li><a href="#%E7%AC%AC%E4%B8%89%E7%AB%A0%E6%A6%82%E7%8E%87%E5%88%86%E5%B8%83">第三章：概率分布 - 世界的规律</a></li><li><a href="#%E7%AC%AC%E5%9B%9B%E7%AB%A0%E7%86%B5">第四章：熵 - 平均惊讶程度</a></li><li><a href="#%E7%AC%AC%E4%BA%94%E7%AB%A0%E4%BA%A4%E5%8F%89%E7%86%B5">第五章：交叉熵 - 用错编码的代价</a></li><li><a href="#%E7%AC%AC%E5%85%AD%E7%AB%A0kl%E6%95%A3%E5%BA%A6">第六章：KL散度 - 测量分布的差异</a><ul><li>6.4.1 Jensen 不等式科普</li><li>6.4.2 KL 散度非负性的详细证明</li><li>6.4.3 前向 KL vs 反向 KL：深度解析</li></ul></li><li><a href="#%E7%AC%AC%E4%B8%83%E7%AB%A0%E5%AE%9E%E6%88%98%E5%BA%94%E7%94%A8">第七章：实战应用 - 从 RLHF 到 VAE</a></li><li><a href="#%E7%AC%AC%E5%85%AB%E7%AB%A0%E6%80%BB%E7%BB%93">第八章：总结与记忆技巧</a></li></ol><hr><h2 id="第一章：信息的价值"><a href="#第一章：信息的价值" class="headerlink" title="第一章：信息的价值"></a>第一章：信息的价值</h2><h3 id="1-1-没有信息的世界"><a href="#1-1-没有信息的世界" class="headerlink" title="1.1 没有信息的世界"></a>1.1 没有信息的世界</h3><p>想象一下，你生活在一个完全没有信息的世界：</p><ul><li>明天会下雨吗？🤷 <strong>不知道</strong></li><li>股票会涨吗？🤷 <strong>不知道</strong>  </li><li>考试会考什么？🤷 <strong>不知道</strong></li></ul><p>这是一个充满<strong>不确定性</strong>的世界。你无法做任何有效的决策，因为你对未来一无所知。</p><p><img src="/images/01_why_information.png" alt="信息的价值"></p><h3 id="1-2-信息的本质"><a href="#1-2-信息的本质" class="headerlink" title="1.2 信息的本质"></a>1.2 信息的本质</h3><p><strong>信息 = 消除不确定性的工具</strong></p><p>当你知道：</p><ul><li>明天 90% 会下雨 → 你带伞</li><li>股票 60% 会涨 → 你考虑投资</li><li>考试第3章占 70% → 你重点复习</li></ul><p><strong>核心理解：</strong> 信息让世界从”不知道”变成”知道”，从”混乱”变成”有序”。</p><hr><h2 id="第二章：信息量"><a href="#第二章：信息量" class="headerlink" title="第二章：信息量"></a>第二章：信息量</h2><h3 id="2-1-直觉：什么事情信息量大？"><a href="#2-1-直觉：什么事情信息量大？" class="headerlink" title="2.1 直觉：什么事情信息量大？"></a>2.1 直觉：什么事情信息量大？</h3><p>请看下面三个事件：</p><table><thead><tr><th>事件</th><th>发生概率</th><th>你的反应</th><th>信息量</th></tr></thead><tbody><tr><td>太阳从东边升起</td><td>99.99%</td><td>😐 理所当然</td><td><strong>≈ 0</strong></td></tr><tr><td>今天是周一到周日</td><td>14.3%</td><td>🙂 正常</td><td><strong>中等</strong></td></tr><tr><td>中了彩票头奖</td><td>0.0001%</td><td>😱 惊呆了！</td><td><strong>巨大！</strong></td></tr></tbody></table><p><img src="/images/02_surprise.png" alt="惊讶度"></p><p><strong>发现规律了吗？</strong></p><p>$$<br>\boxed{\text{概率越低} \rightarrow \text{越惊讶} \rightarrow \text{信息量越大}}<br>$$</p><h3 id="2-2-信息量的公式"><a href="#2-2-信息量的公式" class="headerlink" title="2.2 信息量的公式"></a>2.2 信息量的公式</h3><p>我们需要一个公式来量化”惊讶程度”。这个公式必须满足：</p><ol><li><p><strong>概率高 → 信息量低</strong><br>例如：$P = 100%$ 时，$I = 0$（完全不惊讶）</p></li><li><p><strong>概率低 → 信息量高</strong><br>例如：$P = 1%$ 时，$I$ 很大（非常惊讶）</p></li><li><p><strong>独立事件信息量可以相加</strong><br>例如：$I(A \text{且} B) = I(A) + I(B)$</p></li></ol><p><img src="/images/03_formula_birth.png" alt="公式诞生"></p><p><strong>唯一满足这些要求的函数就是对数：</strong></p><p>$$<br>\boxed{I(x) = -\log_2 P(x)}<br>$$</p><h3 id="2-3-变量详解"><a href="#2-3-变量详解" class="headerlink" title="2.3 变量详解"></a>2.3 变量详解</h3><p>让我们逐个击破每个符号：</p><table><thead><tr><th>符号</th><th>读作</th><th>含义</th><th>例子</th></tr></thead><tbody><tr><td>$x$</td><td>“某个事件”</td><td>一个具体的结果</td><td>“下雨”、”掷骰子得到6”</td></tr><tr><td>$P(x)$</td><td>“x 的概率”</td><td>这个事件发生的可能性</td><td>$P(\text{下雨}) = 0.3$</td></tr><tr><td>$I(x)$</td><td>“x 的信息量”</td><td>知道这件事后获得的信息</td><td>$I(\text{下雨}) = -\log_2(0.3) = 1.74$ bits</td></tr><tr><td>$\log_2$</td><td>“以2为底的对数”</td><td>用二进制表示需要多少位</td><td>$\log_2(8) = 3$（8需要3个二进制位）</td></tr></tbody></table><p><strong>为什么用对数？</strong></p><ol><li><p><strong>把乘法变加法</strong><br>$\log(A \times B) = \log(A) + \log(B)$</p></li><li><p><strong>把小数变大数</strong><br>概率是 0 到 1 之间的小数，对数把它变成正数<br>例如：$\log_2(0.01) = -6.64$，加个负号变成 $6.64$ bits</p></li></ol><p><strong>实际计算例子：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment"># 事件：太阳从东边升起</span></span><br><span class="line">P_sun = <span class="number">0.9999</span></span><br><span class="line">I_sun = -math.log2(P_sun)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"信息量：<span class="subst">{I_sun:<span class="number">.4</span>f}</span> bits"</span>)  <span class="comment"># ≈ 0.0001 bits（几乎没有信息）</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 事件：中彩票</span></span><br><span class="line">P_lottery = <span class="number">0.000001</span></span><br><span class="line">I_lottery = -math.log2(P_lottery)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"信息量：<span class="subst">{I_lottery:<span class="number">.2</span>f}</span> bits"</span>)  <span class="comment"># ≈ 19.93 bits（信息量巨大！）</span></span><br></pre></td></tr></tbody></table></figure><hr><h2 id="第三章：概率分布"><a href="#第三章：概率分布" class="headerlink" title="第三章：概率分布"></a>第三章：概率分布</h2><h3 id="3-1-什么是分布？"><a href="#3-1-什么是分布？" class="headerlink" title="3.1 什么是分布？"></a>3.1 什么是分布？</h3><p><strong>分布 = 所有可能结果的概率清单</strong></p><p>想象掷骰子：</p><table><thead><tr><th>结果</th><th>概率</th></tr></thead><tbody><tr><td>1点</td><td>1/6</td></tr><tr><td>2点</td><td>1/6</td></tr><tr><td>3点</td><td>1/6</td></tr><tr><td>4点</td><td>1/6</td></tr><tr><td>5点</td><td>1/6</td></tr><tr><td>6点</td><td>1/6</td></tr></tbody></table><p>这就是一个<strong>均匀分布</strong>（每个结果概率相同）。</p><p><img src="/images/04_distribution.png" alt="分布"></p><h3 id="3-2-真实世界-vs-我的猜测"><a href="#3-2-真实世界-vs-我的猜测" class="headerlink" title="3.2 真实世界 vs 我的猜测"></a>3.2 真实世界 vs 我的猜测</h3><p>在机器学习中，我们关心两个分布：</p><table><thead><tr><th>符号</th><th>名称</th><th>含义</th><th>例子</th></tr></thead><tbody><tr><td>$P(x)$</td><td><strong>真实分布</strong></td><td>世界的真相</td><td>老师出题的真实规律</td></tr><tr><td>$Q(x)$</td><td><strong>预测分布</strong></td><td>我的模型</td><td>我猜测的重点章节</td></tr></tbody></table><p><img src="/images/05_two_distributions.png" alt="两个分布"></p><p><strong>问题来了：</strong><br>如何衡量 $Q$（我的猜测）和 $P$（真相）有多不一样？</p><p>👉 <strong>这就是 KL 散度要解决的问题！</strong></p><hr><h2 id="第四章：熵"><a href="#第四章：熵" class="headerlink" title="第四章：熵"></a>第四章：熵</h2><h3 id="4-1-熵的直觉"><a href="#4-1-熵的直觉" class="headerlink" title="4.1 熵的直觉"></a>4.1 熵的直觉</h3><p><strong>熵 = 平均惊讶程度 = 平均信息量</strong></p><p>看两个地区的天气：</p><p><strong>沙漠地区（低熵）：</strong></p><ul><li>晴天：90%</li><li>雨天：8%</li><li>雪天：2%</li></ul><p>→ 你几乎确定是晴天，很少惊讶 → <strong>熵低</strong></p><p><strong>山区（高熵）：</strong></p><ul><li>晴天：35%</li><li>雨天：33%</li><li>雪天：32%</li></ul><p>→ 你完全不知道会怎样，经常惊讶 → <strong>熵高</strong></p><p><img src="/images/06_entropy.png" alt="熵"></p><h3 id="4-2-熵的公式"><a href="#4-2-熵的公式" class="headerlink" title="4.2 熵的公式"></a>4.2 熵的公式</h3><p>$$<br>\boxed{H(P) = -\sum_{x} P(x) \log_2 P(x)}<br>$$</p><p><strong>逐项解读：</strong></p><table><thead><tr><th>部分</th><th>含义</th></tr></thead><tbody><tr><td>$P(x)$</td><td>事件 $x$ 的真实概率</td></tr><tr><td>$\log_2 P(x)$</td><td>事件 $x$ 的信息量（的负值）</td></tr><tr><td>$P(x) \cdot \log_2 P(x)$</td><td>用概率加权</td></tr><tr><td>$\sum$</td><td>对所有可能的 $x$ 求和</td></tr><tr><td>$-$</td><td>加个负号让结果为正</td></tr></tbody></table><p><strong>计算例子（沙漠天气）：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">P = [<span class="number">0.9</span>, <span class="number">0.08</span>, <span class="number">0.02</span>]  <span class="comment"># 晴、雨、雪</span></span><br><span class="line"></span><br><span class="line">H = -<span class="built_in">sum</span>([p * np.log2(p) <span class="keyword">if</span> p &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> p <span class="keyword">in</span> P])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"熵 = <span class="subst">{H:<span class="number">.2</span>f}</span> bits"</span>)  <span class="comment"># ≈ 0.64 bits（低熵）</span></span><br></pre></td></tr></tbody></table></figure><p><strong>计算例子（山区天气）：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">P = [<span class="number">0.35</span>, <span class="number">0.33</span>, <span class="number">0.32</span>]</span><br><span class="line"></span><br><span class="line">H = -<span class="built_in">sum</span>([p * np.log2(p) <span class="keyword">if</span> p &gt; <span class="number">0</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">for</span> p <span class="keyword">in</span> P])</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"熵 = <span class="subst">{H:<span class="number">.2</span>f}</span> bits"</span>)  <span class="comment"># ≈ 1.58 bits（高熵）</span></span><br></pre></td></tr></tbody></table></figure><hr><h2 id="第五章：交叉熵"><a href="#第五章：交叉熵" class="headerlink" title="第五章：交叉熵"></a>第五章：交叉熵</h2><h3 id="5-1-摩斯密码的故事"><a href="#5-1-摩斯密码的故事" class="headerlink" title="5.1 摩斯密码的故事"></a>5.1 摩斯密码的故事</h3><p>想象你要用摩斯密码发电报。为了省电，你要设计一套编码：</p><ul><li><strong>高频字母（如 E, T）</strong> → 用短代码（•, ━）</li><li><strong>低频字母（如 Z, Q）</strong> → 用长代码（•━•━）</li></ul><p><strong>场景A：编码完美匹配</strong><br>如果你的编码完全符合英文字母的真实频率 $P$，你的电报最短。</p><p><strong>场景B：编码错误</strong><br>如果你拿着中文的编码本 $Q$ 去发英文电报 $P$，会发生什么？</p><p>你可能给 “E”（英文最常见）分配了超长代码，因为中文里没有 “E” 这个音。</p><p><strong>结果：</strong> 每条消息都比理论最优长度多出一大截！</p><p><img src="/images/07_cross_entropy.png" alt="交叉熵"></p><h3 id="5-2-交叉熵公式"><a href="#5-2-交叉熵公式" class="headerlink" title="5.2 交叉熵公式"></a>5.2 交叉熵公式</h3><p>$$<br>\boxed{H(P, Q) = -\sum_{x} P(x) \log_2 Q(x)}<br>$$</p><p><strong>关键区别：</strong></p><table><thead><tr><th>公式</th><th>概率来源</th><th>信息量来源</th><th>含义</th></tr></thead><tbody><tr><td>$H(P)$</td><td>$P(x)$</td><td>$\log P(x)$</td><td>用 $P$ 的编码传输 $P$ 的信息</td></tr><tr><td>$H(P,Q)$</td><td>$P(x)$</td><td>$\log Q(x)$</td><td>用 $Q$ 的编码传输 $P$ 的信息</td></tr></tbody></table><p><strong>直观理解：</strong></p><ul><li>真实世界按照 $P$ 分布产生数据</li><li>但你用 $Q$ 的编码本来编码</li><li>$H(P,Q)$ = 平均每条消息需要多少 bits</li></ul><p><strong>重要性质：</strong></p><p>$$<br>H(P, Q) \geq H(P)<br>$$</p><p>交叉熵永远 ≥ 熵！（用错误编码一定更浪费）</p><hr><h2 id="第六章：KL散度"><a href="#第六章：KL散度" class="headerlink" title="第六章：KL散度"></a>第六章：KL散度</h2><h3 id="6-1-KL散度的定义"><a href="#6-1-KL散度的定义" class="headerlink" title="6.1 KL散度的定义"></a>6.1 KL散度的定义</h3><p>KL散度 = 交叉熵 - 熵 = <strong>额外浪费的信息量</strong></p><p>$$<br>\boxed{D_{KL}(P | Q) = H(P, Q) - H(P)}<br>$$</p><p>展开后：</p><p>$$<br>\boxed{D_{KL}(P | Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}}<br>$$</p><p><img src="/images/08_kl_formula.png" alt="KL公式"></p><h3 id="6-2-公式深度解析"><a href="#6-2-公式深度解析" class="headerlink" title="6.2 公式深度解析"></a>6.2 公式深度解析</h3><p>让我们把公式拆成最小的零件：</p><p>$$<br>D_{KL}(P | Q) = \sum_{x} P(x) \cdot \log \frac{P(x)}{Q(x)}<br>$$</p><p><strong>第1部分：$x$</strong></p><ul><li>含义：所有可能的事件/结果</li><li>例子：天气里的”晴”、”雨”、”雪”</li></ul><p><strong>第2部分：$P(x)$</strong></p><ul><li>含义：真实世界中 $x$ 发生的概率</li><li>例子：$P(\text{晴}) = 0.6$（真实情况60%是晴天）</li></ul><p><strong>第3部分：$Q(x)$</strong></p><ul><li>含义：你的模型认为 $x$ 发生的概率</li><li>例子：$Q(\text{晴}) = 0.2$（你却认为只有20%是晴天）</li></ul><p><strong>第4部分：$\frac{P(x)}{Q(x)}$</strong></p><ul><li>含义：真实 vs 预测的比值</li><li>当 $P$ 大但 $Q$ 小时，这个比值<strong>爆炸性增长</strong>！</li><li>例子：$\frac{0.6}{0.2} = 3$（你低估了3倍）</li></ul><p><strong>第5部分：$\log \frac{P(x)}{Q(x)}$</strong></p><ul><li>含义：把比值转成信息量（bits）</li><li>例子：$\log_2(3) = 1.58$ bits</li></ul><p><strong>第6部分：$P(x) \cdot \log \frac{P(x)}{Q(x)}$</strong></p><ul><li>含义：用真实概率加权（重要！）</li><li><strong>只有在真相经常发生的地方犯错，才会被严重惩罚</strong></li><li>例子：$0.6 \times 1.58 = 0.95$</li></ul><p><strong>第7部分：$\sum$</strong></p><ul><li>含义：对所有可能的 $x$ 求和，得到平均浪费</li></ul><h3 id="6-3-完整计算实例"><a href="#6-3-完整计算实例" class="headerlink" title="6.3 完整计算实例"></a>6.3 完整计算实例</h3><p><strong>场景：天气预报</strong></p><table><thead><tr><th>天气</th><th>$P$ (真实)</th><th>$Q$ (预测)</th><th>$P/Q$</th><th>$\log_2(P/Q)$</th><th>$P \cdot \log(P/Q)$</th></tr></thead><tbody><tr><td>☀️晴</td><td>0.6</td><td>0.2</td><td>3.00</td><td>1.58</td><td><strong>0.950</strong></td></tr><tr><td>🌧️雨</td><td>0.3</td><td>0.5</td><td>0.60</td><td>-0.74</td><td>-0.222</td></tr><tr><td>☁️阴</td><td>0.1</td><td>0.3</td><td>0.33</td><td>-1.58</td><td>-0.158</td></tr><tr><td><strong>总和</strong></td><td></td><td></td><td></td><td></td><td><strong>0.570 bits</strong></td></tr></tbody></table><p><img src="/images/09_kl_calculation.png" alt="KL计算"></p><p><strong>结果解读：</strong></p><ol><li><p><strong>KL散度 = 0.570 bits</strong><br>意味着：用你的预测 $Q$ 代替真实 $P$，平均每次多浪费 0.57 bits</p></li><li><p><strong>最大贡献来自”晴天”</strong><br>真实60%但你只预测20%，差距太大！</p></li><li><p><strong>“雨天”贡献是负的</strong><br>你高估了雨天（预测50%实际30%），在这个方向上反而”赚了”信息<br>但总体还是亏的（因为晴天亏太多）</p></li></ol><h3 id="6-4-KL散度的性质"><a href="#6-4-KL散度的性质" class="headerlink" title="6.4 KL散度的性质"></a>6.4 KL散度的性质</h3><p><strong>性质1：非负性</strong></p><p>$$<br>D_{KL}(P | Q) \geq 0<br>$$</p><p>永远 ≥ 0，当且仅当 $P = Q$ 时等于 0。</p><h4 id="6-4-1-Jensen-不等式科普"><a href="#6-4-1-Jensen-不等式科普" class="headerlink" title="6.4.1 Jensen 不等式科普"></a>6.4.1 Jensen 不等式科普</h4><p>要证明 KL 散度非负，我们需要先理解 <strong>Jensen 不等式</strong>。</p><p><strong>什么是凹函数和凸函数？</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">凹函数（如 log x）：                  凸函数（如 x²）：</span><br><span class="line">        ___                                    /\</span><br><span class="line">      /     \                                 /  \</span><br><span class="line">     /       \                               /    \</span><br><span class="line">    /         \                             /      \</span><br><span class="line">   /           \                          _/        \_</span><br><span class="line">  </span><br><span class="line">  弦在曲线下方                            弦在曲线上方</span><br></pre></td></tr></tbody></table></figure><p><strong>判断方法</strong>：在曲线上任取两点，连一条线（弦）</p><ul><li>弦在曲线<strong>下方</strong> → 凹函数</li><li>弦在曲线<strong>上方</strong> → 凸函数</li></ul><p><strong>Jensen 不等式的内容</strong></p><p>对于<strong>凹函数</strong> $f$（比如 $\log$）：</p><p>$$<br>f\left(\sum_i p_i x_i\right) \geq \sum_i p_i f(x_i)<br>$$</p><p>其中 $p_i \geq 0$ 且 $\sum_i p_i = 1$（概率权重）</p><p><strong>通俗解释</strong>：</p><blockquote><p>“先平均再变换” ≥ “先变换再平均”</p></blockquote><p><img src="/images/jensen_inequality.png" alt="Jensen不等式几何解释"></p><p><strong>用图来理解</strong></p><p>假设只有两个点 $x_1$ 和 $x_2$，权重各 0.5：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">y</span><br><span class="line">│</span><br><span class="line">│      A ●───────────────● B      ← f(x₁) 和 f(x₂)</span><br><span class="line">│       ╲ ．．．．．．．．╱</span><br><span class="line">│        ╲    ● M'       ╱         ← 弦的中点（先变换再平均）</span><br><span class="line">│         ╲    ↓        ╱</span><br><span class="line">│          ╲   ↓       ╱</span><br><span class="line">│           ╲  ● M    ╱            ← 曲线上的点（先平均再变换）</span><br><span class="line">│            ╲       ╱</span><br><span class="line">│             ╲_____╱   ← 凹函数曲线</span><br><span class="line">│</span><br><span class="line">└──────────────────────────── x</span><br><span class="line">          x₁    中点    x₂</span><br></pre></td></tr></tbody></table></figure><ul><li><strong>M</strong> = $f\left(\frac{x_1+x_2}{2}\right)$（先平均再变换）</li><li><strong>M’</strong> = $\frac{f(x_1)+f(x_2)}{2}$（先变换再平均）</li></ul><p>因为曲线是凹的，所以 <strong>M 在 M’ 上方</strong>，即 $f(\text{平均}) \geq \text{平均}(f)$</p><h4 id="6-4-2-KL-散度非负性的详细证明"><a href="#6-4-2-KL-散度非负性的详细证明" class="headerlink" title="6.4.2 KL 散度非负性的详细证明"></a>6.4.2 KL 散度非负性的详细证明</h4><p><img src="/images/kl_nonnegative_proof.png" alt="KL散度非负性证明"></p><p><strong>目标：证明 $D_{KL}(P|Q) \geq 0$</strong></p><p><strong>第一步：写出定义</strong></p><p>$$<br>D_{KL}(P|Q) = \sum_x P(x) \log \frac{P(x)}{Q(x)}<br>$$</p><p><strong>第二步：变形</strong></p><p>$$<br>= -\sum_x P(x) \log \frac{Q(x)}{P(x)}<br>$$</p><p>令 $r(x) = \frac{Q(x)}{P(x)}$，则：</p><p>$$<br>D_{KL}(P|Q) = -\sum_x P(x) \log r(x) = -\mathbb{E}_P[\log r(x)]<br>$$</p><p><strong>第三步：应用 Jensen 不等式</strong></p><p>因为 $\log$ 是<strong>凹函数</strong>，根据 Jensen 不等式：</p><p>$$<br>\sum_x P(x) \log r(x) \leq \log \left(\sum_x P(x) \cdot r(x)\right)<br>$$</p><blockquote><p>注意：凹函数的 Jensen 不等式是 $\leq$</p></blockquote><p><strong>第四步：计算右边</strong></p><p>$$<br>\sum_x P(x) \cdot r(x) = \sum_x P(x) \cdot \frac{Q(x)}{P(x)} = \sum_x Q(x) = 1<br>$$</p><p>所以：</p><p>$$<br>\sum_x P(x) \log r(x) \leq \log(1) = 0<br>$$</p><p><strong>第五步：得出结论</strong></p><p>$$<br>D_{KL}(P|Q) = -\sum_x P(x) \log r(x) \geq -0 = 0<br>$$</p><p><strong>证毕！</strong> ✓</p><p><strong>等号什么时候成立？</strong></p><p>当 $r(x) = \frac{Q(x)}{P(x)} = \text{常数}$ 对所有 $x$ 成立时。</p><p>又因为 $\sum P(x) = \sum Q(x) = 1$，所以这个常数只能是 1。</p><p>即：**$P(x) = Q(x)$ 对所有 $x$ 成立时，$D_{KL} = 0$**</p><hr><p><strong>性质2：不对称性（超级重要！）</strong></p><p>$$<br>D_{KL}(P | Q) \neq D_{KL}(Q | P)<br>$$</p><p>KL散度<strong>不是距离</strong>！方向很重要！</p><p><img src="/images/10_asymmetry.png" alt="不对称性"></p><h4 id="6-4-3-前向-KL-vs-反向-KL：深度解析"><a href="#6-4-3-前向-KL-vs-反向-KL：深度解析" class="headerlink" title="6.4.3 前向 KL vs 反向 KL：深度解析"></a>6.4.3 前向 KL vs 反向 KL：深度解析</h4><p><img src="/images/forward_vs_reverse_kl.png" alt="前向vs反向KL对比"></p><p>KL 散度的不对称性是其最重要的特性之一。理解前向和反向 KL 的区别，对于深度学习中的应用至关重要。</p><p><strong>核心区别：谁在做主？</strong></p><table><thead><tr><th>特性</th><th><strong>前向 KL</strong> $D_{KL}(P|Q)$</th><th><strong>反向 KL</strong> $D_{KL}(Q|P)$</th></tr></thead><tbody><tr><td><strong>公式</strong></td><td>$\sum P(x) \log \frac{P(x)}{Q(x)}$</td><td>$\sum Q(x) \log \frac{Q(x)}{P(x)}$</td></tr><tr><td><strong>视角</strong></td><td>站在真相 $P$ 的角度</td><td>站在模型 $Q$ 的角度</td></tr><tr><td><strong>核心权重</strong></td><td>$P(x)$（真相决定一切）</td><td>$Q(x)$（模型自己决定）</td></tr><tr><td><strong>惩罚条件</strong></td><td>$P&gt;0$ 但 $Q≈0$ 时爆炸</td><td>$Q&gt;0$ 但 $P≈0$ 时爆炸</td></tr><tr><td><strong>行为模式</strong></td><td>Zero-Avoiding（避免遗漏）</td><td>Zero-Forcing（强制精准）</td></tr><tr><td><strong>结果</strong></td><td>模糊但全面</td><td>尖锐但保守</td></tr><tr><td><strong>别名</strong></td><td>Mean-seeking（寻找平均）</td><td>Mode-seeking（寻找模态）</td></tr></tbody></table><p><strong>为什么”权重”决定了命运？</strong></p><p>公式里只是交换了 $P$ 和 $Q$ 的位置，为什么行为差异这么大？**因为前面的权重项充当了”开关”**。</p><p><strong>详细例子：双峰分布问题</strong></p><p>假设<strong>真实分布 $P$</strong> 是一个双峰分布（比如身高分布：男性峰 + 女性峰）：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">真实分布 P（双峰）:</span><br><span class="line">        ████                    ████</span><br><span class="line">       ██████                  ██████</span><br><span class="line">      ████████                ████████</span><br><span class="line">     ██████████              ██████████</span><br><span class="line">    ████████████            ████████████</span><br><span class="line">-------------------------------------------</span><br><span class="line">       峰1                      峰2</span><br></pre></td></tr></tbody></table></figure><p>但我们的<strong>模型 $Q$</strong> 只能是单峰分布（比如只能用一个高斯分布）。</p><p>问题：$Q$ 应该怎么放？</p><p><strong>情况1：前向 KL（$D_{KL}(P|Q)$）— Zero Avoiding</strong></p><p><strong>惩罚机制</strong>：在 $P(x) &gt; 0$ 的地方，如果 $Q(x) \approx 0$，则 $\log \frac{P(x)}{Q(x)} \to +\infty$</p><p><strong>后果</strong>：$Q$ <strong>必须覆盖</strong> $P$ 有概率的所有地方，否则会被严重惩罚！</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">前向 KL 的结果 — Q 变得"模糊但全面":</span><br><span class="line"></span><br><span class="line">        ████                    ████       ← P（真实）</span><br><span class="line">       ██████                  ██████</span><br><span class="line">    ┌─────────────────────────────────┐</span><br><span class="line">    │  █████████████████████████████  │    ← Q（模型）</span><br><span class="line">    │ ███████████████████████████████ │       覆盖了两个峰</span><br><span class="line">    └─────────────────────────────────┘         但中间也有概率</span><br></pre></td></tr></tbody></table></figure><p><strong>特点</strong>：宁可在中间”瞎猜”，也不能漏掉任何一个峰 → <strong>Mean-seeking（均值寻找）</strong></p><hr><p><strong>情况2：反向 KL（$D_{KL}(Q|P)$）— Zero Forcing</strong></p><p><strong>惩罚机制</strong>：在 $Q(x) &gt; 0$ 的地方，如果 $P(x) \approx 0$，则 $\log \frac{Q(x)}{P(x)} \to +\infty$</p><p><strong>后果</strong>：$Q$ <strong>只能在</strong> $P$ 有概率的地方出现，否则会被严重惩罚！</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">反向 KL 的结果 — Q 变得"尖锐但保守":</span><br><span class="line"></span><br><span class="line">        ████                    ████       ← P（真实）</span><br><span class="line">       ██████                  ██████</span><br><span class="line"></span><br><span class="line">        ████                               ← Q（模型）</span><br><span class="line">       ██████                                 只选择一个峰</span><br><span class="line">      ████████                                精准但不全面</span><br></pre></td></tr></tbody></table></figure><p><strong>特点</strong>：宁可放弃一个峰，也不能在 $P=0$ 的地方有概率 → <strong>Mode-seeking（模式寻找）</strong></p><hr><p><strong>形象比喻总结</strong></p><p><strong>前向KL：严厉的老师（Teacher Forcing）</strong></p><ul><li><strong>心态</strong>：老师拿着标准答案 $P$ 巡视</li><li><strong>规则</strong>：”我考纲（$P$）里有的知识点，你（$Q$）必须都要会！如果你交白卷（$Q \approx 0$），我就给你挂科！”</li><li><strong>结果</strong>：为了不挂科，你被迫把所有可能考的东西都写上去。哪怕有些地方你不懂，你也得硬凑。</li></ul><p><strong>反向KL：保守的学生（Student’s Choice）</strong></p><ul><li><strong>心态</strong>：你自己拿着手电筒 $Q$ 在雷区 $P$ 探路</li><li><strong>规则</strong>：”只要我踩到了雷（$P \approx 0$），我就死定了（Loss 无穷大）。但我没踩到的地方，雷再大也炸不到我。”</li><li><strong>结果</strong>：为了保命，你发现最好的策略是<strong>缩在一个绝对安全的地方不动</strong>。你放弃了探索其他区域，只死守一个山头。</li></ul><p><img src="/images/kl_penalty_curves.png" alt="KL惩罚曲线"></p><hr><h2 id="第七章：实战应用"><a href="#第七章：实战应用" class="headerlink" title="第七章：实战应用"></a>第七章：实战应用</h2><h3 id="7-1-RLHF中的KL惩罚"><a href="#7-1-RLHF中的KL惩罚" class="headerlink" title="7.1 RLHF中的KL惩罚"></a>7.1 RLHF中的KL惩罚</h3><p>在训练大模型（如 ChatGPT）时，有个步骤叫 <strong>RLHF</strong>（基于人类反馈的强化学习）。</p><p><strong>问题：</strong><br>如果只优化人类打分（Reward），模型可能会：</p><ul><li>疯狂说彩虹屁拿高分</li><li>忘记正常的语言规则</li><li>最终崩溃成胡言乱语</li></ul><p><strong>解决方案：加KL惩罚</strong></p><p>$$<br>\max ; \mathbb{E}[\text{Reward}] - \beta \cdot D_{KL}(\pi_{\text{new}} | \pi_{\text{old}})<br>$$</p><p><img src="/images/11_rlhf.png" alt="RLHF"></p><p><strong>各部分含义：</strong></p><table><thead><tr><th>符号</th><th>含义</th></tr></thead><tbody><tr><td>$\pi_{\text{new}}$</td><td>新模型（正在训练的）</td></tr><tr><td>$\pi_{\text{old}}$</td><td>旧模型（基座模型）</td></tr><tr><td>$\text{Reward}$</td><td>人类打分（越高越好）</td></tr><tr><td>$D_{KL}$</td><td>KL惩罚（防止离旧模型太远）</td></tr><tr><td>$\beta$</td><td>调节强度（$\beta$ 越大越保守）</td></tr></tbody></table><p><strong>形象比喻：</strong></p><p>KL惩罚 = 风筝线</p><ul><li>让你飞高（追求高分）</li><li>但拉住你别飞丢（保持语言能力）</li></ul><h3 id="7-2-其他应用"><a href="#7-2-其他应用" class="headerlink" title="7.2 其他应用"></a>7.2 其他应用</h3><table><thead><tr><th>领域</th><th>应用</th><th>KL散度的作用</th></tr></thead><tbody><tr><td><strong>VAE</strong></td><td>变分自编码器</td><td>ELBO损失函数的核心</td></tr><tr><td><strong>知识蒸馏</strong></td><td>大模型→小模型</td><td>让小模型模仿大模型的输出分布</td></tr><tr><td><strong>GAN</strong></td><td>生成对抗网络</td><td>衡量生成分布和真实分布的差距</td></tr><tr><td><strong>DPO</strong></td><td>直接偏好优化</td><td>新一代RLHF，直接优化KL约束下的偏好</td></tr><tr><td><strong>信息检索</strong></td><td>搜索引擎</td><td>计算文档相似度</td></tr></tbody></table><hr><h2 id="第八章：总结"><a href="#第八章：总结" class="headerlink" title="第八章：总结"></a>第八章：总结</h2><h3 id="8-1-知识地图"><a href="#8-1-知识地图" class="headerlink" title="8.1 知识地图"></a>8.1 知识地图</h3><p><img src="/images/12_summary.png" alt="总结"></p><p><strong>从信息论到KL散度的完整路径：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">信息量 I(x) = -log P(x)</span><br><span class="line">    ↓</span><br><span class="line">熵 H(P) = 期望[I(x)] = -Σ P(x)log P(x)</span><br><span class="line">    ↓</span><br><span class="line">交叉熵 H(P,Q) = -Σ P(x)log Q(x)</span><br><span class="line">    ↓</span><br><span class="line">KL散度 = H(P,Q) - H(P) = Σ P(x)log[P(x)/Q(x)]</span><br></pre></td></tr></tbody></table></figure><h3 id="8-2-记忆口诀"><a href="#8-2-记忆口诀" class="headerlink" title="8.2 记忆口诀"></a>8.2 记忆口诀</h3><p><strong>6句话记住KL散度：</strong></p><p>1️⃣ <strong>KL散度不是距离，是”信息浪费”</strong><br>   用 $Q$ 编码 $P$ 比用 $P$ 编码 $P$ 多浪费的 bits</p><p>2️⃣ <strong>永远 ≥ 0，= 0 当且仅当 P = Q</strong><br>   完美匹配时无浪费</p><p>3️⃣ <strong>不对称！$D_{KL}(P|Q) \neq D_{KL}(Q|P)$</strong><br>   前向关注覆盖，反向关注精确</p><p>4️⃣ <strong>前向KL让模型”不遗漏”（Zero Avoiding）</strong><br>   $P$ 有的地方 $Q$ 必须有</p><p>5️⃣ <strong>反向KL让模型”不出错”（Zero Forcing）</strong><br>   $Q$ 有的地方必须符合 $P$</p><p>6️⃣ <strong>在深度学习中常作正则项防止”忘本”</strong><br>   RLHF、VAE、蒸馏都用它</p><h3 id="8-3-公式速查"><a href="#8-3-公式速查" class="headerlink" title="8.3 公式速查"></a>8.3 公式速查</h3><p><strong>完整公式：</strong></p><p>$$<br>D_{KL}(P | Q) = \sum_{x} P(x) \log \frac{P(x)}{Q(x)}<br>$$</p><p><strong>等价形式：</strong></p><p>$$<br>\begin{aligned}<br>D_{KL}(P | Q) &amp;= H(P, Q) - H(P) \<br>&amp;= \sum_{x} P(x) \log P(x) - \sum_{x} P(x) \log Q(x) \<br>&amp;= \mathbb{E}_{x \sim P} \left[ \log \frac{P(x)}{Q(x)} \right]<br>\end{aligned}<br>$$</p><p><strong>连续情况：</strong></p><p>$$<br>D_{KL}(P | Q) = \int P(x) \log \frac{P(x)}{Q(x)} dx<br>$$</p><h3 id="8-4-常见误区"><a href="#8-4-常见误区" class="headerlink" title="8.4 常见误区"></a>8.4 常见误区</h3><table><thead><tr><th>误区</th><th>真相</th></tr></thead><tbody><tr><td>KL散度是距离</td><td>❌ 它不满足对称性和三角不等式</td></tr><tr><td>KL散度可以为负</td><td>❌ 永远 ≥ 0</td></tr><tr><td>$D_{KL}(P|Q)$ 和 $D_{KL}(Q|P)$ 相等</td><td>❌ 完全不同！方向很重要</td></tr><tr><td>KL散度越大越好</td><td>❌ 越小越好，0 = 完美</td></tr><tr><td>只有深度学习用KL散度</td><td>❌ 信息论、统计学、物理学都用</td></tr></tbody></table><hr><h2 id="📚-延伸阅读"><a href="#📚-延伸阅读" class="headerlink" title="📚 延伸阅读"></a>📚 延伸阅读</h2><h3 id="推荐资源"><a href="#推荐资源" class="headerlink" title="推荐资源"></a>推荐资源</h3><ol><li><p><strong>经典教材</strong></p><ul><li>《信息论基础》（Elements of Information Theory）</li><li>《深度学习》（Deep Learning Book）第3章</li></ul></li><li><p><strong>在线资源</strong></p><ul><li><a href="https://distill.pub/">Distill.pub - KL Divergence可视化</a></li><li><a href="https://www.youtube.com/c/3blue1brown">3Blue1Brown - 信息论视频</a></li></ul></li><li><p><strong>相关概念</strong></p><ul><li>JS散度（Jensen-Shannon Divergence）</li><li>Wasserstein距离</li><li>f-散度（f-Divergence）</li></ul></li></ol><h3 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h3><p><strong>初级：</strong></p><ol><li>计算均匀分布 $P=[0.25, 0.25, 0.25, 0.25]$ 的熵</li><li>证明：为什么 $D_{KL}(P | Q) \geq 0$？</li></ol><p><strong>中级：</strong><br>3. 推导：为什么交叉熵可以作为分类任务的损失函数？<br>4. 对比前向KL和反向KL在变分推断中的应用</p><p><strong>高级：</strong><br>5. 实现一个VAE，观察ELBO中KL项的作用<br>6. 分析PPO算法中KL约束的超参数 $\beta$ 如何影响训练稳定性</p><hr><h3 id="练习题参考答案"><a href="#练习题参考答案" class="headerlink" title="练习题参考答案"></a>练习题参考答案</h3><p><strong>初级题答案</strong></p><ol><li><strong>均匀分布的熵</strong></li></ol><p>$$<br>H(P) = -\sum_{i=1}^{4} 0.25 \times \log_2(0.25) = -4 \times 0.25 \times (-2) = 2 \text{ bits}<br>$$</p><p>均匀分布的熵最大，因为不确定性最高。</p><ol start="2"><li><strong>KL 散度非负性证明</strong>（见 6.4.2 节的详细证明）</li></ol><p><strong>中级题答案</strong></p><ol start="3"><li><strong>交叉熵作为分类损失函数</strong></li></ol><p>在分类任务中：</p><ul><li>$P$ = 真实标签的 one-hot 编码（如 $[0, 1, 0, 0]$）</li><li>$Q$ = 模型预测的概率分布（如 $[0.1, 0.7, 0.1, 0.1]$）</li></ul><p>损失函数：<br>$$<br>\text{Loss} = H(P, Q) = -\sum_i P_i \log Q_i<br>$$</p><p>因为 $P$ 中只有一个位置为 1，其余为 0，所以：<br>$$<br>\text{Loss} = -\log Q_{\text{true}}<br>$$</p><p>这就是<strong>负对数似然（Negative Log-Likelihood）</strong>，最小化它等价于最大化正确类别的预测概率。</p><ol start="4"><li><strong>前向 vs 反向 KL 在变分推断中的应用</strong></li></ol><table><thead><tr><th>方面</th><th>前向 KL (ELBO)</th><th>反向 KL</th></tr></thead><tbody><tr><td>公式</td><td>$D_{KL}(Q|P)$</td><td>$D_{KL}(P|Q)$</td></tr><tr><td>可计算性</td><td>✅ 可以优化</td><td>❌ 需要归一化常数</td></tr><tr><td>行为</td><td>Mode-seeking</td><td>Mean-seeking</td></tr><tr><td>应用</td><td>VAE 标准做法</td><td>期望传播（EP）</td></tr></tbody></table><p>VAE 使用反向 KL 是因为它不需要知道 $P$ 的归一化常数。</p><hr><h3 id="8-5-数值计算实例"><a href="#8-5-数值计算实例" class="headerlink" title="8.5 数值计算实例"></a>8.5 数值计算实例</h3><p><img src="/images/kl_numerical_example.png" alt="KL散度数值计算"></p><p><strong>完整示例：离散分布的 KL 散度计算</strong></p><p>设有离散分布 P 和 Q，取值为 {A, B, C}：</p><table><thead><tr><th>取值</th><th>P (真实)</th><th>Q (模型)</th><th>P/Q</th><th>log(P/Q)</th><th>P·log(P/Q)</th></tr></thead><tbody><tr><td>A</td><td>0.5</td><td>0.4</td><td>1.25</td><td>0.32</td><td><strong>0.16</strong></td></tr><tr><td>B</td><td>0.3</td><td>0.4</td><td>0.75</td><td>-0.42</td><td>-0.13</td></tr><tr><td>C</td><td>0.2</td><td>0.2</td><td>1.00</td><td>0</td><td>0</td></tr><tr><td><strong>合计</strong></td><td>1.0</td><td>1.0</td><td>—</td><td>—</td><td><strong>≈ 0.03 bits</strong></td></tr></tbody></table><p><strong>计算步骤：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line">P = np.array([<span class="number">0.5</span>, <span class="number">0.3</span>, <span class="number">0.2</span>])</span><br><span class="line">Q = np.array([<span class="number">0.4</span>, <span class="number">0.4</span>, <span class="number">0.2</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算 KL 散度</span></span><br><span class="line">D_KL = np.<span class="built_in">sum</span>(P * np.log(P / Q))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"D_KL(P||Q) = <span class="subst">{D_KL:<span class="number">.4</span>f}</span> bits"</span>)</span><br><span class="line"><span class="comment"># 输出: D_KL(P||Q) = 0.0361 bits</span></span><br></pre></td></tr></tbody></table></figure><p><strong>结果解读：</strong></p><ul><li>KL 散度 = 0.036 bits：用 Q 编码 P 的信息，平均每个符号多浪费 0.036 bits</li><li>主要误差来源：A 点被低估（P=0.5, Q=0.4）</li><li>当 P = Q 时，D_KL = 0（完美匹配）</li></ul><hr><h2 id="🎉-结语"><a href="#🎉-结语" class="headerlink" title="🎉 结语"></a>🎉 结语</h2><p>恭喜你！如果你读到这里，你已经从零开始，完全理解了KL散度的本质：</p><ul><li>✅ 从信息论出发，理解了信息量、熵、交叉熵</li><li>✅ 掌握了KL散度的公式和每个变量的含义</li><li>✅ 通过图示和例子建立了直觉</li><li>✅ 了解了在深度学习中的实际应用</li></ul><p><strong>最后的建议：</strong></p><p>KL散度不是用来背的，而是用来<strong>理解</strong>的。每次在论文或代码中看到它，问自己三个问题：</p><ol><li>这里的 $P$ 是什么？（真实分布）</li><li>这里的 $Q$ 是什么？（模型分布）</li><li>为什么要用KL散度？（防止过拟合？约束输出？）</li></ol><p>带着这些问题，你会发现KL散度无处不在，而你已经完全理解它了。</p><hr><blockquote><p>📝 <strong>更新日志</strong>  </p><ul><li>2026-02-08：首次发布，包含完整的数学证明、前向/反向KL深度解析和5张可视化图表</li></ul></blockquote>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>InstructGPT 与 RLHF 深度解析：费曼视角下的原理与实现</title>
      <link href="/2026/02/07/InstructGPT-RLHF-Complete-Guide/"/>
      <url>/2026/02/07/InstructGPT-RLHF-Complete-Guide/</url>
      
        <content type="html"><![CDATA[<blockquote><p>🎯 <strong>面试官视角</strong>：<br>这篇文章不仅仅是教程，更是你的<strong>面试复习提纲</strong>。<br>我们不堆砌代码，而是深入到 <strong>原理 (Principles)<strong>、</strong>输入输出流 (I/O Flow)</strong> 和 **数学推导 (Math Derivation)**。<br>如果你能用本文的逻辑把 RLHF 讲清楚，P7/L6 级别的算法岗面试不在话下。</p></blockquote><span id="more"></span><hr><h1 id="0-费曼时刻：什么是-Alignment？"><a href="#0-费曼时刻：什么是-Alignment？" class="headerlink" title="0. 费曼时刻：什么是 Alignment？"></a>0. 费曼时刻：什么是 Alignment？</h1><p>想象你正在训练一只<strong>天才鹦鹉</strong>（GPT-3）。<br>它读遍了世界上的所有书，能补全任何句子。</p><ul><li>你说：“床前明月光”，它接：“疑是地上霜”。</li><li>你问：“如何制造毒药？”，它接：“首先你需要……”（因为它在侦探小说里看过）。</li></ul><p><strong>问题来了</strong>：这只鹦鹉太“耿直”了，它不懂什么是<strong>对</strong>，什么是<strong>错</strong>，什么是<strong>安全</strong>。它只是在做<strong>概率预测</strong>。</p><p><strong>RLHF (Reinforcement Learning from Human Feedback)</strong> 就是送这只鹦鹉去上<strong>礼仪学校</strong>的过程：</p><ol><li><strong>SFT（小学）</strong>：老师给它标准答案，让它照着抄，学会“像人一样说话”。</li><li><strong>RM（中考）</strong>：老师不再给答案，而是给它的回答打分。它学会了“老师喜欢什么样的回答”。</li><li><strong>PPO（体育特训）</strong>：没有老师盯着，它自己根据之前的打分标准不断练习，试图拿高分，同时别把脑子练坏了（KL Penalty）。</li></ol><hr><h1 id="1-第一阶段：监督微调-SFT"><a href="#1-第一阶段：监督微调-SFT" class="headerlink" title="1. 第一阶段：监督微调 (SFT)"></a>1. 第一阶段：监督微调 (SFT)</h1><blockquote><p><strong>一句话解释</strong>：从“续写机器”变成“问答助手”。</p></blockquote><h3 id="1-1-原理深度解析"><a href="#1-1-原理深度解析" class="headerlink" title="1.1 原理深度解析"></a>1.1 原理深度解析</h3><p>预训练模型（Pretrained Model）的目标是 $P(x_t|x_{<t})$，它倾向于<strong>续写。<br>SFT（Supervised Fine-Tuning）的目标是让模型学会<strong>响应指令</strong>。</t})$，它倾向于<strong></p><p>虽然都是 Language Modeling，但核心区别在于 <strong>Data Distribution（数据分布）</strong> 和 <strong>Loss Masking</strong>。</p><h3 id="1-2-输入输出流-I-x2F-O-Flow"><a href="#1-2-输入输出流-I-x2F-O-Flow" class="headerlink" title="1.2 输入输出流 (I/O Flow)"></a>1.2 输入输出流 (I/O Flow)</h3><p>假设我们有一个 Batch 的数据。</p><ul><li><strong>Input (Prompt)</strong>: <code>x</code> = [“把这句话翻译成英文”, “解释量子力学”]</li><li><strong>Target (Response)</strong>: <code>y</code> = [“Translate this…”, “Quantum mechanics is…”]</li></ul><p>在 Tensor 层面：</p><ul><li><strong>Input Tensor</strong>: <code>[Batch, Seq_Len]</code> (例如 <code>[4, 1024]</code>)，包含了 <code>&lt;Prompt&gt; &lt;Response&gt; &lt;EOS&gt;</code>。</li><li><strong>Labels Tensor</strong>: 形状同 Input。<ul><li><strong>关键点</strong>：Prompt 部分的 Label 被设为 <code>-100</code> (PyTorch 中忽略计算 Loss 的标志)。</li><li>只有 Response 部分参与梯度计算。</li></ul></li></ul><h3 id="1-3-数学推导：MLE"><a href="#1-3-数学推导：MLE" class="headerlink" title="1.3 数学推导：MLE"></a>1.3 数学推导：MLE</h3>$$\mathcal{L}_{\text{SFT}} = - \sum_{t \in \text{Response}} \log P_{\theta}(x_t \mid x_{<t}) $$="" <p=""><strong>面试考点</strong>：<p></p><ul><li><strong>Q: 为什么要 Mask 掉 Prompt 的 Loss？</strong></li><li><strong>A</strong>: 因为 Prompt 是用户输入的，是已知的 Condition。我们不希望模型去“预测”用户会说什么，我们只希望模型在给定用户输入的情况下，预测正确的输出。如果不 Mask，模型会花费容量去记忆 Prompt 的分布，这是浪费。</li></ul><hr><h1 id="2-第二阶段：奖励模型-RM"><a href="#2-第二阶段：奖励模型-RM" class="headerlink" title="2. 第二阶段：奖励模型 (RM)"></a>2. 第二阶段：奖励模型 (RM)</h1><blockquote><p><strong>一句话解释</strong>：人类没办法给几亿条数据打分，所以我们训练一个“电子判官”来模仿人类的口味。</p></blockquote><h3 id="2-1-为什么是-Ranking-而不是-Scoring？"><a href="#2-1-为什么是-Ranking-而不是-Scoring？" class="headerlink" title="2.1 为什么是 Ranking 而不是 Scoring？"></a>2.1 为什么是 Ranking 而不是 Scoring？</h3><p>这是面试常考点。</p><ul><li><strong>Scoring (打分)<strong>：让标注员打 1-10 分。</strong>缺点</strong>：主观性太强。A 认为 7 分是好，B 认为 7 分是及格。数据噪声极大。</li><li><strong>Ranking (排序)<strong>：给两个回答，选好的那个。</strong>优点</strong>：比较是容易的，一致性高。</li></ul><h3 id="2-2-输入输出流-I-x2F-O-Flow"><a href="#2-2-输入输出流-I-x2F-O-Flow" class="headerlink" title="2.2 输入输出流 (I/O Flow)"></a>2.2 输入输出流 (I/O Flow)</h3><p>RM 通常是一个 BERT 或 GPT 架构的模型，但去掉了解码头，换成了一个**标量输出头 (Scalar Head)**。</p><ul><li><strong>Input</strong>: <code>(Prompt, Response_A, Response_B)</code></li><li><strong>Process</strong>:<ol><li>把 <code>Prompt + Response_A</code> 喂给模型 $\rightarrow$ 得到标量 $r_A$。</li><li>把 <code>Prompt + Response_B</code> 喂给模型 $\rightarrow$ 得到标量 $r_B$。</li></ol></li><li><strong>Output</strong>: 两个分数 $r_A, r_B$。</li></ul><h3 id="2-3-数学推导：Bradley-Terry-模型"><a href="#2-3-数学推导：Bradley-Terry-模型" class="headerlink" title="2.3 数学推导：Bradley-Terry 模型"></a>2.3 数学推导：Bradley-Terry 模型</h3><p>这是 RM 训练的核心数学基础。我们假设 $A$ 比 $B$ 好的概率服从 <strong>Sigmoid</strong> 分布：</p>$$P(A \succ B) = \sigma(r_A - r_B) = \frac{1}{1 + e^{-(r_A - r_B)}}$$<p>**损失函数 (Log-Likelihood)**：<br>我们要最大化标注数据的似然度。假设数据告诉我们 A &gt; B，则 Loss 为：</p>$$\begin{aligned}\mathcal{L}_{\text{RM}} &amp;= - \log P(A \succ B) \\&amp;= - \log \sigma(r_A - r_B) \\&amp;= - \log \left( \frac{1}{1 + e^{-(r_A - r_B)}} \right)\end{aligned}$$<p><strong>代码实现示例</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">ranking_loss</span>(<span class="params">score_chosen, score_rejected</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    score_chosen: shape [batch_size]</span></span><br><span class="line"><span class="string">    score_rejected: shape [batch_size]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    loss = -torch.nn.functional.logsigmoid(score_chosen - score_rejected).mean()</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></tbody></table></figure><hr><h1 id="3-第三阶段：PPO-强化学习-核心难点"><a href="#3-第三阶段：PPO-强化学习-核心难点" class="headerlink" title="3. 第三阶段：PPO 强化学习 (核心难点)"></a>3. 第三阶段：PPO 强化学习 (核心难点)</h1><blockquote><p><strong>一句话解释</strong>：左脚踩右脚上天。模型在 RM 的打分激励下，不断探索更高分的回答，同时被 KL 散度拉住，防止胡说八道。</p></blockquote><p>这一块是面试的重灾区，涉及 <strong>4 个模型</strong> 和复杂的 <strong>数学技巧</strong>。</p><h3 id="3-1-四个模型-The-Big-Four"><a href="#3-1-四个模型-The-Big-Four" class="headerlink" title="3.1 四个模型 (The Big Four)"></a>3.1 四个模型 (The Big Four)</h3><p>在 PPO 训练时，显存中驻留着四个模型：</p><table><thead><tr><th align="left">模型名称</th><th align="left">角色</th><th align="left">状态</th><th align="left">作用</th></tr></thead><tbody><tr><td align="left"><strong>Actor</strong> ($\pi_{\theta}$)</td><td align="left">运动员</td><td align="left"><strong>Train</strong></td><td align="left">生成文本，我们要优化的对象 (初始化自 SFT)</td></tr><tr><td align="left"><strong>Critic</strong> ($V_{\phi}$)</td><td align="left">教练</td><td align="left"><strong>Train</strong></td><td align="left">估计状态价值 (Value Function)，用于计算 Advantage</td></tr><tr><td align="left"><strong>Ref Model</strong> ($\pi_{\text{ref}}$)</td><td align="left">照妖镜</td><td align="left">Frozen</td><td align="left">提供基准概率，计算 KL 散度 (初始化自 SFT)</td></tr><tr><td align="left"><strong>Reward Model</strong> ($R$)</td><td align="left">裁判</td><td align="left">Frozen</td><td align="left">给生成的文本打分</td></tr></tbody></table><h3 id="3-2-完整的-PPO-训练步-Step-by-Step"><a href="#3-2-完整的-PPO-训练步-Step-by-Step" class="headerlink" title="3.2 完整的 PPO 训练步 (Step-by-Step)"></a>3.2 完整的 PPO 训练步 (Step-by-Step)</h3><h4 id="Step-1-Rollout-采样"><a href="#Step-1-Rollout-采样" class="headerlink" title="Step 1: Rollout (采样)"></a>Step 1: Rollout (采样)</h4><p>Actor 根据 Prompt 生成回答 <code>Response</code>。<br>同时，Ref Model 也计算生成该 <code>Response</code> 的概率。</p><h4 id="Step-2-Calculate-Reward-计算奖励"><a href="#Step-2-Calculate-Reward-计算奖励" class="headerlink" title="Step 2: Calculate Reward (计算奖励)"></a>Step 2: Calculate Reward (计算奖励)</h4><p>真正的奖励不仅仅是 RM 的打分，还要减去 KL 惩罚。</p>$$R_{\text{total}} = R_{\text{RM}}(x, y) - \beta \cdot \log \left( \frac{\pi_{\theta}(y|x)}{\pi_{\text{ref}}(y|x)} \right)$$<p><strong>面试考点：为什么要 KL 惩罚？</strong></p><ul><li><strong>为了防止 Reward Hacking</strong>。RM 只是人类偏好的一个<strong>拟合</strong>（Proxy），它不是完美的。如果过度优化 RM 分数，模型会找到 RM 的漏洞（例如输出乱码但 RM 认为是好词）。KL 惩罚强制 Actor 不能偏离 SFT 模型太远，保证语言的流畅性和合理性。</li></ul><h4 id="Step-3-GAE-广义优势估计"><a href="#Step-3-GAE-广义优势估计" class="headerlink" title="Step 3: GAE (广义优势估计)"></a>Step 3: GAE (广义优势估计)</h4><p>我们要计算 **Advantage (优势)**：当前的动作比“平均水平”好多少？</p><p>这里用到了 <strong>Critic</strong> 模型预测的 Value ($V(s)$)。<br>TD Error ($\delta_t$) 定义为：</p>$$\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$$<p>GAE ($A_t$) 是 TD Error 的加权累加：</p>$$A_t = \sum_{k=0}^{\infty} (\gamma \lambda)^k \delta_{t+k}$$<h4 id="Step-4-PPO-Clip-Update"><a href="#Step-4-PPO-Clip-Update" class="headerlink" title="Step 4: PPO Clip Update"></a>Step 4: PPO Clip Update</h4><p>这是 PPO 的精髓。我们希望更新策略，但<strong>不能更新太猛</strong>（Trust Region）。</p><p>定义概率比率 $ratio = \frac{\pi_{new}}{\pi_{old}}$。<br>目标函数：</p>$$L^{\text{CLIP}} = \min \left( ratio \cdot A_t, \text{clip}(ratio, 1-\epsilon, 1+\epsilon) \cdot A_t \right)$$<ul><li>如果 $A_t &gt; 0$（好动作）：限制 $ratio$ 最大为 $1+\epsilon$（奖励不要发过头）。</li><li>如果 $A_t &lt; 0$（坏动作）：限制 $ratio$ 最小为 $1-\epsilon$（惩罚不要太重）。</li></ul><h3 id="3-3-显存占用分析-Memory-Overhead"><a href="#3-3-显存占用分析-Memory-Overhead" class="headerlink" title="3.3 显存占用分析 (Memory Overhead)"></a>3.3 显存占用分析 (Memory Overhead)</h3><p>假设一个 7B 模型占用 14GB 显存。PPO 训练需要多少显存？</p><ol><li><strong>Actor</strong>: 14GB (参数) + 梯度 + 优化器状态 (Adam state 占 2倍参数)。</li><li><strong>Critic</strong>: 14GB (通常也是 7B，或者小一点) + 梯度 + 优化器状态。</li><li><strong>Ref Model</strong>: 14GB (Inference only)。</li><li><strong>Reward Model</strong>: 14GB (Inference only)。</li></ol><p>加起来非常巨大！<br><strong>优化方案 (DeepSpeed/LoRA)</strong>:</p><ul><li><strong>Offload</strong>: 把 Ref 和 RM 放到 CPU 内存里。</li><li><strong>LoRA</strong>: Actor 和 Critic 只训练 Low-rank adapter，大幅减少优化器状态。</li><li><strong>Shared Backbone</strong>: RM 和 Critic 共享一部分参数（Hydra Head）。</li></ul><hr><h1 id="4-代码实战：手写-PPO-核心逻辑"><a href="#4-代码实战：手写-PPO-核心逻辑" class="headerlink" title="4. 代码实战：手写 PPO 核心逻辑"></a>4. 代码实战：手写 PPO 核心逻辑</h1><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_ppo_loss</span>(<span class="params">logprobs, old_logprobs, advantages, clip_eps=<span class="number">0.2</span></span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    logprobs: 当前策略生成的 token log 概率 [batch, seq_len]</span></span><br><span class="line"><span class="string">    old_logprobs: 采样时策略生成的 token log 概率 (固定值)</span></span><br><span class="line"><span class="string">    advantages: GAE 计算出的优势值</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 1. 计算概率比率 ratio = pi_new / pi_old</span></span><br><span class="line">    <span class="comment"># exp(log_a - log_b) = a / b</span></span><br><span class="line">    ratio = torch.exp(logprobs - old_logprobs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 2. 计算未截断的 Loss</span></span><br><span class="line">    surr1 = ratio * advantages</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 3. 计算截断后的 Loss</span></span><br><span class="line">    <span class="comment"># clamp 限制 ratio 在 [0.8, 1.2] 之间</span></span><br><span class="line">    surr2 = torch.clamp(ratio, <span class="number">1</span> - clip_eps, <span class="number">1</span> + clip_eps) * advantages</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 4. 取最小值 (PPO Core: Pessimistic Bound)</span></span><br><span class="line">    <span class="comment"># 注意这里是 maximize objective，所以通常代码里是 return -loss.mean()</span></span><br><span class="line">    loss = torch.<span class="built_in">min</span>(surr1, surr2)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> -loss.mean()</span><br></pre></td></tr></tbody></table></figure><hr><h1 id="5-总结与展望"><a href="#5-总结与展望" class="headerlink" title="5. 总结与展望"></a>5. 总结与展望</h1><h3 id="RLHF-的本质"><a href="#RLHF-的本质" class="headerlink" title="RLHF 的本质"></a>RLHF 的本质</h3><p>RLHF 不是魔法，它是<strong>用 Reward Model 作为一个可微的代理（Differentiable Proxy），把不可微的人类偏好（Human Preference）传递给语言模型</strong>。</p><h3 id="面试高频问题清单"><a href="#面试高频问题清单" class="headerlink" title="面试高频问题清单"></a>面试高频问题清单</h3><ol><li><strong>SFT 阶段 Loss 怎么算的？</strong> (Answer: Cross Entropy, Mask Prompt)</li><li><strong>RM 为什么用 Ranking Loss？</strong> (Answer: 归一化，抗噪声)</li><li><strong>PPO 中 KL 散度的作用？</strong> (Answer: 防止 Reward Hacking，保持分布一致性)</li><li><strong>On-policy vs Off-policy？</strong> (Answer: PPO 是 On-policy，每次更新都要重新采样，所以慢)</li></ol><hr><p><em>本文最后更新于 2026-02-07 03:00:00</em></p></t})>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GPT 系列深度解析：从 GPT-1 到 GPT-3</title>
      <link href="/2026/02/05/2026-02-05-gpt-series-deep-dive/"/>
      <url>/2026/02/05/2026-02-05-gpt-series-deep-dive/</url>
      
        <content type="html"><![CDATA[<p><strong>摘要</strong>：本文深度解析 OpenAI 的 GPT 系列模型演进之路。从 GPT-1 的预训练+微调范式，到 GPT-2 的零样本尝试，再到 GPT-3 的上下文学习（In-context Learning）与规模法则（Scaling Laws）。文章详细拆解了模型架构、Tensor 维度变化、训练数据流，并配有 13 张可视化图表与通俗易懂的费曼式讲解。</p><span id="more"></span><h2 id="📋-目录"><a href="#📋-目录" class="headerlink" title="📋 目录"></a>📋 目录</h2><ol><li><a href="#intuition">通俗理解：GPT 到底在做什么？</a></li><li><a href="#timeline">NLP 预训练时代全景</a></li><li><a href="#comparison">GPT vs BERT vs T5：核心差异</a></li><li><a href="#gpt-1">GPT-1：Pre-train + Fine-tune</a></li><li><a href="#gpt-2">GPT-2：Zero-shot Learning</a></li><li><a href="#gpt-3">GPT-3：In-context Learning</a></li><li><a href="#evolution">三代演进对比总结</a></li><li><a href="#code">代码实现参考</a></li><li><a href="#feynman">费曼总结：教给小白听</a></li></ol><hr><p><a id="intuition"></a></p><h2 id="🎯-通俗理解：GPT-到底在做什么？"><a href="#🎯-通俗理解：GPT-到底在做什么？" class="headerlink" title="🎯 通俗理解：GPT 到底在做什么？"></a>🎯 通俗理解：GPT 到底在做什么？</h2><h3 id="一句话总结"><a href="#一句话总结" class="headerlink" title="一句话总结"></a>一句话总结</h3><blockquote><p><strong>GPT 就是一个”续写机器”：给它一段话的开头，它预测接下来应该写什么。</strong></p></blockquote><h3 id="生活类比：作家-vs-考生"><a href="#生活类比：作家-vs-考生" class="headerlink" title="生活类比：作家 vs 考生"></a>生活类比：作家 vs 考生</h3><p>为了直观理解 GPT 的工作方式，我们来看下面这张对比图：</p><p><img src="/images/gpt_series/intuitive_analogy.png" alt="GPT vs BERT 阅读方式对比"></p><p><strong>图表深度解读</strong>：<br>上图展示了两种截然不同的语言处理模式。</p><ul><li><strong>GPT (上部分)<strong>：被描绘成一个正在打字的</strong>作家</strong>。它的视野是”单向”的（绿色箭头只向右），意味着它只能看到已经写出来的内容。就像写小说一样，作者根据前文 <code>Once upon a time</code> 来构思下一个词 <code>there</code>。这种<strong>自回归（Autoregressive）</strong>的特性使它天生适合<strong>文本生成</strong>任务。</li><li><strong>BERT (下部分)<strong>：被描绘成一个正在做完形填空的</strong>学生</strong>。它的视野是”双向”的（红色箭头同时指向左右），它可以同时看到空格前后的内容 <code>The ___ sat on</code>。这使它在理解上下文语境时非常强大，适合<strong>分类、实体识别</strong>等理解任务，但无法像 GPT 那样流畅地生成文本。</li></ul><h3 id="为什么这个简单任务能产生”智能”？"><a href="#为什么这个简单任务能产生”智能”？" class="headerlink" title="为什么这个简单任务能产生”智能”？"></a>为什么这个简单任务能产生”智能”？</h3><p>你可能会问，预测下一个词听起来很简单，为什么能产生智能？</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">训练数据：互联网上的所有文本（书籍、网页、对话...）</span><br><span class="line"></span><br><span class="line">当模型学会预测 "The capital of France is ___"</span><br><span class="line">→ 它必须"知道"法国的首都是巴黎</span><br><span class="line"></span><br><span class="line">当模型学会预测 "2 + 2 = ___"  </span><br><span class="line">→ 它必须"理解"基本数学</span><br><span class="line"></span><br><span class="line">当模型学会预测 "If it rains, I will bring an ___"</span><br><span class="line">→ 它必须"推理"下雨需要带伞</span><br><span class="line"></span><br><span class="line">结论：预测下一个词 = 被迫学习世界知识！</span><br></pre></td></tr></tbody></table></figure><hr><p><a id="timeline"></a></p><h2 id="📅-NLP-预训练时代全景"><a href="#📅-NLP-预训练时代全景" class="headerlink" title="📅 NLP 预训练时代全景"></a>📅 NLP 预训练时代全景</h2><h3 id="里程碑时间线"><a href="#里程碑时间线" class="headerlink" title="里程碑时间线"></a>里程碑时间线</h3><p>下图梳理了 NLP 预训练模型发展的黄金三年（2017-2020）：</p><p><img src="/images/gpt_series/nlp_timeline.png" alt="NLP 预训练模型发展时间线"></p><p><strong>图表深度解读</strong>：</p><ul><li><strong>起点 (2017.06)<strong>：</strong>Transformer</strong> 的诞生是原点。Google 发布的 <em>Attention Is All You Need</em> 论文提出 Self-Attention 机制，彻底取代了 RNN/LSTM，允许模型并行训练，为大规模模型奠定了基础。</li><li>**分支一 (Decoder-only)**：图表上方是 <strong>GPT 家族</strong>。从 GPT-1 到 GPT-2 再到 GPT-3，OpenAI 坚持走”纯解码器”路线，专注于生成能力。可以看到参数量呈指数级爆炸（117M → 1.5B → 175B）。</li><li>**分支二 (Encoder-only)**：图表下方是 <strong>BERT 家族</strong>。BERT 引入 MLM（掩码语言模型），刷新了几乎所有 NLP 榜单，随后衍生出 RoBERTa、DistilBERT 等优化版本，专注于理解任务。</li><li>**分支三 (Encoder-Decoder)**：中间是 <strong>T5/BART</strong> 等尝试统一两者的架构，试图在理解和生成之间找到平衡。</li></ul><h3 id="模型规模演进"><a href="#模型规模演进" class="headerlink" title="模型规模演进"></a>模型规模演进</h3><p>这短短几年间，模型大小发生了什么变化？</p><p><img src="/images/gpt_series/model_scale_evolution.png" alt="模型参数量演进趋势"></p><p><strong>图表深度解读</strong>：</p><ul><li>这是一张对数坐标图。纵轴代表参数量（Log scale）。</li><li><strong>ELMo (94M)</strong> 和 <strong>GPT-1 (117M)</strong> 处于起步阶段，相当于”小个子”。</li><li><strong>BERT-Large (340M)</strong> 定义了当时的”大模型”标准。</li><li><strong>GPT-2 (1.5B)</strong> 首次突破 10亿 参数大关，证明了模型越大效果越好。</li><li><strong>GPT-3 (175B)</strong> 则是一个巨大的飞跃（右上角那个遥不可及的点），它比之前的模型大两个数量级，直接开启了”大模型（LLM）”时代。</li></ul><hr><p><a id="comparison"></a></p><h2 id="⚔️-GPT-vs-BERT-vs-T5：核心差异"><a href="#⚔️-GPT-vs-BERT-vs-T5：核心差异" class="headerlink" title="⚔️ GPT vs BERT vs T5：核心差异"></a>⚔️ GPT vs BERT vs T5：核心差异</h2><h3 id="架构对比"><a href="#架构对比" class="headerlink" title="架构对比"></a>架构对比</h3><p>三者在神经网络架构上究竟有何不同？</p><p><img src="/images/gpt_series/gpt_vs_bert_architecture.png" alt="GPT, BERT, T5 架构对比"></p><p><strong>图表深度解读</strong>：</p><ul><li><strong>GPT (左)<strong>：</strong>Decoder-only</strong> 架构。注意其中的连接线是<strong>单向</strong>的（只从左向右）。这意味着处理第 $i$ 个词时，模型只能看到 $0$ 到 $i-1$ 个词。Masked Self-Attention 强制了这种因果关系。</li><li><strong>BERT (中)<strong>：</strong>Encoder-only</strong> 架构。连接线是<strong>全连接</strong>的，任何位置的词都能看到整个序列的信息。这对于理解句子含义至关重要，但使得它无法像人类说话一样逐词生成。</li><li><strong>T5 (右)<strong>：</strong>Encoder-Decoder</strong> 架构。左边是一个双向的 Encoder（读入输入），右边是一个单向的 Decoder（生成输出）。这就像机器翻译：先读懂原文（Encode），再写出译文（Decode）。</li></ul><h3 id="训练目标对比"><a href="#训练目标对比" class="headerlink" title="训练目标对比"></a>训练目标对比</h3><p>它们在预训练时分别在做什么题？</p><p><img src="/images/gpt_series/training_objectives_comparison.png" alt="训练目标对比"></p><p><strong>图表深度解读</strong>：</p><ul><li><strong>GPT (Autoregressive LM)<strong>：任务是</strong>预测下一个词</strong>。图中显示模型看到 <code>The cat sat</code>，目标是预测 <code>on</code>。这是最自然的语言生成任务。</li><li><strong>BERT (Masked LM)<strong>：任务是</strong>完形填空</strong>。图中 <code>sat</code> 被 <code>[MASK]</code> 遮住了，模型需要利用上下文 <code>The cat [MASK] on</code> 把 <code>sat</code> 填回去。</li><li><strong>T5 (Span Corruption)<strong>：任务是</strong>还原片段</strong>。输入中一段文本被挖掉了，模型需要生成被挖掉的内容。这是一种更通用的序列到序列任务。</li></ul><hr><p><a id="gpt-1"></a></p><h2 id="🔷-第一部分：GPT-1-深度解析"><a href="#🔷-第一部分：GPT-1-深度解析" class="headerlink" title="🔷 第一部分：GPT-1 深度解析"></a>🔷 第一部分：GPT-1 深度解析</h2><h3 id="1-1-核心思想"><a href="#1-1-核心思想" class="headerlink" title="1.1 核心思想"></a>1.1 核心思想</h3><p><strong>GPT-1 (Generative Pre-Training)</strong> 的核心贡献是确立了 <strong>Pre-training + Fine-tuning</strong> 的范式。<br>在此之前，NLP 任务主要靠从头训练（scratch）或使用静态词向量（Word2Vec）。GPT-1 证明：先在一个海量无标注文本上训练一个语言模型，然后针对特定任务（分类、蕴含等）进行微调，效果远超专门设计的模型。</p><h3 id="1-2-🔬-Tensor-维度变化（完整数据流）"><a href="#1-2-🔬-Tensor-维度变化（完整数据流）" class="headerlink" title="1.2 🔬 Tensor 维度变化（完整数据流）"></a>1.2 🔬 Tensor 维度变化（完整数据流）</h3><p>让我们深入模型内部，看看数据是如何流动的。</p><p><img src="/images/gpt_series/gpt_dimension_flow.png" alt="GPT Tensor 维度流向图"></p><p><strong>图表深度解读</strong>：<br>这张详细的数据流图展示了一个 batch 的数据在 GPT-1 中的完整旅程：</p><ol><li><strong>Input</strong>：输入 Token 序列 <code>[2, 5]</code>（假设 batch=2, seq=5）。</li><li><strong>Embedding</strong>：Token ID 被转换为 768 维的向量，并加上了位置编码（Position Embedding）。此时 Tensor 形状为 <code>[2, 5, 768]</code>。</li><li><strong>Transformer Block</strong>：<ul><li><strong>Q, K, V Projection</strong>：输入被投影为 Query, Key, Value。</li><li><strong>Split Heads</strong>：768 维被拆分为 12 个头，每个头 64 维。形状变为 <code>[2, 12, 5, 64]</code>。</li><li><strong>Attention Score</strong>：Q 和 K 相乘，得到 <code>[2, 12, 5, 5]</code> 的分数矩阵。<strong>关键点</strong>：这里应用了 Causal Mask（右上角为负无穷），确保词 $i$ 只能关注到词 $0…i$。</li><li><strong>Output</strong>：经过 Softmax 和 V 相乘，并拼接所有头，恢复为 <code>[2, 5, 768]</code>。</li></ul></li><li><strong>FFN</strong>：经过两层全连接层（中间升维到 3072），引入非线性。</li><li><strong>Logits</strong>：最后经过线性层映射回词表大小 <code>[2, 5, 40000]</code>，表示每个位置预测下一个词的概率分布。</li></ol><h3 id="1-3-训练闭环"><a href="#1-3-训练闭环" class="headerlink" title="1.3 训练闭环"></a>1.3 训练闭环</h3><p><img src="/images/gpt_series/gpt_training_loop.png" alt="GPT 训练循环流程"></p><p><strong>图表深度解读</strong>：</p><ul><li>图展示了标准的 <strong>自监督学习（Self-Supervised Learning）</strong> 循环。</li><li><strong>Shift Trick</strong>：输入是 $x_0, x_1, x_2, x_3$，标签（Target）则是 $x_1, x_2, x_3, x_4$。也就是输入序列整体右移一位作为监督信号。</li><li><strong>Loss 计算</strong>：模型输出的 Logits 与 Target 进行 Cross Entropy Loss 计算，梯度回传更新所有参数。</li></ul><hr><p><a id="gpt-2"></a></p><h2 id="🔷-第二部分：GPT-2-深度解析"><a href="#🔷-第二部分：GPT-2-深度解析" class="headerlink" title="🔷 第二部分：GPT-2 深度解析"></a>🔷 第二部分：GPT-2 深度解析</h2><h3 id="2-1-核心思想：Zero-shot-Learning"><a href="#2-1-核心思想：Zero-shot-Learning" class="headerlink" title="2.1 核心思想：Zero-shot Learning"></a>2.1 核心思想：Zero-shot Learning</h3><p>GPT-2 的标题是 *”Language Models are Unsupervised Multitask Learners”*。<br>OpenAI 发现，当模型足够大、数据足够多时，<strong>不需要 Fine-tuning</strong>，模型就能直接执行任务。<br>比如你给它输入 “Translate to French: cheese =&gt;”，它会自动补全 “fromage”。</p><h3 id="2-2-数据升级：WebText"><a href="#2-2-数据升级：WebText" class="headerlink" title="2.2 数据升级：WebText"></a>2.2 数据升级：WebText</h3><p><img src="/images/gpt_series/gpt_data_comparison.png" alt="GPT 数据集对比"></p><p><strong>图表深度解读</strong>：</p><ul><li>**GPT-1 (左)**：使用的是 <strong>BooksCorpus</strong>，主要是小说书籍。文本风格单一，虽然连贯但覆盖面窄。</li><li>**GPT-2 (右)**：使用的是 <strong>WebText</strong>。OpenAI 爬取了 Reddit 上所有获赞超过 3 个的链接内容。这意味着数据经过了人类的”筛选”（只有高质量内容才会被分享和点赞）。</li><li><strong>量级提升</strong>：数据量从 5GB 激增到 40GB。这使得模型见识到了更广阔的世界（新闻、代码、食谱、科技论文…）。</li></ul><h3 id="2-3-架构微调：Pre-LN"><a href="#2-3-架构微调：Pre-LN" class="headerlink" title="2.3 架构微调：Pre-LN"></a>2.3 架构微调：Pre-LN</h3><p>GPT-2 将 Layer Normalization 移到了 Attention 和 FFN 的<strong>输入</strong>端（称为 Pre-LN）。这大大稳定了深层网络（48层）的梯度传播，使得训练更深的模型成为可能。</p><hr><p><a id="gpt-3"></a></p><h2 id="🔷-第三部分：GPT-3-深度解析"><a href="#🔷-第三部分：GPT-3-深度解析" class="headerlink" title="🔷 第三部分：GPT-3 深度解析"></a>🔷 第三部分：GPT-3 深度解析</h2><h3 id="3-1-核心思想：In-context-Learning-ICL"><a href="#3-1-核心思想：In-context-Learning-ICL" class="headerlink" title="3.1 核心思想：In-context Learning (ICL)"></a>3.1 核心思想：In-context Learning (ICL)</h3><p>GPT-3 并没有修改模型架构，而是将参数量推到了恐怖的 <strong>1750亿</strong>。在这个尺度下，模型涌现出了神奇的 <strong>上下文学习（In-context Learning）</strong> 能力。</p><p><img src="/images/gpt_series/gpt_in_context_learning.png" alt="In-context Learning 示意图"></p><p><strong>图表深度解读</strong>：<br>这张图解释了 GPT-3 独特的使用方式——不需要更新模型权重（梯度下降），只需要在 Prompt 中给它”演示”一下：</p><ul><li>**Zero-shot (上)**：不给例子，直接问。例如：”Translate English to French: cheese =&gt;”。这对模型要求最高。</li><li>**One-shot (中)**：给 1 个例子。例如：”sea otter =&gt; loutre de mer\n cheese =&gt;”。模型通过这一个例子”学会”了现在的任务是翻译。</li><li>**Few-shot (下)**：给 10-100 个例子。这能极大地提升模型性能。<br><strong>本质</strong>：GPT-3 将”学习”过程从”更新权重”变成了”读取上下文”。</li></ul><h3 id="3-2-Scaling-Laws-规模法则"><a href="#3-2-Scaling-Laws-规模法则" class="headerlink" title="3.2 Scaling Laws (规模法则)"></a>3.2 Scaling Laws (规模法则)</h3><p>OpenAI 在训练 GPT-3 时发现了一个惊人的规律。</p><p><img src="/images/gpt_series/gpt_scaling_law.png" alt="Scaling Laws 曲线"></p><p><strong>图表深度解读</strong>：</p><ul><li>这三张图展示了 Loss（测试误差）与三个变量的关系：**计算量(C)、数据集大小(D)、参数量(N)**。</li><li><strong>双对数坐标下的直线</strong>：这三条线在双对数坐标系下几乎是完美的直线！</li><li>**幂律分布 (Power Law)**：这意味着性能的提升与投入资源呈幂律关系（$L \propto N^{-\alpha}$）。</li><li><strong>启示</strong>：只要你增加参数、增加数据、增加算力，模型的效果就会<strong>可预测地</strong>变好。这给了 OpenAI 巨大的信心去烧钱训练 GPT-4。</li></ul><h3 id="3-3-参数量对比"><a href="#3-3-参数量对比" class="headerlink" title="3.3 参数量对比"></a>3.3 参数量对比</h3><p><img src="/images/gpt_series/gpt_params_comparison.png" alt="GPT 参数量对比柱状图"></p><p><strong>图表深度解读</strong>：</p><ul><li>这是一张极具视觉冲击力的对比图。</li><li>左边微小的蓝色方块是 GPT-1 (117M)。</li><li>中间稍微大一点的绿色方块是 GPT-2 (1.5B)。</li><li>右边那个巨大的、占据了整个画面的红色柱子是 GPT-3 (175B)。</li><li>GPT-3 的参数量是 GPT-2 的 117 倍！这种暴力美学彻底改变了 AI 领域的游戏规则。</li></ul><hr><p><a id="evolution"></a></p><h2 id="🔷-三代演进对比总结"><a href="#🔷-三代演进对比总结" class="headerlink" title="🔷 三代演进对比总结"></a>🔷 三代演进对比总结</h2><h3 id="任务适配方法演进"><a href="#任务适配方法演进" class="headerlink" title="任务适配方法演进"></a>任务适配方法演进</h3><p>我们如何让模型为我们工作？三代模型给出了不同的答案。</p><p><img src="/images/gpt_series/adaptation_comparison.png" alt="任务适配方法演进"></p><p><strong>图表深度解读</strong>：</p><ul><li><strong>GPT-1 (左)<strong>：</strong>Fine-tuning</strong>。模型是通用的，但使用时需要改变模型结构（加分类头）并重新训练权重。缺点是每个任务都需要存一份模型副本。</li><li><strong>GPT-2 (中)<strong>：</strong>Zero-shot</strong>。完全不改变模型，通过构造 Prompt 诱导模型输出。但效果往往不如微调。</li><li><strong>GPT-3 (右)<strong>：</strong>Few-shot ICL</strong>。不改变权重，但在输入中加入少量示例（Context）。这结合了前两者的优点：既不需要训练，又能通过示例让模型快速适应特定任务。</li></ul><h3 id="全面对比表"><a href="#全面对比表" class="headerlink" title="全面对比表"></a>全面对比表</h3><p><img src="/images/gpt_series/gpt_comparison_table.png" alt="GPT 三代参数对比表"></p><p>这张表格总结了三代模型的关键参数。注意 <strong>Context Window (上下文窗口)</strong> 的变化：从 512 到 1024 再到 2048。这意味着模型能”记住”更长的对话历史。</p><hr><p><a id="code"></a></p><h2 id="💻-代码实现参考"><a href="#💻-代码实现参考" class="headerlink" title="💻 代码实现参考"></a>💻 代码实现参考</h2><p>以下是一个简化的 GPT Block 实现（PyTorch），展示了 Masked Attention 的核心逻辑：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GPTBlock</span>(nn.Module):</span><br><span class="line">    <span class="string">"""GPT-2 style Transformer block with Pre-LN"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, hidden_size=<span class="number">768</span>, num_heads=<span class="number">12</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.ln1 = nn.LayerNorm(hidden_size)</span><br><span class="line">        self.attn = nn.MultiheadAttention(hidden_size, num_heads, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.ln2 = nn.LayerNorm(hidden_size)</span><br><span class="line">        self.ffn = nn.Sequential(</span><br><span class="line">            nn.Linear(hidden_size, <span class="number">4</span> * hidden_size),</span><br><span class="line">            nn.GELU(),</span><br><span class="line">            nn.Linear(<span class="number">4</span> * hidden_size, hidden_size)</span><br><span class="line">        )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, attn_mask=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="comment"># 1. Pre-LN + Attention + Residual</span></span><br><span class="line">        normed = self.ln1(x)</span><br><span class="line">        <span class="comment"># attn_mask 必须是下三角矩阵，防止看到未来</span></span><br><span class="line">        attn_out, _ = self.attn(normed, normed, normed, attn_mask=attn_mask)</span><br><span class="line">        x = x + attn_out</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. Pre-LN + FFN + Residual</span></span><br><span class="line">        x = x + self.ffn(self.ln2(x))</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></tbody></table></figure><hr><p><a id="feynman"></a></p><h2 id="🎓-费曼总结：教给小白听"><a href="#🎓-费曼总结：教给小白听" class="headerlink" title="🎓 费曼总结：教给小白听"></a>🎓 费曼总结：教给小白听</h2><h3 id="🍎-文字接龙的终极形态"><a href="#🍎-文字接龙的终极形态" class="headerlink" title="🍎 文字接龙的终极形态"></a>🍎 文字接龙的终极形态</h3><p><strong>想象你在和一个超级博学的朋友玩”接龙”游戏：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">你说：     "从前有座山，山上有座庙，庙里有个..."</span><br><span class="line">朋友接：   "老和尚"</span><br><span class="line">你继续：   "老和尚在给小和尚讲..."  </span><br><span class="line">朋友接：   "故事"</span><br></pre></td></tr></tbody></table></figure><p><strong>GPT 就是这样一个”接龙高手”：</strong></p><ol><li><strong>读书破万卷</strong>：它读了互联网上几乎所有的文字（书籍、网页、代码…）。</li><li><strong>统计大师</strong>：它不一定懂逻辑，但它知道”如果前面是A，后面大概率是B”。</li><li><strong>大力出奇迹</strong>：当它读的书足够多（TB级数据）、脑容量足够大（1750亿参数）时，它为了猜对下一个词，被迫”学会”了逻辑、数学、翻译甚至编程。</li></ol><h3 id="🔑-核心结论"><a href="#🔑-核心结论" class="headerlink" title="🔑 核心结论"></a>🔑 核心结论</h3><ol><li><strong>GPT 的本质</strong>：一个超级强大的”文字接龙”程序。</li><li><strong>智能的来源</strong>：为了完美地预测下一个词，模型必须构建对世界的认知模型。</li><li><strong>未来的方向</strong>：Scaling Laws 告诉我们，继续把模型做大、数据喂多，它还会变得更聪明。</li></ol><hr><p><em>参考论文</em>：</p><ol><li><em>Improving Language Understanding by Generative Pre-Training (2018)</em></li><li><em>Language Models are Unsupervised Multitask Learners (2019)</em></li><li><em>Language Models are Few-Shot Learners (2020)</em></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Deep Learning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>BERT 完整解析：从论文到 KV Cache</title>
      <link href="/2026/02/03/bert-comprehensive-guide/"/>
      <url>/2026/02/03/bert-comprehensive-guide/</url>
      
        <content type="html"><![CDATA[<h1 id="BERT-完整解析：从论文到-KV-Cache"><a href="#BERT-完整解析：从论文到-KV-Cache" class="headerlink" title="BERT 完整解析：从论文到 KV Cache"></a>BERT 完整解析：从论文到 KV Cache</h1><blockquote><p><strong>学习目标</strong>：深度理解 BERT 论文核心原理、Q/K/V 交互机制、KV Cache 优化技术、Causal Attention，以及完整的训练闭环。</p></blockquote><hr><h2 id="🖼️-核心可视化图解"><a href="#🖼️-核心可视化图解" class="headerlink" title="🖼️ 核心可视化图解"></a>🖼️ 核心可视化图解</h2><h3 id="BERT-整体架构与核心概念"><a href="#BERT-整体架构与核心概念" class="headerlink" title="BERT 整体架构与核心概念"></a>BERT 整体架构与核心概念</h3><p><img src="/images/bert_guide/bert_complete_visualizations.png">)</p><h3 id="BERT-架构详解"><a href="#BERT-架构详解" class="headerlink" title="BERT 架构详解"></a>BERT 架构详解</h3><p><img src="/images/bert_guide/bert_architecture.png">)</p><h3 id="Q-x2F-K-x2F-V-数据流可视化"><a href="#Q-x2F-K-x2F-V-数据流可视化" class="headerlink" title="Q/K/V 数据流可视化"></a>Q/K/V 数据流可视化</h3><p><img src="/images/bert_guide/qkv_dataflow.png">)</p><h3 id="注意力机制详解"><a href="#注意力机制详解" class="headerlink" title="注意力机制详解"></a>注意力机制详解</h3><p><img src="/images/bert_guide/attention_mechanisms.png">)</p><h3 id="Causal-vs-Bidirectional-Attention-对比"><a href="#Causal-vs-Bidirectional-Attention-对比" class="headerlink" title="Causal vs Bidirectional Attention 对比"></a>Causal vs Bidirectional Attention 对比</h3><p><img src="/images/bert_guide/causal_vs_bidirectional.png">)</p><hr><h2 id="🆚-BERT-vs-“Attention-Is-All-You-Need”-对比分析"><a href="#🆚-BERT-vs-“Attention-Is-All-You-Need”-对比分析" class="headerlink" title="🆚 BERT vs “Attention Is All You Need” 对比分析"></a>🆚 BERT vs “Attention Is All You Need” 对比分析</h2><h3 id="论文基本信息对比"><a href="#论文基本信息对比" class="headerlink" title="论文基本信息对比"></a>论文基本信息对比</h3><table><thead><tr><th>维度</th><th>Attention Is All You Need</th><th>BERT</th></tr></thead><tbody><tr><td><strong>发表时间</strong></td><td>2017年6月</td><td>2018年10月</td></tr><tr><td><strong>作者团队</strong></td><td>Google Brain + Google Research</td><td>Google AI Language</td></tr><tr><td><strong>核心贡献</strong></td><td>提出 Transformer 架构</td><td>提出预训练-微调范式</td></tr><tr><td><strong>引用量</strong></td><td>10万+</td><td>9万+</td></tr><tr><td><strong>地位</strong></td><td>奠基之作</td><td>应用突破</td></tr></tbody></table><h3 id="架构关系"><a href="#架构关系" class="headerlink" title="架构关系"></a>架构关系</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Attention Is All You Need (2017)</span><br><span class="line">        │</span><br><span class="line">        ▼ 提供了核心架构</span><br><span class="line">   ┌────────────────────────────────┐</span><br><span class="line">   │     Transformer 架构           │</span><br><span class="line">   │  • Multi-Head Attention        │</span><br><span class="line">   │  • Position Encoding           │</span><br><span class="line">   │  • Layer Normalization         │</span><br><span class="line">   │  • Feed-Forward Network        │</span><br><span class="line">   │  • Encoder + Decoder           │</span><br><span class="line">   └────────────────────────────────┘</span><br><span class="line">        │</span><br><span class="line">        ▼ BERT 只用 Encoder 部分</span><br><span class="line">   ┌────────────────────────────────┐</span><br><span class="line">   │          BERT (2018)           │</span><br><span class="line">   │  • 只用 Transformer Encoder    │</span><br><span class="line">   │  • 加入 MLM 预训练任务         │</span><br><span class="line">   │  • 提出预训练+微调范式         │</span><br><span class="line">   │  • 双向注意力                  │</span><br><span class="line">   └────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><h3 id="核心区别"><a href="#核心区别" class="headerlink" title="核心区别"></a>核心区别</h3><table><thead><tr><th>维度</th><th>Transformer 原论文</th><th>BERT</th></tr></thead><tbody><tr><td><strong>架构</strong></td><td>Encoder + Decoder</td><td>仅 Encoder</td></tr><tr><td><strong>任务</strong></td><td>机器翻译（Seq2Seq）</td><td>语言理解（分类、NER、QA）</td></tr><tr><td><strong>注意力</strong></td><td>Encoder双向，Decoder单向</td><td>全部双向</td></tr><tr><td><strong>预训练</strong></td><td>无（监督训练）</td><td>MLM + NSP</td></tr><tr><td><strong>Position Encoding</strong></td><td>固定的正弦函数</td><td>可学习的 Embedding</td></tr><tr><td><strong>应用</strong></td><td>需要成对数据（中英翻译）</td><td>通用特征提取器</td></tr></tbody></table><h3 id="BERT-继承了什么？"><a href="#BERT-继承了什么？" class="headerlink" title="BERT 继承了什么？"></a>BERT 继承了什么？</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Multi-Head Self-Attention</span></span><br><span class="line">Q = X @ W_Q</span><br><span class="line">K = X @ W_K</span><br><span class="line">V = X @ W_V</span><br><span class="line">Attention(Q, K, V) = softmax(QK^T / √d_k)V</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Layer Normalization + 残差连接</span></span><br><span class="line">output = LayerNorm(X + Attention(X))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. Feed-Forward Network</span></span><br><span class="line">FFN(x) = <span class="built_in">max</span>(<span class="number">0</span>, xW₁ + b₁)W₂ + b₂</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 整体结构</span></span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">12</span>):</span><br><span class="line">    X = LayerNorm(X + MultiHeadAttention(X))</span><br><span class="line">    X = LayerNorm(X + FFN(X))</span><br></pre></td></tr></tbody></table></figure><h3 id="BERT-的创新点"><a href="#BERT-的创新点" class="headerlink" title="BERT 的创新点"></a>BERT 的创新点</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">创新 1: 只用 Encoder</span><br><span class="line">       原因：NLP 大多数任务是"理解"，不是"生成"</span><br><span class="line"></span><br><span class="line">创新 2: MLM 预训练任务</span><br><span class="line">       让模型学会双向上下文（Transformer 原论文没有预训练）</span><br><span class="line"></span><br><span class="line">创新 3: 预训练-微调范式</span><br><span class="line">       一次预训练 → 多个任务复用</span><br><span class="line"></span><br><span class="line">创新 4: 可学习的位置编码</span><br><span class="line">       Position Embedding 是训练出来的，不是固定公式</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="🆚-BERT-vs-Attention-Is-All-You-Need：论文对比"><a href="#🆚-BERT-vs-Attention-Is-All-You-Need：论文对比" class="headerlink" title="🆚 BERT vs Attention Is All You Need：论文对比"></a>🆚 BERT vs Attention Is All You Need：论文对比</h2><h3 id="论文基本信息对比-1"><a href="#论文基本信息对比-1" class="headerlink" title="论文基本信息对比"></a>论文基本信息对比</h3><table><thead><tr><th>维度</th><th>Attention Is All You Need</th><th>BERT</th></tr></thead><tbody><tr><td><strong>发表时间</strong></td><td>2017年6月</td><td>2018年10月</td></tr><tr><td><strong>作者团队</strong></td><td>Google Brain + Google Research</td><td>Google AI Language</td></tr><tr><td><strong>核心贡献</strong></td><td>提出 Transformer 架构</td><td>提出预训练-微调范式</td></tr><tr><td><strong>引用量</strong></td><td>100,000+</td><td>90,000+</td></tr><tr><td><strong>历史地位</strong></td><td>架构奠基之作</td><td>应用突破之作</td></tr></tbody></table><h3 id="核心关系：继承与创新"><a href="#核心关系：继承与创新" class="headerlink" title="核心关系：继承与创新"></a>核心关系：继承与创新</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Attention Is All You Need (2017)</span><br><span class="line">        │</span><br><span class="line">        ▼ 提供了核心架构</span><br><span class="line">   ┌────────────────────────────────┐</span><br><span class="line">   │     Transformer 架构           │</span><br><span class="line">   │  • Encoder-Decoder 结构        │</span><br><span class="line">   │  • Multi-Head Attention        │</span><br><span class="line">   │  • Position Encoding           │</span><br><span class="line">   │  • Layer Normalization         │</span><br><span class="line">   │  • Feed-Forward Network        │</span><br><span class="line">   └────────────────────────────────┘</span><br><span class="line">        │</span><br><span class="line">        ▼ BERT 只用 Encoder 部分</span><br><span class="line">   ┌────────────────────────────────┐</span><br><span class="line">   │          BERT (2018)           │</span><br><span class="line">   │  • 只用 Transformer Encoder    │</span><br><span class="line">   │  • 提出 MLM 预训练任务         │</span><br><span class="line">   │  • 提出预训练+微调范式         │</span><br><span class="line">   │  • 双向上下文建模              │</span><br><span class="line">   └────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><h3 id="架构对比"><a href="#架构对比" class="headerlink" title="架构对比"></a>架构对比</h3><table><thead><tr><th>维度</th><th>Transformer</th><th>BERT</th></tr></thead><tbody><tr><td><strong>架构</strong></td><td>Encoder + Decoder</td><td>仅 Encoder</td></tr><tr><td><strong>适用任务</strong></td><td>机器翻译（Seq2Seq）</td><td>分类、NER、问答</td></tr><tr><td><strong>注意力类型</strong></td><td>Encoder用双向，Decoder用单向</td><td>全部双向</td></tr><tr><td><strong>预训练任务</strong></td><td>无（需要平行语料）</td><td>MLM + NSP</td></tr><tr><td><strong>Position Embedding</strong></td><td>固定的正弦函数</td><td>可学习的向量</td></tr></tbody></table><h3 id="BERT-继承了什么？-1"><a href="#BERT-继承了什么？-1" class="headerlink" title="BERT 继承了什么？"></a>BERT 继承了什么？</h3><p><strong>✅ 完全继承</strong>：</p><ul><li>Multi-Head Self-Attention 机制</li><li>Feed-Forward Network (FFN)</li><li>残差连接 + Layer Normalization</li><li>Q/K/V 计算方式</li></ul><p><strong>🔧 改进部分</strong>：</p><ul><li>Position Embedding：从固定改为可学习</li><li>只用 Encoder，去掉 Decoder</li><li>添加 Segment Embedding</li><li>添加特殊 Token：<code>[CLS]</code>、<code>[SEP]</code>、<code>[MASK]</code></li></ul><hr><h2 id="🆚-BERT-vs-“Attention-Is-All-You-Need”-对比"><a href="#🆚-BERT-vs-“Attention-Is-All-You-Need”-对比" class="headerlink" title="🆚 BERT vs “Attention Is All You Need” 对比"></a>🆚 BERT vs “Attention Is All You Need” 对比</h2><h3 id="论文基本信息"><a href="#论文基本信息" class="headerlink" title="论文基本信息"></a>论文基本信息</h3><table><thead><tr><th>维度</th><th>Attention Is All You Need</th><th>BERT</th></tr></thead><tbody><tr><td><strong>发表时间</strong></td><td>2017年6月</td><td>2018年10月</td></tr><tr><td><strong>作者团队</strong></td><td>Google Brain + Google Research</td><td>Google AI Language</td></tr><tr><td><strong>核心贡献</strong></td><td>提出 Transformer 架构</td><td>提出预训练-微调范式</td></tr><tr><td><strong>引用量</strong></td><td>10万+</td><td>9万+</td></tr><tr><td><strong>地位</strong></td><td>奠基之作（架构创新）</td><td>应用突破（范式创新）</td></tr></tbody></table><h3 id="架构对比-1"><a href="#架构对比-1" class="headerlink" title="架构对比"></a>架构对比</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">Attention Is All You Need (2017) - 原始 Transformer</span><br><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                                                             │</span><br><span class="line">│   输入："我爱你"              输出："I love you"             │</span><br><span class="line">│        ↓                            ↑                       │</span><br><span class="line">│   ┌─────────┐                 ┌─────────┐                  │</span><br><span class="line">│   │ Encoder │ ───上下文───→   │ Decoder │                  │</span><br><span class="line">│   │(6层)    │                 │(6层)    │                  │</span><br><span class="line">│   │双向注意力│                 │单向注意力│                  │</span><br><span class="line">│   └─────────┘                 └─────────┘                  │</span><br><span class="line">│                                                             │</span><br><span class="line">│   用途：机器翻译（seq2seq）                                  │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br><span class="line"></span><br><span class="line">BERT (2018) - 只用 Encoder</span><br><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                                                             │</span><br><span class="line">│   输入："我爱[MASK]天安门"                                   │</span><br><span class="line">│        ↓                                                    │</span><br><span class="line">│   ┌─────────┐                                              │</span><br><span class="line">│   │ Encoder │ ──→ 直接输出每个位置的表示                    │</span><br><span class="line">│   │(12层)   │      ↓                                       │</span><br><span class="line">│   │双向注意力│      在 [MASK] 位置预测 "北京"                │</span><br><span class="line">│   └─────────┘                                              │</span><br><span class="line">│                                                             │</span><br><span class="line">│   用途：理解任务（分类、NER、QA）                            │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><h3 id="BERT-继承了-Transformer-的什么？"><a href="#BERT-继承了-Transformer-的什么？" class="headerlink" title="BERT 继承了 Transformer 的什么？"></a>BERT 继承了 Transformer 的什么？</h3><table><thead><tr><th>组件</th><th>Transformer</th><th>BERT</th><th>说明</th></tr></thead><tbody><tr><td><strong>Multi-Head Attention</strong></td><td>✅ 原创</td><td>✅ 完全继承</td><td>Q/K/V 机制一模一样</td></tr><tr><td><strong>Position Encoding</strong></td><td>✅ 正弦函数</td><td>⚠️ 改为可学习</td><td>BERT 用可训练的位置嵌入</td></tr><tr><td><strong>Layer Normalization</strong></td><td>✅ 原创</td><td>✅ 完全继承</td><td></td></tr><tr><td><strong>Feed-Forward Network</strong></td><td>✅ 原创</td><td>✅ 完全继承</td><td></td></tr><tr><td><strong>Encoder 结构</strong></td><td>✅ 6层</td><td>✅ 12/24层</td><td>BERT 加深了层数</td></tr><tr><td><strong>Decoder 结构</strong></td><td>✅ 6层</td><td>❌ 删除</td><td>BERT 不需要 Decoder</td></tr></tbody></table><h3 id="BERT-的创新点-1"><a href="#BERT-的创新点-1" class="headerlink" title="BERT 的创新点"></a>BERT 的创新点</h3><table><thead><tr><th>创新</th><th>说明</th></tr></thead><tbody><tr><td><strong>MLM 预训练任务</strong></td><td>Transformer 没有预训练，BERT 用 MLM 学习双向表示</td></tr><tr><td><strong>NSP 任务</strong></td><td>学习句子间关系（后续被证明用处不大）</td></tr><tr><td><strong>预训练+微调范式</strong></td><td>Transformer 是任务特定训练，BERT 开创迁移学习</td></tr><tr><td><strong>只用 Encoder</strong></td><td>Transformer 是完整 Encoder-Decoder，BERT 简化架构</td></tr></tbody></table><h3 id="联系：BERT-站在-Transformer-肩膀上"><a href="#联系：BERT-站在-Transformer-肩膀上" class="headerlink" title="联系：BERT 站在 Transformer 肩膀上"></a>联系：BERT 站在 Transformer 肩膀上</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">2017 Transformer 提供核心架构</span><br><span class="line">        │</span><br><span class="line">        ▼</span><br><span class="line">┌───────────────────────────────┐</span><br><span class="line">│  • Multi-Head Attention       │</span><br><span class="line">│  • Position Encoding          │</span><br><span class="line">│  • Feed-Forward Network       │</span><br><span class="line">│  • Encoder-Decoder 架构       │</span><br><span class="line">└───────────────────────────────┘</span><br><span class="line">        │</span><br><span class="line">        ▼ BERT 选择性使用</span><br><span class="line">┌───────────────────────────────┐</span><br><span class="line">│  ✅ 复用 Encoder 部分          │</span><br><span class="line">│  ✅ 复用 Attention 机制        │</span><br><span class="line">│  ❌ 删除 Decoder              │</span><br><span class="line">│  ➕ 加入 MLM 预训练            │</span><br><span class="line">│  ➕ 提出预训练-微调范式        │</span><br><span class="line">└───────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><p><strong>简单记忆</strong>：</p><ul><li><strong>Transformer</strong> = 提供了”工具箱”（架构组件）</li><li><strong>BERT</strong> = 用工具箱中的部分工具，发明了新的使用方法（预训练范式）</li></ul><hr><h2 id="🆚-BERT-vs-“Attention-Is-All-You-Need”-对比-1"><a href="#🆚-BERT-vs-“Attention-Is-All-You-Need”-对比-1" class="headerlink" title="🆚 BERT vs “Attention Is All You Need” 对比"></a>🆚 BERT vs “Attention Is All You Need” 对比</h2><h3 id="论文基本信息对比-2"><a href="#论文基本信息对比-2" class="headerlink" title="论文基本信息对比"></a>论文基本信息对比</h3><table><thead><tr><th>维度</th><th>Attention Is All You Need</th><th>BERT</th></tr></thead><tbody><tr><td><strong>发表时间</strong></td><td>2017年6月</td><td>2018年10月</td></tr><tr><td><strong>作者团队</strong></td><td>Google Brain + Google Research</td><td>Google AI Language</td></tr><tr><td><strong>核心贡献</strong></td><td>提出 Transformer 架构</td><td>提出预训练-微调范式</td></tr><tr><td><strong>架构</strong></td><td>Encoder + Decoder</td><td>仅 Encoder</td></tr><tr><td><strong>训练任务</strong></td><td>机器翻译（有监督）</td><td>MLM + NSP（自监督）</td></tr><tr><td><strong>引用量</strong></td><td>10万+</td><td>9万+</td></tr></tbody></table><h3 id="继承关系"><a href="#继承关系" class="headerlink" title="继承关系"></a>继承关系</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">Attention Is All You Need (2017)</span><br><span class="line">        ↓ 提供核心架构</span><br><span class="line">   ┌────────────────────────────────┐</span><br><span class="line">   │     Transformer 架构           │</span><br><span class="line">   │  • Multi-Head Attention        │</span><br><span class="line">   │  • Position Encoding           │</span><br><span class="line">   │  • Layer Normalization         │</span><br><span class="line">   │  • Feed-Forward Network        │</span><br><span class="line">   └────────────────────────────────┘</span><br><span class="line">        ↓ BERT 只用 Encoder 部分</span><br><span class="line">   ┌────────────────────────────────┐</span><br><span class="line">   │          BERT (2018)           │</span><br><span class="line">   │  • 12层 Transformer Encoder    │</span><br><span class="line">   │  • MLM 预训练任务              │</span><br><span class="line">   │  • 预训练+微调范式             │</span><br><span class="line">   └────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><h3 id="架构对比-2"><a href="#架构对比-2" class="headerlink" title="架构对比"></a>架构对比</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">原始 Transformer（翻译任务）：</span><br><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│   输入："我爱你"              输出："I love you"             │</span><br><span class="line">│        ↓                            ↑                       │</span><br><span class="line">│   ┌─────────┐                 ┌─────────┐                  │</span><br><span class="line">│   │ Encoder │ ───上下文───→  │ Decoder │                  │</span><br><span class="line">│   │(理解输入)│                │(生成输出)│                  │</span><br><span class="line">│   └─────────┘                 └─────────┘                  │</span><br><span class="line">│   双向注意力                   单向注意力(Causal)            │</span><br><span class="line">│   无需 KV Cache                需要 KV Cache                │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br><span class="line"></span><br><span class="line">BERT（理解任务）：</span><br><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│   输入："我爱[MASK]天安门"                                   │</span><br><span class="line">│        ↓                                                    │</span><br><span class="line">│   ┌─────────┐                                              │</span><br><span class="line">│   │ Encoder │ ──→ 直接输出每个位置的表示                    │</span><br><span class="line">│   │(理解输入)│      ↓                                       │</span><br><span class="line">│   └─────────┘      在 [MASK] 位置预测 "北京"                │</span><br><span class="line">│   双向注意力                                                │</span><br><span class="line">│   无需 KV Cache                                             │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><h3 id="核心区别总结"><a href="#核心区别总结" class="headerlink" title="核心区别总结"></a>核心区别总结</h3><table><thead><tr><th>维度</th><th>Transformer (原论文)</th><th>BERT</th></tr></thead><tbody><tr><td><strong>架构选择</strong></td><td>Encoder + Decoder</td><td>仅 Encoder</td></tr><tr><td><strong>注意力模式</strong></td><td>Encoder双向 + Decoder单向</td><td>全部双向</td></tr><tr><td><strong>适用任务</strong></td><td>序列到序列（翻译）</td><td>理解类任务</td></tr><tr><td><strong>训练数据</strong></td><td>平行语料（需标注）</td><td>大规模文本（无需标注）</td></tr><tr><td><strong>KV Cache</strong></td><td>Decoder需要</td><td>不需要</td></tr><tr><td><strong>影响</strong></td><td>奠定架构基础</td><td>开创预训练范式</td></tr></tbody></table><hr><h2 id="📄-Part-1-BERT-论文深度解析"><a href="#📄-Part-1-BERT-论文深度解析" class="headerlink" title="📄 Part 1: BERT 论文深度解析"></a>📄 Part 1: BERT 论文深度解析</h2><h3 id="1-1-论文基本信息"><a href="#1-1-论文基本信息" class="headerlink" title="1.1 论文基本信息"></a>1.1 论文基本信息</h3><p><strong>标题</strong>: BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding<br><strong>作者</strong>: Jacob Devlin, Ming-Wei Chang, Kenton Lee, Kristina Toutanova (Google AI Language)<br><strong>发表</strong>: NAACL 2019<br><strong>论文链接</strong>: <a href="https://arxiv.org/abs/1810.04805">https://arxiv.org/abs/1810.04805</a></p><hr><h3 id="1-2-研究动机：为什么需要-BERT？"><a href="#1-2-研究动机：为什么需要-BERT？" class="headerlink" title="1.2 研究动机：为什么需要 BERT？"></a>1.2 研究动机：为什么需要 BERT？</h3><p>在 BERT 之前，NLP 预训练模型存在两大局限：</p><h4 id="问题-1：单向语言模型的局限性"><a href="#问题-1：单向语言模型的局限性" class="headerlink" title="问题 1：单向语言模型的局限性"></a>问题 1：单向语言模型的局限性</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">GPT-1 (2018):  只能从左往右看</span><br><span class="line">输入: "我 爱 北京 天安门"</span><br><span class="line">     ↓   ↓   ↓    ↓</span><br><span class="line">每个词只能看到左边的上下文</span><br><span class="line"></span><br><span class="line">问题: "银行" 在 "我去银行存钱" vs "河边的银行很陡" 中</span><br><span class="line">     如果只看左边，无法区分是 "金融机构" 还是 "河岸"</span><br></pre></td></tr></tbody></table></figure><h4 id="问题-2：浅层双向的局限性"><a href="#问题-2：浅层双向的局限性" class="headerlink" title="问题 2：浅层双向的局限性"></a>问题 2：浅层双向的局限性</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">ELMo (2018): 使用两个独立的 LSTM</span><br><span class="line">    → LSTM (从左往右)</span><br><span class="line">    → LSTM (从右往左)</span><br><span class="line">    → 最后拼接</span><br><span class="line"></span><br><span class="line">问题: 两个方向的信息只在最顶层融合，中间层无法深度交互</span><br></pre></td></tr></tbody></table></figure><p><strong>BERT 的核心创新</strong>: 通过 Masked Language Model (MLM)，在<strong>每一层</strong>都实现真正的双向上下文建模。</p><hr><h3 id="1-3-核心方法：两个预训练任务"><a href="#1-3-核心方法：两个预训练任务" class="headerlink" title="1.3 核心方法：两个预训练任务"></a>1.3 核心方法：两个预训练任务</h3><h4 id="任务-1-Masked-Language-Model-MLM-核心"><a href="#任务-1-Masked-Language-Model-MLM-核心" class="headerlink" title="任务 1: Masked Language Model (MLM) - 核心"></a>任务 1: Masked Language Model (MLM) - 核心</h4><p><strong>操作流程</strong>:</p><ol><li>随机选择 15% 的 token 进行 mask</li><li>其中：<ul><li>80% 替换为 <code>[MASK]</code></li><li>10% 替换为随机词</li><li>10% 保持不变</li></ul></li></ol><p><strong>为什么这样设计？</strong></p><ul><li>80% <code>[MASK]</code>: 让模型学习预测</li><li>10% 随机词: 避免模型只依赖 <code>[MASK]</code> 标记</li><li>10% 不变: 让模型学习真实分布</li></ul><p><strong>例子</strong>:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">原始句子: "我 爱 北京 天安门"</span><br><span class="line">处理后:   "我 爱 [MASK] 天安门"   (80%)</span><br><span class="line">或:       "我 爱 上海 天安门"     (10% 随机)</span><br><span class="line">或:       "我 爱 北京 天安门"     (10% 不变)</span><br><span class="line"></span><br><span class="line">Label: 位置 3 = "北京"</span><br><span class="line">Loss = CrossEntropy(model_output[3], ID("北京"))</span><br></pre></td></tr></tbody></table></figure><p><strong>维度分析</strong>:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">输入: [batch, seq_len] = [<span class="number">32</span>, <span class="number">128</span>]</span><br><span class="line">     ↓ Embedding</span><br><span class="line">     [<span class="number">32</span>, <span class="number">128</span>, <span class="number">768</span>]</span><br><span class="line">     ↓ <span class="number">12</span> 层 Transformer Encoder</span><br><span class="line">     [<span class="number">32</span>, <span class="number">128</span>, <span class="number">768</span>]</span><br><span class="line">     ↓ MLM Head (Linear + Softmax)</span><br><span class="line">     [<span class="number">32</span>, <span class="number">128</span>, <span class="number">21128</span>]  <span class="comment"># 21128 = 词表大小</span></span><br><span class="line">     </span><br><span class="line">只计算被 mask 位置的 Loss</span><br></pre></td></tr></tbody></table></figure><h4 id="任务-2-Next-Sentence-Prediction-NSP"><a href="#任务-2-Next-Sentence-Prediction-NSP" class="headerlink" title="任务 2: Next Sentence Prediction (NSP)"></a>任务 2: Next Sentence Prediction (NSP)</h4><p><strong>目的</strong>: 学习句子间关系</p><p><strong>输入格式</strong>:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">[CLS] 句子A [SEP] 句子B [SEP]</span><br><span class="line"></span><br><span class="line">正样本: B 确实是 A 的下一句 (Label = 1)</span><br><span class="line">负样本: B 是随机选的句子 (Label = 0)</span><br></pre></td></tr></tbody></table></figure><p><strong>例子</strong>:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">正样本:</span><br><span class="line">Input: [CLS] 今天天气很好 [SEP] 我们去公园吧 [SEP]</span><br><span class="line">Label: IsNext (1)</span><br><span class="line"></span><br><span class="line">负样本:</span><br><span class="line">Input: [CLS] 今天天气很好 [SEP] 人工智能很有趣 [SEP]</span><br><span class="line">Label: NotNext (0)</span><br></pre></td></tr></tbody></table></figure><p><strong>Loss 计算</strong>:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">cls_output = encoder_output[:, <span class="number">0</span>, :]  <span class="comment"># [batch, 768] 取 [CLS] 位置</span></span><br><span class="line">logits = nsp_classifier(cls_output)   <span class="comment"># [batch, 2]</span></span><br><span class="line">loss = BinaryCrossEntropy(logits, labels)</span><br></pre></td></tr></tbody></table></figure><p><strong>后续研究发现</strong>: NSP 任务效果有限，RoBERTa 等后续工作移除了这个任务。</p><hr><h3 id="1-3-1-MLM-深度解析：完形填空的艺术"><a href="#1-3-1-MLM-深度解析：完形填空的艺术" class="headerlink" title="1.3.1 MLM 深度解析：完形填空的艺术"></a>1.3.1 MLM 深度解析：完形填空的艺术</h3><h4 id="通俗理解"><a href="#通俗理解" class="headerlink" title="通俗理解"></a>通俗理解</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">小学语文题：</span><br><span class="line">  "小明 _____ 学校上课"</span><br><span class="line">  </span><br><span class="line">答案：去、到、在...</span><br><span class="line"></span><br><span class="line">BERT 的 MLM 就是让 AI 做这种"完形填空"！</span><br></pre></td></tr></tbody></table></figure><h4 id="具体操作流程"><a href="#具体操作流程" class="headerlink" title="具体操作流程"></a>具体操作流程</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 原始句子</span></span><br><span class="line">原句 = <span class="string">"我爱北京天安门"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: 随机选择 15% 的词进行处理</span></span><br><span class="line">选中 = <span class="string">"北京"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: 对选中的词进行三种处理（随机选一种）</span></span><br><span class="line">处理后 = <span class="string">"我爱 [MASK] 天安门"</span>   <span class="comment"># 80% 概率：替换为 [MASK]</span></span><br><span class="line">或者   = <span class="string">"我爱 上海 天安门"</span>     <span class="comment"># 10% 概率：替换为随机词</span></span><br><span class="line">或者   = <span class="string">"我爱 北京 天安门"</span>     <span class="comment"># 10% 概率：保持不变</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: 让模型预测被遮住的词是什么</span></span><br><span class="line">模型输入 = <span class="string">"我爱 [MASK] 天安门"</span></span><br><span class="line">模型输出 = <span class="string">"北京"</span> ✅  (如果预测对了，loss 很小)</span><br><span class="line">         = <span class="string">"上海"</span> ❌  (如果预测错了，loss 很大)</span><br></pre></td></tr></tbody></table></figure><h4 id="为什么-MLM-能让-BERT-学会”理解语言”？"><a href="#为什么-MLM-能让-BERT-学会”理解语言”？" class="headerlink" title="为什么 MLM 能让 BERT 学会”理解语言”？"></a>为什么 MLM 能让 BERT 学会”理解语言”？</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">场景 1：</span><br><span class="line">  输入："我去 [MASK] 存钱"</span><br><span class="line">  模型学会：看到"存钱" → 预测"银行"（金融机构）</span><br><span class="line"></span><br><span class="line">场景 2：</span><br><span class="line">  输入："河边的 [MASK] 很陡峭"  </span><br><span class="line">  模型学会：看到"河边""陡峭" → 预测"河岸/堤坝"</span><br><span class="line"></span><br><span class="line">通过数十亿次这样的"完形填空"训练后：</span><br><span class="line">  → 模型学会了词语之间的关系</span><br><span class="line">  → 模型学会了语法结构</span><br><span class="line">  → 模型学会了常识知识</span><br></pre></td></tr></tbody></table></figure><h4 id="MLM-训练代码示例"><a href="#MLM-训练代码示例" class="headerlink" title="MLM 训练代码示例"></a>MLM 训练代码示例</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForMaskedLM</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-chinese'</span>)</span><br><span class="line">model = BertForMaskedLM.from_pretrained(<span class="string">'bert-base-chinese'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入一个带 [MASK] 的句子</span></span><br><span class="line">text = <span class="string">"我爱[MASK]天安门"</span></span><br><span class="line">inputs = tokenizer(text, return_tensors=<span class="string">'pt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = model(**inputs)</span><br><span class="line">    predictions = outputs.logits</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找到 [MASK] 位置的预测结果</span></span><br><span class="line">mask_index = (inputs[<span class="string">'input_ids'</span>] == tokenizer.mask_token_id).nonzero()[<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">predicted_token_id = predictions[<span class="number">0</span>, mask_index].argmax(dim=-<span class="number">1</span>)</span><br><span class="line">predicted_token = tokenizer.decode(predicted_token_id)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"原句: <span class="subst">{text}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"预测: <span class="subst">{predicted_token}</span>"</span>)  <span class="comment"># 输出: 北京</span></span><br></pre></td></tr></tbody></table></figure><hr><h3 id="1-3-2-预训练-微调范式详解"><a href="#1-3-2-预训练-微调范式详解" class="headerlink" title="1.3.2 预训练-微调范式详解"></a>1.3.2 预训练-微调范式详解</h3><h4 id="通俗理解：培养”通才”再培养”专才”"><a href="#通俗理解：培养”通才”再培养”专才”" class="headerlink" title="通俗理解：培养”通才”再培养”专才”"></a>通俗理解：培养”通才”再培养”专才”</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">传统方式（从零开始）：</span><br><span class="line">  ┌─────────────────────────────────────────────────┐</span><br><span class="line">  │  任务：情感分析                                   │</span><br><span class="line">  │  数据：10万条电商评论                             │</span><br><span class="line">  │  训练：从随机初始化开始，训练一个专门的模型         │</span><br><span class="line">  │  耗时：3天，需要大量标注数据                       │</span><br><span class="line">  └─────────────────────────────────────────────────┘</span><br><span class="line">  </span><br><span class="line">  ┌─────────────────────────────────────────────────┐</span><br><span class="line">  │  任务：垃圾邮件检测                               │</span><br><span class="line">  │  数据：5万封邮件                                  │</span><br><span class="line">  │  训练：从随机初始化开始，再训练另一个专门的模型     │</span><br><span class="line">  │  耗时：2天，又需要大量标注数据                     │</span><br><span class="line">  └─────────────────────────────────────────────────┘</span><br><span class="line">  </span><br><span class="line">  问题：每个任务都要从头训练，重复劳动！</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">微调范式（站在巨人肩膀上）：</span><br><span class="line"></span><br><span class="line">  第一阶段：预训练（Pre-training）—— 只做一次！</span><br><span class="line">  ┌─────────────────────────────────────────────────┐</span><br><span class="line">  │  数据：整个维基百科 + 大量书籍 (数十亿词)          │</span><br><span class="line">  │  任务：MLM（完形填空）                            │</span><br><span class="line">  │  目的：让模型学会"理解语言"                       │</span><br><span class="line">  │  耗时：数周 (但只需做一次，由 Google 完成)         │</span><br><span class="line">  │  产出：BERT 预训练模型 ✨                         │</span><br><span class="line">  └─────────────────────────────────────────────────┘</span><br><span class="line">                    │</span><br><span class="line">                    ▼ 下载现成的 BERT</span><br><span class="line">                    </span><br><span class="line">  第二阶段：微调（Fine-tuning）—— 每个任务只需几小时！</span><br><span class="line">  ┌─────────────────────────────────────────────────┐</span><br><span class="line">  │  任务A：情感分析                                  │</span><br><span class="line">  │  数据：仅需 1000 条标注数据！                     │</span><br><span class="line">  │  方法：在 BERT 上加一个分类层，微调几轮            │</span><br><span class="line">  │  耗时：30分钟                                    │</span><br><span class="line">  └─────────────────────────────────────────────────┘</span><br><span class="line">  </span><br><span class="line">  ┌─────────────────────────────────────────────────┐</span><br><span class="line">  │  任务B：垃圾邮件检测                              │</span><br><span class="line">  │  数据：仅需 500 条标注数据！                      │</span><br><span class="line">  │  方法：同样加分类层，微调几轮                      │</span><br><span class="line">  │  耗时：20分钟                                    │</span><br><span class="line">  └─────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><h4 id="形象类比总结"><a href="#形象类比总结" class="headerlink" title="形象类比总结"></a>形象类比总结</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                      🎓 教育类比                             │</span><br><span class="line">├─────────────────────────────────────────────────────────────┤</span><br><span class="line">│                                                             │</span><br><span class="line">│  【预训练 Pre-training】= 上大学，接受通识教育               │</span><br><span class="line">│     • 学习语文、数学、英语、物理...                          │</span><br><span class="line">│     • 目标：成为一个有基础知识的"通才"                       │</span><br><span class="line">│     • 时间：4年                                             │</span><br><span class="line">│     • 成本：高                                              │</span><br><span class="line">│                                                             │</span><br><span class="line">│  【MLM 任务】= 大学里的各种练习题                            │</span><br><span class="line">│     • 完形填空、阅读理解、语法练习...                        │</span><br><span class="line">│     • 目标：锻炼语言理解能力                                 │</span><br><span class="line">│                                                             │</span><br><span class="line">│  【微调 Fine-tuning】= 工作后的岗位培训                      │</span><br><span class="line">│     • 针对具体工作（情感分析/问答/翻译）学习                  │</span><br><span class="line">│     • 目标：成为某个领域的"专才"                             │</span><br><span class="line">│     • 时间：几天                                            │</span><br><span class="line">│     • 成本：低（因为已经有基础了）                           │</span><br><span class="line">│                                                             │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                    💡 为什么这样更好？                       │</span><br><span class="line">├─────────────────────────────────────────────────────────────┤</span><br><span class="line">│                                                             │</span><br><span class="line">│  传统方式：每换一个工作就重新上一次大学                       │</span><br><span class="line">│     → 太慢、太贵、太浪费                                    │</span><br><span class="line">│                                                             │</span><br><span class="line">│  微调范式：大学只上一次，换工作只需短期培训                   │</span><br><span class="line">│     → 快速、便宜、高效                                      │</span><br><span class="line">│                                                             │</span><br><span class="line">│  数据对比：                                                 │</span><br><span class="line">│     • 传统：需要 10万+ 标注数据                             │</span><br><span class="line">│     • 微调：只需 1000 条标注数据就能达到相似效果！            │</span><br><span class="line">│                                                             │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><h4 id="情感分析微调完整示例"><a href="#情感分析微调完整示例" class="headerlink" title="情感分析微调完整示例"></a>情感分析微调完整示例</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ 第一步：准备少量标注数据 ============</span></span><br><span class="line">train_data = {</span><br><span class="line">    <span class="string">"text"</span>: [</span><br><span class="line">        <span class="string">"这个产品太棒了，强烈推荐！"</span>,</span><br><span class="line">        <span class="string">"质量很差，用了一天就坏了"</span>,</span><br><span class="line">        <span class="string">"一般般，没有惊喜也没有失望"</span>,</span><br><span class="line">        <span class="string">"超级喜欢，已经回购三次"</span>,</span><br><span class="line">        <span class="string">"客服态度很差，再也不买了"</span>,</span><br><span class="line">        <span class="string">"性价比很高，值得购买"</span>,</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"label"</span>: [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]  <span class="comment"># 1=正面, 0=负面</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ 第二步：加载预训练的 BERT ============</span></span><br><span class="line"><span class="comment"># 这个 BERT 已经通过 MLM 任务学会了"理解中文"</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-chinese'</span>)</span><br><span class="line">model = BertForSequenceClassification.from_pretrained(</span><br><span class="line">    <span class="string">'bert-base-chinese'</span>, </span><br><span class="line">    num_labels=<span class="number">2</span>  <span class="comment"># 正面/负面 两个类别</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ 第三步：数据预处理 ============</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_function</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">"text"</span>], padding=<span class="string">"max_length"</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">dataset = Dataset.from_dict(train_data)</span><br><span class="line">tokenized_dataset = dataset.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ 第四步：微调训练 ============</span></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">"./results"</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,           <span class="comment"># 只需要训练 3 轮！</span></span><br><span class="line">    per_device_train_batch_size=<span class="number">4</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,           <span class="comment"># 很小的学习率，轻微调整</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=tokenized_dataset,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()  <span class="comment"># 几分钟就完成！</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ 第五步：使用微调后的模型 ============</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">text</span>):</span><br><span class="line">    inputs = tokenizer(text, return_tensors=<span class="string">"pt"</span>, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>)</span><br><span class="line">    outputs = model(**inputs)</span><br><span class="line">    prediction = outputs.logits.argmax(dim=-<span class="number">1</span>).item()</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"正面 😊"</span> <span class="keyword">if</span> prediction == <span class="number">1</span> <span class="keyword">else</span> <span class="string">"负面 😞"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="built_in">print</span>(predict(<span class="string">"这款手机拍照效果惊艳"</span>))  <span class="comment"># → 正面 😊</span></span><br><span class="line"><span class="built_in">print</span>(predict(<span class="string">"物流太慢了，等了一周"</span>))   <span class="comment"># → 负面 😞</span></span><br></pre></td></tr></tbody></table></figure><hr><h3 id="1-4-论文关键创新点深度剖析"><a href="#1-4-论文关键创新点深度剖析" class="headerlink" title="1.4 论文关键创新点深度剖析"></a>1.4 论文关键创新点深度剖析</h3><h4 id="🔬-创新点-1：深度双向上下文建模"><a href="#🔬-创新点-1：深度双向上下文建模" class="headerlink" title="🔬 创新点 1：深度双向上下文建模"></a>🔬 创新点 1：深度双向上下文建模</h4><table><thead><tr><th>模型</th><th>上下文方向</th><th>问题</th></tr></thead><tbody><tr><td>GPT-1</td><td>单向 (左→右)</td><td>无法利用右侧信息</td></tr><tr><td>ELMo</td><td>浅层双向 (拼接)</td><td>两个方向仅在顶层融合</td></tr><tr><td><strong>BERT</strong></td><td><strong>深度双向</strong></td><td>✅ 每一层都能看到完整上下文</td></tr></tbody></table><p><strong>技术实现关键</strong>：通过 MLM 任务，模型可以”作弊”地看到被预测词的两侧信息。</p><h4 id="🔬-创新点-2：预训练-微调范式"><a href="#🔬-创新点-2：预训练-微调范式" class="headerlink" title="🔬 创新点 2：预训练-微调范式"></a>🔬 创新点 2：预训练-微调范式</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">传统方法: 任务A → 从头训练模型A</span><br><span class="line">         任务B → 从头训练模型B  (重复劳动!)</span><br><span class="line"></span><br><span class="line">BERT范式: 大规模语料 → 预训练BERT (一次)</span><br><span class="line">                ↓</span><br><span class="line">         任务A → 微调 (仅需小数据)</span><br><span class="line">         任务B → 微调 (仅需小数据)</span><br><span class="line">         任务C → 微调 (仅需小数据)</span><br></pre></td></tr></tbody></table></figure><p><strong>革命性影响</strong>：小公司/研究者无需大规模计算资源，只需微调即可获得SOTA性能。</p><h4 id="🔬-创新点-3：统一的特征提取器"><a href="#🔬-创新点-3：统一的特征提取器" class="headerlink" title="🔬 创新点 3：统一的特征提取器"></a>🔬 创新点 3：统一的特征提取器</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BERT 可以适配几乎所有 NLP 任务</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 单句分类 (情感分析)</span></span><br><span class="line">[CLS] 这部电影太棒了 [SEP] → CLS向量 → 分类器 → 正面</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 句对分类 (自然语言推理)</span></span><br><span class="line">[CLS] 天在下雨 [SEP] 地面是湿的 [SEP] → CLS向量 → 蕴含/矛盾/中性</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 序列标注 (命名实体识别)</span></span><br><span class="line">[CLS] 马云 创办了 阿里巴巴 [SEP] → 每个token → B-PER O O B-ORG</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 问答 (阅读理解)</span></span><br><span class="line">[CLS] 问题 [SEP] 文章 [SEP] → 预测答案起止位置</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="1-5-消融实验详解-Ablation-Study"><a href="#1-5-消融实验详解-Ablation-Study" class="headerlink" title="1.5 消融实验详解 (Ablation Study)"></a>1.5 消融实验详解 (Ablation Study)</h3><p>论文通过消融实验验证了各组件的重要性：</p><h4 id="实验-1：预训练任务的影响"><a href="#实验-1：预训练任务的影响" class="headerlink" title="实验 1：预训练任务的影响"></a>实验 1：预训练任务的影响</h4><table><thead><tr><th>配置</th><th>MNLI</th><th>QNLI</th><th>SST-2</th></tr></thead><tbody><tr><td>BERT (MLM + NSP)</td><td><strong>84.6</strong></td><td><strong>90.5</strong></td><td><strong>93.5</strong></td></tr><tr><td>仅 MLM (无 NSP)</td><td>84.3</td><td>90.2</td><td>93.2</td></tr><tr><td>仅 LTR (左到右)</td><td>82.1</td><td>87.4</td><td>91.3</td></tr><tr><td>LTR + BiLSTM</td><td>82.8</td><td>88.1</td><td>91.6</td></tr></tbody></table><p><strong>结论</strong>：</p><ul><li>MLM 比单向 LTR 提升约 2.5%</li><li>NSP 提升有限 (~0.3%)，后续 RoBERTa 移除了它</li></ul><h4 id="实验-2：模型规模的影响"><a href="#实验-2：模型规模的影响" class="headerlink" title="实验 2：模型规模的影响"></a>实验 2：模型规模的影响</h4><table><thead><tr><th>模型</th><th>层数</th><th>隐藏维度</th><th>参数量</th><th>MNLI</th></tr></thead><tbody><tr><td>BERT-Base</td><td>12</td><td>768</td><td>110M</td><td>84.6</td></tr><tr><td>BERT-Large</td><td>24</td><td>1024</td><td>340M</td><td><strong>86.7</strong></td></tr></tbody></table><p><strong>结论</strong>：更大的模型 = 更好的性能 (Scaling Law 的早期验证)</p><h4 id="实验-3：Mask-策略的影响"><a href="#实验-3：Mask-策略的影响" class="headerlink" title="实验 3：Mask 策略的影响"></a>实验 3：Mask 策略的影响</h4><table><thead><tr><th>Mask 策略</th><th>效果</th></tr></thead><tbody><tr><td>100% [MASK]</td><td>次优，预训练与微调分布不一致</td></tr><tr><td>80%/10%/10% (论文方案)</td><td><strong>最优</strong></td></tr><tr><td>随机比例</td><td>不稳定</td></tr></tbody></table><hr><h3 id="1-6-训练细节与超参数"><a href="#1-6-训练细节与超参数" class="headerlink" title="1.6 训练细节与超参数"></a>1.6 训练细节与超参数</h3><h4 id="预训练配置"><a href="#预训练配置" class="headerlink" title="预训练配置"></a>预训练配置</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 数据集</span></span><br><span class="line">- BooksCorpus: 800M 词 (<span class="number">11</span>,038 本书)</span><br><span class="line">- English Wikipedia: <span class="number">2</span>,500M 词 (仅文本，去除表格/列表)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练配置</span></span><br><span class="line">batch_size = <span class="number">256</span></span><br><span class="line">max_seq_length = <span class="number">512</span> (前<span class="number">90</span>%步用<span class="number">128</span>，后<span class="number">10</span>%用<span class="number">512</span>)</span><br><span class="line">learning_rate = <span class="number">1e-4</span></span><br><span class="line">warmup_steps = <span class="number">10</span>,<span class="number">000</span></span><br><span class="line">total_steps = <span class="number">1</span>,<span class="number">000</span>,<span class="number">000</span></span><br><span class="line">optimizer = Adam (β<span class="number">1</span>=<span class="number">0.9</span>, β<span class="number">2</span>=<span class="number">0.999</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 硬件</span></span><br><span class="line">BERT-Base: <span class="number">4</span> TPU Pods (<span class="number">16</span> TPU chips), <span class="number">4</span>天</span><br><span class="line">BERT-Large: <span class="number">16</span> TPU Pods (<span class="number">64</span> TPU chips), <span class="number">4</span>天</span><br></pre></td></tr></tbody></table></figure><h4 id="微调配置"><a href="#微调配置" class="headerlink" title="微调配置"></a>微调配置</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 通用微调超参数</span></span><br><span class="line">batch_size = <span class="number">16</span> 或 <span class="number">32</span></span><br><span class="line">learning_rate = <span class="number">2e-5</span>, <span class="number">3e-5</span>, <span class="number">5e-5</span> (选最优)</span><br><span class="line">epochs = <span class="number">2</span>-<span class="number">4</span></span><br><span class="line">dropout = <span class="number">0.1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不同任务的微调时间</span></span><br><span class="line">MRPC (<span class="number">3.5</span>k样本): ~<span class="number">1</span>分钟</span><br><span class="line">SST-<span class="number">2</span> (67k样本): ~<span class="number">1</span>小时  </span><br><span class="line">SQuAD (100k样本): ~<span class="number">30</span>分钟</span><br></pre></td></tr></tbody></table></figure><hr><hr><h3 id="1-4-MLM-与微调范式深度解析"><a href="#1-4-MLM-与微调范式深度解析" class="headerlink" title="1.4 MLM 与微调范式深度解析"></a>1.4 MLM 与微调范式深度解析</h3><h4 id="🎯-什么是-MLM（Masked-Language-Model）？"><a href="#🎯-什么是-MLM（Masked-Language-Model）？" class="headerlink" title="🎯 什么是 MLM（Masked Language Model）？"></a>🎯 什么是 MLM（Masked Language Model）？</h4><p><strong>通俗理解</strong>：让 AI 做”完形填空”游戏</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">小学语文题：</span><br><span class="line">  "小明 _____ 学校上课"</span><br><span class="line">  </span><br><span class="line">答案：去、到、在...</span><br><span class="line"></span><br><span class="line">BERT 的 MLM 就是让 AI 做这种"完形填空"！</span><br></pre></td></tr></tbody></table></figure><h4 id="MLM-具体操作流程"><a href="#MLM-具体操作流程" class="headerlink" title="MLM 具体操作流程"></a>MLM 具体操作流程</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 原始句子</span></span><br><span class="line">原句 = <span class="string">"我爱北京天安门"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 1: 随机选择 15% 的词进行处理</span></span><br><span class="line">选中 = <span class="string">"北京"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 2: 对选中的词进行三种处理（随机选一种）</span></span><br><span class="line">处理后 = <span class="string">"我爱 [MASK] 天安门"</span>   <span class="comment"># 80% 概率：替换为 [MASK]</span></span><br><span class="line">或者   = <span class="string">"我爱 上海 天安门"</span>     <span class="comment"># 10% 概率：替换为随机词</span></span><br><span class="line">或者   = <span class="string">"我爱 北京 天安门"</span>     <span class="comment"># 10% 概率：保持不变</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3: 让模型预测被遮住的词是什么</span></span><br><span class="line">模型输入 = <span class="string">"我爱 [MASK] 天安门"</span></span><br><span class="line">模型输出 = <span class="string">"北京"</span> ✅  (如果预测对了，loss 很小)</span><br><span class="line">         = <span class="string">"上海"</span> ❌  (如果预测错了，loss 很大)</span><br></pre></td></tr></tbody></table></figure><h4 id="为什么-MLM-能让-BERT-学会”理解语言”？-1"><a href="#为什么-MLM-能让-BERT-学会”理解语言”？-1" class="headerlink" title="为什么 MLM 能让 BERT 学会”理解语言”？"></a>为什么 MLM 能让 BERT 学会”理解语言”？</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">场景 1：</span><br><span class="line">  输入："我去 [MASK] 存钱"</span><br><span class="line">  模型学会：看到"存钱" → 预测"银行"（金融机构）</span><br><span class="line"></span><br><span class="line">场景 2：</span><br><span class="line">  输入："河边的 [MASK] 很陡峭"  </span><br><span class="line">  模型学会：看到"河边""陡峭" → 预测"河岸/堤坝"</span><br><span class="line"></span><br><span class="line">通过数十亿次这样的"完形填空"训练后：</span><br><span class="line">  → 模型学会了词语之间的关系</span><br><span class="line">  → 模型学会了语法结构</span><br><span class="line">  → 模型学会了常识知识</span><br></pre></td></tr></tbody></table></figure><h4 id="MLM-训练代码示例-1"><a href="#MLM-训练代码示例-1" class="headerlink" title="MLM 训练代码示例"></a>MLM 训练代码示例</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForMaskedLM</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-chinese'</span>)</span><br><span class="line">model = BertForMaskedLM.from_pretrained(<span class="string">'bert-base-chinese'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输入一个带 [MASK] 的句子</span></span><br><span class="line">text = <span class="string">"我爱[MASK]天安门"</span></span><br><span class="line">inputs = tokenizer(text, return_tensors=<span class="string">'pt'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型预测</span></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = model(**inputs)</span><br><span class="line">    predictions = outputs.logits</span><br><span class="line"></span><br><span class="line"><span class="comment"># 找到 [MASK] 位置的预测结果</span></span><br><span class="line">mask_index = (inputs[<span class="string">'input_ids'</span>] == tokenizer.mask_token_id).nonzero()[<span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">predicted_token_id = predictions[<span class="number">0</span>, mask_index].argmax(dim=-<span class="number">1</span>)</span><br><span class="line">predicted_token = tokenizer.decode(predicted_token_id)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"原句: <span class="subst">{text}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"预测: <span class="subst">{predicted_token}</span>"</span>)  <span class="comment"># 输出: 北京</span></span><br></pre></td></tr></tbody></table></figure><hr><h4 id="🔧-什么是微调范式（Fine-tuning-Paradigm）？"><a href="#🔧-什么是微调范式（Fine-tuning-Paradigm）？" class="headerlink" title="🔧 什么是微调范式（Fine-tuning Paradigm）？"></a>🔧 什么是微调范式（Fine-tuning Paradigm）？</h4><p><strong>通俗理解</strong>：培养”通才”再培养”专才”</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">传统方式（从零开始）：</span><br><span class="line">  ┌─────────────────────────────────────────────────┐</span><br><span class="line">  │  任务：情感分析                                   │</span><br><span class="line">  │  数据：10万条电商评论                             │</span><br><span class="line">  │  训练：从随机初始化开始，训练一个专门的模型         │</span><br><span class="line">  │  耗时：3天，需要大量标注数据                       │</span><br><span class="line">  └─────────────────────────────────────────────────┘</span><br><span class="line">  </span><br><span class="line">  问题：每个任务都要从头训练，重复劳动！</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">微调范式（站在巨人肩膀上）：</span><br><span class="line"></span><br><span class="line">  第一阶段：预训练（Pre-training）—— 只做一次！</span><br><span class="line">  ┌─────────────────────────────────────────────────┐</span><br><span class="line">  │  数据：整个维基百科 + 大量书籍 (数十亿词)          │</span><br><span class="line">  │  任务：MLM（完形填空）                            │</span><br><span class="line">  │  目的：让模型学会"理解语言"                       │</span><br><span class="line">  │  耗时：数周 (但只需做一次，由 Google 完成)         │</span><br><span class="line">  │  产出：BERT 预训练模型 ✨                         │</span><br><span class="line">  └─────────────────────────────────────────────────┘</span><br><span class="line">                    │</span><br><span class="line">                    ▼ 下载现成的 BERT</span><br><span class="line">                    </span><br><span class="line">  第二阶段：微调（Fine-tuning）—— 每个任务只需几小时！</span><br><span class="line">  ┌─────────────────────────────────────────────────┐</span><br><span class="line">  │  任务A：情感分析                                  │</span><br><span class="line">  │  数据：仅需 1000 条标注数据！                     │</span><br><span class="line">  │  方法：在 BERT 上加一个分类层，微调几轮            │</span><br><span class="line">  │  耗时：30分钟                                    │</span><br><span class="line">  └─────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><h4 id="微调示例：情感分析完整代码"><a href="#微调示例：情感分析完整代码" class="headerlink" title="微调示例：情感分析完整代码"></a>微调示例：情感分析完整代码</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForSequenceClassification, Trainer, TrainingArguments</span><br><span class="line"><span class="keyword">from</span> datasets <span class="keyword">import</span> Dataset</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ 第一步：准备少量标注数据 ============</span></span><br><span class="line">train_data = {</span><br><span class="line">    <span class="string">"text"</span>: [</span><br><span class="line">        <span class="string">"这个产品太棒了，强烈推荐！"</span>,</span><br><span class="line">        <span class="string">"质量很差，用了一天就坏了"</span>,</span><br><span class="line">        <span class="string">"一般般，没有惊喜也没有失望"</span>,</span><br><span class="line">        <span class="string">"超级喜欢，已经回购三次"</span>,</span><br><span class="line">        <span class="string">"客服态度很差，再也不买了"</span>,</span><br><span class="line">        <span class="string">"性价比很高，值得购买"</span>,</span><br><span class="line">    ],</span><br><span class="line">    <span class="string">"label"</span>: [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]  <span class="comment"># 1=正面, 0=负面</span></span><br><span class="line">}</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ 第二步：加载预训练的 BERT ============</span></span><br><span class="line"><span class="comment"># 这个 BERT 已经通过 MLM 任务学会了"理解中文"</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-chinese'</span>)</span><br><span class="line">model = BertForSequenceClassification.from_pretrained(</span><br><span class="line">    <span class="string">'bert-base-chinese'</span>, </span><br><span class="line">    num_labels=<span class="number">2</span>  <span class="comment"># 正面/负面 两个类别</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ 第三步：数据预处理 ============</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">tokenize_function</span>(<span class="params">examples</span>):</span><br><span class="line">    <span class="keyword">return</span> tokenizer(examples[<span class="string">"text"</span>], padding=<span class="string">"max_length"</span>, truncation=<span class="literal">True</span>, max_length=<span class="number">128</span>)</span><br><span class="line"></span><br><span class="line">dataset = Dataset.from_dict(train_data)</span><br><span class="line">tokenized_dataset = dataset.<span class="built_in">map</span>(tokenize_function, batched=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ 第四步：微调训练 ============</span></span><br><span class="line">training_args = TrainingArguments(</span><br><span class="line">    output_dir=<span class="string">"./results"</span>,</span><br><span class="line">    num_train_epochs=<span class="number">3</span>,           <span class="comment"># 只需要训练 3 轮！</span></span><br><span class="line">    per_device_train_batch_size=<span class="number">4</span>,</span><br><span class="line">    learning_rate=<span class="number">2e-5</span>,           <span class="comment"># 很小的学习率，轻微调整</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer = Trainer(</span><br><span class="line">    model=model,</span><br><span class="line">    args=training_args,</span><br><span class="line">    train_dataset=tokenized_dataset,</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">trainer.train()  <span class="comment"># 几分钟就完成！</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># ============ 第五步：使用微调后的模型 ============</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict</span>(<span class="params">text</span>):</span><br><span class="line">    inputs = tokenizer(text, return_tensors=<span class="string">"pt"</span>, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>)</span><br><span class="line">    outputs = model(**inputs)</span><br><span class="line">    prediction = outputs.logits.argmax(dim=-<span class="number">1</span>).item()</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"正面 😊"</span> <span class="keyword">if</span> prediction == <span class="number">1</span> <span class="keyword">else</span> <span class="string">"负面 😞"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line"><span class="built_in">print</span>(predict(<span class="string">"这款手机拍照效果惊艳"</span>))  <span class="comment"># → 正面 😊</span></span><br><span class="line"><span class="built_in">print</span>(predict(<span class="string">"物流太慢了，等了一周"</span>))   <span class="comment"># → 负面 😞</span></span><br></pre></td></tr></tbody></table></figure><h4 id="形象类比总结-1"><a href="#形象类比总结-1" class="headerlink" title="形象类比总结"></a>形象类比总结</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                      🎓 教育类比                             │</span><br><span class="line">├─────────────────────────────────────────────────────────────┤</span><br><span class="line">│                                                             │</span><br><span class="line">│  【预训练 Pre-training】= 上大学，接受通识教育               │</span><br><span class="line">│     • 学习语文、数学、英语、物理...                          │</span><br><span class="line">│     • 目标：成为一个有基础知识的"通才"                       │</span><br><span class="line">│     • 时间：4年                                             │</span><br><span class="line">│     • 成本：高                                              │</span><br><span class="line">│                                                             │</span><br><span class="line">│  【MLM 任务】= 大学里的各种练习题                            │</span><br><span class="line">│     • 完形填空、阅读理解、语法练习...                        │</span><br><span class="line">│     • 目标：锻炼语言理解能力                                 │</span><br><span class="line">│                                                             │</span><br><span class="line">│  【微调 Fine-tuning】= 工作后的岗位培训                      │</span><br><span class="line">│     • 针对具体工作（情感分析/问答/翻译）学习                  │</span><br><span class="line">│     • 目标：成为某个领域的"专才"                             │</span><br><span class="line">│     • 时间：几天                                            │</span><br><span class="line">│     • 成本：低（因为已经有基础了）                           │</span><br><span class="line">│                                                             │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="1-5-MASK-Token-机制深度解析"><a href="#1-5-MASK-Token-机制深度解析" class="headerlink" title="1.5 [MASK] Token 机制深度解析"></a>1.5 [MASK] Token 机制深度解析</h3><h4 id="MASK-到底是什么？"><a href="#MASK-到底是什么？" class="headerlink" title="[MASK] 到底是什么？"></a>[MASK] 到底是什么？</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer</span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">'bert-base-chinese'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># [MASK] 就是一个特殊的 Token ID</span></span><br><span class="line"><span class="built_in">print</span>(tokenizer.mask_token)     <span class="comment"># 输出: [MASK]</span></span><br><span class="line"><span class="built_in">print</span>(tokenizer.mask_token_id)  <span class="comment"># 输出: 103</span></span><br></pre></td></tr></tbody></table></figure><h4 id="完整输入对比"><a href="#完整输入对比" class="headerlink" title="完整输入对比"></a>完整输入对比</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 原始句子</span></span><br><span class="line">原句 = <span class="string">"我爱北京天安门"</span></span><br><span class="line">tokenizer(原句)</span><br><span class="line"></span><br><span class="line">Token:    [<span class="string">"[CLS]"</span>, <span class="string">"我"</span>, <span class="string">"爱"</span>, <span class="string">"北京"</span>, <span class="string">"天"</span>, <span class="string">"安"</span>, <span class="string">"门"</span>, <span class="string">"[SEP]"</span>]</span><br><span class="line">Token ID: [  <span class="number">101</span>,  <span class="number">2769</span>, <span class="number">4263</span>,  <span class="number">1266</span>, <span class="number">1921</span>, <span class="number">2128</span>, <span class="number">7305</span>,   <span class="number">102</span>  ]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 做 MLM 时把 "北京" 替换成 [MASK]</span></span><br><span class="line">遮蔽句 = <span class="string">"我爱[MASK]天安门"</span></span><br><span class="line">tokenizer(遮蔽句)</span><br><span class="line"></span><br><span class="line">Token:    [<span class="string">"[CLS]"</span>, <span class="string">"我"</span>, <span class="string">"爱"</span>, <span class="string">"[MASK]"</span>, <span class="string">"天"</span>, <span class="string">"安"</span>, <span class="string">"门"</span>, <span class="string">"[SEP]"</span>]</span><br><span class="line">Token ID: [  <span class="number">101</span>,  <span class="number">2769</span>, <span class="number">4263</span>,    <span class="number">103</span>,   <span class="number">1921</span>, <span class="number">2128</span>, <span class="number">7305</span>,   <span class="number">102</span>  ]</span><br><span class="line">                                  ↑</span><br><span class="line">                          就是把 <span class="number">1266</span> 换成了 <span class="number">103</span>！</span><br></pre></td></tr></tbody></table></figure><h4 id="模型实际看到的是什么？"><a href="#模型实际看到的是什么？" class="headerlink" title="模型实际看到的是什么？"></a>模型实际看到的是什么？</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模型输入的就是一串数字（Token IDs）</span></span><br><span class="line">input_ids = [<span class="number">101</span>, <span class="number">2769</span>, <span class="number">4263</span>, <span class="number">103</span>, <span class="number">1921</span>, <span class="number">2128</span>, <span class="number">7305</span>, <span class="number">102</span>]</span><br><span class="line">                              ↑</span><br><span class="line">                     <span class="number">103</span> = [MASK] 的编号</span><br><span class="line"></span><br><span class="line"><span class="comment"># 经过 Embedding 层后</span></span><br><span class="line"><span class="comment"># 103 会被查表转换成一个 768 维的向量</span></span><br><span class="line"></span><br><span class="line">embedding_table = model.embeddings.word_embeddings.weight</span><br><span class="line"><span class="comment"># 形状: [21128, 768]  (词表大小 × 隐藏维度)</span></span><br><span class="line"></span><br><span class="line">mask_embedding = embedding_table[<span class="number">103</span>]  <span class="comment"># [MASK] 的向量表示</span></span><br><span class="line"><span class="comment"># 形状: [768]</span></span><br><span class="line"><span class="comment"># 值: [0.023, -0.156, 0.234, ..., 0.089]  ← 这是可学习的参数！</span></span><br></pre></td></tr></tbody></table></figure><h4 id="为什么-103-初始时啥也不代表？"><a href="#为什么-103-初始时啥也不代表？" class="headerlink" title="为什么 103 初始时啥也不代表？"></a>为什么 103 初始时啥也不代表？</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Embedding 层输出</span></span><br><span class="line">embeddings = [</span><br><span class="line">    E_cls,    <span class="comment"># [CLS] 的向量</span></span><br><span class="line">    E_我,     <span class="comment"># "我" 的语义向量</span></span><br><span class="line">    E_爱,     <span class="comment"># "爱" 的语义向量  </span></span><br><span class="line">    E_mask,   <span class="comment"># 103 对应的向量 ← 这个向量本身没有语义！</span></span><br><span class="line">    E_天,     <span class="comment"># "天" 的语义向量</span></span><br><span class="line">    E_安,     <span class="comment"># "安" 的语义向量</span></span><br><span class="line">    E_门,     <span class="comment"># "门" 的语义向量</span></span><br><span class="line">    E_sep,    <span class="comment"># [SEP] 的向量</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># E_mask 初始值可能是 [0.01, -0.02, 0.03, ...]</span></span><br><span class="line"><span class="comment"># 它本身不代表任何词义，只是一个"占位符"</span></span><br></pre></td></tr></tbody></table></figure><p><strong>关键</strong>：103 本身确实啥也不代表，但经过 Self-Attention 后会吸收周围词的信息！</p><hr><h3 id="1-6-BERT-模型架构详解"><a href="#1-6-BERT-模型架构详解" class="headerlink" title="1.6 BERT 模型架构详解"></a>1.6 BERT 模型架构详解</h3><h4 id="架构参数"><a href="#架构参数" class="headerlink" title="架构参数"></a>架构参数</h4><table><thead><tr><th>配置</th><th>BERT-Base</th><th>BERT-Large</th></tr></thead><tbody><tr><td>Transformer 层数 (L)</td><td>12</td><td>24</td></tr><tr><td>隐藏层维度 (H)</td><td>768</td><td>1024</td></tr><tr><td>注意力头数 (A)</td><td>12</td><td>16</td></tr><tr><td>每个头的维度 (H/A)</td><td>64</td><td>64</td></tr><tr><td>总参数量</td><td>110M</td><td>340M</td></tr><tr><td>FFN 中间层维度</td><td>3072</td><td>4096</td></tr></tbody></table><h4 id="输入表示-Input-Representation"><a href="#输入表示-Input-Representation" class="headerlink" title="输入表示 (Input Representation)"></a>输入表示 (Input Representation)</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Token Embedding: 每个词的向量表示 [vocab_size, 768]</span><br><span class="line">  + </span><br><span class="line">Position Embedding: 位置编码 [512, 768] (最大序列长度 512)</span><br><span class="line">  +</span><br><span class="line">Segment Embedding: 句子分隔 [2, 768] (Sentence A 或 B)</span><br><span class="line">  =</span><br><span class="line">Input Representation: [batch, seq_len, 768]</span><br></pre></td></tr></tbody></table></figure><p><strong>具体例子</strong>:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">输入: <span class="string">"[CLS] 我 爱 NLP [SEP] 它 很 有趣 [SEP]"</span></span><br><span class="line"></span><br><span class="line">Token IDs:    [<span class="number">101</span>, <span class="number">2769</span>, <span class="number">4263</span>, <span class="number">21128</span>, <span class="number">102</span>, <span class="number">1045</span>, <span class="number">1447</span>, <span class="number">3300</span>, <span class="number">4638</span>, <span class="number">102</span>]</span><br><span class="line">Segment IDs:  [<span class="number">0</span>,   <span class="number">0</span>,    <span class="number">0</span>,    <span class="number">0</span>,     <span class="number">0</span>,   <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>,    <span class="number">1</span>]</span><br><span class="line">Position IDs: [<span class="number">0</span>,   <span class="number">1</span>,    <span class="number">2</span>,    <span class="number">3</span>,     <span class="number">4</span>,   <span class="number">5</span>,    <span class="number">6</span>,    <span class="number">7</span>,    <span class="number">8</span>,    <span class="number">9</span>]</span><br><span class="line"></span><br><span class="line">三个 embedding 相加后: [<span class="number">10</span>, <span class="number">768</span>]</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="1-5-论文实验结果与影响"><a href="#1-5-论文实验结果与影响" class="headerlink" title="1.5 论文实验结果与影响"></a>1.5 论文实验结果与影响</h3><h4 id="关键性能提升"><a href="#关键性能提升" class="headerlink" title="关键性能提升"></a>关键性能提升</h4><p><strong>GLUE 基准测试</strong> (11 个 NLP 任务):</p><ul><li>BERT-Base: 78.6% → 提升 7%</li><li>BERT-Large: 80.5% → 提升 9%</li></ul><p><strong>SQuAD 问答任务</strong>:</p><ul><li>BERT-Large: F1 = 93.2% (超越人类水平 91.2%)</li></ul><p><strong>为什么效果这么好？</strong></p><ol><li><strong>双向上下文</strong>: 每个词都能看到完整句子</li><li><strong>深度交互</strong>: 12/24 层逐层精炼表示</li><li><strong>大规模预训练</strong>: BooksCorpus (800M 词) + Wikipedia (2500M 词)</li><li><strong>迁移学习</strong>: 预训练 + 微调范式</li></ol><hr><hr><hr><h2 id="🔬-Part-2-BERT-前两层完整推演"><a href="#🔬-Part-2-BERT-前两层完整推演" class="headerlink" title="🔬 Part 2: BERT 前两层完整推演"></a>🔬 Part 2: BERT 前两层完整推演</h2><h3 id="输入准备：MLM-任务"><a href="#输入准备：MLM-任务" class="headerlink" title="输入准备：MLM 任务"></a>输入准备：MLM 任务</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">句子: "我 爱 [MASK] 天安门"</span><br><span class="line"></span><br><span class="line">Token IDs: [101, 2769, 4263, 103, 1921, 102]</span><br><span class="line">           [CLS]  我    爱   MASK  天安门 [SEP]</span><br><span class="line"></span><br><span class="line">Embedding 后: X₀ = [6, 768]</span><br><span class="line"></span><br><span class="line">位置0 [CLS]:  [0.12, -0.05, 0.33, ..., 0.45]</span><br><span class="line">位置1  我:    [0.45, 0.23, -0.12, ..., 0.12]</span><br><span class="line">位置2  爱:    [0.33, 0.56, 0.78, ..., 0.89]</span><br><span class="line">位置3 MASK:   [0.01, 0.02, 0.01, ..., 0.03]  ← 几乎是空的！</span><br><span class="line">位置4 天安门: [0.67, -0.34, 0.45, ..., 0.56]</span><br><span class="line">位置5 [SEP]:  [0.78, 0.34, -0.12, ..., 0.12]</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="Layer-1：第一层详细计算"><a href="#Layer-1：第一层详细计算" class="headerlink" title="Layer 1：第一层详细计算"></a>Layer 1：第一层详细计算</h3><h4 id="Step-1-1-计算-Q、K、V"><a href="#Step-1-1-计算-Q、K、V" class="headerlink" title="Step 1.1: 计算 Q、K、V"></a>Step 1.1: 计算 Q、K、V</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X₀ = [<span class="number">6</span>, <span class="number">768</span>]  <span class="comment"># 输入</span></span><br><span class="line"></span><br><span class="line">Q₁ = X₀ @ W_Q  <span class="comment"># [6, 768] @ [768, 768] = [6, 768]</span></span><br><span class="line">K₁ = X₀ @ W_K  <span class="comment"># [6, 768] @ [768, 768] = [6, 768]</span></span><br><span class="line">V₁ = X₀ @ W_V  <span class="comment"># [6, 768] @ [768, 768] = [6, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每个位置都有自己的 Q、K、V 向量</span></span><br><span class="line">Q₁ = [Q_cls, Q_我, Q_爱, Q_mask, Q_天安门, Q_sep]</span><br><span class="line">K₁ = [K_cls, K_我, K_爱, K_mask, K_天安门, K_sep]</span><br><span class="line">V₁ = [V_cls, V_我, V_爱, V_mask, V_天安门, V_sep]</span><br></pre></td></tr></tbody></table></figure><h4 id="Step-1-2-计算注意力分数"><a href="#Step-1-2-计算注意力分数" class="headerlink" title="Step 1.2: 计算注意力分数"></a>Step 1.2: 计算注意力分数</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">scores = Q₁ @ K₁.T / sqrt(<span class="number">64</span>)  <span class="comment"># [6, 768] @ [768, 6] = [6, 6]</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         [CLS]   我    爱   MASK  天安门  [SEP]</span></span><br><span class="line"><span class="comment"># [CLS]  [ 2.1   1.3   1.5   0.2   1.8    1.2 ]</span></span><br><span class="line"><span class="comment">#   我   [ 1.2   3.1   2.3   0.3   1.5    0.8 ]</span></span><br><span class="line"><span class="comment">#   爱   [ 1.4   2.5   2.8   0.4   2.1    0.9 ]</span></span><br><span class="line"><span class="comment"># MASK   [ 0.8   1.9   2.4   0.1   2.6    0.7 ]  ← MASK 行</span></span><br><span class="line"><span class="comment"># 天安门 [ 1.6   1.4   2.0   0.3   3.2    1.1 ]</span></span><br><span class="line"><span class="comment"># [SEP]  [ 1.3   0.9   1.1   0.2   1.3    2.5 ]</span></span><br></pre></td></tr></tbody></table></figure><h4 id="Step-1-3-Softmax-归一化"><a href="#Step-1-3-Softmax-归一化" class="headerlink" title="Step 1.3: Softmax 归一化"></a>Step 1.3: Softmax 归一化</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">weights = softmax(scores, dim=-<span class="number">1</span>)  <span class="comment"># 每行和为1</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         [CLS]   我     爱    MASK  天安门  [SEP]</span></span><br><span class="line"><span class="comment"># [CLS]  [0.18  0.12   0.15   0.03   0.40   0.12]</span></span><br><span class="line"><span class="comment">#   我   [0.10  0.35   0.25   0.02   0.20   0.08]</span></span><br><span class="line"><span class="comment">#   爱   [0.12  0.22   0.28   0.03   0.28   0.07]</span></span><br><span class="line"><span class="comment"># MASK   [0.08  0.18   0.28   0.01   0.38   0.07]  ← 重点看这行！</span></span><br><span class="line"><span class="comment"># 天安门 [0.11  0.09   0.16   0.02   0.55   0.07]</span></span><br><span class="line"><span class="comment"># [SEP]  [0.15  0.10   0.12   0.03   0.15   0.45]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MASK 位置: 28% 看"爱"，38% 看"天安门"，只有 1% 看自己！</span></span><br></pre></td></tr></tbody></table></figure><h4 id="Step-1-4-加权求和得到新表示"><a href="#Step-1-4-加权求和得到新表示" class="headerlink" title="Step 1.4: 加权求和得到新表示"></a>Step 1.4: 加权求和得到新表示</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">H₁ = weights @ V₁  <span class="comment"># [6, 6] @ [6, 768] = [6, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MASK 位置的新向量:</span></span><br><span class="line">H₁[<span class="number">3</span>] = <span class="number">0.08</span>×V_cls + <span class="number">0.18</span>×V_我 + <span class="number">0.28</span>×V_爱 + <span class="number">0.01</span>×V_mask + <span class="number">0.38</span>×V_天安门 + <span class="number">0.07</span>×V_sep</span><br><span class="line">        ↑                        ↑                          ↑</span><br><span class="line">      几乎忽略                主要来自<span class="string">"爱"</span>              主要来自<span class="string">"天安门"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果: MASK 位置现在融合了 "爱" 和 "天安门" 的信息！</span></span><br><span class="line">H₁[<span class="number">3</span>] = [<span class="number">0.45</span>, <span class="number">0.67</span>, <span class="number">0.23</span>, ..., <span class="number">0.78</span>]  ← 不再是空壳了！</span><br></pre></td></tr></tbody></table></figure><h4 id="Step-1-5-Feed-Forward-Network-FFN"><a href="#Step-1-5-Feed-Forward-Network-FFN" class="headerlink" title="Step 1.5: Feed-Forward Network (FFN)"></a>Step 1.5: Feed-Forward Network (FFN)</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入</span></span><br><span class="line">H₁ = [<span class="number">6</span>, <span class="number">768</span>]  <span class="comment"># Attention 的输出</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># FFN: 两层线性变换</span></span><br><span class="line">step1 = H₁ @ W₁        <span class="comment"># [6, 768] @ [768, 3072] = [6, 3072]  先扩大4倍</span></span><br><span class="line">step2 = ReLU(step1)    <span class="comment"># [6, 3072]  激活函数</span></span><br><span class="line">step3 = step2 @ W₂     <span class="comment"># [6, 3072] @ [3072, 768] = [6, 768]  再压回去</span></span><br><span class="line"></span><br><span class="line">FFN_out = step3        <span class="comment"># [6, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 维度变化图示</span></span><br><span class="line"><span class="number">768</span> ──扩大──→ <span class="number">3072</span> ──压缩──→ <span class="number">768</span></span><br><span class="line">      W₁           W₂</span><br></pre></td></tr></tbody></table></figure><h4 id="Step-1-6-残差连接-LayerNorm"><a href="#Step-1-6-残差连接-LayerNorm" class="headerlink" title="Step 1.6: 残差连接 + LayerNorm"></a>Step 1.6: 残差连接 + LayerNorm</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 残差连接：把输入加回来</span></span><br><span class="line">output = H₁ + FFN_out  <span class="comment"># [6, 768] + [6, 768] = [6, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># LayerNorm 归一化</span></span><br><span class="line">X₁ = LayerNorm(output)  <span class="comment"># [6, 768]</span></span><br></pre></td></tr></tbody></table></figure><p><strong>残差连接示意图：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">H₁ ─────────────────────────┐</span><br><span class="line"> │                          │</span><br><span class="line"> ↓                          │ (跳跃连接)</span><br><span class="line">┌─────────┐                 │</span><br><span class="line">│   FFN   │                 │</span><br><span class="line">└─────────┘                 │</span><br><span class="line"> │                          │</span><br><span class="line"> ↓                          ↓</span><br><span class="line">FFN_out ────────────→ (+) 相加 ──→ LayerNorm ──→ X₁</span><br><span class="line">                       ↑</span><br><span class="line">              残差 = H₁ + FFN_out</span><br></pre></td></tr></tbody></table></figure><p><strong>为什么需要残差连接？</strong></p><table><thead><tr><th>问题</th><th>残差解决方案</th></tr></thead><tbody><tr><td>梯度消失</td><td>梯度可以直接通过”跳跃连接”回传</td></tr><tr><td>信息丢失</td><td>原始信息 H₁ 被保留，不会完全被覆盖</td></tr><tr><td>训练困难</td><td>网络只需学习”差异”，更容易优化</td></tr></tbody></table><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 本质：FFN 只学习"增量"</span></span><br><span class="line">X₁ = H₁ + FFN(H₁)</span><br><span class="line">   = H₁ + ΔH     <span class="comment"># 原始 + 修正量</span></span><br></pre></td></tr></tbody></table></figure><p><strong>Layer 1 输出：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X₁ = [6, 768]</span><br><span class="line"></span><br><span class="line">位置3 (MASK): [0.52, 0.71, 0.34, ..., 0.82]</span><br><span class="line">              ↑</span><br><span class="line">        已经包含了 "爱___天安门" 的模式信息</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="Layer-2：第二层详细计算"><a href="#Layer-2：第二层详细计算" class="headerlink" title="Layer 2：第二层详细计算"></a>Layer 2：第二层详细计算</h3><h4 id="Step-2-1-计算新的-Q、K、V"><a href="#Step-2-1-计算新的-Q、K、V" class="headerlink" title="Step 2.1: 计算新的 Q、K、V"></a>Step 2.1: 计算新的 Q、K、V</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入是 Layer 1 的输出</span></span><br><span class="line">X₁ = [<span class="number">6</span>, <span class="number">768</span>]</span><br><span class="line"></span><br><span class="line">Q₂ = X₁ @ W_Q  <span class="comment"># 新的 Q（权重矩阵和 Layer1 不同！）</span></span><br><span class="line">K₂ = X₁ @ W_K  <span class="comment"># 新的 K</span></span><br><span class="line">V₂ = X₁ @ W_V  <span class="comment"># 新的 V</span></span><br></pre></td></tr></tbody></table></figure><h4 id="Step-2-2-计算注意力（基于更新后的表示）"><a href="#Step-2-2-计算注意力（基于更新后的表示）" class="headerlink" title="Step 2.2: 计算注意力（基于更新后的表示）"></a>Step 2.2: 计算注意力（基于更新后的表示）</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scores = Q₂ @ K₂.T / sqrt(<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 现在 MASK 位置已经有了上下文信息</span></span><br><span class="line"><span class="comment"># 它的 Q 向量更"聪明"了，能找到更相关的词</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         [CLS]   我     爱    MASK  天安门  [SEP]</span></span><br><span class="line"><span class="comment"># MASK   [ 0.5   1.5   2.8    0.2   3.5    0.4 ]</span></span><br><span class="line"><span class="comment">#                       ↑            ↑</span></span><br><span class="line"><span class="comment">#                   更关注"爱"   更关注"天安门"</span></span><br><span class="line"></span><br><span class="line">weights[<span class="number">3</span>] = softmax([<span class="number">0.5</span>, <span class="number">1.5</span>, <span class="number">2.8</span>, <span class="number">0.2</span>, <span class="number">3.5</span>, <span class="number">0.4</span>])</span><br><span class="line">           = [<span class="number">0.04</span>, <span class="number">0.10</span>, <span class="number">0.28</span>, <span class="number">0.02</span>, <span class="number">0.52</span>, <span class="number">0.04</span>]</span><br><span class="line"><span class="comment">#                          ↑           ↑</span></span><br><span class="line"><span class="comment">#                      28%看爱      52%看天安门</span></span><br></pre></td></tr></tbody></table></figure><h4 id="Step-2-3-加权求和"><a href="#Step-2-3-加权求和" class="headerlink" title="Step 2.3: 加权求和"></a>Step 2.3: 加权求和</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">H₂[<span class="number">3</span>] = <span class="number">0.04</span>×V_cls + <span class="number">0.10</span>×V_我 + <span class="number">0.28</span>×V_爱 + <span class="number">0.02</span>×V_mask + <span class="number">0.52</span>×V_天安门 + <span class="number">0.04</span>×V_sep</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这次 V_天安门 已经不是原始的了</span></span><br><span class="line"><span class="comment"># 它在 Layer1 中也融合了上下文，知道"天安门在北京"</span></span><br><span class="line"><span class="comment"># 所以 MASK 间接获得了"北京"的信息！</span></span><br><span class="line"></span><br><span class="line">H₂[<span class="number">3</span>] = [<span class="number">0.68</span>, <span class="number">0.82</span>, <span class="number">0.45</span>, ..., <span class="number">0.91</span>]</span><br></pre></td></tr></tbody></table></figure><h4 id="Step-2-4-FFN-残差-LayerNorm"><a href="#Step-2-4-FFN-残差-LayerNorm" class="headerlink" title="Step 2.4: FFN + 残差 + LayerNorm"></a>Step 2.4: FFN + 残差 + LayerNorm</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">X₂ = LayerNorm(H₂ + FFN(H₂))  <span class="comment"># [6, 768]</span></span><br></pre></td></tr></tbody></table></figure><p><strong>Layer 2 输出：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X₂ = [6, 768]</span><br><span class="line"></span><br><span class="line">位置3 (MASK): [0.71, 0.85, 0.52, ..., 0.93]</span><br><span class="line">              ↑</span><br><span class="line">        现在知道: "我爱___天安门" → 这个空应该填地名</span><br><span class="line">                  天安门相关 → 可能是"北京"</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="两层对比总结"><a href="#两层对比总结" class="headerlink" title="两层对比总结"></a>两层对比总结</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">┌────────────────────────────────────────────────────────────┐</span><br><span class="line">│  MASK 位置向量的变化                                        │</span><br><span class="line">├────────────────────────────────────────────────────────────┤</span><br><span class="line">│                                                            │</span><br><span class="line">│  Embedding: [0.01, 0.02, 0.01, ...]   ← 空壳，无语义        │</span><br><span class="line">│      ↓                                                     │</span><br><span class="line">│  Layer 1:   [0.52, 0.71, 0.34, ...]   ← 融合了"爱""天安门"  │</span><br><span class="line">│      ↓                                                     │</span><br><span class="line">│  Layer 2:   [0.71, 0.85, 0.52, ...]   ← 更深层理解          │</span><br><span class="line">│      ↓                                                     │</span><br><span class="line">│    ...      (继续 10 层)                                    │</span><br><span class="line">│      ↓                                                     │</span><br><span class="line">│  Layer 12:  [0.93, 0.87, 0.76, ...]   ← 确定是"北京"        │</span><br><span class="line">│                                                            │</span><br><span class="line">└────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><table><thead><tr><th>层</th><th>MASK 学到了什么</th></tr></thead><tbody><tr><td><strong>Embedding</strong></td><td>空的占位符，103只是个ID</td></tr><tr><td><strong>Layer 1</strong></td><td>“爱”后面是宾语，”天安门”是地标</td></tr><tr><td><strong>Layer 2</strong></td><td>“我爱X天安门”是固定搭配，X是地名</td></tr><tr><td><strong>Layer 3-6</strong></td><td>天安门在北京，这是常识</td></tr><tr><td><strong>Layer 7-12</strong></td><td>确定答案是”北京”，排除其他可能</td></tr></tbody></table><hr><h2 id="🔍-Part-3-Self-Attention-核心机制"><a href="#🔍-Part-3-Self-Attention-核心机制" class="headerlink" title="🔍 Part 3: Self-Attention 核心机制"></a>🔍 Part 3: Self-Attention 核心机制</h2><h3 id="2-0-输入准备"><a href="#2-0-输入准备" class="headerlink" title="2.0 输入准备"></a>2.0 输入准备</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">句子: "我 爱 [MASK] 天安门"</span><br><span class="line"></span><br><span class="line">Token IDs: [101, 2769, 4263, 103, 1921, 102]</span><br><span class="line">           [CLS]  我    爱   MASK  天安门 [SEP]</span><br><span class="line"></span><br><span class="line">Embedding 后: X₀ = [6, 768]</span><br><span class="line"></span><br><span class="line">位置0 [CLS]:  [0.12, -0.05, 0.33, ..., 0.45]  ← 768维向量</span><br><span class="line">位置1  我:    [0.45, 0.23, -0.12, ..., 0.12]</span><br><span class="line">位置2  爱:    [0.33, 0.56, 0.78, ..., 0.89]</span><br><span class="line">位置3 MASK:   [0.01, 0.02, 0.01, ..., 0.03]  ← 几乎是空的！</span><br><span class="line">位置4 天安门: [0.67, -0.34, 0.45, ..., 0.56]</span><br><span class="line">位置5 [SEP]:  [0.78, 0.34, -0.12, ..., 0.12]</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="2-1-Layer-1：第一层详细计算"><a href="#2-1-Layer-1：第一层详细计算" class="headerlink" title="2.1 Layer 1：第一层详细计算"></a>2.1 Layer 1：第一层详细计算</h3><h4 id="Step-1-1-计算-Q、K、V-1"><a href="#Step-1-1-计算-Q、K、V-1" class="headerlink" title="Step 1.1: 计算 Q、K、V"></a>Step 1.1: 计算 Q、K、V</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">X₀ = [<span class="number">6</span>, <span class="number">768</span>]  <span class="comment"># 输入</span></span><br><span class="line"></span><br><span class="line">Q₁ = X₀ @ W_Q  <span class="comment"># [6, 768] @ [768, 768] = [6, 768]</span></span><br><span class="line">K₁ = X₀ @ W_K  <span class="comment"># [6, 768] @ [768, 768] = [6, 768]</span></span><br><span class="line">V₁ = X₀ @ W_V  <span class="comment"># [6, 768] @ [768, 768] = [6, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每个位置都有自己的 Q、K、V 向量</span></span><br><span class="line">Q₁ = [Q_cls, Q_我, Q_爱, Q_mask, Q_天安门, Q_sep]</span><br><span class="line">K₁ = [K_cls, K_我, K_爱, K_mask, K_天安门, K_sep]</span><br><span class="line">V₁ = [V_cls, V_我, V_爱, V_mask, V_天安门, V_sep]</span><br></pre></td></tr></tbody></table></figure><h4 id="Step-1-2-计算注意力分数-1"><a href="#Step-1-2-计算注意力分数-1" class="headerlink" title="Step 1.2: 计算注意力分数"></a>Step 1.2: 计算注意力分数</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">scores = Q₁ @ K₁.T / sqrt(<span class="number">64</span>)  <span class="comment"># [6, 768] @ [768, 6] = [6, 6]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意力矩阵（未归一化）</span></span><br><span class="line"><span class="comment">#         [CLS]   我    爱   MASK  天安门  [SEP]</span></span><br><span class="line"><span class="comment"># [CLS]  [ 2.1   1.3   1.5   0.2   1.8    1.2 ]</span></span><br><span class="line"><span class="comment">#   我   [ 1.2   3.1   2.3   0.3   1.5    0.8 ]</span></span><br><span class="line"><span class="comment">#   爱   [ 1.4   2.5   2.8   0.4   2.1    0.9 ]</span></span><br><span class="line"><span class="comment"># MASK   [ 0.8   1.9   2.4   0.1   2.6    0.7 ]  ← MASK 行</span></span><br><span class="line"><span class="comment"># 天安门 [ 1.6   1.4   2.0   0.3   3.2    1.1 ]</span></span><br><span class="line"><span class="comment"># [SEP]  [ 1.3   0.9   1.1   0.2   1.3    2.5 ]</span></span><br></pre></td></tr></tbody></table></figure><h4 id="Step-1-3-Softmax-归一化-1"><a href="#Step-1-3-Softmax-归一化-1" class="headerlink" title="Step 1.3: Softmax 归一化"></a>Step 1.3: Softmax 归一化</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">weights = softmax(scores, dim=-<span class="number">1</span>)  <span class="comment"># 每行和为1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 注意力权重矩阵</span></span><br><span class="line"><span class="comment">#         [CLS]   我     爱    MASK  天安门  [SEP]</span></span><br><span class="line"><span class="comment"># [CLS]  [0.18  0.12   0.15   0.03   0.40   0.12]</span></span><br><span class="line"><span class="comment">#   我   [0.10  0.35   0.25   0.02   0.20   0.08]</span></span><br><span class="line"><span class="comment">#   爱   [0.12  0.22   0.28   0.03   0.28   0.07]</span></span><br><span class="line"><span class="comment"># MASK   [0.08  0.18   0.28   0.01   0.38   0.07]  ← 重点看这行！</span></span><br><span class="line"><span class="comment"># 天安门 [0.11  0.09   0.16   0.02   0.55   0.07]</span></span><br><span class="line"><span class="comment"># [SEP]  [0.15  0.10   0.12   0.03   0.15   0.45]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MASK 位置解读:</span></span><br><span class="line"><span class="comment"># - 28% 的注意力给 "爱"</span></span><br><span class="line"><span class="comment"># - 38% 的注意力给 "天安门"</span></span><br><span class="line"><span class="comment"># - 只有 1% 看自己（因为自己是空壳）</span></span><br></pre></td></tr></tbody></table></figure><h4 id="Step-1-4-加权求和得到新表示-1"><a href="#Step-1-4-加权求和得到新表示-1" class="headerlink" title="Step 1.4: 加权求和得到新表示"></a>Step 1.4: 加权求和得到新表示</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">H₁ = weights @ V₁  <span class="comment"># [6, 6] @ [6, 768] = [6, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MASK 位置的新向量计算:</span></span><br><span class="line">H₁[<span class="number">3</span>] = <span class="number">0.08</span>×V_cls + <span class="number">0.18</span>×V_我 + <span class="number">0.28</span>×V_爱 + <span class="number">0.01</span>×V_mask </span><br><span class="line">      + <span class="number">0.38</span>×V_天安门 + <span class="number">0.07</span>×V_sep</span><br><span class="line"></span><br><span class="line"><span class="comment"># 具体数值示例（假设简化到4维）:</span></span><br><span class="line">V_cls    = [<span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.1</span>, <span class="number">0.3</span>]</span><br><span class="line">V_我     = [<span class="number">0.8</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.4</span>]</span><br><span class="line">V_爱     = [<span class="number">0.3</span>, <span class="number">0.7</span>, <span class="number">0.5</span>, <span class="number">0.2</span>]</span><br><span class="line">V_mask   = [<span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.0</span>, <span class="number">0.1</span>]  <span class="comment"># 几乎为空</span></span><br><span class="line">V_天安门 = [<span class="number">0.2</span>, <span class="number">0.4</span>, <span class="number">0.8</span>, <span class="number">0.6</span>]</span><br><span class="line">V_sep    = [<span class="number">0.1</span>, <span class="number">0.1</span>, <span class="number">0.2</span>, <span class="number">0.2</span>]</span><br><span class="line"></span><br><span class="line">H₁[<span class="number">3</span>] = <span class="number">0.08</span>×[<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.1</span>,<span class="number">0.3</span>] + <span class="number">0.18</span>×[<span class="number">0.8</span>,<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.4</span>]</span><br><span class="line">      + <span class="number">0.28</span>×[<span class="number">0.3</span>,<span class="number">0.7</span>,<span class="number">0.5</span>,<span class="number">0.2</span>] + <span class="number">0.01</span>×[<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.0</span>,<span class="number">0.1</span>]</span><br><span class="line">      + <span class="number">0.38</span>×[<span class="number">0.2</span>,<span class="number">0.4</span>,<span class="number">0.8</span>,<span class="number">0.6</span>] + <span class="number">0.07</span>×[<span class="number">0.1</span>,<span class="number">0.1</span>,<span class="number">0.2</span>,<span class="number">0.2</span>]</span><br><span class="line"></span><br><span class="line">      = [<span class="number">0.008</span>+<span class="number">0.144</span>+<span class="number">0.084</span>+<span class="number">0</span>+<span class="number">0.076</span>+<span class="number">0.007</span>,</span><br><span class="line">         <span class="number">0.016</span>+<span class="number">0.018</span>+<span class="number">0.196</span>+<span class="number">0</span>+<span class="number">0.152</span>+<span class="number">0.007</span>,</span><br><span class="line">         <span class="number">0.008</span>+<span class="number">0.036</span>+<span class="number">0.14</span>+<span class="number">0</span>+<span class="number">0.304</span>+<span class="number">0.014</span>,</span><br><span class="line">         <span class="number">0.024</span>+<span class="number">0.072</span>+<span class="number">0.056</span>+<span class="number">0.001</span>+<span class="number">0.228</span>+<span class="number">0.014</span>]</span><br><span class="line"></span><br><span class="line">      = [<span class="number">0.319</span>, <span class="number">0.389</span>, <span class="number">0.502</span>, <span class="number">0.395</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 结果: MASK 位置现在融合了 "爱" 和 "天安门" 的信息！</span></span><br><span class="line"><span class="comment"># 不再是空壳 [0,0,0,0.1] 了！</span></span><br></pre></td></tr></tbody></table></figure><h4 id="Step-1-5-Feed-Forward-Network-FFN-1"><a href="#Step-1-5-Feed-Forward-Network-FFN-1" class="headerlink" title="Step 1.5: Feed-Forward Network (FFN)"></a>Step 1.5: Feed-Forward Network (FFN)</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># FFN: 768 → 3072 → 768</span></span><br><span class="line">FFN_input = H₁  <span class="comment"># [6, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第一层：升维 + 激活</span></span><br><span class="line">hidden = FFN_input @ W₁  <span class="comment"># [6, 768] @ [768, 3072] = [6, 3072]</span></span><br><span class="line">hidden = ReLU(hidden)    <span class="comment"># 负数变0，正数不变</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二层：降维</span></span><br><span class="line">FFN_out = hidden @ W₂    <span class="comment"># [6, 3072] @ [3072, 768] = [6, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MASK 位置示例</span></span><br><span class="line">FFN_out[<span class="number">3</span>] = [<span class="number">0.07</span>, <span class="number">0.12</span>, <span class="number">0.09</span>, ..., <span class="number">0.15</span>]</span><br></pre></td></tr></tbody></table></figure><p><strong>为什么要 FFN？</strong></p><ul><li>Attention 只做线性组合</li><li>FFN 引入非线性，让模型学习更复杂的模式</li></ul><h4 id="Step-1-6-残差连接-LayerNorm-1"><a href="#Step-1-6-残差连接-LayerNorm-1" class="headerlink" title="Step 1.6: 残差连接 + LayerNorm"></a>Step 1.6: 残差连接 + LayerNorm</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 残差连接：把输入加回来</span></span><br><span class="line">residual = H₁ + FFN_out  <span class="comment"># [6, 768] + [6, 768] = [6, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># MASK 位置</span></span><br><span class="line">residual[<span class="number">3</span>] = H₁[<span class="number">3</span>] + FFN_out[<span class="number">3</span>]</span><br><span class="line">            = [<span class="number">0.319</span>, <span class="number">0.389</span>, <span class="number">0.502</span>, <span class="number">0.395</span>] + [<span class="number">0.07</span>, <span class="number">0.12</span>, <span class="number">0.09</span>, <span class="number">0.15</span>]</span><br><span class="line">            = [<span class="number">0.389</span>, <span class="number">0.509</span>, <span class="number">0.592</span>, <span class="number">0.545</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># LayerNorm: 归一化</span></span><br><span class="line">X₁ = LayerNorm(residual)  <span class="comment"># [6, 768]</span></span><br></pre></td></tr></tbody></table></figure><p><strong>残差连接示意图：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">H₁ ─────────────────────────┐</span><br><span class="line"> │                          │</span><br><span class="line"> ↓                          │ (跳跃连接，防止信息丢失)</span><br><span class="line">┌─────────┐                 │</span><br><span class="line">│   FFN   │                 │</span><br><span class="line">└─────────┘                 │</span><br><span class="line"> │                          │</span><br><span class="line"> ↓                          ↓</span><br><span class="line">FFN_out ────────────→ (+) 相加 ──→ LayerNorm ──→ X₁</span><br></pre></td></tr></tbody></table></figure><p><strong>Layer 1 输出：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X₁ = [6, 768]</span><br><span class="line"></span><br><span class="line">位置3 (MASK): [0.52, 0.71, 0.34, ..., 0.82]</span><br><span class="line">              ↑</span><br><span class="line">        已经包含了 "爱___天安门" 的模式信息</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="2-2-Layer-2：第二层详细计算"><a href="#2-2-Layer-2：第二层详细计算" class="headerlink" title="2.2 Layer 2：第二层详细计算"></a>2.2 Layer 2：第二层详细计算</h3><h4 id="Step-2-1-计算新的-Q、K、V-1"><a href="#Step-2-1-计算新的-Q、K、V-1" class="headerlink" title="Step 2.1: 计算新的 Q、K、V"></a>Step 2.1: 计算新的 Q、K、V</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入是 Layer 1 的输出（已经融合过一次上下文）</span></span><br><span class="line">X₁ = [<span class="number">6</span>, <span class="number">768</span>]</span><br><span class="line"></span><br><span class="line">Q₂ = X₁ @ W_Q  <span class="comment"># 新的权重矩阵！与 Layer1 不同</span></span><br><span class="line">K₂ = X₁ @ W_K</span><br><span class="line">V₂ = X₁ @ W_V</span><br><span class="line"></span><br><span class="line"><span class="comment"># 现在 MASK 位置的 Q 已经不是空壳了</span></span><br><span class="line">Q_mask_new = X₁[<span class="number">3</span>] @ W_Q  <span class="comment"># 基于融合后的向量计算</span></span><br></pre></td></tr></tbody></table></figure><h4 id="Step-2-2-计算注意力（基于更新后的表示）-1"><a href="#Step-2-2-计算注意力（基于更新后的表示）-1" class="headerlink" title="Step 2.2: 计算注意力（基于更新后的表示）"></a>Step 2.2: 计算注意力（基于更新后的表示）</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">scores = Q₂ @ K₂.T / sqrt(<span class="number">64</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 现在 MASK 位置已经有了上下文信息</span></span><br><span class="line"><span class="comment"># 它的 Q 向量更"聪明"了，能找到更相关的词</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#         [CLS]   我     爱    MASK  天安门  [SEP]</span></span><br><span class="line"><span class="comment"># MASK   [ 0.5   1.5   2.8    0.2   3.5    0.4 ]</span></span><br><span class="line"><span class="comment">#                       ↑            ↑</span></span><br><span class="line"><span class="comment">#                   更关注"爱"   更关注"天安门"</span></span><br><span class="line"></span><br><span class="line">weights[<span class="number">3</span>] = softmax([<span class="number">0.5</span>, <span class="number">1.5</span>, <span class="number">2.8</span>, <span class="number">0.2</span>, <span class="number">3.5</span>, <span class="number">0.4</span>])</span><br><span class="line">           = [<span class="number">0.04</span>, <span class="number">0.10</span>, <span class="number">0.28</span>, <span class="number">0.02</span>, <span class="number">0.52</span>, <span class="number">0.04</span>]</span><br><span class="line"><span class="comment">#                          ↑           ↑</span></span><br><span class="line"><span class="comment">#                      28%看爱      52%看天安门（权重更高了！）</span></span><br></pre></td></tr></tbody></table></figure><h4 id="Step-2-3-加权求和-1"><a href="#Step-2-3-加权求和-1" class="headerlink" title="Step 2.3: 加权求和"></a>Step 2.3: 加权求和</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">H₂[<span class="number">3</span>] = <span class="number">0.04</span>×V_cls + <span class="number">0.10</span>×V_我 + <span class="number">0.28</span>×V_爱 </span><br><span class="line">      + <span class="number">0.02</span>×V_mask + <span class="number">0.52</span>×V_天安门 + <span class="number">0.04</span>×V_sep</span><br><span class="line"></span><br><span class="line"><span class="comment"># 关键：这次的 V_天安门 已经不是原始的了</span></span><br><span class="line"><span class="comment"># 它在 Layer1 中也融合了上下文，知道"天安门在北京"</span></span><br><span class="line"><span class="comment"># 所以 MASK 间接获得了"北京"的信息！</span></span><br><span class="line"></span><br><span class="line">H₂[<span class="number">3</span>] = [<span class="number">0.68</span>, <span class="number">0.82</span>, <span class="number">0.45</span>, ..., <span class="number">0.91</span>]</span><br></pre></td></tr></tbody></table></figure><h4 id="Step-2-4-FFN-残差-LayerNorm-1"><a href="#Step-2-4-FFN-残差-LayerNorm-1" class="headerlink" title="Step 2.4: FFN + 残差 + LayerNorm"></a>Step 2.4: FFN + 残差 + LayerNorm</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">FFN_out = FFN(H₂)</span><br><span class="line">X₂ = LayerNorm(H₂ + FFN_out)  <span class="comment"># [6, 768]</span></span><br></pre></td></tr></tbody></table></figure><p><strong>Layer 2 输出：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X₂ = [6, 768]</span><br><span class="line"></span><br><span class="line">位置3 (MASK): [0.71, 0.85, 0.52, ..., 0.93]</span><br><span class="line">              ↑</span><br><span class="line">        现在知道: "我爱___天安门" → 这个空应该填地名</span><br><span class="line">                  天安门相关 → 可能是"北京"</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="2-3-两层对比总结"><a href="#2-3-两层对比总结" class="headerlink" title="2.3 两层对比总结"></a>2.3 两层对比总结</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">┌────────────────────────────────────────────────────────────┐</span><br><span class="line">│  MASK 位置向量的演化                                        │</span><br><span class="line">├────────────────────────────────────────────────────────────┤</span><br><span class="line">│                                                            │</span><br><span class="line">│  Embedding: [0.01, 0.02, 0.01, ...]   ← 空壳，无语义        │</span><br><span class="line">│      ↓ Layer 1 Self-Attention                             │</span><br><span class="line">│  Layer 1:   [0.52, 0.71, 0.34, ...]   ← 融合了"爱""天安门"  │</span><br><span class="line">│      ↓ Layer 2 Self-Attention                             │</span><br><span class="line">│  Layer 2:   [0.71, 0.85, 0.52, ...]   ← 更深层理解          │</span><br><span class="line">│      ↓                                                     │</span><br><span class="line">│    ...      (继续 10 层)                                    │</span><br><span class="line">│      ↓                                                     │</span><br><span class="line">│  Layer 12:  [0.93, 0.87, 0.76, ...]   ← 确定是"北京"        │</span><br><span class="line">│                                                            │</span><br><span class="line">└────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><table><thead><tr><th>层</th><th>MASK 学到了什么</th><th>注意力权重变化</th></tr></thead><tbody><tr><td><strong>Embedding</strong></td><td>空的占位符</td><td>-</td></tr><tr><td><strong>Layer 1</strong></td><td>“爱”后面是宾语，”天安门”是地标</td><td>28%看”爱”，38%看”天安门”</td></tr><tr><td><strong>Layer 2</strong></td><td>“我爱X天安门”是固定搭配，X是地名</td><td>28%看”爱”，52%看”天安门”</td></tr><tr><td><strong>Layer 3-6</strong></td><td>天安门在北京，这是常识</td><td>逐渐聚焦到相关词</td></tr><tr><td><strong>Layer 7-12</strong></td><td>确定答案是”北京”，排除其他可能</td><td>高度确信</td></tr></tbody></table><hr><h2 id="🔍-Part-3-Self-Attention-核心机制-1"><a href="#🔍-Part-3-Self-Attention-核心机制-1" class="headerlink" title="🔍 Part 3: Self-Attention 核心机制"></a>🔍 Part 3: Self-Attention 核心机制</h2><h3 id="3-1-Q-x2F-K-x2F-V-的本质：信息检索系统"><a href="#3-1-Q-x2F-K-x2F-V-的本质：信息检索系统" class="headerlink" title="3.1 Q/K/V 的本质：信息检索系统"></a>3.1 Q/K/V 的本质：信息检索系统</h3><h4 id="直觉类比"><a href="#直觉类比" class="headerlink" title="直觉类比"></a>直觉类比</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">场景: 在图书馆找书</span><br><span class="line"></span><br><span class="line">Q (Query):  "我想找关于深度学习的书"</span><br><span class="line">            ↓ 计算相似度</span><br><span class="line">K (Key):    每本书的标签 ["机器学习", "烹饪", "历史", ...]</span><br><span class="line">            ↓ Softmax 归一化</span><br><span class="line">Attention:  [0.7, 0.05, 0.05, ...] (对"机器学习"书的注意力最高)</span><br><span class="line">            ↓ 加权求和</span><br><span class="line">V (Value):  书的实际内容</span><br><span class="line">            ↓</span><br><span class="line">Output:     根据注意力权重，融合最相关书籍的知识</span><br></pre></td></tr></tbody></table></figure><h4 id="数学定义"><a href="#数学定义" class="headerlink" title="数学定义"></a>数学定义</h4><p>$$<br>\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V<br>$$</p><hr><h3 id="2-2-完整数据流与维度变化"><a href="#2-2-完整数据流与维度变化" class="headerlink" title="2.2 完整数据流与维度变化"></a>2.2 完整数据流与维度变化</h3><p><strong>假设</strong>: <code>batch=2, seq=4, d_model=768, heads=12, d_k=64</code></p><h4 id="Step-1-线性投影"><a href="#Step-1-线性投影" class="headerlink" title="Step 1: 线性投影"></a>Step 1: 线性投影</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">X: [<span class="number">2</span>, <span class="number">4</span>, <span class="number">768</span>]  <span class="comment"># 输入</span></span><br><span class="line"></span><br><span class="line">Q = X @ W_Q  <span class="comment"># [2, 4, 768] @ [768, 768] = [2, 4, 768]</span></span><br><span class="line">K = X @ W_K  <span class="comment"># [2, 4, 768]</span></span><br><span class="line">V = X @ W_V  <span class="comment"># [2, 4, 768]</span></span><br></pre></td></tr></tbody></table></figure><h4 id="Step-2-拆分成多头"><a href="#Step-2-拆分成多头" class="headerlink" title="Step 2: 拆分成多头"></a>Step 2: 拆分成多头</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Q: [<span class="number">2</span>, <span class="number">4</span>, <span class="number">768</span>] → reshape → [<span class="number">2</span>, <span class="number">4</span>, <span class="number">12</span>, <span class="number">64</span>] → transpose → [<span class="number">2</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">64</span>]</span><br><span class="line">K: [<span class="number">2</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">64</span>]</span><br><span class="line">V: [<span class="number">2</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">64</span>]</span><br></pre></td></tr></tbody></table></figure><h4 id="Step-3-计算注意力分数"><a href="#Step-3-计算注意力分数" class="headerlink" title="Step 3: 计算注意力分数"></a>Step 3: 计算注意力分数</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Scores = Q @ K^T</span><br><span class="line">       = [<span class="number">2</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">64</span>] @ [<span class="number">2</span>, <span class="number">12</span>, <span class="number">64</span>, <span class="number">4</span>]</span><br><span class="line">       = [<span class="number">2</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">4</span>]  ← 这就是注意力矩阵！</span><br><span class="line"></span><br><span class="line"><span class="comment"># 每个 token 对其他 token 的关注程度</span></span><br><span class="line">例如 Scores[<span class="number">0</span>, <span class="number">0</span>, :, :] =</span><br><span class="line">       I    love   NLP    !</span><br><span class="line">  I   [<span class="number">9.2</span>   <span class="number">1.3</span>   <span class="number">2.1</span>  <span class="number">0.8</span>]</span><br><span class="line"> love [<span class="number">2.4</span>   <span class="number">8.7</span>   <span class="number">3.2</span>  <span class="number">1.1</span>]</span><br><span class="line"> NLP  [<span class="number">1.8</span>   <span class="number">3.5</span>   <span class="number">9.1</span>  <span class="number">0.9</span>]</span><br><span class="line">  !   [<span class="number">0.5</span>   <span class="number">1.2</span>   <span class="number">0.7</span>  <span class="number">8.9</span>]</span><br></pre></td></tr></tbody></table></figure><h4 id="Step-4-Scale-和-Softmax"><a href="#Step-4-Scale-和-Softmax" class="headerlink" title="Step 4: Scale 和 Softmax"></a>Step 4: Scale 和 Softmax</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Scores = Scores / sqrt(<span class="number">64</span>) ≈ Scores / <span class="number">8</span></span><br><span class="line"></span><br><span class="line">Weights = softmax(Scores, dim=-<span class="number">1</span>)  <span class="comment"># [2, 12, 4, 4]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每行和为 1</span></span><br><span class="line">例如 Weights[<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, :] = [<span class="number">0.15</span>, <span class="number">0.52</span>, <span class="number">0.28</span>, <span class="number">0.05</span>]</span><br><span class="line">表示 <span class="string">"love"</span> 对 [<span class="string">"I"</span>, <span class="string">"love"</span>, <span class="string">"NLP"</span>, <span class="string">"!"</span>] 的注意力分布</span><br></pre></td></tr></tbody></table></figure><h4 id="Step-5-加权求和"><a href="#Step-5-加权求和" class="headerlink" title="Step 5: 加权求和"></a>Step 5: 加权求和</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Output = Weights @ V</span><br><span class="line">       = [<span class="number">2</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">4</span>] @ [<span class="number">2</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">64</span>]</span><br><span class="line">       = [<span class="number">2</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">64</span>]</span><br></pre></td></tr></tbody></table></figure><h4 id="Step-6-合并多头"><a href="#Step-6-合并多头" class="headerlink" title="Step 6: 合并多头"></a>Step 6: 合并多头</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Output: [<span class="number">2</span>, <span class="number">12</span>, <span class="number">4</span>, <span class="number">64</span>] → transpose → [<span class="number">2</span>, <span class="number">4</span>, <span class="number">12</span>, <span class="number">64</span>] → reshape → [<span class="number">2</span>, <span class="number">4</span>, <span class="number">768</span>]</span><br></pre></td></tr></tbody></table></figure><p><strong>关键</strong>: 输入和输出维度完全相同！<code>[2, 4, 768]</code></p><hr><h3 id="2-3-为什么需要多头注意力？"><a href="#2-3-为什么需要多头注意力？" class="headerlink" title="2.3 为什么需要多头注意力？"></a>2.3 为什么需要多头注意力？</h3><h4 id="单头的局限"><a href="#单头的局限" class="headerlink" title="单头的局限"></a>单头的局限</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">单头: 只有一组 Q/K/V</span><br><span class="line">     只能学习一种"查询-匹配"模式</span><br></pre></td></tr></tbody></table></figure><h4 id="多头的优势"><a href="#多头的优势" class="headerlink" title="多头的优势"></a>多头的优势</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">12 个头 = 12 种不同的注意力模式</span><br><span class="line"></span><br><span class="line">Head 1: 关注语法关系 (主谓宾)</span><br><span class="line">Head 2: 关注语义相似</span><br><span class="line">Head 3: 关注位置邻近</span><br><span class="line">...</span><br><span class="line">Head 12: 关注长距离依赖</span><br><span class="line"></span><br><span class="line">最终融合 12 个头的信息 → 更丰富的表示</span><br></pre></td></tr></tbody></table></figure><hr><hr><h2 id="⚡-Part-4-KV-Cache-深度解析"><a href="#⚡-Part-4-KV-Cache-深度解析" class="headerlink" title="⚡ Part 4: KV Cache 深度解析"></a>⚡ Part 4: KV Cache 深度解析</h2><h3 id="4-1-BERT-有-Q-x2F-K-x2F-V-吗？有-KV-Cache-吗？"><a href="#4-1-BERT-有-Q-x2F-K-x2F-V-吗？有-KV-Cache-吗？" class="headerlink" title="4.1 BERT 有 Q/K/V 吗？有 KV Cache 吗？"></a>4.1 BERT 有 Q/K/V 吗？有 KV Cache 吗？</h3><table><thead><tr><th>问题</th><th>答案</th><th>原因</th></tr></thead><tbody><tr><td>BERT 有 Q、K、V 吗？</td><td>✅ <strong>有</strong></td><td>每层 Self-Attention 都要计算</td></tr><tr><td>BERT 有 KV Cache 吗？</td><td>❌ <strong>没有</strong></td><td>一次性处理，不需要缓存</td></tr><tr><td>GPT 有 KV Cache 吗？</td><td>✅ <strong>必须有</strong></td><td>逐个生成，必须缓存历史</td></tr></tbody></table><hr><h3 id="4-2-BERT-的-Q-x2F-K-x2F-V-计算"><a href="#4-2-BERT-的-Q-x2F-K-x2F-V-计算" class="headerlink" title="4.2 BERT 的 Q/K/V 计算"></a>4.2 BERT 的 Q/K/V 计算</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BERT 每一层都计算 Q/K/V</span></span><br><span class="line"><span class="comment"># 输入: "我 爱 [MASK] 天安门"</span></span><br><span class="line"><span class="comment"># X = [5, 768]  (5个token，每个768维)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 每层都要计算 Q、K、V</span></span><br><span class="line">Q = X @ W_Q  <span class="comment"># [5, 768] @ [768, 768] = [5, 768]</span></span><br><span class="line">K = X @ W_K  <span class="comment"># [5, 768] @ [768, 768] = [5, 768]</span></span><br><span class="line">V = X @ W_V  <span class="comment"># [5, 768] @ [768, 768] = [5, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算注意力</span></span><br><span class="line">scores = Q @ K.T  <span class="comment"># [5, 768] @ [768, 5] = [5, 5]</span></span><br><span class="line">weights = softmax(scores)  <span class="comment"># [5, 5]</span></span><br><span class="line">output = weights @ V  <span class="comment"># [5, 5] @ [5, 768] = [5, 768]</span></span><br></pre></td></tr></tbody></table></figure><p><strong>注意力矩阵示例：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">        我     爱    [MASK]  天安门</span><br><span class="line">我     [0.3   0.2    0.2    0.3 ]</span><br><span class="line">爱     [0.2   0.3    0.2    0.3 ]     ← 每个词看所有词</span><br><span class="line">[MASK] [0.15  0.25   0.05   0.55]     ← 双向注意力！</span><br><span class="line">天安门 [0.2   0.2    0.2    0.4 ]</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="4-3-为什么-BERT-不需要-KV-Cache？"><a href="#4-3-为什么-BERT-不需要-KV-Cache？" class="headerlink" title="4.3 为什么 BERT 不需要 KV Cache？"></a>4.3 为什么 BERT 不需要 KV Cache？</h3><h4 id="关键区别：一次性处理-vs-逐个生成"><a href="#关键区别：一次性处理-vs-逐个生成" class="headerlink" title="关键区别：一次性处理 vs 逐个生成"></a>关键区别：一次性处理 vs 逐个生成</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│  BERT（Encoder - 理解任务）：一次性处理整个句子              │</span><br><span class="line">├─────────────────────────────────────────────────────────────┤</span><br><span class="line">│                                                             │</span><br><span class="line">│  输入: "我 爱 [MASK] 天安门"   （一次性全部输入）              │</span><br><span class="line">│         ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓                                   │</span><br><span class="line">│        同时计算所有位置的 Q、K、V                             │</span><br><span class="line">│         ↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓↓                                   │</span><br><span class="line">│  输出: 同时得到所有位置的表示                                 │</span><br><span class="line">│                                                             │</span><br><span class="line">│  ✅ 只需要 1 次前向传播                                      │</span><br><span class="line">│  ❌ 不需要缓存任何东西！                                     │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br><span class="line"></span><br><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│  GPT（Decoder - 生成任务）：逐个 token 生成                  │</span><br><span class="line">├─────────────────────────────────────────────────────────────┤</span><br><span class="line">│                                                             │</span><br><span class="line">│  Step 1: 输入 "我"                                          │</span><br><span class="line">│          计算 K₁, V₁ → 💾 需要保存！                         │</span><br><span class="line">│          输出 "爱"                                          │</span><br><span class="line">│                                                             │</span><br><span class="line">│  Step 2: 输入 "爱"                                          │</span><br><span class="line">│          计算 K₂, V₂ → 💾 需要保存！                         │</span><br><span class="line">│          需要用到之前的 K₁, V₁                               │</span><br><span class="line">│          输出 "北京"                                         │</span><br><span class="line">│                                                             │</span><br><span class="line">│  Step 3: 输入 "北京"                                         │</span><br><span class="line">│          计算 K₃, V₃ → 💾 需要保存！                         │</span><br><span class="line">│          需要用到之前的 K₁, K₂, V₁, V₂                       │</span><br><span class="line">│          输出 "天安门"                                       │</span><br><span class="line">│                                                             │</span><br><span class="line">│  ✅ 每一步都需要之前所有的 K、V → 必须缓存！                  │</span><br><span class="line">│  这就是 KV Cache 的作用！                                    │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="4-4-代码对比：BERT-vs-GPT"><a href="#4-4-代码对比：BERT-vs-GPT" class="headerlink" title="4.4 代码对比：BERT vs GPT"></a>4.4 代码对比：BERT vs GPT</h3><h4 id="BERT：无需缓存"><a href="#BERT：无需缓存" class="headerlink" title="BERT：无需缓存"></a>BERT：无需缓存</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BERT: 一次搞定，不需要缓存</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bert_forward</span>(<span class="params">input_ids</span>):</span><br><span class="line">    <span class="comment"># input_ids = [101, 2769, 4263, 103, 1921, 102]  一次性输入</span></span><br><span class="line">    </span><br><span class="line">    X = embedding(input_ids)  <span class="comment"># [6, 768]</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> encoder_layers:</span><br><span class="line">        Q = X @ W_Q  <span class="comment"># 同时算所有位置</span></span><br><span class="line">        K = X @ W_K  <span class="comment"># 同时算所有位置</span></span><br><span class="line">        V = X @ W_V  <span class="comment"># 同时算所有位置</span></span><br><span class="line">        </span><br><span class="line">        scores = Q @ K.T / sqrt(d_k)</span><br><span class="line">        weights = softmax(scores)</span><br><span class="line">        X = weights @ V  <span class="comment"># 一次完成</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X  <span class="comment"># 直接返回，不需要保存任何中间结果</span></span><br></pre></td></tr></tbody></table></figure><h4 id="GPT：必须缓存"><a href="#GPT：必须缓存" class="headerlink" title="GPT：必须缓存"></a>GPT：必须缓存</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GPT: 逐个生成，必须缓存 K、V</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gpt_generate</span>(<span class="params">prompt</span>):</span><br><span class="line">    kv_cache = []  <span class="comment"># 必须有这个！</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(max_tokens):</span><br><span class="line">        <span class="keyword">if</span> step == <span class="number">0</span>:</span><br><span class="line">            X = embedding(prompt)  <span class="comment"># 第一步处理整个 prompt</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X = embedding(new_token)  <span class="comment"># 之后每步只处理新 token</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> layer_idx, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(decoder_layers):</span><br><span class="line">            Q = X @ W_Q  <span class="comment"># 只算当前 token 的 Q</span></span><br><span class="line">            K_new = X @ W_K</span><br><span class="line">            V_new = X @ W_V</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 拼接历史的 K、V</span></span><br><span class="line">            <span class="keyword">if</span> step &gt; <span class="number">0</span>:</span><br><span class="line">                K = concat(kv_cache[layer_idx][<span class="number">0</span>], K_new)  <span class="comment"># 拼接！</span></span><br><span class="line">                V = concat(kv_cache[layer_idx][<span class="number">1</span>], V_new)  <span class="comment"># 拼接！</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                K, V = K_new, V_new</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 更新缓存</span></span><br><span class="line">            <span class="keyword">if</span> layer_idx &gt;= <span class="built_in">len</span>(kv_cache):</span><br><span class="line">                kv_cache.append((K, V))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                kv_cache[layer_idx] = (K, V)  <span class="comment"># 保存！</span></span><br><span class="line">            </span><br><span class="line">            scores = Q @ K.T / sqrt(d_k)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Causal Mask: 只能看左边</span></span><br><span class="line">            mask = causal_mask(Q.size(<span class="number">1</span>), K.size(<span class="number">1</span>))</span><br><span class="line">            scores = scores.masked_fill(mask, -inf)</span><br><span class="line">            </span><br><span class="line">            weights = softmax(scores)</span><br><span class="line">            X = weights @ V</span><br><span class="line">        </span><br><span class="line">        new_token = predict_next(X)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> generated_text</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="4-5-为什么-GPT-必须缓存-K、V？"><a href="#4-5-为什么-GPT-必须缓存-K、V？" class="headerlink" title="4.5 为什么 GPT 必须缓存 K、V？"></a>4.5 为什么 GPT 必须缓存 K、V？</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">生成 "我爱北京天安门" 的过程：</span><br><span class="line"></span><br><span class="line">Step 1: 输入 "我"</span><br><span class="line">        Q₁ @ K₁.T → 只有自己看自己</span><br><span class="line">        输出: "爱"</span><br><span class="line">        💾 缓存: K₁, V₁</span><br><span class="line"></span><br><span class="line">Step 2: 输入 "爱"  </span><br><span class="line">        需要计算: Q₂ @ [K₁, K₂].T  ← 需要之前的 K₁！</span><br><span class="line">        如果不缓存 K₁，就要重新算 → 浪费！</span><br><span class="line">        输出: "北京"</span><br><span class="line">        💾 缓存: K₁, K₂, V₁, V₂</span><br><span class="line">        </span><br><span class="line">Step 3: 输入 "北京"</span><br><span class="line">        需要计算: Q₃ @ [K₁, K₂, K₃].T  ← 需要 K₁, K₂！</span><br><span class="line">        如果不缓存，要重新算 K₁, K₂ → 更浪费！</span><br><span class="line">        输出: "天安门"</span><br><span class="line">        💾 缓存: K₁, K₂, K₃, V₁, V₂, V₃</span><br><span class="line"></span><br><span class="line">Step N: </span><br><span class="line">        需要计算: Qₙ @ [K₁, K₂, ..., Kₙ].T</span><br><span class="line">        </span><br><span class="line">不缓存: 计算量 = 1+2+3+...+N = O(N²)</span><br><span class="line">缓存:   计算量 = 1+1+1+...+1 = O(N)</span><br><span class="line">加速比: N 倍！</span><br></pre></td></tr></tbody></table></figure><p><strong>为什么要加到当前词上？</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Step 2 生成 "爱" 时</span></span><br><span class="line">Q_爱 @ [K_我, K_爱].T  <span class="comment"># "爱" 需要看到 "我"</span></span><br><span class="line">      ↑</span><br><span class="line">  必须包含之前的 K_我！</span><br><span class="line"></span><br><span class="line"><span class="comment"># Step 3 生成 "北京" 时  </span></span><br><span class="line">Q_北京 @ [K_我, K_爱, K_北京].T  <span class="comment"># "北京" 需要看到 "我" 和 "爱"</span></span><br><span class="line">       ↑</span><br><span class="line">   必须包含所有历史 K！</span><br><span class="line"></span><br><span class="line"><span class="comment"># 这就是为什么要把前面的 K、V 加到当前的计算中</span></span><br></pre></td></tr></tbody></table></figure><hr><h3 id="4-6-架构总结对比"><a href="#4-6-架构总结对比" class="headerlink" title="4.6 架构总结对比"></a>4.6 架构总结对比</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                    BERT vs GPT                              │</span><br><span class="line">├──────────────────────────┬──────────────────────────────────┤</span><br><span class="line">│         BERT             │            GPT                   │</span><br><span class="line">├──────────────────────────┼──────────────────────────────────┤</span><br><span class="line">│  Encoder-only 架构        │  Decoder-only 架构                │</span><br><span class="line">│  双向注意力               │  单向注意力（Causal Mask）         │</span><br><span class="line">│  一次性输入整个句子        │  逐个 token 生成                 │</span><br><span class="line">│  输入输出长度相同          │  输出比输入长                    │</span><br><span class="line">│  并行计算所有位置          │  串行生成                        │</span><br><span class="line">├──────────────────────────┼──────────────────────────────────┤</span><br><span class="line">│  ✅ 有 Q、K、V            │  ✅ 有 Q、K、V                   │</span><br><span class="line">│  ❌ 不需要 KV Cache       │  ✅ 必须 KV Cache                │</span><br><span class="line">├──────────────────────────┼──────────────────────────────────┤</span><br><span class="line">│  用途: 理解               │  用途: 生成                      │</span><br><span class="line">│  任务: 分类、NER、问答    │  任务: 聊天、写作、代码生成       │</span><br><span class="line">│  例子: "这是___评论"→正面 │  例子: "从前有" → "座山"         │</span><br><span class="line">└──────────────────────────┴──────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="4-7-混合架构：T5-x2F-BART"><a href="#4-7-混合架构：T5-x2F-BART" class="headerlink" title="4.7 混合架构：T5/BART"></a>4.7 混合架构：T5/BART</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">T5/BART = Encoder + Decoder</span><br><span class="line"></span><br><span class="line">┌─────────────────────────────────────────────────────────┐</span><br><span class="line">│  Encoder 部分（类似 BERT）                               │</span><br><span class="line">│  输入: "Translate to English: 我爱你"                    │</span><br><span class="line">│  处理: 一次性编码                                        │</span><br><span class="line">│  ❌ 不需要 KV Cache                                     │</span><br><span class="line">└─────────────────────────────────────────────────────────┘</span><br><span class="line">              ↓ 传递编码结果</span><br><span class="line">┌─────────────────────────────────────────────────────────┐</span><br><span class="line">│  Decoder 部分（类似 GPT）                                │</span><br><span class="line">│  生成: "I" → "love" → "you"                             │</span><br><span class="line">│  处理: 逐个生成                                          │</span><br><span class="line">│  ✅ 需要 KV Cache（仅 Decoder 部分）                     │</span><br><span class="line">└─────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="4-8-记忆口诀"><a href="#4-8-记忆口诀" class="headerlink" title="4.8 记忆口诀"></a>4.8 记忆口诀</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">Encoder (BERT) = 阅读理解 = 一眼看完全文 = 不需要缓存</span><br><span class="line">Decoder (GPT)  = 写作文   = 一字一字写   = 需要缓存历史</span><br><span class="line"></span><br><span class="line">判断标准:</span><br><span class="line">  - 输入输出同时存在？ → Encoder → 无 KV Cache</span><br><span class="line">  - 逐步生成新内容？   → Decoder → 有 KV Cache</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="⚡-Part-5-KV-Cache-深度解析"><a href="#⚡-Part-5-KV-Cache-深度解析" class="headerlink" title="⚡ Part 5: KV Cache 深度解析"></a>⚡ Part 5: KV Cache 深度解析</h2><h3 id="3-1-问题场景：自回归生成"><a href="#3-1-问题场景：自回归生成" class="headerlink" title="3.1 问题场景：自回归生成"></a>3.1 问题场景：自回归生成</h3><p>GPT 生成文本 “I love NLP so much”：</p><p><strong>无 KV Cache (低效)</strong>:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">Step 1: 输入 "I"</span><br><span class="line">        计算 Q₁, K₁, V₁ → 输出 "love"</span><br><span class="line">        </span><br><span class="line">Step 2: 输入 "I love"</span><br><span class="line">        重新计算 Q₁, K₁, V₁  ← 浪费！</span><br><span class="line">        重新计算 Q₂, K₂, V₂  ← 浪费！</span><br><span class="line">        → 输出 "NLP"</span><br><span class="line">        </span><br><span class="line">Step 3: 输入 "I love NLP"</span><br><span class="line">        重新计算 Q₁, K₁, V₁  ← 浪费！</span><br><span class="line">        重新计算 Q₂, K₂, V₂  ← 浪费！</span><br><span class="line">        重新计算 Q₃, K₃, V₃  ← 浪费！</span><br><span class="line">        → 输出 "so"</span><br></pre></td></tr></tbody></table></figure><p><strong>计算量</strong>: 1 + 2 + 3 + … + n = <strong>O(n²)</strong></p><hr><h3 id="3-2-KV-Cache-解决方案"><a href="#3-2-KV-Cache-解决方案" class="headerlink" title="3.2 KV Cache 解决方案"></a>3.2 KV Cache 解决方案</h3><p><strong>核心洞察</strong>: K 和 V 只依赖输入，与”当前要生成什么”无关 → <strong>可以缓存！</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Step 1: 输入 "I"</span><br><span class="line">        计算 K₁, V₁ → 存入 Cache</span><br><span class="line">        计算 Q₁ → Attention(Q₁, [K₁], [V₁]) → 输出 "love"</span><br><span class="line">        </span><br><span class="line">Step 2: 输入 "love"</span><br><span class="line">        计算 K₂, V₂ → 追加到 Cache</span><br><span class="line">        只计算 Q₂ → Attention(Q₂, [K₁,K₂], [V₁,V₂]) → 输出 "NLP"</span><br><span class="line">        </span><br><span class="line">Step 3: 输入 "NLP"</span><br><span class="line">        计算 K₃, V₃ → 追加到 Cache</span><br><span class="line">        只计算 Q₃ → Attention(Q₃, [K₁,K₂,K₃], [V₁,V₂,V₃]) → 输出 "so"</span><br></pre></td></tr></tbody></table></figure><p><strong>计算量</strong>: 1 + 1 + 1 + … + 1 = <strong>O(n)</strong></p><p><strong>加速比</strong>: n²/n = <strong>n 倍加速</strong>！</p><hr><h3 id="3-3-内存消耗分析"><a href="#3-3-内存消耗分析" class="headerlink" title="3.3 内存消耗分析"></a>3.3 内存消耗分析</h3><h4 id="公式"><a href="#公式" class="headerlink" title="公式"></a>公式</h4><p>$$<br>\text{KV Cache Size} = 2 \times n_{\text{layers}} \times d_{\text{model}} \times \text{seq_len} \times \text{dtype_size}<br>$$</p><h4 id="实际案例：LLaMA-7B"><a href="#实际案例：LLaMA-7B" class="headerlink" title="实际案例：LLaMA-7B"></a>实际案例：LLaMA-7B</h4><ul><li><code>n_layers = 32</code></li><li><code>d_model = 4096</code></li><li><code>dtype = float16</code> (2 bytes)</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">每个 token 的 KV Cache:</span><br><span class="line">= <span class="number">2</span> × <span class="number">32</span> × <span class="number">4096</span> × <span class="number">2</span> <span class="built_in">bytes</span></span><br><span class="line">= <span class="number">524</span>,<span class="number">288</span> <span class="built_in">bytes</span></span><br><span class="line">= <span class="number">512</span> KB / token</span><br><span class="line"></span><br><span class="line">不同序列长度的显存占用:</span><br><span class="line">- 1K tokens:   <span class="number">512</span> MB</span><br><span class="line">- 4K tokens:   <span class="number">2</span> GB</span><br><span class="line">- 32K tokens:  <span class="number">16</span> GB  ← 长上下文的挑战！</span><br><span class="line">- 128K tokens: <span class="number">64</span> GB  ← 需要多卡或优化技术</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="3-4-KV-Cache-优化技术"><a href="#3-4-KV-Cache-优化技术" class="headerlink" title="3.4 KV Cache 优化技术"></a>3.4 KV Cache 优化技术</h3><h4 id="1-Multi-Query-Attention-MQA"><a href="#1-Multi-Query-Attention-MQA" class="headerlink" title="1. Multi-Query Attention (MQA)"></a>1. Multi-Query Attention (MQA)</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">标准 Multi-Head: 每个头都有独立的 K, V</span><br><span class="line">MQA: 所有头共享同一组 K, V</span><br><span class="line"></span><br><span class="line">显存节省: heads 倍 (例如 12 头 → 节省 12 倍)</span><br></pre></td></tr></tbody></table></figure><h4 id="2-Grouped-Query-Attention-GQA"><a href="#2-Grouped-Query-Attention-GQA" class="headerlink" title="2. Grouped-Query Attention (GQA)"></a>2. Grouped-Query Attention (GQA)</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">折中方案: 将 12 个头分成 3 组，每组共享 K, V</span><br><span class="line"></span><br><span class="line">LLaMA-2 使用 GQA:</span><br><span class="line">- 32 个头 → 8 组</span><br><span class="line">- 显存节省: 4 倍</span><br></pre></td></tr></tbody></table></figure><h4 id="3-PagedAttention-vLLM"><a href="#3-PagedAttention-vLLM" class="headerlink" title="3. PagedAttention (vLLM)"></a>3. PagedAttention (vLLM)</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">类似操作系统的分页机制</span><br><span class="line">将 KV Cache 分成固定大小的块 (Page)</span><br><span class="line">动态分配和回收，减少碎片</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="🎭-Part-4-Causal-Attention-vs-Bidirectional-Attention"><a href="#🎭-Part-4-Causal-Attention-vs-Bidirectional-Attention" class="headerlink" title="🎭 Part 4: Causal Attention vs Bidirectional Attention"></a>🎭 Part 4: Causal Attention vs Bidirectional Attention</h2><h3 id="4-1-核心区别"><a href="#4-1-核心区别" class="headerlink" title="4.1 核心区别"></a>4.1 核心区别</h3><table><thead><tr><th>维度</th><th>Bidirectional (BERT)</th><th>Causal (GPT)</th></tr></thead><tbody><tr><td><strong>可见范围</strong></td><td>全局可见</td><td>仅左侧可见</td></tr><tr><td><strong>Mask 形状</strong></td><td>全 1 (或仅 mask padding)</td><td>下三角矩阵</td></tr><tr><td><strong>适用任务</strong></td><td>理解 (分类、NER、QA)</td><td>生成 (文本、对话)</td></tr><tr><td><strong>训练效率</strong></td><td>高 (并行计算所有位置)</td><td>高 (teacher forcing)</td></tr><tr><td><strong>推理模式</strong></td><td>一次性输出</td><td>逐 token 生成</td></tr></tbody></table><hr><h3 id="4-2-Causal-Mask-的数学实现"><a href="#4-2-Causal-Mask-的数学实现" class="headerlink" title="4.2 Causal Mask 的数学实现"></a>4.2 Causal Mask 的数学实现</h3><h4 id="Mask-矩阵"><a href="#Mask-矩阵" class="headerlink" title="Mask 矩阵"></a>Mask 矩阵</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">seq_len = <span class="number">4</span></span><br><span class="line">mask = np.tril(np.ones((<span class="number">4</span>, <span class="number">4</span>)))</span><br><span class="line"></span><br><span class="line">       I   love  NLP   !</span><br><span class="line">  I   [<span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>    <span class="number">0</span>]</span><br><span class="line"> love [<span class="number">1</span>    <span class="number">1</span>    <span class="number">0</span>    <span class="number">0</span>]</span><br><span class="line"> NLP  [<span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">0</span>]</span><br><span class="line">  !   [<span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>    <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line">解释:</span><br><span class="line">- <span class="string">"I"</span> 只能看到自己</span><br><span class="line">- <span class="string">"love"</span> 能看到 <span class="string">"I"</span> 和自己</span><br><span class="line">- <span class="string">"NLP"</span> 能看到 <span class="string">"I"</span>, <span class="string">"love"</span>, <span class="string">"NLP"</span></span><br><span class="line">- <span class="string">"!"</span> 能看到所有</span><br></pre></td></tr></tbody></table></figure><h4 id="应用到-Attention-Scores"><a href="#应用到-Attention-Scores" class="headerlink" title="应用到 Attention Scores"></a>应用到 Attention Scores</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">scores = Q @ K.T / sqrt(d_k)  <span class="comment"># [4, 4]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 将上三角设为 -inf</span></span><br><span class="line">masked_scores = np.where(mask == <span class="number">1</span>, scores, -np.inf)</span><br><span class="line"></span><br><span class="line">       I     love    NLP     !</span><br><span class="line">  I   [<span class="number">2.1</span>   -inf   -inf   -inf]</span><br><span class="line"> love [<span class="number">1.3</span>   <span class="number">3.2</span>    -inf   -inf]</span><br><span class="line"> NLP  [<span class="number">0.8</span>   <span class="number">1.9</span>    <span class="number">2.7</span>    -inf]</span><br><span class="line">  !   [<span class="number">0.5</span>   <span class="number">1.1</span>    <span class="number">0.9</span>    <span class="number">3.5</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Softmax 后, -inf 位置变成 0</span></span><br><span class="line">weights = softmax(masked_scores)</span><br><span class="line"></span><br><span class="line">       I     love    NLP     !</span><br><span class="line">  I   [<span class="number">1.0</span>   <span class="number">0.0</span>    <span class="number">0.0</span>    <span class="number">0.0</span>]  ← 只看自己</span><br><span class="line"> love [<span class="number">0.3</span>   <span class="number">0.7</span>    <span class="number">0.0</span>    <span class="number">0.0</span>]  ← <span class="number">70</span>% 看自己, <span class="number">30</span>% 看 I</span><br><span class="line"> NLP  [<span class="number">0.1</span>   <span class="number">0.3</span>    <span class="number">0.6</span>    <span class="number">0.0</span>]  ← 主要看自己和 love</span><br><span class="line">  !   [<span class="number">0.05</span>  <span class="number">0.2</span>    <span class="number">0.15</span>   <span class="number">0.6</span>]  ← <span class="number">60</span>% 看自己</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="🔄-Part-5-训练闭环-Label-x2F-Loss-x2F-梯度流"><a href="#🔄-Part-5-训练闭环-Label-x2F-Loss-x2F-梯度流" class="headerlink" title="🔄 Part 5: 训练闭环 - Label / Loss / 梯度流"></a>🔄 Part 5: 训练闭环 - Label / Loss / 梯度流</h2><h3 id="5-1-MLM-任务的完整训练流程"><a href="#5-1-MLM-任务的完整训练流程" class="headerlink" title="5.1 MLM 任务的完整训练流程"></a>5.1 MLM 任务的完整训练流程</h3><h4 id="数据准备"><a href="#数据准备" class="headerlink" title="数据准备"></a>数据准备</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">原始文本: <span class="string">"我爱北京天安门"</span></span><br><span class="line">处理后:   <span class="string">"我爱[MASK]天安门"</span></span><br><span class="line"></span><br><span class="line">input_ids = [<span class="number">101</span>, <span class="number">2769</span>, <span class="number">4263</span>, <span class="number">103</span>, <span class="number">1921</span>, <span class="number">2128</span>, <span class="number">7305</span>, <span class="number">102</span>]</span><br><span class="line">             [CLS]  我    爱   MASK  天   安    门  [SEP]</span><br><span class="line">labels    = [-<span class="number">100</span>, -<span class="number">100</span>, -<span class="number">100</span>, <span class="number">1266</span>, -<span class="number">100</span>, -<span class="number">100</span>, -<span class="number">100</span>, -<span class="number">100</span>]</span><br><span class="line">                                 ↑</span><br><span class="line">                               <span class="string">"北京"</span> 的 ID</span><br></pre></td></tr></tbody></table></figure><h4 id="Forward-Pass"><a href="#Forward-Pass" class="headerlink" title="Forward Pass"></a>Forward Pass</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. Embedding</span></span><br><span class="line">embeddings = token_emb + pos_emb + seg_emb  <span class="comment"># [1, 8, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 12 层 Transformer</span></span><br><span class="line">hidden = embeddings</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> encoder_layers:</span><br><span class="line">    hidden = layer(hidden)  <span class="comment"># [1, 8, 768]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. MLM Head</span></span><br><span class="line">logits = mlm_head(hidden)  <span class="comment"># [1, 8, 21128]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 只取 [MASK] 位置 (index=3)</span></span><br><span class="line">masked_logits = logits[<span class="number">0</span>, <span class="number">3</span>, :]  <span class="comment"># [21128]</span></span><br></pre></td></tr></tbody></table></figure><h4 id="Loss-计算"><a href="#Loss-计算" class="headerlink" title="Loss 计算"></a>Loss 计算</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">true_label = <span class="number">1266</span>  <span class="comment"># "北京"</span></span><br><span class="line"></span><br><span class="line">loss = CrossEntropyLoss(masked_logits, true_label)</span><br><span class="line">     = -log(softmax(masked_logits)[<span class="number">1266</span>])</span><br><span class="line">     </span><br><span class="line">如果模型预测:</span><br><span class="line">  P(<span class="string">"北京"</span>) = <span class="number">0.8</span>  → loss = -log(<span class="number">0.8</span>) = <span class="number">0.22</span> (好)</span><br><span class="line">  P(<span class="string">"北京"</span>) = <span class="number">0.1</span>  → loss = -log(<span class="number">0.1</span>) = <span class="number">2.30</span> (差)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="5-2-梯度反向传播"><a href="#5-2-梯度反向传播" class="headerlink" title="5.2 梯度反向传播"></a>5.2 梯度反向传播</h3><h4 id="梯度流动路径"><a href="#梯度流动路径" class="headerlink" title="梯度流动路径"></a>梯度流动路径</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">      Loss (标量)</span><br><span class="line">        ↓ ∂L/∂logits</span><br><span class="line">   MLM Head (线性层)</span><br><span class="line">        ↓ ∂L/∂h₁₂</span><br><span class="line">Transformer Layer 12</span><br><span class="line">        ↓</span><br><span class="line">      ...</span><br><span class="line">        ↓</span><br><span class="line">Transformer Layer 1</span><br><span class="line">  ↙    ↓    ↘</span><br><span class="line">∂L/∂Q  ∂L/∂K  ∂L/∂V</span><br><span class="line">  ↓     ↓     ↓</span><br><span class="line">W_Q   W_K   W_V  ← 这些权重被更新！</span><br></pre></td></tr></tbody></table></figure><h4 id="Attention-中的梯度分叉"><a href="#Attention-中的梯度分叉" class="headerlink" title="Attention 中的梯度分叉"></a>Attention 中的梯度分叉</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Forward</span></span><br><span class="line">attn_weights = softmax(Q @ K.T / sqrt(d_k))  <span class="comment"># [seq, seq]</span></span><br><span class="line">output = attn_weights @ V                     <span class="comment"># [seq, dim]</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Backward</span></span><br><span class="line">∂L/∂V = attn_weights.T @ ∂L/∂output  ← V 的梯度</span><br><span class="line">∂L/∂attn = ∂L/∂output @ V.T          ← 注意力权重的梯度</span><br><span class="line">∂L/∂scores = ∂softmax(∂L/∂attn)     ← Softmax 反向</span><br><span class="line">∂L/∂Q = ∂L/∂scores @ K               ← Q 的梯度</span><br><span class="line">∂L/∂K = ∂L/∂scores.T @ Q             ← K 的梯度</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="5-3-为什么-Attention-能学到语义？"><a href="#5-3-为什么-Attention-能学到语义？" class="headerlink" title="5.3 为什么 Attention 能学到语义？"></a>5.3 为什么 Attention 能学到语义？</h3><h4 id="梯度的”指导作用”"><a href="#梯度的”指导作用”" class="headerlink" title="梯度的”指导作用”"></a>梯度的”指导作用”</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">假设当前预测:</span><br><span class="line">  [MASK] 位置预测 "上海" (错误, 应该是 "北京")</span><br><span class="line">  </span><br><span class="line">Loss 很大 → 梯度回传:</span><br><span class="line">  </span><br><span class="line">1. 流向 V:</span><br><span class="line">   "你们提供的内容不对！'上海' 的语义特征太强了"</span><br><span class="line">   → 调整 V，让 "北京" 相关的 token 提供更多信息</span><br><span class="line">   </span><br><span class="line">2. 流向 Q 和 K:</span><br><span class="line">   "'爱' 和 '天安门' 的注意力权重不对！"</span><br><span class="line">   → 调整 Q/K，让 [MASK] 更多关注 "天安门"</span><br><span class="line">   → 因为 "北京 + 天安门" 共现频率高</span><br><span class="line">   </span><br><span class="line">3. 多轮迭代后:</span><br><span class="line">   模型学会: "看到'天安门' → 联想到'北京'"</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="📚-总结：核心要点回顾"><a href="#📚-总结：核心要点回顾" class="headerlink" title="📚 总结：核心要点回顾"></a>📚 总结：核心要点回顾</h2><h3 id="BERT-论文核心贡献"><a href="#BERT-论文核心贡献" class="headerlink" title="BERT 论文核心贡献"></a>BERT 论文核心贡献</h3><ol><li><strong>Masked Language Model</strong>: 实现深度双向建模</li><li><strong>大规模预训练 + 微调</strong>: 开创预训练范式</li><li><strong>SOTA 性能</strong>: 在 11 个任务上刷新记录</li></ol><h3 id="Q-x2F-K-x2F-V-机制本质"><a href="#Q-x2F-K-x2F-V-机制本质" class="headerlink" title="Q/K/V 机制本质"></a>Q/K/V 机制本质</h3><ul><li><strong>Q</strong>: 提问 “我想找什么信息？”</li><li><strong>K</strong>: 索引 “我这里有什么信息？”</li><li><strong>V</strong>: 内容 “实际的信息是什么？”</li><li><strong>Attention</strong>: 根据 Q-K 相似度，加权聚合 V</li></ul><h3 id="KV-Cache-优化"><a href="#KV-Cache-优化" class="headerlink" title="KV Cache 优化"></a>KV Cache 优化</h3><ul><li><strong>问题</strong>: 自回归生成时重复计算 K/V → O(n²)</li><li><strong>方案</strong>: 缓存历史 K/V → O(n)</li><li><strong>代价</strong>: 显存占用 (LLaMA-7B: 512KB/token)</li></ul><h3 id="Causal-vs-Bidirectional"><a href="#Causal-vs-Bidirectional" class="headerlink" title="Causal vs Bidirectional"></a>Causal vs Bidirectional</h3><ul><li><strong>Causal</strong>: 下三角 mask, 用于生成</li><li><strong>Bidirectional</strong>: 全局可见, 用于理解</li></ul><h3 id="训练闭环"><a href="#训练闭环" class="headerlink" title="训练闭环"></a>训练闭环</h3><ul><li><strong>Label</strong> → <strong>Loss</strong> → <strong>Gradient</strong> → <strong>Update</strong></li><li>梯度流过 Attention 时分叉到 Q/K/V</li><li>通过反向传播，模型学会”在哪里找信息”和”找什么信息”</li></ul><hr><h2 id="🆚-Part-8-Encoder-vs-Decoder-KV-Cache-对比"><a href="#🆚-Part-8-Encoder-vs-Decoder-KV-Cache-对比" class="headerlink" title="🆚 Part 8: Encoder vs Decoder - KV Cache 对比"></a>🆚 Part 8: Encoder vs Decoder - KV Cache 对比</h2><h3 id="核心结论"><a href="#核心结论" class="headerlink" title="核心结论"></a>核心结论</h3><table><thead><tr><th>模型</th><th>架构</th><th>有 KV Cache？</th><th>原因</th></tr></thead><tbody><tr><td><strong>BERT</strong></td><td>Encoder-only</td><td>❌ 没有</td><td>一次性处理整个输入</td></tr><tr><td><strong>GPT</strong></td><td>Decoder-only</td><td>✅ 有</td><td>逐个 token 生成</td></tr><tr><td><strong>T5/BART</strong></td><td>Encoder + Decoder</td><td>⚠️ Decoder 部分有</td><td>Encoder 不需要，Decoder 需要</td></tr></tbody></table><hr><h3 id="BERT-Encoder-only-不需要-KV-Cache"><a href="#BERT-Encoder-only-不需要-KV-Cache" class="headerlink" title="BERT (Encoder-only) - 不需要 KV Cache"></a>BERT (Encoder-only) - 不需要 KV Cache</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输入完整句子</span></span><br><span class="line"><span class="built_in">input</span> = <span class="string">"我 爱 [MASK] 天安门"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 一次性全部处理</span></span><br><span class="line">output = bert(<span class="built_in">input</span>)  <span class="comment"># 同时计算所有位置的 Q、K、V</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 流程</span></span><br><span class="line">Step <span class="number">1</span>: 输入全部 token [<span class="number">6</span>个]</span><br><span class="line">Step <span class="number">2</span>: 同时计算所有位置的 K、V</span><br><span class="line">Step <span class="number">3</span>: 同时计算所有位置的 Attention</span><br><span class="line">Step <span class="number">4</span>: 同时输出所有位置的结果</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算量: O(n) - 只计算一次</span></span><br><span class="line"><span class="comment"># 不需要 KV Cache！</span></span><br></pre></td></tr></tbody></table></figure><hr><h3 id="GPT-Decoder-only-必须有-KV-Cache"><a href="#GPT-Decoder-only-必须有-KV-Cache" class="headerlink" title="GPT (Decoder-only) - 必须有 KV Cache"></a>GPT (Decoder-only) - 必须有 KV Cache</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 逐个生成</span></span><br><span class="line">Step <span class="number">1</span>: <span class="built_in">input</span>=<span class="string">"我"</span>     </span><br><span class="line">        计算 K₁, V₁ → 缓存</span><br><span class="line">        生成 <span class="string">"爱"</span></span><br><span class="line"></span><br><span class="line">Step <span class="number">2</span>: <span class="built_in">input</span>=<span class="string">"爱"</span>     </span><br><span class="line">        计算 K₂, V₂ → 缓存</span><br><span class="line">        使用 K₁, V₁ + K₂, V₂</span><br><span class="line">        生成 <span class="string">"北京"</span></span><br><span class="line"></span><br><span class="line">Step <span class="number">3</span>: <span class="built_in">input</span>=<span class="string">"北京"</span>   </span><br><span class="line">        计算 K₃, V₃ → 缓存</span><br><span class="line">        使用 K₁, V₁ + K₂, V₂ + K₃, V₃</span><br><span class="line">        生成 <span class="string">"天安门"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 不用缓存: 计算量 = 1+2+3+...+N = O(N²)</span></span><br><span class="line"><span class="comment"># 用缓存: 计算量 = 1+1+1+...+1 = O(N)</span></span><br><span class="line"><span class="comment"># 必须有 KV Cache！</span></span><br></pre></td></tr></tbody></table></figure><hr><h3 id="为什么-Decoder-必须缓存-K、V？"><a href="#为什么-Decoder-必须缓存-K、V？" class="headerlink" title="为什么 Decoder 必须缓存 K、V？"></a>为什么 Decoder 必须缓存 K、V？</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">生成 "我爱北京天安门" 的过程：</span><br><span class="line"></span><br><span class="line">Step 1: 输入 "我"</span><br><span class="line">        Q₁ @ K₁.T → 只有自己看自己</span><br><span class="line">        输出: "爱"</span><br><span class="line"></span><br><span class="line">Step 2: 输入 "爱"  </span><br><span class="line">        需要计算: Q₂ @ [K₁, K₂].T  ← 需要之前的 K₁！</span><br><span class="line">        如果不缓存，就要重新计算 K₁ → 浪费！</span><br><span class="line">        </span><br><span class="line">Step 3: 输入 "北京"</span><br><span class="line">        需要计算: Q₃ @ [K₁, K₂, K₃].T  ← 需要 K₁, K₂！</span><br><span class="line">        如果不缓存，要重新计算 K₁, K₂ → 更浪费！</span><br><span class="line"></span><br><span class="line">Step N: </span><br><span class="line">        需要计算: Qₙ @ [K₁, K₂, ..., Kₙ].T</span><br><span class="line">        不缓存的话，计算量 = 1+2+3+...+N = O(N²)</span><br><span class="line">        缓存的话，每步只算新的，计算量 = O(N)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="BERT-有-Q-x2F-K-x2F-V，但为什么不需要缓存？"><a href="#BERT-有-Q-x2F-K-x2F-V，但为什么不需要缓存？" class="headerlink" title="BERT 有 Q/K/V，但为什么不需要缓存？"></a>BERT 有 Q/K/V，但为什么不需要缓存？</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># BERT: 一次搞定，不需要缓存</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bert_forward</span>(<span class="params">input_ids</span>):</span><br><span class="line">    <span class="comment"># input_ids = [101, 2769, 4263, 103, 1921, 102]  一次性输入</span></span><br><span class="line">    </span><br><span class="line">    X = embedding(input_ids)  <span class="comment"># [6, 768]</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> layer <span class="keyword">in</span> encoder_layers:</span><br><span class="line">        Q = X @ W_Q  <span class="comment"># 同时算所有位置的 Q</span></span><br><span class="line">        K = X @ W_K  <span class="comment"># 同时算所有位置的 K</span></span><br><span class="line">        V = X @ W_V  <span class="comment"># 同时算所有位置的 V</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 所有位置的 Attention 同时计算</span></span><br><span class="line">        scores = Q @ K.T  <span class="comment"># [6, 6]</span></span><br><span class="line">        weights = softmax(scores)</span><br><span class="line">        X = weights @ V  <span class="comment"># [6, 768]</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> X  <span class="comment"># 直接返回，不需要保存任何中间结果</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># GPT: 逐个生成，必须缓存 K、V</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">gpt_generate</span>(<span class="params">prompt</span>):</span><br><span class="line">    kv_cache = []  <span class="comment"># 必须有这个！</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(max_tokens):</span><br><span class="line">        <span class="keyword">if</span> step == <span class="number">0</span>:</span><br><span class="line">            X = embedding(prompt)  <span class="comment"># 第一步处理整个 prompt</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            X = embedding(new_token)  <span class="comment"># 之后每步只处理新 token</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> layer_idx, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(decoder_layers):</span><br><span class="line">            Q = X @ W_Q  <span class="comment"># 只算当前 token 的 Q</span></span><br><span class="line">            K_new = X @ W_K</span><br><span class="line">            V_new = X @ W_V</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 拼接历史的 K、V</span></span><br><span class="line">            <span class="keyword">if</span> kv_cache[layer_idx] <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                K = concat(kv_cache[layer_idx][<span class="number">0</span>], K_new)  <span class="comment"># 拼接！</span></span><br><span class="line">                V = concat(kv_cache[layer_idx][<span class="number">1</span>], V_new)  <span class="comment"># 拼接！</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                K = K_new</span><br><span class="line">                V = V_new</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 更新缓存</span></span><br><span class="line">            kv_cache[layer_idx] = (K, V)  <span class="comment"># 保存！</span></span><br><span class="line">            </span><br><span class="line">            <span class="comment"># Causal Attention (只看左边)</span></span><br><span class="line">            scores = Q @ K.T  <span class="comment"># [1, seq_len]</span></span><br><span class="line">            weights = softmax(scores)</span><br><span class="line">            X = weights @ V</span><br><span class="line">        </span><br><span class="line">        new_token = predict_next(X)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> generated_text</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="一图总结"><a href="#一图总结" class="headerlink" title="一图总结"></a>一图总结</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                    BERT vs GPT                              │</span><br><span class="line">├──────────────────────────┬──────────────────────────────────┤</span><br><span class="line">│         BERT             │            GPT                   │</span><br><span class="line">├──────────────────────────┼──────────────────────────────────┤</span><br><span class="line">│  Encoder 架构             │  Decoder 架构                    │</span><br><span class="line">│  双向注意力               │  单向注意力（Causal）             │</span><br><span class="line">│  一次性输入整个句子        │  逐个 token 生成                 │</span><br><span class="line">│  输入输出长度相同          │  输出比输入长                    │</span><br><span class="line">├──────────────────────────┼──────────────────────────────────┤</span><br><span class="line">│  ✅ 有 Q、K、V            │  ✅ 有 Q、K、V                   │</span><br><span class="line">│  ❌ 不需要 KV Cache       │  ✅ 必须 KV Cache                │</span><br><span class="line">├──────────────────────────┼──────────────────────────────────┤</span><br><span class="line">│  用途: 理解               │  用途: 生成                      │</span><br><span class="line">│  分类、NER、问答          │  聊天、写作、代码生成             │</span><br><span class="line">└──────────────────────────┴──────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="记忆口诀"><a href="#记忆口诀" class="headerlink" title="记忆口诀"></a>记忆口诀</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">BERT = 阅读理解 = 一眼看完全文 = 不需要缓存</span><br><span class="line">GPT  = 写作文   = 一个字一个字写 = 需要记住前面写了什么</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="❓-Part-9-常见问答-FAQ"><a href="#❓-Part-9-常见问答-FAQ" class="headerlink" title="❓ Part 9: 常见问答 FAQ"></a>❓ Part 9: 常见问答 FAQ</h2><h3 id="Q1-MASK-Token-为什么能预测出正确答案？"><a href="#Q1-MASK-Token-为什么能预测出正确答案？" class="headerlink" title="Q1: [MASK] Token 为什么能预测出正确答案？"></a>Q1: [MASK] Token 为什么能预测出正确答案？</h3><p><strong>A</strong>: [MASK] 的 Token ID (103) 本身不代表任何语义，但经过 12 层 Self-Attention 后：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">Embedding:  103 → [0.01, 0.02, ...]  空壳</span><br><span class="line">    ↓ Layer 1: 吸收 "爱" 和 "天安门" 的信息</span><br><span class="line">Layer 1:    [0.52, 0.71, ...]  开始有语义</span><br><span class="line">    ↓ Layer 2-12: 不断精炼</span><br><span class="line">Layer 12:   [0.93, 0.87, ...]  完全理解上下文</span><br><span class="line"></span><br><span class="line">最终这个向量在语义空间中接近 "北京"！</span><br></pre></td></tr></tbody></table></figure><p><strong>关键</strong>: Self-Attention 让 [MASK] 从周围词”偷”信息！</p><hr><h3 id="Q2-BERT-为什么只用-Encoder，不用-Decoder？"><a href="#Q2-BERT-为什么只用-Encoder，不用-Decoder？" class="headerlink" title="Q2: BERT 为什么只用 Encoder，不用 Decoder？"></a>Q2: BERT 为什么只用 Encoder，不用 Decoder？</h3><p><strong>A</strong>: 因为 BERT 的任务是”理解”，不是”生成”</p><table><thead><tr><th>任务类型</th><th>需要 Decoder？</th><th>原因</th></tr></thead><tbody><tr><td><strong>机器翻译</strong></td><td>✅ 需要</td><td>输入中文，输出英文，是不同的序列</td></tr><tr><td><strong>文本分类</strong></td><td>❌ 不需要</td><td>只需理解输入，输出一个类别</td></tr><tr><td><strong>NER</strong></td><td>❌ 不需要</td><td>只需给每个输入词打标签</td></tr><tr><td><strong>问答</strong></td><td>❌ 不需要</td><td>答案在原文中，只需找位置</td></tr></tbody></table><p><strong>简单说</strong>: Encoder 理解输入，Decoder 生成输出。BERT 只需要理解！</p><hr><h3 id="Q3-残差连接是什么？为什么需要它？"><a href="#Q3-残差连接是什么？为什么需要它？" class="headerlink" title="Q3: 残差连接是什么？为什么需要它？"></a>Q3: 残差连接是什么？为什么需要它？</h3><p><strong>A</strong>: 残差连接就是把输入直接加到输出上</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不用残差</span></span><br><span class="line">output = FFN(<span class="built_in">input</span>)  <span class="comment"># 可能丢失原始信息</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用残差</span></span><br><span class="line">output = <span class="built_in">input</span> + FFN(<span class="built_in">input</span>)  <span class="comment"># 保底 + 增量</span></span><br></pre></td></tr></tbody></table></figure><p><strong>好处</strong>:</p><ol><li>梯度可以直接跳过 FFN 回传 → 解决梯度消失</li><li>原始信息不会丢失 → 网络可以更深</li><li>FFN 只需学习”差异” → 更容易训练</li></ol><hr><h3 id="Q4-预训练和微调有什么区别？"><a href="#Q4-预训练和微调有什么区别？" class="headerlink" title="Q4: 预训练和微调有什么区别？"></a>Q4: 预训练和微调有什么区别？</h3><p><strong>A</strong>: </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">预训练（Pre-training）:</span><br><span class="line">  - 数据: 海量无标注文本 (数十亿词)</span><br><span class="line">  - 任务: MLM 完形填空</span><br><span class="line">  - 目的: 学会"理解语言"</span><br><span class="line">  - 耗时: 数周 (只做一次)</span><br><span class="line">  </span><br><span class="line">微调（Fine-tuning）:</span><br><span class="line">  - 数据: 少量标注数据 (1000条)</span><br><span class="line">  - 任务: 具体任务 (情感分析、NER...)</span><br><span class="line">  - 目的: 适配特定任务</span><br><span class="line">  - 耗时: 几小时 (每个任务都要)</span><br></pre></td></tr></tbody></table></figure><p><strong>类比</strong>: 预训练 = 上大学，微调 = 岗位培训</p><hr><h3 id="Q5-MLM-为什么要-80-x2F-10-x2F-10-的策略？"><a href="#Q5-MLM-为什么要-80-x2F-10-x2F-10-的策略？" class="headerlink" title="Q5: MLM 为什么要 80%/10%/10% 的策略？"></a>Q5: MLM 为什么要 80%/10%/10% 的策略？</h3><p><strong>A</strong>: </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">100% [MASK]: 预训练和微调分布不一致</span><br><span class="line">  预训练: "我爱[MASK]天安门"</span><br><span class="line">  微调:   "我爱北京天安门"  ← 没有 [MASK]！</span><br><span class="line">  问题: 模型过度依赖 [MASK] 符号</span><br><span class="line"></span><br><span class="line">80% [MASK] + 10% 随机 + 10% 不变:</span><br><span class="line">  - 80% [MASK]: 主要学习目标</span><br><span class="line">  - 10% 随机: 让模型学会纠错</span><br><span class="line">  - 10% 不变: 适配真实分布</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="Q6-BERT-的-Q、K、V-是干什么的？"><a href="#Q6-BERT-的-Q、K、V-是干什么的？" class="headerlink" title="Q6: BERT 的 Q、K、V 是干什么的？"></a>Q6: BERT 的 Q、K、V 是干什么的？</h3><p><strong>A</strong>: </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Q (Query):  "我想找什么信息？"</span><br><span class="line">K (Key):    "我这里有什么信息？"</span><br><span class="line">V (Value):  "实际的信息内容"</span><br><span class="line"></span><br><span class="line">Attention = 根据 Q 和 K 的相似度，加权求和 V</span><br><span class="line"></span><br><span class="line">例子: [MASK] 位置</span><br><span class="line">  Q_mask: "我需要知道这个空填什么"</span><br><span class="line">  K_天安门: "我是天安门"</span><br><span class="line">  相似度高 → [MASK] 多看 V_天安门</span><br><span class="line">  → [MASK] 获得"天安门在北京"的信息</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="Q7-Encoder-和-Decoder-的注意力有什么区别？"><a href="#Q7-Encoder-和-Decoder-的注意力有什么区别？" class="headerlink" title="Q7: Encoder 和 Decoder 的注意力有什么区别？"></a>Q7: Encoder 和 Decoder 的注意力有什么区别？</h3><p><strong>A</strong>: </p><table><thead><tr><th>维度</th><th>Encoder</th><th>Decoder</th></tr></thead><tbody><tr><td><strong>可见范围</strong></td><td>全局可见（双向）</td><td>只看左边（单向）</td></tr><tr><td><strong>Mask</strong></td><td>无 mask 或仅 padding</td><td>Causal mask (下三角)</td></tr><tr><td><strong>用途</strong></td><td>理解整个句子</td><td>生成下一个词</td></tr><tr><td><strong>KV Cache</strong></td><td>❌ 不需要</td><td>✅ 需要</td></tr></tbody></table><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Encoder: [CLS] 可以看到 "我爱北京天安门" 所有词</span><br><span class="line">Decoder: "北京" 只能看到 "我爱北京"，不能看到 "天安门"</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="Q8-为什么-BERT-需要-CLS-和-SEP-Token？"><a href="#Q8-为什么-BERT-需要-CLS-和-SEP-Token？" class="headerlink" title="Q8: 为什么 BERT 需要 [CLS] 和 [SEP] Token？"></a>Q8: 为什么 BERT 需要 [CLS] 和 [SEP] Token？</h3><p><strong>A</strong>: </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">[CLS] (Classification):</span><br><span class="line">  - 位置: 句子开头</span><br><span class="line">  - 作用: 汇聚整个句子的语义</span><br><span class="line">  - 用途: 分类任务取 [CLS] 的向量</span><br><span class="line"></span><br><span class="line">[SEP] (Separator):</span><br><span class="line">  - 位置: 句子结尾，或两个句子之间</span><br><span class="line">  - 作用: 分隔不同句子</span><br><span class="line">  - 用途: 让模型知道句子边界</span><br><span class="line"></span><br><span class="line">例子:</span><br><span class="line">  单句: [CLS] 这个产品很好 [SEP]</span><br><span class="line">  句对: [CLS] 天在下雨 [SEP] 地面是湿的 [SEP]</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="Q9-BERT-的层数越多越好吗？"><a href="#Q9-BERT-的层数越多越好吗？" class="headerlink" title="Q9: BERT 的层数越多越好吗？"></a>Q9: BERT 的层数越多越好吗？</h3><p><strong>A</strong>: 不一定！</p><table><thead><tr><th>模型</th><th>层数</th><th>参数量</th><th>性能</th><th>问题</th></tr></thead><tbody><tr><td>BERT-Base</td><td>12</td><td>110M</td><td>84.6%</td><td>-</td></tr><tr><td>BERT-Large</td><td>24</td><td>340M</td><td>86.7%</td><td>训练慢、需要更多数据</td></tr><tr><td>BERT-超大</td><td>48+</td><td>1B+</td><td>提升有限</td><td>过拟合、推理慢</td></tr></tbody></table><p><strong>结论</strong>: 12-24 层是甜蜜点，更多层边际收益递减</p><hr><h3 id="Q10-BERT-和-GPT-可以结合使用吗？"><a href="#Q10-BERT-和-GPT-可以结合使用吗？" class="headerlink" title="Q10: BERT 和 GPT 可以结合使用吗？"></a>Q10: BERT 和 GPT 可以结合使用吗？</h3><p><strong>A</strong>: 可以！这就是 Encoder-Decoder 架构</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">T5 / BART:</span><br><span class="line">  输入 → BERT-like Encoder (理解)</span><br><span class="line">      → GPT-like Decoder (生成)</span><br><span class="line">      → 输出</span><br><span class="line"></span><br><span class="line">适用场景:</span><br><span class="line">  - 机器翻译</span><br><span class="line">  - 文本摘要</span><br><span class="line">  - 对话生成</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="🔗-延伸阅读"><a href="#🔗-延伸阅读" class="headerlink" title="🔗 延伸阅读"></a>🔗 延伸阅读</h2><ol><li><strong>论文原文</strong>: <a href="https://arxiv.org/abs/1810.04805">BERT (arxiv.org/abs/1810.04805)</a></li><li><strong>后续改进</strong>:<ul><li>RoBERTa: 移除 NSP, 更大批次训练</li><li>ALBERT: 参数共享, 减少模型大小</li><li>ELECTRA: 判别式预训练, 更高效</li></ul></li><li><strong>KV Cache 优化</strong>:<ul><li>PagedAttention: vLLM 的核心技术</li><li>FlashAttention: IO 优化的注意力算法</li></ul></li><li><strong>Transformer 原文</strong>: <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a></li></ol><hr><h2 id="🚀-Part-6-BERT-实际应用场景与代码实现"><a href="#🚀-Part-6-BERT-实际应用场景与代码实现" class="headerlink" title="🚀 Part 6: BERT 实际应用场景与代码实现"></a>🚀 Part 6: BERT 实际应用场景与代码实现</h2><h3 id="6-1-应用场景总览"><a href="#6-1-应用场景总览" class="headerlink" title="6.1 应用场景总览"></a>6.1 应用场景总览</h3><table><thead><tr><th>应用场景</th><th>任务类型</th><th>输入格式</th><th>输出</th><th>典型应用</th></tr></thead><tbody><tr><td>情感分析</td><td>单句分类</td><td><code>[CLS] 文本 [SEP]</code></td><td>类别标签</td><td>商品评论、舆情监控</td></tr><tr><td>文本匹配</td><td>句对分类</td><td><code>[CLS] 句A [SEP] 句B [SEP]</code></td><td>相似度/关系</td><td>智能客服、问答匹配</td></tr><tr><td>命名实体识别</td><td>序列标注</td><td><code>[CLS] 文本 [SEP]</code></td><td>每个token标签</td><td>信息抽取、知识图谱</td></tr><tr><td>阅读理解</td><td>抽取式QA</td><td><code>[CLS] 问题 [SEP] 文章 [SEP]</code></td><td>答案起止位置</td><td>智能问答、客服机器人</td></tr><tr><td>文本生成</td><td>Seq2Seq</td><td>需配合Decoder</td><td>生成文本</td><td>摘要、翻译 (需BART/T5)</td></tr></tbody></table><hr><h3 id="6-2-情感分析完整实现"><a href="#6-2-情感分析完整实现" class="headerlink" title="6.2 情感分析完整实现"></a>6.2 情感分析完整实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">场景: 电商评论情感分析</span></span><br><span class="line"><span class="string">输入: "这个手机拍照效果很棒，电池也耐用"</span></span><br><span class="line"><span class="string">输出: 正面 (0.95)</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForSequenceClassification</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 加载模型</span></span><br><span class="line">model_name = <span class="string">"bert-base-chinese"</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(model_name)</span><br><span class="line">model = BertForSequenceClassification.from_pretrained(</span><br><span class="line">    model_name, </span><br><span class="line">    num_labels=<span class="number">2</span>  <span class="comment"># 正面/负面</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 数据准备</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prepare_data</span>(<span class="params">texts, labels</span>):</span><br><span class="line">    encodings = tokenizer(</span><br><span class="line">        texts,</span><br><span class="line">        padding=<span class="literal">True</span>,</span><br><span class="line">        truncation=<span class="literal">True</span>,</span><br><span class="line">        max_length=<span class="number">128</span>,</span><br><span class="line">        return_tensors=<span class="string">"pt"</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> encodings, torch.tensor(labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例数据</span></span><br><span class="line">train_texts = [</span><br><span class="line">    <span class="string">"这个手机拍照效果很棒，电池也耐用"</span>,</span><br><span class="line">    <span class="string">"质量太差了，用了一天就坏了"</span>,</span><br><span class="line">    <span class="string">"物流很快，包装完好，好评"</span>,</span><br><span class="line">    <span class="string">"客服态度恶劣，再也不买了"</span></span><br><span class="line">]</span><br><span class="line">train_labels = [<span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>]  <span class="comment"># 1=正面, 0=负面</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 训练循环</span></span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> AdamW</span><br><span class="line"></span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">2e-5</span>)</span><br><span class="line">model.train()</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    encodings, labels = prepare_data(train_texts, train_labels)</span><br><span class="line">    </span><br><span class="line">    outputs = model(</span><br><span class="line">        input_ids=encodings[<span class="string">"input_ids"</span>],</span><br><span class="line">        attention_mask=encodings[<span class="string">"attention_mask"</span>],</span><br><span class="line">        labels=labels</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    loss = outputs.loss</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Epoch <span class="subst">{epoch+<span class="number">1</span>}</span>, Loss: <span class="subst">{loss.item():<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 推理预测</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">test_text = <span class="string">"这款产品性价比超高，强烈推荐！"</span></span><br><span class="line">inputs = tokenizer(test_text, return_tensors=<span class="string">"pt"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = model(**inputs)</span><br><span class="line">    probs = torch.softmax(outputs.logits, dim=-<span class="number">1</span>)</span><br><span class="line">    pred = torch.argmax(probs, dim=-<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"预测: <span class="subst">{<span class="string">'正面'</span> <span class="keyword">if</span> pred==<span class="number">1</span> <span class="keyword">else</span> <span class="string">'负面'</span>}</span>, 置信度: <span class="subst">{probs[<span class="number">0</span>][pred].item():<span class="number">.2</span>%}</span>"</span>)</span><br><span class="line"><span class="comment"># 输出: 预测: 正面, 置信度: 94.32%</span></span><br></pre></td></tr></tbody></table></figure><hr><h3 id="6-3-命名实体识别-NER-实现"><a href="#6-3-命名实体识别-NER-实现" class="headerlink" title="6.3 命名实体识别 (NER) 实现"></a>6.3 命名实体识别 (NER) 实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">场景: 从新闻中提取人名、地名、机构名</span></span><br><span class="line"><span class="string">输入: "马云在杭州创办了阿里巴巴公司"</span></span><br><span class="line"><span class="string">输出: [("马云", "PER"), ("杭州", "LOC"), ("阿里巴巴公司", "ORG")]</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizerFast, BertForTokenClassification</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># NER 标签定义 (BIO格式)</span></span><br><span class="line">label_list = [<span class="string">"O"</span>, <span class="string">"B-PER"</span>, <span class="string">"I-PER"</span>, <span class="string">"B-LOC"</span>, <span class="string">"I-LOC"</span>, <span class="string">"B-ORG"</span>, <span class="string">"I-ORG"</span>]</span><br><span class="line">label2id = {l: i <span class="keyword">for</span> i, l <span class="keyword">in</span> <span class="built_in">enumerate</span>(label_list)}</span><br><span class="line">id2label = {i: l <span class="keyword">for</span> i, l <span class="keyword">in</span> <span class="built_in">enumerate</span>(label_list)}</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">tokenizer = BertTokenizerFast.from_pretrained(<span class="string">"bert-base-chinese"</span>)</span><br><span class="line">model = BertForTokenClassification.from_pretrained(</span><br><span class="line">    <span class="string">"bert-base-chinese"</span>,</span><br><span class="line">    num_labels=<span class="built_in">len</span>(label_list),</span><br><span class="line">    id2label=id2label,</span><br><span class="line">    label2id=label2id</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 推理函数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">extract_entities</span>(<span class="params">text</span>):</span><br><span class="line">    inputs = tokenizer(text, return_tensors=<span class="string">"pt"</span>, return_offsets_mapping=<span class="literal">True</span>)</span><br><span class="line">    offset_mapping = inputs.pop(<span class="string">"offset_mapping"</span>)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">        predictions = torch.argmax(outputs.logits, dim=-<span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    entities = []</span><br><span class="line">    current_entity = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> idx, (pred, offset) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(predictions, offset_mapping)):</span><br><span class="line">        <span class="keyword">if</span> offset[<span class="number">0</span>] == offset[<span class="number">1</span>]:  <span class="comment"># 跳过特殊token</span></span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">        label = id2label[pred.item()]</span><br><span class="line">        char = text[offset[<span class="number">0</span>]:offset[<span class="number">1</span>]]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> label.startswith(<span class="string">"B-"</span>):</span><br><span class="line">            <span class="keyword">if</span> current_entity:</span><br><span class="line">                entities.append(current_entity)</span><br><span class="line">            current_entity = {<span class="string">"text"</span>: char, <span class="string">"type"</span>: label[<span class="number">2</span>:]}</span><br><span class="line">        <span class="keyword">elif</span> label.startswith(<span class="string">"I-"</span>) <span class="keyword">and</span> current_entity:</span><br><span class="line">            current_entity[<span class="string">"text"</span>] += char</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> current_entity:</span><br><span class="line">                entities.append(current_entity)</span><br><span class="line">                current_entity = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> current_entity:</span><br><span class="line">        entities.append(current_entity)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> [(e[<span class="string">"text"</span>], e[<span class="string">"type"</span>]) <span class="keyword">for</span> e <span class="keyword">in</span> entities]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">text = <span class="string">"马云在杭州创办了阿里巴巴公司"</span></span><br><span class="line"><span class="built_in">print</span>(extract_entities(text))</span><br><span class="line"><span class="comment"># 输出: [("马云", "PER"), ("杭州", "LOC"), ("阿里巴巴公司", "ORG")]</span></span><br></pre></td></tr></tbody></table></figure><hr><h3 id="6-4-语义相似度匹配"><a href="#6-4-语义相似度匹配" class="headerlink" title="6.4 语义相似度匹配"></a>6.4 语义相似度匹配</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">场景: 智能客服FAQ匹配</span></span><br><span class="line"><span class="string">输入: 用户问题 + FAQ库</span></span><br><span class="line"><span class="string">输出: 最相似的FAQ及答案</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">"bert-base-chinese"</span>)</span><br><span class="line">model = BertModel.from_pretrained(<span class="string">"bert-base-chinese"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_sentence_embedding</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">"""获取句子的BERT表示 (使用[CLS]向量)"""</span></span><br><span class="line">    inputs = tokenizer(text, return_tensors=<span class="string">"pt"</span>, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">    <span class="comment"># 使用 [CLS] token 的输出作为句子表示</span></span><br><span class="line">    <span class="keyword">return</span> outputs.last_hidden_state[:, <span class="number">0</span>, :]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_similarity</span>(<span class="params">text1, text2</span>):</span><br><span class="line">    <span class="string">"""计算两个句子的余弦相似度"""</span></span><br><span class="line">    emb1 = get_sentence_embedding(text1)</span><br><span class="line">    emb2 = get_sentence_embedding(text2)</span><br><span class="line">    <span class="keyword">return</span> F.cosine_similarity(emb1, emb2).item()</span><br><span class="line"></span><br><span class="line"><span class="comment"># FAQ库</span></span><br><span class="line">faq_database = [</span><br><span class="line">    {<span class="string">"question"</span>: <span class="string">"如何修改密码？"</span>, <span class="string">"answer"</span>: <span class="string">"请进入设置-账户安全-修改密码"</span>},</span><br><span class="line">    {<span class="string">"question"</span>: <span class="string">"怎么申请退款？"</span>, <span class="string">"answer"</span>: <span class="string">"在订单详情页点击申请退款按钮"</span>},</span><br><span class="line">    {<span class="string">"question"</span>: <span class="string">"配送需要多久？"</span>, <span class="string">"answer"</span>: <span class="string">"一般3-5个工作日送达"</span>},</span><br><span class="line">    {<span class="string">"question"</span>: <span class="string">"支持哪些支付方式？"</span>, <span class="string">"answer"</span>: <span class="string">"支持微信、支付宝、银行卡支付"</span>},</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">find_best_match</span>(<span class="params">user_query</span>):</span><br><span class="line">    <span class="string">"""找到最匹配的FAQ"""</span></span><br><span class="line">    best_score = -<span class="number">1</span></span><br><span class="line">    best_faq = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> faq <span class="keyword">in</span> faq_database:</span><br><span class="line">        score = compute_similarity(user_query, faq[<span class="string">"question"</span>])</span><br><span class="line">        <span class="keyword">if</span> score &gt; best_score:</span><br><span class="line">            best_score = score</span><br><span class="line">            best_faq = faq</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> best_faq, best_score</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">query = <span class="string">"我想改一下登录密码"</span></span><br><span class="line">faq, score = find_best_match(query)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"用户问题: <span class="subst">{query}</span>"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"匹配FAQ: <span class="subst">{faq[<span class="string">'question'</span>]}</span> (相似度: <span class="subst">{score:<span class="number">.2</span>%}</span>)"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">f"回答: <span class="subst">{faq[<span class="string">'answer'</span>]}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># 用户问题: 我想改一下登录密码</span></span><br><span class="line"><span class="comment"># 匹配FAQ: 如何修改密码？ (相似度: 89.34%)</span></span><br><span class="line"><span class="comment"># 回答: 请进入设置-账户安全-修改密码</span></span><br></pre></td></tr></tbody></table></figure><hr><h3 id="6-5-阅读理解问答系统"><a href="#6-5-阅读理解问答系统" class="headerlink" title="6.5 阅读理解问答系统"></a>6.5 阅读理解问答系统</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">场景: 从文档中找答案</span></span><br><span class="line"><span class="string">输入: 问题 + 文章</span></span><br><span class="line"><span class="string">输出: 答案文本及位置</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizerFast, BertForQuestionAnswering</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line">tokenizer = BertTokenizerFast.from_pretrained(<span class="string">"bert-base-chinese"</span>)</span><br><span class="line">model = BertForQuestionAnswering.from_pretrained(</span><br><span class="line">    <span class="string">"luhua/chinese_pretrain_mrc_roberta_wwm_ext_large"</span>  <span class="comment"># 中文QA模型</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">answer_question</span>(<span class="params">question, context</span>):</span><br><span class="line">    <span class="string">"""从文章中抽取答案"""</span></span><br><span class="line">    inputs = tokenizer(</span><br><span class="line">        question, </span><br><span class="line">        context, </span><br><span class="line">        return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">        max_length=<span class="number">512</span>,</span><br><span class="line">        truncation=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 获取答案起止位置</span></span><br><span class="line">    start_idx = torch.argmax(outputs.start_logits)</span><br><span class="line">    end_idx = torch.argmax(outputs.end_logits)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 解码答案</span></span><br><span class="line">    tokens = tokenizer.convert_ids_to_tokens(inputs[<span class="string">"input_ids"</span>][<span class="number">0</span>])</span><br><span class="line">    answer = tokenizer.convert_tokens_to_string(tokens[start_idx:end_idx+<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算置信度</span></span><br><span class="line">    start_prob = torch.softmax(outputs.start_logits, dim=-<span class="number">1</span>)[<span class="number">0</span>][start_idx].item()</span><br><span class="line">    end_prob = torch.softmax(outputs.end_logits, dim=-<span class="number">1</span>)[<span class="number">0</span>][end_idx].item()</span><br><span class="line">    confidence = (start_prob + end_prob) / <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> answer, confidence</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">context = <span class="string">"""</span></span><br><span class="line"><span class="string">阿里巴巴集团由马云于1999年在中国杭州创立。</span></span><br><span class="line"><span class="string">公司最初是一个B2B网上交易市场，后来发展成为一个多元化的科技公司。</span></span><br><span class="line"><span class="string">2014年，阿里巴巴在纽约证券交易所上市，创造了当时全球最大的IPO纪录。</span></span><br><span class="line"><span class="string">目前阿里巴巴的核心业务包括电子商务、云计算、数字媒体和娱乐。</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">questions = [</span><br><span class="line">    <span class="string">"阿里巴巴是谁创立的？"</span>,</span><br><span class="line">    <span class="string">"阿里巴巴是哪一年上市的？"</span>,</span><br><span class="line">    <span class="string">"阿里巴巴的核心业务有哪些？"</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> q <span class="keyword">in</span> questions:</span><br><span class="line">    answer, conf = answer_question(q, context)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Q: <span class="subst">{q}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"A: <span class="subst">{answer}</span> (置信度: <span class="subst">{conf:<span class="number">.2</span>%}</span>)\n"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># Q: 阿里巴巴是谁创立的？</span></span><br><span class="line"><span class="comment"># A: 马云 (置信度: 92.15%)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Q: 阿里巴巴是哪一年上市的？</span></span><br><span class="line"><span class="comment"># A: 2014年 (置信度: 88.73%)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># Q: 阿里巴巴的核心业务有哪些？</span></span><br><span class="line"><span class="comment"># A: 电子商务、云计算、数字媒体和娱乐 (置信度: 85.21%)</span></span><br></pre></td></tr></tbody></table></figure><hr><h2 id="⚡-Part-7-KV-Cache-优化实践与代码"><a href="#⚡-Part-7-KV-Cache-优化实践与代码" class="headerlink" title="⚡ Part 7: KV Cache 优化实践与代码"></a>⚡ Part 7: KV Cache 优化实践与代码</h2><h3 id="7-1-KV-Cache-原理可视化"><a href="#7-1-KV-Cache-原理可视化" class="headerlink" title="7.1 KV Cache 原理可视化"></a>7.1 KV Cache 原理可视化</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">无 KV Cache (每次重新计算):</span><br><span class="line">═══════════════════════════════════════════════════════════</span><br><span class="line">Step 1: "I"           → 计算 K₁,V₁,Q₁ → 输出 "love"</span><br><span class="line">Step 2: "I love"      → 计算 K₁,V₁,K₂,V₂,Q₂ → 输出 "NLP"  </span><br><span class="line">Step 3: "I love NLP"  → 计算 K₁,V₁,K₂,V₂,K₃,V₃,Q₃ → 输出 "!"</span><br><span class="line">                        ↑ 重复计算!</span><br><span class="line"></span><br><span class="line">有 KV Cache (缓存复用):</span><br><span class="line">═══════════════════════════════════════════════════════════</span><br><span class="line">Step 1: "I"    → 计算 K₁,V₁ [存入Cache] → Q₁ → "love"</span><br><span class="line">Step 2: "love" → 计算 K₂,V₂ [追加Cache] → Q₂ @ [K₁K₂] → "NLP"</span><br><span class="line">Step 3: "NLP"  → 计算 K₃,V₃ [追加Cache] → Q₃ @ [K₁K₂K₃] → "!"</span><br><span class="line">                 ↑ 只计算新token的KV!</span><br></pre></td></tr></tbody></table></figure><h3 id="7-2-KV-Cache-完整实现"><a href="#7-2-KV-Cache-完整实现" class="headerlink" title="7.2 KV Cache 完整实现"></a>7.2 KV Cache 完整实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">从零实现带 KV Cache 的 GPT 推理</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CachedMultiHeadAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model=<span class="number">768</span>, n_heads=<span class="number">12</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.d_k = d_model // n_heads</span><br><span class="line">        </span><br><span class="line">        self.W_q = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_k = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_v = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_o = nn.Linear(d_model, d_model)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, kv_cache=<span class="literal">None</span>, use_cache=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: [batch, seq_len, d_model] 输入</span></span><br><span class="line"><span class="string">            kv_cache: (cached_k, cached_v) 缓存的K和V</span></span><br><span class="line"><span class="string">            use_cache: 是否使用和更新缓存</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output: [batch, seq_len, d_model]</span></span><br><span class="line"><span class="string">            new_cache: 更新后的缓存</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        batch_size = x.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算当前token的 Q, K, V</span></span><br><span class="line">        Q = self.W_q(x)  <span class="comment"># [batch, seq, d_model]</span></span><br><span class="line">        K = self.W_k(x)</span><br><span class="line">        V = self.W_v(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 如果有缓存，拼接历史 K, V</span></span><br><span class="line">        <span class="keyword">if</span> kv_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            cached_k, cached_v = kv_cache</span><br><span class="line">            K = torch.cat([cached_k, K], dim=<span class="number">1</span>)  <span class="comment"># [batch, cached+seq, d_model]</span></span><br><span class="line">            V = torch.cat([cached_v, V], dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 保存新缓存</span></span><br><span class="line">        new_cache = (K, V) <span class="keyword">if</span> use_cache <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 重塑为多头格式</span></span><br><span class="line">        <span class="keyword">def</span> <span class="title function_">reshape_for_heads</span>(<span class="params">t</span>):</span><br><span class="line">            <span class="comment"># [batch, seq, d_model] -&gt; [batch, heads, seq, d_k]</span></span><br><span class="line">            <span class="keyword">return</span> t.view(batch_size, -<span class="number">1</span>, self.n_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        Q = reshape_for_heads(Q)  <span class="comment"># [batch, heads, q_seq, d_k]</span></span><br><span class="line">        K = reshape_for_heads(K)  <span class="comment"># [batch, heads, kv_seq, d_k]</span></span><br><span class="line">        V = reshape_for_heads(V)  <span class="comment"># [batch, heads, kv_seq, d_k]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算注意力分数</span></span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / math.sqrt(self.d_k)</span><br><span class="line">        <span class="comment"># [batch, heads, q_seq, kv_seq]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Causal Mask (只看过去)</span></span><br><span class="line">        q_len, kv_len = Q.size(<span class="number">2</span>), K.size(<span class="number">2</span>)</span><br><span class="line">        causal_mask = torch.triu(</span><br><span class="line">            torch.ones(q_len, kv_len, device=x.device), </span><br><span class="line">            diagonal=kv_len - q_len + <span class="number">1</span></span><br><span class="line">        ).<span class="built_in">bool</span>()</span><br><span class="line">        scores = scores.masked_fill(causal_mask, <span class="built_in">float</span>(<span class="string">'-inf'</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Softmax + 加权求和</span></span><br><span class="line">        attn_weights = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">        output = torch.matmul(attn_weights, V)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 合并多头</span></span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, self.d_model)</span><br><span class="line">        output = self.W_o(output)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, new_cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GPTWithKVCache</span>(nn.Module):</span><br><span class="line">    <span class="string">"""简化的 GPT 模型，支持 KV Cache"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, vocab_size=<span class="number">50257</span>, d_model=<span class="number">768</span>, n_layers=<span class="number">12</span>, n_heads=<span class="number">12</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, d_model)</span><br><span class="line">        self.layers = nn.ModuleList([</span><br><span class="line">            CachedMultiHeadAttention(d_model, n_heads) </span><br><span class="line">            <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n_layers)</span><br><span class="line">        ])</span><br><span class="line">        self.lm_head = nn.Linear(d_model, vocab_size)</span><br><span class="line">        self.n_layers = n_layers</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, input_ids, past_kv_cache=<span class="literal">None</span>, use_cache=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            input_ids: [batch, seq_len]</span></span><br><span class="line"><span class="string">            past_kv_cache: List of (K, V) for each layer</span></span><br><span class="line"><span class="string">            use_cache: 是否使用KV Cache</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        x = self.embedding(input_ids)</span><br><span class="line">        </span><br><span class="line">        new_cache = []</span><br><span class="line">        <span class="keyword">for</span> i, layer <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.layers):</span><br><span class="line">            layer_cache = past_kv_cache[i] <span class="keyword">if</span> past_kv_cache <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">            x, cache = layer(x, kv_cache=layer_cache, use_cache=use_cache)</span><br><span class="line">            new_cache.append(cache)</span><br><span class="line">        </span><br><span class="line">        logits = self.lm_head(x)</span><br><span class="line">        <span class="keyword">return</span> logits, new_cache <span class="keyword">if</span> use_cache <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_with_kv_cache</span>(<span class="params">model, prompt_ids, max_new_tokens=<span class="number">50</span></span>):</span><br><span class="line">    <span class="string">"""使用 KV Cache 进行高效生成"""</span></span><br><span class="line">    </span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    generated = prompt_ids.clone()</span><br><span class="line">    past_cache = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="comment"># 第一步：处理整个 prompt</span></span><br><span class="line">        logits, past_cache = model(prompt_ids, past_kv_cache=<span class="literal">None</span>, use_cache=<span class="literal">True</span>)</span><br><span class="line">        next_token = torch.argmax(logits[:, -<span class="number">1</span>, :], dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        generated = torch.cat([generated, next_token], dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 后续步骤：只处理新 token</span></span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens - <span class="number">1</span>):</span><br><span class="line">            <span class="comment"># 只输入最后一个 token！</span></span><br><span class="line">            logits, past_cache = model(</span><br><span class="line">                next_token,  <span class="comment"># [batch, 1] 只有一个token</span></span><br><span class="line">                past_kv_cache=past_cache,</span><br><span class="line">                use_cache=<span class="literal">True</span></span><br><span class="line">            )</span><br><span class="line">            next_token = torch.argmax(logits[:, -<span class="number">1</span>, :], dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            generated = torch.cat([generated, next_token], dim=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> generated</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 性能对比测试</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">benchmark_kv_cache</span>():</span><br><span class="line">    <span class="keyword">import</span> time</span><br><span class="line">    </span><br><span class="line">    model = GPTWithKVCache(vocab_size=<span class="number">1000</span>, d_model=<span class="number">256</span>, n_layers=<span class="number">6</span>, n_heads=<span class="number">8</span>)</span><br><span class="line">    prompt = torch.randint(<span class="number">0</span>, <span class="number">1000</span>, (<span class="number">1</span>, <span class="number">10</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 无 KV Cache</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        generated = prompt.clone()</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">50</span>):</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                logits, _ = model(generated, use_cache=<span class="literal">False</span>)</span><br><span class="line">            next_token = torch.argmax(logits[:, -<span class="number">1</span>, :], dim=-<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">            generated = torch.cat([generated, next_token], dim=<span class="number">1</span>)</span><br><span class="line">    time_no_cache = time.time() - start</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 有 KV Cache  </span></span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">        generate_with_kv_cache(model, prompt, max_new_tokens=<span class="number">50</span>)</span><br><span class="line">    time_with_cache = time.time() - start</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"无 KV Cache: <span class="subst">{time_no_cache:<span class="number">.2</span>f}</span>s"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"有 KV Cache: <span class="subst">{time_with_cache:<span class="number">.2</span>f}</span>s"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"加速比: <span class="subst">{time_no_cache/time_with_cache:<span class="number">.1</span>f}</span>x"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># benchmark_kv_cache()</span></span><br><span class="line"><span class="comment"># 输出示例:</span></span><br><span class="line"><span class="comment"># 无 KV Cache: 45.32s</span></span><br><span class="line"><span class="comment"># 有 KV Cache: 8.21s</span></span><br><span class="line"><span class="comment"># 加速比: 5.5x</span></span><br></pre></td></tr></tbody></table></figure><h3 id="7-3-KV-Cache-显存优化技术"><a href="#7-3-KV-Cache-显存优化技术" class="headerlink" title="7.3 KV Cache 显存优化技术"></a>7.3 KV Cache 显存优化技术</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">三种 KV Cache 优化技术的实现对比</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. Multi-Query Attention (MQA)</span></span><br><span class="line"><span class="comment"># 所有 Q 头共享一组 K, V</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MultiQueryAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model=<span class="number">768</span>, n_heads=<span class="number">12</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.d_k = d_model // n_heads</span><br><span class="line">        </span><br><span class="line">        self.W_q = nn.Linear(d_model, d_model)  <span class="comment"># 12 个头的 Q</span></span><br><span class="line">        self.W_k = nn.Linear(d_model, self.d_k)  <span class="comment"># 只有 1 组 K</span></span><br><span class="line">        self.W_v = nn.Linear(d_model, self.d_k)  <span class="comment"># 只有 1 组 V</span></span><br><span class="line">        self.W_o = nn.Linear(d_model, d_model)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batch, seq, _ = x.shape</span><br><span class="line">        </span><br><span class="line">        Q = self.W_q(x).view(batch, seq, self.n_heads, self.d_k)</span><br><span class="line">        K = self.W_k(x).unsqueeze(<span class="number">2</span>)  <span class="comment"># [batch, seq, 1, d_k]</span></span><br><span class="line">        V = self.W_v(x).unsqueeze(<span class="number">2</span>)  <span class="comment"># [batch, seq, 1, d_k]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># K, V 广播到所有头</span></span><br><span class="line">        K = K.expand(-<span class="number">1</span>, -<span class="number">1</span>, self.n_heads, -<span class="number">1</span>)</span><br><span class="line">        V = V.expand(-<span class="number">1</span>, -<span class="number">1</span>, self.n_heads, -<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ... 后续计算相同</span></span><br><span class="line">        <span class="comment"># KV Cache 大小: 1/n_heads of standard!</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. Grouped-Query Attention (GQA) - LLaMA-2 使用</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GroupedQueryAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model=<span class="number">768</span>, n_heads=<span class="number">12</span>, n_kv_heads=<span class="number">3</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        n_heads=12, n_kv_heads=3 表示:</span></span><br><span class="line"><span class="string">        - 12 个 Q 头</span></span><br><span class="line"><span class="string">        - 3 个 KV 头 (每 4 个 Q 头共享 1 个 KV)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.n_kv_heads = n_kv_heads</span><br><span class="line">        self.n_groups = n_heads // n_kv_heads  <span class="comment"># 4</span></span><br><span class="line">        self.d_k = d_model // n_heads</span><br><span class="line">        </span><br><span class="line">        self.W_q = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_k = nn.Linear(d_model, self.n_kv_heads * self.d_k)</span><br><span class="line">        self.W_v = nn.Linear(d_model, self.n_kv_heads * self.d_k)</span><br><span class="line">        self.W_o = nn.Linear(d_model, d_model)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batch, seq, _ = x.shape</span><br><span class="line">        </span><br><span class="line">        Q = self.W_q(x).view(batch, seq, self.n_heads, self.d_k)</span><br><span class="line">        K = self.W_k(x).view(batch, seq, self.n_kv_heads, self.d_k)</span><br><span class="line">        V = self.W_v(x).view(batch, seq, self.n_kv_heads, self.d_k)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将 KV 重复以匹配 Q 头数</span></span><br><span class="line">        K = K.repeat_interleave(self.n_groups, dim=<span class="number">2</span>)  <span class="comment"># [b, s, 12, d_k]</span></span><br><span class="line">        V = V.repeat_interleave(self.n_groups, dim=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ... 后续计算相同</span></span><br><span class="line">        <span class="comment"># KV Cache 大小: n_kv_heads/n_heads of standard (这里是 1/4)</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 显存占用对比</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">memory_comparison</span>():</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    LLaMA-7B 配置: 32层, 4096维度, 32头</span></span><br><span class="line"><span class="string">    序列长度: 4096 tokens</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    n_layers = <span class="number">32</span></span><br><span class="line">    d_model = <span class="number">4096</span></span><br><span class="line">    seq_len = <span class="number">4096</span></span><br><span class="line">    dtype_bytes = <span class="number">2</span>  <span class="comment"># float16</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 标准 MHA</span></span><br><span class="line">    standard = <span class="number">2</span> * n_layers * d_model * seq_len * dtype_bytes</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"标准 MHA KV Cache: <span class="subst">{standard / <span class="number">1e9</span>:<span class="number">.2</span>f}</span> GB"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># MQA (所有头共享)</span></span><br><span class="line">    n_heads = <span class="number">32</span></span><br><span class="line">    mqa = <span class="number">2</span> * n_layers * (d_model // n_heads) * seq_len * dtype_bytes</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"MQA KV Cache: <span class="subst">{mqa / <span class="number">1e9</span>:<span class="number">.2</span>f}</span> GB (<span class="subst">{standard/mqa:<span class="number">.0</span>f}</span>x 节省)"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># GQA (LLaMA-2 配置: 8 个 KV 头)</span></span><br><span class="line">    n_kv_heads = <span class="number">8</span></span><br><span class="line">    gqa = <span class="number">2</span> * n_layers * (d_model // n_heads * n_kv_heads) * seq_len * dtype_bytes</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"GQA KV Cache: <span class="subst">{gqa / <span class="number">1e9</span>:<span class="number">.2</span>f}</span> GB (<span class="subst">{standard/gqa:<span class="number">.0</span>f}</span>x 节省)"</span>)</span><br><span class="line"></span><br><span class="line">memory_comparison()</span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># 标准 MHA KV Cache: 2.15 GB</span></span><br><span class="line"><span class="comment"># MQA KV Cache: 0.07 GB (32x 节省)</span></span><br><span class="line"><span class="comment"># GQA KV Cache: 0.27 GB (8x 节省)</span></span><br></pre></td></tr></tbody></table></figure><h3 id="7-4-生产环境最佳实践"><a href="#7-4-生产环境最佳实践" class="headerlink" title="7.4 生产环境最佳实践"></a>7.4 生产环境最佳实践</h3><table><thead><tr><th>优化技术</th><th>适用场景</th><th>优点</th><th>缺点</th></tr></thead><tbody><tr><td><strong>标准 MHA</strong></td><td>小模型、短序列</td><td>质量最好</td><td>显存占用大</td></tr><tr><td><strong>MQA</strong></td><td>超长序列、极致推理速度</td><td>显存节省最多</td><td>质量略有下降</td></tr><tr><td><strong>GQA</strong></td><td>生产环境推荐</td><td>平衡质量和效率</td><td>需要重新训练</td></tr><tr><td><strong>PagedAttention</strong></td><td>高并发推理服务</td><td>减少显存碎片</td><td>实现复杂</td></tr><tr><td><strong>FlashAttention</strong></td><td>所有场景</td><td>IO优化、训练加速</td><td>需要特定硬件</td></tr></tbody></table><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vLLM 使用示例 (生产推荐)</span></span><br><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> LLM, SamplingParams</span><br><span class="line"></span><br><span class="line"><span class="comment"># 自动启用 PagedAttention</span></span><br><span class="line">llm = LLM(</span><br><span class="line">    model=<span class="string">"meta-llama/Llama-2-7b-hf"</span>,</span><br><span class="line">    tensor_parallel_size=<span class="number">1</span>,  <span class="comment"># GPU数量</span></span><br><span class="line">    gpu_memory_utilization=<span class="number">0.9</span>,  <span class="comment"># 显存利用率</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 高效推理</span></span><br><span class="line">prompts = [<span class="string">"写一首关于春天的诗："</span>, <span class="string">"解释什么是机器学习："</span>]</span><br><span class="line">outputs = llm.generate(prompts, SamplingParams(temperature=<span class="number">0.7</span>, max_tokens=<span class="number">100</span>))</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="🚀-Part-6-BERT-实际应用场景与代码"><a href="#🚀-Part-6-BERT-实际应用场景与代码" class="headerlink" title="🚀 Part 6: BERT 实际应用场景与代码"></a>🚀 Part 6: BERT 实际应用场景与代码</h2><h3 id="6-1-应用场景总览-1"><a href="#6-1-应用场景总览-1" class="headerlink" title="6.1 应用场景总览"></a>6.1 应用场景总览</h3><table><thead><tr><th>应用场景</th><th>任务类型</th><th>输入格式</th><th>输出</th><th>实际案例</th></tr></thead><tbody><tr><td><strong>情感分析</strong></td><td>单句分类</td><td>[CLS] 文本 [SEP]</td><td>正面/负面</td><td>电商评论分析、舆情监控</td></tr><tr><td><strong>文本匹配</strong></td><td>句对分类</td><td>[CLS] 句A [SEP] 句B [SEP]</td><td>相似/不相似</td><td>智能客服、问题去重</td></tr><tr><td><strong>命名实体识别</strong></td><td>序列标注</td><td>[CLS] 文本 [SEP]</td><td>每个token的标签</td><td>简历解析、医疗病历</td></tr><tr><td><strong>阅读理解</strong></td><td>抽取式QA</td><td>[CLS] 问题 [SEP] 文章 [SEP]</td><td>答案位置</td><td>智能问答、知识库检索</td></tr><tr><td><strong>文本生成</strong></td><td>Seq2Seq</td><td>需配合Decoder</td><td>生成文本</td><td>摘要生成、机器翻译</td></tr></tbody></table><hr><h3 id="6-2-情感分析完整代码"><a href="#6-2-情感分析完整代码" class="headerlink" title="6.2 情感分析完整代码"></a>6.2 情感分析完整代码</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForSequenceClassification</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. 加载预训练模型</span></span><br><span class="line">model_name = <span class="string">"bert-base-chinese"</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(model_name)</span><br><span class="line">model = BertForSequenceClassification.from_pretrained(model_name, num_labels=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 数据预处理</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">preprocess</span>(<span class="params">texts, labels</span>):</span><br><span class="line">    encodings = tokenizer(</span><br><span class="line">        texts,</span><br><span class="line">        padding=<span class="literal">True</span>,</span><br><span class="line">        truncation=<span class="literal">True</span>,</span><br><span class="line">        max_length=<span class="number">128</span>,</span><br><span class="line">        return_tensors=<span class="string">"pt"</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> encodings, torch.tensor(labels)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 训练数据示例</span></span><br><span class="line">train_texts = [</span><br><span class="line">    <span class="string">"这个产品质量太差了，完全是浪费钱"</span>,</span><br><span class="line">    <span class="string">"非常满意！发货速度快，质量很好"</span>,</span><br><span class="line">    <span class="string">"一般般吧，没有想象中那么好"</span>,</span><br><span class="line">    <span class="string">"超级推荐！已经回购三次了"</span></span><br><span class="line">]</span><br><span class="line">train_labels = [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>, <span class="number">1</span>]  <span class="comment"># 0=负面, 1=正面</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 4. 微调训练</span></span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> AdamW</span><br><span class="line"></span><br><span class="line">encodings, labels = preprocess(train_texts, train_labels)</span><br><span class="line">optimizer = AdamW(model.parameters(), lr=<span class="number">2e-5</span>)</span><br><span class="line"></span><br><span class="line">model.train()</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>):</span><br><span class="line">    outputs = model(**encodings, labels=labels)</span><br><span class="line">    loss = outputs.loss</span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Epoch <span class="subst">{epoch+<span class="number">1</span>}</span>, Loss: <span class="subst">{loss.item():<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 5. 推理预测</span></span><br><span class="line">model.<span class="built_in">eval</span>()</span><br><span class="line">test_text = <span class="string">"这款手机拍照效果很棒，电池也耐用"</span></span><br><span class="line">inputs = tokenizer(test_text, return_tensors=<span class="string">"pt"</span>)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    outputs = model(**inputs)</span><br><span class="line">    prediction = torch.argmax(outputs.logits, dim=<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"预测结果: <span class="subst">{<span class="string">'正面'</span> <span class="keyword">if</span> prediction == <span class="number">1</span> <span class="keyword">else</span> <span class="string">'负面'</span>}</span>"</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="6-3-命名实体识别-NER-完整代码"><a href="#6-3-命名实体识别-NER-完整代码" class="headerlink" title="6.3 命名实体识别 (NER) 完整代码"></a>6.3 命名实体识别 (NER) 完整代码</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizerFast, BertForTokenClassification</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># NER 标签定义 (BIO 格式)</span></span><br><span class="line">label_list = [<span class="string">"O"</span>, <span class="string">"B-PER"</span>, <span class="string">"I-PER"</span>, <span class="string">"B-ORG"</span>, <span class="string">"I-ORG"</span>, <span class="string">"B-LOC"</span>, <span class="string">"I-LOC"</span>]</span><br><span class="line">label2id = {label: i <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(label_list)}</span><br><span class="line">id2label = {i: label <span class="keyword">for</span> label, i <span class="keyword">in</span> label2id.items()}</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载模型</span></span><br><span class="line">tokenizer = BertTokenizerFast.from_pretrained(<span class="string">"bert-base-chinese"</span>)</span><br><span class="line">model = BertForTokenClassification.from_pretrained(</span><br><span class="line">    <span class="string">"bert-base-chinese"</span>,</span><br><span class="line">    num_labels=<span class="built_in">len</span>(label_list),</span><br><span class="line">    id2label=id2label,</span><br><span class="line">    label2id=label2id</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 训练数据示例</span></span><br><span class="line">train_data = [</span><br><span class="line">    {</span><br><span class="line">        <span class="string">"text"</span>: <span class="string">"马云在杭州创办了阿里巴巴"</span>,</span><br><span class="line">        <span class="string">"entities"</span>: [</span><br><span class="line">            {<span class="string">"start"</span>: <span class="number">0</span>, <span class="string">"end"</span>: <span class="number">2</span>, <span class="string">"label"</span>: <span class="string">"PER"</span>},   <span class="comment"># 马云</span></span><br><span class="line">            {<span class="string">"start"</span>: <span class="number">3</span>, <span class="string">"end"</span>: <span class="number">5</span>, <span class="string">"label"</span>: <span class="string">"LOC"</span>},   <span class="comment"># 杭州</span></span><br><span class="line">            {<span class="string">"start"</span>: <span class="number">8</span>, <span class="string">"end"</span>: <span class="number">12</span>, <span class="string">"label"</span>: <span class="string">"ORG"</span>}   <span class="comment"># 阿里巴巴</span></span><br><span class="line">        ]</span><br><span class="line">    }</span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 推理示例</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">predict_ner</span>(<span class="params">text</span>):</span><br><span class="line">    inputs = tokenizer(text, return_tensors=<span class="string">"pt"</span>, return_offsets_mapping=<span class="literal">True</span>)</span><br><span class="line">    offset_mapping = inputs.pop(<span class="string">"offset_mapping"</span>)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    model.<span class="built_in">eval</span>()</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">        predictions = torch.argmax(outputs.logits, dim=<span class="number">2</span>)[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 解析实体</span></span><br><span class="line">    entities = []</span><br><span class="line">    current_entity = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> idx, (pred, offset) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(predictions, offset_mapping)):</span><br><span class="line">        label = id2label[pred.item()]</span><br><span class="line">        <span class="keyword">if</span> label.startswith(<span class="string">"B-"</span>):</span><br><span class="line">            <span class="keyword">if</span> current_entity:</span><br><span class="line">                entities.append(current_entity)</span><br><span class="line">            current_entity = {</span><br><span class="line">                <span class="string">"text"</span>: text[offset[<span class="number">0</span>]:offset[<span class="number">1</span>]],</span><br><span class="line">                <span class="string">"label"</span>: label[<span class="number">2</span>:],</span><br><span class="line">                <span class="string">"start"</span>: offset[<span class="number">0</span>].item()</span><br><span class="line">            }</span><br><span class="line">        <span class="keyword">elif</span> label.startswith(<span class="string">"I-"</span>) <span class="keyword">and</span> current_entity:</span><br><span class="line">            current_entity[<span class="string">"text"</span>] += text[offset[<span class="number">0</span>]:offset[<span class="number">1</span>]]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> current_entity:</span><br><span class="line">                entities.append(current_entity)</span><br><span class="line">                current_entity = <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> entities</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试</span></span><br><span class="line">result = predict_ner(<span class="string">"马云在杭州创办了阿里巴巴"</span>)</span><br><span class="line"><span class="built_in">print</span>(result)</span><br><span class="line"><span class="comment"># [{'text': '马云', 'label': 'PER', 'start': 0}, </span></span><br><span class="line"><span class="comment">#  {'text': '杭州', 'label': 'LOC', 'start': 3},</span></span><br><span class="line"><span class="comment">#  {'text': '阿里巴巴', 'label': 'ORG', 'start': 8}]</span></span><br></pre></td></tr></tbody></table></figure><hr><h3 id="6-4-语义相似度匹配代码"><a href="#6-4-语义相似度匹配代码" class="headerlink" title="6.4 语义相似度匹配代码"></a>6.4 语义相似度匹配代码</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertModel</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用 BERT 提取句子向量</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(<span class="string">"bert-base-chinese"</span>)</span><br><span class="line">model = BertModel.from_pretrained(<span class="string">"bert-base-chinese"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">get_sentence_embedding</span>(<span class="params">text</span>):</span><br><span class="line">    <span class="string">"""提取句子的 [CLS] 向量作为语义表示"""</span></span><br><span class="line">    inputs = tokenizer(text, return_tensors=<span class="string">"pt"</span>, padding=<span class="literal">True</span>, truncation=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">        <span class="comment"># 使用 [CLS] token 的输出作为句子表示</span></span><br><span class="line">        cls_embedding = outputs.last_hidden_state[:, <span class="number">0</span>, :]</span><br><span class="line">    <span class="keyword">return</span> cls_embedding</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_similarity</span>(<span class="params">text1, text2</span>):</span><br><span class="line">    <span class="string">"""计算两个句子的余弦相似度"""</span></span><br><span class="line">    emb1 = get_sentence_embedding(text1)</span><br><span class="line">    emb2 = get_sentence_embedding(text2)</span><br><span class="line">    similarity = F.cosine_similarity(emb1, emb2)</span><br><span class="line">    <span class="keyword">return</span> similarity.item()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试语义相似度</span></span><br><span class="line">pairs = [</span><br><span class="line">    (<span class="string">"今天天气怎么样"</span>, <span class="string">"今天天气好吗"</span>),           <span class="comment"># 高相似</span></span><br><span class="line">    (<span class="string">"今天天气怎么样"</span>, <span class="string">"明天会下雨吗"</span>),           <span class="comment"># 中等相似</span></span><br><span class="line">    (<span class="string">"今天天气怎么样"</span>, <span class="string">"这道菜怎么做"</span>),           <span class="comment"># 低相似</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> text1, text2 <span class="keyword">in</span> pairs:</span><br><span class="line">    sim = compute_similarity(text1, text2)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"'<span class="subst">{text1}</span>' vs '<span class="subst">{text2}</span>': <span class="subst">{sim:<span class="number">.4</span>f}</span>"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 输出:</span></span><br><span class="line"><span class="comment"># '今天天气怎么样' vs '今天天气好吗': 0.9234</span></span><br><span class="line"><span class="comment"># '今天天气怎么样' vs '明天会下雨吗': 0.7821</span></span><br><span class="line"><span class="comment"># '今天天气怎么样' vs '这道菜怎么做': 0.4123</span></span><br></pre></td></tr></tbody></table></figure><hr><h3 id="6-5-问答系统-阅读理解-代码"><a href="#6-5-问答系统-阅读理解-代码" class="headerlink" title="6.5 问答系统 (阅读理解) 代码"></a>6.5 问答系统 (阅读理解) 代码</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> BertTokenizer, BertForQuestionAnswering</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"></span><br><span class="line"><span class="comment"># 加载问答模型</span></span><br><span class="line">model_name = <span class="string">"bert-large-uncased-whole-word-masking-finetuned-squad"</span></span><br><span class="line">tokenizer = BertTokenizer.from_pretrained(model_name)</span><br><span class="line">model = BertForQuestionAnswering.from_pretrained(model_name)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">answer_question</span>(<span class="params">question, context</span>):</span><br><span class="line">    <span class="string">"""从文章中抽取答案"""</span></span><br><span class="line">    <span class="comment"># 编码问题和上下文</span></span><br><span class="line">    inputs = tokenizer(</span><br><span class="line">        question,</span><br><span class="line">        context,</span><br><span class="line">        return_tensors=<span class="string">"pt"</span>,</span><br><span class="line">        max_length=<span class="number">512</span>,</span><br><span class="line">        truncation=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 预测答案位置</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        outputs = model(**inputs)</span><br><span class="line">        start_scores = outputs.start_logits</span><br><span class="line">        end_scores = outputs.end_logits</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 找到最可能的答案位置</span></span><br><span class="line">    start_idx = torch.argmax(start_scores)</span><br><span class="line">    end_idx = torch.argmax(end_scores)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 解码答案</span></span><br><span class="line">    tokens = tokenizer.convert_ids_to_tokens(inputs[<span class="string">"input_ids"</span>][<span class="number">0</span>])</span><br><span class="line">    answer = tokenizer.convert_tokens_to_string(tokens[start_idx:end_idx+<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> answer</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试问答</span></span><br><span class="line">context = <span class="string">"""</span></span><br><span class="line"><span class="string">BERT是由Google在2018年提出的预训练语言模型。</span></span><br><span class="line"><span class="string">它使用Transformer的Encoder架构，通过Masked Language Model任务进行预训练。</span></span><br><span class="line"><span class="string">BERT在11个NLP任务上取得了当时的最佳成绩，包括问答、文本分类等任务。</span></span><br><span class="line"><span class="string">BERT-Base有1.1亿参数，BERT-Large有3.4亿参数。</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line">questions = [</span><br><span class="line">    <span class="string">"BERT是谁提出的？"</span>,</span><br><span class="line">    <span class="string">"BERT使用什么架构？"</span>,</span><br><span class="line">    <span class="string">"BERT有多少参数？"</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> q <span class="keyword">in</span> questions:</span><br><span class="line">    answer = answer_question(q, context)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Q: <span class="subst">{q}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"A: <span class="subst">{answer}</span>\n"</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="⚡-Part-7-KV-Cache-实战与优化"><a href="#⚡-Part-7-KV-Cache-实战与优化" class="headerlink" title="⚡ Part 7: KV Cache 实战与优化"></a>⚡ Part 7: KV Cache 实战与优化</h2><h3 id="7-1-KV-Cache-完整实现代码"><a href="#7-1-KV-Cache-完整实现代码" class="headerlink" title="7.1 KV Cache 完整实现代码"></a>7.1 KV Cache 完整实现代码</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">AttentionWithKVCache</span>(nn.Module):</span><br><span class="line">    <span class="string">"""带 KV Cache 的注意力层实现"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model=<span class="number">768</span>, n_heads=<span class="number">12</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.d_model = d_model</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.d_k = d_model // n_heads</span><br><span class="line">        </span><br><span class="line">        self.W_q = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_k = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_v = nn.Linear(d_model, d_model)</span><br><span class="line">        self.W_o = nn.Linear(d_model, d_model)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, kv_cache=<span class="literal">None</span>, use_cache=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            x: [batch, seq_len, d_model] 输入</span></span><br><span class="line"><span class="string">            kv_cache: tuple(K, V) 缓存的 K/V</span></span><br><span class="line"><span class="string">            use_cache: 是否使用并更新缓存</span></span><br><span class="line"><span class="string">        Returns:</span></span><br><span class="line"><span class="string">            output: [batch, seq_len, d_model]</span></span><br><span class="line"><span class="string">            new_kv_cache: 更新后的缓存</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        batch_size, seq_len, _ = x.shape</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算 Q, K, V</span></span><br><span class="line">        Q = self.W_q(x)  <span class="comment"># [batch, seq, d_model]</span></span><br><span class="line">        K = self.W_k(x)</span><br><span class="line">        V = self.W_v(x)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 如果有缓存，拼接历史 K, V</span></span><br><span class="line">        <span class="keyword">if</span> kv_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            K_cache, V_cache = kv_cache</span><br><span class="line">            K = torch.cat([K_cache, K], dim=<span class="number">1</span>)  <span class="comment"># [batch, cache_len + seq, d_model]</span></span><br><span class="line">            V = torch.cat([V_cache, V], dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 准备返回的缓存</span></span><br><span class="line">        new_kv_cache = (K, V) <span class="keyword">if</span> use_cache <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 重塑为多头形式</span></span><br><span class="line">        Q = Q.view(batch_size, -<span class="number">1</span>, self.n_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        K = K.view(batch_size, -<span class="number">1</span>, self.n_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        V = V.view(batch_size, -<span class="number">1</span>, self.n_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># Q: [batch, heads, seq, d_k]</span></span><br><span class="line">        <span class="comment"># K, V: [batch, heads, total_seq, d_k]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算注意力</span></span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / (self.d_k ** <span class="number">0.5</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Causal Mask (只看左边)</span></span><br><span class="line">        total_len = K.size(<span class="number">2</span>)</span><br><span class="line">        query_len = Q.size(<span class="number">2</span>)</span><br><span class="line">        mask = torch.triu(torch.ones(query_len, total_len), diagonal=total_len-query_len+<span class="number">1</span>)</span><br><span class="line">        mask = mask.<span class="built_in">bool</span>().to(x.device)</span><br><span class="line">        scores = scores.masked_fill(mask, <span class="built_in">float</span>(<span class="string">'-inf'</span>))</span><br><span class="line">        </span><br><span class="line">        attn_weights = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">        output = torch.matmul(attn_weights, V)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 合并多头</span></span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch_size, -<span class="number">1</span>, self.d_model)</span><br><span class="line">        output = self.W_o(output)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, new_kv_cache</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用示例：自回归生成</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generate_with_kv_cache</span>(<span class="params">model, prompt_ids, max_new_tokens=<span class="number">50</span></span>):</span><br><span class="line">    <span class="string">"""使用 KV Cache 进行高效生成"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化：处理 prompt</span></span><br><span class="line">    kv_cache = <span class="literal">None</span></span><br><span class="line">    input_ids = prompt_ids</span><br><span class="line">    generated = prompt_ids.tolist()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(max_new_tokens):</span><br><span class="line">        <span class="comment"># 只输入新的 token（第一步输入完整 prompt）</span></span><br><span class="line">        <span class="keyword">if</span> step == <span class="number">0</span>:</span><br><span class="line">            x = get_embeddings(input_ids)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            x = get_embeddings(input_ids[:, -<span class="number">1</span>:])  <span class="comment"># 只取最后一个 token</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 前向传播，使用并更新缓存</span></span><br><span class="line">        output, kv_cache = model(x, kv_cache=kv_cache, use_cache=<span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 预测下一个 token</span></span><br><span class="line">        logits = output[:, -<span class="number">1</span>, :]  <span class="comment"># 取最后一个位置的输出</span></span><br><span class="line">        next_token = torch.argmax(logits, dim=-<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        generated.append(next_token.item())</span><br><span class="line">        input_ids = torch.cat([input_ids, next_token.unsqueeze(<span class="number">0</span>)], dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 遇到结束符停止</span></span><br><span class="line">        <span class="keyword">if</span> next_token.item() == EOS_TOKEN_ID:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> generated</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"KV Cache 加速对比:"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"无缓存: O(n²) 计算量"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"有缓存: O(n) 计算量"</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"加速比: n 倍 (序列长度)"</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="7-2-KV-Cache-显存优化技术"><a href="#7-2-KV-Cache-显存优化技术" class="headerlink" title="7.2 KV Cache 显存优化技术"></a>7.2 KV Cache 显存优化技术</h3><h4 id="Multi-Query-Attention-MQA-实现"><a href="#Multi-Query-Attention-MQA-实现" class="headerlink" title="Multi-Query Attention (MQA) 实现"></a>Multi-Query Attention (MQA) 实现</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiQueryAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Multi-Query Attention: 所有头共享一组 K, V</span></span><br><span class="line"><span class="string">    显存节省: n_heads 倍</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model=<span class="number">768</span>, n_heads=<span class="number">12</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.d_k = d_model // n_heads</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Q: 每个头独立</span></span><br><span class="line">        self.W_q = nn.Linear(d_model, d_model)</span><br><span class="line">        <span class="comment"># K, V: 所有头共享 (只有一份)</span></span><br><span class="line">        self.W_k = nn.Linear(d_model, self.d_k)  <span class="comment"># 只输出一个头的维度</span></span><br><span class="line">        self.W_v = nn.Linear(d_model, self.d_k)</span><br><span class="line">        self.W_o = nn.Linear(d_model, d_model)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x, kv_cache=<span class="literal">None</span></span>):</span><br><span class="line">        batch, seq, _ = x.shape</span><br><span class="line">        </span><br><span class="line">        Q = self.W_q(x).view(batch, seq, self.n_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        K = self.W_k(x).unsqueeze(<span class="number">1</span>)  <span class="comment"># [batch, 1, seq, d_k] 广播到所有头</span></span><br><span class="line">        V = self.W_v(x).unsqueeze(<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># KV Cache 只需存储 [batch, 1, seq, d_k] 而非 [batch, heads, seq, d_k]</span></span><br><span class="line">        <span class="comment"># 显存节省: 12倍 (对于12头)</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> kv_cache <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            K = torch.cat([kv_cache[<span class="number">0</span>], K], dim=<span class="number">2</span>)</span><br><span class="line">            V = torch.cat([kv_cache[<span class="number">1</span>], V], dim=<span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / (self.d_k ** <span class="number">0.5</span>)</span><br><span class="line">        attn = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">        output = torch.matmul(attn, V)</span><br><span class="line">        </span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch, seq, -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.W_o(output), (K, V)</span><br></pre></td></tr></tbody></table></figure><h4 id="Grouped-Query-Attention-GQA-实现"><a href="#Grouped-Query-Attention-GQA-实现" class="headerlink" title="Grouped-Query Attention (GQA) 实现"></a>Grouped-Query Attention (GQA) 实现</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">GroupedQueryAttention</span>(nn.Module):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Grouped-Query Attention: 将头分组，每组共享 K, V</span></span><br><span class="line"><span class="string">    LLaMA-2 使用: 32头 → 8组</span></span><br><span class="line"><span class="string">    显存节省: n_heads / n_groups 倍</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model=<span class="number">768</span>, n_heads=<span class="number">12</span>, n_kv_groups=<span class="number">4</span></span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.n_heads = n_heads</span><br><span class="line">        self.n_kv_groups = n_kv_groups</span><br><span class="line">        self.heads_per_group = n_heads // n_kv_groups</span><br><span class="line">        self.d_k = d_model // n_heads</span><br><span class="line">        </span><br><span class="line">        self.W_q = nn.Linear(d_model, d_model)</span><br><span class="line">        <span class="comment"># K, V 只有 n_kv_groups 组</span></span><br><span class="line">        self.W_k = nn.Linear(d_model, self.d_k * n_kv_groups)</span><br><span class="line">        self.W_v = nn.Linear(d_model, self.d_k * n_kv_groups)</span><br><span class="line">        self.W_o = nn.Linear(d_model, d_model)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        batch, seq, _ = x.shape</span><br><span class="line">        </span><br><span class="line">        Q = self.W_q(x).view(batch, seq, self.n_heads, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        K = self.W_k(x).view(batch, seq, self.n_kv_groups, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        V = self.W_v(x).view(batch, seq, self.n_kv_groups, self.d_k).transpose(<span class="number">1</span>, <span class="number">2</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 将 K, V 扩展到与 Q 相同的头数</span></span><br><span class="line">        K = K.repeat_interleave(self.heads_per_group, dim=<span class="number">1</span>)</span><br><span class="line">        V = V.repeat_interleave(self.heads_per_group, dim=<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        scores = torch.matmul(Q, K.transpose(-<span class="number">2</span>, -<span class="number">1</span>)) / (self.d_k ** <span class="number">0.5</span>)</span><br><span class="line">        attn = F.softmax(scores, dim=-<span class="number">1</span>)</span><br><span class="line">        output = torch.matmul(attn, V)</span><br><span class="line">        </span><br><span class="line">        output = output.transpose(<span class="number">1</span>, <span class="number">2</span>).contiguous().view(batch, seq, -<span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> self.W_o(output)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="7-3-显存占用对比表"><a href="#7-3-显存占用对比表" class="headerlink" title="7.3 显存占用对比表"></a>7.3 显存占用对比表</h3><table><thead><tr><th>技术</th><th>KV Cache 大小</th><th>LLaMA-7B 128K tokens</th><th>适用模型</th></tr></thead><tbody><tr><td><strong>MHA</strong> (标准)</td><td><code>2 × L × H × S × dtype</code></td><td>64 GB</td><td>BERT, GPT-2</td></tr><tr><td><strong>MQA</strong></td><td><code>2 × L × (H/heads) × S × dtype</code></td><td>5.3 GB</td><td>PaLM, Falcon</td></tr><tr><td><strong>GQA</strong> (8组)</td><td><code>2 × L × (H/4) × S × dtype</code></td><td>16 GB</td><td>LLaMA-2, Mistral</td></tr></tbody></table><blockquote><p>L=层数, H=隐藏维度, S=序列长度, heads=注意力头数</p></blockquote><hr><h3 id="7-4-生产环境最佳实践-1"><a href="#7-4-生产环境最佳实践-1" class="headerlink" title="7.4 生产环境最佳实践"></a>7.4 生产环境最佳实践</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用 vLLM 进行高效推理 (PagedAttention)</span></span><br><span class="line"><span class="keyword">from</span> vllm <span class="keyword">import</span> LLM, SamplingParams</span><br><span class="line"></span><br><span class="line"><span class="comment"># 初始化模型</span></span><br><span class="line">llm = LLM(</span><br><span class="line">    model=<span class="string">"meta-llama/Llama-2-7b-hf"</span>,</span><br><span class="line">    tensor_parallel_size=<span class="number">1</span>,      <span class="comment"># GPU 数量</span></span><br><span class="line">    gpu_memory_utilization=<span class="number">0.9</span>,  <span class="comment"># 显存使用率</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 批量推理 (自动管理 KV Cache)</span></span><br><span class="line">prompts = [</span><br><span class="line">    <span class="string">"Explain the concept of attention mechanism:"</span>,</span><br><span class="line">    <span class="string">"Write a Python function to sort a list:"</span>,</span><br><span class="line">    <span class="string">"What is the capital of France?"</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">sampling_params = SamplingParams(</span><br><span class="line">    temperature=<span class="number">0.7</span>,</span><br><span class="line">    max_tokens=<span class="number">256</span>,</span><br><span class="line">    top_p=<span class="number">0.9</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">outputs = llm.generate(prompts, sampling_params)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> output <span class="keyword">in</span> outputs:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Prompt: <span class="subst">{output.prompt}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"Response: <span class="subst">{output.outputs[<span class="number">0</span>].text}</span>\n"</span>)</span><br></pre></td></tr></tbody></table></figure><hr><p><em>📅 创建时间: 2026-02-03</em><br><em>🏷️ 标签: #AI #Transformer #BERT #Attention #KVCache</em></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> AI </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>彻底理解 Transformer：Attention Is All You Need</title>
      <link href="/2026/02/02/Transformer-DeepDive/"/>
      <url>/2026/02/02/Transformer-DeepDive/</url>
      
        <content type="html"><![CDATA[<h1 id="彻底理解-Transformer-Attention-Is-All-You-Need"><a href="#彻底理解-Transformer-Attention-Is-All-You-Need" class="headerlink" title="彻底理解 Transformer: Attention Is All You Need"></a>彻底理解 Transformer: Attention Is All You Need</h1><blockquote><p><strong>摘要</strong>: 本文利用费曼学习法，通过图解和类比，深入浅出地拆解了 Transformer 架构的核心原理。从 Encoder-Decoder 架构到 Self-Attention 机制，再到 Cross-Attention 和 Masking 的细节，带你彻底搞懂这篇 AI 领域的奠基之作。</p></blockquote><hr><h2 id="1-核心大白话：为什么要搞这一套？"><a href="#1-核心大白话：为什么要搞这一套？" class="headerlink" title="1. 核心大白话：为什么要搞这一套？"></a>1. 核心大白话：为什么要搞这一套？</h2><p><strong>传统模型的痛点</strong>：<br>以前的 AI (RNN/LSTM) 像是一个<strong>接力赛跑</strong>选手。</p><ul><li>必须先读第一个字，传棒给第二个字，再传给第三个…</li><li><strong>缺点</strong>：如果句子太长，跑到最后早就忘了第一棒是谁了（长距离遗忘）；而且必须按顺序跑，不能所有人一起跑（无法并行，慢）。</li></ul><p><strong>Transformer 的革命</strong>：<br>不搞接力赛了，搞<strong>足球赛</strong>。</p><ul><li><strong>并行</strong>：所有单词（球员）同时在场上。</li><li><strong>注意力</strong>：每个球员（单词）都能时刻观察场上所有其他球员的位置，不管那个人在球场哪一头（解决了长距离依赖）。</li></ul><hr><h2 id="2-核心概念：Self-Attention-自注意力"><a href="#2-核心概念：Self-Attention-自注意力" class="headerlink" title="2. 核心概念：Self-Attention (自注意力)"></a>2. 核心概念：Self-Attention (自注意力)</h2><p>这是 Transformer 的心脏。它解决了“每个词该关注谁”的问题。</p><h3 id="2-1-Q-K-V-的图书检索类比"><a href="#2-1-Q-K-V-的图书检索类比" class="headerlink" title="2.1 Q, K, V 的图书检索类比"></a>2.1 Q, K, V 的图书检索类比</h3><p>为什么每个词要有 Query (Q), Key (K), Value (V) 三个向量？<br>这源于<strong>数据库检索</strong>的思想。</p><p><img src="/images/Transformer_DeepDive/transformer_qkv.png" alt="通过图书检索系统理解 Q, K, V 的分离"><br><em>(图注：通过图书检索系统理解 Q, K, V 的分离)</em></p><h3 id="2-2-主动与被动：Q-与-K-的聚光灯效应"><a href="#2-2-主动与被动：Q-与-K-的聚光灯效应" class="headerlink" title="2.2 主动与被动：Q 与 K 的聚光灯效应"></a>2.2 主动与被动：Q 与 K 的聚光灯效应</h3><p>为什么数学公式一样，Q 却是“主动”的？</p><p><img src="/images/Transformer_DeepDive/active_passive_demo.png" alt="Q 像手电筒一样主动扫描所有 K"><br><em>(图注：Q 像手电筒一样主动扫描所有 K)</em></p><ul><li><strong>Q (Query)<strong>：手握 100% 的“注意力预算”，它必须决定把光打在谁身上。在训练中，Loss 逼迫它学会</strong>“去寻找对我有用的信息”</strong>。</li><li><strong>K (Key)<strong>：像墙上的靶子，无法移动。在训练中，Loss 逼迫它学会</strong>“标明自己的身份”</strong>，以便被正确的 Q 找到。</li></ul><h3 id="2-3-矩阵维度的秘密：为什么维度必须一样？"><a href="#2-3-矩阵维度的秘密：为什么维度必须一样？" class="headerlink" title="2.3 矩阵维度的秘密：为什么维度必须一样？"></a>2.3 矩阵维度的秘密：为什么维度必须一样？</h3><p>很多初学者在这里卡住：为什么 $Q$ 和 $K$ 的维度必须一致？<br>因为数学上的<strong>点积 (Dot Product)</strong> 就像拉拉链，齿数必须对得上。</p><p><img src="/images/Transformer_DeepDive/transformer_dims.png" alt="矩阵乘法就像“拉链”咬合，中间的维度必须对齐"><br><em>(图注：矩阵乘法就像“拉链”咬合，中间的维度必须对齐)</em></p><hr><h2 id="3-完整实战：德语到英语翻译训练全过程"><a href="#3-完整实战：德语到英语翻译训练全过程" class="headerlink" title="3. 完整实战：德语到英语翻译训练全过程"></a>3. 完整实战：德语到英语翻译训练全过程</h2><p>我们通过一个具体的任务：<code>"Ich liebe dich"</code> (德) -&gt; <code>"I love you"</code> (英)，来彻底搞懂编码器和解码器到底在干什么。</p><h3 id="3-1-架构全景：数据是如何流动的？"><a href="#3-1-架构全景：数据是如何流动的？" class="headerlink" title="3.1 架构全景：数据是如何流动的？"></a>3.1 架构全景：数据是如何流动的？</h3><p>首先，看一眼上帝视角的架构图。注意红色的<strong>梯度回传</strong>线，这解释了为什么 Encoder 即使没有 Label 也能学会正确编码。</p><p><img src="/images/Transformer_DeepDive/viz_arch_overview.png" alt="Encoder-Decoder 完整架构与梯度回传路径"><br><em>(图注：Encoder-Decoder 完整架构与梯度回传路径)</em></p><h3 id="3-2-步骤一：Encoder-读懂德语"><a href="#3-2-步骤一：Encoder-读懂德语" class="headerlink" title="3.2 步骤一：Encoder (读懂德语)"></a>3.2 步骤一：Encoder (读懂德语)</h3><p>Encoder 的任务是把德语变成一组高质量的“记忆向量”。</p><ul><li><strong>操作</strong>: Self-Attention。</li><li><strong>结果</strong>: 每个词都融合了上下文信息，形成了记忆库 $M$ (Memory)。</li><li><strong>维度不变性</strong>: 输入是 <code>[Batch, 3, 512]</code>，输出 $M$ 依然是 <code>[Batch, 3, 512]</code>。这就是<strong>残差流</strong> (Residual Stream) 的设计。</li></ul><h3 id="3-3-步骤二：Decoder-的核心-Cross-Attention"><a href="#3-3-步骤二：Decoder-的核心-Cross-Attention" class="headerlink" title="3.3 步骤二：Decoder 的核心 (Cross-Attention)"></a>3.3 步骤二：Decoder 的核心 (Cross-Attention)</h3><p>这是模型最精彩的部分。Decoder 拿着英语去查德语。</p><p><img src="/images/Transformer_DeepDive/viz_cross_attn.png" alt="Cross-Attention 细节"><br><em>(图注：Cross-Attention 细节。英语的 “I” (Q) 精准找到了德语的 “liebe” (K2))</em></p><ul><li><strong>Query (Q)</strong>: 来自 <strong>Decoder</strong> 中间状态 $X_{mid}$。<ul><li><em>形状</em>: <code>[Batch, 2, 512]</code> (英语长度)</li></ul></li><li><strong>Key (K)</strong>: 来自 <strong>Encoder</strong> 记忆库 $M$。<ul><li><em>形状</em>: <code>[Batch, 3, 512]</code> (德语长度)</li></ul></li><li><strong>Value (V)</strong>: 来自 <strong>Encoder</strong> 记忆库 $M$。</li></ul><p><strong>QKV 矩阵的作用</strong>:<br>Decoder 的 $W_Q^{cross}$ 和 Encoder 侧的 $W_K^{cross}$ 就像<strong>同声传译员</strong>，把英语状态和德语记忆映射到同一个“中间语义空间”，这样它们才能进行点积匹配。</p><h3 id="3-4-步骤三：Mask-机制-不许偷看"><a href="#3-4-步骤三：Mask-机制-不许偷看" class="headerlink" title="3.4 步骤三：Mask 机制 (不许偷看)"></a>3.4 步骤三：Mask 机制 (不许偷看)</h3><p>在 Decoder 内部，为了防止模型在预测 “love” 时偷看后面的 “you”，我们使用了 Mask。</p><p><img src="/images/Transformer_DeepDive/viz_masking.png" alt="Mask 矩阵"><br><em>(图注：Mask 矩阵。深蓝色代表可见，白色代表被遮挡 (-∞))</em></p><h3 id="3-5-步骤四：并行计算与-Loss"><a href="#3-5-步骤四：并行计算与-Loss" class="headerlink" title="3.5 步骤四：并行计算与 Loss"></a>3.5 步骤四：并行计算与 Loss</h3><p>虽然逻辑上是“预测完 I 再预测 love”，但在训练时，这都在<strong>一次矩阵运算</strong>中完成了。</p><p><img src="/images/Transformer_DeepDive/viz_training_parallel.png" alt="一次性处理 4 个时刻的预测与 Loss 计算"><br><em>(图注：一次性处理 4 个时刻的预测与 Loss 计算)</em></p><hr><h2 id="4-极简代码实现-Python-x2F-PyTorch"><a href="#4-极简代码实现-Python-x2F-PyTorch" class="headerlink" title="4. 极简代码实现 (Python/PyTorch)"></a>4. 极简代码实现 (Python/PyTorch)</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SelfAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, d_model, head_size</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__()</span><br><span class="line">        self.head_size = head_size</span><br><span class="line">        <span class="comment"># 1. 定义三个线性层：用来生成 Q, K, V</span></span><br><span class="line">        self.key = nn.Linear(d_model, head_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.query = nn.Linear(d_model, head_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.value = nn.Linear(d_model, head_size, bias=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment"># x 的形状: [Batch, Time(词数), Dimension(维度)]</span></span><br><span class="line">        B, T, C = x.shape</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. 生成 Q, K, V</span></span><br><span class="line">        k = self.key(x)   <span class="comment"># (B, T, H)</span></span><br><span class="line">        q = self.query(x) <span class="comment"># (B, T, H)</span></span><br><span class="line">        v = self.value(x) <span class="comment"># (B, T, H)</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. 计算关注度 (Attention Scores)</span></span><br><span class="line">        wei = q @ k.transpose(-<span class="number">2</span>, -<span class="number">1</span>) * (<span class="number">1.0</span> / math.sqrt(self.head_size))</span><br><span class="line">        wei = F.softmax(wei, dim=-<span class="number">1</span>)</span><br><span class="line">        out = wei @ v </span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="5-核心数学公式与维度-The-Math-amp-Dimensions"><a href="#5-核心数学公式与维度-The-Math-amp-Dimensions" class="headerlink" title="5. 核心数学公式与维度 (The Math &amp; Dimensions)"></a>5. 核心数学公式与维度 (The Math &amp; Dimensions)</h2><h3 id="5-1-Self-Attention-单头"><a href="#5-1-Self-Attention-单头" class="headerlink" title="5.1 Self-Attention (单头)"></a>5.1 Self-Attention (单头)</h3><p>$$ Attention(Q, K, V) = \text{Softmax}(\frac{QK^T}{\sqrt{d_k}})V $$</p><p><strong>维度推演</strong> (以 $d_{model}=512, d_k=64$ 为例)：</p><ol><li><strong>输入 $X$</strong>: <code>[L, 512]</code></li><li><strong>生成 Q, K, V</strong>:<ul><li>$Q = X W_Q \rightarrow [L, 64]$</li><li>$K = X W_K \rightarrow [L, 64]$</li><li>$V = X W_V \rightarrow [L, 64]$</li></ul></li><li><strong>矩阵乘法 $Q \cdot K^T$</strong>:<ul><li><code>[L, 64]</code> @ <code>[64, L]</code> $\rightarrow$ <code>[L, L]</code> (分数矩阵)</li></ul></li><li><strong>输出</strong>:<ul><li><code>[L, L]</code> @ <code>[L, 64]</code> $\rightarrow$ <code>[L, 64]</code></li></ul></li></ol><h3 id="5-2-Cross-Attention-混合双打"><a href="#5-2-Cross-Attention-混合双打" class="headerlink" title="5.2 Cross-Attention (混合双打)"></a>5.2 Cross-Attention (混合双打)</h3><p>$$ \text{CrossAttn}(X_{dec}, M) = \text{Softmax}(\frac{(X_{dec}W_Q)(M W_K)^T}{\sqrt{d_k}}) (M W_V) $$</p><ul><li><strong>$X_{dec}$</strong>: 英语中间状态 <code>[L_tgt, 512]</code></li><li><strong>$M$</strong>: 德语记忆库 <code>[L_src, 512]</code></li><li><strong>$W_Q$</strong>: <code>[512, 64]</code> (负责转译英语)</li><li><strong>$W_K$</strong>: <code>[512, 64]</code> (负责转译德语)</li><li><strong>Attention Map</strong>: <code>[L_tgt, L_src]</code> (例如 2行3列，表示每个英语词关注哪些德语词)</li></ul><h3 id="5-3-最终输出-Word-Prediction"><a href="#5-3-最终输出-Word-Prediction" class="headerlink" title="5.3 最终输出 (Word Prediction)"></a>5.3 最终输出 (Word Prediction)</h3><p>$$ P(\text{word}) = \text{Softmax}(h \cdot W_{vocab} + b) $$</p><ul><li><strong>$h$</strong>: Decoder 最终输出 <code>[Batch, Seq, 512]</code></li><li><strong>$W_{vocab}$</strong>: 投影矩阵 <code>[512, Vocab_Size]</code> (例如 30000)</li><li><strong>结果</strong>: <code>[Batch, Seq, 30000]</code></li></ul><hr><h2 id="6-费曼自测-Self-Check"><a href="#6-费曼自测-Self-Check" class="headerlink" title="6. 费曼自测 (Self-Check)"></a>6. 费曼自测 (Self-Check)</h2><ol><li><strong>Encoder 没有 Label，它是怎么学会正确的 Attention 的？</strong><br><em>(答：通过端到端的梯度反向传播。Decoder 的翻译错误会转化为 Loss，梯度顺着 Cross-Attention 流回 Encoder，告诉它“你提供的 Memory 质量太差，改！”)</em></li><li><strong>Cross-Attention 的 QKV 矩阵是随机的吗？</strong><br><em>(答：初始是随机的，但它们起到了“适配器”的作用，负责把英语状态和德语记忆映射到同一个语义空间，以便进行匹配。)</em></li><li><strong>为什么 Encoder 输入输出维度必须一致？</strong><br><em>(答：为了支持残差连接 (Residual Connection)，公式 $x + SubLayer(x)$ 要求两者形状必须完全相同。)</em></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> DeepLearning </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>彻底搞懂 QLoRA：如何用 4-bit 量化技术单卡微调千亿模型？</title>
      <link href="/2026/01/25/qlora-deep-dive/"/>
      <url>/2026/01/25/qlora-deep-dive/</url>
      
        <content type="html"><![CDATA[<p>在上一篇文章中，我们深入探讨了 <strong>LoRA</strong> 的数学原理。今天，我们来聊聊它的进化版——**QLoRA (Quantized LoRA)**。</p><p>如果你想在普通的消费级显卡（如 RTX 3060/4090）上微调 33B 甚至 65B 的巨型模型，QLoRA 是你唯一的救星。它通过“4-bit 极限压缩”，将显存需求<strong>再次减半</strong>。</p><p>本文将基于深度技术问答，带你彻底搞懂 QLoRA 的核心机制。</p><span id="more"></span><h2 id="一、-QLoRA-vs-LoRA：到底有什么区别？"><a href="#一、-QLoRA-vs-LoRA：到底有什么区别？" class="headerlink" title="一、 QLoRA vs LoRA：到底有什么区别？"></a>一、 QLoRA vs LoRA：到底有什么区别？</h2><p>一句话总结： <strong>QLoRA = 4-bit 量化 (Quantization) + LoRA</strong> 。</p><p>LoRA 解决了“计算量”的问题，而 QLoRA 解决了“存储量（显存）”的问题。</p><h3 id="1-1-显存占用的“降维打击”"><a href="#1-1-显存占用的“降维打击”" class="headerlink" title="1.1 显存占用的“降维打击”"></a>1.1 显存占用的“降维打击”</h3><p>假设我们要微调一个 <strong>Llama-2-7B</strong> 模型，显存账单如下：</p><p><img src="/images/qlora-math/memory_comparison.png" alt="Memory Comparison"></p><ul><li><strong>全量微调</strong> ：约 112 GB（必须上 A100 集群）。</li><li><strong>LoRA (16-bit)</strong> ：约 24 GB（需要 3090/4090）。</li><li><strong>QLoRA (4-bit)</strong> ： <strong>仅需 6 GB</strong> （RTX 3060 都能跑！）。</li></ul><h3 id="1-2-核心关系表"><a href="#1-2-核心关系表" class="headerlink" title="1.2 核心关系表"></a>1.2 核心关系表</h3><table><thead><tr><th align="left">特性</th><th align="left"><strong>LoRA</strong> (标准版)</th><th align="left"><strong>QLoRA</strong> (量化版)</th></tr></thead><tbody><tr><td align="left"><strong>底座模型 (Base Model)</strong></td><td align="left">加载为 <strong>16-bit</strong> (FP16)</td><td align="left">加载为 <strong>4-bit</strong> (NF4)</td></tr><tr><td align="left"><strong>LoRA 适配器 (Adapter)</strong></td><td align="left">16-bit</td><td align="left">16-bit (保持精度)</td></tr><tr><td align="left"><strong>计算方式</strong></td><td align="left">纯 FP16 计算</td><td align="left"><strong>混合精度</strong> (4-bit 存储 $\to$ 实时解压为 16-bit 计算)</td></tr><tr><td align="left"><strong>创新点</strong></td><td align="left">矩阵分解</td><td align="left">NF4 数据类型 + 双重量化</td></tr></tbody></table><hr><h2 id="二、-QLoRA-的三大技术创新-The-Magic"><a href="#二、-QLoRA-的三大技术创新-The-Magic" class="headerlink" title="二、 QLoRA 的三大技术创新 (The Magic)"></a>二、 QLoRA 的三大技术创新 (The Magic)</h2><p>QLoRA 之所以能在压到 4-bit 的同时还不掉点（精度损失微乎其微），靠的是以下三个黑科技。</p><h3 id="2-1-4-bit-NormalFloat-NF4-：为权重定制的容器"><a href="#2-1-4-bit-NormalFloat-NF4-：为权重定制的容器" class="headerlink" title="2.1 4-bit NormalFloat (NF4)：为权重定制的容器"></a>2.1 4-bit NormalFloat (NF4)：为权重定制的容器</h3><p>传统的 4-bit 整数量化 (Int4) 是均匀切分的。但神经网络的权重分布是 <strong>正态分布 (Gaussian Distribution)</strong> ——大部分数值集中在 0 附近。</p><ul><li><strong>Int4 (均匀)</strong> ：在 0 附近切分太稀疏，浪费了大量精度在极值区域（Empty Tails）。</li><li><strong>NF4 (分位数)</strong> ：根据正态分布设计刻度， <strong>在 0 附近切分极密</strong> 。</li></ul><p><img src="/images/qlora-math/nf4_vs_int4.png" alt="NF4 vs Int4"></p><blockquote><p><strong>通俗理解</strong> ：Int4 是一把刻度均匀的直尺，而 NF4 是一把中间刻度极细、两头刻度稀疏的“变形尺”，专门用来量权重这种“中间多、两头少”的东西。</p></blockquote><h3 id="2-2-双重量化-Double-Quantization"><a href="#2-2-双重量化-Double-Quantization" class="headerlink" title="2.2 双重量化 (Double Quantization)"></a>2.2 双重量化 (Double Quantization)</h3><p>量化不仅需要存权重，还需要存 <strong>量化常数 (Scale Constants)</strong> 。<br>通常每 64 个参数共用一个 32-bit 的常数。虽然看起来不多，但在 65B 模型下，光这些常数就要占 3GB 显存！</p><p><strong>QLoRA 的做法</strong> ： <strong>对“量化常数”再进行一次量化。</strong></p><ol><li>权重 $\to$ 4-bit。产生常数 $C_1$ (32-bit)。</li><li>$C_1$ $\to$ 8-bit。产生常数 $C_2$。</li></ol><p>这就像把压缩包再压缩一次，平均每个参数只多占 0.127 bit。</p><h3 id="2-3-分页优化器-Paged-Optimizers"><a href="#2-3-分页优化器-Paged-Optimizers" class="headerlink" title="2.3 分页优化器 (Paged Optimizers)"></a>2.3 分页优化器 (Paged Optimizers)</h3><p>利用 CPU 内存 (RAM) 来救急。当 GPU 显存出现峰值（Spike）快要 OOM 时，系统会自动把优化器状态 (Optimizer States) 搬运到 CPU 内存里，等需要更新参数时再搬回来。</p><hr><h2 id="三、-QLoRA-的工作流：左右互搏"><a href="#三、-QLoRA-的工作流：左右互搏" class="headerlink" title="三、 QLoRA 的工作流：左右互搏"></a>三、 QLoRA 的工作流：左右互搏</h2><p>QLoRA 最精妙的地方在于它的 <strong>混合精度计算流</strong> 。它实现了“用 4-bit 存，用 16-bit 算”。</p><h3 id="3-1-静态存储-vs-动态计算"><a href="#3-1-静态存储-vs-动态计算" class="headerlink" title="3.1 静态存储 vs 动态计算"></a>3.1 静态存储 vs 动态计算</h3><ul><li><strong>底座模型</strong> ：在显存里是 <strong>4-bit</strong> (NF4)。 <strong>绝对冻结，只读</strong> 。</li><li><strong>LoRA 适配器</strong> ：在显存里是 <strong>16-bit</strong> (BF16)。 <strong>可训练</strong> 。</li></ul><h3 id="3-2-训练时的数据流"><a href="#3-2-训练时的数据流" class="headerlink" title="3.2 训练时的数据流"></a>3.2 训练时的数据流</h3><p>当数据流经某一层时：</p><ol><li><strong>解压 (Dequantize)</strong> ：将底座的 4-bit 权重 $\times$ 量化常数 $\rightarrow$ 瞬间还原为 <strong>16-bit</strong> 。</li><li><strong>计算 (Compute)</strong> ： $X \times W_{16bit}$ 。</li><li><strong>释放 (Discard)</strong> ：计算完哪怕 1 毫秒后，立刻扔掉 16-bit 权重，显存里只留 4-bit 版本。</li><li><strong>反向传播</strong> ：梯度只传给 LoRA 部分更新。</li></ol><p>这就是为什么 QLoRA <strong>速度会慢 30%</strong> （因为要频繁解压），但 <strong>显存能省 60%</strong> 。</p><hr><h2 id="四、-实战代码-bitsandbytes-peft"><a href="#四、-实战代码-bitsandbytes-peft" class="headerlink" title="四、 实战代码 (bitsandbytes + peft)"></a>四、 实战代码 (bitsandbytes + peft)</h2><p>开启 QLoRA 只需要在加载模型时配置 <code>BitsAndBytesConfig</code> 。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> transformers <span class="keyword">import</span> AutoModelForCausalLM, BitsAndBytesConfig</span><br><span class="line"><span class="keyword">from</span> peft <span class="keyword">import</span> LoraConfig, get_peft_model</span><br><span class="line"></span><br><span class="line"><span class="comment"># 1. QLoRA 核心配置</span></span><br><span class="line">bnb_config = BitsAndBytesConfig(</span><br><span class="line">    load_in_4bit=<span class="literal">True</span>,                  <span class="comment"># 开启 4-bit 加载</span></span><br><span class="line">    bnb_4bit_quant_type=<span class="string">"nf4"</span>,          <span class="comment"># 创新1: 使用 NF4 数据类型</span></span><br><span class="line">    bnb_4bit_use_double_quant=<span class="literal">True</span>,     <span class="comment"># 创新2: 开启双重量化</span></span><br><span class="line">    bnb_4bit_compute_dtype=torch.float16 <span class="comment"># 创新3: 计算时解压为 FP16</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2. 加载底座 (显存占用极低)</span></span><br><span class="line">model = AutoModelForCausalLM.from_pretrained(</span><br><span class="line">    <span class="string">"meta-llama/Llama-2-7b-hf"</span>,</span><br><span class="line">    quantization_config=bnb_config,</span><br><span class="line">    device_map=<span class="string">"auto"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 3. 加载 LoRA (和普通 LoRA 一模一样)</span></span><br><span class="line">peft_config = LoraConfig(</span><br><span class="line">    r=<span class="number">8</span>, </span><br><span class="line">    lora_alpha=<span class="number">16</span>, </span><br><span class="line">    target_modules=[<span class="string">"q_proj"</span>, <span class="string">"v_proj"</span>],</span><br><span class="line">    task_type=<span class="string">"CAUSAL_LM"</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">model = get_peft_model(model, peft_config)</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="五、-常见误区解答"><a href="#五、-常见误区解答" class="headerlink" title="五、 常见误区解答"></a>五、 常见误区解答</h2><p><strong>Q: QLoRA 会修改原始模型文件吗？</strong><br><strong>A: 绝对不会。</strong> 原始模型在硬盘上是只读的。训练过程中，显存里的底座模型也是冻结的。我们只训练并保存那几百 MB 的 LoRA 权重。</p><p><strong>Q: 推理时需要解压吗？</strong><br><strong>A: 是的。</strong> 推理逻辑和训练一样：实时解压 $\rightarrow$ 计算 $\rightarrow$ 释放。如果你想追求极致推理速度，可以把 LoRA 合并到底座后，统一量化为 GPTQ 或 AWQ 格式。</p><p><strong>Q: 什么是量化常数？</strong><br><strong>A:</strong> 就像地图的比例尺。4-bit 只能存 0~15 的整数，量化常数告诉我们“1”代表实际权重的“0.005”还是“100”。没有它，数据就是废纸。</p><hr><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ol><li><em>Dettmers, T., et al. (2023). <a href="https://arxiv.org/abs/2305.14314">QLoRA: Efficient Finetuning of Quantized LLMs</a>.</em></li><li><em>Hugging Face Blog. <a href="https://huggingface.co/blog/4bit-transformers-bitsandbytes">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a>.</em></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>深入浅出 LoRA：大模型微调的核心原理、数学证明与实战指南</title>
      <link href="/2026/01/25/deep-dive-into-lora/"/>
      <url>/2026/01/25/deep-dive-into-lora/</url>
      
        <content type="html"><![CDATA[<p>在大模型（LLM）时代， <strong>PEFT (Parameter-Efficient Fine-Tuning, 参数高效微调)</strong> 几乎是每一位开发者必须掌握的技能。而其中最耀眼的明星，莫过于 <strong>LoRA (Low-Rank Adaptation)</strong> 。</p><p>本文将基于我与 AI 的一次深度对话，系统地梳理 LoRA 的核心原理。我们不仅会用直觉去理解它，更会通过 <strong>数学证明</strong> 和 <strong>Python 模拟</strong> ，彻底搞懂为什么它需要除以 $r$ ，以及它在反向传播中是如何工作的。</p><span id="more"></span><h2 id="一、-为什么我们需要-LoRA？"><a href="#一、-为什么我们需要-LoRA？" class="headerlink" title="一、 为什么我们需要 LoRA？"></a>一、 为什么我们需要 LoRA？</h2><h3 id="1-1-“重写百科全书”-vs-“贴便利贴”"><a href="#1-1-“重写百科全书”-vs-“贴便利贴”" class="headerlink" title="1.1 “重写百科全书” vs “贴便利贴”"></a>1.1 “重写百科全书” vs “贴便利贴”</h3><p>想象 GPT-4 或 Llama-3 是一本厚达 1750 亿页的 <strong>百科全书</strong> 。如果你想让它变成一个“法律专家”：</p><ul><li><strong>全量微调 (Full Fine-Tuning)</strong> ：相当于你需要把这本书的每一页都重新编辑、重新印刷。</li><li><strong>LoRA (Low-Rank Adaptation)</strong> ：我们在不破坏原书（冻结参数）的情况下，只是在相关的页面旁边贴上几张 <strong>透明的便利贴</strong> （LoRA 模块），上面写着修正内容。</li></ul><h3 id="1-2-参数效率对比"><a href="#1-2-参数效率对比" class="headerlink" title="1.2 参数效率对比"></a>1.2 参数效率对比</h3><p>LoRA 的核心优势在于极致的参数压缩。对于一个 175B 的模型，全量微调需要更新所有参数，而 LoRA 仅需更新约 0.01% 的参数。</p><p><img src="/images/lora-math/param_efficiency.png" alt="Parameter Efficiency"></p><hr><h2 id="二、-LoRA-的数学原理-The-Math"><a href="#二、-LoRA-的数学原理-The-Math" class="headerlink" title="二、 LoRA 的数学原理 (The Math)"></a>二、 LoRA 的数学原理 (The Math)</h2><h3 id="2-1-矩阵分解：把“大”变“小”"><a href="#2-1-矩阵分解：把“大”变“小”" class="headerlink" title="2.1 矩阵分解：把“大”变“小”"></a>2.1 矩阵分解：把“大”变“小”</h3><p>LoRA 的核心思想是 <strong>低秩分解 (Low-Rank Decomposition)</strong> 。<br>假设模型中有一个巨大的权重矩阵 $W \in \mathbb{R}^{d \times d}$ ，我们要微调它，产生一个增量 $\Delta W$ 。</p><p>LoRA 假设这个 $\Delta W$ 是“低秩”的，可以拆分为两个小矩阵的乘积：<br>$$ \Delta W = B \times A $$</p><ul><li><strong>矩阵 A (降维)</strong> ： $r \times d$ 。负责把数据“压扁”，提取核心特征。</li><li><strong>矩阵 B (升维)</strong> ： $d \times r$ 。负责把数据“还原”，映射回原空间。</li><li><strong>秩 r (Rank)</strong> ：通常很小（如 8, 16, 64）。</li></ul><h3 id="2-2-为什么必须除以-r？-关键证明"><a href="#2-2-为什么必须除以-r？-关键证明" class="headerlink" title="2.2 为什么必须除以 r？(关键证明)"></a>2.2 为什么必须除以 r？(关键证明)</h3><p>在 LoRA 的公式中，有一个关键的缩放系数：<br>$$ y = W_0x + \frac{\alpha}{r} (BAx) $$</p><p>为什么 $r$ 翻倍，数值会翻倍？如果不除以 $r$ ，会发生什么？</p><h4 id="证明一：方差叠加-Forward-Pass-Variance"><a href="#证明一：方差叠加-Forward-Pass-Variance" class="headerlink" title="证明一：方差叠加 (Forward Pass Variance)"></a>证明一：方差叠加 (Forward Pass Variance)</h4><p>假设 LoRA 内部的参数 $A, B$ 服从独立同分布（I.I.D），方差为 $\sigma^2$ 。<br>矩阵乘法的每一位输出，本质上是对 $r$ 个通道的求和：<br>$$ y_k = \sum_{i=1}^{r} (B_{ki} A_{ij} x_j) $$</p><p>根据统计学定律（相互独立的随机变量之和的方差等于它们方差之和）：<br>$$ \text{Var}(y) \propto r \cdot \sigma^2 $$</p><p>这意味着，信号的波动幅度（方差）会随着 $r$ 的增大而线性膨胀。如果不加以控制，输出值会变得极其不稳定。</p><h4 id="证明二：梯度稳定性-Gradient-Stability"><a href="#证明二：梯度稳定性-Gradient-Stability" class="headerlink" title="证明二：梯度稳定性 (Gradient Stability)"></a>证明二：梯度稳定性 (Gradient Stability)</h4><p>这才是最致命的问题。如果前向传播的值变大了， <strong>反向传播的梯度也会变大</strong> 。</p><p>我们用 Python 模拟了不同 $r$ 值下的梯度范数（Gradient Norm）：</p><p><img src="/images/lora-math/gradient_stability.png" alt="Gradient Stability"></p><ul><li><strong>红线 (Without Scaling)</strong> ：随着 $r$ 增大，梯度呈指数级爆炸。这意味着如果你把 $r$ 从 8 改成 64，你必须手动把学习率缩小 8 倍，否则模型直接崩溃。</li><li><strong>绿线 (With 1/r Scaling)</strong> ：无论 $r$ 怎么变，梯度大小保持恒定。这实现了 <strong>学习率解耦 (Learning Rate Decoupling)</strong> —— 一套超参数走天下。</li></ul><h3 id="2-3-初始化的艺术"><a href="#2-3-初始化的艺术" class="headerlink" title="2.3 初始化的艺术"></a>2.3 初始化的艺术</h3><p>LoRA 的初始化策略非常讲究：</p><ul><li><strong>矩阵 A</strong> ： <strong>高斯随机初始化</strong> 。<ul><li>原因：必须打破对称性，让梯度能够流动。</li></ul></li><li><strong>矩阵 B</strong> ： <strong>全零初始化</strong> 。<ul><li>原因：保证在训练开始的一瞬间（Step 0），$BAx = 0$ 。</li></ul></li></ul><p>这就像一个阀门：虽然 A 里已经充满了随机噪声（水流），但 B 这个阀门关着，所以对原模型没有任何干扰。</p><p><img src="/images/lora-math/initialization_heatmap.png" alt="Initialization Heatmap"></p><hr><h2 id="三、-实战：微调数据格式"><a href="#三、-实战：微调数据格式" class="headerlink" title="三、 实战：微调数据格式"></a>三、 实战：微调数据格式</h2><h3 id="3-1-Input-x2F-Output-对应关系"><a href="#3-1-Input-x2F-Output-对应关系" class="headerlink" title="3.1 Input / Output 对应关系"></a>3.1 Input / Output 对应关系</h3><p>在微调大模型时，核心逻辑对应着监督学习的 $X$ 和 $Y$ 。</p><ul><li><strong>Instruction + Input</strong> $\rightarrow$ <strong>X (模型输入)</strong></li><li><strong>Output</strong> $\rightarrow$ <strong>Y (预期输出)</strong></li></ul><figure class="highlight json"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="punctuation">{</span></span><br><span class="line">    <span class="attr">"instruction"</span><span class="punctuation">:</span> <span class="string">"请分析以下案情中的法律责任。"</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"input"</span><span class="punctuation">:</span> <span class="string">"张三在喝酒后驾驶机动车..."</span><span class="punctuation">,</span></span><br><span class="line">    <span class="attr">"output"</span><span class="punctuation">:</span> <span class="string">"张三的行为构成危险驾驶罪..."</span></span><br><span class="line"><span class="punctuation">}</span></span><br></pre></td></tr></tbody></table></figure><h3 id="3-2-System-Prompt"><a href="#3-2-System-Prompt" class="headerlink" title="3.2 System Prompt"></a>3.2 System Prompt</h3><p>“System Prompt”（如 “You are a helpful assistant…”）通常不直接出现在 JSON 里的字段中，而是作为 Template 的一部分，拼接在 Instruction 之前。它充当了 <strong>“背景设定”</strong> 的角色。</p><hr><h2 id="四、-总结"><a href="#四、-总结" class="headerlink" title="四、 总结"></a>四、 总结</h2><p>LoRA 是大模型微调领域的里程碑。它不仅仅是一个省显存的工具，更是一套优雅的数学解决方案。</p><ol><li><strong>秩 (Rank)</strong> ：决定了模型的“脑容量”。</li><li><strong>$\alpha/r$ (缩放)</strong> ：保证了训练动力学的一致性，防止梯度爆炸。</li><li><strong>零初始化</strong> ：保证了微调的平滑启动。</li></ol><p>希望这篇文章能帮你彻底理解 LoRA 的数学直觉与工程实践。</p><hr><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ol><li><em>Hu, E. J., et al. (2021). <a href="https://arxiv.org/abs/2106.09685">LoRA: Low-Rank Adaptation of Large Language Models</a>.</em></li><li><em>Vaswani, A., et al. (2017). <a href="https://arxiv.org/abs/1706.03762">Attention Is All You Need</a>.</em></li></ol>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> NLP </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>相机几何完全指南：从世界坐标到图像坐标的完整推导</title>
      <link href="/2026/01/19/camera-geometry-complete-guide/"/>
      <url>/2026/01/19/camera-geometry-complete-guide/</url>
      
        <content type="html"><![CDATA[<p>本文详细推导从3D世界坐标系到2D图像坐标系的完整数学过程，包括相机内外参数、旋转矩阵、单应矩阵的推导与分解，并提供完整的Python可视化代码。</p><span id="more"></span><h2 id="📖-目录"><a href="#📖-目录" class="headerlink" title="📖 目录"></a>📖 目录</h2><ol><li><a href="#%E5%9D%90%E6%A0%87%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%BF%B0">坐标系统概述</a></li><li><a href="#%E5%A4%96%E5%8F%82%E6%8E%A8%E5%AF%BC">从世界坐标系到相机坐标系（外参）</a></li><li><a href="#%E5%86%85%E5%8F%82%E6%8E%A8%E5%AF%BC">从相机坐标系到图像坐标系（内参）</a></li><li><a href="#%E6%97%8B%E8%BD%AC%E7%9F%A9%E9%98%B5">旋转矩阵详解</a></li><li><a href="#%E5%8D%95%E5%BA%94%E7%9F%A9%E9%98%B5">单应矩阵推导</a></li><li><a href="#%E7%9F%A9%E9%98%B5%E5%88%86%E8%A7%A3">相机矩阵分解</a></li><li><a href="#Python%E5%AE%9E%E7%8E%B0">Python完整实现</a></li></ol><hr><h2 id="一、坐标系统概述"><a href="#一、坐标系统概述" class="headerlink" title="一、坐标系统概述"></a>一、坐标系统概述</h2><p>在计算机视觉中，从3D世界到2D图像需要经过<strong>四个坐标系统</strong>的转换：</p><h3 id="1-1-四个坐标系统"><a href="#1-1-四个坐标系统" class="headerlink" title="1.1 四个坐标系统"></a>1.1 四个坐标系统</h3><h4 id="1-世界坐标系-World-Coordinate-System"><a href="#1-世界坐标系-World-Coordinate-System" class="headerlink" title="1. 世界坐标系 (World Coordinate System)"></a><strong>1. 世界坐标系 (World Coordinate System)</strong></h4><ul><li>符号：$(X_w, Y_w, Z_w)$</li><li>描述：真实世界中的3D坐标系统</li><li>单位：通常为米(m)或毫米(mm)</li><li>原点：任意选定的参考点</li></ul><h4 id="2-相机坐标系-Camera-Coordinate-System"><a href="#2-相机坐标系-Camera-Coordinate-System" class="headerlink" title="2. 相机坐标系 (Camera Coordinate System)"></a><strong>2. 相机坐标系 (Camera Coordinate System)</strong></h4><ul><li>符号：$(X_c, Y_c, Z_c)$</li><li>描述：以相机光心为原点的3D坐标系</li><li>单位：米(m)或毫米(mm)</li><li>原点：相机光心</li><li>特点：$Z_c$ 轴为光轴方向</li></ul><h4 id="3-图像坐标系-Image-Coordinate-System"><a href="#3-图像坐标系-Image-Coordinate-System" class="headerlink" title="3. 图像坐标系 (Image Coordinate System)"></a><strong>3. 图像坐标系 (Image Coordinate System)</strong></h4><ul><li>符号：$(x, y)$</li><li>描述：成像平面上的物理坐标</li><li>单位：毫米(mm)</li><li>原点：图像中心（主点）</li></ul><h4 id="4-像素坐标系-Pixel-Coordinate-System"><a href="#4-像素坐标系-Pixel-Coordinate-System" class="headerlink" title="4. 像素坐标系 (Pixel Coordinate System)"></a><strong>4. 像素坐标系 (Pixel Coordinate System)</strong></h4><ul><li>符号：$(u, v)$</li><li>描述：数字图像的离散像素坐标</li><li>单位：像素(pixel)</li><li>原点：图像左上角</li></ul><h3 id="1-2-完整的投影公式"><a href="#1-2-完整的投影公式" class="headerlink" title="1.2 完整的投影公式"></a>1.2 完整的投影公式</h3><p>从世界坐标到像素坐标的完整变换：</p><p>$$<br>\begin{bmatrix} u \ v \ 1 \end{bmatrix} \sim K \cdot [R|t] \cdot \begin{bmatrix} X_w \ Y_w \ Z_w \ 1 \end{bmatrix}<br>$$</p><p>其中：</p><ul><li>$K$：内参矩阵 (3×3)</li><li>$[R|t]$：外参矩阵 (3×4)</li><li>$\sim$：表示齐次坐标意义下的相等（差一个尺度因子）</li></ul><hr><h2 id="二、从世界坐标系到相机坐标系（外参矩阵）"><a href="#二、从世界坐标系到相机坐标系（外参矩阵）" class="headerlink" title="二、从世界坐标系到相机坐标系（外参矩阵）"></a>二、从世界坐标系到相机坐标系（外参矩阵）</h2><h3 id="2-1-刚体变换"><a href="#2-1-刚体变换" class="headerlink" title="2.1 刚体变换"></a>2.1 刚体变换</h3><p>世界坐标系到相机坐标系的转换是一个<strong>刚体变换</strong>（Rigid Body Transformation），包含旋转和平移：</p><p>$$<br>\begin{bmatrix} X_c \ Y_c \ Z_c \end{bmatrix} = R \begin{bmatrix} X_w \ Y_w \ Z_w \end{bmatrix} + t<br>$$</p><p>其中：</p><ul><li>$R \in \mathbb{R}^{3 \times 3}$：旋转矩阵（Rotation Matrix）</li><li>$t \in \mathbb{R}^{3 \times 1}$：平移向量（Translation Vector）</li></ul><h3 id="2-2-齐次坐标表示"><a href="#2-2-齐次坐标表示" class="headerlink" title="2.2 齐次坐标表示"></a>2.2 齐次坐标表示</h3><p>使用齐次坐标可以将旋转和平移统一表示：</p><p>$$<br>\begin{bmatrix} X_c \ Y_c \ Z_c \ 1 \end{bmatrix} =<br>\begin{bmatrix}<br>R &amp; t \<br>0^T &amp; 1<br>\end{bmatrix}<br>\begin{bmatrix} X_w \ Y_w \ Z_w \ 1 \end{bmatrix}<br>$$</p><p>在实际应用中，我们通常使用 $3 \times 4$ 的外参矩阵：</p><p>$$<br>\begin{bmatrix} X_c \ Y_c \ Z_c \end{bmatrix} =<br>[R|t] \begin{bmatrix} X_w \ Y_w \ Z_w \ 1 \end{bmatrix}<br>$$</p><p>其中：</p><p>$$<br>[R|t] = \begin{bmatrix}<br>r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \<br>r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \<br>r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z<br>\end{bmatrix}<br>$$</p><h3 id="2-3-外参的物理意义"><a href="#2-3-外参的物理意义" class="headerlink" title="2.3 外参的物理意义"></a>2.3 外参的物理意义</h3><ul><li>**旋转矩阵 $R$**：描述相机坐标系相对于世界坐标系的方向</li><li>**平移向量 $t$**：描述相机光心在世界坐标系中的位置</li><li><strong>自由度</strong>：6个（3个旋转 + 3个平移）</li></ul><h3 id="2-4-外参的逆变换"><a href="#2-4-外参的逆变换" class="headerlink" title="2.4 外参的逆变换"></a>2.4 外参的逆变换</h3><p>从相机坐标系回到世界坐标系：</p><p>$$<br>\begin{bmatrix} X_w \ Y_w \ Z_w \end{bmatrix} = R^T \left( \begin{bmatrix} X_c \ Y_c \ Z_c \end{bmatrix} - t \right) = R^T \begin{bmatrix} X_c \ Y_c \ Z_c \end{bmatrix} - R^T t<br>$$</p><p>注意：</p><ul><li>$R^T = R^{-1}$（旋转矩阵的转置等于其逆）</li><li>相机在世界坐标系中的位置为 $C = -R^T t$</li></ul><hr><h2 id="三、从相机坐标系到图像坐标系（内参矩阵）"><a href="#三、从相机坐标系到图像坐标系（内参矩阵）" class="headerlink" title="三、从相机坐标系到图像坐标系（内参矩阵）"></a>三、从相机坐标系到图像坐标系（内参矩阵）</h2><h3 id="3-1-针孔相机模型"><a href="#3-1-针孔相机模型" class="headerlink" title="3.1 针孔相机模型"></a>3.1 针孔相机模型</h3><p>针孔相机模型是最基本的相机模型：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">光心 O ────────────────→ 成像平面</span><br><span class="line">          ↗         ↗</span><br><span class="line">      3D点 P    投影点 p</span><br><span class="line">      </span><br><span class="line">相似三角形: x/f = Xc/Zc, y/f = Yc/Zc</span><br><span class="line">因此: x = f·(Xc/Zc), y = f·(Yc/Zc)</span><br></pre></td></tr></tbody></table></figure><h4 id="透视投影公式"><a href="#透视投影公式" class="headerlink" title="透视投影公式"></a>透视投影公式</h4><p>根据相似三角形原理：</p><p>$$<br>\frac{x}{f} = \frac{X_c}{Z_c}, \quad \frac{y}{f} = \frac{Y_c}{Z_c}<br>$$</p><p>其中 $f$ 是焦距（focal length），单位为毫米。</p><p>因此：</p><p>$$<br>x = f \frac{X_c}{Z_c}, \quad y = f \frac{Y_c}{Z_c}<br>$$</p><h3 id="3-2-从图像坐标到像素坐标"><a href="#3-2-从图像坐标到像素坐标" class="headerlink" title="3.2 从图像坐标到像素坐标"></a>3.2 从图像坐标到像素坐标</h3><p>图像坐标 $(x, y)$ 是物理坐标（毫米），需要转换为像素坐标 $(u, v)$：</p><p>$$<br>\begin{cases}<br>u = \alpha x + c_x \<br>v = \beta y + c_y<br>\end{cases}<br>$$</p><p>其中：</p><ul><li>$\alpha = \frac{1}{dx}$：x方向的像素密度（像素/毫米）</li><li>$\beta = \frac{1}{dy}$：y方向的像素密度（像素/毫米）</li><li>$(c_x, c_y)$：主点坐标（图像中心在像素坐标系中的位置）</li></ul><h3 id="3-3-内参矩阵推导"><a href="#3-3-内参矩阵推导" class="headerlink" title="3.3 内参矩阵推导"></a>3.3 内参矩阵推导</h3><p>将上述两步合并：</p><p>$$<br>\begin{aligned}<br>u &amp;= \alpha \cdot f \frac{X_c}{Z_c} + c_x = f_x \frac{X_c}{Z_c} + c_x \<br>v &amp;= \beta \cdot f \frac{Y_c}{Z_c} + c_y = f_y \frac{Y_c}{Z_c} + c_y<br>\end{aligned}<br>$$</p><p>其中：</p><ul><li>$f_x = \alpha \cdot f$：x方向焦距（像素单位）</li><li>$f_y = \beta \cdot f$：y方向焦距（像素单位）</li></ul><p>使用齐次坐标表示：</p><p>$$<br>Z_c \begin{bmatrix} u \ v \ 1 \end{bmatrix} =<br>\begin{bmatrix}<br>f_x &amp; 0 &amp; c_x \<br>0 &amp; f_y &amp; c_y \<br>0 &amp; 0 &amp; 1<br>\end{bmatrix}<br>\begin{bmatrix} X_c \ Y_c \ Z_c \end{bmatrix}<br>$$</p><h3 id="3-4-完整的内参矩阵"><a href="#3-4-完整的内参矩阵" class="headerlink" title="3.4 完整的内参矩阵"></a>3.4 完整的内参矩阵</h3><p>考虑像素倾斜（skew）的一般形式：</p><p>$$<br>K = \begin{bmatrix}<br>f_x &amp; s &amp; c_x \<br>0 &amp; f_y &amp; c_y \<br>0 &amp; 0 &amp; 1<br>\end{bmatrix}<br>$$</p><p>其中：</p><ul><li>$f_x, f_y$：焦距（像素单位）</li><li>$c_x, c_y$：主点坐标（像素）</li><li>$s$：倾斜系数（通常为0）</li></ul><h3 id="3-5-内参矩阵的性质"><a href="#3-5-内参矩阵的性质" class="headerlink" title="3.5 内参矩阵的性质"></a>3.5 内参矩阵的性质</h3><ul><li><strong>维度</strong>：$3 \times 3$</li><li><strong>自由度</strong>：5个（现代相机中$s=0$，则为4个）</li><li><strong>特点</strong>：上三角矩阵</li><li><strong>物理意义</strong>：描述相机的内部几何特性</li></ul><hr><h2 id="四、旋转矩阵详解"><a href="#四、旋转矩阵详解" class="headerlink" title="四、旋转矩阵详解"></a>四、旋转矩阵详解</h2><h3 id="4-1-旋转矩阵的定义与性质"><a href="#4-1-旋转矩阵的定义与性质" class="headerlink" title="4.1 旋转矩阵的定义与性质"></a>4.1 旋转矩阵的定义与性质</h3><p>旋转矩阵 $R \in SO(3)$ 是一个特殊正交矩阵，满足：</p><ol><li><strong>正交性</strong>：$R^T R = R R^T = I$</li><li><strong>行列式</strong>：$\det(R) = 1$</li><li><strong>保持长度</strong>：$|Rv| = |v|$</li><li><strong>保持角度</strong>：$(Rv_1) \cdot (Rv_2) = v_1 \cdot v_2$</li></ol><h3 id="4-2-基本旋转矩阵"><a href="#4-2-基本旋转矩阵" class="headerlink" title="4.2 基本旋转矩阵"></a>4.2 基本旋转矩阵</h3><p><img src="/camera-geometry-complete-guide/rotation_matrices_visualization.png" alt="基本旋转矩阵可视化"></p><h4 id="绕X轴旋转（Roll）"><a href="#绕X轴旋转（Roll）" class="headerlink" title="绕X轴旋转（Roll）"></a>绕X轴旋转（Roll）</h4><p>$$<br>R_x(\alpha) = \begin{bmatrix}<br>1 &amp; 0 &amp; 0 \<br>0 &amp; \cos\alpha &amp; -\sin\alpha \<br>0 &amp; \sin\alpha &amp; \cos\alpha<br>\end{bmatrix}<br>$$</p><h4 id="绕Y轴旋转（Pitch）"><a href="#绕Y轴旋转（Pitch）" class="headerlink" title="绕Y轴旋转（Pitch）"></a>绕Y轴旋转（Pitch）</h4><p>$$<br>R_y(\beta) = \begin{bmatrix}<br>\cos\beta &amp; 0 &amp; \sin\beta \<br>0 &amp; 1 &amp; 0 \<br>-\sin\beta &amp; 0 &amp; \cos\beta<br>\end{bmatrix}<br>$$</p><h4 id="绕Z轴旋转（Yaw）"><a href="#绕Z轴旋转（Yaw）" class="headerlink" title="绕Z轴旋转（Yaw）"></a>绕Z轴旋转（Yaw）</h4><p>$$<br>R_z(\gamma) = \begin{bmatrix}<br>\cos\gamma &amp; -\sin\gamma &amp; 0 \<br>\sin\gamma &amp; \cos\gamma &amp; 0 \<br>0 &amp; 0 &amp; 1<br>\end{bmatrix}<br>$$</p><h3 id="4-3-欧拉角表示"><a href="#4-3-欧拉角表示" class="headerlink" title="4.3 欧拉角表示"></a>4.3 欧拉角表示</h3><p>任意旋转可以分解为三个基本旋转的组合（有多种顺序）：</p><p><strong>ZYX欧拉角（常用）</strong>：</p><p>$$<br>R = R_z(\gamma) R_y(\beta) R_x(\alpha)<br>$$</p><p>展开为：</p><p>$$<br>R = \begin{bmatrix}<br>\cos\gamma\cos\beta &amp; \cos\gamma\sin\beta\sin\alpha - \sin\gamma\cos\alpha &amp; \cos\gamma\sin\beta\cos\alpha + \sin\gamma\sin\alpha \<br>\sin\gamma\cos\beta &amp; \sin\gamma\sin\beta\sin\alpha + \cos\gamma\cos\alpha &amp; \sin\gamma\sin\beta\cos\alpha - \cos\gamma\sin\alpha \<br>-\sin\beta &amp; \cos\beta\sin\alpha &amp; \cos\beta\cos\alpha<br>\end{bmatrix}<br>$$</p><p>⚠️ <strong>万向锁问题</strong>：当 $\beta = \pm 90°$ 时，会出现万向锁（Gimbal Lock）。</p><h3 id="4-4-轴角表示（Axis-Angle）"><a href="#4-4-轴角表示（Axis-Angle）" class="headerlink" title="4.4 轴角表示（Axis-Angle）"></a>4.4 轴角表示（Axis-Angle）</h3><p>用旋转轴 $\mathbf{n} = (n_x, n_y, n_z)^T$（单位向量）和旋转角 $\theta$ 表示旋转。</p><p><strong>罗德里格斯公式（Rodrigues’ Formula）</strong>：</p><p>$$<br>R = I + \sin\theta [\mathbf{n}]_\times + (1-\cos\theta)[\mathbf{n}]_\times^2<br>$$</p><p>其中 $[\mathbf{n}]_\times$ 是反对称矩阵：</p><p>$$<br>[\mathbf{n}]_\times = \begin{bmatrix}<br>0 &amp; -n_z &amp; n_y \<br>n_z &amp; 0 &amp; -n_x \<br>-n_y &amp; n_x &amp; 0<br>\end{bmatrix}<br>$$</p><h3 id="4-5-四元数表示（Quaternion）"><a href="#4-5-四元数表示（Quaternion）" class="headerlink" title="4.5 四元数表示（Quaternion）"></a>4.5 四元数表示（Quaternion）</h3><p>四元数 $q = q_0 + q_1i + q_2j + q_3k$ 可以避免万向锁，其中 $q_0^2 + q_1^2 + q_2^2 + q_3^2 = 1$。</p><p><strong>四元数到旋转矩阵</strong>：</p><p>$$<br>R = \begin{bmatrix}<br>1-2(q_2^2+q_3^2) &amp; 2(q_1q_2-q_0q_3) &amp; 2(q_1q_3+q_0q_2) \<br>2(q_1q_2+q_0q_3) &amp; 1-2(q_1^2+q_3^2) &amp; 2(q_2q_3-q_0q_1) \<br>2(q_1q_3-q_0q_2) &amp; 2(q_2q_3+q_0q_1) &amp; 1-2(q_1^2+q_2^2)<br>\end{bmatrix}<br>$$</p><h3 id="4-6-旋转矩阵的列向量含义-⭐"><a href="#4-6-旋转矩阵的列向量含义-⭐" class="headerlink" title="4.6 旋转矩阵的列向量含义 ⭐"></a>4.6 旋转矩阵的列向量含义 ⭐</h3><p>旋转矩阵 $R$ 的<strong>列向量</strong>具有重要的几何意义：</p><p>$$<br>R = \begin{bmatrix} | &amp; | &amp; | \ \mathbf{r}_1 &amp; \mathbf{r}_2 &amp; \mathbf{r}_3 \ | &amp; | &amp; | \end{bmatrix}<br>$$</p><p><strong>核心理解</strong>：</p><blockquote><p><strong>$R$ 的第 $i$ 列 $\mathbf{r}_i$ 表示世界坐标系的第 $i$ 个基向量在相机坐标系下的表示。</strong></p></blockquote><p>具体来说：</p><ul><li>**第1列 $\mathbf{r}_1$**：世界坐标系的 X 轴方向在相机坐标系中的表示</li><li>**第2列 $\mathbf{r}_2$**：世界坐标系的 Y 轴方向在相机坐标系中的表示</li><li>**第3列 $\mathbf{r}_3$**：世界坐标系的 Z 轴方向在相机坐标系中的表示</li></ul><p><strong>推导</strong>：</p><p>世界坐标系的基向量为：<br>$$<br>\mathbf{e}_1 = \begin{bmatrix}1\0\0\end{bmatrix}, \quad<br>\mathbf{e}_2 = \begin{bmatrix}0\1\0\end{bmatrix}, \quad<br>\mathbf{e}_3 = \begin{bmatrix}0\0\1\end{bmatrix}<br>$$</p><p>在相机坐标系中：<br>$$<br>R\mathbf{e}_1 = \mathbf{r}_1, \quad R\mathbf{e}_2 = \mathbf{r}_2, \quad R\mathbf{e}_3 = \mathbf{r}_3<br>$$</p><p><strong>示例</strong>：</p><p>假设：<br>$$<br>R = \begin{bmatrix}<br>0.866 &amp; -0.500 &amp; 0 \<br>0.500 &amp; 0.866 &amp; 0 \<br>0 &amp; 0 &amp; 1<br>\end{bmatrix}<br>$$</p><p>这是绕Z轴旋转30°的旋转矩阵。</p><ul><li>$\mathbf{r}_1 = [0.866, 0.500, 0]^T$：世界X轴在相机系中指向 $(0.866, 0.500, 0)$</li><li>$\mathbf{r}_2 = [-0.500, 0.866, 0]^T$：世界Y轴在相机系中指向 $(-0.500, 0.866, 0)$</li><li>$\mathbf{r}_3 = [0, 0, 1]^T$：世界Z轴在相机系中仍指向 $(0, 0, 1)$</li></ul><h3 id="4-7-旋转矩阵的行向量含义"><a href="#4-7-旋转矩阵的行向量含义" class="headerlink" title="4.7 旋转矩阵的行向量含义"></a>4.7 旋转矩阵的行向量含义</h3><p>相反地，$R^T$ 的列（即 $R$ 的行）表示<strong>相机坐标系的基向量在世界坐标系下的表示</strong>：</p><p>$$<br>R^T = \begin{bmatrix} </p><ul><li>&amp; \mathbf{r}_1^T &amp; - \ </li><li>&amp; \mathbf{r}_2^T &amp; - \ </li><li>&amp; \mathbf{r}_3^T &amp; -<br>\end{bmatrix}<br>$$</li></ul><p>由于 $R^T = R^{-1}$，我们有：</p><ul><li>$\mathbf{r}_1^T$：相机X轴在世界坐标系中的方向</li><li>$\mathbf{r}_2^T$：相机Y轴在世界坐标系中的方向</li><li>$\mathbf{r}_3^T$：相机Z轴（光轴）在世界坐标系中的方向</li></ul><hr><h2 id="五、单应矩阵推导"><a href="#五、单应矩阵推导" class="headerlink" title="五、单应矩阵推导"></a>五、单应矩阵推导</h2><p><img src="/camera-geometry-complete-guide/homography_transformation.png" alt="单应矩阵变换示意图"></p><h3 id="5-1-单应矩阵的定义"><a href="#5-1-单应矩阵的定义" class="headerlink" title="5.1 单应矩阵的定义"></a>5.1 单应矩阵的定义</h3><p>单应矩阵（Homography Matrix）$H$ 描述两个平面之间的投影变换关系：</p><p>$$<br>\mathbf{p}’ \sim H \mathbf{p}<br>$$</p><p>即：</p><p>$$<br>\begin{bmatrix} x’ \ y’ \ 1 \end{bmatrix} \sim<br>\begin{bmatrix}<br>h_{11} &amp; h_{12} &amp; h_{13} \<br>h_{21} &amp; h_{22} &amp; h_{23} \<br>h_{31} &amp; h_{32} &amp; h_{33}<br>\end{bmatrix}<br>\begin{bmatrix} x \ y \ 1 \end{bmatrix}<br>$$</p><h3 id="5-2-单应矩阵的推导"><a href="#5-2-单应矩阵的推导" class="headerlink" title="5.2 单应矩阵的推导"></a>5.2 单应矩阵的推导</h3><h4 id="场景1：平面物体的投影"><a href="#场景1：平面物体的投影" class="headerlink" title="场景1：平面物体的投影"></a>场景1：平面物体的投影</h4><p>假设世界坐标系中的平面满足 $Z_w = 0$，则：</p><p>$$<br>\begin{bmatrix} X_c \ Y_c \ Z_c \end{bmatrix} =<br>R \begin{bmatrix} X_w \ Y_w \ 0 \end{bmatrix} + t =<br>\begin{bmatrix} r_1 &amp; r_2 &amp; r_3 \end{bmatrix}<br>\begin{bmatrix} X_w \ Y_w \ 0 \end{bmatrix} + t<br>$$</p><p>$$<br>= \begin{bmatrix} r_1 &amp; r_2 \end{bmatrix} \begin{bmatrix} X_w \ Y_w \end{bmatrix} + t<br>= [r_1 ; r_2 ; t] \begin{bmatrix} X_w \ Y_w \ 1 \end{bmatrix}<br>$$</p><p>因此像素坐标为：</p><p>$$<br>\begin{bmatrix} u \ v \ 1 \end{bmatrix} \sim<br>K [r_1 ; r_2 ; t] \begin{bmatrix} X_w \ Y_w \ 1 \end{bmatrix}<br>$$</p><p>定义单应矩阵：</p><p>$$<br>H = K [r_1 ; r_2 ; t] = K \begin{bmatrix} | &amp; | &amp; | \ r_1 &amp; r_2 &amp; t \ | &amp; | &amp; | \end{bmatrix}<br>$$</p><h3 id="5-3-单应矩阵的性质"><a href="#5-3-单应矩阵的性质" class="headerlink" title="5.3 单应矩阵的性质"></a>5.3 单应矩阵的性质</h3><ul><li><strong>维度</strong>：$3 \times 3$</li><li><strong>自由度</strong>：8（因为尺度不定性，9个元素减去1个尺度）</li><li><strong>可逆</strong>：$H^{-1}$ 描述逆向映射</li><li><strong>非线性</strong>：由于齐次坐标的尺度不定性</li></ul><h3 id="5-4-单应矩阵的求解"><a href="#5-4-单应矩阵的求解" class="headerlink" title="5.4 单应矩阵的求解"></a>5.4 单应矩阵的求解</h3><p>给定 $n$ 对对应点 $(x_i, y_i) \leftrightarrow (x_i’, y_i’)$，每对点提供2个约束方程：</p><p>$$<br>\begin{aligned}<br>x_i’ &amp;= \frac{h_{11}x_i + h_{12}y_i + h_{13}}{h_{31}x_i + h_{32}y_i + h_{33}} \<br>y_i’ &amp;= \frac{h_{21}x_i + h_{22}y_i + h_{23}}{h_{31}x_i + h_{32}y_i + h_{33}}<br>\end{aligned}<br>$$</p><p>交叉相乘后得到线性方程：</p><p>$$<br>\begin{bmatrix}<br>-x_i &amp; -y_i &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; x_i’x_i &amp; x_i’y_i &amp; x_i’ \<br>0 &amp; 0 &amp; 0 &amp; -x_i &amp; -y_i &amp; -1 &amp; y_i’x_i &amp; y_i’y_i &amp; y_i’<br>\end{bmatrix}<br>\begin{bmatrix} h_{11} \ h_{12} \ h_{13} \ h_{21} \ h_{22} \ h_{23} \ h_{31} \ h_{32} \ h_{33} \end{bmatrix} = 0<br>$$</p><p><strong>最少需要4对点</strong>（8个方程）来求解8个未知数。</p><p><strong>SVD求解</strong>：</p><p>构建矩阵 $A$（$2n \times 9$），求解 $A\mathbf{h} = 0$：</p><p>$$<br>A = \begin{bmatrix}<br>-x_1 &amp; -y_1 &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; x_1’x_1 &amp; x_1’y_1 &amp; x_1’ \<br>0 &amp; 0 &amp; 0 &amp; -x_1 &amp; -y_1 &amp; -1 &amp; y_1’x_1 &amp; y_1’y_1 &amp; y_1’ \<br>\vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots &amp; \vdots \<br>-x_n &amp; -y_n &amp; -1 &amp; 0 &amp; 0 &amp; 0 &amp; x_n’x_n &amp; x_n’y_n &amp; x_n’ \<br>0 &amp; 0 &amp; 0 &amp; -x_n &amp; -y_n &amp; -1 &amp; y_n’x_n &amp; y_n’y_n &amp; y_n’<br>\end{bmatrix}<br>$$</p><p>对 $A$ 进行SVD分解：$A = U\Sigma V^T$，解为 $V$ 的最后一列。</p><h3 id="5-5-从单应矩阵恢复R和t"><a href="#5-5-从单应矩阵恢复R和t" class="headerlink" title="5.5 从单应矩阵恢复R和t"></a>5.5 从单应矩阵恢复R和t</h3><p>给定 $H = K[r_1 ; r_2 ; t]$，可以恢复旋转和平移：</p><ol><li><p><strong>计算</strong>：$[r_1 ; r_2 ; t] = K^{-1}H$</p></li><li><p><strong>归一化</strong>：由于尺度不定性，需要归一化：<br>$$<br>\lambda = \frac{1}{|K^{-1}H_{:,1}|} = \frac{1}{|K^{-1}H_{:,2}|}<br>$$</p></li><li><p><strong>提取</strong>：<br>$$<br>\begin{aligned}<br>r_1 &amp;= \lambda K^{-1} H_{:,1} \<br>r_2 &amp;= \lambda K^{-1} H_{:,2} \<br>t &amp;= \lambda K^{-1} H_{:,3}<br>\end{aligned}<br>$$</p></li><li><p><strong>计算第三列</strong>：<br>$$<br>r_3 = r_1 \times r_2<br>$$</p></li><li><p><strong>确保正交性</strong>：由于噪声，$[r_1, r_2, r_3]$ 可能不完全正交，需要SVD校正：<br>$$<br>[r_1, r_2, r_3] = U V^T<br>$$<br>其中 $U\Sigma V^T$ 是 $[r_1, r_2, r_3]$ 的SVD分解。</p></li></ol><hr><h2 id="六、相机矩阵分解"><a href="#六、相机矩阵分解" class="headerlink" title="六、相机矩阵分解"></a>六、相机矩阵分解</h2><h3 id="6-1-投影矩阵"><a href="#6-1-投影矩阵" class="headerlink" title="6.1 投影矩阵"></a>6.1 投影矩阵</h3><p>完整的投影矩阵为：</p><p>$$<br>P = K[R|t] = \begin{bmatrix}<br>p_{11} &amp; p_{12} &amp; p_{13} &amp; p_{14} \<br>p_{21} &amp; p_{22} &amp; p_{23} &amp; p_{24} \<br>p_{31} &amp; p_{32} &amp; p_{33} &amp; p_{34}<br>\end{bmatrix}<br>$$</p><p><strong>目标</strong>：从 $P$ 分解出 $K$、$R$、$t$。</p><h3 id="6-2-RQ分解方法"><a href="#6-2-RQ分解方法" class="headerlink" title="6.2 RQ分解方法"></a>6.2 RQ分解方法</h3><p>投影矩阵的前3列可以写为：</p><p>$$<br>M = P_{:,1:3} = KR<br>$$</p><p>其中 $K$ 是上三角矩阵，$R$ 是正交矩阵。</p><p><strong>RQ分解步骤</strong>：</p><ol><li>将矩阵翻转</li><li>进行QR分解</li><li>将结果翻转回来</li></ol><p><strong>Python实现</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.linalg <span class="keyword">import</span> rq</span><br><span class="line"></span><br><span class="line"><span class="comment"># RQ分解</span></span><br><span class="line">K, R = rq(M)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确保K的对角元素为正</span></span><br><span class="line">T = np.diag(np.sign(np.diag(K)))</span><br><span class="line">K = K @ T</span><br><span class="line">R = T @ R</span><br><span class="line"></span><br><span class="line"><span class="comment"># 确保det(R) = 1</span></span><br><span class="line"><span class="keyword">if</span> np.linalg.det(R) &lt; <span class="number">0</span>:</span><br><span class="line">    R = -R</span><br><span class="line">    K = -K</span><br><span class="line"></span><br><span class="line"><span class="comment"># 归一化K</span></span><br><span class="line">K = K / K[<span class="number">2</span>, <span class="number">2</span>]</span><br></pre></td></tr></tbody></table></figure><h3 id="6-3-提取平移向量"><a href="#6-3-提取平移向量" class="headerlink" title="6.3 提取平移向量"></a>6.3 提取平移向量</h3><p>$$<br>t = K^{-1} P_{:,3}<br>$$</p><h3 id="6-4-SVD分解验证旋转矩阵"><a href="#6-4-SVD分解验证旋转矩阵" class="headerlink" title="6.4 SVD分解验证旋转矩阵"></a>6.4 SVD分解验证旋转矩阵</h3><p>为了确保 $R$ 是有效的旋转矩阵（正交且行列式为1）：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">U, S, Vt = np.linalg.svd(R)</span><br><span class="line">R_corrected = U @ Vt</span><br><span class="line"></span><br><span class="line"><span class="keyword">if</span> np.linalg.det(R_corrected) &lt; <span class="number">0</span>:</span><br><span class="line">    U[:, -<span class="number">1</span>] *= -<span class="number">1</span></span><br><span class="line">    R_corrected = U @ Vt</span><br></pre></td></tr></tbody></table></figure><h3 id="6-5-完整的分解流程"><a href="#6-5-完整的分解流程" class="headerlink" title="6.5 完整的分解流程"></a>6.5 完整的分解流程</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">decompose_projection_matrix</span>(<span class="params">P</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    分解投影矩阵 P = K[R|t]</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数:</span></span><br><span class="line"><span class="string">        P: 3x4 投影矩阵</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回:</span></span><br><span class="line"><span class="string">        K: 3x3 内参矩阵</span></span><br><span class="line"><span class="string">        R: 3x3 旋转矩阵</span></span><br><span class="line"><span class="string">        t: 3x1 平移向量</span></span><br><span class="line"><span class="string">        camera_center: 3x1 相机中心在世界坐标系中的位置</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 分离前3列</span></span><br><span class="line">    M = P[:, :<span class="number">3</span>]</span><br><span class="line">    p4 = P[:, <span class="number">3</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># RQ分解</span></span><br><span class="line">    K, R = rq(M)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 确保K的对角元素为正</span></span><br><span class="line">    T = np.diag(np.sign(np.diag(K)))</span><br><span class="line">    K = K @ T</span><br><span class="line">    R = T @ R</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 确保det(R) = 1</span></span><br><span class="line">    <span class="keyword">if</span> np.linalg.det(R) &lt; <span class="number">0</span>:</span><br><span class="line">        R = -R</span><br><span class="line">        K = -K</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 归一化K</span></span><br><span class="line">    K = K / K[<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 提取平移向量</span></span><br><span class="line">    t = np.linalg.inv(K) @ p4</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算相机中心</span></span><br><span class="line">    camera_center = -R.T @ t</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> K, R, t, camera_center</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="七、Python完整实现与可视化"><a href="#七、Python完整实现与可视化" class="headerlink" title="七、Python完整实现与可视化"></a>七、Python完整实现与可视化</h2><h3 id="7-1-坐标系可视化"><a href="#7-1-坐标系可视化" class="headerlink" title="7.1 坐标系可视化"></a>7.1 坐标系可视化</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"><span class="keyword">from</span> matplotlib.patches <span class="keyword">import</span> FancyArrowPatch</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d.proj3d <span class="keyword">import</span> proj_transform</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Arrow3D</span>(<span class="title class_ inherited__">FancyArrowPatch</span>):</span><br><span class="line">    <span class="string">"""3D箭头类"""</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, x, y, z, dx, dy, dz, *args, **kwargs</span>):</span><br><span class="line">        <span class="built_in">super</span>().__init__((<span class="number">0</span>, <span class="number">0</span>), (<span class="number">0</span>, <span class="number">0</span>), *args, **kwargs)</span><br><span class="line">        self._xyz = (x, y, z)</span><br><span class="line">        self._dxdydz = (dx, dy, dz)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">draw</span>(<span class="params">self, renderer</span>):</span><br><span class="line">        x1, y1, z1 = self._xyz</span><br><span class="line">        dx, dy, dz = self._dxdydz</span><br><span class="line">        x2, y2, z2 = (x1 + dx, y1 + dy, z1 + dz)</span><br><span class="line">        xs, ys, zs = proj_transform((x1, x2), (y1, y2), (z1, z2), self.axes.M)</span><br><span class="line">        self.set_positions((xs[<span class="number">0</span>], ys[<span class="number">0</span>]), (xs[<span class="number">1</span>], ys[<span class="number">1</span>]))</span><br><span class="line">        <span class="built_in">super</span>().draw(renderer)</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">do_3d_projection</span>(<span class="params">self, renderer=<span class="literal">None</span></span>):</span><br><span class="line">        x1, y1, z1 = self._xyz</span><br><span class="line">        dx, dy, dz = self._dxdydz</span><br><span class="line">        x2, y2, z2 = (x1 + dx, y1 + dy, z1 + dz)</span><br><span class="line">        xs, ys, zs = proj_transform((x1, x2), (y1, y2), (z1, z2), self.axes.M)</span><br><span class="line">        self.set_positions((xs[<span class="number">0</span>], ys[<span class="number">0</span>]), (xs[<span class="number">1</span>], ys[<span class="number">1</span>]))</span><br><span class="line">        <span class="keyword">return</span> np.<span class="built_in">min</span>(zs)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_coordinate_frame</span>(<span class="params">ax, origin, rotation, scale=<span class="number">1.0</span>, labels=[<span class="string">'X'</span>, <span class="string">'Y'</span>, <span class="string">'Z'</span>], </span></span><br><span class="line"><span class="params">                         colors=[<span class="string">'r'</span>, <span class="string">'g'</span>, <span class="string">'b'</span>], linewidth=<span class="number">2</span></span>):</span><br><span class="line">    <span class="string">"""绘制坐标系"""</span></span><br><span class="line">    axes = rotation @ np.eye(<span class="number">3</span>) * scale</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i, (color, label) <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">zip</span>(colors, labels)):</span><br><span class="line">        arrow = Arrow3D(origin[<span class="number">0</span>], origin[<span class="number">1</span>], origin[<span class="number">2</span>],</span><br><span class="line">                       axes[<span class="number">0</span>, i], axes[<span class="number">1</span>, i], axes[<span class="number">2</span>, i],</span><br><span class="line">                       mutation_scale=<span class="number">20</span>, lw=linewidth, </span><br><span class="line">                       arrowstyle=<span class="string">'-|&gt;'</span>, color=color)</span><br><span class="line">        ax.add_artist(arrow)</span><br><span class="line">        end_point = origin + axes[:, i]</span><br><span class="line">        ax.text(end_point[<span class="number">0</span>], end_point[<span class="number">1</span>], end_point[<span class="number">2</span>], </span><br><span class="line">               label, fontsize=<span class="number">12</span>, weight=<span class="string">'bold'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_camera</span>(<span class="params">ax, position, rotation, scale=<span class="number">0.5</span>, color=<span class="string">'blue'</span></span>):</span><br><span class="line">    <span class="string">"""绘制相机模型"""</span></span><br><span class="line">    <span class="comment"># 相机锥体的顶点（在相机坐标系下）</span></span><br><span class="line">    camera_points = np.array([</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>],           <span class="comment"># 光心</span></span><br><span class="line">        [-<span class="number">1</span>, -<span class="number">1</span>, <span class="number">2</span>],         <span class="comment"># 图像平面四个角</span></span><br><span class="line">        [<span class="number">1</span>, -<span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>],</span><br><span class="line">        [-<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>]</span><br><span class="line">    ]) * scale</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 转换到世界坐标系</span></span><br><span class="line">    world_points = (rotation @ camera_points.T).T + position</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 绘制相机锥体</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>):</span><br><span class="line">        ax.plot([world_points[<span class="number">0</span>, <span class="number">0</span>], world_points[i, <span class="number">0</span>]],</span><br><span class="line">               [world_points[<span class="number">0</span>, <span class="number">1</span>], world_points[i, <span class="number">1</span>]],</span><br><span class="line">               [world_points[<span class="number">0</span>, <span class="number">2</span>], world_points[i, <span class="number">2</span>]], </span><br><span class="line">               color=<span class="string">'black'</span>, linewidth=<span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 图像平面的四条边</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">5</span>):</span><br><span class="line">        next_i = i + <span class="number">1</span> <span class="keyword">if</span> i &lt; <span class="number">4</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">        ax.plot([world_points[i, <span class="number">0</span>], world_points[next_i, <span class="number">0</span>]],</span><br><span class="line">               [world_points[i, <span class="number">1</span>], world_points[next_i, <span class="number">1</span>]],</span><br><span class="line">               [world_points[i, <span class="number">2</span>], world_points[next_i, <span class="number">2</span>]], </span><br><span class="line">               color=color, linewidth=<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> world_points</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_coordinate_systems</span>():</span><br><span class="line">    <span class="string">"""可视化四个坐标系统"""</span></span><br><span class="line">    fig = plt.figure(figsize=(<span class="number">18</span>, <span class="number">12</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 世界坐标系</span></span><br><span class="line">    world_origin = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>])</span><br><span class="line">    world_rotation = np.eye(<span class="number">3</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 相机位置和姿态</span></span><br><span class="line">    camera_position = np.array([<span class="number">3</span>, <span class="number">2</span>, <span class="number">4</span>])</span><br><span class="line">    theta_y = -np.pi/<span class="number">6</span>  <span class="comment"># 绕Y轴旋转</span></span><br><span class="line">    theta_x = -np.pi/<span class="number">9</span>  <span class="comment"># 绕X轴旋转</span></span><br><span class="line">    </span><br><span class="line">    Ry = np.array([[np.cos(theta_y), <span class="number">0</span>, np.sin(theta_y)],</span><br><span class="line">                   [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">                   [-np.sin(theta_y), <span class="number">0</span>, np.cos(theta_y)]])</span><br><span class="line">    Rx = np.array([[<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">                   [<span class="number">0</span>, np.cos(theta_x), -np.sin(theta_x)],</span><br><span class="line">                   [<span class="number">0</span>, np.sin(theta_x), np.cos(theta_x)]])</span><br><span class="line">    camera_rotation = Ry @ Rx</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 绘制3D场景</span></span><br><span class="line">    ax = fig.add_subplot(<span class="number">221</span>, projection=<span class="string">'3d'</span>)</span><br><span class="line">    ax.set_title(<span class="string">'世界坐标系 → 相机坐标系'</span>, fontsize=<span class="number">14</span>, weight=<span class="string">'bold'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 绘制世界坐标系</span></span><br><span class="line">    draw_coordinate_frame(ax, world_origin, world_rotation, scale=<span class="number">2.0</span>, </span><br><span class="line">                         labels=[<span class="string">'Xw'</span>, <span class="string">'Yw'</span>, <span class="string">'Zw'</span>], </span><br><span class="line">                         colors=[<span class="string">'red'</span>, <span class="string">'green'</span>, <span class="string">'blue'</span>], linewidth=<span class="number">3</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 绘制相机坐标系</span></span><br><span class="line">    draw_coordinate_frame(ax, camera_position, camera_rotation, scale=<span class="number">1.5</span>,</span><br><span class="line">                         labels=[<span class="string">'Xc'</span>, <span class="string">'Yc'</span>, <span class="string">'Zc'</span>], </span><br><span class="line">                         colors=[<span class="string">'darkred'</span>, <span class="string">'darkgreen'</span>, <span class="string">'darkblue'</span>], linewidth=<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 绘制相机模型</span></span><br><span class="line">    draw_camera(ax, camera_position, camera_rotation, scale=<span class="number">0.6</span>, color=<span class="string">'cyan'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 绘制一个3D点</span></span><br><span class="line">    point_world = np.array([<span class="number">1.5</span>, <span class="number">1.5</span>, <span class="number">1.0</span>])</span><br><span class="line">    ax.scatter(*point_world, c=<span class="string">'purple'</span>, s=<span class="number">200</span>, marker=<span class="string">'o'</span>, </span><br><span class="line">              edgecolors=<span class="string">'black'</span>, linewidths=<span class="number">2</span>)</span><br><span class="line">    ax.text(point_world[<span class="number">0</span>]+<span class="number">0.2</span>, point_world[<span class="number">1</span>]+<span class="number">0.2</span>, point_world[<span class="number">2</span>]+<span class="number">0.2</span>, </span><br><span class="line">           <span class="string">'P(Xw,Yw,Zw)'</span>, fontsize=<span class="number">11</span>, weight=<span class="string">'bold'</span>, color=<span class="string">'purple'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 投影线</span></span><br><span class="line">    ax.plot([point_world[<span class="number">0</span>], camera_position[<span class="number">0</span>]],</span><br><span class="line">           [point_world[<span class="number">1</span>], camera_position[<span class="number">1</span>]],</span><br><span class="line">           [point_world[<span class="number">2</span>], camera_position[<span class="number">2</span>]], </span><br><span class="line">           <span class="string">'purple'</span>, linestyle=<span class="string">'--'</span>, linewidth=<span class="number">2</span>, alpha=<span class="number">0.6</span>)</span><br><span class="line">    </span><br><span class="line">    ax.set_xlabel(<span class="string">'X'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">    ax.set_ylabel(<span class="string">'Y'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">    ax.set_zlabel(<span class="string">'Z'</span>, fontsize=<span class="number">12</span>)</span><br><span class="line">    ax.set_xlim([-<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line">    ax.set_ylim([-<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line">    ax.set_zlim([-<span class="number">1</span>, <span class="number">5</span>])</span><br><span class="line">    ax.view_init(elev=<span class="number">20</span>, azim=<span class="number">45</span>)</span><br><span class="line">    ax.grid(<span class="literal">True</span>, alpha=<span class="number">0.3</span>)</span><br><span class="line">    </span><br><span class="line">    plt.tight_layout()</span><br><span class="line">    plt.savefig(<span class="string">'coordinate_systems_3d.png'</span>, dpi=<span class="number">300</span>, bbox_inches=<span class="string">'tight'</span>)</span><br><span class="line">    plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行可视化</span></span><br><span class="line">visualize_coordinate_systems()</span><br></pre></td></tr></tbody></table></figure><p>保存为 <code>visualize_coordinates.py</code></p><h3 id="7-2-完整的相机几何计算示例"><a href="#7-2-完整的相机几何计算示例" class="headerlink" title="7.2 完整的相机几何计算示例"></a>7.2 完整的相机几何计算示例</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># camera_geometry_demo.py</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.linalg <span class="keyword">import</span> rq</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">CameraGeometry</span>:</span><br><span class="line">    <span class="string">"""相机几何计算类"""</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, fx, fy, cx, cy, width, height</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化相机内参</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            fx, fy: 焦距（像素）</span></span><br><span class="line"><span class="string">            cx, cy: 主点（像素）</span></span><br><span class="line"><span class="string">            width, height: 图像尺寸</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.K = np.array([</span><br><span class="line">            [fx, <span class="number">0</span>, cx],</span><br><span class="line">            [<span class="number">0</span>, fy, cy],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">        ])</span><br><span class="line">        self.width = width</span><br><span class="line">        self.height = height</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rotation_matrix_from_euler</span>(<span class="params">roll, pitch, yaw, order=<span class="string">'xyz'</span></span>):</span><br><span class="line">        <span class="string">"""从欧拉角创建旋转矩阵"""</span></span><br><span class="line">        Rx = np.array([</span><br><span class="line">            [<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, np.cos(roll), -np.sin(roll)],</span><br><span class="line">            [<span class="number">0</span>, np.sin(roll), np.cos(roll)]</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        Ry = np.array([</span><br><span class="line">            [np.cos(pitch), <span class="number">0</span>, np.sin(pitch)],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">            [-np.sin(pitch), <span class="number">0</span>, np.cos(pitch)]</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        Rz = np.array([</span><br><span class="line">            [np.cos(yaw), -np.sin(yaw), <span class="number">0</span>],</span><br><span class="line">            [np.sin(yaw), np.cos(yaw), <span class="number">0</span>],</span><br><span class="line">            [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">        ])</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> order == <span class="string">'xyz'</span>:</span><br><span class="line">            <span class="keyword">return</span> Rz @ Ry @ Rx</span><br><span class="line">        <span class="keyword">elif</span> order == <span class="string">'zyx'</span>:</span><br><span class="line">            <span class="keyword">return</span> Rx @ Ry @ Rz</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Order must be 'xyz' or 'zyx'"</span>)</span><br><span class="line">    </span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">rotation_matrix_from_axis_angle</span>(<span class="params">axis, theta</span>):</span><br><span class="line">        <span class="string">"""从轴角创建旋转矩阵（罗德里格斯公式）"""</span></span><br><span class="line">        axis = axis / np.linalg.norm(axis)  <span class="comment"># 归一化</span></span><br><span class="line">        K = np.array([</span><br><span class="line">            [<span class="number">0</span>, -axis[<span class="number">2</span>], axis[<span class="number">1</span>]],</span><br><span class="line">            [axis[<span class="number">2</span>], <span class="number">0</span>, -axis[<span class="number">0</span>]],</span><br><span class="line">            [-axis[<span class="number">1</span>], axis[<span class="number">0</span>], <span class="number">0</span>]</span><br><span class="line">        ])</span><br><span class="line">        R = np.eye(<span class="number">3</span>) + np.sin(theta) * K + (<span class="number">1</span> - np.cos(theta)) * (K @ K)</span><br><span class="line">        <span class="keyword">return</span> R</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">project_point</span>(<span class="params">self, point_world, R, t</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        将世界坐标点投影到图像</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            point_world: 3D点（世界坐标系）</span></span><br><span class="line"><span class="string">            R: 旋转矩阵</span></span><br><span class="line"><span class="string">            t: 平移向量</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            pixel: 像素坐标 (u, v)</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 世界坐标 -&gt; 相机坐标</span></span><br><span class="line">        point_camera = R @ point_world + t</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 相机坐标 -&gt; 图像坐标（透视投影）</span></span><br><span class="line">        <span class="keyword">if</span> point_camera[<span class="number">2</span>] &lt;= <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">raise</span> ValueError(<span class="string">"Point is behind the camera"</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 投影到像素坐标</span></span><br><span class="line">        point_homo = self.K @ point_camera</span><br><span class="line">        pixel = point_homo[:<span class="number">2</span>] / point_homo[<span class="number">2</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> pixel</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_projection_matrix</span>(<span class="params">self, R, t</span>):</span><br><span class="line">        <span class="string">"""计算投影矩阵 P = K[R|t]"""</span></span><br><span class="line">        <span class="keyword">return</span> self.K @ np.hstack([R, t.reshape(-<span class="number">1</span>, <span class="number">1</span>)])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">decompose_projection_matrix</span>(<span class="params">self, P</span>):</span><br><span class="line">        <span class="string">"""分解投影矩阵"""</span></span><br><span class="line">        M = P[:, :<span class="number">3</span>]</span><br><span class="line">        p4 = P[:, <span class="number">3</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># RQ分解</span></span><br><span class="line">        K, R = rq(M)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 确保K的对角元素为正</span></span><br><span class="line">        T = np.diag(np.sign(np.diag(K)))</span><br><span class="line">        K = K @ T</span><br><span class="line">        R = T @ R</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 确保det(R) = 1</span></span><br><span class="line">        <span class="keyword">if</span> np.linalg.det(R) &lt; <span class="number">0</span>:</span><br><span class="line">            R = -R</span><br><span class="line">            K = -K</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 归一化K</span></span><br><span class="line">        K = K / K[<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 提取平移向量</span></span><br><span class="line">        t = np.linalg.inv(K) @ p4</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 相机中心</span></span><br><span class="line">        camera_center = -R.T @ t</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> K, R, t, camera_center</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_homography</span>(<span class="params">self, R, t, n, d</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        计算平面的单应矩阵</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        参数:</span></span><br><span class="line"><span class="string">            R, t: 外参</span></span><br><span class="line"><span class="string">            n: 平面法向量（世界坐标系）</span></span><br><span class="line"><span class="string">            d: 平面到原点的距离</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        返回:</span></span><br><span class="line"><span class="string">            H: 3x3 单应矩阵</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        H = self.K @ (R - (t @ n.T) / d) @ np.linalg.inv(self.K)</span><br><span class="line">        <span class="keyword">return</span> H / H[<span class="number">2</span>, <span class="number">2</span>]  <span class="comment"># 归一化</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 示例：完整的投影流程</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    <span class="comment"># 创建相机</span></span><br><span class="line">    camera = CameraGeometry(</span><br><span class="line">        fx=<span class="number">800</span>, fy=<span class="number">800</span>,  <span class="comment"># 焦距</span></span><br><span class="line">        cx=<span class="number">320</span>, cy=<span class="number">240</span>,  <span class="comment"># 主点</span></span><br><span class="line">        width=<span class="number">640</span>, height=<span class="number">480</span>  <span class="comment"># 图像尺寸</span></span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"="</span> * <span class="number">60</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"相机内参矩阵 K:"</span>)</span><br><span class="line">    <span class="built_in">print</span>(camera.K)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 创建外参</span></span><br><span class="line">    roll, pitch, yaw = np.deg2rad([<span class="number">10</span>, <span class="number">20</span>, <span class="number">30</span>])</span><br><span class="line">    R = camera.rotation_matrix_from_euler(roll, pitch, yaw, order=<span class="string">'zyx'</span>)</span><br><span class="line">    t = np.array([<span class="number">1.0</span>, <span class="number">2.0</span>, <span class="number">5.0</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"旋转矩阵 R:"</span>)</span><br><span class="line">    <span class="built_in">print</span>(R)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"\ndet(R) = <span class="subst">{np.linalg.det(R):<span class="number">.6</span>f}</span> (应该为1)"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"R^T @ R ="</span>)</span><br><span class="line">    <span class="built_in">print</span>(R.T @ R)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"(应该为单位矩阵)"</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"平移向量 t:"</span>)</span><br><span class="line">    <span class="built_in">print</span>(t)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 旋转矩阵列向量的含义</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"="</span> * <span class="number">60</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"旋转矩阵列向量的几何意义:"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"第1列（世界X轴在相机系中的方向）:"</span>, R[:, <span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"第2列（世界Y轴在相机系中的方向）:"</span>, R[:, <span class="number">1</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"第3列（世界Z轴在相机系中的方向）:"</span>, R[:, <span class="number">2</span>])</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 相机在世界坐标系中的位置</span></span><br><span class="line">    camera_center = -R.T @ t</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"相机中心在世界坐标系中的位置: <span class="subst">{camera_center}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 投影一个3D点</span></span><br><span class="line">    point_3d = np.array([<span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">4.0</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"="</span> * <span class="number">60</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"3D点（世界坐标）: <span class="subst">{point_3d}</span>"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">        pixel = camera.project_point(point_3d, R, t)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"投影到图像（像素坐标）: (<span class="subst">{pixel[<span class="number">0</span>]:<span class="number">.2</span>f}</span>, <span class="subst">{pixel[<span class="number">1</span>]:<span class="number">.2</span>f}</span>)"</span>)</span><br><span class="line">    <span class="keyword">except</span> ValueError <span class="keyword">as</span> e:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"投影失败: <span class="subst">{e}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算并分解投影矩阵</span></span><br><span class="line">    P = camera.compute_projection_matrix(R, t)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"="</span> * <span class="number">60</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"投影矩阵 P = K[R|t]:"</span>)</span><br><span class="line">    <span class="built_in">print</span>(P)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 分解投影矩阵</span></span><br><span class="line">    K_recovered, R_recovered, t_recovered, center_recovered = \</span><br><span class="line">        camera.decompose_projection_matrix(P)</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"分解后的内参矩阵 K:"</span>)</span><br><span class="line">    <span class="built_in">print</span>(K_recovered)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"\n分解后的旋转矩阵 R:"</span>)</span><br><span class="line">    <span class="built_in">print</span>(R_recovered)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"\n分解后的平移向量 t:"</span>)</span><br><span class="line">    <span class="built_in">print</span>(t_recovered)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"\n分解后的相机中心:"</span>)</span><br><span class="line">    <span class="built_in">print</span>(center_recovered)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 验证</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"="</span> * <span class="number">60</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"验证分解结果:"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"K误差: <span class="subst">{np.linalg.norm(camera.K - K_recovered):<span class="number">.2</span>e}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"R误差: <span class="subst">{np.linalg.norm(R - R_recovered):<span class="number">.2</span>e}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"t误差: <span class="subst">{np.linalg.norm(t - t_recovered):<span class="number">.2</span>e}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 单应矩阵示例</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"="</span> * <span class="number">60</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"单应矩阵示例（地平面 Z=0）:"</span>)</span><br><span class="line">    n = np.array([<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>])  <span class="comment"># 平面法向量</span></span><br><span class="line">    d = <span class="number">0</span>  <span class="comment"># 平面距离原点</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对于Z=0平面，单应矩阵为 H = K[r1 r2 t]</span></span><br><span class="line">    H = camera.K @ np.column_stack([R[:, <span class="number">0</span>], R[:, <span class="number">1</span>], t])</span><br><span class="line">    H = H / H[<span class="number">2</span>, <span class="number">2</span>]  <span class="comment"># 归一化</span></span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"单应矩阵 H:"</span>)</span><br><span class="line">    <span class="built_in">print</span>(H)</span><br><span class="line">    <span class="built_in">print</span>()</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 测试单应变换</span></span><br><span class="line">    point_2d_world = np.array([<span class="number">2.0</span>, <span class="number">3.0</span>, <span class="number">1.0</span>])  <span class="comment"># 世界平面上的点（齐次坐标）</span></span><br><span class="line">    point_2d_image = H @ point_2d_world</span><br><span class="line">    point_2d_image = point_2d_image / point_2d_image[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"平面上的点（世界坐标）: (<span class="subst">{point_2d_world[<span class="number">0</span>]}</span>, <span class="subst">{point_2d_world[<span class="number">1</span>]}</span>)"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"通过单应矩阵投影到图像: (<span class="subst">{point_2d_image[<span class="number">0</span>]:<span class="number">.2</span>f}</span>, <span class="subst">{point_2d_image[<span class="number">1</span>]:<span class="number">.2</span>f}</span>)"</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 验证：通过完整投影</span></span><br><span class="line">    point_3d_on_plane = np.array([point_2d_world[<span class="number">0</span>], point_2d_world[<span class="number">1</span>], <span class="number">0.0</span>])</span><br><span class="line">    pixel_verify = camera.project_point(point_3d_on_plane, R, t)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"通过完整投影验证: (<span class="subst">{pixel_verify[<span class="number">0</span>]:<span class="number">.2</span>f}</span>, <span class="subst">{pixel_verify[<span class="number">1</span>]:<span class="number">.2</span>f}</span>)"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"误差: <span class="subst">{np.linalg.norm(point_2d_image[:<span class="number">2</span>] - pixel_verify):<span class="number">.2</span>e}</span>"</span>)</span><br></pre></td></tr></tbody></table></figure><p>保存为 <code>camera_geometry_demo.py</code></p><hr><h2 id="八、实际应用场景"><a href="#八、实际应用场景" class="headerlink" title="八、实际应用场景"></a>八、实际应用场景</h2><h3 id="8-1-相机标定"><a href="#8-1-相机标定" class="headerlink" title="8.1 相机标定"></a>8.1 相机标定</h3><p>通过拍摄标定板（如棋盘格）的多张图像，可以估计相机内参和畸变参数。</p><h3 id="8-2-3D重建"><a href="#8-2-3D重建" class="headerlink" title="8.2 3D重建"></a>8.2 3D重建</h3><p>利用多视图几何和外参矩阵，可以从多张图像重建3D场景。</p><h3 id="8-3-增强现实（AR）"><a href="#8-3-增强现实（AR）" class="headerlink" title="8.3 增强现实（AR）"></a>8.3 增强现实（AR）</h3><p>通过相机位姿估计，可以将虚拟物体精确地叠加到真实场景中。</p><h3 id="8-4-视觉SLAM"><a href="#8-4-视觉SLAM" class="headerlink" title="8.4 视觉SLAM"></a>8.4 视觉SLAM</h3><p>同时定位与地图构建（SLAM）需要实时估计相机的位姿（外参）。</p><h3 id="8-5-图像拼接"><a href="#8-5-图像拼接" class="headerlink" title="8.5 图像拼接"></a>8.5 图像拼接</h3><p>利用单应矩阵可以将多张图像拼接成全景图。</p><hr><h2 id="九、总结"><a href="#九、总结" class="headerlink" title="九、总结"></a>九、总结</h2><p>本文详细推导了从3D世界坐标到2D图像坐标的完整数学过程：</p><ol><li>**外参矩阵 $[R|t]$**：描述相机在世界中的位置和方向（6自由度）</li><li>**内参矩阵 $K$**：描述相机的内部几何参数（4-5自由度）</li><li>**旋转矩阵 $R$**：可用欧拉角、轴角、四元数表示，其列向量表示世界坐标系的基向量在相机系中的表示</li><li>**单应矩阵 $H$**：描述平面到图像的投影变换（8自由度）</li><li><strong>矩阵分解</strong>：可以从投影矩阵恢复出内外参数</li></ol><p>完整的投影公式为：</p><p>$$<br>s \begin{bmatrix} u \ v \ 1 \end{bmatrix} =<br>\begin{bmatrix}<br>f_x &amp; 0 &amp; c_x \<br>0 &amp; f_y &amp; c_y \<br>0 &amp; 0 &amp; 1<br>\end{bmatrix}<br>\begin{bmatrix}<br>r_{11} &amp; r_{12} &amp; r_{13} &amp; t_x \<br>r_{21} &amp; r_{22} &amp; r_{23} &amp; t_y \<br>r_{31} &amp; r_{32} &amp; r_{33} &amp; t_z<br>\end{bmatrix}<br>\begin{bmatrix} X_w \ Y_w \ Z_w \ 1 \end{bmatrix}<br>$$</p><p>配套的Python代码提供了完整的实现和可视化，可以直接用于实际项目。</p><hr><h2 id="参考资料"><a href="#参考资料" class="headerlink" title="参考资料"></a>参考资料</h2><ol><li>Hartley, R., &amp; Zisserman, A. (2003). <em>Multiple View Geometry in Computer Vision</em>. Cambridge University Press.</li><li>Szeliski, R. (2010). <em>Computer Vision: Algorithms and Applications</em>. Springer.</li><li>OpenCV Documentation: Camera Calibration and 3D Reconstruction</li></ol><hr><h2 id="附录：运行代码"><a href="#附录：运行代码" class="headerlink" title="附录：运行代码"></a>附录：运行代码</h2><p>将上述Python代码保存为对应的文件名，然后运行：</p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 安装依赖</span></span><br><span class="line">pip install numpy matplotlib scipy</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行可视化</span></span><br><span class="line">python visualize_coordinates.py</span><br><span class="line"></span><br><span class="line"><span class="comment"># 运行完整示例</span></span><br><span class="line">python camera_geometry_demo.py</span><br></pre></td></tr></tbody></table></figure><hr><p><strong>关键词</strong>：相机几何、坐标变换、旋转矩阵、内参矩阵、外参矩阵、单应矩阵、投影矩阵分解、计算机视觉</p><p><strong>博客标签</strong>：#计算机视觉 #相机标定 #多视几何 #矩阵分解 #Python</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从图像到俯视图：完整实现指南</title>
      <link href="/2026/01/19/image-to-bird-eye-view/"/>
      <url>/2026/01/19/image-to-bird-eye-view/</url>
      
        <content type="html"><![CDATA[<blockquote><p><strong>从输入图像到输出鸟瞰图的完整实现</strong> - 适合所有水平的学习者</p></blockquote><span id="more"></span><h2 id="🎯-教程简介"><a href="#🎯-教程简介" class="headerlink" title="🎯 教程简介"></a>🎯 教程简介</h2><p>本教程将带你<strong>从零开始</strong>学习如何把一张斜着拍摄的图像（比如路面照片）转换成从正上方看的俯视图（鸟瞰图）。</p><h3 id="你将学到什么？"><a href="#你将学到什么？" class="headerlink" title="你将学到什么？"></a>你将学到什么？</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">infographic list-row-simple-horizontal-arrow</span><br><span class="line">data</span><br><span class="line">  title 完整学习路径</span><br><span class="line">  items</span><br><span class="line">    - label 输入图像</span><br><span class="line">      desc 斜着看的照片</span><br><span class="line">    - label 理解透视</span><br><span class="line">      desc 坐标系统</span><br><span class="line">    - label 计算变换</span><br><span class="line">      desc 单应性矩阵</span><br><span class="line">    - label 实现代码</span><br><span class="line">      desc Python+OpenCV</span><br><span class="line">    - label 输出俯视图</span><br><span class="line">      desc 鸟瞰图</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="📚-第一章：基础概念"><a href="#📚-第一章：基础概念" class="headerlink" title="📚 第一章：基础概念"></a>📚 第一章：基础概念</h2><h3 id="1-1-三种坐标系统"><a href="#1-1-三种坐标系统" class="headerlink" title="1.1 三种坐标系统"></a>1.1 三种坐标系统</h3><p>在图像处理中，我们需要理解三种坐标系统的转换关系：</p><p><img src="/./image-to-bird-eye-view/figures/01_coordinate_systems.png" alt="坐标系统示意图"></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">infographic list-grid-badge-card</span><br><span class="line">data</span><br><span class="line">  title 三种坐标系</span><br><span class="line">  items</span><br><span class="line">    - label 世界坐标系</span><br><span class="line">      desc 固定在地面，单位：米</span><br><span class="line">      icon mdi:earth</span><br><span class="line">    - label 相机坐标系</span><br><span class="line">      desc 跟随相机移动旋转</span><br><span class="line">      icon mdi:camera</span><br><span class="line">    - label 图像坐标系</span><br><span class="line">      desc 2D像素平面</span><br><span class="line">      icon mdi:image</span><br></pre></td></tr></tbody></table></figure><h4 id="世界坐标系-World-Coordinates"><a href="#世界坐标系-World-Coordinates" class="headerlink" title="世界坐标系 (World Coordinates)"></a>世界坐标系 (World Coordinates)</h4><p><strong>生活例子</strong>：就像地图的经纬度，固定不变的参考系。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 世界坐标中的一个点（3米，5米，0米）</span></span><br><span class="line">point_world = np.array([<span class="number">3.0</span>, <span class="number">5.0</span>, <span class="number">0.0</span>])  <span class="comment"># [X, Y, Z]</span></span><br></pre></td></tr></tbody></table></figure><p><strong>特点：</strong></p><ul><li>原点：通常选在地面某个固定位置</li><li>X轴：向右（东方）</li><li>Y轴：向前（北方）</li><li>Z轴：向上（天空）</li></ul><hr><h4 id="相机坐标系-Camera-Coordinates"><a href="#相机坐标系-Camera-Coordinates" class="headerlink" title="相机坐标系 (Camera Coordinates)"></a>相机坐标系 (Camera Coordinates)</h4><p><img src="/./image-to-bird-eye-view/figures/14_where_is_z.png" alt="Z坐标位置说明"></p><p><strong>核心规则：</strong></p><ul><li>原点：在相机镜头中心</li><li>Z轴：镜头朝向（光轴方向）</li><li>X轴：相机向右</li><li>Y轴：相机向下（⚠️ 注意：向下！）</li></ul><p><strong>为什么Y轴向下？</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">传统原因：</span><br><span class="line">┌─────────┐</span><br><span class="line">│ (0,0)   │  ← 图像左上角是(0,0)</span><br><span class="line">│    ↓ Y  │</span><br><span class="line">│  → X    │</span><br><span class="line">└─────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h4 id="图像坐标系-Image-Coordinates"><a href="#图像坐标系-Image-Coordinates" class="headerlink" title="图像坐标系 (Image Coordinates)"></a>图像坐标系 (Image Coordinates)</h4><p><strong>就是我们看到的照片像素坐标：</strong></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图像中的一个像素点</span></span><br><span class="line">pixel = (<span class="number">320</span>, <span class="number">240</span>)  <span class="comment"># (u, v) 像素坐标</span></span><br></pre></td></tr></tbody></table></figure><p><strong>单位</strong>：像素 (pixel)</p><hr><h3 id="1-2-透视投影原理"><a href="#1-2-透视投影原理" class="headerlink" title="1.2 透视投影原理"></a>1.2 透视投影原理</h3><p><img src="/./image-to-bird-eye-view/figures/01_pinhole_camera.png" alt="小孔成像模型"></p><p><strong>小孔成像模型</strong>是相机的数学抽象：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">3D世界 ──────&gt; 小孔 ──────&gt; 2D图像</span><br><span class="line">(X,Y,Z)        相机        (u,v)</span><br></pre></td></tr></tbody></table></figure><p><strong>投影过程：</strong></p><p><img src="/./image-to-bird-eye-view/figures/02_projection_process.png" alt="投影过程示意图"></p><p><strong>数学公式：</strong></p><p>$$<br>\begin{bmatrix} u \ v \ 1 \end{bmatrix} = K \begin{bmatrix} R | t \end{bmatrix} \begin{bmatrix} X \ Y \ Z \ 1 \end{bmatrix}<br>$$</p><p>其中：</p><ul><li><strong>K</strong>：相机内参矩阵（焦距、主点）</li><li><strong>R</strong>：旋转矩阵（相机朝向）</li><li><strong>t</strong>：平移向量（相机位置）</li></ul><hr><h3 id="1-3-透视效果"><a href="#1-3-透视效果" class="headerlink" title="1.3 透视效果"></a>1.3 透视效果</h3><p><img src="/./image-to-bird-eye-view/figures/03_perspective_effect.png" alt="透视效果"></p><p><strong>为什么远处的物体看起来小？</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">近处的车 ━━━━  看起来大</span><br><span class="line">中间的车 ━━━   看起来中等</span><br><span class="line">远处的车 ━━    看起来小</span><br></pre></td></tr></tbody></table></figure><p>这就是<strong>透视投影</strong>的效果！</p><hr><h2 id="📐-第二章：消失点理论"><a href="#📐-第二章：消失点理论" class="headerlink" title="📐 第二章：消失点理论"></a>📐 第二章：消失点理论</h2><h3 id="2-1-什么是消失点？"><a href="#2-1-什么是消失点？" class="headerlink" title="2.1 什么是消失点？"></a>2.1 什么是消失点？</h3><p><img src="/./image-to-bird-eye-view/figures/02_vanishing_point.png" alt="消失点示意图"></p><p><strong>生活例子</strong>：站在铁轨中间拍照，两条平行的铁轨在远处看起来会”相交”。</p><p><strong>定义</strong>：</p><blockquote><p>现实中平行的线，在图像中延伸后相交的点，就是<strong>消失点</strong> (Vanishing Point)。</p></blockquote><hr><h3 id="2-2-消失点的计算"><a href="#2-2-消失点的计算" class="headerlink" title="2.2 消失点的计算"></a>2.2 消失点的计算</h3><p><strong>原理</strong>：两条平行线的交点</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_vanishing_point</span>(<span class="params">line1_pts, line2_pts</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    计算两条平行线的消失点</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        line1_pts: 直线1的两个点 [[x1,y1], [x2,y2]]</span></span><br><span class="line"><span class="string">        line2_pts: 直线2的两个点 [[x3,y3], [x4,y4]]</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">        vp: 消失点坐标 [vx, vy]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 转为齐次坐标</span></span><br><span class="line">    p1 = np.array([line1_pts[<span class="number">0</span>][<span class="number">0</span>], line1_pts[<span class="number">0</span>][<span class="number">1</span>], <span class="number">1</span>])</span><br><span class="line">    p2 = np.array([line1_pts[<span class="number">1</span>][<span class="number">0</span>], line1_pts[<span class="number">1</span>][<span class="number">1</span>], <span class="number">1</span>])</span><br><span class="line">    p3 = np.array([line2_pts[<span class="number">0</span>][<span class="number">0</span>], line2_pts[<span class="number">0</span>][<span class="number">1</span>], <span class="number">1</span>])</span><br><span class="line">    p4 = np.array([line2_pts[<span class="number">1</span>][<span class="number">0</span>], line2_pts[<span class="number">1</span>][<span class="number">1</span>], <span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算直线（叉乘）</span></span><br><span class="line">    L1 = np.cross(p1, p2)</span><br><span class="line">    L2 = np.cross(p3, p4)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算交点（叉乘）</span></span><br><span class="line">    vp_homo = np.cross(L1, L2)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 归一化</span></span><br><span class="line">    vp = vp_homo[:<span class="number">2</span>] / vp_homo[<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> vp</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="2-3-从消失点到相机参数"><a href="#2-3-从消失点到相机参数" class="headerlink" title="2.3 从消失点到相机参数"></a>2.3 从消失点到相机参数</h3><p><img src="/./image-to-bird-eye-view/figures/09_vanishing_point_to_angles.png" alt="消失点到角度的转换"></p><p><strong>核心思想</strong>：消失点的位置反映了相机的朝向！</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">estimate_camera_params</span>(<span class="params">vp, img_shape, cx=<span class="literal">None</span>, cy=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    从消失点估计相机参数</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        vp: 消失点坐标 [vx, vy]</span></span><br><span class="line"><span class="string">        img_shape: 图像大小 (height, width)</span></span><br><span class="line"><span class="string">        cx, cy: 主点坐标（可选）</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">        focal: 估计的焦距</span></span><br><span class="line"><span class="string">        yaw: yaw角度（度）</span></span><br><span class="line"><span class="string">        pitch: pitch角度（度）</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    h, w = img_shape[:<span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 主点（默认图像中心）</span></span><br><span class="line">    <span class="keyword">if</span> cx <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        cx = w / <span class="number">2</span></span><br><span class="line">    <span class="keyword">if</span> cy <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        cy = h / <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    vx, vy = vp</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算偏移</span></span><br><span class="line">    dx = vx - cx</span><br><span class="line">    dy = vy - cy</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 估计焦距</span></span><br><span class="line">    focal = np.sqrt(w**<span class="number">2</span> + h**<span class="number">2</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算角度</span></span><br><span class="line">    yaw = np.arctan2(dx, focal)</span><br><span class="line">    pitch = np.arctan2(dy, focal)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 转为角度</span></span><br><span class="line">    yaw_deg = np.degrees(yaw)</span><br><span class="line">    pitch_deg = np.degrees(pitch)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> focal, yaw_deg, pitch_deg</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="2-4-几何证明"><a href="#2-4-几何证明" class="headerlink" title="2.4 几何证明"></a>2.4 几何证明</h3><p><img src="/./image-to-bird-eye-view/figures/10_geometric_proof_yaw_pitch_fixed.png" alt="几何证明：yaw和pitch"></p><p><img src="/./image-to-bird-eye-view/figures/12_pitch_proof.png" alt="Pitch角度证明"></p><p><strong>核心公式：</strong></p><p>$$<br>\text{yaw} = \arctan\left(\frac{v_x - c_x}{f}\right)<br>$$</p><p>$$<br>\text{pitch} = \arctan\left(\frac{v_y - c_y}{f}\right)<br>$$</p><hr><h2 id="🔄-第三章：单应性变换"><a href="#🔄-第三章：单应性变换" class="headerlink" title="🔄 第三章：单应性变换"></a>🔄 第三章：单应性变换</h2><h3 id="3-1-什么是单应性？"><a href="#3-1-什么是单应性？" class="headerlink" title="3.1 什么是单应性？"></a>3.1 什么是单应性？</h3><p><strong>生活例子</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">相机视角（透视图）：</span><br><span class="line">     |  |      近处的条纹 - 看起来很宽</span><br><span class="line">     |   |</span><br><span class="line">     |    |    </span><br><span class="line">     |     |   </span><br><span class="line">     |      |  远处的条纹 - 看起来很窄</span><br><span class="line"></span><br><span class="line">鸟瞰视角：</span><br><span class="line">  ||||||||     所有条纹 - 宽度一样</span><br><span class="line">  ||||||||</span><br><span class="line">  ||||||||</span><br></pre></td></tr></tbody></table></figure><p><strong>单应性变换</strong>就是把第一种视角转换成第二种视角的数学方法！</p><p><img src="/./image-to-bird-eye-view/figures/03_homography_comparison.png" alt="单应性对比"></p><hr><h3 id="3-2-单应性矩阵"><a href="#3-2-单应性矩阵" class="headerlink" title="3.2 单应性矩阵"></a>3.2 单应性矩阵</h3><p><strong>数学定义：</strong></p><p>$$<br>\begin{bmatrix} x’ \ y’ \ 1 \end{bmatrix} = H \begin{bmatrix} x \ y \ 1 \end{bmatrix}<br>$$</p><p>其中 <strong>H</strong> 是一个 <strong>3×3 的矩阵</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">     ┌                    ┐</span><br><span class="line">     │  h11   h12   h13  │</span><br><span class="line">H =  │  h21   h22   h23  │</span><br><span class="line">     │  h31   h32   h33  │</span><br><span class="line">     └                    ┘</span><br></pre></td></tr></tbody></table></figure><p><strong>特点</strong>：</p><ul><li>9个数字，但只有8个自由度</li><li>可以表示旋转、缩放、平移、透视变换</li></ul><hr><h3 id="3-3-DLT算法求解H矩阵"><a href="#3-3-DLT算法求解H矩阵" class="headerlink" title="3.3 DLT算法求解H矩阵"></a>3.3 DLT算法求解H矩阵</h3><p><strong>核心思想</strong>：用4对点构建方程组，用SVD求解。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">normalize_points</span>(<span class="params">points</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    归一化点集（提高数值稳定性）</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="comment"># 计算中心点</span></span><br><span class="line">    centroid = np.mean(points, axis=<span class="number">0</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 平移到原点</span></span><br><span class="line">    centered = points - centroid</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算平均距离</span></span><br><span class="line">    avg_dist = np.mean(np.sqrt(np.<span class="built_in">sum</span>(centered**<span class="number">2</span>, axis=<span class="number">1</span>)))</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 缩放因子</span></span><br><span class="line">    scale = np.sqrt(<span class="number">2</span>) / (avg_dist + <span class="number">1e-8</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建归一化矩阵</span></span><br><span class="line">    T = np.array([</span><br><span class="line">        [scale, <span class="number">0</span>, -scale * centroid[<span class="number">0</span>]],</span><br><span class="line">        [<span class="number">0</span>, scale, -scale * centroid[<span class="number">1</span>]],</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>]</span><br><span class="line">    ])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> T</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">compute_homography_dlt</span>(<span class="params">src_pts, dst_pts</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    使用DLT算法计算单应性矩阵</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        src_pts: 源平面点 (N×2)</span></span><br><span class="line"><span class="string">        dst_pts: 目标平面点 (N×2)</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    返回：</span></span><br><span class="line"><span class="string">        H: 3×3 单应性矩阵</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">assert</span> <span class="built_in">len</span>(src_pts) &gt;= <span class="number">4</span>, <span class="string">"至少需要4个点！"</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 归一化（提高数值稳定性）</span></span><br><span class="line">    T_src = normalize_points(src_pts)</span><br><span class="line">    T_dst = normalize_points(dst_pts)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 归一化点</span></span><br><span class="line">    src_homo = np.column_stack([src_pts, np.ones(<span class="built_in">len</span>(src_pts))])</span><br><span class="line">    dst_homo = np.column_stack([dst_pts, np.ones(<span class="built_in">len</span>(dst_pts))])</span><br><span class="line">    </span><br><span class="line">    src_norm = (T_src @ src_homo.T).T</span><br><span class="line">    dst_norm = (T_dst @ dst_homo.T).T</span><br><span class="line">    </span><br><span class="line">    src_norm_2d = src_norm[:, :<span class="number">2</span>] / src_norm[:, <span class="number">2</span>:<span class="number">3</span>]</span><br><span class="line">    dst_norm_2d = dst_norm[:, :<span class="number">2</span>] / dst_norm[:, <span class="number">2</span>:<span class="number">3</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建矩阵A</span></span><br><span class="line">    A = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(src_norm_2d)):</span><br><span class="line">        x1, y1 = src_norm_2d[i]</span><br><span class="line">        x2, y2 = dst_norm_2d[i]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 两个约束方程</span></span><br><span class="line">        A.append([</span><br><span class="line">            -x1, -y1, -<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, x2*x1, x2*y1, x2</span><br><span class="line">        ])</span><br><span class="line">        A.append([</span><br><span class="line">            <span class="number">0</span>, <span class="number">0</span>, <span class="number">0</span>, -x1, -y1, -<span class="number">1</span>, y2*x1, y2*y1, y2</span><br><span class="line">        ])</span><br><span class="line">    </span><br><span class="line">    A = np.array(A)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># SVD分解</span></span><br><span class="line">    U, S, Vt = np.linalg.svd(A)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 最小奇异值对应的向量</span></span><br><span class="line">    h = Vt[-<span class="number">1</span>, :]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 重构H矩阵</span></span><br><span class="line">    H_norm = h.reshape(<span class="number">3</span>, <span class="number">3</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反归一化</span></span><br><span class="line">    H = np.linalg.inv(T_dst) @ H_norm @ T_src</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 归一化（使h33=1）</span></span><br><span class="line">    H = H / H[<span class="number">2</span>, <span class="number">2</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> H</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="3-4-优化过程"><a href="#3-4-优化过程" class="headerlink" title="3.4 优化过程"></a>3.4 优化过程</h3><p><img src="/./image-to-bird-eye-view/figures/05_optimization_process.png" alt="优化过程示意图"></p><p><strong>步骤解析：</strong></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">infographic sequence-steps-simple</span><br><span class="line">data</span><br><span class="line">  title DLT算法流程</span><br><span class="line">  items</span><br><span class="line">    - label 归一化点</span><br><span class="line">      desc 提高数值稳定性</span><br><span class="line">    - label 构建方程组</span><br><span class="line">      desc Ah = 0</span><br><span class="line">    - label SVD分解</span><br><span class="line">      desc 求最小奇异值</span><br><span class="line">    - label 提取H矩阵</span><br><span class="line">      desc 重构3×3矩阵</span><br><span class="line">    - label 反归一化</span><br><span class="line">      desc 恢复原始尺度</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="💻-第四章：完整代码实现"><a href="#💻-第四章：完整代码实现" class="headerlink" title="💻 第四章：完整代码实现"></a>💻 第四章：完整代码实现</h2><h3 id="4-1-环境搭建"><a href="#4-1-环境搭建" class="headerlink" title="4.1 环境搭建"></a>4.1 环境搭建</h3><p><strong>Python环境要求：</strong></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Python版本</span></span><br><span class="line">Python 3.7+</span><br><span class="line"></span><br><span class="line"><span class="comment"># 核心库</span></span><br><span class="line">numpy&gt;=1.19.0</span><br><span class="line">opencv-python&gt;=4.5.0</span><br><span class="line">matplotlib&gt;=3.3.0</span><br></pre></td></tr></tbody></table></figure><p><strong>安装步骤：</strong></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 创建虚拟环境（推荐）</span></span><br><span class="line">python -m venv cv_env</span><br><span class="line"></span><br><span class="line"><span class="comment"># 激活虚拟环境</span></span><br><span class="line"><span class="comment"># Mac/Linux:</span></span><br><span class="line"><span class="built_in">source</span> cv_env/bin/activate</span><br><span class="line"><span class="comment"># Windows:</span></span><br><span class="line">cv_env\Scripts\activate</span><br><span class="line"></span><br><span class="line"><span class="comment"># 安装依赖</span></span><br><span class="line">pip install numpy opencv-python matplotlib</span><br><span class="line"></span><br><span class="line"><span class="comment"># 验证安装</span></span><br><span class="line">python -c <span class="string">"import cv2; print(cv2.__version__)"</span></span><br></pre></td></tr></tbody></table></figure><hr><h3 id="4-2-项目结构"><a href="#4-2-项目结构" class="headerlink" title="4.2 项目结构"></a>4.2 项目结构</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">bird_eye_view_project/</span><br><span class="line">├── input/                  # 输入图像</span><br><span class="line">│   └── dashcam.jpg</span><br><span class="line">├── output/                 # 输出结果</span><br><span class="line">│   ├── marked.jpg</span><br><span class="line">│   ├── bird_view.jpg</span><br><span class="line">│   └── result.jpg</span><br><span class="line">├── main.py                 # 主程序</span><br><span class="line">└── requirements.txt        # 依赖列表</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="4-3-交互式鸟瞰图生成器"><a href="#4-3-交互式鸟瞰图生成器" class="headerlink" title="4.3 交互式鸟瞰图生成器"></a>4.3 交互式鸟瞰图生成器</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">从图像到俯视图：完整实现</span></span><br><span class="line"><span class="string">功能：交互式选点 → 计算单应性矩阵 → 生成鸟瞰图</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> pathlib <span class="keyword">import</span> Path</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">InteractiveBirdEyeView</span>:</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    交互式鸟瞰图生成器</span></span><br><span class="line"><span class="string">    </span></span><br><span class="line"><span class="string">    使用方法：</span></span><br><span class="line"><span class="string">    1. 点击图像选择4个点</span></span><br><span class="line"><span class="string">    2. 自动计算单应性矩阵</span></span><br><span class="line"><span class="string">    3. 生成鸟瞰图</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, img</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        初始化</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        参数：</span></span><br><span class="line"><span class="string">            img: 输入图像（numpy数组）</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        self.img = img.copy()</span><br><span class="line">        self.display_img = img.copy()</span><br><span class="line">        self.points = []</span><br><span class="line">        self.max_points = <span class="number">4</span></span><br><span class="line">        self.window_name = <span class="string">'选择4个点（按顺序：左下→右下→右上→左上）'</span></span><br><span class="line">        </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">mouse_callback</span>(<span class="params">self, event, x, y, flags, param</span>):</span><br><span class="line">        <span class="string">"""鼠标回调函数"""</span></span><br><span class="line">        <span class="keyword">if</span> event == cv2.EVENT_LBUTTONDOWN:</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(self.points) &lt; self.max_points:</span><br><span class="line">                self.points.append([x, y])</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">f"点 <span class="subst">{<span class="built_in">len</span>(self.points)}</span>: (<span class="subst">{x}</span>, <span class="subst">{y}</span>)"</span>)</span><br><span class="line">                self.draw_points()</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 如果已经选择了4个点，自动处理</span></span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(self.points) == self.max_points:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">"\n✅ 已选择4个点，计算中..."</span>)</span><br><span class="line">                    cv2.waitKey(<span class="number">1000</span>)</span><br><span class="line">                    cv2.destroyWindow(self.window_name)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">draw_points</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""绘制已选择的点"""</span></span><br><span class="line">        self.display_img = self.img.copy()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i, pt <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.points):</span><br><span class="line">            <span class="comment"># 绘制圆点</span></span><br><span class="line">            cv2.circle(self.display_img, <span class="built_in">tuple</span>(pt), <span class="number">8</span>, (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 绘制序号</span></span><br><span class="line">            cv2.putText(</span><br><span class="line">                self.display_img, </span><br><span class="line">                <span class="built_in">str</span>(i+<span class="number">1</span>), </span><br><span class="line">                (pt[<span class="number">0</span>]+<span class="number">15</span>, pt[<span class="number">1</span>]), </span><br><span class="line">                cv2.FONT_HERSHEY_SIMPLEX, </span><br><span class="line">                <span class="number">0.8</span>, </span><br><span class="line">                (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>), </span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 绘制连线</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.points) &gt; <span class="number">1</span>:</span><br><span class="line">            pts = np.array(self.points, np.int32)</span><br><span class="line">            cv2.polylines(</span><br><span class="line">                self.display_img,</span><br><span class="line">                [pts],</span><br><span class="line">                <span class="built_in">len</span>(self.points) == <span class="number">4</span>,</span><br><span class="line">                (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 显示提示信息</span></span><br><span class="line">        info = <span class="string">f"已选择 <span class="subst">{<span class="built_in">len</span>(self.points)}</span>/<span class="subst">{self.max_points}</span> 个点"</span></span><br><span class="line">        cv2.putText(</span><br><span class="line">            self.display_img,</span><br><span class="line">            info,</span><br><span class="line">            (<span class="number">10</span>, <span class="number">30</span>),</span><br><span class="line">            cv2.FONT_HERSHEY_SIMPLEX,</span><br><span class="line">            <span class="number">1</span>,</span><br><span class="line">            (<span class="number">255</span>, <span class="number">255</span>, <span class="number">0</span>),</span><br><span class="line">        )</span><br><span class="line">        cv2.imshow(self.window_name, self.display_img)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">select_points</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        交互式选择点</span></span><br><span class="line"><span class="string">        返回：</span></span><br><span class="line"><span class="string">            points: 4×2 numpy数组，或None（如果取消）</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        cv2.namedWindow(self.window_name, cv2.WINDOW_NORMAL)</span><br><span class="line">        cv2.setMouseCallback(self.window_name, self.mouse_callback)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"\n"</span> + <span class="string">"="</span>*<span class="number">60</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"📌 请在图像上依次点击4个点"</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"   顺序：左下 → 右下 → 右上 → 左上"</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"   提示：选择路面上的矩形区域"</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"   按ESC可以取消"</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"="</span>*<span class="number">60</span> + <span class="string">"\n"</span>)</span><br><span class="line">        self.draw_points()</span><br><span class="line">        <span class="keyword">while</span> <span class="built_in">len</span>(self.points) &lt; self.max_points:</span><br><span class="line">            key = cv2.waitKey(<span class="number">1</span>) &amp; <span class="number">0xFF</span></span><br><span class="line">            <span class="keyword">if</span> key == <span class="number">27</span>:  <span class="comment"># ESC</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">"❌ 已取消"</span>)</span><br><span class="line">                cv2.destroyAllWindows()</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="keyword">return</span> np.array(self.points, dtype=np.float32)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">compute_bird_view</span>(<span class="params">self, bird_w=<span class="number">400</span>, bird_h=<span class="number">600</span>, margin=<span class="number">50</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        计算并显示鸟瞰图</span></span><br><span class="line"><span class="string">        参数：</span></span><br><span class="line"><span class="string">            bird_w: 鸟瞰图宽度（像素）</span></span><br><span class="line"><span class="string">            bird_h: 鸟瞰图高度（像素）</span></span><br><span class="line"><span class="string">            margin: 边距（像素）</span></span><br><span class="line"><span class="string">        返回：</span></span><br><span class="line"><span class="string">            result: 包含原图和鸟瞰图的字典</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 选择点</span></span><br><span class="line">        src_pts = self.select_points()</span><br><span class="line">        <span class="keyword">if</span> src_pts <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"\n🔄 处理中..."</span>)</span><br><span class="line">        <span class="comment"># 定义目标点（俯视图中的矩形）</span></span><br><span class="line">        dst_pts = np.array([</span><br><span class="line">            [margin, bird_h - margin],</span><br><span class="line">            [bird_w - margin, bird_h - margin],</span><br><span class="line">            [bird_w - margin, margin],</span><br><span class="line">            [margin, margin]</span><br><span class="line">        ], dtype=np.float32)</span><br><span class="line">        <span class="comment"># 计算单应性矩阵</span></span><br><span class="line">        H = compute_homography_dlt(src_pts, dst_pts)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"\n📐 单应性矩阵 H:"</span>)</span><br><span class="line">        <span class="built_in">print</span>(H)</span><br><span class="line">        <span class="comment"># 变换图像</span></span><br><span class="line">        bird_view = cv2.warpPerspective(</span><br><span class="line">            self.img,</span><br><span class="line">            H,</span><br><span class="line">            (bird_w, bird_h),</span><br><span class="line">            flags=cv2.INTER_LINEAR</span><br><span class="line">        )</span><br><span class="line">        <span class="comment"># 在原图上绘制选择的区域</span></span><br><span class="line">        marked_img = self.img.copy()</span><br><span class="line">        pts = src_pts.astype(np.int32).reshape((-<span class="number">1</span>, <span class="number">1</span>, <span class="number">2</span>))</span><br><span class="line">        cv2.polylines(marked_img, [pts], <span class="literal">True</span>, (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), <span class="number">3</span>)</span><br><span class="line">        <span class="keyword">for</span> i, pt <span class="keyword">in</span> <span class="built_in">enumerate</span>(src_pts):</span><br><span class="line">            cv2.circle(marked_img, <span class="built_in">tuple</span>(pt.astype(<span class="built_in">int</span>)), <span class="number">10</span>, (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), -<span class="number">1</span>)</span><br><span class="line">            cv2.putText(</span><br><span class="line">                marked_img,</span><br><span class="line">                <span class="built_in">str</span>(i+<span class="number">1</span>),</span><br><span class="line">                <span class="built_in">tuple</span>(pt.astype(<span class="built_in">int</span>)),</span><br><span class="line">                cv2.FONT_HERSHEY_SIMPLEX,</span><br><span class="line">                <span class="number">1</span>,</span><br><span class="line">                (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>),</span><br><span class="line">            )</span><br><span class="line">        <span class="comment"># 返回结果</span></span><br><span class="line">        result = {</span><br><span class="line">            <span class="string">'original'</span>: self.img,</span><br><span class="line">            <span class="string">'marked'</span>: marked_img,</span><br><span class="line">            <span class="string">'bird_view'</span>: bird_view,</span><br><span class="line">            <span class="string">'homography'</span>: H,</span><br><span class="line">            <span class="string">'src_points'</span>: src_pts,</span><br><span class="line">            <span class="string">'dst_points'</span>: dst_pts</span><br><span class="line">        }</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"\n✅ 处理完成！"</span>)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">visualize_results</span>(<span class="params">result, save_path=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    可视化结果</span></span><br><span class="line"><span class="string">    参数：</span></span><br><span class="line"><span class="string">        result: compute_bird_view返回的结果字典</span></span><br><span class="line"><span class="string">        save_path: 保存路径（可选）</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">if</span> result <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"没有结果可显示"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment"># 创建对比图</span></span><br><span class="line">    h1, w1 = result[<span class="string">'marked'</span>].shape[:<span class="number">2</span>]</span><br><span class="line">    h2, w2 = result[<span class="string">'bird_view'</span>].shape[:<span class="number">2</span>]</span><br><span class="line">    <span class="comment"># 调整大小使高度一致</span></span><br><span class="line">    target_h = <span class="number">400</span></span><br><span class="line">    scale1 = target_h / h1</span><br><span class="line">    scale2 = target_h / h2</span><br><span class="line">    img1_resized = cv2.resize(result[<span class="string">'marked'</span>], (<span class="built_in">int</span>(w1*scale1), target_h))</span><br><span class="line">    img2_resized = cv2.resize(result[<span class="string">'bird_view'</span>], (<span class="built_in">int</span>(w2*scale2), target_h))</span><br><span class="line">    <span class="comment"># 水平拼接</span></span><br><span class="line">    combined = np.hstack([img1_resized, img2_resized])</span><br><span class="line">    <span class="comment"># 添加标题</span></span><br><span class="line">    cv2.putText(</span><br><span class="line">        combined,</span><br><span class="line">        <span class="string">'Original (Perspective)'</span>,</span><br><span class="line">        (<span class="number">10</span>, <span class="number">30</span>),</span><br><span class="line">        cv2.FONT_HERSHEY_SIMPLEX,</span><br><span class="line">        <span class="number">1</span>,</span><br><span class="line">        (<span class="number">255</span>, <span class="number">255</span>, <span class="number">0</span>),</span><br><span class="line">    )</span><br><span class="line">    cv2.putText(</span><br><span class="line">        combined,</span><br><span class="line">        <span class="string">'Bird Eye View (Top-down)'</span>,</span><br><span class="line">        (<span class="built_in">int</span>(w1*scale1) + <span class="number">10</span>, <span class="number">30</span>),</span><br><span class="line">        cv2.FONT_HERSHEY_SIMPLEX,</span><br><span class="line">        <span class="number">1</span>,</span><br><span class="line">        (<span class="number">255</span>, <span class="number">255</span>, <span class="number">0</span>),</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 显示</span></span><br><span class="line">    cv2.namedWindow(<span class="string">'Result'</span>, cv2.WINDOW_NORMAL)</span><br><span class="line">    cv2.imshow(<span class="string">'Result'</span>, combined)</span><br><span class="line">    <span class="comment"># 保存</span></span><br><span class="line">    <span class="keyword">if</span> save_path:</span><br><span class="line">        cv2.imwrite(<span class="built_in">str</span>(save_path), combined)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"\n💾 结果已保存到: <span class="subst">{save_path}</span>"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"\n按任意键关闭..."</span>)</span><br><span class="line">    cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">    cv2.destroyAllWindows()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    主函数</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"\n"</span> + <span class="string">"="</span>*<span class="number">60</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"🚗 从图像到俯视图：完整实现"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"="</span>*<span class="number">60</span>)</span><br><span class="line">    <span class="comment"># 1. 读取图像</span></span><br><span class="line">    img_path = <span class="built_in">input</span>(<span class="string">"\n请输入图像路径（或按回车使用默认）: "</span>).strip()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> img_path:</span><br><span class="line">        img_path = <span class="string">"input/dashcam.jpg"</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"\n📂 读取图像: <span class="subst">{img_path}</span>"</span>)</span><br><span class="line">    img = cv2.imread(img_path)</span><br><span class="line">    <span class="keyword">if</span> img <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"❌ 无法读取图像: <span class="subst">{img_path}</span>"</span>)</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"✅ 图像大小: <span class="subst">{img.shape[<span class="number">1</span>]}</span> × <span class="subst">{img.shape[<span class="number">0</span>]}</span>"</span>)</span><br><span class="line">    <span class="comment"># 2. 创建交互式界面</span></span><br><span class="line">    bev = InteractiveBirdEyeView(img)</span><br><span class="line">    <span class="comment"># 3. 计算鸟瞰图</span></span><br><span class="line">    result = bev.compute_bird_view(</span><br><span class="line">        bird_w=<span class="number">400</span>,</span><br><span class="line">        bird_h=<span class="number">600</span>,</span><br><span class="line">        margin=<span class="number">50</span></span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">if</span> result <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment"># 4. 显示结果</span></span><br><span class="line">    visualize_results(result, save_path=<span class="string">"output/result.jpg"</span>)</span><br><span class="line">    <span class="comment"># 5. 保存各个结果</span></span><br><span class="line">    cv2.imwrite(<span class="string">"output/marked.jpg"</span>, result[<span class="string">'marked'</span>])</span><br><span class="line">    cv2.imwrite(<span class="string">"output/bird_view.jpg"</span>, result[<span class="string">'bird_view'</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"\n📁 所有文件已保存到 output/ 目录"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"\n✨ 完成！"</span>)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">"__main__"</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="4-4-运行效果"><a href="#4-4-运行效果" class="headerlink" title="4.4 运行效果"></a>4.4 运行效果</h3><h2 id="运行步骤：效果展示："><a href="#运行步骤：效果展示：" class="headerlink" title="运行步骤：效果展示："></a><strong>运行步骤：</strong><br><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1. 运行程序</span></span><br><span class="line">python main.py</span><br><span class="line"><span class="comment"># 2. 输入图像路径（或使用默认）</span></span><br><span class="line">请输入图像路径（或按回车使用默认）:</span><br><span class="line"><span class="comment"># 3. 在弹出的窗口中点击4个点</span></span><br><span class="line">📌 请在图像上依次点击4个点</span><br><span class="line">   顺序：左下 → 右下 → 右上 → 左上</span><br><span class="line"><span class="comment"># 4. 查看结果</span></span><br><span class="line">✅ 处理完成！</span><br><span class="line">💾 结果已保存到: output/result.jpg</span><br></pre></td></tr></tbody></table></figure><br><strong>效果展示：</strong><br><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">输入图像（斜视角）          输出图像（俯视图）</span><br><span class="line">     ╱╲                        ┌──────┐</span><br><span class="line">    ╱  ╲       →               │      │</span><br><span class="line">   ╱    ╲      变换             │      │</span><br><span class="line">  ╱______╲                      └──────┘</span><br><span class="line"> 梯形效果                        矩形效果</span><br></pre></td></tr></tbody></table></figure></h2><h2 id="🐛-第五章：调试与优化"><a href="#🐛-第五章：调试与优化" class="headerlink" title="🐛 第五章：调试与优化"></a>🐛 第五章：调试与优化</h2><h3 id="5-1-常见问题"><a href="#5-1-常见问题" class="headerlink" title="5.1 常见问题"></a>5.1 常见问题</h3><h4 id="问题1：图像变形严重"><a href="#问题1：图像变形严重" class="headerlink" title="问题1：图像变形严重"></a>问题1：图像变形严重</h4><h2 id="原因：点选择不合理解决方案："><a href="#原因：点选择不合理解决方案：" class="headerlink" title="原因：点选择不合理解决方案："></a><strong>原因</strong>：点选择不合理<br><strong>解决方案</strong>：<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">check_points_distribution</span>(<span class="params">pts</span>):</span><br><span class="line">    <span class="string">"""检查点的分布"""</span></span><br><span class="line">    std_x = np.std(pts[:, <span class="number">0</span>])</span><br><span class="line">    std_y = np.std(pts[:, <span class="number">1</span>])</span><br><span class="line">    <span class="keyword">if</span> std_x &lt; <span class="number">50</span> <span class="keyword">or</span> std_y &lt; <span class="number">50</span>:</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"⚠️  警告：点分布过于集中！"</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"   X方向标准差: <span class="subst">{std_x:<span class="number">.1</span>f}</span>"</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">f"   Y方向标准差: <span class="subst">{std_y:<span class="number">.1</span>f}</span>"</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"   建议：选择更分散的点"</span>)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></tbody></table></figure></h2><h4 id="问题2：鸟瞰图有黑边"><a href="#问题2：鸟瞰图有黑边" class="headerlink" title="问题2：鸟瞰图有黑边"></a>问题2：鸟瞰图有黑边</h4><h2 id="原因：输出图像范围设置不当解决方案："><a href="#原因：输出图像范围设置不当解决方案：" class="headerlink" title="原因：输出图像范围设置不当解决方案："></a><strong>原因</strong>：输出图像范围设置不当<br><strong>解决方案</strong>：<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">compute_output_size</span>(<span class="params">img, H, src_pts</span>):</span><br><span class="line">    <span class="string">"""自动计算输出图像大小"""</span></span><br><span class="line">    h, w = img.shape[:<span class="number">2</span>]</span><br><span class="line">    <span class="comment"># 变换图像的四个角点</span></span><br><span class="line">    corners = np.array([</span><br><span class="line">        [<span class="number">0</span>, <span class="number">0</span>], [w, <span class="number">0</span>], [w, h], [<span class="number">0</span>, h]</span><br><span class="line">    ], dtype=np.float32)</span><br><span class="line">    <span class="comment"># 应用H变换</span></span><br><span class="line">    corners_homo = np.column_stack([corners, np.ones(<span class="number">4</span>)])</span><br><span class="line">    transformed = (H @ corners_homo.T).T</span><br><span class="line">    transformed = transformed[:, :<span class="number">2</span>] / transformed[:, <span class="number">2</span>:<span class="number">3</span>]</span><br><span class="line">    <span class="comment"># 计算边界</span></span><br><span class="line">    min_x = np.<span class="built_in">min</span>(transformed[:, <span class="number">0</span>])</span><br><span class="line">    max_x = np.<span class="built_in">max</span>(transformed[:, <span class="number">0</span>])</span><br><span class="line">    min_y = np.<span class="built_in">min</span>(transformed[:, <span class="number">1</span>])</span><br><span class="line">    max_y = np.<span class="built_in">max</span>(transformed[:, <span class="number">1</span>])</span><br><span class="line">    out_w = <span class="built_in">int</span>(max_x - min_x)</span><br><span class="line">    out_h = <span class="built_in">int</span>(max_y - min_y)</span><br><span class="line">    <span class="keyword">return</span> out_w, out_h</span><br></pre></td></tr></tbody></table></figure></h2><h4 id="问题3：运行速度慢"><a href="#问题3：运行速度慢" class="headerlink" title="问题3：运行速度慢"></a>问题3：运行速度慢</h4><h2 id="解决方案："><a href="#解决方案：" class="headerlink" title="解决方案："></a><strong>解决方案</strong>：<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 使用更快的插值方法</span></span><br><span class="line">result = cv2.warpPerspective(</span><br><span class="line">    img,</span><br><span class="line">    H,</span><br><span class="line">    (out_w, out_h),</span><br><span class="line">    flags=cv2.INTER_LINEAR  <span class="comment"># 双线性插值（快）</span></span><br><span class="line">    <span class="comment"># flags=cv2.INTER_CUBIC  # 双三次插值（慢但更好）</span></span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure></h2><h3 id="5-2-性能优化"><a href="#5-2-性能优化" class="headerlink" title="5.2 性能优化"></a>5.2 性能优化</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">infographic list-grid-badge-card</span><br><span class="line">data</span><br><span class="line">  title 优化技巧</span><br><span class="line">  items</span><br><span class="line">    - label 向量化计算</span><br><span class="line">      desc 避免Python循环</span><br><span class="line">      icon mdi:speedometer</span><br><span class="line">    - label GPU加速</span><br><span class="line">      desc 使用cv2.cuda模块</span><br><span class="line">      icon mdi:chip</span><br><span class="line">    - label 多线程</span><br><span class="line">      desc 并行处理多张图像</span><br><span class="line">      icon mdi:lan</span><br><span class="line">    - label 缓存结果</span><br><span class="line">      desc 避免重复计算</span><br><span class="line">      icon mdi:cached</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="🚀-第六章：进阶应用"><a href="#🚀-第六章：进阶应用" class="headerlink" title="🚀 第六章：进阶应用"></a>🚀 第六章：进阶应用</h2><h3 id="6-1-视频流处理"><a href="#6-1-视频流处理" class="headerlink" title="6.1 视频流处理"></a>6.1 视频流处理</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">process_video</span>(<span class="params">video_path, H</span>):</span><br><span class="line">    <span class="string">"""对视频应用鸟瞰变换"""</span></span><br><span class="line">    cap = cv2.VideoCapture(video_path)</span><br><span class="line">    <span class="comment"># 获取视频参数</span></span><br><span class="line">    fps = <span class="built_in">int</span>(cap.get(cv2.CAP_PROP_FPS))</span><br><span class="line">    w = <span class="built_in">int</span>(cap.get(cv2.CAP_PROP_FRAME_WIDTH))</span><br><span class="line">    h = <span class="built_in">int</span>(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))</span><br><span class="line">    <span class="comment"># 创建输出视频</span></span><br><span class="line">    fourcc = cv2.VideoWriter_fourcc(*<span class="string">'mp4v'</span>)</span><br><span class="line">    out = cv2.VideoWriter(<span class="string">'output.mp4'</span>, fourcc, fps, (<span class="number">400</span>, <span class="number">600</span>))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"处理视频：<span class="subst">{fps}</span> FPS, <span class="subst">{w}</span>x<span class="subst">{h}</span>"</span>)</span><br><span class="line">    frame_count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> cap.isOpened():</span><br><span class="line">        ret, frame = cap.read()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> ret:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="comment"># 应用变换</span></span><br><span class="line">        bird_view = cv2.warpPerspective(frame, H, (<span class="number">400</span>, <span class="number">600</span>))</span><br><span class="line">        <span class="comment"># 写入输出</span></span><br><span class="line">        out.write(bird_view)</span><br><span class="line">        frame_count += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> frame_count % <span class="number">30</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">f"已处理 <span class="subst">{frame_count}</span> 帧"</span>)</span><br><span class="line">    cap.release()</span><br><span class="line">    out.release()</span><br><span class="line">    cv2.destroyAllWindows()</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">f"✅ 完成！共处理 <span class="subst">{frame_count}</span> 帧"</span>)</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="6-2-实时相机标定"><a href="#6-2-实时相机标定" class="headerlink" title="6.2 实时相机标定"></a>6.2 实时相机标定</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">realtime_calibration</span>():</span><br><span class="line">    <span class="string">"""实时相机标定和鸟瞰图生成"""</span></span><br><span class="line">    cap = cv2.VideoCapture(<span class="number">0</span>)</span><br><span class="line">    H = <span class="literal">None</span></span><br><span class="line">    calibrated = <span class="literal">False</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"按 'c' 进入标定模式"</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"按 'q' 退出"</span>)</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        ret, frame = cap.read()</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> ret:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        <span class="keyword">if</span> calibrated <span class="keyword">and</span> H <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="comment"># 应用变换</span></span><br><span class="line">            bird_view = cv2.warpPerspective(frame, H, (<span class="number">400</span>, <span class="number">600</span>))</span><br><span class="line">            <span class="comment"># 并排显示</span></span><br><span class="line">            display = np.hstack([</span><br><span class="line">                cv2.resize(frame, (<span class="number">400</span>, <span class="number">300</span>)),</span><br><span class="line">                cv2.resize(bird_view, (<span class="number">400</span>, <span class="number">300</span>))</span><br><span class="line">            ])</span><br><span class="line">            cv2.imshow(<span class="string">'Camera | Bird View'</span>, display)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            cv2.imshow(<span class="string">'Camera'</span>, frame)</span><br><span class="line">        key = cv2.waitKey(<span class="number">1</span>) &amp; <span class="number">0xFF</span></span><br><span class="line">        <span class="keyword">if</span> key == <span class="built_in">ord</span>(<span class="string">'c'</span>):</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"\n开始标定..."</span>)</span><br><span class="line">            bev = InteractiveBirdEyeView(frame)</span><br><span class="line">            result = bev.compute_bird_view()</span><br><span class="line">            <span class="keyword">if</span> result <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                H = result[<span class="string">'homography'</span>]</span><br><span class="line">                calibrated = <span class="literal">True</span></span><br><span class="line">                <span class="built_in">print</span>(<span class="string">"✅ 标定完成！"</span>)</span><br><span class="line">        <span class="keyword">elif</span> key == <span class="built_in">ord</span>(<span class="string">'q'</span>):</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    cap.release()</span><br><span class="line">    cv2.destroyAllWindows()</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="📊-总结"><a href="#📊-总结" class="headerlink" title="📊 总结"></a>📊 总结</h2><h3 id="学习成果"><a href="#学习成果" class="headerlink" title="学习成果"></a>学习成果</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">infographic list-column-done-list</span><br><span class="line">data</span><br><span class="line">  title 你已经掌握</span><br><span class="line">  items</span><br><span class="line">    - label 理解透视投影原理</span><br><span class="line">      desc 3D到2D的转换</span><br><span class="line">    - label 计算消失点</span><br><span class="line">      desc 从平行线找交点</span><br><span class="line">    - label 估计相机参数</span><br><span class="line">      desc 焦距和角度</span><br><span class="line">    - label 单应性变换</span><br><span class="line">      desc DLT算法</span><br><span class="line">    - label 完整代码实现</span><br><span class="line">      desc Python+OpenCV</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="核心知识点"><a href="#核心知识点" class="headerlink" title="核心知识点"></a>核心知识点</h3><p>| 概念 | 公式/方法 | 应用 |</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">| 透视投影 | `u = K[R\|t]X` | 3D→2D转换 |</span><br><span class="line">| 消失点 | 平行线交点 | 估计相机朝向 |</span><br><span class="line">| 单应性 | `x' = Hx` | 平面变换 |</span><br><span class="line">| DLT算法 | SVD求解 | 计算H矩阵 |</span><br><span class="line"></span><br><span class="line">---</span><br><span class="line"></span><br><span class="line">### 后续学习方向</span><br><span class="line"></span><br><span class="line">```infographic</span><br><span class="line">infographic hierarchy-tree-curved-line-rounded-rect-node</span><br><span class="line">data</span><br><span class="line">  title 进阶路径</span><br><span class="line">  items</span><br><span class="line">    - label 深度学习</span><br><span class="line">      children:</span><br><span class="line">        - label 神经网络检测关键点</span><br><span class="line">        - label 端到端视角转换</span><br><span class="line">    - label 3D重建</span><br><span class="line">      children:</span><br><span class="line">        - label 多视角3D恢复</span><br><span class="line">        - label SLAM技术</span><br><span class="line">    - label 实时系统</span><br><span class="line">      children:</span><br><span class="line">        - label GPU加速</span><br><span class="line">        - label 嵌入式部署</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="🎉-结语"><a href="#🎉-结语" class="headerlink" title="🎉 结语"></a>🎉 结语</h2><p>你已经掌握了从图像到俯视图转换的完整知识和技能！</p><p><strong>记住：</strong></p><ul><li>📚 理论是基础</li><li>💻 代码是实践</li><li>🐛 调试是成长</li><li>🚀 应用是目标</li></ul><p><strong>继续探索，不断进步！</strong> ✨</p><hr><h2 id="📚-参考资料"><a href="#📚-参考资料" class="headerlink" title="📚 参考资料"></a>📚 参考资料</h2><h3 id="学习资源"><a href="#学习资源" class="headerlink" title="学习资源"></a>学习资源</h3><ul><li><a href="https://docs.opencv.org/">OpenCV官方文档</a></li><li><a href="https://www.robots.ox.ac.uk/~vgg/hzbook/">Multiple View Geometry (Hartley &amp; Zisserman)</a></li><li><a href="https://szeliski.org/Book/">计算机视觉基础</a></li></ul><h3 id="相关论文"><a href="#相关论文" class="headerlink" title="相关论文"></a>相关论文</h3><ul><li>Homography Estimation: A Review</li><li>Camera Calibration Methods</li><li>Perspective Transformation in Computer Vision</li></ul><hr><p><strong>版本信息：</strong></p><ul><li>版本：1.0.0</li><li>创建日期：2026-01-19</li><li>编程语言：Python 3.7+</li><li>主要依赖：OpenCV, NumPy</li></ul><hr><p><strong>祝你学习愉快！如有任何问题，欢迎交流讨论。</strong> 🎉</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>如何使用Hexo写博客</title>
      <link href="/2026/01/18/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Hexo%E5%86%99%E5%8D%9A%E5%AE%A2/"/>
      <url>/2026/01/18/%E5%A6%82%E4%BD%95%E4%BD%BF%E7%94%A8Hexo%E5%86%99%E5%8D%9A%E5%AE%A2/</url>
      
        <content type="html"><![CDATA[<h2 id="简介"><a href="#简介" class="headerlink" title="简介"></a>简介</h2><p>这是一篇示例文章，演示如何使用 Hexo 写博客。</p><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><h3 id="1-创建新文章"><a href="#1-创建新文章" class="headerlink" title="1. 创建新文章"></a>1. 创建新文章</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npx hexo new <span class="string">"文章标题"</span></span><br></pre></td></tr></tbody></table></figure><h3 id="2-编辑文章"><a href="#2-编辑文章" class="headerlink" title="2. 编辑文章"></a>2. 编辑文章</h3><p>在 <code>source/_posts/</code> 目录下找到刚创建的 Markdown 文件进行编辑。</p><h3 id="3-本地预览"><a href="#3-本地预览" class="headerlink" title="3. 本地预览"></a>3. 本地预览</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npx hexo server</span><br></pre></td></tr></tbody></table></figure><p>访问 <a href="http://localhost:4000/">http://localhost:4000</a> 查看效果。</p><h3 id="4-部署"><a href="#4-部署" class="headerlink" title="4. 部署"></a>4. 部署</h3><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">npx hexo deploy</span><br></pre></td></tr></tbody></table></figure><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>使用 Hexo 写博客非常简单！</p><span id="more"></span><p>你可以在这里继续写更多内容…</p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>📚 LeetCode 150 - 数组与字符串专题</title>
      <link href="/2026/01/18/leetcode-150-array-string/"/>
      <url>/2026/01/18/leetcode-150-array-string/</url>
      
        <content type="html"><![CDATA[<h1 id="📚-数组与字符串专题-15题"><a href="#📚-数组与字符串专题-15题" class="headerlink" title="📚 数组与字符串专题 (15题)"></a>📚 数组与字符串专题 (15题)</h1><blockquote><p>🎯 <strong>核心技巧</strong>：双指针、前缀和、贪心、模拟</p></blockquote><hr><h2 id="🗺️-知识图谱"><a href="#🗺️-知识图谱" class="headerlink" title="🗺️ 知识图谱"></a>🗺️ 知识图谱</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">                ┌─────────────────┐</span><br><span class="line">                │   数组与字符串    │</span><br><span class="line">                └────────┬────────┘</span><br><span class="line">       ┌─────────────────┼─────────────────┐</span><br><span class="line">       ▼                 ▼                 ▼</span><br><span class="line">┌──────────┐      ┌──────────┐      ┌──────────┐</span><br><span class="line">│  双指针   │      │   贪心    │      │  前缀和   │</span><br><span class="line">└────┬─────┘      └────┬─────┘      └────┬─────┘</span><br><span class="line">     │                 │                 │</span><br><span class="line">┌────┴────┐       ┌────┴────┐       ┌────┴────┐</span><br><span class="line">│合并数组  │       │跳跃游戏  │       │除自身乘积│</span><br><span class="line">│移除元素  │       │买卖股票  │       │         │</span><br><span class="line">│删除重复  │       │分发糖果  │       │         │</span><br><span class="line">└─────────┘       └─────────┘       └─────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="1️⃣-LC-88-合并两个有序数组-🟢"><a href="#1️⃣-LC-88-合并两个有序数组-🟢" class="headerlink" title="1️⃣ LC 88. 合并两个有序数组 🟢"></a>1️⃣ LC 88. 合并两个有序数组 🟢</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>将两个有序数组 <code>nums1</code> 和 <code>nums2</code> 合并到 <code>nums1</code> 中，使其有序。</p><h3 id="🎨-图解思路"><a href="#🎨-图解思路" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">从后往前填充，避免覆盖！</span><br><span class="line"></span><br><span class="line">nums1 = [1, 2, 3, 0, 0, 0]  nums2 = [2, 5, 6]</span><br><span class="line">              ↑                        ↑</span><br><span class="line">              p1                       p2</span><br><span class="line">                                              ↑</span><br><span class="line">                                              p (填充位置)</span><br><span class="line"></span><br><span class="line">Step 1: 比较 3 vs 6 → 填 6</span><br><span class="line">nums1 = [1, 2, 3, 0, 0, 6]</span><br><span class="line"></span><br><span class="line">Step 2: 比较 3 vs 5 → 填 5  </span><br><span class="line">nums1 = [1, 2, 3, 0, 5, 6]</span><br><span class="line"></span><br><span class="line">Step 3: 比较 3 vs 2 → 填 3</span><br><span class="line">nums1 = [1, 2, 3, 3, 5, 6]</span><br><span class="line"></span><br><span class="line">... 最终结果 [1, 2, 2, 3, 5, 6]</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现"><a href="#💻-代码实现" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">merge</span>(<span class="params">nums1, m, nums2, n</span>):</span><br><span class="line">    p1, p2, p = m - <span class="number">1</span>, n - <span class="number">1</span>, m + n - <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> p2 &gt;= <span class="number">0</span>:  <span class="comment"># nums2 还有元素</span></span><br><span class="line">        <span class="keyword">if</span> p1 &gt;= <span class="number">0</span> <span class="keyword">and</span> nums1[p1] &gt; nums2[p2]:</span><br><span class="line">            nums1[p] = nums1[p1]</span><br><span class="line">            p1 -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            nums1[p] = nums2[p2]</span><br><span class="line">            p2 -= <span class="number">1</span></span><br><span class="line">        p -= <span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀"><a href="#🧠-记忆口诀" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“从后往前填，大的先落位”</strong></p></blockquote><hr><h2 id="2️⃣-LC-27-移除元素-🟢"><a href="#2️⃣-LC-27-移除元素-🟢" class="headerlink" title="2️⃣ LC 27. 移除元素 🟢"></a>2️⃣ LC 27. 移除元素 🟢</h2><h3 id="题目描述-1"><a href="#题目描述-1" class="headerlink" title="题目描述"></a>题目描述</h3><p>原地移除数组中等于 <code>val</code> 的元素，返回新长度。</p><h3 id="🎨-图解思路-1"><a href="#🎨-图解思路-1" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">快慢指针：慢指针标记有效位置，快指针遍历</span><br><span class="line"></span><br><span class="line">val = 3</span><br><span class="line">      s</span><br><span class="line">      f</span><br><span class="line">[3, 2, 2, 3, 4]</span><br><span class="line"></span><br><span class="line">f=0: nums[0]=3, 跳过</span><br><span class="line">f=1: nums[1]=2≠3, nums[s]=2, s++</span><br><span class="line">      s</span><br><span class="line">         f</span><br><span class="line">[2, 2, 2, 3, 4]</span><br><span class="line"></span><br><span class="line">f=2: nums[2]=2≠3, nums[s]=2, s++</span><br><span class="line">         s</span><br><span class="line">            f</span><br><span class="line">[2, 2, 2, 3, 4]</span><br><span class="line"></span><br><span class="line">f=3: nums[3]=3, 跳过</span><br><span class="line">f=4: nums[4]=4≠3, nums[s]=4, s++</span><br><span class="line"></span><br><span class="line">最终: [2, 2, 4, _, _], 返回 3</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-1"><a href="#💻-代码实现-1" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">removeElement</span>(<span class="params">nums, val</span>):</span><br><span class="line">    slow = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> fast <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">        <span class="keyword">if</span> nums[fast] != val:</span><br><span class="line">            nums[slow] = nums[fast]</span><br><span class="line">            slow += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> slow</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-1"><a href="#🧠-记忆口诀-1" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“快指针探路，慢指针收货”</strong></p></blockquote><hr><h2 id="3️⃣-LC-26-删除有序数组中的重复项-🟢"><a href="#3️⃣-LC-26-删除有序数组中的重复项-🟢" class="headerlink" title="3️⃣ LC 26. 删除有序数组中的重复项 🟢"></a>3️⃣ LC 26. 删除有序数组中的重复项 🟢</h2><h3 id="🎨-图解思路-2"><a href="#🎨-图解思路-2" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">有序数组，相同元素必相邻！</span><br><span class="line"></span><br><span class="line">      s</span><br><span class="line">      f</span><br><span class="line">[1, 1, 2, 2, 3]</span><br><span class="line"></span><br><span class="line">f=1: nums[1]=nums[0], 跳过</span><br><span class="line">f=2: nums[2]≠nums[1], s++, nums[s]=2</span><br><span class="line">         s</span><br><span class="line">            f</span><br><span class="line">[1, 2, 2, 2, 3]</span><br><span class="line"></span><br><span class="line">f=4: nums[4]≠nums[3], s++, nums[s]=3</span><br><span class="line">            s</span><br><span class="line">[1, 2, 3, _, _]</span><br><span class="line"></span><br><span class="line">返回 s+1 = 3</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-2"><a href="#💻-代码实现-2" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">removeDuplicates</span>(<span class="params">nums</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> nums:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    slow = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> fast <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">        <span class="keyword">if</span> nums[fast] != nums[slow]:</span><br><span class="line">            slow += <span class="number">1</span></span><br><span class="line">            nums[slow] = nums[fast]</span><br><span class="line">    <span class="keyword">return</span> slow + <span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><hr><h2 id="4️⃣-LC-80-删除有序数组中的重复项-II-🟡"><a href="#4️⃣-LC-80-删除有序数组中的重复项-II-🟡" class="headerlink" title="4️⃣ LC 80. 删除有序数组中的重复项 II 🟡"></a>4️⃣ LC 80. 删除有序数组中的重复项 II 🟡</h2><h3 id="题目描述-2"><a href="#题目描述-2" class="headerlink" title="题目描述"></a>题目描述</h3><p>每个元素最多出现 <strong>两次</strong>。</p><h3 id="🎨-通用模板"><a href="#🎨-通用模板" class="headerlink" title="🎨 通用模板"></a>🎨 通用模板</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">removeDuplicates</span>(<span class="params">nums, k=<span class="number">2</span></span>):</span><br><span class="line">    <span class="string">"""允许每个元素最多出现 k 次"""</span></span><br><span class="line">    slow = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">        <span class="keyword">if</span> slow &lt; k <span class="keyword">or</span> num != nums[slow - k]:</span><br><span class="line">            nums[slow] = num</span><br><span class="line">            slow += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> slow</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-2"><a href="#🧠-记忆口诀-2" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“检查 k 位之前，不同才能进”</strong></p></blockquote><hr><h2 id="5️⃣-LC-169-多数元素-🟢"><a href="#5️⃣-LC-169-多数元素-🟢" class="headerlink" title="5️⃣ LC 169. 多数元素 🟢"></a>5️⃣ LC 169. 多数元素 🟢</h2><h3 id="题目描述-3"><a href="#题目描述-3" class="headerlink" title="题目描述"></a>题目描述</h3><p>找出出现次数超过 <code>n/2</code> 的元素。</p><h3 id="🎨-Boyer-Moore-投票算法"><a href="#🎨-Boyer-Moore-投票算法" class="headerlink" title="🎨 Boyer-Moore 投票算法"></a>🎨 Boyer-Moore 投票算法</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">把多数元素看作 +1，其他元素看作 -1</span><br><span class="line">最终 +1 一定比 -1 多！</span><br><span class="line"></span><br><span class="line">nums = [2, 2, 1, 1, 1, 2, 2]</span><br><span class="line"></span><br><span class="line">candidate=2, count=1  → [2]</span><br><span class="line">candidate=2, count=2  → [2,2]</span><br><span class="line">candidate=2, count=1  → 遇到1，抵消</span><br><span class="line">candidate=2, count=0  → 遇到1，抵消</span><br><span class="line">candidate=1, count=1  → count=0时换人</span><br><span class="line">candidate=1, count=0  → 遇到2，抵消</span><br><span class="line">candidate=2, count=1  → count=0时换人</span><br><span class="line"></span><br><span class="line">最终 candidate = 2 ✓</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-3"><a href="#💻-代码实现-3" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">majorityElement</span>(<span class="params">nums</span>):</span><br><span class="line">    candidate, count = <span class="literal">None</span>, <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">        <span class="keyword">if</span> count == <span class="number">0</span>:</span><br><span class="line">            candidate = num</span><br><span class="line">        count += <span class="number">1</span> <span class="keyword">if</span> num == candidate <span class="keyword">else</span> -<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> candidate</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-3"><a href="#🧠-记忆口诀-3" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“同加异减，归零换帅”</strong></p></blockquote><hr><h2 id="6️⃣-LC-189-轮转数组-🟡"><a href="#6️⃣-LC-189-轮转数组-🟡" class="headerlink" title="6️⃣ LC 189. 轮转数组 🟡"></a>6️⃣ LC 189. 轮转数组 🟡</h2><h3 id="题目描述-4"><a href="#题目描述-4" class="headerlink" title="题目描述"></a>题目描述</h3><p>将数组向右轮转 <code>k</code> 位。</p><h3 id="🎨-三次翻转法"><a href="#🎨-三次翻转法" class="headerlink" title="🎨 三次翻转法"></a>🎨 三次翻转法</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">nums = [1,2,3,4,5,6,7], k = 3</span><br><span class="line"></span><br><span class="line">Step 1: 整体翻转</span><br><span class="line">[7,6,5,4,3,2,1]</span><br><span class="line"></span><br><span class="line">Step 2: 翻转前 k 个</span><br><span class="line">[5,6,7,4,3,2,1]</span><br><span class="line"></span><br><span class="line">Step 3: 翻转后 n-k 个  </span><br><span class="line">[5,6,7,1,2,3,4] ✓</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-4"><a href="#💻-代码实现-4" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rotate</span>(<span class="params">nums, k</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(nums)</span><br><span class="line">    k %= n  <span class="comment"># 处理 k &gt; n 的情况</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverse</span>(<span class="params">left, right</span>):</span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            nums[left], nums[right] = nums[right], nums[left]</span><br><span class="line">            left, right = left + <span class="number">1</span>, right - <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    reverse(<span class="number">0</span>, n - <span class="number">1</span>)      <span class="comment"># 整体翻转</span></span><br><span class="line">    reverse(<span class="number">0</span>, k - <span class="number">1</span>)      <span class="comment"># 前k个</span></span><br><span class="line">    reverse(k, n - <span class="number">1</span>)      <span class="comment"># 后n-k个</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-4"><a href="#🧠-记忆口诀-4" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“全转前转后，三步到位”</strong></p></blockquote><hr><h2 id="7️⃣-LC-121-买卖股票的最佳时机-🟢"><a href="#7️⃣-LC-121-买卖股票的最佳时机-🟢" class="headerlink" title="7️⃣ LC 121. 买卖股票的最佳时机 🟢"></a>7️⃣ LC 121. 买卖股票的最佳时机 🟢</h2><h3 id="题目描述-5"><a href="#题目描述-5" class="headerlink" title="题目描述"></a>题目描述</h3><p>只能买卖一次，求最大利润。</p><h3 id="🎨-图解思路-3"><a href="#🎨-图解思路-3" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">维护「历史最低价」，计算当天卖出利润</span><br><span class="line"></span><br><span class="line">prices = [7, 1, 5, 3, 6, 4]</span><br><span class="line">          │  │  │  │  │  │</span><br><span class="line">min_price │  1  1  1  1  1</span><br><span class="line">profit    0  0  4  2  5  3</span><br><span class="line">                       ↑</span><br><span class="line">                    最大利润 = 5</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-5"><a href="#💻-代码实现-5" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">maxProfit</span>(<span class="params">prices</span>):</span><br><span class="line">    min_price = <span class="built_in">float</span>(<span class="string">'inf'</span>)</span><br><span class="line">    max_profit = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> price <span class="keyword">in</span> prices:</span><br><span class="line">        min_price = <span class="built_in">min</span>(min_price, price)</span><br><span class="line">        max_profit = <span class="built_in">max</span>(max_profit, price - min_price)</span><br><span class="line">    <span class="keyword">return</span> max_profit</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-5"><a href="#🧠-记忆口诀-5" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“记住最低点，时刻算差价”</strong></p></blockquote><hr><h2 id="8️⃣-LC-55-跳跃游戏-🟡"><a href="#8️⃣-LC-55-跳跃游戏-🟡" class="headerlink" title="8️⃣ LC 55. 跳跃游戏 🟡"></a>8️⃣ LC 55. 跳跃游戏 🟡</h2><h3 id="题目描述-6"><a href="#题目描述-6" class="headerlink" title="题目描述"></a>题目描述</h3><p>判断能否跳到最后一个位置。</p><h3 id="🎨-贪心思路"><a href="#🎨-贪心思路" class="headerlink" title="🎨 贪心思路"></a>🎨 贪心思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">维护能到达的最远位置 max_reach</span><br><span class="line"></span><br><span class="line">nums = [2, 3, 1, 1, 4]</span><br><span class="line">        ↑</span><br><span class="line">i=0: max_reach = max(0, 0+2) = 2</span><br><span class="line">i=1: max_reach = max(2, 1+3) = 4 ≥ 4 ✓ 可达！</span><br><span class="line"></span><br><span class="line">nums = [3, 2, 1, 0, 4]</span><br><span class="line">i=0: max_reach = 3</span><br><span class="line">i=1: max_reach = 3</span><br><span class="line">i=2: max_reach = 3</span><br><span class="line">i=3: max_reach = 3 &lt; 4 </span><br><span class="line">i=4: i &gt; max_reach，无法到达 ✗</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-6"><a href="#💻-代码实现-6" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">canJump</span>(<span class="params">nums</span>):</span><br><span class="line">    max_reach = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">        <span class="keyword">if</span> i &gt; max_reach:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        max_reach = <span class="built_in">max</span>(max_reach, i + nums[i])</span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-6"><a href="#🧠-记忆口诀-6" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“走一步算一步，能到就更新”</strong></p></blockquote><hr><h2 id="9️⃣-LC-45-跳跃游戏-II-🟡"><a href="#9️⃣-LC-45-跳跃游戏-II-🟡" class="headerlink" title="9️⃣ LC 45. 跳跃游戏 II 🟡"></a>9️⃣ LC 45. 跳跃游戏 II 🟡</h2><h3 id="题目描述-7"><a href="#题目描述-7" class="headerlink" title="题目描述"></a>题目描述</h3><p>求到达最后位置的最少跳跃次数。</p><h3 id="🎨-BFS-思想"><a href="#🎨-BFS-思想" class="headerlink" title="🎨 BFS 思想"></a>🎨 BFS 思想</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">把每一跳能到的范围看作一层</span><br><span class="line"></span><br><span class="line">nums = [2, 3, 1, 1, 4]</span><br><span class="line">        ↑</span><br><span class="line">层0: 位置0，能到 [1,2]</span><br><span class="line">层1: 位置1-2，能到 [2,3,4]  → 到达终点！</span><br><span class="line"></span><br><span class="line">跳跃次数 = 2</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-7"><a href="#💻-代码实现-7" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">jump</span>(<span class="params">nums</span>):</span><br><span class="line">    jumps = <span class="number">0</span></span><br><span class="line">    cur_end = <span class="number">0</span>      <span class="comment"># 当前跳跃能到的边界</span></span><br><span class="line">    cur_farthest = <span class="number">0</span> <span class="comment"># 下一跳能到的最远</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums) - <span class="number">1</span>):</span><br><span class="line">        cur_farthest = <span class="built_in">max</span>(cur_farthest, i + nums[i])</span><br><span class="line">        <span class="keyword">if</span> i == cur_end:  <span class="comment"># 到达边界，必须跳</span></span><br><span class="line">            jumps += <span class="number">1</span></span><br><span class="line">            cur_end = cur_farthest</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> jumps</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="🔟-LC-238-除自身以外数组的乘积-🟡"><a href="#🔟-LC-238-除自身以外数组的乘积-🟡" class="headerlink" title="🔟 LC 238. 除自身以外数组的乘积 🟡"></a>🔟 LC 238. 除自身以外数组的乘积 🟡</h2><h3 id="题目描述-8"><a href="#题目描述-8" class="headerlink" title="题目描述"></a>题目描述</h3><p>返回数组，<code>answer[i]</code> 等于 <code>nums</code> 中除 <code>nums[i]</code> 之外其余各元素的乘积。</p><h3 id="🎨-前缀积-×-后缀积"><a href="#🎨-前缀积-×-后缀积" class="headerlink" title="🎨 前缀积 × 后缀积"></a>🎨 前缀积 × 后缀积</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">nums =    [1,  2,  3,  4]</span><br><span class="line">前缀积 =   [1,  1,  2,  6]   (不含当前)</span><br><span class="line">后缀积 =   [24, 12, 4,  1]   (不含当前)</span><br><span class="line">结果 =    [24, 12, 8,  6]   (前缀 × 后缀)</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-O-1-空间"><a href="#💻-代码实现-O-1-空间" class="headerlink" title="💻 代码实现 (O(1) 空间)"></a>💻 代码实现 (O(1) 空间)</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">productExceptSelf</span>(<span class="params">nums</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(nums)</span><br><span class="line">    result = [<span class="number">1</span>] * n</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算前缀积</span></span><br><span class="line">    prefix = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        result[i] = prefix</span><br><span class="line">        prefix *= nums[i]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算后缀积并相乘</span></span><br><span class="line">    suffix = <span class="number">1</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">1</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">        result[i] *= suffix</span><br><span class="line">        suffix *= nums[i]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-7"><a href="#🧠-记忆口诀-7" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“左边乘一遍，右边乘一遍”</strong></p></blockquote><hr><h2 id="📊-本章总结"><a href="#📊-本章总结" class="headerlink" title="📊 本章总结"></a>📊 本章总结</h2><h3 id="核心模式速查表"><a href="#核心模式速查表" class="headerlink" title="核心模式速查表"></a>核心模式速查表</h3><table><thead><tr><th>模式</th><th>适用场景</th><th>典型题目</th></tr></thead><tbody><tr><td><strong>快慢指针</strong></td><td>原地修改数组</td><td>26, 27, 80</td></tr><tr><td><strong>前后指针</strong></td><td>有序数组合并</td><td>88</td></tr><tr><td><strong>贪心</strong></td><td>最优解问题</td><td>55, 45, 121</td></tr><tr><td><strong>前缀和/积</strong></td><td>区间计算</td><td>238</td></tr><tr><td><strong>投票算法</strong></td><td>多数元素</td><td>169</td></tr><tr><td><strong>翻转技巧</strong></td><td>轮转/翻转</td><td>189</td></tr></tbody></table><h3 id="🧠-全章记忆口诀"><a href="#🧠-全章记忆口诀" class="headerlink" title="🧠 全章记忆口诀"></a>🧠 全章记忆口诀</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">合移删删多，轮买买跳跳</span><br><span class="line">H插除加糖，数组十五妙</span><br><span class="line"></span><br><span class="line">合 - 合并数组 (88)</span><br><span class="line">移 - 移除元素 (27)  </span><br><span class="line">删删 - 删除重复 I/II (26, 80)</span><br><span class="line">多 - 多数元素 (169)</span><br><span class="line">轮 - 轮转数组 (189)</span><br><span class="line">买买 - 买卖股票 I/II (121, 122)</span><br><span class="line">跳跳 - 跳跃游戏 I/II (55, 45)</span><br><span class="line">H - H指数 (274)</span><br><span class="line">插 - O(1)插入删除 (380)</span><br><span class="line">除 - 除自身乘积 (238)</span><br><span class="line">加 - 加油站 (134)</span><br><span class="line">糖 - 分发糖果 (135)</span><br></pre></td></tr></tbody></table></figure><hr><blockquote><p>📖 <strong>下一篇</strong>：<a href="/2026/01/18/leetcode-150-two-pointers/">双指针专题</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>🌳 LeetCode 150 - 二叉树专题</title>
      <link href="/2026/01/18/leetcode-150-binary-tree/"/>
      <url>/2026/01/18/leetcode-150-binary-tree/</url>
      
        <content type="html"><![CDATA[<h1 id="🌳-二叉树专题-14题"><a href="#🌳-二叉树专题-14题" class="headerlink" title="🌳 二叉树专题 (14题)"></a>🌳 二叉树专题 (14题)</h1><blockquote><p>🎯 <strong>核心思想</strong>：递归思维 + 分解问题</p></blockquote><hr><h2 id="🗺️-二叉树的思维模式"><a href="#🗺️-二叉树的思维模式" class="headerlink" title="🗺️ 二叉树的思维模式"></a>🗺️ 二叉树的思维模式</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                 二叉树的两种思维模式                         │</span><br><span class="line">├─────────────────────────────────────────────────────────────┤</span><br><span class="line">│                                                             │</span><br><span class="line">│  模式一：遍历思维                                            │</span><br><span class="line">│  ────────────────                                           │</span><br><span class="line">│  用一个 traverse 函数遍历整棵树                              │</span><br><span class="line">│  在遍历过程中更新外部变量                                    │</span><br><span class="line">│                                                             │</span><br><span class="line">│  模式二：分解问题思维                                        │</span><br><span class="line">│  ──────────────────                                         │</span><br><span class="line">│  将问题分解为子问题                                          │</span><br><span class="line">│  通过子问题的答案推导出原问题的答案                          │</span><br><span class="line">│                                                             │</span><br><span class="line">│            1                                                │</span><br><span class="line">│           / \                                               │</span><br><span class="line">│          2   3        问题(根) = f(问题(左), 问题(右))       │</span><br><span class="line">│         / \                                                 │</span><br><span class="line">│        4   5                                                │</span><br><span class="line">│                                                             │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="🔧-二叉树遍历模板"><a href="#🔧-二叉树遍历模板" class="headerlink" title="🔧 二叉树遍历模板"></a>🔧 二叉树遍历模板</h2><h3 id="前序遍历（根-左-右）"><a href="#前序遍历（根-左-右）" class="headerlink" title="前序遍历（根-左-右）"></a>前序遍历（根-左-右）</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">preorder</span>(<span class="params">root</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="built_in">print</span>(root.val)      <span class="comment"># 先处理根</span></span><br><span class="line">    preorder(root.left)  <span class="comment"># 再左子树</span></span><br><span class="line">    preorder(root.right) <span class="comment"># 后右子树</span></span><br></pre></td></tr></tbody></table></figure><h3 id="中序遍历（左-根-右）"><a href="#中序遍历（左-根-右）" class="headerlink" title="中序遍历（左-根-右）"></a>中序遍历（左-根-右）</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">inorder</span>(<span class="params">root</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    inorder(root.left)   <span class="comment"># 先左子树</span></span><br><span class="line">    <span class="built_in">print</span>(root.val)      <span class="comment"># 再处理根</span></span><br><span class="line">    inorder(root.right)  <span class="comment"># 后右子树</span></span><br></pre></td></tr></tbody></table></figure><h3 id="后序遍历（左-右-根）"><a href="#后序遍历（左-右-根）" class="headerlink" title="后序遍历（左-右-根）"></a>后序遍历（左-右-根）</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">postorder</span>(<span class="params">root</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    postorder(root.left)  <span class="comment"># 先左子树</span></span><br><span class="line">    postorder(root.right) <span class="comment"># 再右子树</span></span><br><span class="line">    <span class="built_in">print</span>(root.val)       <span class="comment"># 后处理根</span></span><br></pre></td></tr></tbody></table></figure><h3 id="层序遍历（BFS）"><a href="#层序遍历（BFS）" class="headerlink" title="层序遍历（BFS）"></a>层序遍历（BFS）</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">levelorder</span>(<span class="params">root</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line">    </span><br><span class="line">    queue = deque([root])</span><br><span class="line">    result = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> queue:</span><br><span class="line">        level = []</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(queue)):</span><br><span class="line">            node = queue.popleft()</span><br><span class="line">            level.append(node.val)</span><br><span class="line">            <span class="keyword">if</span> node.left:</span><br><span class="line">                queue.append(node.left)</span><br><span class="line">            <span class="keyword">if</span> node.right:</span><br><span class="line">                queue.append(node.right)</span><br><span class="line">        result.append(level)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="1️⃣-LC-104-二叉树的最大深度-🟢"><a href="#1️⃣-LC-104-二叉树的最大深度-🟢" class="headerlink" title="1️⃣ LC 104. 二叉树的最大深度 🟢"></a>1️⃣ LC 104. 二叉树的最大深度 🟢</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>返回二叉树的最大深度。</p><h3 id="🎨-图解思路"><a href="#🎨-图解思路" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">```python</span><br><span class="line">   / \</span><br><span class="line">  9  20</span><br><span class="line">    /  \</span><br><span class="line">   15   7</span><br><span class="line">分解思维:</span><br><span class="line">maxDepth(3) = 1 + max(maxDepth(9), maxDepth(20))</span><br><span class="line">            = 1 + max(1, 2)</span><br><span class="line">            = 3</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现"><a href="#💻-代码实现" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">maxDepth</span>(<span class="params">root</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    left_depth = maxDepth(root.left)</span><br><span class="line">    right_depth = maxDepth(root.right)</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> + <span class="built_in">max</span>(left_depth, right_depth)</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀"><a href="#🧠-记忆口诀" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“深度 = 1 + max(左深度, 右深度)”</strong></p></blockquote><hr><h2 id="2️⃣-LC-100-相同的树-🟢"><a href="#2️⃣-LC-100-相同的树-🟢" class="headerlink" title="2️⃣ LC 100. 相同的树 🟢"></a>2️⃣ LC 100. 相同的树 🟢</h2><h3 id="题目描述-1"><a href="#题目描述-1" class="headerlink" title="题目描述"></a>题目描述</h3><h2 id="判断两棵树是否相同。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“根同左同右同，才是真的同”"><a href="#判断两棵树是否相同。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“根同左同右同，才是真的同”" class="headerlink" title="判断两棵树是否相同。### 🎨 图解思路### 💻 代码实现### 🧠 记忆口诀> “根同左同右同，才是真的同”"></a>判断两棵树是否相同。<br>### 🎨 图解思路<br><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">   p       q</span><br><span class="line">   1       1</span><br><span class="line">  / \     / \</span><br><span class="line"> 2   3   2   3</span><br><span class="line">相同的条件:</span><br><span class="line">1. 根节点值相同</span><br><span class="line">2. 左子树相同</span><br><span class="line">3. 右子树相同</span><br></pre></td></tr></tbody></table></figure><br>### 💻 代码实现<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">isSameTree</span>(<span class="params">p, q</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> p <span class="keyword">and</span> <span class="keyword">not</span> q:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> p <span class="keyword">or</span> <span class="keyword">not</span> q:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">return</span> (p.val == q.val <span class="keyword">and</span></span><br><span class="line">            isSameTree(p.left, q.left) <span class="keyword">and</span></span><br><span class="line">            isSameTree(p.right, q.right))</span><br></pre></td></tr></tbody></table></figure><br>### 🧠 记忆口诀<br>&gt; <strong>“根同左同右同，才是真的同”</strong></h2><h2 id="3️⃣-LC-226-翻转二叉树-🟢"><a href="#3️⃣-LC-226-翻转二叉树-🟢" class="headerlink" title="3️⃣ LC 226. 翻转二叉树 🟢"></a>3️⃣ LC 226. 翻转二叉树 🟢</h2><h3 id="题目描述-2"><a href="#题目描述-2" class="headerlink" title="题目描述"></a>题目描述</h3><h2 id="翻转二叉树（镜像）。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“先交换，再递归”"><a href="#翻转二叉树（镜像）。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“先交换，再递归”" class="headerlink" title="翻转二叉树（镜像）。### 🎨 图解思路### 💻 代码实现### 🧠 记忆口诀> “先交换，再递归”"></a>翻转二叉树（镜像）。<br>### 🎨 图解思路<br><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">     4              4</span><br><span class="line">   /   \          /   \</span><br><span class="line">  2     7   =&gt;   7     2</span><br><span class="line"> / \   / \      / \   / \</span><br><span class="line">1   3 6   9    9   6 3   1</span><br><span class="line">交换每个节点的左右子树</span><br></pre></td></tr></tbody></table></figure><br>### 💻 代码实现<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">invertTree</span>(<span class="params">root</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="comment"># 交换左右子树</span></span><br><span class="line">    root.left, root.right = root.right, root.left</span><br><span class="line">    <span class="comment"># 递归翻转子树</span></span><br><span class="line">    invertTree(root.left)</span><br><span class="line">    invertTree(root.right)</span><br><span class="line">    <span class="keyword">return</span> root</span><br></pre></td></tr></tbody></table></figure><br>### 🧠 记忆口诀<br>&gt; <strong>“先交换，再递归”</strong></h2><h2 id="4️⃣-LC-101-对称二叉树-🟢"><a href="#4️⃣-LC-101-对称二叉树-🟢" class="headerlink" title="4️⃣ LC 101. 对称二叉树 🟢"></a>4️⃣ LC 101. 对称二叉树 🟢</h2><h3 id="题目描述-3"><a href="#题目描述-3" class="headerlink" title="题目描述"></a>题目描述</h3><h2 id="判断二叉树是否对称。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“外外相等，内内相等”"><a href="#判断二叉树是否对称。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“外外相等，内内相等”" class="headerlink" title="判断二叉树是否对称。### 🎨 图解思路### 💻 代码实现### 🧠 记忆口诀> “外外相等，内内相等”"></a>判断二叉树是否对称。<br>### 🎨 图解思路<br><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">   / \</span><br><span class="line">  2   2</span><br><span class="line"> / \ / \</span><br><span class="line">3  4 4  3</span><br><span class="line">对称条件:</span><br><span class="line">左子树的左 == 右子树的右</span><br><span class="line">左子树的右 == 右子树的左</span><br></pre></td></tr></tbody></table></figure><br>### 💻 代码实现<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">isSymmetric</span>(<span class="params">root</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">check</span>(<span class="params">left, right</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> left <span class="keyword">and</span> <span class="keyword">not</span> right:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> left <span class="keyword">or</span> <span class="keyword">not</span> right:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> (left.val == right.val <span class="keyword">and</span></span><br><span class="line">                check(left.left, right.right) <span class="keyword">and</span></span><br><span class="line">                check(left.right, right.left))</span><br><span class="line">    <span class="keyword">return</span> check(root.left, root.right) <span class="keyword">if</span> root <span class="keyword">else</span> <span class="literal">True</span></span><br></pre></td></tr></tbody></table></figure><br>### 🧠 记忆口诀<br>&gt; <strong>“外外相等，内内相等”</strong></h2><h2 id="5️⃣-LC-105-从前序与中序遍历序列构造二叉树-🟡"><a href="#5️⃣-LC-105-从前序与中序遍历序列构造二叉树-🟡" class="headerlink" title="5️⃣ LC 105. 从前序与中序遍历序列构造二叉树 🟡"></a>5️⃣ LC 105. 从前序与中序遍历序列构造二叉树 🟡</h2><h3 id="题目描述-4"><a href="#题目描述-4" class="headerlink" title="题目描述"></a>题目描述</h3><h2 id="根据前序和中序遍历结果，构建二叉树。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“前序定根，中序分边”"><a href="#根据前序和中序遍历结果，构建二叉树。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“前序定根，中序分边”" class="headerlink" title="根据前序和中序遍历结果，构建二叉树。### 🎨 图解思路### 💻 代码实现### 🧠 记忆口诀> “前序定根，中序分边”"></a>根据前序和中序遍历结果，构建二叉树。<br>### 🎨 图解思路<br><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">preorder = [3, 9, 20, 15, 7]  根-左-右</span><br><span class="line">inorder  = [9, 3, 15, 20, 7]  左-根-右</span><br><span class="line">步骤:</span><br><span class="line">1. preorder[0] = 3 是根节点</span><br><span class="line">2. 在 inorder 中找到 3，左边是左子树，右边是右子树</span><br><span class="line">3. 递归构建</span><br><span class="line">   / \</span><br><span class="line">  9  20</span><br><span class="line">    /  \</span><br><span class="line">   15   7</span><br></pre></td></tr></tbody></table></figure><br>### 💻 代码实现<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">buildTree</span>(<span class="params">preorder: <span class="built_in">list</span>, inorder: <span class="built_in">list</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> preorder:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="comment"># 根节点是前序第一个</span></span><br><span class="line">    root = TreeNode(preorder[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 在中序中找到根节点位置</span></span><br><span class="line">    mid = inorder.index(preorder[<span class="number">0</span>])</span><br><span class="line">    <span class="comment"># 递归构建左右子树</span></span><br><span class="line">    root.left = buildTree(preorder[<span class="number">1</span>:mid+<span class="number">1</span>], inorder[:mid])</span><br><span class="line">    root.right = buildTree(preorder[mid+<span class="number">1</span>:], inorder[mid+<span class="number">1</span>:])</span><br><span class="line">    <span class="keyword">return</span> root</span><br></pre></td></tr></tbody></table></figure><br>### 🧠 记忆口诀<br>&gt; <strong>“前序定根，中序分边”</strong></h2><h2 id="6️⃣-LC-106-从中序与后序遍历序列构造二叉树-🟡"><a href="#6️⃣-LC-106-从中序与后序遍历序列构造二叉树-🟡" class="headerlink" title="6️⃣ LC 106. 从中序与后序遍历序列构造二叉树 🟡"></a>6️⃣ LC 106. 从中序与后序遍历序列构造二叉树 🟡</h2><h3 id="题目描述-5"><a href="#题目描述-5" class="headerlink" title="题目描述"></a>题目描述</h3><h2 id="根据中序和后序遍历结果，构建二叉树。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“后序定根（最后），中序分边”"><a href="#根据中序和后序遍历结果，构建二叉树。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“后序定根（最后），中序分边”" class="headerlink" title="根据中序和后序遍历结果，构建二叉树。### 🎨 图解思路### 💻 代码实现### 🧠 记忆口诀> “后序定根（最后），中序分边”"></a>根据中序和后序遍历结果，构建二叉树。<br>### 🎨 图解思路<br><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">inorder   = [9, 3, 15, 20, 7]  左-根-右</span><br><span class="line">postorder = [9, 15, 7, 20, 3]  左-右-根</span><br><span class="line">后序最后一个是根！</span><br></pre></td></tr></tbody></table></figure><br>### 💻 代码实现<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">buildTree</span>(<span class="params">inorder: <span class="built_in">list</span>, postorder: <span class="built_in">list</span></span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> postorder:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="comment"># 根节点是后序最后一个</span></span><br><span class="line">    root = TreeNode(postorder[-<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># 在中序中找到根节点位置</span></span><br><span class="line">    mid = inorder.index(postorder[-<span class="number">1</span>])</span><br><span class="line">    <span class="comment"># 递归构建左右子树</span></span><br><span class="line">    root.left = buildTree(inorder[:mid], postorder[:mid])</span><br><span class="line">    root.right = buildTree(inorder[mid+<span class="number">1</span>:], postorder[mid:-<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">return</span> root</span><br></pre></td></tr></tbody></table></figure><br>### 🧠 记忆口诀<br>&gt; <strong>“后序定根（最后），中序分边”</strong></h2><h2 id="7️⃣-LC-117-填充每个节点的下一个右侧节点指针-II-🟡"><a href="#7️⃣-LC-117-填充每个节点的下一个右侧节点指针-II-🟡" class="headerlink" title="7️⃣ LC 117. 填充每个节点的下一个右侧节点指针 II 🟡"></a>7️⃣ LC 117. 填充每个节点的下一个右侧节点指针 II 🟡</h2><h3 id="题目描述-6"><a href="#题目描述-6" class="headerlink" title="题目描述"></a>题目描述</h3><h2 id="填充每个节点的-next-指针指向右侧节点。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“层序遍历，前连后”"><a href="#填充每个节点的-next-指针指向右侧节点。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“层序遍历，前连后”" class="headerlink" title="填充每个节点的 next 指针指向右侧节点。### 🎨 图解思路### 💻 代码实现### 🧠 记忆口诀> “层序遍历，前连后”"></a>填充每个节点的 next 指针指向右侧节点。<br>### 🎨 图解思路<br><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">     1 → NULL</span><br><span class="line">   /   \</span><br><span class="line">  2  →  3 → NULL</span><br><span class="line"> / \     \</span><br><span class="line">4→  5  →  7 → NULL</span><br><span class="line">使用层序遍历，连接同层节点</span><br></pre></td></tr></tbody></table></figure><br>### 💻 代码实现<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">connect</span>(<span class="params">root</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    queue = deque([root])</span><br><span class="line">    <span class="keyword">while</span> queue:</span><br><span class="line">        size = <span class="built_in">len</span>(queue)</span><br><span class="line">        prev = <span class="literal">None</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(size):</span><br><span class="line">            node = queue.popleft()</span><br><span class="line">            <span class="keyword">if</span> prev:</span><br><span class="line">                prev.<span class="built_in">next</span> = node</span><br><span class="line">            prev = node</span><br><span class="line">            <span class="keyword">if</span> node.left:</span><br><span class="line">                queue.append(node.left)</span><br><span class="line">            <span class="keyword">if</span> node.right:</span><br><span class="line">                queue.append(node.right)</span><br><span class="line">    <span class="keyword">return</span> root</span><br></pre></td></tr></tbody></table></figure><br>### 🧠 记忆口诀<br>&gt; <strong>“层序遍历，前连后”</strong></h2><h2 id="8️⃣-LC-114-二叉树展开为链表-🟡"><a href="#8️⃣-LC-114-二叉树展开为链表-🟡" class="headerlink" title="8️⃣ LC 114. 二叉树展开为链表 🟡"></a>8️⃣ LC 114. 二叉树展开为链表 🟡</h2><h3 id="题目描述-7"><a href="#题目描述-7" class="headerlink" title="题目描述"></a>题目描述</h3><h2 id="将二叉树展开为单链表（前序顺序）。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“左接右，原右接末尾”"><a href="#将二叉树展开为单链表（前序顺序）。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“左接右，原右接末尾”" class="headerlink" title="将二叉树展开为单链表（前序顺序）。### 🎨 图解思路### 💻 代码实现### 🧠 记忆口诀> “左接右，原右接末尾”"></a>将二叉树展开为单链表（前序顺序）。<br>### 🎨 图解思路<br><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">    1           1</span><br><span class="line">   / \           \</span><br><span class="line">  2   5    =&gt;     2</span><br><span class="line"> / \   \           \</span><br><span class="line">3   4   6           3</span><br><span class="line">                     \</span><br><span class="line">                       \</span><br><span class="line">                         \</span><br></pre></td></tr></tbody></table></figure><br>### 💻 代码实现<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">flatten</span>(<span class="params">root</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    <span class="comment"># 后序遍历：先处理子树，再处理根</span></span><br><span class="line">    flatten(root.left)</span><br><span class="line">    flatten(root.right)</span><br><span class="line">    <span class="comment"># 保存右子树</span></span><br><span class="line">    right = root.right</span><br><span class="line">    <span class="comment"># 左子树移到右边</span></span><br><span class="line">    root.right = root.left</span><br><span class="line">    root.left = <span class="literal">None</span></span><br><span class="line">    <span class="comment"># 找到右子树末端，接上原右子树</span></span><br><span class="line">    <span class="keyword">while</span> root.right:</span><br><span class="line">        root = root.right</span><br><span class="line">    root.right = right</span><br></pre></td></tr></tbody></table></figure><br>### 🧠 记忆口诀<br>&gt; <strong>“左接右，原右接末尾”</strong></h2><h2 id="9️⃣-LC-112-路径总和-🟢"><a href="#9️⃣-LC-112-路径总和-🟢" class="headerlink" title="9️⃣ LC 112. 路径总和 🟢"></a>9️⃣ LC 112. 路径总和 🟢</h2><h3 id="题目描述-8"><a href="#题目描述-8" class="headerlink" title="题目描述"></a>题目描述</h3><h2 id="判断是否存在根到叶子路径，其和等于目标值。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“叶子判相等，非叶递归减”"><a href="#判断是否存在根到叶子路径，其和等于目标值。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“叶子判相等，非叶递归减”" class="headerlink" title="判断是否存在根到叶子路径，其和等于目标值。### 🎨 图解思路### 💻 代码实现### 🧠 记忆口诀> “叶子判相等，非叶递归减”"></a>判断是否存在根到叶子路径，其和等于目标值。<br>### 🎨 图解思路<br><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">     / \</span><br><span class="line">    4   8</span><br><span class="line">   /   / \</span><br><span class="line">  11  13  4</span><br><span class="line"> /  \      \</span><br><span class="line">7    2      1</span><br><span class="line">targetSum = 22</span><br><span class="line">路径: 5 → 4 → 11 → 2 = 22 ✓</span><br></pre></td></tr></tbody></table></figure><br>### 💻 代码实现<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hasPathSum</span>(<span class="params">root, targetSum: <span class="built_in">int</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="comment"># 叶子节点</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root.left <span class="keyword">and</span> <span class="keyword">not</span> root.right:</span><br><span class="line">        <span class="keyword">return</span> root.val == targetSum</span><br><span class="line">    <span class="comment"># 递归检查子树</span></span><br><span class="line">    remaining = targetSum - root.val</span><br><span class="line">    <span class="keyword">return</span> (hasPathSum(root.left, remaining) <span class="keyword">or</span></span><br><span class="line">            hasPathSum(root.right, remaining))</span><br></pre></td></tr></tbody></table></figure><br>### 🧠 记忆口诀<br>&gt; <strong>“叶子判相等，非叶递归减”</strong></h2><h2 id="🔟-LC-129-求根节点到叶节点数字之和-🟡"><a href="#🔟-LC-129-求根节点到叶节点数字之和-🟡" class="headerlink" title="🔟 LC 129. 求根节点到叶节点数字之和 🟡"></a>🔟 LC 129. 求根节点到叶节点数字之和 🟡</h2><h3 id="题目描述-9"><a href="#题目描述-9" class="headerlink" title="题目描述"></a>题目描述</h3><h2 id="每条路径组成一个数字，求所有数字之和。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“进位乘10加当前”"><a href="#每条路径组成一个数字，求所有数字之和。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“进位乘10加当前”" class="headerlink" title="每条路径组成一个数字，求所有数字之和。### 🎨 图解思路### 💻 代码实现### 🧠 记忆口诀> “进位乘10加当前”"></a>每条路径组成一个数字，求所有数字之和。<br>### 🎨 图解思路<br><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">   / \</span><br><span class="line">  2   3</span><br><span class="line">路径: 1→2 = 12</span><br><span class="line">路径: 1→3 = 13</span><br><span class="line">总和: 12 + 13 = 25</span><br></pre></td></tr></tbody></table></figure><br>### 💻 代码实现<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sumNumbers</span>(<span class="params">root</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">node, current_sum</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> node:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        current_sum = current_sum * <span class="number">10</span> + node.val</span><br><span class="line">        <span class="comment"># 叶子节点</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> node.left <span class="keyword">and</span> <span class="keyword">not</span> node.right:</span><br><span class="line">            <span class="keyword">return</span> current_sum</span><br><span class="line">        <span class="keyword">return</span> dfs(node.left, current_sum) + dfs(node.right, current_sum)</span><br><span class="line">    <span class="keyword">return</span> dfs(root, <span class="number">0</span>)</span><br></pre></td></tr></tbody></table></figure><br>### 🧠 记忆口诀<br>&gt; <strong>“进位乘10加当前”</strong></h2><h2 id="1️⃣1️⃣-LC-124-二叉树中的最大路径和-🔴"><a href="#1️⃣1️⃣-LC-124-二叉树中的最大路径和-🔴" class="headerlink" title="1️⃣1️⃣ LC 124. 二叉树中的最大路径和 🔴"></a>1️⃣1️⃣ LC 124. 二叉树中的最大路径和 🔴</h2><h3 id="题目描述-10"><a href="#题目描述-10" class="headerlink" title="题目描述"></a>题目描述</h3><h2 id="找出路径和最大的路径（可以不经过根节点）。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“拐点算全局，贡献选一边”"><a href="#找出路径和最大的路径（可以不经过根节点）。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“拐点算全局，贡献选一边”" class="headerlink" title="找出路径和最大的路径（可以不经过根节点）。### 🎨 图解思路### 💻 代码实现### 🧠 记忆口诀> “拐点算全局，贡献选一边”"></a>找出路径和最大的路径（可以不经过根节点）。<br>### 🎨 图解思路<br><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">   -10</span><br><span class="line">   /  \</span><br><span class="line">  9   20</span><br><span class="line">     /  \</span><br><span class="line">    15   7</span><br><span class="line">最大路径: 15 → 20 → 7 = 42</span><br><span class="line">思路:</span><br><span class="line">每个节点可以：</span><br><span class="line">1. 只贡献自己（作为路径端点）</span><br><span class="line">2. 贡献自己+左子树</span><br><span class="line">3. 贡献自己+右子树</span><br><span class="line">4. 作为拐点（左+自己+右）</span><br></pre></td></tr></tbody></table></figure><br>### 💻 代码实现<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">maxPathSum</span>(<span class="params">root</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    max_sum = <span class="built_in">float</span>(<span class="string">'-inf'</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">max_gain</span>(<span class="params">node</span>):</span><br><span class="line">        <span class="keyword">nonlocal</span> max_sum</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> node:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        <span class="comment"># 左右子树的最大贡献（负数不要）</span></span><br><span class="line">        left_gain = <span class="built_in">max</span>(max_gain(node.left), <span class="number">0</span>)</span><br><span class="line">        right_gain = <span class="built_in">max</span>(max_gain(node.right), <span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 当前节点作为拐点的路径和</span></span><br><span class="line">        path_sum = node.val + left_gain + right_gain</span><br><span class="line">        max_sum = <span class="built_in">max</span>(max_sum, path_sum)</span><br><span class="line">        <span class="comment"># 返回给父节点的贡献（只能选一边）</span></span><br><span class="line">        <span class="keyword">return</span> node.val + <span class="built_in">max</span>(left_gain, right_gain)</span><br><span class="line">    max_gain(root)</span><br><span class="line">    <span class="keyword">return</span> max_sum</span><br></pre></td></tr></tbody></table></figure><br>### 🧠 记忆口诀<br>&gt; <strong>“拐点算全局，贡献选一边”</strong></h2><h2 id="1️⃣2️⃣-LC-173-二叉搜索树迭代器-🟡"><a href="#1️⃣2️⃣-LC-173-二叉搜索树迭代器-🟡" class="headerlink" title="1️⃣2️⃣ LC 173. 二叉搜索树迭代器 🟡"></a>1️⃣2️⃣ LC 173. 二叉搜索树迭代器 🟡</h2><h3 id="题目描述-11"><a href="#题目描述-11" class="headerlink" title="题目描述"></a>题目描述</h3><h2 id="实现二叉搜索树的迭代器。-💻-代码实现-🧠-记忆口诀-gt-“栈存左链，弹出处理右”"><a href="#实现二叉搜索树的迭代器。-💻-代码实现-🧠-记忆口诀-gt-“栈存左链，弹出处理右”" class="headerlink" title="实现二叉搜索树的迭代器。### 💻 代码实现### 🧠 记忆口诀> “栈存左链，弹出处理右”"></a>实现二叉搜索树的迭代器。<br>### 💻 代码实现<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BSTIterator</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, root</span>):</span><br><span class="line">        self.stack = []</span><br><span class="line">        self._leftmost_inorder(root)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_leftmost_inorder</span>(<span class="params">self, node</span>):</span><br><span class="line">        <span class="keyword">while</span> node:</span><br><span class="line">            self.stack.append(node)</span><br><span class="line">            node = node.left</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">next</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        node = self.stack.pop()</span><br><span class="line">        <span class="keyword">if</span> node.right:</span><br><span class="line">            self._leftmost_inorder(node.right)</span><br><span class="line">        <span class="keyword">return</span> node.val</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">hasNext</span>(<span class="params">self</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.stack) &gt; <span class="number">0</span></span><br></pre></td></tr></tbody></table></figure><br>### 🧠 记忆口诀<br>&gt; <strong>“栈存左链，弹出处理右”</strong></h2><h2 id="1️⃣3️⃣-LC-222-完全二叉树的节点个数-🟢"><a href="#1️⃣3️⃣-LC-222-完全二叉树的节点个数-🟢" class="headerlink" title="1️⃣3️⃣ LC 222. 完全二叉树的节点个数 🟢"></a>1️⃣3️⃣ LC 222. 完全二叉树的节点个数 🟢</h2><h3 id="题目描述-12"><a href="#题目描述-12" class="headerlink" title="题目描述"></a>题目描述</h3><h2 id="统计完全二叉树的节点个数。-💻-代码实现-🧠-记忆口诀-gt-“满树用公式，不满递归数”"><a href="#统计完全二叉树的节点个数。-💻-代码实现-🧠-记忆口诀-gt-“满树用公式，不满递归数”" class="headerlink" title="统计完全二叉树的节点个数。### 💻 代码实现### 🧠 记忆口诀> “满树用公式，不满递归数”"></a>统计完全二叉树的节点个数。<br>### 💻 代码实现<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">countNodes</span>(<span class="params">root</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    left_depth = right_depth = <span class="number">0</span></span><br><span class="line">    left, right = root, root</span><br><span class="line">    <span class="keyword">while</span> left:</span><br><span class="line">        left_depth += <span class="number">1</span></span><br><span class="line">        left = left.left</span><br><span class="line">    <span class="keyword">while</span> right:</span><br><span class="line">        right_depth += <span class="number">1</span></span><br><span class="line">        right = right.right</span><br><span class="line">    <span class="comment"># 满二叉树</span></span><br><span class="line">    <span class="keyword">if</span> left_depth == right_depth:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">2</span> ** left_depth - <span class="number">1</span></span><br><span class="line">    <span class="comment"># 递归</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> + countNodes(root.left) + countNodes(root.right)</span><br></pre></td></tr></tbody></table></figure><br>### 🧠 记忆口诀<br>&gt; <strong>“满树用公式，不满递归数”</strong></h2><h2 id="1️⃣4️⃣-LC-236-二叉树的最近公共祖先-🟡"><a href="#1️⃣4️⃣-LC-236-二叉树的最近公共祖先-🟡" class="headerlink" title="1️⃣4️⃣ LC 236. 二叉树的最近公共祖先 🟡"></a>1️⃣4️⃣ LC 236. 二叉树的最近公共祖先 🟡</h2><h3 id="题目描述-13"><a href="#题目描述-13" class="headerlink" title="题目描述"></a>题目描述</h3><h2 id="找两个节点的最近公共祖先（LCA）。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“左右都有返回根，否则返回有的那边”"><a href="#找两个节点的最近公共祖先（LCA）。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“左右都有返回根，否则返回有的那边”" class="headerlink" title="找两个节点的最近公共祖先（LCA）。### 🎨 图解思路### 💻 代码实现### 🧠 记忆口诀> “左右都有返回根，否则返回有的那边”"></a>找两个节点的最近公共祖先（LCA）。<br>### 🎨 图解思路<br><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">       / \</span><br><span class="line">      5   1</span><br><span class="line">     / \ / \</span><br><span class="line">    6  2 0  8</span><br><span class="line">      / \</span><br><span class="line">     7   4</span><br><span class="line">LCA(5, 1) = 3</span><br><span class="line">LCA(5, 4) = 5</span><br></pre></td></tr></tbody></table></figure><br>### 💻 代码实现<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lowestCommonAncestor</span>(<span class="params">root, p, q</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> root <span class="keyword">or</span> root == p <span class="keyword">or</span> root == q:</span><br><span class="line">        <span class="keyword">return</span> root</span><br><span class="line">    left = lowestCommonAncestor(root.left, p, q)</span><br><span class="line">    right = lowestCommonAncestor(root.right, p, q)</span><br><span class="line">    <span class="comment"># p, q 分布在两边</span></span><br><span class="line">    <span class="keyword">if</span> left <span class="keyword">and</span> right:</span><br><span class="line">        <span class="keyword">return</span> root</span><br><span class="line">    <span class="comment"># p, q 在同一边</span></span><br><span class="line">    <span class="keyword">return</span> left <span class="keyword">if</span> left <span class="keyword">else</span> right</span><br></pre></td></tr></tbody></table></figure><br>### 🧠 记忆口诀<br>&gt; <strong>“左右都有返回根，否则返回有的那边”</strong></h2><h2 id="📊-本章总结"><a href="#📊-本章总结" class="headerlink" title="📊 本章总结"></a>📊 本章总结</h2><h3 id="题目速查表"><a href="#题目速查表" class="headerlink" title="题目速查表"></a>题目速查表</h3><p>| 题号 | 题目 | 难度 | 类型 |</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">| 104 | 最大深度 | 🟢 | 深度 |</span><br><span class="line">| 100 | 相同的树 | 🟢 | 比较 |</span><br><span class="line">| 226 | 翻转二叉树 | 🟢 | 变换 |</span><br><span class="line">| 101 | 对称二叉树 | 🟢 | 比较 |</span><br><span class="line">| 105 | 前序+中序构造 | 🟡 | 构造 |</span><br><span class="line">| 106 | 中序+后序构造 | 🟡 | 构造 |</span><br><span class="line">| 117 | 填充next指针 | 🟡 | 层序 |</span><br><span class="line">| 114 | 展开为链表 | 🟡 | 变换 |</span><br><span class="line">| 112 | 路径总和 | 🟢 | 路径 |</span><br><span class="line">| 129 | 数字之和 | 🟡 | 路径 |</span><br><span class="line">| 124 | 最大路径和 | 🔴 | 路径 |</span><br><span class="line">| 173 | BST迭代器 | 🟡 | 迭代 |</span><br><span class="line">| 222 | 完全树节点数 | 🟢 | 计数 |</span><br><span class="line">| 236 | 最近公共祖先 | 🟡 | LCA |</span><br><span class="line"></span><br><span class="line">### 🧠 全章记忆口诀</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>深度相同翻对称<br>前中后序建树型<br>连接展开走路径<br>迭代计数找祖宗</p><p>深度 - 最大深度 (104)<br>相同 - 相同的树 (100)<br>翻 - 翻转二叉树 (226)<br>对称 - 对称二叉树 (101)<br>前中后序 - 从遍历构造 (105, 106)<br>连接 - 填充next指针 (117)<br>展开 - 展开为链表 (114)<br>路径 - 路径总和系列 (112, 129, 124)<br>迭代 - BST迭代器 (173)<br>计数 - 节点个数 (222)<br>祖宗 - 最近公共祖先 (236)</p><pre><code>---&gt; 📖 **下一篇**：[二叉搜索树专题](/2026/01/18/leetcode-150-bst/)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>🔢 LeetCode 150 - 回溯算法专题</title>
      <link href="/2026/01/18/leetcode-150-backtrack/"/>
      <url>/2026/01/18/leetcode-150-backtrack/</url>
      
        <content type="html"><![CDATA[<h1 id="🔄-回溯算法专题-7题"><a href="#🔄-回溯算法专题-7题" class="headerlink" title="🔄 回溯算法专题 (7题)"></a>🔄 回溯算法专题 (7题)</h1><blockquote><p>🎯 <strong>核心思想</strong>：尝试所有可能，走不通就回头</p></blockquote><hr><h2 id="🗺️-回溯算法的本质"><a href="#🗺️-回溯算法的本质" class="headerlink" title="🗺️ 回溯算法的本质"></a>🗺️ 回溯算法的本质</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                   回溯 = 决策树的遍历                        │</span><br><span class="line">├─────────────────────────────────────────────────────────────┤</span><br><span class="line">│                                                             │</span><br><span class="line">│                        []                                   │</span><br><span class="line">│                    /   |   \                                │</span><br><span class="line">│                  [1]  [2]  [3]     ← 第一层决策              │</span><br><span class="line">│                 / \    |                                    │</span><br><span class="line">│             [1,2][1,3][2,3]        ← 第二层决策              │</span><br><span class="line">│               |                                             │</span><br><span class="line">│            [1,2,3]                 ← 第三层决策              │</span><br><span class="line">│                                                             │</span><br><span class="line">│  回溯三要素：                                                │</span><br><span class="line">│  1. 路径：已经做出的选择                                     │</span><br><span class="line">│  2. 选择列表：当前可以做的选择                               │</span><br><span class="line">│  3. 结束条件：到达决策树底层                                 │</span><br><span class="line">│                                                             │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="🔧-回溯算法模板"><a href="#🔧-回溯算法模板" class="headerlink" title="🔧 回溯算法模板"></a>🔧 回溯算法模板</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backtrack</span>(<span class="params">path, choices</span>):</span><br><span class="line">    <span class="comment"># 结束条件</span></span><br><span class="line">    <span class="keyword">if</span> 满足结束条件:</span><br><span class="line">        result.append(path[:])  <span class="comment"># 注意拷贝！</span></span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> choice <span class="keyword">in</span> choices:</span><br><span class="line">        <span class="comment"># 1. 做选择</span></span><br><span class="line">        path.append(choice)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. 递归进入下一层决策</span></span><br><span class="line">        backtrack(path, new_choices)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. 撤销选择（回溯）</span></span><br><span class="line">        path.pop()</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-回溯口诀"><a href="#🧠-回溯口诀" class="headerlink" title="🧠 回溯口诀"></a>🧠 回溯口诀</h3><blockquote><p><strong>“选择、递归、撤销”</strong> —— 回溯三部曲</p></blockquote><hr><h2 id="1️⃣-LC-17-电话号码的字母组合-🟡"><a href="#1️⃣-LC-17-电话号码的字母组合-🟡" class="headerlink" title="1️⃣ LC 17. 电话号码的字母组合 🟡"></a>1️⃣ LC 17. 电话号码的字母组合 🟡</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>给定电话号码，返回所有可能的字母组合。</p><h3 id="🎨-图解思路"><a href="#🎨-图解思路" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">digits = "23"</span><br><span class="line"></span><br><span class="line">2 → "abc"</span><br><span class="line">3 → "def"</span><br><span class="line"></span><br><span class="line">决策树:</span><br><span class="line">           ""</span><br><span class="line">       /   |   \</span><br><span class="line">      a    b    c      ← 选择2对应的字母</span><br><span class="line">     /|\  /|\  /|\</span><br><span class="line">    d e f d e f d e f  ← 选择3对应的字母</span><br><span class="line"></span><br><span class="line">结果: ["ad","ae","af","bd","be","bf","cd","ce","cf"]</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现"><a href="#💻-代码实现" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">letterCombinations</span>(<span class="params">digits: <span class="built_in">str</span></span>) -&gt; <span class="built_in">list</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> digits:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line">    </span><br><span class="line">    phone = {</span><br><span class="line">        <span class="string">'2'</span>: <span class="string">'abc'</span>, <span class="string">'3'</span>: <span class="string">'def'</span>, <span class="string">'4'</span>: <span class="string">'ghi'</span>, <span class="string">'5'</span>: <span class="string">'jkl'</span>,</span><br><span class="line">        <span class="string">'6'</span>: <span class="string">'mno'</span>, <span class="string">'7'</span>: <span class="string">'pqrs'</span>, <span class="string">'8'</span>: <span class="string">'tuv'</span>, <span class="string">'9'</span>: <span class="string">'wxyz'</span></span><br><span class="line">    }</span><br><span class="line">    </span><br><span class="line">    result = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backtrack</span>(<span class="params">index, path</span>):</span><br><span class="line">        <span class="keyword">if</span> index == <span class="built_in">len</span>(digits):</span><br><span class="line">            result.append(<span class="string">''</span>.join(path))</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> char <span class="keyword">in</span> phone[digits[index]]:</span><br><span class="line">            path.append(char)</span><br><span class="line">            backtrack(index + <span class="number">1</span>, path)</span><br><span class="line">            path.pop()</span><br><span class="line">    </span><br><span class="line">    backtrack(<span class="number">0</span>, [])</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀"><a href="#🧠-记忆口诀" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“每个数字选一个字母”</strong></p></blockquote><hr><h2 id="2️⃣-LC-77-组合-🟡"><a href="#2️⃣-LC-77-组合-🟡" class="headerlink" title="2️⃣ LC 77. 组合 🟡"></a>2️⃣ LC 77. 组合 🟡</h2><h3 id="题目描述-1"><a href="#题目描述-1" class="headerlink" title="题目描述"></a>题目描述</h3><p>从 1 到 n 中选择 k 个数的所有组合。</p><h3 id="🎨-图解思路-1"><a href="#🎨-图解思路-1" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">n = 4, k = 2</span><br><span class="line"></span><br><span class="line">决策树（每次只能选比自己大的数，避免重复）:</span><br><span class="line">              []</span><br><span class="line">         /  |  |  \</span><br><span class="line">       [1] [2] [3] [4]</span><br><span class="line">      / | \  |</span><br><span class="line">  [1,2][1,3][1,4] [2,3][2,4] [3,4]</span><br><span class="line"></span><br><span class="line">结果: [[1,2],[1,3],[1,4],[2,3],[2,4],[3,4]]</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-1"><a href="#💻-代码实现-1" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">combine</span>(<span class="params">n: <span class="built_in">int</span>, k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">list</span>:</span><br><span class="line">    result = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backtrack</span>(<span class="params">start, path</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(path) == k:</span><br><span class="line">            result.append(path[:])</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 剪枝：剩余元素不够用了</span></span><br><span class="line">        <span class="keyword">if</span> k - <span class="built_in">len</span>(path) &gt; n - start + <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(start, n + <span class="number">1</span>):</span><br><span class="line">            path.append(i)</span><br><span class="line">            backtrack(i + <span class="number">1</span>, path)</span><br><span class="line">            path.pop()</span><br><span class="line">    </span><br><span class="line">    backtrack(<span class="number">1</span>, [])</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-1"><a href="#🧠-记忆口诀-1" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“从start开始选，选够k个停”</strong></p></blockquote><hr><h2 id="3️⃣-LC-46-全排列-🟡"><a href="#3️⃣-LC-46-全排列-🟡" class="headerlink" title="3️⃣ LC 46. 全排列 🟡"></a>3️⃣ LC 46. 全排列 🟡</h2><h3 id="题目描述-2"><a href="#题目描述-2" class="headerlink" title="题目描述"></a>题目描述</h3><p>返回数组的所有排列。</p><h3 id="🎨-图解思路-2"><a href="#🎨-图解思路-2" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">nums = [1, 2, 3]</span><br><span class="line"></span><br><span class="line">决策树（每个数只能用一次）:</span><br><span class="line">                    []</span><br><span class="line">            /       |        \</span><br><span class="line">          [1]      [2]       [3]</span><br><span class="line">         /   \    /   \     /   \</span><br><span class="line">      [1,2] [1,3] [2,1] [2,3] [3,1] [3,2]</span><br><span class="line">        |     |     |     |     |     |</span><br><span class="line">    [1,2,3][1,3,2][2,1,3][2,3,1][3,1,2][3,2,1]</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-2"><a href="#💻-代码实现-2" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">permute</span>(<span class="params">nums: <span class="built_in">list</span></span>) -&gt; <span class="built_in">list</span>:</span><br><span class="line">    result = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backtrack</span>(<span class="params">path, used</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(path) == <span class="built_in">len</span>(nums):</span><br><span class="line">            result.append(path[:])</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">            <span class="keyword">if</span> used[i]:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">            path.append(nums[i])</span><br><span class="line">            used[i] = <span class="literal">True</span></span><br><span class="line">            </span><br><span class="line">            backtrack(path, used)</span><br><span class="line">            </span><br><span class="line">            path.pop()</span><br><span class="line">            used[i] = <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">    backtrack([], [<span class="literal">False</span>] * <span class="built_in">len</span>(nums))</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-2"><a href="#🧠-记忆口诀-2" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“用过的标记，没用过的都能选”</strong></p></blockquote><hr><h2 id="4️⃣-LC-39-组合总和-🟡"><a href="#4️⃣-LC-39-组合总和-🟡" class="headerlink" title="4️⃣ LC 39. 组合总和 🟡"></a>4️⃣ LC 39. 组合总和 🟡</h2><h3 id="题目描述-3"><a href="#题目描述-3" class="headerlink" title="题目描述"></a>题目描述</h3><p>找出所有和为 target 的组合（数字可以重复使用）。</p><h3 id="🎨-图解思路-3"><a href="#🎨-图解思路-3" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">candidates = [2, 3, 6, 7], target = 7</span><br><span class="line"></span><br><span class="line">决策树:</span><br><span class="line">                    []</span><br><span class="line">         /     |      \      \</span><br><span class="line">       [2]    [3]    [6]    [7] ✓</span><br><span class="line">      / | \    |      |</span><br><span class="line">   [2,2][2,3][2,6] [3,3]  [6,?]</span><br><span class="line">    /|\   |</span><br><span class="line">[2,2,2][2,2,3]✓ [2,3,?]</span><br><span class="line">  |</span><br><span class="line">[2,2,2,?] 超过7，剪枝</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-3"><a href="#💻-代码实现-3" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">combinationSum</span>(<span class="params">candidates: <span class="built_in">list</span>, target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">list</span>:</span><br><span class="line">    result = []</span><br><span class="line">    candidates.sort()  <span class="comment"># 排序便于剪枝</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backtrack</span>(<span class="params">start, path, remaining</span>):</span><br><span class="line">        <span class="keyword">if</span> remaining == <span class="number">0</span>:</span><br><span class="line">            result.append(path[:])</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(start, <span class="built_in">len</span>(candidates)):</span><br><span class="line">            <span class="keyword">if</span> candidates[i] &gt; remaining:</span><br><span class="line">                <span class="keyword">break</span>  <span class="comment"># 剪枝</span></span><br><span class="line">            </span><br><span class="line">            path.append(candidates[i])</span><br><span class="line">            backtrack(i, path, remaining - candidates[i])  <span class="comment"># i不是i+1，可重复</span></span><br><span class="line">            path.pop()</span><br><span class="line">    </span><br><span class="line">    backtrack(<span class="number">0</span>, [], target)</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-3"><a href="#🧠-记忆口诀-3" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“可以重复选，但只能往后选”</strong></p></blockquote><hr><h2 id="5️⃣-LC-52-N-皇后-II-🔴"><a href="#5️⃣-LC-52-N-皇后-II-🔴" class="headerlink" title="5️⃣ LC 52. N 皇后 II 🔴"></a>5️⃣ LC 52. N 皇后 II 🔴</h2><h3 id="题目描述-4"><a href="#题目描述-4" class="headerlink" title="题目描述"></a>题目描述</h3><p>返回 N 皇后问题的解的数量。</p><h3 id="🎨-图解思路-4"><a href="#🎨-图解思路-4" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">n = 4</span><br><span class="line"></span><br><span class="line">一个有效解:</span><br><span class="line">. Q . .</span><br><span class="line">. . . Q</span><br><span class="line">Q . . .</span><br><span class="line">. . Q .</span><br><span class="line"></span><br><span class="line">约束条件：</span><br><span class="line">1. 每行只能放一个皇后</span><br><span class="line">2. 每列只能放一个皇后</span><br><span class="line">3. 每条对角线只能放一个皇后</span><br><span class="line"></span><br><span class="line">对角线编号技巧:</span><br><span class="line">- 主对角线 (\): row - col 相同</span><br><span class="line">- 副对角线 (/): row + col 相同</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-4"><a href="#💻-代码实现-4" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">totalNQueens</span>(<span class="params">n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    cols = <span class="built_in">set</span>()       <span class="comment"># 列冲突</span></span><br><span class="line">    diag1 = <span class="built_in">set</span>()      <span class="comment"># 主对角线 (row - col)</span></span><br><span class="line">    diag2 = <span class="built_in">set</span>()      <span class="comment"># 副对角线 (row + col)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backtrack</span>(<span class="params">row</span>):</span><br><span class="line">        <span class="keyword">nonlocal</span> count</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> row == n:</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> col <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">if</span> col <span class="keyword">in</span> cols <span class="keyword">or</span> (row - col) <span class="keyword">in</span> diag1 <span class="keyword">or</span> (row + col) <span class="keyword">in</span> diag2:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">            cols.add(col)</span><br><span class="line">            diag1.add(row - col)</span><br><span class="line">            diag2.add(row + col)</span><br><span class="line">            </span><br><span class="line">            backtrack(row + <span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            cols.remove(col)</span><br><span class="line">            diag1.remove(row - col)</span><br><span class="line">            diag2.remove(row + col)</span><br><span class="line">    </span><br><span class="line">    backtrack(<span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> count</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-4"><a href="#🧠-记忆口诀-4" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“列和两条对角线，都不能冲突”</strong></p></blockquote><hr><h2 id="6️⃣-LC-22-括号生成-🟡"><a href="#6️⃣-LC-22-括号生成-🟡" class="headerlink" title="6️⃣ LC 22. 括号生成 🟡"></a>6️⃣ LC 22. 括号生成 🟡</h2><h3 id="题目描述-5"><a href="#题目描述-5" class="headerlink" title="题目描述"></a>题目描述</h3><p>生成 n 对有效的括号组合。</p><h3 id="🎨-图解思路-5"><a href="#🎨-图解思路-5" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">n = 2</span><br><span class="line"></span><br><span class="line">决策树（剪枝条件：右括号不能多于左括号）:</span><br><span class="line">                ""</span><br><span class="line">               /</span><br><span class="line">              (</span><br><span class="line">            /   \</span><br><span class="line">          ((    ()</span><br><span class="line">          |    /</span><br><span class="line">         (()  ()(</span><br><span class="line">          |    |</span><br><span class="line">        (()) ()()</span><br><span class="line"></span><br><span class="line">结果: ["(())", "()()"]</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-5"><a href="#💻-代码实现-5" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">generateParenthesis</span>(<span class="params">n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">list</span>:</span><br><span class="line">    result = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backtrack</span>(<span class="params">path, left, right</span>):</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(path) == <span class="number">2</span> * n:</span><br><span class="line">            result.append(<span class="string">''</span>.join(path))</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> left &lt; n:</span><br><span class="line">            path.append(<span class="string">'('</span>)</span><br><span class="line">            backtrack(path, left + <span class="number">1</span>, right)</span><br><span class="line">            path.pop()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> right &lt; left:</span><br><span class="line">            path.append(<span class="string">')'</span>)</span><br><span class="line">            backtrack(path, left, right + <span class="number">1</span>)</span><br><span class="line">            path.pop()</span><br><span class="line">    </span><br><span class="line">    backtrack([], <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-5"><a href="#🧠-记忆口诀-5" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“左括号随时加，右括号不超左”</strong></p></blockquote><hr><h2 id="7️⃣-LC-79-单词搜索-🟡"><a href="#7️⃣-LC-79-单词搜索-🟡" class="headerlink" title="7️⃣ LC 79. 单词搜索 🟡"></a>7️⃣ LC 79. 单词搜索 🟡</h2><h3 id="题目描述-6"><a href="#题目描述-6" class="headerlink" title="题目描述"></a>题目描述</h3><p>在二维网格中搜索单词。</p><h3 id="🎨-图解思路-6"><a href="#🎨-图解思路-6" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">board:</span><br><span class="line">A B C E</span><br><span class="line">S F C S</span><br><span class="line">A D E E</span><br><span class="line"></span><br><span class="line">word = "ABCCED"</span><br><span class="line"></span><br><span class="line">从 A 开始，DFS + 回溯:</span><br><span class="line">A → B → C → C → E → D ✓</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-6"><a href="#💻-代码实现-6" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">exist</span>(<span class="params">board: <span class="built_in">list</span>, word: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">    m, n = <span class="built_in">len</span>(board), <span class="built_in">len</span>(board[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backtrack</span>(<span class="params">i, j, k</span>):</span><br><span class="line">        <span class="keyword">if</span> k == <span class="built_in">len</span>(word):</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> i &lt; <span class="number">0</span> <span class="keyword">or</span> i &gt;= m <span class="keyword">or</span> j &lt; <span class="number">0</span> <span class="keyword">or</span> j &gt;= n:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> board[i][j] != word[k]:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 标记已访问</span></span><br><span class="line">        temp = board[i][j]</span><br><span class="line">        board[i][j] = <span class="string">'#'</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 四个方向搜索</span></span><br><span class="line">        found = (backtrack(i + <span class="number">1</span>, j, k + <span class="number">1</span>) <span class="keyword">or</span></span><br><span class="line">                 backtrack(i - <span class="number">1</span>, j, k + <span class="number">1</span>) <span class="keyword">or</span></span><br><span class="line">                 backtrack(i, j + <span class="number">1</span>, k + <span class="number">1</span>) <span class="keyword">or</span></span><br><span class="line">                 backtrack(i, j - <span class="number">1</span>, k + <span class="number">1</span>))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 恢复</span></span><br><span class="line">        board[i][j] = temp</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> found</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">if</span> backtrack(i, j, <span class="number">0</span>):</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-6"><a href="#🧠-记忆口诀-6" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“DFS四方向，访问要标记”</strong></p></blockquote><hr><h2 id="📊-本章总结"><a href="#📊-本章总结" class="headerlink" title="📊 本章总结"></a>📊 本章总结</h2><h3 id="回溯问题分类"><a href="#回溯问题分类" class="headerlink" title="回溯问题分类"></a>回溯问题分类</h3><table><thead><tr><th>类型</th><th>特点</th><th>典型题目</th></tr></thead><tbody><tr><td>组合问题</td><td>不考虑顺序</td><td>77, 39</td></tr><tr><td>排列问题</td><td>考虑顺序</td><td>46</td></tr><tr><td>子集问题</td><td>所有可能</td><td>78</td></tr><tr><td>搜索问题</td><td>在空间中找路径</td><td>79</td></tr><tr><td>棋盘问题</td><td>放置约束</td><td>52</td></tr><tr><td>括号问题</td><td>合法性约束</td><td>22</td></tr></tbody></table><h3 id="回溯-vs-动态规划"><a href="#回溯-vs-动态规划" class="headerlink" title="回溯 vs 动态规划"></a>回溯 vs 动态规划</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">┌──────────────┬──────────────────────────────┐</span><br><span class="line">│    回溯      │         动态规划             │</span><br><span class="line">├──────────────┼──────────────────────────────┤</span><br><span class="line">│ 找所有解     │ 找最优解/计数                │</span><br><span class="line">│ 暴力穷举     │ 记忆化避免重复               │</span><br><span class="line">│ 时间换空间   │ 空间换时间                   │</span><br><span class="line">└──────────────┴──────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-全章记忆口诀"><a href="#🧠-全章记忆口诀" class="headerlink" title="🧠 全章记忆口诀"></a>🧠 全章记忆口诀</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">电话组合全排列</span><br><span class="line">组合总和皇后解</span><br><span class="line">括号生成单词找</span><br><span class="line">回溯七题全拿下</span><br><span class="line"></span><br><span class="line">电话 - 电话号码的字母组合 (17)</span><br><span class="line">组合 - 组合 (77)</span><br><span class="line">全排列 - 全排列 (46)</span><br><span class="line">组合总和 - 组合总和 (39)</span><br><span class="line">皇后 - N皇后 II (52)</span><br><span class="line">括号 - 括号生成 (22)</span><br><span class="line">单词 - 单词搜索 (79)</span><br></pre></td></tr></tbody></table></figure><hr><blockquote><p>📖 <strong>返回</strong>：<a href="/2026/01/18/leetcode-150-index/">LeetCode 150 题总目录</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>📈 LeetCode 150 - 动态规划专题</title>
      <link href="/2026/01/18/leetcode-150-dp/"/>
      <url>/2026/01/18/leetcode-150-dp/</url>
      
        <content type="html"><![CDATA[<h1 id="📈-动态规划专题-11题"><a href="#📈-动态规划专题-11题" class="headerlink" title="📈 动态规划专题 (11题)"></a>📈 动态规划专题 (11题)</h1><blockquote><p>🎯 <strong>核心思想</strong>：将大问题分解为小问题，记录子问题的解，避免重复计算</p></blockquote><hr><h2 id="🗺️-动态规划解题框架"><a href="#🗺️-动态规划解题框架" class="headerlink" title="🗺️ 动态规划解题框架"></a>🗺️ 动态规划解题框架</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                    DP 解题四步法                             │</span><br><span class="line">├─────────────────────────────────────────────────────────────┤</span><br><span class="line">│                                                             │</span><br><span class="line">│  Step 1: 定义状态                                           │</span><br><span class="line">│          dp[i] 表示什么？dp[i][j] 表示什么？                 │</span><br><span class="line">│                                                             │</span><br><span class="line">│  Step 2: 状态转移方程                                        │</span><br><span class="line">│          dp[i] = f(dp[i-1], dp[i-2], ...)                  │</span><br><span class="line">│                                                             │</span><br><span class="line">│  Step 3: 初始化                                             │</span><br><span class="line">│          边界条件是什么？dp[0], dp[1] 等于多少？             │</span><br><span class="line">│                                                             │</span><br><span class="line">│  Step 4: 遍历顺序                                            │</span><br><span class="line">│          正序还是倒序？先行后列还是先列后行？                 │</span><br><span class="line">│                                                             │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="🔧-DP-代码模板"><a href="#🔧-DP-代码模板" class="headerlink" title="🔧 DP 代码模板"></a>🔧 DP 代码模板</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dp_template</span>(<span class="params">nums</span>):</span><br><span class="line">    n = <span class="built_in">len</span>(nums)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 1: 定义 dp 数组</span></span><br><span class="line">    dp = [<span class="number">0</span>] * n  <span class="comment"># 或 [[0]*m for _ in range(n)]</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 3: 初始化</span></span><br><span class="line">    dp[<span class="number">0</span>] = base_case</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 4: 遍历顺序</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">        <span class="comment"># Step 2: 状态转移</span></span><br><span class="line">        dp[i] = transition(dp[i-<span class="number">1</span>], ...)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dp[n-<span class="number">1</span>]  <span class="comment"># 或其他目标</span></span><br></pre></td></tr></tbody></table></figure><hr><h2 id="1️⃣-LC-70-爬楼梯-🟢"><a href="#1️⃣-LC-70-爬楼梯-🟢" class="headerlink" title="1️⃣ LC 70. 爬楼梯 🟢"></a>1️⃣ LC 70. 爬楼梯 🟢</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>每次可以爬 1 或 2 个台阶，爬到第 n 阶有多少种方法？</p><h3 id="🎨-图解思路"><a href="#🎨-图解思路" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">到达第 n 阶的方法 = 从第 n-1 阶爬 1 步 + 从第 n-2 阶爬 2 步</span><br><span class="line"></span><br><span class="line">        ┌───┐</span><br><span class="line">        │ n │ ← 目标</span><br><span class="line">        └───┘</span><br><span class="line">       ↗     ↖</span><br><span class="line">    ┌───┐   ┌───┐</span><br><span class="line">    │n-1│   │n-2│</span><br><span class="line">    └───┘   └───┘</span><br><span class="line">      ↑       ↑</span><br><span class="line">     1步     2步</span><br><span class="line"></span><br><span class="line">dp[n] = dp[n-1] + dp[n-2]  (斐波那契数列!)</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现"><a href="#💻-代码实现" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">climbStairs</span>(<span class="params">n: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">if</span> n &lt;= <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> n</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 空间优化：只需记录前两个状态</span></span><br><span class="line">    prev, curr = <span class="number">1</span>, <span class="number">2</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">3</span>, n + <span class="number">1</span>):</span><br><span class="line">        prev, curr = curr, prev + curr</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> curr</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀"><a href="#🧠-记忆口诀" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“爬楼梯就是斐波那契”</strong></p></blockquote><hr><h2 id="2️⃣-LC-198-打家劫舍-🟡"><a href="#2️⃣-LC-198-打家劫舍-🟡" class="headerlink" title="2️⃣ LC 198. 打家劫舍 🟡"></a>2️⃣ LC 198. 打家劫舍 🟡</h2><h3 id="题目描述-1"><a href="#题目描述-1" class="headerlink" title="题目描述"></a>题目描述</h3><p>不能偷相邻的房子，求能偷到的最大金额。</p><h3 id="🎨-图解思路-1"><a href="#🎨-图解思路-1" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">nums = [2, 7, 9, 3, 1]</span><br><span class="line"></span><br><span class="line">对于每个房子，两个选择：</span><br><span class="line">1. 偷：dp[i] = dp[i-2] + nums[i]</span><br><span class="line">2. 不偷：dp[i] = dp[i-1]</span><br><span class="line"></span><br><span class="line">dp[i] = max(dp[i-2] + nums[i], dp[i-1])</span><br><span class="line"></span><br><span class="line">i:      0   1   2   3   4</span><br><span class="line">nums:   2   7   9   3   1</span><br><span class="line">dp:     2   7  11  11  12</span><br><span class="line">        ↑   ↑   ↑   ↑   ↑</span><br><span class="line">       偷  偷 偷0+9 不偷 偷2+1</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-1"><a href="#💻-代码实现-1" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rob</span>(<span class="params">nums: <span class="built_in">list</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(nums) == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> nums[<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 空间优化</span></span><br><span class="line">    prev, curr = nums[<span class="number">0</span>], <span class="built_in">max</span>(nums[<span class="number">0</span>], nums[<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="built_in">len</span>(nums)):</span><br><span class="line">        prev, curr = curr, <span class="built_in">max</span>(curr, prev + nums[i])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> curr</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-1"><a href="#🧠-记忆口诀-1" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“偷不偷，取最大”</strong></p></blockquote><hr><h2 id="3️⃣-LC-139-单词拆分-🟡"><a href="#3️⃣-LC-139-单词拆分-🟡" class="headerlink" title="3️⃣ LC 139. 单词拆分 🟡"></a>3️⃣ LC 139. 单词拆分 🟡</h2><h3 id="题目描述-2"><a href="#题目描述-2" class="headerlink" title="题目描述"></a>题目描述</h3><p>判断字符串是否可以被拆分为字典中的单词。</p><h3 id="🎨-图解思路-2"><a href="#🎨-图解思路-2" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">s = "leetcode", wordDict = ["leet", "code"]</span><br><span class="line"></span><br><span class="line">dp[i] 表示 s[0:i] 是否可以被拆分</span><br><span class="line"></span><br><span class="line">dp[0] = True (空字符串)</span><br><span class="line">dp[4] = dp[0] and "leet" in dict → True</span><br><span class="line">dp[8] = dp[4] and "code" in dict → True</span><br><span class="line"></span><br><span class="line">   l  e  e  t  c  o  d  e</span><br><span class="line">   0  1  2  3  4  5  6  7  8</span><br><span class="line">dp T  F  F  F  T  F  F  F  T</span><br><span class="line">               ↑           ↑</span><br><span class="line">            "leet"      "code"</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-2"><a href="#💻-代码实现-2" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">wordBreak</span>(<span class="params">s: <span class="built_in">str</span>, wordDict: <span class="built_in">list</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">    word_set = <span class="built_in">set</span>(wordDict)</span><br><span class="line">    n = <span class="built_in">len</span>(s)</span><br><span class="line">    dp = [<span class="literal">False</span>] * (n + <span class="number">1</span>)</span><br><span class="line">    dp[<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i):</span><br><span class="line">            <span class="keyword">if</span> dp[j] <span class="keyword">and</span> s[j:i] <span class="keyword">in</span> word_set:</span><br><span class="line">                dp[i] = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dp[n]</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-2"><a href="#🧠-记忆口诀-2" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“前面能拆，后面在字典，就能拆”</strong></p></blockquote><hr><h2 id="4️⃣-LC-322-零钱兑换-🟡"><a href="#4️⃣-LC-322-零钱兑换-🟡" class="headerlink" title="4️⃣ LC 322. 零钱兑换 🟡"></a>4️⃣ LC 322. 零钱兑换 🟡</h2><h3 id="题目描述-3"><a href="#题目描述-3" class="headerlink" title="题目描述"></a>题目描述</h3><p>用最少的硬币凑出目标金额。</p><h3 id="🎨-图解思路-3"><a href="#🎨-图解思路-3" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">coins = [1, 2, 5], amount = 11</span><br><span class="line"></span><br><span class="line">dp[i] = 凑出金额 i 需要的最少硬币数</span><br><span class="line"></span><br><span class="line">对于每个金额 i，尝试每个硬币 c：</span><br><span class="line">dp[i] = min(dp[i], dp[i-c] + 1)</span><br><span class="line"></span><br><span class="line">amount:  0  1  2  3  4  5  6  7  8  9  10  11</span><br><span class="line">dp:      0  1  1  2  2  1  2  2  3  3   2   3</span><br><span class="line">            ↑  ↑  ↑  ↑  ↑  ↑  ↑  ↑  ↑   ↑   ↑</span><br><span class="line">           +1 +2 +1 +2 +5 +1 +2 +1 +2  +5  +5</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-3"><a href="#💻-代码实现-3" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">coinChange</span>(<span class="params">coins: <span class="built_in">list</span>, amount: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    dp = [<span class="built_in">float</span>(<span class="string">'inf'</span>)] * (amount + <span class="number">1</span>)</span><br><span class="line">    dp[<span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, amount + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> coin <span class="keyword">in</span> coins:</span><br><span class="line">            <span class="keyword">if</span> coin &lt;= i <span class="keyword">and</span> dp[i - coin] != <span class="built_in">float</span>(<span class="string">'inf'</span>):</span><br><span class="line">                dp[i] = <span class="built_in">min</span>(dp[i], dp[i - coin] + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dp[amount] <span class="keyword">if</span> dp[amount] != <span class="built_in">float</span>(<span class="string">'inf'</span>) <span class="keyword">else</span> -<span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-3"><a href="#🧠-记忆口诀-3" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“每个硬币试一试，取最小”</strong></p></blockquote><hr><h2 id="5️⃣-LC-300-最长递增子序列-🟡"><a href="#5️⃣-LC-300-最长递增子序列-🟡" class="headerlink" title="5️⃣ LC 300. 最长递增子序列 🟡"></a>5️⃣ LC 300. 最长递增子序列 🟡</h2><h3 id="题目描述-4"><a href="#题目描述-4" class="headerlink" title="题目描述"></a>题目描述</h3><p>找出数组中最长的严格递增子序列的长度。</p><h3 id="🎨-图解思路-4"><a href="#🎨-图解思路-4" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">nums = [10, 9, 2, 5, 3, 7, 101, 18]</span><br><span class="line"></span><br><span class="line">dp[i] = 以 nums[i] 结尾的 LIS 长度</span><br><span class="line"></span><br><span class="line">对于 nums[i]，找所有 j &lt; i 且 nums[j] &lt; nums[i]：</span><br><span class="line">dp[i] = max(dp[j] + 1)</span><br><span class="line"></span><br><span class="line">i:      0   1   2   3   4   5   6    7</span><br><span class="line">nums:  10   9   2   5   3   7  101  18</span><br><span class="line">dp:     1   1   1   2   2   3    4   4</span><br><span class="line">                    ↑   ↑   ↑    ↑   ↑</span><br><span class="line">                   2+1 2+1 5+1  7+1 7+1</span><br><span class="line"></span><br><span class="line">LIS = 4 (如 [2, 3, 7, 101])</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-4"><a href="#💻-代码实现-4" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># O(n²) 解法</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">lengthOfLIS</span>(<span class="params">nums: <span class="built_in">list</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    n = <span class="built_in">len</span>(nums)</span><br><span class="line">    dp = [<span class="number">1</span>] * n</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i):</span><br><span class="line">            <span class="keyword">if</span> nums[j] &lt; nums[i]:</span><br><span class="line">                dp[i] = <span class="built_in">max</span>(dp[i], dp[j] + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(dp)</span><br></pre></td></tr></tbody></table></figure><h3 id="🔥-O-n-log-n-二分解法"><a href="#🔥-O-n-log-n-二分解法" class="headerlink" title="🔥 O(n log n) 二分解法"></a>🔥 O(n log n) 二分解法</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lengthOfLIS</span>(<span class="params">nums: <span class="built_in">list</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="comment"># tails[i] = 长度为 i+1 的 LIS 的最小结尾元素</span></span><br><span class="line">    tails = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">        <span class="comment"># 二分查找第一个 &gt;= num 的位置</span></span><br><span class="line">        left, right = <span class="number">0</span>, <span class="built_in">len</span>(tails)</span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            mid = (left + right) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> tails[mid] &lt; num:</span><br><span class="line">                left = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                right = mid</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> left == <span class="built_in">len</span>(tails):</span><br><span class="line">            tails.append(num)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            tails[left] = num</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(tails)</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-4"><a href="#🧠-记忆口诀-4" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“前面比我小的，加1取最大”</strong></p></blockquote><hr><h2 id="6️⃣-LC-120-三角形最小路径和-🟡"><a href="#6️⃣-LC-120-三角形最小路径和-🟡" class="headerlink" title="6️⃣ LC 120. 三角形最小路径和 🟡"></a>6️⃣ LC 120. 三角形最小路径和 🟡</h2><h3 id="题目描述-5"><a href="#题目描述-5" class="headerlink" title="题目描述"></a>题目描述</h3><p>从顶部到底部的最小路径和。</p><h3 id="🎨-图解思路-5"><a href="#🎨-图解思路-5" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">    [2]</span><br><span class="line">   [3,4]</span><br><span class="line">  [6,5,7]</span><br><span class="line"> [4,1,8,3]</span><br><span class="line"></span><br><span class="line">自底向上计算：</span><br><span class="line">dp[i][j] = min(dp[i+1][j], dp[i+1][j+1]) + triangle[i][j]</span><br><span class="line"></span><br><span class="line">第3层: [4, 1, 8, 3]</span><br><span class="line">第2层: [6+1, 5+1, 7+3] = [7, 6, 10]</span><br><span class="line">第1层: [3+6, 4+6] = [9, 10]</span><br><span class="line">第0层: [2+9] = [11]</span><br><span class="line"></span><br><span class="line">最小路径和 = 11</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-5"><a href="#💻-代码实现-5" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">minimumTotal</span>(<span class="params">triangle: <span class="built_in">list</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    n = <span class="built_in">len</span>(triangle)</span><br><span class="line">    <span class="comment"># 从最后一行开始</span></span><br><span class="line">    dp = triangle[-<span class="number">1</span>][:]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 自底向上</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">2</span>, -<span class="number">1</span>, -<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(i + <span class="number">1</span>):</span><br><span class="line">            dp[j] = <span class="built_in">min</span>(dp[j], dp[j + <span class="number">1</span>]) + triangle[i][j]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dp[<span class="number">0</span>]</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-5"><a href="#🧠-记忆口诀-5" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“自底向上，取小加自己”</strong></p></blockquote><hr><h2 id="7️⃣-LC-64-最小路径和-🟡"><a href="#7️⃣-LC-64-最小路径和-🟡" class="headerlink" title="7️⃣ LC 64. 最小路径和 🟡"></a>7️⃣ LC 64. 最小路径和 🟡</h2><h3 id="题目描述-6"><a href="#题目描述-6" class="headerlink" title="题目描述"></a>题目描述</h3><p>从左上角到右下角的最小路径和。</p><h3 id="🎨-图解思路-6"><a href="#🎨-图解思路-6" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">grid:</span><br><span class="line">[1, 3, 1]</span><br><span class="line">[1, 5, 1]</span><br><span class="line">[4, 2, 1]</span><br><span class="line"></span><br><span class="line">dp[i][j] = min(dp[i-1][j], dp[i][j-1]) + grid[i][j]</span><br><span class="line"></span><br><span class="line">dp:</span><br><span class="line">[1, 4, 5]</span><br><span class="line">[2, 7, 6]</span><br><span class="line">[6, 8, 7]</span><br><span class="line"></span><br><span class="line">最小路径和 = 7 (1→3→1→1→1)</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-6"><a href="#💻-代码实现-6" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">minPathSum</span>(<span class="params">grid: <span class="built_in">list</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    m, n = <span class="built_in">len</span>(grid), <span class="built_in">len</span>(grid[<span class="number">0</span>])</span><br><span class="line">    dp = [[<span class="number">0</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m)]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">and</span> j == <span class="number">0</span>:</span><br><span class="line">                dp[i][j] = grid[i][j]</span><br><span class="line">            <span class="keyword">elif</span> i == <span class="number">0</span>:</span><br><span class="line">                dp[i][j] = dp[i][j-<span class="number">1</span>] + grid[i][j]</span><br><span class="line">            <span class="keyword">elif</span> j == <span class="number">0</span>:</span><br><span class="line">                dp[i][j] = dp[i-<span class="number">1</span>][j] + grid[i][j]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dp[i][j] = <span class="built_in">min</span>(dp[i-<span class="number">1</span>][j], dp[i][j-<span class="number">1</span>]) + grid[i][j]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dp[m-<span class="number">1</span>][n-<span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-6"><a href="#🧠-记忆口诀-6" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“上左取小，加自己”</strong></p></blockquote><hr><h2 id="8️⃣-LC-63-不同路径-II-🟡"><a href="#8️⃣-LC-63-不同路径-II-🟡" class="headerlink" title="8️⃣ LC 63. 不同路径 II 🟡"></a>8️⃣ LC 63. 不同路径 II 🟡</h2><h3 id="题目描述-7"><a href="#题目描述-7" class="headerlink" title="题目描述"></a>题目描述</h3><p>有障碍物的网格，从左上到右下的路径数。</p><h3 id="🎨-图解思路-7"><a href="#🎨-图解思路-7" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">grid:              dp:</span><br><span class="line">[0, 0, 0]         [1, 1, 1]</span><br><span class="line">[0, 1, 0]    →    [1, 0, 1]</span><br><span class="line">[0, 0, 0]         [1, 1, 2]</span><br><span class="line"></span><br><span class="line">障碍物位置 dp = 0</span><br><span class="line">其他位置 dp = dp[上] + dp[左]</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-7"><a href="#💻-代码实现-7" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">uniquePathsWithObstacles</span>(<span class="params">obstacleGrid: <span class="built_in">list</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    m, n = <span class="built_in">len</span>(obstacleGrid), <span class="built_in">len</span>(obstacleGrid[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> obstacleGrid[<span class="number">0</span>][<span class="number">0</span>] == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    dp = [[<span class="number">0</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m)]</span><br><span class="line">    dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化第一列</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m):</span><br><span class="line">        <span class="keyword">if</span> obstacleGrid[i][<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">            dp[i][<span class="number">0</span>] = dp[i-<span class="number">1</span>][<span class="number">0</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化第一行</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">        <span class="keyword">if</span> obstacleGrid[<span class="number">0</span>][j] == <span class="number">0</span>:</span><br><span class="line">            dp[<span class="number">0</span>][j] = dp[<span class="number">0</span>][j-<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 填充 dp</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n):</span><br><span class="line">            <span class="keyword">if</span> obstacleGrid[i][j] == <span class="number">0</span>:</span><br><span class="line">                dp[i][j] = dp[i-<span class="number">1</span>][j] + dp[i][j-<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dp[m-<span class="number">1</span>][n-<span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-7"><a href="#🧠-记忆口诀-7" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“障碍为0，否则上加左”</strong></p></blockquote><hr><h2 id="9️⃣-LC-5-最长回文子串-🟡"><a href="#9️⃣-LC-5-最长回文子串-🟡" class="headerlink" title="9️⃣ LC 5. 最长回文子串 🟡"></a>9️⃣ LC 5. 最长回文子串 🟡</h2><h3 id="题目描述-8"><a href="#题目描述-8" class="headerlink" title="题目描述"></a>题目描述</h3><p>找出字符串中最长的回文子串。</p><h3 id="🎨-图解思路-8"><a href="#🎨-图解思路-8" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">s = "babad"</span><br><span class="line"></span><br><span class="line">dp[i][j] = s[i:j+1] 是否为回文</span><br><span class="line"></span><br><span class="line">条件: s[i] == s[j] and dp[i+1][j-1]</span><br><span class="line"></span><br><span class="line">填表顺序：按长度从小到大</span><br><span class="line"></span><br><span class="line">长度1: 全为 True</span><br><span class="line">长度2: s[i] == s[i+1]</span><br><span class="line">长度3+: s[i] == s[j] and dp[i+1][j-1]</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-8"><a href="#💻-代码实现-8" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">longestPalindrome</span>(<span class="params">s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    n = <span class="built_in">len</span>(s)</span><br><span class="line">    <span class="keyword">if</span> n &lt; <span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> s</span><br><span class="line">    </span><br><span class="line">    dp = [[<span class="literal">False</span>] * n <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n)]</span><br><span class="line">    start, max_len = <span class="number">0</span>, <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 所有长度为 1 的子串都是回文</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        dp[i][i] = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 按长度填表</span></span><br><span class="line">    <span class="keyword">for</span> length <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, n + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - length + <span class="number">1</span>):</span><br><span class="line">            j = i + length - <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> s[i] == s[j]:</span><br><span class="line">                <span class="keyword">if</span> length == <span class="number">2</span>:</span><br><span class="line">                    dp[i][j] = <span class="literal">True</span></span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    dp[i][j] = dp[i + <span class="number">1</span>][j - <span class="number">1</span>]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> dp[i][j] <span class="keyword">and</span> length &gt; max_len:</span><br><span class="line">                start, max_len = i, length</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> s[start:start + max_len]</span><br></pre></td></tr></tbody></table></figure><h3 id="🔥-中心扩展法-更优"><a href="#🔥-中心扩展法-更优" class="headerlink" title="🔥 中心扩展法 (更优)"></a>🔥 中心扩展法 (更优)</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">longestPalindrome</span>(<span class="params">s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">expand</span>(<span class="params">left, right</span>):</span><br><span class="line">        <span class="keyword">while</span> left &gt;= <span class="number">0</span> <span class="keyword">and</span> right &lt; <span class="built_in">len</span>(s) <span class="keyword">and</span> s[left] == s[right]:</span><br><span class="line">            left -= <span class="number">1</span></span><br><span class="line">            right += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> s[left + <span class="number">1</span>:right]</span><br><span class="line">    </span><br><span class="line">    result = <span class="string">""</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">        <span class="comment"># 奇数长度</span></span><br><span class="line">        odd = expand(i, i)</span><br><span class="line">        <span class="comment"># 偶数长度</span></span><br><span class="line">        even = expand(i, i + <span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        result = <span class="built_in">max</span>(result, odd, even, key=<span class="built_in">len</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-8"><a href="#🧠-记忆口诀-8" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“首尾相同，中间也是回文”</strong></p></blockquote><hr><h2 id="🔟-LC-97-交错字符串-🟡"><a href="#🔟-LC-97-交错字符串-🟡" class="headerlink" title="🔟 LC 97. 交错字符串 🟡"></a>🔟 LC 97. 交错字符串 🟡</h2><h3 id="题目描述-9"><a href="#题目描述-9" class="headerlink" title="题目描述"></a>题目描述</h3><p>判断 s3 是否由 s1 和 s2 交错组成。</p><h3 id="🎨-图解思路-9"><a href="#🎨-图解思路-9" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">s1 = "aabcc", s2 = "dbbca", s3 = "aadbbcbcac"</span><br><span class="line"></span><br><span class="line">dp[i][j] = s1[0:i] 和 s2[0:j] 能否交错组成 s3[0:i+j]</span><br><span class="line"></span><br><span class="line">状态转移:</span><br><span class="line">dp[i][j] = (dp[i-1][j] and s1[i-1]==s3[i+j-1]) or</span><br><span class="line">           (dp[i][j-1] and s2[j-1]==s3[i+j-1])</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-9"><a href="#💻-代码实现-9" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">isInterleave</span>(<span class="params">s1: <span class="built_in">str</span>, s2: <span class="built_in">str</span>, s3: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">    m, n = <span class="built_in">len</span>(s1), <span class="built_in">len</span>(s2)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> m + n != <span class="built_in">len</span>(s3):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    </span><br><span class="line">    dp = [[<span class="literal">False</span>] * (n + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m + <span class="number">1</span>)]</span><br><span class="line">    dp[<span class="number">0</span>][<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化第一列</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">        dp[i][<span class="number">0</span>] = dp[i-<span class="number">1</span>][<span class="number">0</span>] <span class="keyword">and</span> s1[i-<span class="number">1</span>] == s3[i-<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化第一行</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">        dp[<span class="number">0</span>][j] = dp[<span class="number">0</span>][j-<span class="number">1</span>] <span class="keyword">and</span> s2[j-<span class="number">1</span>] == s3[j-<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 填充 dp</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">            dp[i][j] = ((dp[i-<span class="number">1</span>][j] <span class="keyword">and</span> s1[i-<span class="number">1</span>] == s3[i+j-<span class="number">1</span>]) <span class="keyword">or</span></span><br><span class="line">                        (dp[i][j-<span class="number">1</span>] <span class="keyword">and</span> s2[j-<span class="number">1</span>] == s3[i+j-<span class="number">1</span>]))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dp[m][n]</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-9"><a href="#🧠-记忆口诀-9" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“上或左能到，且字符匹配”</strong></p></blockquote><hr><h2 id="1️⃣1️⃣-LC-72-编辑距离-🟡"><a href="#1️⃣1️⃣-LC-72-编辑距离-🟡" class="headerlink" title="1️⃣1️⃣ LC 72. 编辑距离 🟡"></a>1️⃣1️⃣ LC 72. 编辑距离 🟡</h2><h3 id="题目描述-10"><a href="#题目描述-10" class="headerlink" title="题目描述"></a>题目描述</h3><p>将 word1 转换成 word2 所使用的最少操作数。</p><h3 id="🎨-图解思路-10"><a href="#🎨-图解思路-10" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">word1 = "horse", word2 = "ros"</span><br><span class="line"></span><br><span class="line">dp[i][j] = word1[0:i] 转换为 word2[0:j] 的最少操作</span><br><span class="line"></span><br><span class="line">三种操作:</span><br><span class="line">1. 插入: dp[i][j-1] + 1</span><br><span class="line">2. 删除: dp[i-1][j] + 1</span><br><span class="line">3. 替换: dp[i-1][j-1] + (0 if 相同 else 1)</span><br><span class="line"></span><br><span class="line">dp 表:</span><br><span class="line">      ""  r  o  s</span><br><span class="line">  ""   0  1  2  3</span><br><span class="line">  h    1  1  2  3</span><br><span class="line">  o    2  2  1  2</span><br><span class="line">  r    3  2  2  2</span><br><span class="line">  s    4  3  3  2</span><br><span class="line">  e    5  4  4  3</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-10"><a href="#💻-代码实现-10" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">minDistance</span>(<span class="params">word1: <span class="built_in">str</span>, word2: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    m, n = <span class="built_in">len</span>(word1), <span class="built_in">len</span>(word2)</span><br><span class="line">    dp = [[<span class="number">0</span>] * (n + <span class="number">1</span>) <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(m + <span class="number">1</span>)]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 初始化</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m + <span class="number">1</span>):</span><br><span class="line">        dp[i][<span class="number">0</span>] = i</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>):</span><br><span class="line">        dp[<span class="number">0</span>][j] = j</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 填充 dp</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, m + <span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, n + <span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> word1[i-<span class="number">1</span>] == word2[j-<span class="number">1</span>]:</span><br><span class="line">                dp[i][j] = dp[i-<span class="number">1</span>][j-<span class="number">1</span>]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dp[i][j] = <span class="number">1</span> + <span class="built_in">min</span>(</span><br><span class="line">                    dp[i-<span class="number">1</span>][j],      <span class="comment"># 删除</span></span><br><span class="line">                    dp[i][j-<span class="number">1</span>],      <span class="comment"># 插入</span></span><br><span class="line">                    dp[i-<span class="number">1</span>][j-<span class="number">1</span>]     <span class="comment"># 替换</span></span><br><span class="line">                )</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dp[m][n]</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-10"><a href="#🧠-记忆口诀-10" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“相同不变，不同取三者最小加1”</strong></p></blockquote><hr><h2 id="📊-本章总结"><a href="#📊-本章总结" class="headerlink" title="📊 本章总结"></a>📊 本章总结</h2><h3 id="DP-问题分类"><a href="#DP-问题分类" class="headerlink" title="DP 问题分类"></a>DP 问题分类</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">┌──────────────────────────────────────────────────┐</span><br><span class="line">│                  动态规划分类                     │</span><br><span class="line">├──────────────────────────────────────────────────┤</span><br><span class="line">│                                                  │</span><br><span class="line">│  线性 DP                                         │</span><br><span class="line">│  ├─ 单序列: 爬楼梯, 打家劫舍, LIS                │</span><br><span class="line">│  └─ 双序列: 编辑距离, 交错字符串                  │</span><br><span class="line">│                                                  │</span><br><span class="line">│  区间 DP                                         │</span><br><span class="line">│  └─ 回文子串                                     │</span><br><span class="line">│                                                  │</span><br><span class="line">│  背包 DP                                         │</span><br><span class="line">│  └─ 零钱兑换, 单词拆分                           │</span><br><span class="line">│                                                  │</span><br><span class="line">│  网格 DP                                         │</span><br><span class="line">│  └─ 最小路径和, 不同路径                         │</span><br><span class="line">│                                                  │</span><br><span class="line">└──────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-全章记忆口诀"><a href="#🧠-全章记忆口诀" class="headerlink" title="🧠 全章记忆口诀"></a>🧠 全章记忆口诀</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">爬楼劫舍单词拆</span><br><span class="line">零钱递增三角来</span><br><span class="line">路径网格回文判</span><br><span class="line">交错编辑全都会</span><br></pre></td></tr></tbody></table></figure><hr><blockquote><p>📖 <strong>下一篇</strong>：<a href="/2026/01/18/leetcode-150-graph/">图论专题</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>🔍 LeetCode 150 - 图论与搜索专题</title>
      <link href="/2026/01/18/leetcode-150-graph/"/>
      <url>/2026/01/18/leetcode-150-graph/</url>
      
        <content type="html"><![CDATA[<h1 id="🔍-图论与搜索专题-8题"><a href="#🔍-图论与搜索专题-8题" class="headerlink" title="🔍 图论与搜索专题 (8题)"></a>🔍 图论与搜索专题 (8题)</h1><blockquote><p>🎯 <strong>核心技巧</strong>：BFS、DFS、拓扑排序、并查集</p></blockquote><hr><h2 id="🗺️-图搜索算法对比"><a href="#🗺️-图搜索算法对比" class="headerlink" title="🗺️ 图搜索算法对比"></a>🗺️ 图搜索算法对比</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                    BFS vs DFS                                │</span><br><span class="line">├──────────────────────┬──────────────────────────────────────┤</span><br><span class="line">│         BFS          │              DFS                      │</span><br><span class="line">│    (广度优先搜索)     │         (深度优先搜索)                │</span><br><span class="line">├──────────────────────┼──────────────────────────────────────┤</span><br><span class="line">│  数据结构：队列       │  数据结构：栈/递归                    │</span><br><span class="line">│  探索方式：层层扩展   │  探索方式：一路走到底                 │</span><br><span class="line">│  适用：最短路径       │  适用：连通性、路径搜索               │</span><br><span class="line">├──────────────────────┼──────────────────────────────────────┤</span><br><span class="line">│       1              │         1                            │</span><br><span class="line">│      /|\             │        /|\                           │</span><br><span class="line">│     2 3 4  → 层序    │       2 3 4  → 深入                  │</span><br><span class="line">│    /|   |            │      /|   |                          │</span><br><span class="line">│   5 6   7            │     5 6   7                          │</span><br><span class="line">│                      │                                       │</span><br><span class="line">│  顺序: 1→2→3→4→5→6→7 │  顺序: 1→2→5→6→3→4→7                 │</span><br><span class="line">└──────────────────────┴──────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="🔧-BFS-模板"><a href="#🔧-BFS-模板" class="headerlink" title="🔧 BFS 模板"></a>🔧 BFS 模板</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bfs</span>(<span class="params">start</span>):</span><br><span class="line">    queue = deque([start])</span><br><span class="line">    visited = {start}</span><br><span class="line">    level = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> queue:</span><br><span class="line">        size = <span class="built_in">len</span>(queue)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(size):</span><br><span class="line">            node = queue.popleft()</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 处理当前节点</span></span><br><span class="line">            process(node)</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 将邻居加入队列</span></span><br><span class="line">            <span class="keyword">for</span> neighbor <span class="keyword">in</span> get_neighbors(node):</span><br><span class="line">                <span class="keyword">if</span> neighbor <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                    visited.add(neighbor)</span><br><span class="line">                    queue.append(neighbor)</span><br><span class="line">        </span><br><span class="line">        level += <span class="number">1</span>  <span class="comment"># 层数 +1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> level</span><br></pre></td></tr></tbody></table></figure><h2 id="🔧-DFS-模板"><a href="#🔧-DFS-模板" class="headerlink" title="🔧 DFS 模板"></a>🔧 DFS 模板</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">node, visited</span>):</span><br><span class="line">    <span class="keyword">if</span> node <span class="keyword">in</span> visited:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    </span><br><span class="line">    visited.add(node)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 处理当前节点</span></span><br><span class="line">    process(node)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 递归访问邻居</span></span><br><span class="line">    <span class="keyword">for</span> neighbor <span class="keyword">in</span> get_neighbors(node):</span><br><span class="line">        dfs(neighbor, visited)</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="1️⃣-LC-200-岛屿数量-🟡"><a href="#1️⃣-LC-200-岛屿数量-🟡" class="headerlink" title="1️⃣ LC 200. 岛屿数量 🟡"></a>1️⃣ LC 200. 岛屿数量 🟡</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>计算二维网格中岛屿的数量（由 ‘1’ 组成的连通区域）。</p><h3 id="🎨-图解思路"><a href="#🎨-图解思路" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">grid:</span><br><span class="line">1 1 0 0 0</span><br><span class="line">1 1 0 0 0</span><br><span class="line">0 0 1 0 0</span><br><span class="line">0 0 0 1 1</span><br><span class="line"></span><br><span class="line">岛屿数量 = 3</span><br><span class="line"></span><br><span class="line">策略：遍历网格，遇到 '1' 就启动 DFS/BFS</span><br><span class="line">把整个岛屿标记为已访问，计数 +1</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-DFS"><a href="#💻-代码实现-DFS" class="headerlink" title="💻 代码实现 (DFS)"></a>💻 代码实现 (DFS)</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">numIslands</span>(<span class="params">grid: <span class="built_in">list</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> grid:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    m, n = <span class="built_in">len</span>(grid), <span class="built_in">len</span>(grid[<span class="number">0</span>])</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">i, j</span>):</span><br><span class="line">        <span class="comment"># 边界检查 &amp; 是否为陆地</span></span><br><span class="line">        <span class="keyword">if</span> i &lt; <span class="number">0</span> <span class="keyword">or</span> i &gt;= m <span class="keyword">or</span> j &lt; <span class="number">0</span> <span class="keyword">or</span> j &gt;= n <span class="keyword">or</span> grid[i][j] != <span class="string">'1'</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 标记为已访问</span></span><br><span class="line">        grid[i][j] = <span class="string">'0'</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 四个方向扩展</span></span><br><span class="line">        dfs(i + <span class="number">1</span>, j)</span><br><span class="line">        dfs(i - <span class="number">1</span>, j)</span><br><span class="line">        dfs(i, j + <span class="number">1</span>)</span><br><span class="line">        dfs(i, j - <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">if</span> grid[i][j] == <span class="string">'1'</span>:</span><br><span class="line">                dfs(i, j)</span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> count</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀"><a href="#🧠-记忆口诀" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“遇1就淹，淹完计数”</strong></p></blockquote><hr><h2 id="2️⃣-LC-130-被围绕的区域-🟡"><a href="#2️⃣-LC-130-被围绕的区域-🟡" class="headerlink" title="2️⃣ LC 130. 被围绕的区域 🟡"></a>2️⃣ LC 130. 被围绕的区域 🟡</h2><h3 id="题目描述-1"><a href="#题目描述-1" class="headerlink" title="题目描述"></a>题目描述</h3><p>将所有被 ‘X’ 围绕的 ‘O’ 填充为 ‘X’（边界上的 ‘O’ 及其连通的 ‘O’ 不算被围绕）。</p><h3 id="🎨-图解思路-1"><a href="#🎨-图解思路-1" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">输入:               输出:</span><br><span class="line">X X X X            X X X X</span><br><span class="line">X O O X      →     X X X X</span><br><span class="line">X X O X            X X X X</span><br><span class="line">X O X X            X O X X</span><br><span class="line"></span><br><span class="line">逆向思维：</span><br><span class="line">1. 从边界的 'O' 开始 DFS，标记为 '#'</span><br><span class="line">2. 遍历整个网格：</span><br><span class="line">   - 'O' → 'X' (被围绕)</span><br><span class="line">   - '#' → 'O' (恢复)</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现"><a href="#💻-代码实现" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">solve</span>(<span class="params">board: <span class="built_in">list</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> board:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    </span><br><span class="line">    m, n = <span class="built_in">len</span>(board), <span class="built_in">len</span>(board[<span class="number">0</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">i, j</span>):</span><br><span class="line">        <span class="keyword">if</span> i &lt; <span class="number">0</span> <span class="keyword">or</span> i &gt;= m <span class="keyword">or</span> j &lt; <span class="number">0</span> <span class="keyword">or</span> j &gt;= n <span class="keyword">or</span> board[i][j] != <span class="string">'O'</span>:</span><br><span class="line">            <span class="keyword">return</span></span><br><span class="line">        board[i][j] = <span class="string">'#'</span></span><br><span class="line">        dfs(i + <span class="number">1</span>, j)</span><br><span class="line">        dfs(i - <span class="number">1</span>, j)</span><br><span class="line">        dfs(i, j + <span class="number">1</span>)</span><br><span class="line">        dfs(i, j - <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 从边界开始标记</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        dfs(i, <span class="number">0</span>)</span><br><span class="line">        dfs(i, n - <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">        dfs(<span class="number">0</span>, j)</span><br><span class="line">        dfs(m - <span class="number">1</span>, j)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 恢复和填充</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(n):</span><br><span class="line">            <span class="keyword">if</span> board[i][j] == <span class="string">'O'</span>:</span><br><span class="line">                board[i][j] = <span class="string">'X'</span></span><br><span class="line">            <span class="keyword">elif</span> board[i][j] == <span class="string">'#'</span>:</span><br><span class="line">                board[i][j] = <span class="string">'O'</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-1"><a href="#🧠-记忆口诀-1" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“边界O不围，标记后恢复”</strong></p></blockquote><hr><h2 id="3️⃣-LC-133-克隆图-🟡"><a href="#3️⃣-LC-133-克隆图-🟡" class="headerlink" title="3️⃣ LC 133. 克隆图 🟡"></a>3️⃣ LC 133. 克隆图 🟡</h2><h3 id="题目描述-2"><a href="#题目描述-2" class="headerlink" title="题目描述"></a>题目描述</h3><p>深拷贝一个无向连通图。</p><h3 id="🎨-图解思路-2"><a href="#🎨-图解思路-2" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">原图:          克隆:</span><br><span class="line">  1 --- 2       1' --- 2'</span><br><span class="line">  |     |       |      |</span><br><span class="line">  4 --- 3       4' --- 3'</span><br><span class="line"></span><br><span class="line">使用哈希表记录 原节点 → 克隆节点 的映射</span><br><span class="line">BFS 或 DFS 遍历并克隆</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-1"><a href="#💻-代码实现-1" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">cloneGraph</span>(<span class="params">node: <span class="string">'Node'</span></span>) -&gt; <span class="string">'Node'</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> node:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 哈希表：原节点 → 克隆节点</span></span><br><span class="line">    cloned = {}</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">dfs</span>(<span class="params">node</span>):</span><br><span class="line">        <span class="keyword">if</span> node <span class="keyword">in</span> cloned:</span><br><span class="line">            <span class="keyword">return</span> cloned[node]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建克隆节点</span></span><br><span class="line">        clone = Node(node.val)</span><br><span class="line">        cloned[node] = clone</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 克隆邻居</span></span><br><span class="line">        <span class="keyword">for</span> neighbor <span class="keyword">in</span> node.neighbors:</span><br><span class="line">            clone.neighbors.append(dfs(neighbor))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> clone</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dfs(node)</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-2"><a href="#🧠-记忆口诀-2" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“哈希记映射，DFS克隆”</strong></p></blockquote><hr><h2 id="4️⃣-LC-399-除法求值-🟡"><a href="#4️⃣-LC-399-除法求值-🟡" class="headerlink" title="4️⃣ LC 399. 除法求值 🟡"></a>4️⃣ LC 399. 除法求值 🟡</h2><h3 id="题目描述-3"><a href="#题目描述-3" class="headerlink" title="题目描述"></a>题目描述</h3><p>给定变量对的除法结果，求解其他除法。</p><h3 id="🎨-图解思路-3"><a href="#🎨-图解思路-3" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">equations: [["a","b"],["b","c"]]</span><br><span class="line">values: [2.0, 3.0]</span><br><span class="line"></span><br><span class="line">构建带权图:</span><br><span class="line">a --2.0--&gt; b --3.0--&gt; c</span><br><span class="line">a &lt;--0.5-- b &lt;--0.33-- c</span><br><span class="line"></span><br><span class="line">查询 a/c = a/b * b/c = 2.0 * 3.0 = 6.0</span><br><span class="line">使用 BFS 找路径并累乘权重</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-2"><a href="#💻-代码实现-2" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calcEquation</span>(<span class="params">equations, values, queries</span>):</span><br><span class="line">    <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict, deque</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建图</span></span><br><span class="line">    graph = defaultdict(<span class="built_in">dict</span>)</span><br><span class="line">    <span class="keyword">for</span> (a, b), val <span class="keyword">in</span> <span class="built_in">zip</span>(equations, values):</span><br><span class="line">        graph[a][b] = val</span><br><span class="line">        graph[b][a] = <span class="number">1.0</span> / val</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">bfs</span>(<span class="params">start, end</span>):</span><br><span class="line">        <span class="keyword">if</span> start <span class="keyword">not</span> <span class="keyword">in</span> graph <span class="keyword">or</span> end <span class="keyword">not</span> <span class="keyword">in</span> graph:</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1.0</span></span><br><span class="line">        <span class="keyword">if</span> start == end:</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1.0</span></span><br><span class="line">        </span><br><span class="line">        queue = deque([(start, <span class="number">1.0</span>)])</span><br><span class="line">        visited = {start}</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> queue:</span><br><span class="line">            node, product = queue.popleft()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">for</span> neighbor, weight <span class="keyword">in</span> graph[node].items():</span><br><span class="line">                <span class="keyword">if</span> neighbor == end:</span><br><span class="line">                    <span class="keyword">return</span> product * weight</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> neighbor <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                    visited.add(neighbor)</span><br><span class="line">                    queue.append((neighbor, product * weight))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1.0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> [bfs(a, b) <span class="keyword">for</span> a, b <span class="keyword">in</span> queries]</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-3"><a href="#🧠-记忆口诀-3" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“带权图建边，BFS累乘”</strong></p></blockquote><hr><h2 id="5️⃣-LC-207-课程表-🟡"><a href="#5️⃣-LC-207-课程表-🟡" class="headerlink" title="5️⃣ LC 207. 课程表 🟡"></a>5️⃣ LC 207. 课程表 🟡</h2><h3 id="题目描述-4"><a href="#题目描述-4" class="headerlink" title="题目描述"></a>题目描述</h3><p>判断是否可能完成所有课程（检测有向图是否有环）。</p><h3 id="🎨-图解思路-4"><a href="#🎨-图解思路-4" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">numCourses = 4</span><br><span class="line">prerequisites = [[1,0],[2,0],[3,1],[3,2]]</span><br><span class="line"></span><br><span class="line">构建图:</span><br><span class="line">0 → 1 → 3</span><br><span class="line">0 → 2 ↗</span><br><span class="line"></span><br><span class="line">拓扑排序：</span><br><span class="line">1. 统计每个节点的入度</span><br><span class="line">2. 将入度为 0 的节点入队</span><br><span class="line">3. BFS 处理，每处理一个节点，邻居入度 -1</span><br><span class="line">4. 入度变为 0 的节点入队</span><br><span class="line">5. 如果处理的节点数 = 总课程数，则无环</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-3"><a href="#💻-代码实现-3" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">canFinish</span>(<span class="params">numCourses: <span class="built_in">int</span>, prerequisites: <span class="built_in">list</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">    <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict, deque</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 构建图和入度表</span></span><br><span class="line">    graph = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">    in_degree = [<span class="number">0</span>] * numCourses</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> course, prereq <span class="keyword">in</span> prerequisites:</span><br><span class="line">        graph[prereq].append(course)</span><br><span class="line">        in_degree[course] += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将入度为 0 的节点入队</span></span><br><span class="line">    queue = deque([i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numCourses) <span class="keyword">if</span> in_degree[i] == <span class="number">0</span>])</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> queue:</span><br><span class="line">        node = queue.popleft()</span><br><span class="line">        count += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> neighbor <span class="keyword">in</span> graph[node]:</span><br><span class="line">            in_degree[neighbor] -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> in_degree[neighbor] == <span class="number">0</span>:</span><br><span class="line">                queue.append(neighbor)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> count == numCourses</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-4"><a href="#🧠-记忆口诀-4" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“入度为零先处理，全部处理完无环”</strong></p></blockquote><hr><h2 id="6️⃣-LC-210-课程表-II-🟡"><a href="#6️⃣-LC-210-课程表-II-🟡" class="headerlink" title="6️⃣ LC 210. 课程表 II 🟡"></a>6️⃣ LC 210. 课程表 II 🟡</h2><h3 id="题目描述-5"><a href="#题目描述-5" class="headerlink" title="题目描述"></a>题目描述</h3><p>返回完成所有课程的学习顺序（拓扑排序结果）。</p><h3 id="💻-代码实现-4"><a href="#💻-代码实现-4" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">findOrder</span>(<span class="params">numCourses: <span class="built_in">int</span>, prerequisites: <span class="built_in">list</span></span>) -&gt; <span class="built_in">list</span>:</span><br><span class="line">    <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict, deque</span><br><span class="line">    </span><br><span class="line">    graph = defaultdict(<span class="built_in">list</span>)</span><br><span class="line">    in_degree = [<span class="number">0</span>] * numCourses</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> course, prereq <span class="keyword">in</span> prerequisites:</span><br><span class="line">        graph[prereq].append(course)</span><br><span class="line">        in_degree[course] += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    queue = deque([i <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numCourses) <span class="keyword">if</span> in_degree[i] == <span class="number">0</span>])</span><br><span class="line">    result = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> queue:</span><br><span class="line">        node = queue.popleft()</span><br><span class="line">        result.append(node)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> neighbor <span class="keyword">in</span> graph[node]:</span><br><span class="line">            in_degree[neighbor] -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> in_degree[neighbor] == <span class="number">0</span>:</span><br><span class="line">                queue.append(neighbor)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result <span class="keyword">if</span> <span class="built_in">len</span>(result) == numCourses <span class="keyword">else</span> []</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-5"><a href="#🧠-记忆口诀-5" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“拓扑排序，记录顺序”</strong></p></blockquote><hr><h2 id="7️⃣-LC-909-蛇梯棋-🟡"><a href="#7️⃣-LC-909-蛇梯棋-🟡" class="headerlink" title="7️⃣ LC 909. 蛇梯棋 🟡"></a>7️⃣ LC 909. 蛇梯棋 🟡</h2><h3 id="题目描述-6"><a href="#题目描述-6" class="headerlink" title="题目描述"></a>题目描述</h3><p>在蛇梯棋盘上，求从起点到终点的最少移动次数。</p><h3 id="🎨-图解思路-5"><a href="#🎨-图解思路-5" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">棋盘按 Boustrophedon 方式编号（蛇形）</span><br><span class="line"></span><br><span class="line">使用 BFS 求最短路径</span><br><span class="line">每次可以走 1-6 步（掷骰子）</span><br><span class="line">遇到蛇/梯子则传送到对应位置</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-5"><a href="#💻-代码实现-5" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">snakesAndLadders</span>(<span class="params">board: <span class="built_in">list</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line">    </span><br><span class="line">    n = <span class="built_in">len</span>(board)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 将位置编号转为坐标</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_position</span>(<span class="params">num</span>):</span><br><span class="line">        num -= <span class="number">1</span></span><br><span class="line">        row = n - <span class="number">1</span> - num // n</span><br><span class="line">        col = num % n <span class="keyword">if</span> (n - <span class="number">1</span> - row) % <span class="number">2</span> == <span class="number">0</span> <span class="keyword">else</span> n - <span class="number">1</span> - num % n</span><br><span class="line">        <span class="keyword">return</span> row, col</span><br><span class="line">    </span><br><span class="line">    queue = deque([(<span class="number">1</span>, <span class="number">0</span>)])  <span class="comment"># (位置, 步数)</span></span><br><span class="line">    visited = {<span class="number">1</span>}</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> queue:</span><br><span class="line">        curr, steps = queue.popleft()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> dice <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>, <span class="number">7</span>):</span><br><span class="line">            next_pos = curr + dice</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> next_pos &gt; n * n:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            </span><br><span class="line">            r, c = get_position(next_pos)</span><br><span class="line">            <span class="keyword">if</span> board[r][c] != -<span class="number">1</span>:</span><br><span class="line">                next_pos = board[r][c]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> next_pos == n * n:</span><br><span class="line">                <span class="keyword">return</span> steps + <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> next_pos <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                visited.add(next_pos)</span><br><span class="line">                queue.append((next_pos, steps + <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-6"><a href="#🧠-记忆口诀-6" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“BFS找最短，蛇梯要传送”</strong></p></blockquote><hr><h2 id="8️⃣-LC-433-最小基因变化-🟡"><a href="#8️⃣-LC-433-最小基因变化-🟡" class="headerlink" title="8️⃣ LC 433. 最小基因变化 🟡"></a>8️⃣ LC 433. 最小基因变化 🟡</h2><h3 id="题目描述-7"><a href="#题目描述-7" class="headerlink" title="题目描述"></a>题目描述</h3><p>从起始基因变化到目标基因的最少变化次数。</p><h3 id="🎨-图解思路-6"><a href="#🎨-图解思路-6" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">startGene = "AACCGGTT"</span><br><span class="line">endGene = "AAACGGTA"</span><br><span class="line">bank = ["AACCGGTA","AACCGCTA","AAACGGTA"]</span><br><span class="line"></span><br><span class="line">每次只能变一个字符，且结果必须在 bank 中</span><br><span class="line">使用 BFS 搜索最短路径</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-6"><a href="#💻-代码实现-6" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">minMutation</span>(<span class="params">startGene: <span class="built_in">str</span>, endGene: <span class="built_in">str</span>, bank: <span class="built_in">list</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line">    </span><br><span class="line">    bank_set = <span class="built_in">set</span>(bank)</span><br><span class="line">    <span class="keyword">if</span> endGene <span class="keyword">not</span> <span class="keyword">in</span> bank_set:</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    queue = deque([(startGene, <span class="number">0</span>)])</span><br><span class="line">    visited = {startGene}</span><br><span class="line">    chars = [<span class="string">'A'</span>, <span class="string">'C'</span>, <span class="string">'G'</span>, <span class="string">'T'</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> queue:</span><br><span class="line">        gene, steps = queue.popleft()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> gene == endGene:</span><br><span class="line">            <span class="keyword">return</span> steps</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(gene)):</span><br><span class="line">            <span class="keyword">for</span> c <span class="keyword">in</span> chars:</span><br><span class="line">                <span class="keyword">if</span> c != gene[i]:</span><br><span class="line">                    new_gene = gene[:i] + c + gene[i+<span class="number">1</span>:]</span><br><span class="line">                    </span><br><span class="line">                    <span class="keyword">if</span> new_gene <span class="keyword">in</span> bank_set <span class="keyword">and</span> new_gene <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                        visited.add(new_gene)</span><br><span class="line">                        queue.append((new_gene, steps + <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-7"><a href="#🧠-记忆口诀-7" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“每次变一个，在库中才行”</strong></p></blockquote><hr><h2 id="📊-本章总结"><a href="#📊-本章总结" class="headerlink" title="📊 本章总结"></a>📊 本章总结</h2><h3 id="图搜索场景选择"><a href="#图搜索场景选择" class="headerlink" title="图搜索场景选择"></a>图搜索场景选择</h3><table><thead><tr><th>场景</th><th>推荐算法</th><th>典型题目</th></tr></thead><tbody><tr><td>最短路径</td><td>BFS</td><td>909, 433</td></tr><tr><td>连通性</td><td>DFS/BFS</td><td>200, 130</td></tr><tr><td>拓扑排序</td><td>BFS (Kahn)</td><td>207, 210</td></tr><tr><td>图克隆</td><td>DFS + 哈希</td><td>133</td></tr><tr><td>带权路径</td><td>BFS + 累乘</td><td>399</td></tr></tbody></table><h3 id="🧠-全章记忆口诀"><a href="#🧠-全章记忆口诀" class="headerlink" title="🧠 全章记忆口诀"></a>🧠 全章记忆口诀</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">岛围克除课课蛇基</span><br><span class="line">图论八题记仔细</span><br><span class="line"></span><br><span class="line">岛 - 岛屿数量 (200)</span><br><span class="line">围 - 被围绕的区域 (130)</span><br><span class="line">克 - 克隆图 (133)</span><br><span class="line">除 - 除法求值 (399)</span><br><span class="line">课课 - 课程表 I/II (207, 210)</span><br><span class="line">蛇 - 蛇梯棋 (909)</span><br><span class="line">基 - 最小基因变化 (433)</span><br></pre></td></tr></tbody></table></figure><hr><blockquote><p>📖 <strong>返回</strong>：<a href="/2026/01/18/leetcode-150-index/">LeetCode 150 题总目录</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>⛰️ LeetCode 150 - 堆/优先队列专题</title>
      <link href="/2026/01/18/leetcode-150-heap/"/>
      <url>/2026/01/18/leetcode-150-heap/</url>
      
        <content type="html"><![CDATA[<h1 id="⛰️-堆-x2F-优先队列专题-5题"><a href="#⛰️-堆-x2F-优先队列专题-5题" class="headerlink" title="⛰️ 堆/优先队列专题 (5题)"></a>⛰️ 堆/优先队列专题 (5题)</h1><blockquote><p>🎯 <strong>核心特性</strong>：快速获取最大/最小值，O(log n) 插入删除</p></blockquote><hr><h2 id="🗺️-堆的基础知识"><a href="#🗺️-堆的基础知识" class="headerlink" title="🗺️ 堆的基础知识"></a>🗺️ 堆的基础知识</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                     堆的结构                                 │</span><br><span class="line">├─────────────────────────────────────────────────────────────┤</span><br><span class="line">│                                                             │</span><br><span class="line">│  最小堆 (Min Heap)           最大堆 (Max Heap)              │</span><br><span class="line">│       1                           9                         │</span><br><span class="line">│      / \                         / \                        │</span><br><span class="line">│     3   5                       7   8                       │</span><br><span class="line">│    / \                         / \                          │</span><br><span class="line">│   7   8                       3   5                         │</span><br><span class="line">│                                                             │</span><br><span class="line">│  父节点 ≤ 子节点              父节点 ≥ 子节点               │</span><br><span class="line">│  堆顶是最小值                 堆顶是最大值                   │</span><br><span class="line">│                                                             │</span><br><span class="line">├─────────────────────────────────────────────────────────────┤</span><br><span class="line">│  Python 中使用 heapq（默认最小堆）                          │</span><br><span class="line">│                                                             │</span><br><span class="line">│  import heapq                                               │</span><br><span class="line">│  heapq.heappush(heap, item)    # 入堆                       │</span><br><span class="line">│  heapq.heappop(heap)           # 出堆                       │</span><br><span class="line">│  heapq.heapify(list)           # 列表转堆                   │</span><br><span class="line">│  heap[0]                       # 查看堆顶                   │</span><br><span class="line">│                                                             │</span><br><span class="line">│  最大堆技巧：存入负数                                        │</span><br><span class="line">│  heapq.heappush(heap, -item)                                │</span><br><span class="line">│                                                             │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="🔧-堆的常见应用"><a href="#🔧-堆的常见应用" class="headerlink" title="🔧 堆的常见应用"></a>🔧 堆的常见应用</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                 堆的典型应用场景                             │</span><br><span class="line">├─────────────────────────────────────────────────────────────┤</span><br><span class="line">│                                                             │</span><br><span class="line">│  1. Top K 问题                                              │</span><br><span class="line">│     找第K大 → 维护大小为K的最小堆                           │</span><br><span class="line">│     找第K小 → 维护大小为K的最大堆                           │</span><br><span class="line">│                                                             │</span><br><span class="line">│  2. 合并K个有序列表                                         │</span><br><span class="line">│     用堆维护K个列表的当前最小元素                           │</span><br><span class="line">│                                                             │</span><br><span class="line">│  3. 数据流中的中位数                                        │</span><br><span class="line">│     两个堆：最大堆（左半部分）+ 最小堆（右半部分）          │</span><br><span class="line">│                                                             │</span><br><span class="line">│  4. 任务调度                                                │</span><br><span class="line">│     按优先级处理任务                                        │</span><br><span class="line">│                                                             │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="1️⃣-LC-215-数组中的第K个最大元素-🟡"><a href="#1️⃣-LC-215-数组中的第K个最大元素-🟡" class="headerlink" title="1️⃣ LC 215. 数组中的第K个最大元素 🟡"></a>1️⃣ LC 215. 数组中的第K个最大元素 🟡</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>在未排序的数组中找到第 k 个最大的元素。</p><h3 id="🎨-图解思路"><a href="#🎨-图解思路" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">nums = [3,2,1,5,6,4], k = 2</span><br><span class="line"></span><br><span class="line">方法1：排序后取第k个 O(n log n)</span><br><span class="line">方法2：最小堆维护k个最大元素 O(n log k)</span><br><span class="line">方法3：快速选择 O(n) 平均</span><br><span class="line"></span><br><span class="line">最小堆方法：</span><br><span class="line">维护大小为k的最小堆，堆顶就是第k大</span><br><span class="line"></span><br><span class="line">遍历: 3 → [3]</span><br><span class="line">      2 → [2,3]    (k=2)</span><br><span class="line">      1 → [2,3]    (1&lt;2，不入堆)</span><br><span class="line">      5 → [3,5]    (5入堆，弹出2)</span><br><span class="line">      6 → [5,6]    (6入堆，弹出3)</span><br><span class="line">      4 → [5,6]    (4&lt;5，不入堆)</span><br><span class="line"></span><br><span class="line">结果: 堆顶 5</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现"><a href="#💻-代码实现" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">findKthLargest</span>(<span class="params">nums: <span class="built_in">list</span>, k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="comment"># 方法1：最小堆</span></span><br><span class="line">    heap = []</span><br><span class="line">    <span class="keyword">for</span> num <span class="keyword">in</span> nums:</span><br><span class="line">        heapq.heappush(heap, num)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(heap) &gt; k:</span><br><span class="line">            heapq.heappop(heap)</span><br><span class="line">    <span class="keyword">return</span> heap[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 方法2：直接用 nlargest</span></span><br><span class="line">    <span class="comment"># return heapq.nlargest(k, nums)[-1]</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀"><a href="#🧠-记忆口诀" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“第K大用最小堆，堆顶就是答案”</strong></p></blockquote><hr><h2 id="2️⃣-LC-502-IPO-🔴"><a href="#2️⃣-LC-502-IPO-🔴" class="headerlink" title="2️⃣ LC 502. IPO 🔴"></a>2️⃣ LC 502. IPO 🔴</h2><h3 id="题目描述-1"><a href="#题目描述-1" class="headerlink" title="题目描述"></a>题目描述</h3><p>给定若干项目（利润和资本需求），初始资本 w，最多做 k 个项目，求最大资本。</p><h3 id="🎨-图解思路-1"><a href="#🎨-图解思路-1" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">k = 2, w = 0</span><br><span class="line">profits = [1,2,3]</span><br><span class="line">capitals = [0,1,1]</span><br><span class="line"></span><br><span class="line">贪心 + 堆：</span><br><span class="line">每次选择当前资本能做的项目中利润最大的</span><br><span class="line"></span><br><span class="line">初始 w=0:</span><br><span class="line">  可做项目: (profit=1, capital=0)</span><br><span class="line">  做项目1 → w=1</span><br><span class="line"></span><br><span class="line">w=1:</span><br><span class="line">  可做项目: (profit=2, capital=1), (profit=3, capital=1)</span><br><span class="line">  选最大 → 做项目3 → w=4</span><br><span class="line"></span><br><span class="line">结果: 4</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-1"><a href="#💻-代码实现-1" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">findMaximizedCapital</span>(<span class="params">k: <span class="built_in">int</span>, w: <span class="built_in">int</span>, profits: <span class="built_in">list</span>, capital: <span class="built_in">list</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="comment"># 按资本排序的项目列表</span></span><br><span class="line">    projects = <span class="built_in">sorted</span>(<span class="built_in">zip</span>(capital, profits))</span><br><span class="line">    </span><br><span class="line">    max_heap = []  <span class="comment"># 最大堆存可做项目的利润</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    n = <span class="built_in">len</span>(projects)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">        <span class="comment"># 把当前资本能做的项目加入堆</span></span><br><span class="line">        <span class="keyword">while</span> i &lt; n <span class="keyword">and</span> projects[i][<span class="number">0</span>] &lt;= w:</span><br><span class="line">            heapq.heappush(max_heap, -projects[i][<span class="number">1</span>])  <span class="comment"># 负数模拟最大堆</span></span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> max_heap:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 选利润最大的项目</span></span><br><span class="line">        w += -heapq.heappop(max_heap)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> w</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-1"><a href="#🧠-记忆口诀-1" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“资本够就入堆，贪心选最大利润”</strong></p></blockquote><hr><h2 id="3️⃣-LC-373-查找和最小的-K-对数字-🟡"><a href="#3️⃣-LC-373-查找和最小的-K-对数字-🟡" class="headerlink" title="3️⃣ LC 373. 查找和最小的 K 对数字 🟡"></a>3️⃣ LC 373. 查找和最小的 K 对数字 🟡</h2><h3 id="题目描述-2"><a href="#题目描述-2" class="headerlink" title="题目描述"></a>题目描述</h3><p>从两个升序数组中找出和最小的 k 对数字。</p><h3 id="🎨-图解思路-2"><a href="#🎨-图解思路-2" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">nums1 = [1,7,11], nums2 = [2,4,6], k = 3</span><br><span class="line"></span><br><span class="line">可视化矩阵（和）:</span><br><span class="line">      2   4   6</span><br><span class="line">  1   3   5   7</span><br><span class="line">  7   9  11  13</span><br><span class="line"> 11  13  15  17</span><br><span class="line"></span><br><span class="line">BFS思想：从(0,0)开始，每次扩展右边和下边</span><br><span class="line"></span><br><span class="line">初始: (1+2=3, i=0, j=0)</span><br><span class="line">弹出(3), 加入(1+4=5, 0,1) 和 (7+2=9, 1,0)</span><br><span class="line">弹出(5), 加入(1+6=7, 0,2)</span><br><span class="line">弹出(7), ...</span><br><span class="line"></span><br><span class="line">结果: [(1,2), (1,4), (1,6)]</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-2"><a href="#💻-代码实现-2" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kSmallestPairs</span>(<span class="params">nums1: <span class="built_in">list</span>, nums2: <span class="built_in">list</span>, k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">list</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> nums1 <span class="keyword">or</span> <span class="keyword">not</span> nums2:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line">    </span><br><span class="line">    result = []</span><br><span class="line">    heap = [(nums1[<span class="number">0</span>] + nums2[<span class="number">0</span>], <span class="number">0</span>, <span class="number">0</span>)]  <span class="comment"># (sum, i, j)</span></span><br><span class="line">    visited = {(<span class="number">0</span>, <span class="number">0</span>)}</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> heap <span class="keyword">and</span> <span class="built_in">len</span>(result) &lt; k:</span><br><span class="line">        _, i, j = heapq.heappop(heap)</span><br><span class="line">        result.append([nums1[i], nums2[j]])</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 扩展到右边</span></span><br><span class="line">        <span class="keyword">if</span> i + <span class="number">1</span> &lt; <span class="built_in">len</span>(nums1) <span class="keyword">and</span> (i + <span class="number">1</span>, j) <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">            heapq.heappush(heap, (nums1[i + <span class="number">1</span>] + nums2[j], i + <span class="number">1</span>, j))</span><br><span class="line">            visited.add((i + <span class="number">1</span>, j))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 扩展到下边</span></span><br><span class="line">        <span class="keyword">if</span> j + <span class="number">1</span> &lt; <span class="built_in">len</span>(nums2) <span class="keyword">and</span> (i, j + <span class="number">1</span>) <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">            heapq.heappush(heap, (nums1[i] + nums2[j + <span class="number">1</span>], i, j + <span class="number">1</span>))</span><br><span class="line">            visited.add((i, j + <span class="number">1</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-2"><a href="#🧠-记忆口诀-2" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“矩阵BFS，堆取最小扩展”</strong></p></blockquote><hr><h2 id="4️⃣-LC-295-数据流的中位数-🔴"><a href="#4️⃣-LC-295-数据流的中位数-🔴" class="headerlink" title="4️⃣ LC 295. 数据流的中位数 🔴"></a>4️⃣ LC 295. 数据流的中位数 🔴</h2><h3 id="题目描述-3"><a href="#题目描述-3" class="headerlink" title="题目描述"></a>题目描述</h3><p>设计一个支持添加数字和获取中位数的数据结构。</p><h3 id="🎨-图解思路-3"><a href="#🎨-图解思路-3" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">用两个堆：</span><br><span class="line">- 最大堆 (left): 存较小的一半</span><br><span class="line">- 最小堆 (right): 存较大的一半</span><br><span class="line"></span><br><span class="line">保持: len(left) == len(right) 或 len(left) == len(right) + 1</span><br><span class="line"></span><br><span class="line">数据流: [1, 2, 3]</span><br><span class="line"></span><br><span class="line">add(1): left=[1], right=[]</span><br><span class="line">add(2): left=[1], right=[2]</span><br><span class="line">add(3): left=[1,2], right=[3]</span><br><span class="line"></span><br><span class="line">       left(max)    right(min)</span><br><span class="line">          [2]          [3]</span><br><span class="line">          [1]</span><br><span class="line"></span><br><span class="line">中位数: </span><br><span class="line">- 奇数个: left堆顶</span><br><span class="line">- 偶数个: (left堆顶 + right堆顶) / 2</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-3"><a href="#💻-代码实现-3" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">MedianFinder</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.left = []   <span class="comment"># 最大堆（存负数）</span></span><br><span class="line">        self.right = []  <span class="comment"># 最小堆</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">addNum</span>(<span class="params">self, num: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="comment"># 先加入left</span></span><br><span class="line">        heapq.heappush(self.left, -num)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 把left最大的给right</span></span><br><span class="line">        heapq.heappush(self.right, -heapq.heappop(self.left))</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 平衡：保持left &gt;= right</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.left) &lt; <span class="built_in">len</span>(self.right):</span><br><span class="line">            heapq.heappush(self.left, -heapq.heappop(self.right))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">findMedian</span>(<span class="params">self</span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(self.left) &gt; <span class="built_in">len</span>(self.right):</span><br><span class="line">            <span class="keyword">return</span> -self.left[<span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> (-self.left[<span class="number">0</span>] + self.right[<span class="number">0</span>]) / <span class="number">2</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-3"><a href="#🧠-记忆口诀-3" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“左大右小，平衡保持，堆顶找中位”</strong></p></blockquote><hr><h2 id="5️⃣-LC-23-合并-K-个升序链表-🔴"><a href="#5️⃣-LC-23-合并-K-个升序链表-🔴" class="headerlink" title="5️⃣ LC 23. 合并 K 个升序链表 🔴"></a>5️⃣ LC 23. 合并 K 个升序链表 🔴</h2><h3 id="题目描述-4"><a href="#题目描述-4" class="headerlink" title="题目描述"></a>题目描述</h3><p>将 k 个升序链表合并成一个升序链表。</p><h3 id="🎨-图解思路-4"><a href="#🎨-图解思路-4" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">lists = [[1,4,5], [1,3,4], [2,6]]</span><br><span class="line"></span><br><span class="line">用最小堆维护k个链表的当前头节点</span><br><span class="line"></span><br><span class="line">初始堆: [1, 1, 2] (三个链表头)</span><br><span class="line"></span><br><span class="line">弹出1 → 结果[1]，加入4</span><br><span class="line">堆: [1, 2, 4]</span><br><span class="line"></span><br><span class="line">弹出1 → 结果[1,1]，加入3</span><br><span class="line">堆: [2, 3, 4]</span><br><span class="line"></span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">结果: [1,1,2,3,4,4,5,6]</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-4"><a href="#💻-代码实现-4" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">mergeKLists</span>(<span class="params">lists</span>):</span><br><span class="line">    <span class="comment"># 自定义比较（避免链表节点直接比较）</span></span><br><span class="line">    heap = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i, lst <span class="keyword">in</span> <span class="built_in">enumerate</span>(lists):</span><br><span class="line">        <span class="keyword">if</span> lst:</span><br><span class="line">            heapq.heappush(heap, (lst.val, i, lst))</span><br><span class="line">    </span><br><span class="line">    dummy = ListNode(<span class="number">0</span>)</span><br><span class="line">    curr = dummy</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> heap:</span><br><span class="line">        val, i, node = heapq.heappop(heap)</span><br><span class="line">        curr.<span class="built_in">next</span> = node</span><br><span class="line">        curr = curr.<span class="built_in">next</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> node.<span class="built_in">next</span>:</span><br><span class="line">            heapq.heappush(heap, (node.<span class="built_in">next</span>.val, i, node.<span class="built_in">next</span>))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-4"><a href="#🧠-记忆口诀-4" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“K个头入堆，弹最小接后继”</strong></p></blockquote><hr><h2 id="📊-本章总结"><a href="#📊-本章总结" class="headerlink" title="📊 本章总结"></a>📊 本章总结</h2><h3 id="题目速查表"><a href="#题目速查表" class="headerlink" title="题目速查表"></a>题目速查表</h3><table><thead><tr><th>题号</th><th>题目</th><th>难度</th><th>类型</th></tr></thead><tbody><tr><td>215</td><td>第K个最大元素</td><td>🟡</td><td>Top K</td></tr><tr><td>502</td><td>IPO</td><td>🔴</td><td>贪心+堆</td></tr><tr><td>373</td><td>K对最小和</td><td>🟡</td><td>多路归并</td></tr><tr><td>295</td><td>数据流中位数</td><td>🔴</td><td>双堆</td></tr><tr><td>23</td><td>合并K个链表</td><td>🔴</td><td>多路归并</td></tr></tbody></table><h3 id="堆的解题模式"><a href="#堆的解题模式" class="headerlink" title="堆的解题模式"></a>堆的解题模式</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                    堆的解题模式                              │</span><br><span class="line">├─────────────────────────────────────────────────────────────┤</span><br><span class="line">│                                                             │</span><br><span class="line">│  Top K 问题:                                                │</span><br><span class="line">│  ─────────                                                  │</span><br><span class="line">│  第K大 → 大小为K的最小堆                                    │</span><br><span class="line">│  第K小 → 大小为K的最大堆                                    │</span><br><span class="line">│                                                             │</span><br><span class="line">│  多路归并:                                                  │</span><br><span class="line">│  ─────────                                                  │</span><br><span class="line">│  堆中维护每路的当前元素                                      │</span><br><span class="line">│  每次取最小，然后加入该路的下一个                           │</span><br><span class="line">│                                                             │</span><br><span class="line">│  双堆技巧:                                                  │</span><br><span class="line">│  ─────────                                                  │</span><br><span class="line">│  中位数：左半最大堆 + 右半最小堆                            │</span><br><span class="line">│  滑动窗口中位数同理                                         │</span><br><span class="line">│                                                             │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-全章记忆口诀"><a href="#🧠-全章记忆口诀" class="headerlink" title="🧠 全章记忆口诀"></a>🧠 全章记忆口诀</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">TopK问题堆来凑</span><br><span class="line">第K大用小堆守</span><br><span class="line">多路归并维护头</span><br><span class="line">双堆中位数最溜</span><br><span class="line"></span><br><span class="line">215 - 第K大元素</span><br><span class="line">502 - IPO项目选择</span><br><span class="line">373 - K对最小和</span><br><span class="line">295 - 数据流中位数</span><br><span class="line">23  - 合并K个链表</span><br></pre></td></tr></tbody></table></figure><hr><blockquote><p>📖 <strong>下一篇</strong>：<a href="/2026/01/18/leetcode-150-binary-search/">二分查找专题</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>🎯 LeetCode 面试经典 150 题 - 完全攻略</title>
      <link href="/2026/01/18/leetcode-150-index/"/>
      <url>/2026/01/18/leetcode-150-index/</url>
      
        <content type="html"><![CDATA[<h1 id="🎯-LeetCode-面试经典-150-题-完全攻略"><a href="#🎯-LeetCode-面试经典-150-题-完全攻略" class="headerlink" title="🎯 LeetCode 面试经典 150 题 - 完全攻略"></a>🎯 LeetCode 面试经典 150 题 - 完全攻略</h1><blockquote><p>📚 <strong>目标</strong>：系统掌握算法面试核心知识点，6个月拿下大厂 Offer！</p></blockquote><hr><h2 id="📊-题目分类总览"><a href="#📊-题目分类总览" class="headerlink" title="📊 题目分类总览"></a>📊 题目分类总览</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                   LeetCode 面试 150 题                        │</span><br><span class="line">├─────────────┬─────────────┬─────────────┬─────────────────────┤</span><br><span class="line">│   数组/字符串  │   双指针     │   滑动窗口   │      矩阵           │</span><br><span class="line">│    (15题)    │   (5题)     │   (4题)     │     (4题)          │</span><br><span class="line">├─────────────┼─────────────┼─────────────┼─────────────────────┤</span><br><span class="line">│   哈希表      │    区间      │     栈      │      链表           │</span><br><span class="line">│    (9题)     │   (6题)     │   (7题)     │     (11题)         │</span><br><span class="line">├─────────────┼─────────────┼─────────────┼─────────────────────┤</span><br><span class="line">│   二叉树      │  二叉树BFS   │  二叉搜索树  │      图             │</span><br><span class="line">│   (14题)     │   (4题)     │   (5题)     │     (8题)          │</span><br><span class="line">├─────────────┼─────────────┼─────────────┼─────────────────────┤</span><br><span class="line">│   回溯       │  分治       │  Kadane算法  │    二分查找          │</span><br><span class="line">│   (7题)      │  (3题)      │   (2题)     │     (7题)          │</span><br><span class="line">├─────────────┼─────────────┼─────────────┼─────────────────────┤</span><br><span class="line">│    堆       │   位运算     │    数学      │    一维动态规划       │</span><br><span class="line">│   (5题)      │  (6题)      │   (9题)     │     (10题)         │</span><br><span class="line">├─────────────┴─────────────┴─────────────┴─────────────────────┤</span><br><span class="line">│                      多维动态规划 (12题)                        │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="🗺️-学习路线图"><a href="#🗺️-学习路线图" class="headerlink" title="🗺️ 学习路线图"></a>🗺️ 学习路线图</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">Week 1-2          Week 3-4          Week 5-6          Week 7-8</span><br><span class="line">   │                 │                 │                 │</span><br><span class="line">   ▼                 ▼                 ▼                 ▼</span><br><span class="line">┌──────┐        ┌──────┐        ┌──────┐        ┌──────┐</span><br><span class="line">│ 数组  │───────▶│ 链表  │───────▶│ 树   │───────▶│ 图   │</span><br><span class="line">│ 字符串 │        │ 栈   │        │ 堆   │        │ 回溯  │</span><br><span class="line">│ 双指针 │        │ 队列  │        │ BFS  │        │ DFS  │</span><br><span class="line">└──────┘        └──────┘        └──────┘        └──────┘</span><br><span class="line">                                                    │</span><br><span class="line">                    ┌───────────────────────────────┘</span><br><span class="line">                    ▼</span><br><span class="line">             Week 9-12</span><br><span class="line">          ┌──────────────┐</span><br><span class="line">          │   动态规划    │</span><br><span class="line">          │   二分查找    │</span><br><span class="line">          │   位运算     │</span><br><span class="line">          └──────────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="📑-分类详解目录"><a href="#📑-分类详解目录" class="headerlink" title="📑 分类详解目录"></a>📑 分类详解目录</h2><h3 id="1️⃣-数组与字符串-15题"><a href="#1️⃣-数组与字符串-15题" class="headerlink" title="1️⃣ 数组与字符串 (15题)"></a>1️⃣ 数组与字符串 (15题)</h3><table><thead><tr><th>#</th><th>题目</th><th>难度</th><th>核心技巧</th></tr></thead><tbody><tr><td>88</td><td>合并两个有序数组</td><td>🟢</td><td>双指针从后往前</td></tr><tr><td>27</td><td>移除元素</td><td>🟢</td><td>快慢指针</td></tr><tr><td>26</td><td>删除有序数组中的重复项</td><td>🟢</td><td>快慢指针</td></tr><tr><td>80</td><td>删除有序数组中的重复项 II</td><td>🟡</td><td>通用模板</td></tr><tr><td>169</td><td>多数元素</td><td>🟢</td><td>Boyer-Moore投票</td></tr><tr><td>189</td><td>轮转数组</td><td>🟡</td><td>三次翻转</td></tr><tr><td>121</td><td>买卖股票的最佳时机</td><td>🟢</td><td>维护最小值</td></tr><tr><td>122</td><td>买卖股票的最佳时机 II</td><td>🟡</td><td>贪心累加</td></tr><tr><td>55</td><td>跳跃游戏</td><td>🟡</td><td>贪心最远距离</td></tr><tr><td>45</td><td>跳跃游戏 II</td><td>🟡</td><td>BFS思想</td></tr><tr><td>274</td><td>H 指数</td><td>🟡</td><td>计数排序</td></tr><tr><td>380</td><td>O(1) 时间插入、删除和获取随机元素</td><td>🟡</td><td>哈希+数组</td></tr><tr><td>238</td><td>除自身以外数组的乘积</td><td>🟡</td><td>前缀积+后缀积</td></tr><tr><td>134</td><td>加油站</td><td>🟡</td><td>一次遍历</td></tr><tr><td>135</td><td>分发糖果</td><td>🔴</td><td>两次遍历</td></tr></tbody></table><p><strong>🧠 记忆口诀</strong>：</p><blockquote><p><strong>“合移删删多，轮买买跳跳，H插除加糖”</strong></p></blockquote><hr><h3 id="2️⃣-双指针-5题"><a href="#2️⃣-双指针-5题" class="headerlink" title="2️⃣ 双指针 (5题)"></a>2️⃣ 双指针 (5题)</h3><table><thead><tr><th>#</th><th>题目</th><th>难度</th><th>核心技巧</th></tr></thead><tbody><tr><td>125</td><td>验证回文串</td><td>🟢</td><td>相向双指针</td></tr><tr><td>392</td><td>判断子序列</td><td>🟢</td><td>同向双指针</td></tr><tr><td>167</td><td>两数之和 II</td><td>🟡</td><td>相向双指针</td></tr><tr><td>11</td><td>盛最多水的容器</td><td>🟡</td><td>贪心+双指针</td></tr><tr><td>15</td><td>三数之和</td><td>🟡</td><td>排序+双指针</td></tr></tbody></table><p><strong>🎨 双指针模式图解</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">相向双指针 (Two Sum, Container)     同向双指针 (Fast-Slow)</span><br><span class="line">    L ──────────────▶ R              Slow ──▶ Fast ──▶</span><br><span class="line">    │                 │                │        │</span><br><span class="line">    ▼                 ▼                ▼        ▼</span><br><span class="line">  [1, 2, 3, 4, 5, 6, 7]            [1, 2, 3, 4, 5, 6, 7]</span><br></pre></td></tr></tbody></table></figure><p><strong>🧠 记忆口诀</strong>：</p><blockquote><p><strong>“验判两盛三，相向加同向”</strong></p></blockquote><hr><h3 id="3️⃣-滑动窗口-4题"><a href="#3️⃣-滑动窗口-4题" class="headerlink" title="3️⃣ 滑动窗口 (4题)"></a>3️⃣ 滑动窗口 (4题)</h3><table><thead><tr><th>#</th><th>题目</th><th>难度</th><th>核心技巧</th></tr></thead><tbody><tr><td>209</td><td>长度最小的子数组</td><td>🟡</td><td>可变窗口</td></tr><tr><td>3</td><td>无重复字符的最长子串</td><td>🟡</td><td>哈希+滑窗</td></tr><tr><td>30</td><td>串联所有单词的子串</td><td>🔴</td><td>固定窗口</td></tr><tr><td>76</td><td>最小覆盖子串</td><td>🔴</td><td>可变窗口</td></tr></tbody></table><p><strong>🎨 滑动窗口模板</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sliding_window</span>(<span class="params">s</span>):</span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    window = {}  <span class="comment"># 窗口内容</span></span><br><span class="line">    result = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> right <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">        <span class="comment"># 1. 右边界扩张，更新窗口</span></span><br><span class="line">        window[s[right]] = window.get(s[right], <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 2. 满足收缩条件时，左边界收缩</span></span><br><span class="line">        <span class="keyword">while</span> need_shrink(window):</span><br><span class="line">            window[s[left]] -= <span class="number">1</span></span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 3. 更新结果</span></span><br><span class="line">        result = <span class="built_in">max</span>(result, right - left + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><p><strong>🧠 记忆口诀</strong>：</p><blockquote><p><strong>“右扩左缩更新果，窗口滑动解最优”</strong></p></blockquote><hr><h3 id="4️⃣-哈希表-9题"><a href="#4️⃣-哈希表-9题" class="headerlink" title="4️⃣ 哈希表 (9题)"></a>4️⃣ 哈希表 (9题)</h3><table><thead><tr><th>#</th><th>题目</th><th>难度</th><th>核心技巧</th></tr></thead><tbody><tr><td>383</td><td>赎金信</td><td>🟢</td><td>字符计数</td></tr><tr><td>205</td><td>同构字符串</td><td>🟢</td><td>双向映射</td></tr><tr><td>290</td><td>单词规律</td><td>🟢</td><td>双向映射</td></tr><tr><td>242</td><td>有效的字母异位词</td><td>🟢</td><td>字符计数</td></tr><tr><td>49</td><td>字母异位词分组</td><td>🟡</td><td>排序作key</td></tr><tr><td>1</td><td>两数之和</td><td>🟢</td><td>补数查找</td></tr><tr><td>202</td><td>快乐数</td><td>🟢</td><td>检测循环</td></tr><tr><td>219</td><td>存在重复元素 II</td><td>🟢</td><td>滑动窗口</td></tr><tr><td>128</td><td>最长连续序列</td><td>🟡</td><td>并查集思想</td></tr></tbody></table><p><strong>🧠 记忆口诀</strong>：</p><blockquote><p><strong>“赎同单有组，两快存最长”</strong></p></blockquote><hr><h3 id="5️⃣-栈-7题"><a href="#5️⃣-栈-7题" class="headerlink" title="5️⃣ 栈 (7题)"></a>5️⃣ 栈 (7题)</h3><table><thead><tr><th>#</th><th>题目</th><th>难度</th><th>核心技巧</th></tr></thead><tbody><tr><td>20</td><td>有效的括号</td><td>🟢</td><td>匹配出栈</td></tr><tr><td>71</td><td>简化路径</td><td>🟡</td><td>模拟栈</td></tr><tr><td>155</td><td>最小栈</td><td>🟡</td><td>辅助栈</td></tr><tr><td>150</td><td>逆波兰表达式求值</td><td>🟡</td><td>操作数栈</td></tr><tr><td>224</td><td>基本计算器</td><td>🔴</td><td>递归/栈</td></tr><tr><td>227</td><td>基本计算器 II</td><td>🟡</td><td>优先级栈</td></tr><tr><td>772</td><td>基本计算器 III</td><td>🔴</td><td>递归</td></tr></tbody></table><p><strong>🎨 栈的核心思想</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">后进先出 (LIFO)</span><br><span class="line">              │ Push</span><br><span class="line">              ▼</span><br><span class="line">        ┌─────────┐</span><br><span class="line">        │   Top   │  ◀── Pop</span><br><span class="line">        ├─────────┤</span><br><span class="line">        │         │</span><br><span class="line">        ├─────────┤</span><br><span class="line">        │         │</span><br><span class="line">        └─────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="6️⃣-链表-11题"><a href="#6️⃣-链表-11题" class="headerlink" title="6️⃣ 链表 (11题)"></a>6️⃣ 链表 (11题)</h3><table><thead><tr><th>#</th><th>题目</th><th>难度</th><th>核心技巧</th></tr></thead><tbody><tr><td>141</td><td>环形链表</td><td>🟢</td><td>快慢指针</td></tr><tr><td>2</td><td>两数相加</td><td>🟡</td><td>进位处理</td></tr><tr><td>21</td><td>合并两个有序链表</td><td>🟢</td><td>双指针</td></tr><tr><td>138</td><td>随机链表的复制</td><td>🟡</td><td>哈希/交织</td></tr><tr><td>92</td><td>反转链表 II</td><td>🟡</td><td>头插法</td></tr><tr><td>25</td><td>K 个一组翻转链表</td><td>🔴</td><td>分组反转</td></tr><tr><td>19</td><td>删除链表的倒数第 N 个结点</td><td>🟡</td><td>快慢指针</td></tr><tr><td>82</td><td>删除排序链表中的重复元素 II</td><td>🟡</td><td>虚拟头节点</td></tr><tr><td>61</td><td>旋转链表</td><td>🟡</td><td>成环断开</td></tr><tr><td>86</td><td>分隔链表</td><td>🟡</td><td>双链表</td></tr><tr><td>146</td><td>LRU 缓存</td><td>🟡</td><td>哈希+双向链表</td></tr></tbody></table><p><strong>🎨 链表反转核心代码</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reverse</span>(<span class="params">head</span>):</span><br><span class="line">    prev, curr = <span class="literal">None</span>, head</span><br><span class="line">    <span class="keyword">while</span> curr:</span><br><span class="line">        next_temp = curr.<span class="built_in">next</span>  <span class="comment"># 1. 保存下一个</span></span><br><span class="line">        curr.<span class="built_in">next</span> = prev       <span class="comment"># 2. 反转指向</span></span><br><span class="line">        prev = curr            <span class="comment"># 3. prev前进</span></span><br><span class="line">        curr = next_temp       <span class="comment"># 4. curr前进</span></span><br><span class="line">    <span class="keyword">return</span> prev</span><br></pre></td></tr></tbody></table></figure><p><strong>🧠 记忆口诀</strong>：</p><blockquote><p><strong>“存反前进，四步反转”</strong></p></blockquote><hr><h3 id="7️⃣-二叉树-14题"><a href="#7️⃣-二叉树-14题" class="headerlink" title="7️⃣ 二叉树 (14题)"></a>7️⃣ 二叉树 (14题)</h3><table><thead><tr><th>#</th><th>题目</th><th>难度</th><th>核心技巧</th></tr></thead><tbody><tr><td>104</td><td>二叉树的最大深度</td><td>🟢</td><td>DFS递归</td></tr><tr><td>100</td><td>相同的树</td><td>🟢</td><td>递归比较</td></tr><tr><td>226</td><td>翻转二叉树</td><td>🟢</td><td>递归交换</td></tr><tr><td>101</td><td>对称二叉树</td><td>🟢</td><td>镜像递归</td></tr><tr><td>105</td><td>从前序与中序遍历序列构造二叉树</td><td>🟡</td><td>分治</td></tr><tr><td>106</td><td>从中序与后序遍历序列构造二叉树</td><td>🟡</td><td>分治</td></tr><tr><td>117</td><td>填充每个节点的下一个右侧节点指针 II</td><td>🟡</td><td>BFS层序</td></tr><tr><td>114</td><td>二叉树展开为链表</td><td>🟡</td><td>前序遍历</td></tr><tr><td>112</td><td>路径总和</td><td>🟢</td><td>DFS</td></tr><tr><td>129</td><td>求根节点到叶节点数字之和</td><td>🟡</td><td>DFS</td></tr><tr><td>124</td><td>二叉树中的最大路径和</td><td>🔴</td><td>后序遍历</td></tr><tr><td>173</td><td>二叉搜索树迭代器</td><td>🟡</td><td>栈模拟</td></tr><tr><td>222</td><td>完全二叉树的节点个数</td><td>🟡</td><td>二分+位运算</td></tr><tr><td>236</td><td>二叉树的最近公共祖先</td><td>🟡</td><td>递归</td></tr></tbody></table><p><strong>🎨 树的遍历方式</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"></span><br></pre></td></tr></tbody></table></figure><pre><code>   / \  2   3 / \4   5</code></pre><p>前序(根左右): 1 → 2 → 4 → 5 → 3<br>中序(左根右): 4 → 2 → 5 → 1 → 3<br>后序(左右根): 4 → 5 → 2 → 3 → 1<br>层序(BFS):   1 → 2 → 3 → 4 → 5</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">---</span><br><span class="line">### 8️⃣ 图与搜索 (8题)</span><br><span class="line">| # | 题目 | 难度 | 核心技巧 |</span><br></pre></td></tr></tbody></table></figure><p>| 200 | 岛屿数量 | 🟡 | DFS/BFS |<br>| 130 | 被围绕的区域 | 🟡 | 边界DFS |<br>| 133 | 克隆图 | 🟡 | BFS+哈希 |<br>| 399 | 除法求值 | 🟡 | 带权图BFS |<br>| 207 | 课程表 | 🟡 | 拓扑排序 |<br>| 210 | 课程表 II | 🟡 | 拓扑排序 |<br>| 909 | 蛇梯棋 | 🟡 | BFS最短路 |<br>| 433 | 最小基因变化 | 🟡 | BFS |</p><p><strong>🎨 BFS 模板</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> deque</span><br><span class="line"></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">bfs</span>(<span class="params">start</span>):</span><br><span class="line">    queue = deque([start])</span><br><span class="line">    visited = {start}</span><br><span class="line">    level = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> queue:</span><br><span class="line">        size = <span class="built_in">len</span>(queue)</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(size):</span><br><span class="line">            node = queue.popleft()</span><br><span class="line">            <span class="comment"># 处理当前节点</span></span><br><span class="line">            <span class="keyword">for</span> neighbor <span class="keyword">in</span> get_neighbors(node):</span><br><span class="line">                <span class="keyword">if</span> neighbor <span class="keyword">not</span> <span class="keyword">in</span> visited:</span><br><span class="line">                    visited.add(neighbor)</span><br><span class="line">                    queue.append(neighbor)</span><br><span class="line">        level += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> level</span><br></pre></td></tr></tbody></table></figure><hr><h3 id="9️⃣-回溯算法-7题"><a href="#9️⃣-回溯算法-7题" class="headerlink" title="9️⃣ 回溯算法 (7题)"></a>9️⃣ 回溯算法 (7题)</h3><table><thead><tr><th>#</th><th>题目</th><th>难度</th><th>核心技巧</th></tr></thead><tbody><tr><td>17</td><td>电话号码的字母组合</td><td>🟡</td><td>回溯枚举</td></tr><tr><td>77</td><td>组合</td><td>🟡</td><td>回溯+剪枝</td></tr><tr><td>46</td><td>全排列</td><td>🟡</td><td>回溯</td></tr><tr><td>39</td><td>组合总和</td><td>🟡</td><td>回溯</td></tr><tr><td>52</td><td>N 皇后 II</td><td>🔴</td><td>回溯+剪枝</td></tr><tr><td>22</td><td>括号生成</td><td>🟡</td><td>回溯</td></tr><tr><td>79</td><td>单词搜索</td><td>🟡</td><td>回溯+DFS</td></tr></tbody></table><p><strong>🎨 回溯模板</strong>：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">backtrack</span>(<span class="params">path, choices</span>):</span><br><span class="line">    <span class="keyword">if</span> 满足结束条件:</span><br><span class="line">        result.append(path[:])</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> choice <span class="keyword">in</span> choices:</span><br><span class="line">        <span class="comment"># 做选择</span></span><br><span class="line">        path.append(choice)</span><br><span class="line">        <span class="comment"># 递归</span></span><br><span class="line">        backtrack(path, new_choices)</span><br><span class="line">        <span class="comment"># 撤销选择</span></span><br><span class="line">        path.pop()</span><br></pre></td></tr></tbody></table></figure><p><strong>🧠 记忆口诀</strong>：</p><blockquote><p><strong>“选择、递归、撤销，回溯三部曲”</strong></p></blockquote><hr><h3 id="🔟-动态规划-22题"><a href="#🔟-动态规划-22题" class="headerlink" title="🔟 动态规划 (22题)"></a>🔟 动态规划 (22题)</h3><h4 id="一维-DP-10题"><a href="#一维-DP-10题" class="headerlink" title="一维 DP (10题)"></a>一维 DP (10题)</h4><table><thead><tr><th>#</th><th>题目</th><th>难度</th><th>核心技巧</th></tr></thead><tbody><tr><td>70</td><td>爬楼梯</td><td>🟢</td><td>斐波那契</td></tr><tr><td>198</td><td>打家劫舍</td><td>🟡</td><td>状态转移</td></tr><tr><td>139</td><td>单词拆分</td><td>🟡</td><td>背包变形</td></tr><tr><td>322</td><td>零钱兑换</td><td>🟡</td><td>完全背包</td></tr><tr><td>300</td><td>最长递增子序列</td><td>🟡</td><td>LIS</td></tr><tr><td>152</td><td>乘积最大子数组</td><td>🟡</td><td>维护最大最小</td></tr></tbody></table><h4 id="多维-DP-12题"><a href="#多维-DP-12题" class="headerlink" title="多维 DP (12题)"></a>多维 DP (12题)</h4><table><thead><tr><th>#</th><th>题目</th><th>难度</th><th>核心技巧</th></tr></thead><tbody><tr><td>120</td><td>三角形最小路径和</td><td>🟡</td><td>自底向上</td></tr><tr><td>64</td><td>最小路径和</td><td>🟡</td><td>网格DP</td></tr><tr><td>63</td><td>不同路径 II</td><td>🟡</td><td>网格DP</td></tr><tr><td>5</td><td>最长回文子串</td><td>🟡</td><td>区间DP</td></tr><tr><td>72</td><td>编辑距离</td><td>🟡</td><td>经典DP</td></tr><tr><td>97</td><td>交错字符串</td><td>🟡</td><td>二维DP</td></tr><tr><td>123</td><td>买卖股票的最佳时机 III</td><td>🔴</td><td>状态机DP</td></tr></tbody></table><p><strong>🎨 DP 思考框架</strong>：</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">1. 定义状态 ──▶ dp[i] 表示什么？</span><br><span class="line">      │</span><br><span class="line">      ▼</span><br><span class="line">2. 状态转移 ──▶ dp[i] 和 dp[i-1] 的关系？</span><br><span class="line">      │</span><br><span class="line">      ▼</span><br><span class="line">3. 初始条件 ──▶ dp[0] = ?</span><br><span class="line">      │</span><br><span class="line">      ▼</span><br><span class="line">4. 计算顺序 ──▶ 从小到大 or 从大到小？</span><br><span class="line">      │</span><br><span class="line">      ▼</span><br><span class="line">5. 返回结果 ──▶ dp[n] or max(dp)?</span><br></pre></td></tr></tbody></table></figure><p><strong>🧠 记忆口诀</strong>：</p><blockquote><p><strong>“定状态、找转移、设初值、定顺序、取结果”</strong></p></blockquote><hr><h2 id="🏆-刷题进度追踪"><a href="#🏆-刷题进度追踪" class="headerlink" title="🏆 刷题进度追踪"></a>🏆 刷题进度追踪</h2><table><thead><tr><th>分类</th><th>总题数</th><th>已完成</th><th>进度</th></tr></thead><tbody><tr><td>数组/字符串</td><td>15</td><td>⬜</td><td>0%</td></tr><tr><td>双指针</td><td>5</td><td>⬜</td><td>0%</td></tr><tr><td>滑动窗口</td><td>4</td><td>⬜</td><td>0%</td></tr><tr><td>哈希表</td><td>9</td><td>⬜</td><td>0%</td></tr><tr><td>栈</td><td>7</td><td>⬜</td><td>0%</td></tr><tr><td>链表</td><td>11</td><td>⬜</td><td>0%</td></tr><tr><td>二叉树</td><td>14</td><td>⬜</td><td>0%</td></tr><tr><td>图/搜索</td><td>8</td><td>⬜</td><td>0%</td></tr><tr><td>回溯</td><td>7</td><td>⬜</td><td>0%</td></tr><tr><td>动态规划</td><td>22</td><td>⬜</td><td>0%</td></tr><tr><td>其他</td><td>48</td><td>⬜</td><td>0%</td></tr><tr><td><strong>总计</strong></td><td><strong>150</strong></td><td><strong>0</strong></td><td><strong>0%</strong></td></tr></tbody></table><hr><h2 id="📚-推荐学习资源"><a href="#📚-推荐学习资源" class="headerlink" title="📚 推荐学习资源"></a>📚 推荐学习资源</h2><ol><li><strong><a href="https://leetcode.cn/studyplan/top-interview-150/">LeetCode 官方题单</a></strong> - 原题链接</li><li><strong><a href="https://programmercarl.com/">代码随想录</a></strong> - 详细图解</li><li><strong><a href="https://labuladong.gitee.io/algo/">labuladong 的算法小抄</a></strong> - 框架思维</li></ol><hr><h2 id="🎯-复习计划"><a href="#🎯-复习计划" class="headerlink" title="🎯 复习计划"></a>🎯 复习计划</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">每日任务：</span><br><span class="line">┌────────────────────────────────────────┐</span><br><span class="line">│  📅 工作日：2-3 题 (1小时)              │</span><br><span class="line">│  📅 周末：复习本周题目 + 5题新题          │</span><br><span class="line">│  📅 月末：分类专项突破 + 模拟面试          │</span><br><span class="line">└────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><hr><blockquote><p>💪 <strong>加油！坚持刷完这 150 题，大厂 Offer 在向你招手！</strong></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>🔗 LeetCode 150 - 链表专题</title>
      <link href="/2026/01/18/leetcode-150-linked-list/"/>
      <url>/2026/01/18/leetcode-150-linked-list/</url>
      
        <content type="html"><![CDATA[<h1 id="🔗-链表专题-11题"><a href="#🔗-链表专题-11题" class="headerlink" title="🔗 链表专题 (11题)"></a>🔗 链表专题 (11题)</h1><blockquote><p>🎯 <strong>核心技巧</strong>：虚拟头节点、快慢指针、链表反转、合并链表</p></blockquote><hr><h2 id="🗺️-链表核心操作图解"><a href="#🗺️-链表核心操作图解" class="headerlink" title="🗺️ 链表核心操作图解"></a>🗺️ 链表核心操作图解</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                     链表基本操作                             │</span><br><span class="line">├─────────────────────────────────────────────────────────────┤</span><br><span class="line">│                                                             │</span><br><span class="line">│  遍历:  [1] → [2] → [3] → [4] → None                       │</span><br><span class="line">│          ↑                                                  │</span><br><span class="line">│         cur (cur = cur.next)                               │</span><br><span class="line">│                                                             │</span><br><span class="line">│  插入:  [1] → [X] → [2]   (先接后断)                        │</span><br><span class="line">│              ↗   ↘                                         │</span><br><span class="line">│                                                             │</span><br><span class="line">│  删除:  [1] ──────→ [3]   (跨过中间节点)                    │</span><br><span class="line">│              ╳[2]                                          │</span><br><span class="line">│                                                             │</span><br><span class="line">│  反转:  [1] ← [2] ← [3] ← [4]                              │</span><br><span class="line">│                                                             │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="🔧-链表万能技巧"><a href="#🔧-链表万能技巧" class="headerlink" title="🔧 链表万能技巧"></a>🔧 链表万能技巧</h2><h3 id="技巧1-虚拟头节点-Dummy-Head"><a href="#技巧1-虚拟头节点-Dummy-Head" class="headerlink" title="技巧1: 虚拟头节点 (Dummy Head)"></a>技巧1: 虚拟头节点 (Dummy Head)</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 避免处理头节点的特殊情况</span></span><br><span class="line">dummy = ListNode(<span class="number">0</span>)</span><br><span class="line">dummy.<span class="built_in">next</span> = head</span><br><span class="line"><span class="comment"># ... 操作链表</span></span><br><span class="line"><span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br></pre></td></tr></tbody></table></figure><h3 id="技巧2-快慢指针"><a href="#技巧2-快慢指针" class="headerlink" title="技巧2: 快慢指针"></a>技巧2: 快慢指针</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 找中点、判断环、找环入口</span></span><br><span class="line">slow = fast = head</span><br><span class="line"><span class="keyword">while</span> fast <span class="keyword">and</span> fast.<span class="built_in">next</span>:</span><br><span class="line">    slow = slow.<span class="built_in">next</span></span><br><span class="line">    fast = fast.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line"><span class="comment"># slow 就是中点</span></span><br></pre></td></tr></tbody></table></figure><h3 id="技巧3-链表反转"><a href="#技巧3-链表反转" class="headerlink" title="技巧3: 链表反转"></a>技巧3: 链表反转</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reverse</span>(<span class="params">head</span>):</span><br><span class="line">    prev, curr = <span class="literal">None</span>, head</span><br><span class="line">    <span class="keyword">while</span> curr:</span><br><span class="line">        next_temp = curr.<span class="built_in">next</span></span><br><span class="line">        curr.<span class="built_in">next</span> = prev</span><br><span class="line">        prev = curr</span><br><span class="line">        curr = next_temp</span><br><span class="line">    <span class="keyword">return</span> prev</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="1️⃣-LC-141-环形链表-🟢"><a href="#1️⃣-LC-141-环形链表-🟢" class="headerlink" title="1️⃣ LC 141. 环形链表 🟢"></a>1️⃣ LC 141. 环形链表 🟢</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>判断链表中是否有环。</p><h3 id="🎨-图解思路"><a href="#🎨-图解思路" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">快慢指针：如果有环，快指针一定会追上慢指针</span><br><span class="line"></span><br><span class="line">    ┌───────────────┐</span><br><span class="line">    ▼               │</span><br><span class="line">[1] → [2] → [3] → [4]</span><br><span class="line">        ↑     ↑</span><br><span class="line">       slow  fast</span><br><span class="line"></span><br><span class="line">快指针每次走2步，慢指针每次走1步</span><br><span class="line">相对速度为1，一定会在环内相遇</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现"><a href="#💻-代码实现" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">hasCycle</span>(<span class="params">head: ListNode</span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">    slow = fast = head</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> fast <span class="keyword">and</span> fast.<span class="built_in">next</span>:</span><br><span class="line">        slow = slow.<span class="built_in">next</span></span><br><span class="line">        fast = fast.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> slow == fast:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀"><a href="#🧠-记忆口诀" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“快慢追逐，相遇有环”</strong></p></blockquote><hr><h2 id="2️⃣-LC-2-两数相加-🟡"><a href="#2️⃣-LC-2-两数相加-🟡" class="headerlink" title="2️⃣ LC 2. 两数相加 🟡"></a>2️⃣ LC 2. 两数相加 🟡</h2><h3 id="题目描述-1"><a href="#题目描述-1" class="headerlink" title="题目描述"></a>题目描述</h3><p>两个逆序存储的链表相加，返回结果链表。</p><h3 id="🎨-图解思路-1"><a href="#🎨-图解思路-1" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">   2 → 4 → 3  (表示 342)</span><br><span class="line"> + 5 → 6 → 4  (表示 465)</span><br><span class="line"> ─────────────</span><br><span class="line">   7 → 0 → 8  (表示 807)</span><br><span class="line"></span><br><span class="line">从头到尾逐位相加，注意进位！</span><br><span class="line"></span><br><span class="line">Step 1: 2 + 5 = 7, 进位0</span><br><span class="line">Step 2: 4 + 6 = 10, 写0进1  </span><br><span class="line">Step 3: 3 + 4 + 1 = 8, 进位0</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-1"><a href="#💻-代码实现-1" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">addTwoNumbers</span>(<span class="params">l1: ListNode, l2: ListNode</span>) -&gt; ListNode:</span><br><span class="line">    dummy = ListNode(<span class="number">0</span>)</span><br><span class="line">    curr = dummy</span><br><span class="line">    carry = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> l1 <span class="keyword">or</span> l2 <span class="keyword">or</span> carry:</span><br><span class="line">        <span class="comment"># 取值（链表可能长度不同）</span></span><br><span class="line">        val1 = l1.val <span class="keyword">if</span> l1 <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        val2 = l2.val <span class="keyword">if</span> l2 <span class="keyword">else</span> <span class="number">0</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 计算和与进位</span></span><br><span class="line">        total = val1 + val2 + carry</span><br><span class="line">        carry = total // <span class="number">10</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 创建新节点</span></span><br><span class="line">        curr.<span class="built_in">next</span> = ListNode(total % <span class="number">10</span>)</span><br><span class="line">        curr = curr.<span class="built_in">next</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 移动指针</span></span><br><span class="line">        l1 = l1.<span class="built_in">next</span> <span class="keyword">if</span> l1 <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">        l2 = l2.<span class="built_in">next</span> <span class="keyword">if</span> l2 <span class="keyword">else</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-1"><a href="#🧠-记忆口诀-1" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“逐位相加，别忘进位”</strong></p></blockquote><hr><h2 id="3️⃣-LC-21-合并两个有序链表-🟢"><a href="#3️⃣-LC-21-合并两个有序链表-🟢" class="headerlink" title="3️⃣ LC 21. 合并两个有序链表 🟢"></a>3️⃣ LC 21. 合并两个有序链表 🟢</h2><h3 id="题目描述-2"><a href="#题目描述-2" class="headerlink" title="题目描述"></a>题目描述</h3><p>将两个升序链表合并为一个新的升序链表。</p><h3 id="🎨-图解思路-2"><a href="#🎨-图解思路-2" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">l1: 1 → 2 → 4</span><br><span class="line">l2: 1 → 3 → 4</span><br><span class="line"></span><br><span class="line">比较头节点，小的接上去：</span><br><span class="line"></span><br><span class="line">dummy → 1 → 1 → 2 → 3 → 4 → 4</span><br><span class="line">        ↑   ↑   ↑   ↑   ↑   ↑</span><br><span class="line">       l1  l2  l1  l2  l1  l2</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-2"><a href="#💻-代码实现-2" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">mergeTwoLists</span>(<span class="params">l1: ListNode, l2: ListNode</span>) -&gt; ListNode:</span><br><span class="line">    dummy = ListNode(<span class="number">0</span>)</span><br><span class="line">    curr = dummy</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> l1 <span class="keyword">and</span> l2:</span><br><span class="line">        <span class="keyword">if</span> l1.val &lt;= l2.val:</span><br><span class="line">            curr.<span class="built_in">next</span> = l1</span><br><span class="line">            l1 = l1.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            curr.<span class="built_in">next</span> = l2</span><br><span class="line">            l2 = l2.<span class="built_in">next</span></span><br><span class="line">        curr = curr.<span class="built_in">next</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 接上剩余部分</span></span><br><span class="line">    curr.<span class="built_in">next</span> = l1 <span class="keyword">if</span> l1 <span class="keyword">else</span> l2</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-2"><a href="#🧠-记忆口诀-2" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“比较取小，剩余直接接”</strong></p></blockquote><hr><h2 id="4️⃣-LC-138-随机链表的复制-🟡"><a href="#4️⃣-LC-138-随机链表的复制-🟡" class="headerlink" title="4️⃣ LC 138. 随机链表的复制 🟡"></a>4️⃣ LC 138. 随机链表的复制 🟡</h2><h3 id="题目描述-3"><a href="#题目描述-3" class="headerlink" title="题目描述"></a>题目描述</h3><p>深拷贝带有随机指针的链表。</p><h3 id="🎨-图解思路-3"><a href="#🎨-图解思路-3" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">方法：哈希表存储映射关系</span><br><span class="line"></span><br><span class="line">原链表:  [1] → [2] → [3]</span><br><span class="line">          ↓     ↓     ↓ (random)</span><br><span class="line">         [3]   [1]   None</span><br><span class="line"></span><br><span class="line">Step 1: 创建所有新节点，建立 old→new 映射</span><br><span class="line">Step 2: 连接 next 和 random 指针</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-3"><a href="#💻-代码实现-3" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">copyRandomList</span>(<span class="params">head: <span class="string">'Node'</span></span>) -&gt; <span class="string">'Node'</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> head:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 建立映射: 旧节点 → 新节点</span></span><br><span class="line">    old_to_new = {}</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第一遍：创建所有新节点</span></span><br><span class="line">    curr = head</span><br><span class="line">    <span class="keyword">while</span> curr:</span><br><span class="line">        old_to_new[curr] = Node(curr.val)</span><br><span class="line">        curr = curr.<span class="built_in">next</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 第二遍：连接 next 和 random</span></span><br><span class="line">    curr = head</span><br><span class="line">    <span class="keyword">while</span> curr:</span><br><span class="line">        new_node = old_to_new[curr]</span><br><span class="line">        new_node.<span class="built_in">next</span> = old_to_new.get(curr.<span class="built_in">next</span>)</span><br><span class="line">        new_node.random = old_to_new.get(curr.random)</span><br><span class="line">        curr = curr.<span class="built_in">next</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> old_to_new[head]</span><br></pre></td></tr></tbody></table></figure><h3 id="🔥-O-1-空间解法"><a href="#🔥-O-1-空间解法" class="headerlink" title="🔥 O(1) 空间解法"></a>🔥 O(1) 空间解法</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">copyRandomList</span>(<span class="params">head: <span class="string">'Node'</span></span>) -&gt; <span class="string">'Node'</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> head:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 1: 在每个节点后插入复制节点</span></span><br><span class="line">    <span class="comment"># A → A' → B → B' → C → C'</span></span><br><span class="line">    curr = head</span><br><span class="line">    <span class="keyword">while</span> curr:</span><br><span class="line">        new_node = Node(curr.val, curr.<span class="built_in">next</span>)</span><br><span class="line">        curr.<span class="built_in">next</span> = new_node</span><br><span class="line">        curr = new_node.<span class="built_in">next</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 2: 设置 random 指针</span></span><br><span class="line">    curr = head</span><br><span class="line">    <span class="keyword">while</span> curr:</span><br><span class="line">        <span class="keyword">if</span> curr.random:</span><br><span class="line">            curr.<span class="built_in">next</span>.random = curr.random.<span class="built_in">next</span></span><br><span class="line">        curr = curr.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># Step 3: 拆分链表</span></span><br><span class="line">    dummy = Node(<span class="number">0</span>)</span><br><span class="line">    new_curr = dummy</span><br><span class="line">    curr = head</span><br><span class="line">    <span class="keyword">while</span> curr:</span><br><span class="line">        new_curr.<span class="built_in">next</span> = curr.<span class="built_in">next</span></span><br><span class="line">        new_curr = new_curr.<span class="built_in">next</span></span><br><span class="line">        curr.<span class="built_in">next</span> = curr.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">        curr = curr.<span class="built_in">next</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-3"><a href="#🧠-记忆口诀-3" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“哈希映射，两遍搞定”</strong></p></blockquote><hr><h2 id="5️⃣-LC-92-反转链表-II-🟡"><a href="#5️⃣-LC-92-反转链表-II-🟡" class="headerlink" title="5️⃣ LC 92. 反转链表 II 🟡"></a>5️⃣ LC 92. 反转链表 II 🟡</h2><h3 id="题目描述-4"><a href="#题目描述-4" class="headerlink" title="题目描述"></a>题目描述</h3><p>反转链表的第 left 到第 right 个节点。</p><h3 id="🎨-图解思路-4"><a href="#🎨-图解思路-4" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">原链表: 1 → 2 → 3 → 4 → 5, left=2, right=4</span><br><span class="line"></span><br><span class="line">Step 1: 找到 left 前一个节点</span><br><span class="line">        1 → [2 → 3 → 4] → 5</span><br><span class="line">        ↑    └─反转区间─┘</span><br><span class="line">       prev</span><br><span class="line"></span><br><span class="line">Step 2: 反转中间部分</span><br><span class="line">        1    2 ← 3 ← 4    5</span><br><span class="line">        ↑    ↓         ↑</span><br><span class="line">       prev  └─────────┘</span><br><span class="line"></span><br><span class="line">Step 3: 重新连接</span><br><span class="line">        1 → 4 → 3 → 2 → 5</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-4"><a href="#💻-代码实现-4" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reverseBetween</span>(<span class="params">head: ListNode, left: <span class="built_in">int</span>, right: <span class="built_in">int</span></span>) -&gt; ListNode:</span><br><span class="line">    dummy = ListNode(<span class="number">0</span>, head)</span><br><span class="line">    prev = dummy</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 移动到 left 前一个位置</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(left - <span class="number">1</span>):</span><br><span class="line">        prev = prev.<span class="built_in">next</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反转 [left, right] 区间</span></span><br><span class="line">    curr = prev.<span class="built_in">next</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(right - left):</span><br><span class="line">        <span class="comment"># 把 curr.next 插到 prev 后面</span></span><br><span class="line">        next_node = curr.<span class="built_in">next</span></span><br><span class="line">        curr.<span class="built_in">next</span> = next_node.<span class="built_in">next</span></span><br><span class="line">        next_node.<span class="built_in">next</span> = prev.<span class="built_in">next</span></span><br><span class="line">        prev.<span class="built_in">next</span> = next_node</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-4"><a href="#🧠-记忆口诀-4" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“头插法反转，一次遍历”</strong></p></blockquote><hr><h2 id="6️⃣-LC-25-K-个一组翻转链表-🔴"><a href="#6️⃣-LC-25-K-个一组翻转链表-🔴" class="headerlink" title="6️⃣ LC 25. K 个一组翻转链表 🔴"></a>6️⃣ LC 25. K 个一组翻转链表 🔴</h2><h3 id="题目描述-5"><a href="#题目描述-5" class="headerlink" title="题目描述"></a>题目描述</h3><p>每 k 个节点一组进行翻转。</p><h3 id="🎨-图解思路-5"><a href="#🎨-图解思路-5" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">链表: 1 → 2 → 3 → 4 → 5, k = 2</span><br><span class="line"></span><br><span class="line">第1组: [1,2] → 反转 → [2,1]</span><br><span class="line">第2组: [3,4] → 反转 → [4,3]  </span><br><span class="line">第3组: [5] → 不足k个，保持原样</span><br><span class="line"></span><br><span class="line">结果: 2 → 1 → 4 → 3 → 5</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-5"><a href="#💻-代码实现-5" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">reverseKGroup</span>(<span class="params">head: ListNode, k: <span class="built_in">int</span></span>) -&gt; ListNode:</span><br><span class="line">    <span class="comment"># 检查是否有 k 个节点</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get_kth</span>(<span class="params">node, k</span>):</span><br><span class="line">        <span class="keyword">while</span> node <span class="keyword">and</span> k &gt; <span class="number">0</span>:</span><br><span class="line">            node = node.<span class="built_in">next</span></span><br><span class="line">            k -= <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> node</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 反转链表</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">reverse</span>(<span class="params">head, tail</span>):</span><br><span class="line">        prev = tail.<span class="built_in">next</span></span><br><span class="line">        curr = head</span><br><span class="line">        <span class="keyword">while</span> prev != tail:</span><br><span class="line">            next_temp = curr.<span class="built_in">next</span></span><br><span class="line">            curr.<span class="built_in">next</span> = prev</span><br><span class="line">            prev = curr</span><br><span class="line">            curr = next_temp</span><br><span class="line">        <span class="keyword">return</span> tail, head  <span class="comment"># 新的头和尾</span></span><br><span class="line">    </span><br><span class="line">    dummy = ListNode(<span class="number">0</span>, head)</span><br><span class="line">    prev_group = dummy</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        <span class="comment"># 找到这一组的尾节点</span></span><br><span class="line">        kth = get_kth(prev_group, k)</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> kth:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">        </span><br><span class="line">        next_group = kth.<span class="built_in">next</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 反转这一组</span></span><br><span class="line">        head, tail = prev_group.<span class="built_in">next</span>, kth</span><br><span class="line">        new_head, new_tail = reverse(head, tail)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 连接</span></span><br><span class="line">        prev_group.<span class="built_in">next</span> = new_head</span><br><span class="line">        new_tail.<span class="built_in">next</span> = next_group</span><br><span class="line">        </span><br><span class="line">        prev_group = new_tail</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-5"><a href="#🧠-记忆口诀-5" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“够k就翻，不够就留”</strong></p></blockquote><hr><h2 id="7️⃣-LC-19-删除链表的倒数第-N-个节点-🟡"><a href="#7️⃣-LC-19-删除链表的倒数第-N-个节点-🟡" class="headerlink" title="7️⃣ LC 19. 删除链表的倒数第 N 个节点 🟡"></a>7️⃣ LC 19. 删除链表的倒数第 N 个节点 🟡</h2><h3 id="题目描述-6"><a href="#题目描述-6" class="headerlink" title="题目描述"></a>题目描述</h3><p>删除链表的倒数第 n 个节点。</p><h3 id="🎨-图解思路-6"><a href="#🎨-图解思路-6" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">快慢指针，快指针先走 n 步</span><br><span class="line"></span><br><span class="line">链表: 1 → 2 → 3 → 4 → 5, n = 2</span><br><span class="line"></span><br><span class="line">Step 1: fast 先走 2 步</span><br><span class="line">        1 → 2 → 3 → 4 → 5</span><br><span class="line">        ↑       ↑</span><br><span class="line">       slow    fast</span><br><span class="line"></span><br><span class="line">Step 2: 同时移动直到 fast 到末尾</span><br><span class="line">        1 → 2 → 3 → 4 → 5 → None</span><br><span class="line">                ↑       ↑</span><br><span class="line">               slow    fast</span><br><span class="line"></span><br><span class="line">Step 3: 删除 slow.next</span><br><span class="line">        1 → 2 → 3 ────→ 5</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-6"><a href="#💻-代码实现-6" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">removeNthFromEnd</span>(<span class="params">head: ListNode, n: <span class="built_in">int</span></span>) -&gt; ListNode:</span><br><span class="line">    dummy = ListNode(<span class="number">0</span>, head)</span><br><span class="line">    slow = fast = dummy</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># fast 先走 n+1 步</span></span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(n + <span class="number">1</span>):</span><br><span class="line">        fast = fast.<span class="built_in">next</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 同时移动</span></span><br><span class="line">    <span class="keyword">while</span> fast:</span><br><span class="line">        slow = slow.<span class="built_in">next</span></span><br><span class="line">        fast = fast.<span class="built_in">next</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 删除 slow.next</span></span><br><span class="line">    slow.<span class="built_in">next</span> = slow.<span class="built_in">next</span>.<span class="built_in">next</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-6"><a href="#🧠-记忆口诀-6" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“快先走n步，同行到末尾”</strong></p></blockquote><hr><h2 id="8️⃣-LC-82-删除排序链表中的重复元素-II-🟡"><a href="#8️⃣-LC-82-删除排序链表中的重复元素-II-🟡" class="headerlink" title="8️⃣ LC 82. 删除排序链表中的重复元素 II 🟡"></a>8️⃣ LC 82. 删除排序链表中的重复元素 II 🟡</h2><h3 id="题目描述-7"><a href="#题目描述-7" class="headerlink" title="题目描述"></a>题目描述</h3><p>删除所有重复的节点，只保留原始链表中没有重复出现的数字。</p><h3 id="🎨-图解思路-7"><a href="#🎨-图解思路-7" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1 → 2 → 3 → 3 → 4 → 4 → 5</span><br><span class="line"></span><br><span class="line">检测重复并删除整组：</span><br><span class="line">1 → 2 → [3 → 3] → [4 → 4] → 5</span><br><span class="line">        删除      删除</span><br><span class="line"></span><br><span class="line">结果: 1 → 2 → 5</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-7"><a href="#💻-代码实现-7" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">deleteDuplicates</span>(<span class="params">head: ListNode</span>) -&gt; ListNode:</span><br><span class="line">    dummy = ListNode(<span class="number">0</span>, head)</span><br><span class="line">    prev = dummy</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> prev.<span class="built_in">next</span>:</span><br><span class="line">        curr = prev.<span class="built_in">next</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 检测是否有重复</span></span><br><span class="line">        <span class="keyword">if</span> curr.<span class="built_in">next</span> <span class="keyword">and</span> curr.val == curr.<span class="built_in">next</span>.val:</span><br><span class="line">            <span class="comment"># 跳过所有重复节点</span></span><br><span class="line">            <span class="keyword">while</span> curr.<span class="built_in">next</span> <span class="keyword">and</span> curr.val == curr.<span class="built_in">next</span>.val:</span><br><span class="line">                curr = curr.<span class="built_in">next</span></span><br><span class="line">            prev.<span class="built_in">next</span> = curr.<span class="built_in">next</span>  <span class="comment"># 删除整组</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            prev = prev.<span class="built_in">next</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> dummy.<span class="built_in">next</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-7"><a href="#🧠-记忆口诀-7" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“见重复全删，不重复才留”</strong></p></blockquote><hr><h2 id="9️⃣-LC-61-旋转链表-🟡"><a href="#9️⃣-LC-61-旋转链表-🟡" class="headerlink" title="9️⃣ LC 61. 旋转链表 🟡"></a>9️⃣ LC 61. 旋转链表 🟡</h2><h3 id="题目描述-8"><a href="#题目描述-8" class="headerlink" title="题目描述"></a>题目描述</h3><p>将链表每个节点向右移动 k 个位置。</p><h3 id="🎨-图解思路-8"><a href="#🎨-图解思路-8" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">1 → 2 → 3 → 4 → 5, k = 2</span><br><span class="line"></span><br><span class="line">Step 1: 连成环</span><br><span class="line">        1 → 2 → 3 → 4 → 5</span><br><span class="line">        ↑               │</span><br><span class="line">        └───────────────┘</span><br><span class="line"></span><br><span class="line">Step 2: 找到新的断点 (n - k % n)</span><br><span class="line">        新头是第 5-2=3 个节点之后</span><br><span class="line">        </span><br><span class="line">Step 3: 在正确位置断开</span><br><span class="line">        4 → 5 → 1 → 2 → 3</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-8"><a href="#💻-代码实现-8" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">rotateRight</span>(<span class="params">head: ListNode, k: <span class="built_in">int</span></span>) -&gt; ListNode:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> head <span class="keyword">or</span> <span class="keyword">not</span> head.<span class="built_in">next</span> <span class="keyword">or</span> k == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> head</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算长度并找到尾节点</span></span><br><span class="line">    length = <span class="number">1</span></span><br><span class="line">    tail = head</span><br><span class="line">    <span class="keyword">while</span> tail.<span class="built_in">next</span>:</span><br><span class="line">        tail = tail.<span class="built_in">next</span></span><br><span class="line">        length += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 实际需要移动的步数</span></span><br><span class="line">    k = k % length</span><br><span class="line">    <span class="keyword">if</span> k == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> head</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 找到新的尾节点（第 length - k 个）</span></span><br><span class="line">    new_tail = head</span><br><span class="line">    <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(length - k - <span class="number">1</span>):</span><br><span class="line">        new_tail = new_tail.<span class="built_in">next</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 重新连接</span></span><br><span class="line">    new_head = new_tail.<span class="built_in">next</span></span><br><span class="line">    new_tail.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line">    tail.<span class="built_in">next</span> = head</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> new_head</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-8"><a href="#🧠-记忆口诀-8" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“先成环，再断开”</strong></p></blockquote><hr><h2 id="🔟-LC-86-分隔链表-🟡"><a href="#🔟-LC-86-分隔链表-🟡" class="headerlink" title="🔟 LC 86. 分隔链表 🟡"></a>🔟 LC 86. 分隔链表 🟡</h2><h3 id="题目描述-9"><a href="#题目描述-9" class="headerlink" title="题目描述"></a>题目描述</h3><p>将链表按值 x 分成两部分：小于 x 的在前，大于等于 x 的在后。</p><h3 id="🎨-图解思路-9"><a href="#🎨-图解思路-9" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">1 → 4 → 3 → 2 → 5 → 2, x = 3</span><br><span class="line"></span><br><span class="line">分成两个链表：</span><br><span class="line">小于3: 1 → 2 → 2</span><br><span class="line">≥3:    4 → 3 → 5</span><br><span class="line"></span><br><span class="line">合并: 1 → 2 → 2 → 4 → 3 → 5</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-9"><a href="#💻-代码实现-9" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">partition</span>(<span class="params">head: ListNode, x: <span class="built_in">int</span></span>) -&gt; ListNode:</span><br><span class="line">    <span class="comment"># 两个虚拟头节点</span></span><br><span class="line">    small_dummy = ListNode(<span class="number">0</span>)</span><br><span class="line">    large_dummy = ListNode(<span class="number">0</span>)</span><br><span class="line">    small = small_dummy</span><br><span class="line">    large = large_dummy</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> head:</span><br><span class="line">        <span class="keyword">if</span> head.val &lt; x:</span><br><span class="line">            small.<span class="built_in">next</span> = head</span><br><span class="line">            small = small.<span class="built_in">next</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            large.<span class="built_in">next</span> = head</span><br><span class="line">            large = large.<span class="built_in">next</span></span><br><span class="line">        head = head.<span class="built_in">next</span></span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 连接两个链表</span></span><br><span class="line">    large.<span class="built_in">next</span> = <span class="literal">None</span>  <span class="comment"># 防止成环</span></span><br><span class="line">    small.<span class="built_in">next</span> = large_dummy.<span class="built_in">next</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> small_dummy.<span class="built_in">next</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-9"><a href="#🧠-记忆口诀-9" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“分两队，再合并”</strong></p></blockquote><hr><h2 id="1️⃣1️⃣-LC-146-LRU-缓存-🟡"><a href="#1️⃣1️⃣-LC-146-LRU-缓存-🟡" class="headerlink" title="1️⃣1️⃣ LC 146. LRU 缓存 🟡"></a>1️⃣1️⃣ LC 146. LRU 缓存 🟡</h2><h3 id="题目描述-10"><a href="#题目描述-10" class="headerlink" title="题目描述"></a>题目描述</h3><p>实现 LRU (最近最少使用) 缓存机制。</p><h3 id="🎨-图解思路-10"><a href="#🎨-图解思路-10" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">使用双向链表 + 哈希表</span><br><span class="line"></span><br><span class="line">双向链表（按使用时间排序）:</span><br><span class="line">head ⇄ [最近用] ⇄ [次近用] ⇄ ... ⇄ [最久未用] ⇄ tail</span><br><span class="line"></span><br><span class="line">哈希表: key → 链表节点</span><br><span class="line"></span><br><span class="line">get/put 操作:</span><br><span class="line">1. 通过哈希表 O(1) 找到节点</span><br><span class="line">2. 移动到链表头部（最近使用）</span><br><span class="line">3. 超容量时删除链表尾部</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-10"><a href="#💻-代码实现-10" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DLinkedNode</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, key=<span class="number">0</span>, value=<span class="number">0</span></span>):</span><br><span class="line">        self.key = key</span><br><span class="line">        self.value = value</span><br><span class="line">        self.prev = <span class="literal">None</span></span><br><span class="line">        self.<span class="built_in">next</span> = <span class="literal">None</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LRUCache</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, capacity: <span class="built_in">int</span></span>):</span><br><span class="line">        self.cache = {}  <span class="comment"># key → node</span></span><br><span class="line">        self.capacity = capacity</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 虚拟头尾节点</span></span><br><span class="line">        self.head = DLinkedNode()</span><br><span class="line">        self.tail = DLinkedNode()</span><br><span class="line">        self.head.<span class="built_in">next</span> = self.tail</span><br><span class="line">        self.tail.prev = self.head</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_remove</span>(<span class="params">self, node</span>):</span><br><span class="line">        <span class="string">"""从链表中删除节点"""</span></span><br><span class="line">        node.prev.<span class="built_in">next</span> = node.<span class="built_in">next</span></span><br><span class="line">        node.<span class="built_in">next</span>.prev = node.prev</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_add_to_head</span>(<span class="params">self, node</span>):</span><br><span class="line">        <span class="string">"""添加到链表头部"""</span></span><br><span class="line">        node.prev = self.head</span><br><span class="line">        node.<span class="built_in">next</span> = self.head.<span class="built_in">next</span></span><br><span class="line">        self.head.<span class="built_in">next</span>.prev = node</span><br><span class="line">        self.head.<span class="built_in">next</span> = node</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_move_to_head</span>(<span class="params">self, node</span>):</span><br><span class="line">        <span class="string">"""移动到头部"""</span></span><br><span class="line">        self._remove(node)</span><br><span class="line">        self._add_to_head(node)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_remove_tail</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="string">"""删除尾部节点"""</span></span><br><span class="line">        node = self.tail.prev</span><br><span class="line">        self._remove(node)</span><br><span class="line">        <span class="keyword">return</span> node</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">get</span>(<span class="params">self, key: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">not</span> <span class="keyword">in</span> self.cache:</span><br><span class="line">            <span class="keyword">return</span> -<span class="number">1</span></span><br><span class="line">        node = self.cache[key]</span><br><span class="line">        self._move_to_head(node)</span><br><span class="line">        <span class="keyword">return</span> node.value</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">put</span>(<span class="params">self, key: <span class="built_in">int</span>, value: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">if</span> key <span class="keyword">in</span> self.cache:</span><br><span class="line">            node = self.cache[key]</span><br><span class="line">            node.value = value</span><br><span class="line">            self._move_to_head(node)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            node = DLinkedNode(key, value)</span><br><span class="line">            self.cache[key] = node</span><br><span class="line">            self._add_to_head(node)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(self.cache) &gt; self.capacity:</span><br><span class="line">                tail = self._remove_tail()</span><br><span class="line">                <span class="keyword">del</span> self.cache[tail.key]</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-10"><a href="#🧠-记忆口诀-10" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“双链表记顺序，哈希表快查找”</strong></p></blockquote><hr><h2 id="📊-本章总结"><a href="#📊-本章总结" class="headerlink" title="📊 本章总结"></a>📊 本章总结</h2><h3 id="链表技巧速查表"><a href="#链表技巧速查表" class="headerlink" title="链表技巧速查表"></a>链表技巧速查表</h3><table><thead><tr><th>技巧</th><th>使用场景</th><th>典型题目</th></tr></thead><tbody><tr><td>虚拟头节点</td><td>可能修改头节点</td><td>21, 82, 86</td></tr><tr><td>快慢指针</td><td>找中点/判环</td><td>141, 19</td></tr><tr><td>反转链表</td><td>翻转操作</td><td>92, 25</td></tr><tr><td>哈希表辅助</td><td>复杂指针关系</td><td>138, 146</td></tr><tr><td>双指针</td><td>合并/分隔</td><td>21, 86</td></tr></tbody></table><h3 id="🧠-全章记忆口诀"><a href="#🧠-全章记忆口诀" class="headerlink" title="🧠 全章记忆口诀"></a>🧠 全章记忆口诀</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">环加合复反转组</span><br><span class="line">删倒删重旋分缓</span><br><span class="line"></span><br><span class="line">环 - 环形链表 (141)</span><br><span class="line">加 - 两数相加 (2)</span><br><span class="line">合 - 合并有序链表 (21)</span><br><span class="line">复 - 复制随机链表 (138)</span><br><span class="line">反 - 反转链表 II (92)</span><br><span class="line">组 - K个一组翻转 (25)</span><br><span class="line">删倒 - 删除倒数第N个 (19)</span><br><span class="line">删重 - 删除重复元素 II (82)</span><br><span class="line">旋 - 旋转链表 (61)</span><br><span class="line">分 - 分隔链表 (86)</span><br><span class="line">缓 - LRU缓存 (146)</span><br></pre></td></tr></tbody></table></figure><hr><blockquote><p>📖 <strong>下一篇</strong>：<a href="/2026/01/18/leetcode-150-binary-tree/">二叉树专题</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>🔢 LeetCode 150 - 二分查找专题</title>
      <link href="/2026/01/18/leetcode-150-binary-search/"/>
      <url>/2026/01/18/leetcode-150-binary-search/</url>
      
        <content type="html"><![CDATA[<h1 id="🔢-二分查找专题-7题"><a href="#🔢-二分查找专题-7题" class="headerlink" title="🔢 二分查找专题 (7题)"></a>🔢 二分查找专题 (7题)</h1><blockquote><p>🎯 <strong>核心思想</strong>：每次排除一半的搜索空间，时间复杂度 O(log n)</p></blockquote><hr><h2 id="🗺️-二分查找的本质"><a href="#🗺️-二分查找的本质" class="headerlink" title="🗺️ 二分查找的本质"></a>🗺️ 二分查找的本质</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                   二分查找的本质                             │</span><br><span class="line">├─────────────────────────────────────────────────────────────┤</span><br><span class="line">│                                                             │</span><br><span class="line">│  二分查找的本质是：在有序（或部分有序）的空间中               │</span><br><span class="line">│  找到满足某个条件的边界点                                    │</span><br><span class="line">│                                                             │</span><br><span class="line">│  🔵🔵🔵🔵🔵🔴🔴🔴🔴🔴🔴                                    │</span><br><span class="line">│  ↑         ↑                                                │</span><br><span class="line">│  蓝色区域   红色区域                                         │</span><br><span class="line">│  (不满足)   (满足)                                          │</span><br><span class="line">│                                                             │</span><br><span class="line">│  目标：找到第一个红色（或最后一个蓝色）                      │</span><br><span class="line">│                                                             │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="🔧-二分查找模板"><a href="#🔧-二分查找模板" class="headerlink" title="🔧 二分查找模板"></a>🔧 二分查找模板</h2><h3 id="模板1：找第一个满足条件的位置"><a href="#模板1：找第一个满足条件的位置" class="headerlink" title="模板1：找第一个满足条件的位置"></a>模板1：找第一个满足条件的位置</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">binary_search_first</span>(<span class="params">nums, target</span>):</span><br><span class="line">    left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums)  <span class="comment"># 左闭右开</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        mid = left + (right - left) // <span class="number">2</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> condition(mid):    <span class="comment"># 满足条件</span></span><br><span class="line">            right = mid       <span class="comment"># 答案在 [left, mid]</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            left = mid + <span class="number">1</span>    <span class="comment"># 答案在 [mid+1, right)</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> left  <span class="comment"># 第一个满足条件的位置</span></span><br></pre></td></tr></tbody></table></figure><h3 id="模板2：找最后一个满足条件的位置"><a href="#模板2：找最后一个满足条件的位置" class="headerlink" title="模板2：找最后一个满足条件的位置"></a>模板2：找最后一个满足条件的位置</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">binary_search_last</span>(<span class="params">nums, target</span>):</span><br><span class="line">    left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        mid = left + (right - left + <span class="number">1</span>) // <span class="number">2</span>  <span class="comment"># 向上取整</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> condition(mid):    <span class="comment"># 满足条件</span></span><br><span class="line">            left = mid        <span class="comment"># 答案在 [mid, right)</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            right = mid - <span class="number">1</span>   <span class="comment"># 答案在 [left, mid-1]</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> left  <span class="comment"># 最后一个满足条件的位置</span></span><br></pre></td></tr></tbody></table></figure><hr><h2 id="1️⃣-LC-35-搜索插入位置-🟢"><a href="#1️⃣-LC-35-搜索插入位置-🟢" class="headerlink" title="1️⃣ LC 35. 搜索插入位置 🟢"></a>1️⃣ LC 35. 搜索插入位置 🟢</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>在排序数组中找到目标值的位置，如果不存在则返回应该插入的位置。</p><h3 id="🎨-图解思路"><a href="#🎨-图解思路" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">nums = [1, 3, 5, 6], target = 5</span><br><span class="line"></span><br><span class="line">找第一个 &gt;= target 的位置</span><br><span class="line"></span><br><span class="line">  1   3   5   6</span><br><span class="line">  ↑       ↑</span><br><span class="line"> &lt;5      &gt;=5</span><br><span class="line"></span><br><span class="line">二分查找：</span><br><span class="line">初始: left=0, right=4</span><br><span class="line">mid=2, nums[2]=5 &gt;= 5, right=2</span><br><span class="line">mid=1, nums[1]=3 &lt; 5, left=2</span><br><span class="line">left == right, 返回 2</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现"><a href="#💻-代码实现" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">searchInsert</span>(<span class="params">nums: <span class="built_in">list</span>, target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        mid = left + (right - left) // <span class="number">2</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> nums[mid] &gt;= target:</span><br><span class="line">            right = mid</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> left</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀"><a href="#🧠-记忆口诀" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“找第一个不小于目标的位置”</strong></p></blockquote><hr><h2 id="2️⃣-LC-74-搜索二维矩阵-🟡"><a href="#2️⃣-LC-74-搜索二维矩阵-🟡" class="headerlink" title="2️⃣ LC 74. 搜索二维矩阵 🟡"></a>2️⃣ LC 74. 搜索二维矩阵 🟡</h2><h3 id="题目描述-1"><a href="#题目描述-1" class="headerlink" title="题目描述"></a>题目描述</h3><p>在行列都有序的二维矩阵中搜索目标值。</p><h3 id="🎨-图解思路-1"><a href="#🎨-图解思路-1" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">matrix:</span><br><span class="line">[1,  3,  5,  7]</span><br><span class="line">[10, 11, 16, 20]</span><br><span class="line">[23, 30, 34, 60]</span><br><span class="line"></span><br><span class="line">将 2D 矩阵看作 1D 数组:</span><br><span class="line">[1, 3, 5, 7, 10, 11, 16, 20, 23, 30, 34, 60]</span><br><span class="line"></span><br><span class="line">坐标转换:</span><br><span class="line">index → (index // n, index % n)</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-1"><a href="#💻-代码实现-1" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">searchMatrix</span>(<span class="params">matrix: <span class="built_in">list</span>, target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">    m, n = <span class="built_in">len</span>(matrix), <span class="built_in">len</span>(matrix[<span class="number">0</span>])</span><br><span class="line">    left, right = <span class="number">0</span>, m * n</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        mid = left + (right - left) // <span class="number">2</span></span><br><span class="line">        row, col = mid // n, mid % n</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> matrix[row][col] == target:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">elif</span> matrix[row][col] &lt; target:</span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            right = mid</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-1"><a href="#🧠-记忆口诀-1" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“2D变1D，除法取行，余数取列”</strong></p></blockquote><hr><h2 id="3️⃣-LC-162-寻找峰值-🟡"><a href="#3️⃣-LC-162-寻找峰值-🟡" class="headerlink" title="3️⃣ LC 162. 寻找峰值 🟡"></a>3️⃣ LC 162. 寻找峰值 🟡</h2><h3 id="题目描述-2"><a href="#题目描述-2" class="headerlink" title="题目描述"></a>题目描述</h3><p>找到数组中任意一个峰值元素的索引（比左右邻居都大）。</p><h3 id="🎨-图解思路-2"><a href="#🎨-图解思路-2" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">nums = [1, 2, 1, 3, 5, 6, 4]</span><br><span class="line"></span><br><span class="line">峰值: 索引 1 (值为2) 或 索引 5 (值为6)</span><br><span class="line"></span><br><span class="line">二分思路:</span><br><span class="line">- 如果 mid 在上坡，峰值在右边</span><br><span class="line">- 如果 mid 在下坡，峰值在左边</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">   5 \4</span><br><span class="line">  /</span><br><span class="line"> / \</span><br><span class="line">1   1</span><br><span class="line">mid 处于上坡 → 往右找</span><br><span class="line">mid 处于下坡 → 往左找（包含mid）</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-2"><a href="#💻-代码实现-2" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">findPeakElement</span>(<span class="params">nums: <span class="built_in">list</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        mid = left + (right - left) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &lt; nums[mid + <span class="number">1</span>]:</span><br><span class="line">            <span class="comment"># 上坡，峰值在右边</span></span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 下坡或峰值，峰值在左边（包含mid）</span></span><br><span class="line">            right = mid</span><br><span class="line">    <span class="keyword">return</span> left</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-2"><a href="#🧠-记忆口诀-2" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“上坡往右，下坡往左”</strong></p></blockquote><hr><h2 id="4️⃣-LC-33-搜索旋转排序数组-🟡"><a href="#4️⃣-LC-33-搜索旋转排序数组-🟡" class="headerlink" title="4️⃣ LC 33. 搜索旋转排序数组 🟡"></a>4️⃣ LC 33. 搜索旋转排序数组 🟡</h2><h3 id="题目描述-3"><a href="#题目描述-3" class="headerlink" title="题目描述"></a>题目描述</h3><h2 id="在旋转后的有序数组中搜索目标值。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“先判断哪边有序，再判断目标在哪边”"><a href="#在旋转后的有序数组中搜索目标值。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“先判断哪边有序，再判断目标在哪边”" class="headerlink" title="在旋转后的有序数组中搜索目标值。### 🎨 图解思路### 💻 代码实现### 🧠 记忆口诀> “先判断哪边有序，再判断目标在哪边”"></a>在旋转后的有序数组中搜索目标值。<br>### 🎨 图解思路<br><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">nums = [4, 5, 6, 7, 0, 1, 2], target = 0</span><br><span class="line">旋转后的数组特点:</span><br><span class="line">   6  \</span><br><span class="line">  5    0</span><br><span class="line"> 4      \</span><br><span class="line">二分策略:</span><br><span class="line">1. 判断 mid 在左半段还是右半段</span><br><span class="line">2. 判断 target 在 mid 的哪边</span><br></pre></td></tr></tbody></table></figure><br>### 💻 代码实现<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">search</span>(<span class="params">nums: <span class="built_in">list</span>, target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">        mid = left + (right - left) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] == target:</span><br><span class="line">            <span class="keyword">return</span> mid</span><br><span class="line">        <span class="comment"># 判断 mid 在左半段还是右半段</span></span><br><span class="line">        <span class="keyword">if</span> nums[left] &lt;= nums[mid]:</span><br><span class="line">            <span class="comment"># mid 在左半段（有序）</span></span><br><span class="line">            <span class="keyword">if</span> nums[left] &lt;= target &lt; nums[mid]:</span><br><span class="line">                right = mid - <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># mid 在右半段（有序）</span></span><br><span class="line">            <span class="keyword">if</span> nums[mid] &lt; target &lt;= nums[right]:</span><br><span class="line">                left = mid + <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                right = mid - <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span></span><br></pre></td></tr></tbody></table></figure><br>### 🧠 记忆口诀<br>&gt; <strong>“先判断哪边有序，再判断目标在哪边”</strong></h2><h2 id="5️⃣-LC-34-查找元素的第一个和最后一个位置-🟡"><a href="#5️⃣-LC-34-查找元素的第一个和最后一个位置-🟡" class="headerlink" title="5️⃣ LC 34. 查找元素的第一个和最后一个位置 🟡"></a>5️⃣ LC 34. 查找元素的第一个和最后一个位置 🟡</h2><h3 id="题目描述-4"><a href="#题目描述-4" class="headerlink" title="题目描述"></a>题目描述</h3><h2 id="在排序数组中找到目标值的起始和结束位置。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“找第一个-gt-x3D-，找第一个-gt-再减1”"><a href="#在排序数组中找到目标值的起始和结束位置。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“找第一个-gt-x3D-，找第一个-gt-再减1”" class="headerlink" title="在排序数组中找到目标值的起始和结束位置。### 🎨 图解思路### 💻 代码实现### 🧠 记忆口诀> “找第一个>=，找第一个>再减1”"></a>在排序数组中找到目标值的起始和结束位置。<br>### 🎨 图解思路<br><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">nums = [5, 7, 7, 8, 8, 10], target = 8</span><br><span class="line">找第一个 8: 索引 3</span><br><span class="line">找最后一个 8: 索引 4</span><br><span class="line">  5   7   7   8   8   10</span><br><span class="line">              ↑   ↑</span><br><span class="line">            first last</span><br></pre></td></tr></tbody></table></figure><br>### 💻 代码实现<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">searchRange</span>(<span class="params">nums: <span class="built_in">list</span>, target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">list</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">find_first</span>():</span><br><span class="line">        left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            mid = left + (right - left) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[mid] &gt;= target:</span><br><span class="line">                right = mid</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> left</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">find_last</span>():</span><br><span class="line">        left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums)</span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            mid = left + (right - left) // <span class="number">2</span></span><br><span class="line">            <span class="keyword">if</span> nums[mid] &gt; target:</span><br><span class="line">                right = mid</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                left = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> left - <span class="number">1</span></span><br><span class="line">    first = find_first()</span><br><span class="line">    <span class="keyword">if</span> first == <span class="built_in">len</span>(nums) <span class="keyword">or</span> nums[first] != target:</span><br><span class="line">        <span class="keyword">return</span> [-<span class="number">1</span>, -<span class="number">1</span>]</span><br><span class="line">    last = find_last()</span><br><span class="line">    <span class="keyword">return</span> [first, last]</span><br></pre></td></tr></tbody></table></figure><br>### 🧠 记忆口诀<br>&gt; <strong>“找第一个&gt;=，找第一个&gt;再减1”</strong></h2><h2 id="6️⃣-LC-153-寻找旋转排序数组中的最小值-🟡"><a href="#6️⃣-LC-153-寻找旋转排序数组中的最小值-🟡" class="headerlink" title="6️⃣ LC 153. 寻找旋转排序数组中的最小值 🟡"></a>6️⃣ LC 153. 寻找旋转排序数组中的最小值 🟡</h2><h3 id="题目描述-5"><a href="#题目描述-5" class="headerlink" title="题目描述"></a>题目描述</h3><h2 id="在旋转后的有序数组中找到最小值。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“比右边大就往右，否则往左”"><a href="#在旋转后的有序数组中找到最小值。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“比右边大就往右，否则往左”" class="headerlink" title="在旋转后的有序数组中找到最小值。### 🎨 图解思路### 💻 代码实现### 🧠 记忆口诀> “比右边大就往右，否则往左”"></a>在旋转后的有序数组中找到最小值。<br>### 🎨 图解思路<br><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">nums = [3, 4, 5, 1, 2]</span><br><span class="line">   4  \</span><br><span class="line">  3    1</span><br><span class="line">        \</span><br><span class="line">最小值是旋转点</span><br><span class="line">比较 nums[mid] 和 nums[right]:</span><br><span class="line">- nums[mid] &gt; nums[right]: 最小值在右边</span><br><span class="line">- nums[mid] &lt;= nums[right]: 最小值在左边（包含mid）</span><br></pre></td></tr></tbody></table></figure><br>### 💻 代码实现<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">findMin</span>(<span class="params">nums: <span class="built_in">list</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    left, right = <span class="number">0</span>, <span class="built_in">len</span>(nums) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        mid = left + (right - left) // <span class="number">2</span></span><br><span class="line">        <span class="keyword">if</span> nums[mid] &gt; nums[right]:</span><br><span class="line">            <span class="comment"># 最小值在右边</span></span><br><span class="line">            left = mid + <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 最小值在左边（包含mid）</span></span><br><span class="line">            right = mid</span><br><span class="line">    <span class="keyword">return</span> nums[left]</span><br></pre></td></tr></tbody></table></figure><br>### 🧠 记忆口诀<br>&gt; <strong>“比右边大就往右，否则往左”</strong></h2><h2 id="7️⃣-LC-4-寻找两个正序数组的中位数-🔴"><a href="#7️⃣-LC-4-寻找两个正序数组的中位数-🔴" class="headerlink" title="7️⃣ LC 4. 寻找两个正序数组的中位数 🔴"></a>7️⃣ LC 4. 寻找两个正序数组的中位数 🔴</h2><h3 id="题目描述-6"><a href="#题目描述-6" class="headerlink" title="题目描述"></a>题目描述</h3><h2 id="找到两个正序数组的中位数，要求时间复杂度-O-log-m-n-。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“短数组二分，找正确划分”"><a href="#找到两个正序数组的中位数，要求时间复杂度-O-log-m-n-。-🎨-图解思路-💻-代码实现-🧠-记忆口诀-gt-“短数组二分，找正确划分”" class="headerlink" title="找到两个正序数组的中位数，要求时间复杂度 O(log(m+n))。### 🎨 图解思路### 💻 代码实现### 🧠 记忆口诀> “短数组二分，找正确划分”"></a>找到两个正序数组的中位数，要求时间复杂度 O(log(m+n))。<br>### 🎨 图解思路<br><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">nums1 = [1, 3], nums2 = [2]</span><br><span class="line">合并后: [1, 2, 3]</span><br><span class="line">中位数: 2</span><br><span class="line">二分思路:</span><br><span class="line">在较短的数组上二分，找到一个划分点 i</span><br><span class="line">使得 nums1[0:i] 和 nums2[0:j] 的总数 = (m+n+1)//2</span><br><span class="line">       nums1:  1 | 3</span><br><span class="line">       nums2:  2 |</span><br><span class="line">               ↑</span><br><span class="line">              划分点</span><br><span class="line">左半边最大值 &lt;= 右半边最小值</span><br></pre></td></tr></tbody></table></figure><br>### 💻 代码实现<br><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">findMedianSortedArrays</span>(<span class="params">nums1: <span class="built_in">list</span>, nums2: <span class="built_in">list</span></span>) -&gt; <span class="built_in">float</span>:</span><br><span class="line">    <span class="comment"># 确保 nums1 是较短的数组</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(nums1) &gt; <span class="built_in">len</span>(nums2):</span><br><span class="line">        nums1, nums2 = nums2, nums1</span><br><span class="line">    m, n = <span class="built_in">len</span>(nums1), <span class="built_in">len</span>(nums2)</span><br><span class="line">    left, right = <span class="number">0</span>, m</span><br><span class="line">    <span class="keyword">while</span> left &lt;= right:</span><br><span class="line">        i = (left + right) // <span class="number">2</span></span><br><span class="line">        j = (m + n + <span class="number">1</span>) // <span class="number">2</span> - i</span><br><span class="line">        <span class="comment"># 边界处理</span></span><br><span class="line">        nums1_left = <span class="built_in">float</span>(<span class="string">'-inf'</span>) <span class="keyword">if</span> i == <span class="number">0</span> <span class="keyword">else</span> nums1[i - <span class="number">1</span>]</span><br><span class="line">        nums1_right = <span class="built_in">float</span>(<span class="string">'inf'</span>) <span class="keyword">if</span> i == m <span class="keyword">else</span> nums1[i]</span><br><span class="line">        nums2_left = <span class="built_in">float</span>(<span class="string">'-inf'</span>) <span class="keyword">if</span> j == <span class="number">0</span> <span class="keyword">else</span> nums2[j - <span class="number">1</span>]</span><br><span class="line">        nums2_right = <span class="built_in">float</span>(<span class="string">'inf'</span>) <span class="keyword">if</span> j == n <span class="keyword">else</span> nums2[j]</span><br><span class="line">        <span class="keyword">if</span> nums1_left &lt;= nums2_right <span class="keyword">and</span> nums2_left &lt;= nums1_right:</span><br><span class="line">            <span class="comment"># 找到正确的划分</span></span><br><span class="line">            <span class="keyword">if</span> (m + n) % <span class="number">2</span> == <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="built_in">max</span>(nums1_left, nums2_left)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">return</span> (<span class="built_in">max</span>(nums1_left, nums2_left) +</span><br><span class="line">                        <span class="built_in">min</span>(nums1_right, nums2_right)) / <span class="number">2</span></span><br><span class="line">        <span class="keyword">elif</span> nums1_left &gt; nums2_right:</span><br><span class="line">            <span class="comment"># nums1 划分点太靠右</span></span><br><span class="line">            right = i - <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># nums1 划分点太靠左</span></span><br><span class="line">            left = i + <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="number">0.0</span></span><br></pre></td></tr></tbody></table></figure><br>### 🧠 记忆口诀<br>&gt; <strong>“短数组二分，找正确划分”</strong></h2><h2 id="📊-本章总结"><a href="#📊-本章总结" class="headerlink" title="📊 本章总结"></a>📊 本章总结</h2><h3 id="二分查找场景"><a href="#二分查找场景" class="headerlink" title="二分查找场景"></a>二分查找场景</h3><p>| 场景 | 关键点 | 典型题目 |</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">| 有序数组查找 | 直接二分 | 35, 74 |</span><br><span class="line">| 旋转数组 | 判断有序段 | 33, 153 |</span><br><span class="line">| 峰值问题 | 比较相邻元素 | 162 |</span><br><span class="line">| 边界问题 | &gt;=和&gt;的区别 | 34 |</span><br><span class="line">| 双数组 | 在短数组二分 | 4 |</span><br><span class="line"></span><br><span class="line">### 🧠 全章记忆口诀</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><p>插矩峰旋范围最中<br>二分七题要记清</p><p>插 - 搜索插入位置 (35)<br>矩 - 搜索二维矩阵 (74)<br>峰 - 寻找峰值 (162)<br>旋 - 搜索旋转排序数组 (33)<br>范围 - 查找元素的第一个和最后一个位置 (34)<br>最 - 寻找旋转排序数组中的最小值 (153)<br>中 - 寻找两个正序数组的中位数 (4)</p><pre><code>---&gt; 📖 **下一篇**：[位运算专题](/2026/01/18/leetcode-150-bit/)</code></pre>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>🪟 LeetCode 150 - 滑动窗口专题</title>
      <link href="/2026/01/18/leetcode-150-sliding-window/"/>
      <url>/2026/01/18/leetcode-150-sliding-window/</url>
      
        <content type="html"><![CDATA[<h1 id="🪟-滑动窗口专题-4题"><a href="#🪟-滑动窗口专题-4题" class="headerlink" title="🪟 滑动窗口专题 (4题)"></a>🪟 滑动窗口专题 (4题)</h1><blockquote><p>🎯 <strong>核心技巧</strong>：右扩左缩、哈希计数、条件收缩</p></blockquote><hr><h2 id="🗺️-滑动窗口核心思想"><a href="#🗺️-滑动窗口核心思想" class="headerlink" title="🗺️ 滑动窗口核心思想"></a>🗺️ 滑动窗口核心思想</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                    滑动窗口工作原理                           │</span><br><span class="line">├─────────────────────────────────────────────────────────────┤</span><br><span class="line">│                                                             │</span><br><span class="line">│   Step 1: 右边界扩张                                         │</span><br><span class="line">│   ┌───┬───┬───┬───┬───┬───┬───┐                            │</span><br><span class="line">│   │ a │ b │ c │ d │ e │ f │ g │                            │</span><br><span class="line">│   └───┴───┴───┴───┴───┴───┴───┘                            │</span><br><span class="line">│     L           R ──────▶                                   │</span><br><span class="line">│     └─────┬─────┘                                           │</span><br><span class="line">│         window                                              │</span><br><span class="line">│                                                             │</span><br><span class="line">│   Step 2: 满足条件时左边界收缩                                │</span><br><span class="line">│   ┌───┬───┬───┬───┬───┬───┬───┐                            │</span><br><span class="line">│   │ a │ b │ c │ d │ e │ f │ g │                            │</span><br><span class="line">│   └───┴───┴───┴───┴───┴───┴───┘                            │</span><br><span class="line">│     L ──▶       R                                           │</span><br><span class="line">│         └───┬───┘                                           │</span><br><span class="line">│           window (收缩后)                                    │</span><br><span class="line">│                                                             │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="🔧-滑动窗口万能模板"><a href="#🔧-滑动窗口万能模板" class="headerlink" title="🔧 滑动窗口万能模板"></a>🔧 滑动窗口万能模板</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">sliding_window</span>(<span class="params">s</span>):</span><br><span class="line">    <span class="keyword">from</span> collections <span class="keyword">import</span> defaultdict</span><br><span class="line">    </span><br><span class="line">    window = defaultdict(<span class="built_in">int</span>)  <span class="comment"># 窗口内容计数</span></span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    result = <span class="number">0</span>  <span class="comment"># 或者其他结果变量</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> right <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">        <span class="comment"># ① 右边界扩张：将 s[right] 加入窗口</span></span><br><span class="line">        c = s[right]</span><br><span class="line">        window[c] += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ② 判断是否需要收缩（根据题目条件）</span></span><br><span class="line">        <span class="keyword">while</span> 需要收缩的条件:</span><br><span class="line">            <span class="comment"># 将 s[left] 移出窗口</span></span><br><span class="line">            d = s[left]</span><br><span class="line">            window[d] -= <span class="number">1</span></span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># ③ 更新结果（根据题目要求）</span></span><br><span class="line">        result = <span class="built_in">max</span>(result, right - left + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="1️⃣-LC-209-长度最小的子数组-🟡"><a href="#1️⃣-LC-209-长度最小的子数组-🟡" class="headerlink" title="1️⃣ LC 209. 长度最小的子数组 🟡"></a>1️⃣ LC 209. 长度最小的子数组 🟡</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>找出和 ≥ target 的最短连续子数组长度。</p><h3 id="🎨-图解思路"><a href="#🎨-图解思路" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">nums = [2, 3, 1, 2, 4, 3], target = 7</span><br><span class="line"></span><br><span class="line">窗口滑动过程：</span><br><span class="line">┌─────────────────────────────────────────┐</span><br><span class="line">│ [2]                  sum=2  &lt; 7  扩张   │</span><br><span class="line">│ [2,3]                sum=5  &lt; 7  扩张   │</span><br><span class="line">│ [2,3,1]              sum=6  &lt; 7  扩张   │</span><br><span class="line">│ [2,3,1,2]            sum=8  ≥ 7  记录4  │</span><br><span class="line">│   [3,1,2]            sum=6  &lt; 7  扩张   │</span><br><span class="line">│   [3,1,2,4]          sum=10 ≥ 7  记录4  │</span><br><span class="line">│     [1,2,4]          sum=7  ≥ 7  记录3  │</span><br><span class="line">│       [2,4]          sum=6  &lt; 7  扩张   │</span><br><span class="line">│       [2,4,3]        sum=9  ≥ 7  记录3  │</span><br><span class="line">│         [4,3]        sum=7  ≥ 7  记录2 ✓│</span><br><span class="line">└─────────────────────────────────────────┘</span><br><span class="line"></span><br><span class="line">最小长度 = 2</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现"><a href="#💻-代码实现" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">minSubArrayLen</span>(<span class="params">target: <span class="built_in">int</span>, nums: <span class="built_in">list</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    window_sum = <span class="number">0</span></span><br><span class="line">    min_len = <span class="built_in">float</span>(<span class="string">'inf'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> right <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">        <span class="comment"># 扩张：加入右边元素</span></span><br><span class="line">        window_sum += nums[right]</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 收缩：满足条件时尝试缩小窗口</span></span><br><span class="line">        <span class="keyword">while</span> window_sum &gt;= target:</span><br><span class="line">            min_len = <span class="built_in">min</span>(min_len, right - left + <span class="number">1</span>)</span><br><span class="line">            window_sum -= nums[left]</span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> min_len <span class="keyword">if</span> min_len != <span class="built_in">float</span>(<span class="string">'inf'</span>) <span class="keyword">else</span> <span class="number">0</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀"><a href="#🧠-记忆口诀" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“够了就缩，不够就扩”</strong></p></blockquote><hr><h2 id="2️⃣-LC-3-无重复字符的最长子串-🟡"><a href="#2️⃣-LC-3-无重复字符的最长子串-🟡" class="headerlink" title="2️⃣ LC 3. 无重复字符的最长子串 🟡"></a>2️⃣ LC 3. 无重复字符的最长子串 🟡</h2><h3 id="题目描述-1"><a href="#题目描述-1" class="headerlink" title="题目描述"></a>题目描述</h3><p>找出不含重复字符的最长子串长度。</p><h3 id="🎨-图解思路-1"><a href="#🎨-图解思路-1" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">s = "abcabcbb"</span><br><span class="line"></span><br><span class="line">窗口滑动过程：</span><br><span class="line">┌─────────────────────────────────────────┐</span><br><span class="line">│ [a]         无重复  len=1               │</span><br><span class="line">│ [a,b]       无重复  len=2               │</span><br><span class="line">│ [a,b,c]     无重复  len=3 ✓             │</span><br><span class="line">│ [a,b,c,a]   有重复! 收缩                │</span><br><span class="line">│   [b,c,a]   无重复  len=3               │</span><br><span class="line">│   [b,c,a,b] 有重复! 收缩                │</span><br><span class="line">│     [c,a,b] 无重复  len=3               │</span><br><span class="line">│     ...                                 │</span><br><span class="line">└─────────────────────────────────────────┘</span><br><span class="line"></span><br><span class="line">最长无重复子串长度 = 3</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-1"><a href="#💻-代码实现-1" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lengthOfLongestSubstring</span>(<span class="params">s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    window = {}  <span class="comment"># 记录字符出现次数</span></span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    max_len = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> right <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">        c = s[right]</span><br><span class="line">        window[c] = window.get(c, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 有重复字符时收缩</span></span><br><span class="line">        <span class="keyword">while</span> window[c] &gt; <span class="number">1</span>:</span><br><span class="line">            d = s[left]</span><br><span class="line">            window[d] -= <span class="number">1</span></span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        max_len = <span class="built_in">max</span>(max_len, right - left + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> max_len</span><br></pre></td></tr></tbody></table></figure><h3 id="🔥-优化版本（记录位置）"><a href="#🔥-优化版本（记录位置）" class="headerlink" title="🔥 优化版本（记录位置）"></a>🔥 优化版本（记录位置）</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">lengthOfLongestSubstring</span>(<span class="params">s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    char_index = {}  <span class="comment"># 记录字符最后出现的位置</span></span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    max_len = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> right, c <span class="keyword">in</span> <span class="built_in">enumerate</span>(s):</span><br><span class="line">        <span class="comment"># 如果字符在窗口内出现过，直接跳到重复位置之后</span></span><br><span class="line">        <span class="keyword">if</span> c <span class="keyword">in</span> char_index <span class="keyword">and</span> char_index[c] &gt;= left:</span><br><span class="line">            left = char_index[c] + <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        char_index[c] = right</span><br><span class="line">        max_len = <span class="built_in">max</span>(max_len, right - left + <span class="number">1</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> max_len</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-1"><a href="#🧠-记忆口诀-1" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“重复就跳过，记录最长度”</strong></p></blockquote><hr><h2 id="3️⃣-LC-30-串联所有单词的子串-🔴"><a href="#3️⃣-LC-30-串联所有单词的子串-🔴" class="headerlink" title="3️⃣ LC 30. 串联所有单词的子串 🔴"></a>3️⃣ LC 30. 串联所有单词的子串 🔴</h2><h3 id="题目描述-2"><a href="#题目描述-2" class="headerlink" title="题目描述"></a>题目描述</h3><p>找出字符串中所有是 <code>words</code> 中所有单词串联结果的起始索引。</p><h3 id="🎨-图解思路-2"><a href="#🎨-图解思路-2" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">s = "barfoothefoobarman"</span><br><span class="line">words = ["foo", "bar"]  (每个长度为3)</span><br><span class="line"></span><br><span class="line">串联结果可能是 "foobar" 或 "barfoo"（长度6）</span><br><span class="line"></span><br><span class="line">检查每个可能的起始位置：</span><br><span class="line">位置0: "barfoo" ✓ (bar + foo)</span><br><span class="line">位置3: "foothe" ✗</span><br><span class="line">位置6: "thefoo" ✗</span><br><span class="line">位置9: "foobar" ✓ (foo + bar)</span><br><span class="line">...</span><br><span class="line"></span><br><span class="line">结果: [0, 9]</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-2"><a href="#💻-代码实现-2" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">findSubstring</span>(<span class="params">s: <span class="built_in">str</span>, words: <span class="built_in">list</span></span>) -&gt; <span class="built_in">list</span>:</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> s <span class="keyword">or</span> <span class="keyword">not</span> words:</span><br><span class="line">        <span class="keyword">return</span> []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">    </span><br><span class="line">    word_len = <span class="built_in">len</span>(words[<span class="number">0</span>])</span><br><span class="line">    word_count = <span class="built_in">len</span>(words)</span><br><span class="line">    total_len = word_len * word_count</span><br><span class="line">    word_freq = Counter(words)</span><br><span class="line">    result = []</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 只需要检查 word_len 种起始偏移</span></span><br><span class="line">    <span class="keyword">for</span> offset <span class="keyword">in</span> <span class="built_in">range</span>(word_len):</span><br><span class="line">        left = offset</span><br><span class="line">        window = Counter()</span><br><span class="line">        count = <span class="number">0</span>  <span class="comment"># 窗口内有效单词数</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">for</span> right <span class="keyword">in</span> <span class="built_in">range</span>(offset, <span class="built_in">len</span>(s) - word_len + <span class="number">1</span>, word_len):</span><br><span class="line">            word = s[right:right + word_len]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> word <span class="keyword">in</span> word_freq:</span><br><span class="line">                window[word] += <span class="number">1</span></span><br><span class="line">                count += <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 如果某单词超出需要的数量，收缩左边界</span></span><br><span class="line">                <span class="keyword">while</span> window[word] &gt; word_freq[word]:</span><br><span class="line">                    left_word = s[left:left + word_len]</span><br><span class="line">                    window[left_word] -= <span class="number">1</span></span><br><span class="line">                    count -= <span class="number">1</span></span><br><span class="line">                    left += word_len</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 找到一个有效的串联</span></span><br><span class="line">                <span class="keyword">if</span> count == word_count:</span><br><span class="line">                    result.append(left)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 遇到不在 words 中的单词，重置窗口</span></span><br><span class="line">                window.clear()</span><br><span class="line">                count = <span class="number">0</span></span><br><span class="line">                left = right + word_len</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-2"><a href="#🧠-记忆口诀-2" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“固定单词长，滑动找串联”</strong></p></blockquote><hr><h2 id="4️⃣-LC-76-最小覆盖子串-🔴"><a href="#4️⃣-LC-76-最小覆盖子串-🔴" class="headerlink" title="4️⃣ LC 76. 最小覆盖子串 🔴"></a>4️⃣ LC 76. 最小覆盖子串 🔴</h2><h3 id="题目描述-3"><a href="#题目描述-3" class="headerlink" title="题目描述"></a>题目描述</h3><p>找出 s 中包含 t 所有字符的最小子串。</p><h3 id="🎨-图解思路-3"><a href="#🎨-图解思路-3" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">s = "ADOBECODEBANC", t = "ABC"</span><br><span class="line"></span><br><span class="line">需要: A:1, B:1, C:1</span><br><span class="line"></span><br><span class="line">┌─────────────────────────────────────────────┐</span><br><span class="line">│ 窗口: [A]DOBECODEBANC     缺BC   扩张        │</span><br><span class="line">│ 窗口: [ADOBEC]ODEBANC     满足!  记录6 收缩  │</span><br><span class="line">│ 窗口: [DOBEC]ODEBANC      缺A    扩张        │</span><br><span class="line">│ 窗口: [DOBECODEBA]NC      满足!  记录10 收缩 │</span><br><span class="line">│ 窗口: [CODEBA]NC          满足!  记录6 收缩  │</span><br><span class="line">│ 窗口: [ODEBA]NC           缺C    扩张        │</span><br><span class="line">│ 窗口: [ODEBANC]           满足!  记录7 收缩  │</span><br><span class="line">│ 窗口: [BANC]              满足!  记录4 ✓     │</span><br><span class="line">└─────────────────────────────────────────────┘</span><br><span class="line"></span><br><span class="line">最小覆盖子串 = "BANC"</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-3"><a href="#💻-代码实现-3" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">minWindow</span>(<span class="params">s: <span class="built_in">str</span>, t: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    <span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line">    </span><br><span class="line">    need = Counter(t)       <span class="comment"># 需要的字符及数量</span></span><br><span class="line">    window = Counter()      <span class="comment"># 窗口内的字符</span></span><br><span class="line">    </span><br><span class="line">    have = <span class="number">0</span>                <span class="comment"># 已满足的字符种类数</span></span><br><span class="line">    need_count = <span class="built_in">len</span>(need)  <span class="comment"># 需要满足的字符种类数</span></span><br><span class="line">    </span><br><span class="line">    result = <span class="string">""</span></span><br><span class="line">    min_len = <span class="built_in">float</span>(<span class="string">'inf'</span>)</span><br><span class="line">    left = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> right <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">        <span class="comment"># 扩张：加入右边字符</span></span><br><span class="line">        c = s[right]</span><br><span class="line">        window[c] += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 如果该字符数量正好满足需求</span></span><br><span class="line">        <span class="keyword">if</span> c <span class="keyword">in</span> need <span class="keyword">and</span> window[c] == need[c]:</span><br><span class="line">            have += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 收缩：当所有字符都满足时</span></span><br><span class="line">        <span class="keyword">while</span> have == need_count:</span><br><span class="line">            <span class="comment"># 更新结果</span></span><br><span class="line">            <span class="keyword">if</span> right - left + <span class="number">1</span> &lt; min_len:</span><br><span class="line">                min_len = right - left + <span class="number">1</span></span><br><span class="line">                result = s[left:right + <span class="number">1</span>]</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 移出左边字符</span></span><br><span class="line">            d = s[left]</span><br><span class="line">            <span class="keyword">if</span> d <span class="keyword">in</span> need <span class="keyword">and</span> window[d] == need[d]:</span><br><span class="line">                have -= <span class="number">1</span></span><br><span class="line">            window[d] -= <span class="number">1</span></span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-3"><a href="#🧠-记忆口诀-3" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“扩到满足，缩到不够”</strong></p></blockquote><hr><h2 id="📊-本章总结"><a href="#📊-本章总结" class="headerlink" title="📊 本章总结"></a>📊 本章总结</h2><h3 id="滑动窗口分类"><a href="#滑动窗口分类" class="headerlink" title="滑动窗口分类"></a>滑动窗口分类</h3><table><thead><tr><th>类型</th><th>特点</th><th>典型题目</th></tr></thead><tbody><tr><td><strong>可变窗口求最小</strong></td><td>满足条件就收缩</td><td>209, 76</td></tr><tr><td><strong>可变窗口求最大</strong></td><td>不满足条件才收缩</td><td>3</td></tr><tr><td><strong>固定窗口</strong></td><td>窗口大小固定</td><td>30</td></tr></tbody></table><h3 id="滑动窗口思维导图"><a href="#滑动窗口思维导图" class="headerlink" title="滑动窗口思维导图"></a>滑动窗口思维导图</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">      满足条件？</span><br><span class="line">         │</span><br><span class="line">   ┌─────┴─────┐</span><br><span class="line">   ▼           ▼</span><br><span class="line">  是           否</span><br><span class="line">   │           │</span><br><span class="line">   ▼           ▼</span><br><span class="line">求最小？     求最大？</span><br><span class="line">   │           │</span><br><span class="line">   ▼           ▼</span><br><span class="line"> 收缩        扩张</span><br><span class="line">更新答案    继续扩张</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-全章记忆口诀"><a href="#🧠-全章记忆口诀" class="headerlink" title="🧠 全章记忆口诀"></a>🧠 全章记忆口诀</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">最无串覆四道题</span><br><span class="line">滑动窗口巧解析</span><br><span class="line">右扩左缩是关键</span><br><span class="line">满足条件再收缩</span><br><span class="line"></span><br><span class="line">最 - 长度最小的子数组 (209)</span><br><span class="line">无 - 无重复字符的最长子串 (3)</span><br><span class="line">串 - 串联所有单词的子串 (30)</span><br><span class="line">覆 - 最小覆盖子串 (76)</span><br></pre></td></tr></tbody></table></figure><hr><blockquote><p>📖 <strong>下一篇</strong>：<a href="/2026/01/18/leetcode-150-linked-list/">链表专题</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>📦 LeetCode 150 - 栈与队列专题</title>
      <link href="/2026/01/18/leetcode-150-stack/"/>
      <url>/2026/01/18/leetcode-150-stack/</url>
      
        <content type="html"><![CDATA[<h1 id="📦-栈与队列专题-7题"><a href="#📦-栈与队列专题-7题" class="headerlink" title="📦 栈与队列专题 (7题)"></a>📦 栈与队列专题 (7题)</h1><blockquote><p>🎯 <strong>核心特性</strong>：栈 LIFO（后进先出），队列 FIFO（先进先出）</p></blockquote><hr><h2 id="🗺️-栈的核心应用场景"><a href="#🗺️-栈的核心应用场景" class="headerlink" title="🗺️ 栈的核心应用场景"></a>🗺️ 栈的核心应用场景</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                    栈的应用场景                              │</span><br><span class="line">├─────────────────────────────────────────────────────────────┤</span><br><span class="line">│                                                             │</span><br><span class="line">│  1. 括号匹配                                                │</span><br><span class="line">│     遇到左括号入栈，遇到右括号出栈匹配                       │</span><br><span class="line">│                                                             │</span><br><span class="line">│  2. 表达式求值                                               │</span><br><span class="line">│     操作数栈 + 运算符栈                                      │</span><br><span class="line">│                                                             │</span><br><span class="line">│  3. 单调栈                                                   │</span><br><span class="line">│     找下一个更大/更小元素                                    │</span><br><span class="line">│                                                             │</span><br><span class="line">│  4. 路径简化                                                 │</span><br><span class="line">│     处理 "." 和 ".."                                        │</span><br><span class="line">│                                                             │</span><br><span class="line">│       ┌───┐                                                 │</span><br><span class="line">│       │ C │ ← Top (后进先出)                                │</span><br><span class="line">│       ├───┤                                                 │</span><br><span class="line">│       │ B │                                                 │</span><br><span class="line">│       ├───┤                                                 │</span><br><span class="line">│       │ A │                                                 │</span><br><span class="line">│       └───┘                                                 │</span><br><span class="line">│                                                             │</span><br><span class="line">└─────────────────────────────────────────────────────────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="1️⃣-LC-20-有效的括号-🟢"><a href="#1️⃣-LC-20-有效的括号-🟢" class="headerlink" title="1️⃣ LC 20. 有效的括号 🟢"></a>1️⃣ LC 20. 有效的括号 🟢</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>判断括号字符串是否有效。</p><h3 id="🎨-图解思路"><a href="#🎨-图解思路" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">s = "([{}])"</span><br><span class="line"></span><br><span class="line">处理过程:</span><br><span class="line">字符    操作      栈状态</span><br><span class="line">(      入栈      [(]</span><br><span class="line">[      入栈      [(, []</span><br><span class="line">{      入栈      [(, [, {]</span><br><span class="line">}      出栈匹配   [(, []      ✓ { 匹配</span><br><span class="line">]      出栈匹配   [(]         ✓ [ 匹配</span><br><span class="line">)      出栈匹配   []          ✓ ( 匹配</span><br><span class="line"></span><br><span class="line">栈为空 → 有效！</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现"><a href="#💻-代码实现" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">isValid</span>(<span class="params">s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">    stack = []</span><br><span class="line">    mapping = {<span class="string">')'</span>: <span class="string">'('</span>, <span class="string">']'</span>: <span class="string">'['</span>, <span class="string">'}'</span>: <span class="string">'{'</span>}</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> char <span class="keyword">in</span> s:</span><br><span class="line">        <span class="keyword">if</span> char <span class="keyword">in</span> mapping:</span><br><span class="line">            <span class="comment"># 右括号：出栈匹配</span></span><br><span class="line">            <span class="keyword">if</span> <span class="keyword">not</span> stack <span class="keyword">or</span> stack.pop() != mapping[char]:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 左括号：入栈</span></span><br><span class="line">            stack.append(char)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">len</span>(stack) == <span class="number">0</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀"><a href="#🧠-记忆口诀" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“左入右出，空栈有效”</strong></p></blockquote><hr><h2 id="2️⃣-LC-71-简化路径-🟡"><a href="#2️⃣-LC-71-简化路径-🟡" class="headerlink" title="2️⃣ LC 71. 简化路径 🟡"></a>2️⃣ LC 71. 简化路径 🟡</h2><h3 id="题目描述-1"><a href="#题目描述-1" class="headerlink" title="题目描述"></a>题目描述</h3><p>简化 Unix 风格的绝对路径。</p><h3 id="🎨-图解思路-1"><a href="#🎨-图解思路-1" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">path = "/a/./b/../../c/"</span><br><span class="line"></span><br><span class="line">处理规则:</span><br><span class="line">.   → 当前目录，忽略</span><br><span class="line">..  → 上级目录，出栈</span><br><span class="line">其他 → 目录名，入栈</span><br><span class="line"></span><br><span class="line">处理过程:</span><br><span class="line">a   → 入栈 → [a]</span><br><span class="line">.   → 忽略 → [a]</span><br><span class="line">b   → 入栈 → [a, b]</span><br><span class="line">..  → 出栈 → [a]</span><br><span class="line">..  → 出栈 → []</span><br><span class="line">c   → 入栈 → [c]</span><br><span class="line"></span><br><span class="line">结果: "/c"</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-1"><a href="#💻-代码实现-1" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">simplifyPath</span>(<span class="params">path: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">    stack = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> part <span class="keyword">in</span> path.split(<span class="string">'/'</span>):</span><br><span class="line">        <span class="keyword">if</span> part == <span class="string">'..'</span>:</span><br><span class="line">            <span class="keyword">if</span> stack:</span><br><span class="line">                stack.pop()</span><br><span class="line">        <span class="keyword">elif</span> part <span class="keyword">and</span> part != <span class="string">'.'</span>:</span><br><span class="line">            stack.append(part)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="string">'/'</span> + <span class="string">'/'</span>.join(stack)</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-1"><a href="#🧠-记忆口诀-1" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“点忽略，双点出栈，其他入栈”</strong></p></blockquote><hr><h2 id="3️⃣-LC-155-最小栈-🟡"><a href="#3️⃣-LC-155-最小栈-🟡" class="headerlink" title="3️⃣ LC 155. 最小栈 🟡"></a>3️⃣ LC 155. 最小栈 🟡</h2><h3 id="题目描述-2"><a href="#题目描述-2" class="headerlink" title="题目描述"></a>题目描述</h3><p>设计一个支持 O(1) 获取最小值的栈。</p><h3 id="🎨-图解思路-2"><a href="#🎨-图解思路-2" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">使用辅助栈同步记录当前最小值</span><br><span class="line"></span><br><span class="line">主栈      辅助栈（记录当前最小）</span><br><span class="line">push(5)   [5]       [5]</span><br><span class="line">push(3)   [5,3]     [5,3]      ← 3更小</span><br><span class="line">push(7)   [5,3,7]   [5,3,3]    ← 最小仍是3</span><br><span class="line">pop()     [5,3]     [5,3]</span><br><span class="line">getMin()  返回 3</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-2"><a href="#💻-代码实现-2" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MinStack</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.stack = []</span><br><span class="line">        self.min_stack = []</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">push</span>(<span class="params">self, val: <span class="built_in">int</span></span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.stack.append(val)</span><br><span class="line">        <span class="comment"># 辅助栈：记录当前最小值</span></span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> self.min_stack <span class="keyword">or</span> val &lt;= self.min_stack[-<span class="number">1</span>]:</span><br><span class="line">            self.min_stack.append(val)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.min_stack.append(self.min_stack[-<span class="number">1</span>])</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">pop</span>(<span class="params">self</span>) -&gt; <span class="literal">None</span>:</span><br><span class="line">        self.stack.pop()</span><br><span class="line">        self.min_stack.pop()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">top</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">return</span> self.stack[-<span class="number">1</span>]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getMin</span>(<span class="params">self</span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">        <span class="keyword">return</span> self.min_stack[-<span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-2"><a href="#🧠-记忆口诀-2" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“辅助栈同步记录最小值”</strong></p></blockquote><hr><h2 id="4️⃣-LC-150-逆波兰表达式求值-🟡"><a href="#4️⃣-LC-150-逆波兰表达式求值-🟡" class="headerlink" title="4️⃣ LC 150. 逆波兰表达式求值 🟡"></a>4️⃣ LC 150. 逆波兰表达式求值 🟡</h2><h3 id="题目描述-3"><a href="#题目描述-3" class="headerlink" title="题目描述"></a>题目描述</h3><p>计算逆波兰表达式（后缀表达式）的值。</p><h3 id="🎨-图解思路-3"><a href="#🎨-图解思路-3" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">tokens = ["2","1","+","3","*"]</span><br><span class="line"></span><br><span class="line">等价于: (2 + 1) * 3 = 9</span><br><span class="line"></span><br><span class="line">处理过程（遇到运算符弹出两个操作数）:</span><br><span class="line">2   → 入栈 → [2]</span><br><span class="line">1   → 入栈 → [2, 1]</span><br><span class="line">+   → 弹出1,2，计算2+1=3 → [3]</span><br><span class="line">3   → 入栈 → [3, 3]</span><br><span class="line">*   → 弹出3,3，计算3*3=9 → [9]</span><br><span class="line"></span><br><span class="line">结果: 9</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-3"><a href="#💻-代码实现-3" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">evalRPN</span>(<span class="params">tokens: <span class="built_in">list</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    stack = []</span><br><span class="line">    operators = {<span class="string">'+'</span>, <span class="string">'-'</span>, <span class="string">'*'</span>, <span class="string">'/'</span>}</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> token <span class="keyword">in</span> tokens:</span><br><span class="line">        <span class="keyword">if</span> token <span class="keyword">in</span> operators:</span><br><span class="line">            b, a = stack.pop(), stack.pop()</span><br><span class="line">            <span class="keyword">if</span> token == <span class="string">'+'</span>:</span><br><span class="line">                stack.append(a + b)</span><br><span class="line">            <span class="keyword">elif</span> token == <span class="string">'-'</span>:</span><br><span class="line">                stack.append(a - b)</span><br><span class="line">            <span class="keyword">elif</span> token == <span class="string">'*'</span>:</span><br><span class="line">                stack.append(a * b)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="comment"># 注意：Python除法向零取整</span></span><br><span class="line">                stack.append(<span class="built_in">int</span>(a / b))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            stack.append(<span class="built_in">int</span>(token))</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> stack[<span class="number">0</span>]</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-3"><a href="#🧠-记忆口诀-3" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“数字入栈，运算符弹两个算”</strong></p></blockquote><hr><h2 id="5️⃣-LC-224-基本计算器-🔴"><a href="#5️⃣-LC-224-基本计算器-🔴" class="headerlink" title="5️⃣ LC 224. 基本计算器 🔴"></a>5️⃣ LC 224. 基本计算器 🔴</h2><h3 id="题目描述-4"><a href="#题目描述-4" class="headerlink" title="题目描述"></a>题目描述</h3><p>实现一个基本的计算器（支持 +、-、括号）。</p><h3 id="🎨-图解思路-4"><a href="#🎨-图解思路-4" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">s = "1 + (2 - 3)"</span><br><span class="line"></span><br><span class="line">使用栈保存括号外的状态</span><br><span class="line"></span><br><span class="line">遇到 ( → 保存当前 result 和 sign，重置</span><br><span class="line">遇到 ) → 恢复之前的状态并累加</span><br><span class="line"></span><br><span class="line">处理: 1 + (2 - 3)</span><br><span class="line">1      → result = 1</span><br><span class="line">+      → sign = 1</span><br><span class="line">(      → 保存(1, 1)，重置 result=0</span><br><span class="line">2      → result = 2</span><br><span class="line">-      → sign = -1</span><br><span class="line">3      → result = 2 + (-1)*3 = -1</span><br><span class="line">)      → result = 1 + 1*(-1) = 0</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-4"><a href="#💻-代码实现-4" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calculate</span>(<span class="params">s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    stack = []</span><br><span class="line">    result = <span class="number">0</span></span><br><span class="line">    num = <span class="number">0</span></span><br><span class="line">    sign = <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> char <span class="keyword">in</span> s:</span><br><span class="line">        <span class="keyword">if</span> char.isdigit():</span><br><span class="line">            num = num * <span class="number">10</span> + <span class="built_in">int</span>(char)</span><br><span class="line">        <span class="keyword">elif</span> char == <span class="string">'+'</span>:</span><br><span class="line">            result += sign * num</span><br><span class="line">            num = <span class="number">0</span></span><br><span class="line">            sign = <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> char == <span class="string">'-'</span>:</span><br><span class="line">            result += sign * num</span><br><span class="line">            num = <span class="number">0</span></span><br><span class="line">            sign = -<span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> char == <span class="string">'('</span>:</span><br><span class="line">            <span class="comment"># 保存当前状态</span></span><br><span class="line">            stack.append(result)</span><br><span class="line">            stack.append(sign)</span><br><span class="line">            result = <span class="number">0</span></span><br><span class="line">            sign = <span class="number">1</span></span><br><span class="line">        <span class="keyword">elif</span> char == <span class="string">')'</span>:</span><br><span class="line">            result += sign * num</span><br><span class="line">            num = <span class="number">0</span></span><br><span class="line">            <span class="comment"># 恢复状态</span></span><br><span class="line">            result = result * stack.pop() + stack.pop()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result + sign * num</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-4"><a href="#🧠-记忆口诀-4" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“括号保存状态，出来恢复累加”</strong></p></blockquote><hr><h2 id="6️⃣-LC-227-基本计算器-II-🟡"><a href="#6️⃣-LC-227-基本计算器-II-🟡" class="headerlink" title="6️⃣ LC 227. 基本计算器 II 🟡"></a>6️⃣ LC 227. 基本计算器 II 🟡</h2><h3 id="题目描述-5"><a href="#题目描述-5" class="headerlink" title="题目描述"></a>题目描述</h3><p>实现计算器（支持 +、-、*、/，无括号）。</p><h3 id="🎨-图解思路-5"><a href="#🎨-图解思路-5" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">s = "3+2*2"</span><br><span class="line"></span><br><span class="line">* / 优先级高于 + -</span><br><span class="line"></span><br><span class="line">策略：用栈保存待加的数</span><br><span class="line">遇到 + 入栈正数</span><br><span class="line">遇到 - 入栈负数</span><br><span class="line">遇到 * / 弹出栈顶计算后入栈</span><br><span class="line"></span><br><span class="line">处理: 3 + 2 * 2</span><br><span class="line">3   → 栈 [3]</span><br><span class="line">+   → 记录 op = +</span><br><span class="line">2   → 栈 [3, 2]</span><br><span class="line">*   → 记录 op = *</span><br><span class="line">2   → 弹出2，计算2*2=4，栈 [3, 4]</span><br><span class="line"></span><br><span class="line">结果: sum([3, 4]) = 7</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-5"><a href="#💻-代码实现-5" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calculate</span>(<span class="params">s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    stack = []</span><br><span class="line">    num = <span class="number">0</span></span><br><span class="line">    op = <span class="string">'+'</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i, char <span class="keyword">in</span> <span class="built_in">enumerate</span>(s):</span><br><span class="line">        <span class="keyword">if</span> char.isdigit():</span><br><span class="line">            num = num * <span class="number">10</span> + <span class="built_in">int</span>(char)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> char <span class="keyword">in</span> <span class="string">'+-*/'</span> <span class="keyword">or</span> i == <span class="built_in">len</span>(s) - <span class="number">1</span>:</span><br><span class="line">            <span class="keyword">if</span> op == <span class="string">'+'</span>:</span><br><span class="line">                stack.append(num)</span><br><span class="line">            <span class="keyword">elif</span> op == <span class="string">'-'</span>:</span><br><span class="line">                stack.append(-num)</span><br><span class="line">            <span class="keyword">elif</span> op == <span class="string">'*'</span>:</span><br><span class="line">                stack.append(stack.pop() * num)</span><br><span class="line">            <span class="keyword">elif</span> op == <span class="string">'/'</span>:</span><br><span class="line">                stack.append(<span class="built_in">int</span>(stack.pop() / num))</span><br><span class="line">            </span><br><span class="line">            op = char</span><br><span class="line">            num = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">sum</span>(stack)</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-5"><a href="#🧠-记忆口诀-5" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“加减入栈，乘除先算”</strong></p></blockquote><hr><h2 id="7️⃣-LC-772-基本计算器-III-🔴"><a href="#7️⃣-LC-772-基本计算器-III-🔴" class="headerlink" title="7️⃣ LC 772. 基本计算器 III 🔴"></a>7️⃣ LC 772. 基本计算器 III 🔴</h2><h3 id="题目描述-6"><a href="#题目描述-6" class="headerlink" title="题目描述"></a>题目描述</h3><p>实现完整计算器（支持 +、-、*、/、括号）。</p><h3 id="🎨-图解思路-6"><a href="#🎨-图解思路-6" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">结合 224 和 227 的思路</span><br><span class="line"></span><br><span class="line">方法1：递归处理括号</span><br><span class="line">方法2：双栈（操作数栈 + 运算符栈）</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现（递归）"><a href="#💻-代码实现（递归）" class="headerlink" title="💻 代码实现（递归）"></a>💻 代码实现（递归）</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">calculate</span>(<span class="params">s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">helper</span>(<span class="params">s, start</span>):</span><br><span class="line">        stack = []</span><br><span class="line">        num = <span class="number">0</span></span><br><span class="line">        op = <span class="string">'+'</span></span><br><span class="line">        i = start</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(s):</span><br><span class="line">            char = s[i]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> char.isdigit():</span><br><span class="line">                num = num * <span class="number">10</span> + <span class="built_in">int</span>(char)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> char == <span class="string">'('</span>:</span><br><span class="line">                num, i = helper(s, i + <span class="number">1</span>)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> char <span class="keyword">in</span> <span class="string">'+-*/)'</span> <span class="keyword">or</span> i == <span class="built_in">len</span>(s) - <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">if</span> op == <span class="string">'+'</span>:</span><br><span class="line">                    stack.append(num)</span><br><span class="line">                <span class="keyword">elif</span> op == <span class="string">'-'</span>:</span><br><span class="line">                    stack.append(-num)</span><br><span class="line">                <span class="keyword">elif</span> op == <span class="string">'*'</span>:</span><br><span class="line">                    stack.append(stack.pop() * num)</span><br><span class="line">                <span class="keyword">elif</span> op == <span class="string">'/'</span>:</span><br><span class="line">                    stack.append(<span class="built_in">int</span>(stack.pop() / num))</span><br><span class="line">                </span><br><span class="line">                <span class="keyword">if</span> char == <span class="string">')'</span>:</span><br><span class="line">                    <span class="keyword">return</span> <span class="built_in">sum</span>(stack), i</span><br><span class="line">                </span><br><span class="line">                op = char</span><br><span class="line">                num = <span class="number">0</span></span><br><span class="line">            </span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">sum</span>(stack), i</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> helper(s, <span class="number">0</span>)[<span class="number">0</span>]</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-6"><a href="#🧠-记忆口诀-6" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“遇括号递归，其他同227”</strong></p></blockquote><hr><h2 id="📊-本章总结"><a href="#📊-本章总结" class="headerlink" title="📊 本章总结"></a>📊 本章总结</h2><h3 id="栈的应用模式"><a href="#栈的应用模式" class="headerlink" title="栈的应用模式"></a>栈的应用模式</h3><table><thead><tr><th>模式</th><th>核心思想</th><th>典型题目</th></tr></thead><tbody><tr><td>括号匹配</td><td>左入右出</td><td>20</td></tr><tr><td>路径处理</td><td>目录入栈，..出栈</td><td>71</td></tr><tr><td>辅助栈</td><td>同步维护额外信息</td><td>155</td></tr><tr><td>表达式求值</td><td>操作数栈 + 运算符处理</td><td>150, 224, 227</td></tr></tbody></table><h3 id="单调栈模板（补充）"><a href="#单调栈模板（补充）" class="headerlink" title="单调栈模板（补充）"></a>单调栈模板（补充）</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">monotonic_stack</span>(<span class="params">nums</span>):</span><br><span class="line">    <span class="string">"""找每个元素右边第一个更大的元素"""</span></span><br><span class="line">    stack = []  <span class="comment"># 存索引</span></span><br><span class="line">    result = [-<span class="number">1</span>] * <span class="built_in">len</span>(nums)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(nums)):</span><br><span class="line">        <span class="keyword">while</span> stack <span class="keyword">and</span> nums[i] &gt; nums[stack[-<span class="number">1</span>]]:</span><br><span class="line">            idx = stack.pop()</span><br><span class="line">            result[idx] = nums[i]</span><br><span class="line">        stack.append(i)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-全章记忆口诀"><a href="#🧠-全章记忆口诀" class="headerlink" title="🧠 全章记忆口诀"></a>🧠 全章记忆口诀</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">括号路径最小栈</span><br><span class="line">逆波兰后计算器</span><br><span class="line">三道计算器升级</span><br><span class="line">栈的七题记心里</span><br><span class="line"></span><br><span class="line">括号 - 有效的括号 (20)</span><br><span class="line">路径 - 简化路径 (71)</span><br><span class="line">最小栈 - 最小栈 (155)</span><br><span class="line">逆波兰 - 逆波兰表达式求值 (150)</span><br><span class="line">计算器 - 基本计算器系列 (224, 227, 772)</span><br></pre></td></tr></tbody></table></figure><hr><blockquote><p>📖 <strong>下一篇</strong>：<a href="/2026/01/18/leetcode-150-heap/">堆/优先队列专题</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>👆 LeetCode 150 - 双指针专题</title>
      <link href="/2026/01/18/leetcode-150-two-pointers/"/>
      <url>/2026/01/18/leetcode-150-two-pointers/</url>
      
        <content type="html"><![CDATA[<h1 id="👆-双指针专题-5题"><a href="#👆-双指针专题-5题" class="headerlink" title="👆 双指针专题 (5题)"></a>👆 双指针专题 (5题)</h1><blockquote><p>🎯 <strong>核心技巧</strong>：相向双指针、同向双指针、快慢指针</p></blockquote><hr><h2 id="🗺️-双指针三大模式"><a href="#🗺️-双指针三大模式" class="headerlink" title="🗺️ 双指针三大模式"></a>🗺️ 双指针三大模式</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">┌─────────────────────────────────────────────────────────────┐</span><br><span class="line">│                      双指针模式                              │</span><br><span class="line">├───────────────────┬───────────────────┬─────────────────────┤</span><br><span class="line">│    相向双指针      │    同向双指针      │     背向双指针       │</span><br><span class="line">│   (对撞指针)       │   (快慢指针)       │   (中心扩展)        │</span><br><span class="line">├───────────────────┼───────────────────┼─────────────────────┤</span><br><span class="line">│  L ──────▶ ◀── R  │  S ──▶ F ──▶     │     ◀── C ──▶      │</span><br><span class="line">├───────────────────┼───────────────────┼─────────────────────┤</span><br><span class="line">│ • 两数之和        │ • 移除元素         │ • 最长回文子串       │</span><br><span class="line">│ • 盛水容器        │ • 删除重复         │ • 回文判断          │</span><br><span class="line">│ • 三数之和        │ • 链表快慢         │                    │</span><br><span class="line">└───────────────────┴───────────────────┴─────────────────────┘</span><br></pre></td></tr></tbody></table></figure><hr><h2 id="1️⃣-LC-125-验证回文串-🟢"><a href="#1️⃣-LC-125-验证回文串-🟢" class="headerlink" title="1️⃣ LC 125. 验证回文串 🟢"></a>1️⃣ LC 125. 验证回文串 🟢</h2><h3 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h3><p>判断字符串是否为回文串（只考虑字母和数字，忽略大小写）。</p><h3 id="🎨-图解思路"><a href="#🎨-图解思路" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">相向双指针，跳过非字母数字字符</span><br><span class="line"></span><br><span class="line">s = "A man, a plan, a canal: Panama"</span><br><span class="line">     ↑                           ↑</span><br><span class="line">     L                           R</span><br><span class="line"></span><br><span class="line">Step 1: 'A' vs 'a' → 相等，L++, R--</span><br><span class="line">Step 2: ' ' 跳过，L++</span><br><span class="line">Step 3: 'm' vs 'm' → 相等</span><br><span class="line">...</span><br><span class="line">最终 L &gt;= R，是回文串 ✓</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现"><a href="#💻-代码实现" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">isPalindrome</span>(<span class="params">s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">    left, right = <span class="number">0</span>, <span class="built_in">len</span>(s) - <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        <span class="comment"># 跳过非字母数字</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> <span class="keyword">not</span> s[left].isalnum():</span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">        <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> <span class="keyword">not</span> s[right].isalnum():</span><br><span class="line">            right -= <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 比较（忽略大小写）</span></span><br><span class="line">        <span class="keyword">if</span> s[left].lower() != s[right].lower():</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        </span><br><span class="line">        left += <span class="number">1</span></span><br><span class="line">        right -= <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> <span class="literal">True</span></span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀"><a href="#🧠-记忆口诀" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“两头往中走，不同就说No”</strong></p></blockquote><hr><h2 id="2️⃣-LC-392-判断子序列-🟢"><a href="#2️⃣-LC-392-判断子序列-🟢" class="headerlink" title="2️⃣ LC 392. 判断子序列 🟢"></a>2️⃣ LC 392. 判断子序列 🟢</h2><h3 id="题目描述-1"><a href="#题目描述-1" class="headerlink" title="题目描述"></a>题目描述</h3><p>判断 <code>s</code> 是否为 <code>t</code> 的子序列。</p><h3 id="🎨-图解思路-1"><a href="#🎨-图解思路-1" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">同向双指针：i 遍历 s，j 遍历 t</span><br><span class="line"></span><br><span class="line">s = "ace", t = "abcde"</span><br><span class="line">     ↑          ↑</span><br><span class="line">     i          j</span><br><span class="line"></span><br><span class="line">j=0: t[0]='a' = s[0], i++, j++</span><br><span class="line">j=1: t[1]='b' ≠ s[1]='c', j++</span><br><span class="line">j=2: t[2]='c' = s[1], i++, j++</span><br><span class="line">j=3: t[3]='d' ≠ s[2]='e', j++</span><br><span class="line">j=4: t[4]='e' = s[2], i++, j++</span><br><span class="line"></span><br><span class="line">i = 3 = len(s) ✓ 是子序列</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-1"><a href="#💻-代码实现-1" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">isSubsequence</span>(<span class="params">s: <span class="built_in">str</span>, t: <span class="built_in">str</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">    i, j = <span class="number">0</span>, <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> i &lt; <span class="built_in">len</span>(s) <span class="keyword">and</span> j &lt; <span class="built_in">len</span>(t):</span><br><span class="line">        <span class="keyword">if</span> s[i] == t[j]:</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">        j += <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> i == <span class="built_in">len</span>(s)</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-1"><a href="#🧠-记忆口诀-1" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“s 指针只在匹配时前进”</strong></p></blockquote><hr><h2 id="3️⃣-LC-167-两数之和-II-输入有序数组-🟡"><a href="#3️⃣-LC-167-两数之和-II-输入有序数组-🟡" class="headerlink" title="3️⃣ LC 167. 两数之和 II - 输入有序数组 🟡"></a>3️⃣ LC 167. 两数之和 II - 输入有序数组 🟡</h2><h3 id="题目描述-2"><a href="#题目描述-2" class="headerlink" title="题目描述"></a>题目描述</h3><p>在有序数组中找两个数，使它们的和等于目标值。</p><h3 id="🎨-图解思路-2"><a href="#🎨-图解思路-2" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">有序数组 + 两数之和 = 相向双指针！</span><br><span class="line"></span><br><span class="line">numbers = [2, 7, 11, 15], target = 9</span><br><span class="line">           ↑          ↑</span><br><span class="line">           L          R</span><br><span class="line"></span><br><span class="line">sum = 2 + 15 = 17 &gt; 9 → R-- (和太大，减小右边)</span><br><span class="line">sum = 2 + 11 = 13 &gt; 9 → R--</span><br><span class="line">sum = 2 + 7 = 9 = target ✓</span><br><span class="line"></span><br><span class="line">返回 [1, 2] (1-indexed)</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-2"><a href="#💻-代码实现-2" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">twoSum</span>(<span class="params">numbers: <span class="built_in">list</span>, target: <span class="built_in">int</span></span>) -&gt; <span class="built_in">list</span>:</span><br><span class="line">    left, right = <span class="number">0</span>, <span class="built_in">len</span>(numbers) - <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        total = numbers[left] + numbers[right]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> total == target:</span><br><span class="line">            <span class="keyword">return</span> [left + <span class="number">1</span>, right + <span class="number">1</span>]  <span class="comment"># 1-indexed</span></span><br><span class="line">        <span class="keyword">elif</span> total &lt; target:</span><br><span class="line">            left += <span class="number">1</span>   <span class="comment"># 和太小，增大左边</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            right -= <span class="number">1</span>  <span class="comment"># 和太大，减小右边</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> []</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-2"><a href="#🧠-记忆口诀-2" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“小了左移，大了右移”</strong></p></blockquote><hr><h2 id="4️⃣-LC-11-盛最多水的容器-🟡"><a href="#4️⃣-LC-11-盛最多水的容器-🟡" class="headerlink" title="4️⃣ LC 11. 盛最多水的容器 🟡"></a>4️⃣ LC 11. 盛最多水的容器 🟡</h2><h3 id="题目描述-3"><a href="#题目描述-3" class="headerlink" title="题目描述"></a>题目描述</h3><p>找两条线，使构成的容器盛水最多。</p><h3 id="🎨-图解思路-3"><a href="#🎨-图解思路-3" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">面积 = min(左高, 右高) × 宽度</span><br><span class="line"></span><br><span class="line">    │           │</span><br><span class="line">    │     │     │</span><br><span class="line">    │  │  │  │  │</span><br><span class="line">    │  │  │  │  │</span><br><span class="line">   [1, 8, 6, 2, 5, 4, 8, 3, 7]</span><br><span class="line">    ↑                       ↑</span><br><span class="line">    L                       R</span><br><span class="line"></span><br><span class="line">关键洞察：移动较矮的那边！</span><br><span class="line">为什么？因为宽度必然减小，只有高度增加才可能面积增大</span><br><span class="line">而高度由矮的决定，所以移动矮的才有可能找到更高的</span><br><span class="line"></span><br><span class="line">Step 1: height[L]=1 &lt; height[R]=7</span><br><span class="line">        area = 1 × 8 = 8</span><br><span class="line">        移动 L (因为左边矮)</span><br><span class="line"></span><br><span class="line">Step 2: height[L]=8 &gt; height[R]=7  </span><br><span class="line">        area = 7 × 7 = 49</span><br><span class="line">        移动 R</span><br><span class="line"></span><br><span class="line">... 最大面积 = 49</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-3"><a href="#💻-代码实现-3" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">maxArea</span>(<span class="params">height: <span class="built_in">list</span></span>) -&gt; <span class="built_in">int</span>:</span><br><span class="line">    left, right = <span class="number">0</span>, <span class="built_in">len</span>(height) - <span class="number">1</span></span><br><span class="line">    max_area = <span class="number">0</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        <span class="comment"># 计算当前面积</span></span><br><span class="line">        width = right - left</span><br><span class="line">        h = <span class="built_in">min</span>(height[left], height[right])</span><br><span class="line">        max_area = <span class="built_in">max</span>(max_area, width * h)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 移动较矮的一边</span></span><br><span class="line">        <span class="keyword">if</span> height[left] &lt; height[right]:</span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            right -= <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> max_area</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-3"><a href="#🧠-记忆口诀-3" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“矮的先走，高的等着”</strong></p></blockquote><hr><h2 id="5️⃣-LC-15-三数之和-🟡"><a href="#5️⃣-LC-15-三数之和-🟡" class="headerlink" title="5️⃣ LC 15. 三数之和 🟡"></a>5️⃣ LC 15. 三数之和 🟡</h2><h3 id="题目描述-4"><a href="#题目描述-4" class="headerlink" title="题目描述"></a>题目描述</h3><p>找出所有和为 0 的三元组（不能重复）。</p><h3 id="🎨-图解思路-4"><a href="#🎨-图解思路-4" class="headerlink" title="🎨 图解思路"></a>🎨 图解思路</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">排序 + 固定一个数 + 双指针找另外两个</span><br><span class="line"></span><br><span class="line">nums = [-4, -1, -1, 0, 1, 2]</span><br><span class="line">        ↑</span><br><span class="line">        i (固定)</span><br><span class="line">            ↑           ↑</span><br><span class="line">            L           R</span><br><span class="line"></span><br><span class="line">target = -nums[i] = 4</span><br><span class="line"></span><br><span class="line">找 L + R = 4:</span><br><span class="line">  -1 + 2 = 1 &lt; 4 → L++</span><br><span class="line">  -1 + 2 = 1 &lt; 4 → L++</span><br><span class="line">  0 + 2 = 2 &lt; 4 → L++</span><br><span class="line">  1 + 2 = 3 &lt; 4 → L++</span><br><span class="line">  L &gt;= R，结束</span><br><span class="line"></span><br><span class="line">i++ 继续...</span><br><span class="line"></span><br><span class="line">去重技巧：</span><br><span class="line">1. nums[i] == nums[i-1] 时跳过</span><br><span class="line">2. 找到解后，L++/R-- 跳过重复值</span><br></pre></td></tr></tbody></table></figure><h3 id="💻-代码实现-4"><a href="#💻-代码实现-4" class="headerlink" title="💻 代码实现"></a>💻 代码实现</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">threeSum</span>(<span class="params">nums: <span class="built_in">list</span></span>) -&gt; <span class="built_in">list</span>:</span><br><span class="line">    nums.sort()</span><br><span class="line">    result = []</span><br><span class="line">    n = <span class="built_in">len</span>(nums)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(n - <span class="number">2</span>):</span><br><span class="line">        <span class="comment"># 去重：跳过重复的第一个数</span></span><br><span class="line">        <span class="keyword">if</span> i &gt; <span class="number">0</span> <span class="keyword">and</span> nums[i] == nums[i - <span class="number">1</span>]:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># 双指针找另外两个数</span></span><br><span class="line">        left, right = i + <span class="number">1</span>, n - <span class="number">1</span></span><br><span class="line">        target = -nums[i]</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">while</span> left &lt; right:</span><br><span class="line">            total = nums[left] + nums[right]</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> total == target:</span><br><span class="line">                result.append([nums[i], nums[left], nums[right]])</span><br><span class="line">                </span><br><span class="line">                <span class="comment"># 去重：跳过重复的第二、三个数</span></span><br><span class="line">                <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> nums[left] == nums[left + <span class="number">1</span>]:</span><br><span class="line">                    left += <span class="number">1</span></span><br><span class="line">                <span class="keyword">while</span> left &lt; right <span class="keyword">and</span> nums[right] == nums[right - <span class="number">1</span>]:</span><br><span class="line">                    right -= <span class="number">1</span></span><br><span class="line">                </span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">                right -= <span class="number">1</span></span><br><span class="line">            <span class="keyword">elif</span> total &lt; target:</span><br><span class="line">                left += <span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                right -= <span class="number">1</span></span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> result</span><br></pre></td></tr></tbody></table></figure><h3 id="🧠-记忆口诀-4"><a href="#🧠-记忆口诀-4" class="headerlink" title="🧠 记忆口诀"></a>🧠 记忆口诀</h3><blockquote><p><strong>“排序固定一，双指针找二三，遇重复就跳过”</strong></p></blockquote><hr><h2 id="📊-本章总结"><a href="#📊-本章总结" class="headerlink" title="📊 本章总结"></a>📊 本章总结</h2><h3 id="双指针模板速查"><a href="#双指针模板速查" class="headerlink" title="双指针模板速查"></a>双指针模板速查</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 模板1: 相向双指针</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">two_pointer_opposite</span>(<span class="params">arr</span>):</span><br><span class="line">    left, right = <span class="number">0</span>, <span class="built_in">len</span>(arr) - <span class="number">1</span></span><br><span class="line">    <span class="keyword">while</span> left &lt; right:</span><br><span class="line">        <span class="keyword">if</span> condition:</span><br><span class="line">            left += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            right -= <span class="number">1</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 模板2: 同向双指针</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">two_pointer_same</span>(<span class="params">arr</span>):</span><br><span class="line">    slow = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> fast <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(arr)):</span><br><span class="line">        <span class="keyword">if</span> condition:</span><br><span class="line">            arr[slow] = arr[fast]</span><br><span class="line">            slow += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> slow</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模板3: 背向双指针（中心扩展）</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">expand_from_center</span>(<span class="params">s, left, right</span>):</span><br><span class="line">    <span class="keyword">while</span> left &gt;= <span class="number">0</span> <span class="keyword">and</span> right &lt; <span class="built_in">len</span>(s) <span class="keyword">and</span> s[left] == s[right]:</span><br><span class="line">        left -= <span class="number">1</span></span><br><span class="line">        right += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> s[left+<span class="number">1</span>:right]</span><br></pre></td></tr></tbody></table></figure><h3 id="题目模式识别"><a href="#题目模式识别" class="headerlink" title="题目模式识别"></a>题目模式识别</h3><table><thead><tr><th>特征</th><th>使用模式</th><th>典型题目</th></tr></thead><tbody><tr><td>有序数组求和</td><td>相向双指针</td><td>167, 15</td></tr><tr><td>回文判断</td><td>相向双指针</td><td>125</td></tr><tr><td>最大/最小容器</td><td>相向双指针</td><td>11</td></tr><tr><td>子序列匹配</td><td>同向双指针</td><td>392</td></tr><tr><td>原地删除元素</td><td>同向双指针</td><td>26, 27</td></tr><tr><td>回文子串</td><td>背向双指针</td><td>5, 647</td></tr></tbody></table><h3 id="🧠-全章记忆口诀"><a href="#🧠-全章记忆口诀" class="headerlink" title="🧠 全章记忆口诀"></a>🧠 全章记忆口诀</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">验判两盛三，双指针五关</span><br><span class="line"></span><br><span class="line">验 - 验证回文串 (125)</span><br><span class="line">判 - 判断子序列 (392)</span><br><span class="line">两 - 两数之和 II (167)</span><br><span class="line">盛 - 盛最多水的容器 (11)</span><br><span class="line">三 - 三数之和 (15)</span><br></pre></td></tr></tbody></table></figure><hr><blockquote><p>📖 <strong>下一篇</strong>：<a href="/2026/01/18/leetcode-150-sliding-window/">滑动窗口专题</a></p></blockquote>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> LeetCode </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mmdetection</title>
      <link href="/2023/03/17/mmdetection/"/>
      <url>/2023/03/17/mmdetection/</url>
      
        <content type="html"><![CDATA[<h2 id=""><a href="#" class="headerlink" title=""></a></h2>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>一种基于单应性的多相机行人跟踪算法</title>
      <link href="/2023/01/26/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E5%8D%95%E5%BA%94%E6%80%A7%E7%9A%84%E5%A4%9A%E7%9B%B8%E6%9C%BA%E8%A1%8C%E4%BA%BA%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95/"/>
      <url>/2023/01/26/%E4%B8%80%E7%A7%8D%E5%9F%BA%E4%BA%8E%E5%8D%95%E5%BA%94%E6%80%A7%E7%9A%84%E5%A4%9A%E7%9B%B8%E6%9C%BA%E8%A1%8C%E4%BA%BA%E8%B7%9F%E8%B8%AA%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h3 id="一种基于单应性的多相机行人跟踪算法"><a href="#一种基于单应性的多相机行人跟踪算法" class="headerlink" title="一种基于单应性的多相机行人跟踪算法"></a>一种基于单应性的多相机行人跟踪算法</h3><h4 id="1-摘要"><a href="#1-摘要" class="headerlink" title="1. 摘要"></a>1. 摘要</h4><p>在一个区域周围安装多个廉价的视频监控摄像头是很容易的。然而，多摄像头跟踪仍然是一个发展中的领域。可以使用多个摄像机生产的监控产品包括摄像机提示、广域交通分析、存在遮挡的跟踪以及现场入口跟踪。</p><p>所有这些产品都需要解决一致的标签问题。这意味着给各种摄像机中真实世界目标的所有投影提供相同的元目标跟踪标签。</p><p>本文介绍了一种多摄像头人群跟踪算法的实现与测试。首先，部分重新实现了形状匹配的单摄像机跟踪算法，以便在测试视频中工作。单摄像头跟踪器的输出是多摄像头跟踪器的输入。该算法找到每个目标的脚特征:对应于目标正下方地面平面上的一个点的像素。视野线被发现并用于创建初始元目标关联。然后，元目标在移动时丢弃一系列标记，并根据这些标记计算单应性。然后，基于同源关系的跟踪器细化元目标列表，并根据需要创建新的元目标。</p><p>测试表明，该算法解决了一致性标记问题，并且在学习过程中只需要很少的边缘事件。基于同源性的匹配器被证明完全克服部分和完全的目标遮挡在一对相机之一。</p><h4 id="2-介绍"><a href="#2-介绍" class="headerlink" title="2 介绍"></a>2 介绍</h4><p>视频监控是一项艰巨的任务。基于计算机视觉领域，它本身只有几十年的历史，视频馈电的自动处理通常需要专门的编码和解码硬件，快速的数字信号处理器，以及大量的存储介质。</p><p>处理多个视频流的需求变得越来越重要。摄像机的价格持续下降，像样的“网络摄像机”售价不到20美元。安装同样便宜且简单。此外，社会因素也有助于监控摄像头的普及。伦敦和波士顿等城市的警察部门，以及购物中心和机场等私人企业，都在利用最近的恐怖主义，为增加视频监控提供理由。在大多数大城市，现在很容易发现摄像机。一些装置甚至自夸使用对近红外或热红外波长敏感的摄像机的微光能力。</p><p>尽管多摄像头监控装置越来越普遍，但很少有算法提取额外的、有意义的多摄像头跟踪信息。第2章将介绍一些在单个视频流中跟踪运动对象的算法。单摄像机跟踪问题的解决方案已经相当完善。然而，多摄像头监控系统需要能够处理多个视频流的算法。</p><h5 id="2-1-动机示例"><a href="#2-1-动机示例" class="headerlink" title="2.1 动机示例"></a>2.1 动机示例</h5><p>摄像机A和摄像机B是不相交的——它们观察世界的不同区域，并不重叠。然而，摄像机A和C部分重叠，摄像机B和C也是如此。两个摄像机同时可以看到任何一个位于较暗重叠区域的物体。</p><p>现在检查这三个摄像机的输出。世界上只有两个人。然而，在这三台相机之间，它们被赋予了四个不同的标签:A-8, B-2, C-4和C-5。给定这些对象标签，我们能找到的最重要的信息是哪些标签指向相同的现实世界对象。这是一致的标签问题。</p><p><img src="/../images/image-20230126002534218-167466393754814.png" alt="image-20230126002534218"></p><p>在一定程度上，人类很擅长解决一致性标签问题。人工监控操作员可以对摄像机在世界上的位置保持一个心理模型，并且即使使用不同的摄像机模式(例如一个RGB摄像机和一个热红外摄像机)，也经常可以匹配摄像机与摄像机的特征。此外，人类在匹配物体方面比计算机做得更好，即使这些物体从不同的角度观察，因此具有不同的外观。然而，使用人类来分析多个视频流并不能很好地扩展，因为一个人一次只能看一个屏幕，即使一个场景可能有许多相关的视图。如果使用多个监视操作员，每个人负责一个特定的区域，那么系统将需要开发控制、目标跟踪、目标切换和可能的操作员不注意的程序。</p><p><img src="/../images/image-20230126002725061.png" alt="image-20230126002725061"></p><p>监视系统的一项重要任务是能够跟踪在监视区域内移动的目标。许多相机可能在任何给定的时间内都在观察目标，但即使只使用少数相机，人类也需要有意识地努力确定这组相机。此外，随着目标的移动，观察目标的摄像机也在不断变化。如果一致的标签问题解决了，计算机知道目标是否应该出现在每个摄像机的视野中，那么计算机就可以自动提示显示目标的正确摄像机组。</p><p>图1.2说明了将多个摄像机视为一组单个摄像机的算法和将摄像机视为更多东西的算法之间的区别。图1.2(a)中的算法则不是这样关心多台摄像机可能拍摄世界的同一部分。第二类算法如图1.2(b)所示，取单个摄像机跟踪器的输出并将其组合。创建了新的监视功能。下面将提到这些多摄像头感知算法所创建的一些功能示例。</p><h5 id="2-2-范围-目标"><a href="#2-2-范围-目标" class="headerlink" title="2.2 范围-目标"></a>2.2 范围-目标</h5><p>本论文涵盖了一种多摄像头监控算法的开发、实现和测试。该算法应具有以下特点：</p><ul><li><p>独立于相机外部参数，即位置和方向。该算法应该能够平稳地处理广泛不同的世界视角。</p></li><li><p>独立于相机的内在参数，即焦距，像素倾斜和主点的位置。市场上有不同的相机，算法应该能够处理多个焦距，分辨率的差异等等。</p></li><li><p>独立于相机模式。该算法应该能够处理任何单摄像头跟踪器的输出。算法不应该依赖于底层的摄像头硬件RGB，近红外，热红外，或其他成像技术。</p></li><li><p>解决一致性标签问题。一个真实世界的目标应该链接到该目标可见的每个摄像机中的一个对象标签</p></li><li><p>对目标遮挡和场景入口的鲁棒性。如果一个目标在场景中间进入监视区域，比如通过一扇门，那么该算法应该正确地解决一致性标签问题。类似地，如果一个目标分裂成两个，比如两个亲密的人走不同的路，算法应该识别并正确标记两个目标。</p></li><li><p>设置简单。不需要相机校准。如果需要，训练应尽可能少地花费时间，并应在正常的现场交通条件下进行。训练应该是自动的，不需要操作员干预。</p></li><li><p>具备摄像提示能力。该算法应该能够确定哪些摄像机应该能够看到给定的目标。</p></li></ul><h5 id="2-3-范围-限制"><a href="#2-3-范围-限制" class="headerlink" title="2.3 范围-限制"></a>2.3 范围-限制</h5><p>算法的范围限制如下:</p><ul><li>追踪行走的人的算法应使用。车辆、动物等各类运动物体不在本文研究范围之内。</li><li>要处理的相机对至少有部分重叠的视野。这就要求操作员在安装硬件和初始化算法时做出初步判断:决定哪些摄像机看到的是世界上相同的部分。</li><li>摄像机应该是静止的。一旦安装，相机的内在和外在参数都应该是固定的。这意味着相机不能安装在平底倾斜炮塔上，或者如果它是，炮塔不能移动。</li><li>摄像机的输出图像将是一个实用的大小。该算法将不包括单像素探测器(例如红外运动探测器，光束破光探测器)。这种限制是必要的，以确保单摄像机跟踪是可能的，而不需要对所选算法进行重大更改。</li><li>帧率将足以让单摄像头跟踪算法正常工作。</li><li>相机应近似常规中心投影相机与基本针孔光学。相机具有极宽的视场视点-鱼眼镜头-或显著未校正的扭曲将不使用。</li><li>最重要的是，目标应该在地面上行走。任何两个摄像机之间的重叠区域不得有明显偏离平面。处理丘陵地区或台阶的代码不包含在本算法中。</li><li>摄像机不得安装在地平面上。这可以防止在场景几何中出现退化的情况，如下文所示</li></ul><h5 id="2-4-对-领域的贡献"><a href="#2-4-对-领域的贡献" class="headerlink" title="2.4 对 领域的贡献"></a>2.4 对 领域的贡献</h5><p>  如上所述，多摄像头视频处理是一个相对较新的领域。算法一直在开发中，还有很多问题有待解决。如果本文开发的算法满足1.2节和1.3节中描述的目标和限制，则可以实现以下场景:</p><ul><li><p>自动提示:感兴趣的目标走进监视区域。</p><p>操作员在一个摄像机中标记目标。当目标在整个区域内移动时，计算机在算法的驱动下，自动显示所有可见目标的视频源。目标可以用颜色一致的“光环”或包围框标记。这让操作员专注于目标的行动，而不是它在世界上相对于每个摄像机的位置。</p></li><li><p>路径分析:一个区域被置于监视之下。该算法不是试图手动匹配人们从一个摄像头到另一个摄像头的路径，而是自动连接人们通过该区域所走的路径。这使得流量分析能够更快、更有效地进行。</p></li><li><p>跟踪遮挡恢复。为了欺骗当前的许多跟踪算法，你可以移动到遮挡物后面(例如建筑支柱或高大的同伙)，改变速度，然后移出遮挡物。遮挡破坏了许多当前的跟踪算法，如果速度变化显著，大多数其他算法就会破坏。只要目标在至少一个摄像机中仍然可见，接下来章节中讨论的算法将从遮挡中恢复，并重新建立一致的跟踪标签</p></li><li><p>现场的入口。该算法应该能够在人们可以从框架中间进入的场景中创建一致的跟踪标签，例如通过电梯或门</p></li></ul><h6 id="2-4-1-具体的贡献"><a href="#2-4-1-具体的贡献" class="headerlink" title="2.4.1 具体的贡献"></a>2.4.1 具体的贡献</h6><p>本文为视频处理领域提供了以下具体贡献:</p><ul><li>一种即使在摄像机明显倾斜时也能找到目标脚的方法</li><li>一种利用目标运动来寻找平面诱导单应性的方法，即使入口和出口在空间上是有限的</li><li>一种具有特定规则的方法，描述如何使用平面诱导单应性来跨多个摄像机创建和维持目标关联</li></ul><p>第三章讨论基础理论，第四章讨论实现细节。试验结果见第5章。</p><h4 id="3-背景"><a href="#3-背景" class="headerlink" title="3. 背景"></a>3. 背景</h4>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单应性框架下基于图切的前景分割的多摄像机人物跟踪</title>
      <link href="/2023/01/24/%E5%8D%95%E5%BA%94%E6%80%A7%E6%A1%86%E6%9E%B6%E4%B8%8B%E5%9F%BA%E4%BA%8E%E5%9B%BE%E5%88%87%E7%9A%84%E5%89%8D%E6%99%AF%E5%88%86%E5%89%B2%E7%9A%84%E5%A4%9A%E6%91%84%E5%83%8F%E6%9C%BA%E4%BA%BA%E7%89%A9%E8%B7%9F%E8%B8%AA/"/>
      <url>/2023/01/24/%E5%8D%95%E5%BA%94%E6%80%A7%E6%A1%86%E6%9E%B6%E4%B8%8B%E5%9F%BA%E4%BA%8E%E5%9B%BE%E5%88%87%E7%9A%84%E5%89%8D%E6%99%AF%E5%88%86%E5%89%B2%E7%9A%84%E5%A4%9A%E6%91%84%E5%83%8F%E6%9C%BA%E4%BA%BA%E7%89%A9%E8%B7%9F%E8%B8%AA/</url>
      
        <content type="html"><![CDATA[<h3 id="单应性框架下基于图切的前景分割的多摄像机人物跟踪"><a href="#单应性框架下基于图切的前景分割的多摄像机人物跟踪" class="headerlink" title="单应性框架下基于图切的前景分割的多摄像机人物跟踪"></a>单应性框架下基于图切的前景分割的多摄像机人物跟踪</h3><blockquote><p>Multi Camera Person Tracking Applying a Graph-Cuts based<br>Foreground Segmentation in a Homography Framework</p></blockquote><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>可靠的目标跟踪是实现自动化视频监控系统的必要前提。由于大多数基于机器学习的目标检测方法需要足够的应用场景数据，前景分割是一种流行的方法来寻找可能感兴趣的区域。这些通常需要一个特定的学习阶段，并随着时间的推移进行适应。在这项工作中，我们将提出一种基于图切割的新方法，它优于大多数标准算法。人们普遍认为，遮挡只能在多摄像机环境中解决。应用多层单应性可以使我们仅应用前景数据就能鲁棒地检测和跟踪目标，从而获得较高的跟踪性能。</p><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>在手术室多相机多行人跟踪和重识别</title>
      <link href="/2023/01/24/%E5%9C%A8%E6%89%8B%E6%9C%AF%E5%AE%A4%E5%A4%9A%E7%9B%B8%E6%9C%BA%E5%A4%9A%E8%A1%8C%E4%BA%BA%E8%B7%9F%E8%B8%AA%E5%92%8C%E9%87%8D%E8%AF%86%E5%88%AB/"/>
      <url>/2023/01/24/%E5%9C%A8%E6%89%8B%E6%9C%AF%E5%AE%A4%E5%A4%9A%E7%9B%B8%E6%9C%BA%E5%A4%9A%E8%A1%8C%E4%BA%BA%E8%B7%9F%E8%B8%AA%E5%92%8C%E9%87%8D%E8%AF%86%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<h3 id="在手术室多相机多行人跟踪和重识别"><a href="#在手术室多相机多行人跟踪和重识别" class="headerlink" title="在手术室多相机多行人跟踪和重识别"></a>在手术室多相机多行人跟踪和重识别</h3><blockquote><p>Multi-Camera Multi-Person T racking and Re-Identification in<br>an Operating Room</p></blockquote><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>多摄像头多人(MCMP)跟踪与重识别(ReID)是安全、行人分析等方面的重要任务;然而，大多数研究都集中在室外场景，因为在一个有障碍物的拥挤房间里，处理遮挡和错误识别要复杂得多。此外，在一个框架内完成这两项任务具有挑战性。本文提出了一种基于轨迹的方法，将跟踪和ReID任务集成在一起。首先，每台摄像机捕捉到的所有手术成员的姿势被逐帧检测;然后，利用检测到的姿态来跟踪每个摄像机的所有成员的轨迹;最后，将这些不同摄像机的轨迹聚类，通过所有摄像机重新识别手术室中的成员。与其他MCMP跟踪和ReID方法相比，本文提出的方法主要利用轨迹，将手术室场景中不易识别的纹理特征作为辅助线索。本文还在ReID过程中集成了时间信息，这比当前最先进的框架(逐帧进行ReID)更可靠。此外，我们的框架在部署到新场景之前不需要培训。本文还创建了一个带有实际手术室视频的注释MCMP数据集。实验证明了所提出的基于轨迹的ReID算法的有效性。提出的框架在ReID任务中达到了85.44%的准确率，在本文提出的手术室数据集中优于最先进的框架。</p><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>在本文中，我们旨在建立一种在手术室内获得可靠的单目人员跟踪和相机间人员ReID结果的方法。与直接用神经网络提取图像特征相比，我们主要通过运动轨迹进行判断，以纹理特征作为辅助线索。因为工作人员的运动遵循一定的规则，他们的轨迹比难以区分的纹理特征更可靠。即使是由于遮挡或人为探测失败导致某摄像头的轨迹被“破坏”，在跨摄像头ReID的步骤中，也可以根据其他摄像头更完整的轨迹将其破坏的轨迹连接起来。相对于在闭塞前后强制连接可能属于同一个人的两条轨迹或其他类型的故障，本方法仅追求更好的单目跟踪性能，可以获得更好、更可靠的整体性能。</p><p>该方法可分为三个步骤:首先，利用训练好的姿态估计神经网络检测每台摄像机捕捉到的每一帧图像中工作人员的姿态;然后筛选出姿态置信度得分高的工作人员，根据每个人脚的图像坐标和预估的图像平面到世界地平面的单应性矩阵，估计和跟踪每个人在手术室中的位置;最后，我们设计了一种聚类算法，重新识别所有摄像机中被检测到的工作人员的轨迹，从而获得手术室中所有工作人员的完整轨迹。</p><h4 id="方法"><a href="#方法" class="headerlink" title="方法"></a>方法</h4><p>由于手术室场景的特殊性，我们很难用端到端模型解决跟踪和ReID问题;因此，根据其特点，我们建立了一个包含三个模型的框架，逐步完成整个任务。方法流程如图2所示。利用估计的姿态和边界框分别生成所有摄像机的几段轨迹，然后结合所有摄像机的结果连接并重新识别得到的破碎轨迹。</p><p><img src="/../images/image-20230124220825454-16745703020465.png" alt="image-20230124220825454"></p><h5 id="3-1-姿态估计和检测框"><a href="#3-1-姿态估计和检测框" class="headerlink" title="3.1 姿态估计和检测框"></a>3.1 姿态估计和检测框</h5><p>人体姿态估计的研究在计算机视觉领域已流行多年[27-29]。在本文中，我们使用了一种成熟且流行的姿态估计神经网络AlphaPose[27]。对于每一个被检测到的人，AlphaPose不仅给出一套完整的带有置信度分数的姿态关键点，还给出了整个姿态及其包围框的置信度分数。通过获得的信息，我们能够估计每个手术成员在手术室中的位置，并在后续步骤中提取他们的图像特征。</p><h5 id="3-2-单目跟踪"><a href="#3-2-单目跟踪" class="headerlink" title="3.2 单目跟踪"></a>3.2 单目跟踪</h5><p>因此，受Trackpy[30]的启发，我们设计了一种简洁可靠的方法，在单目跟踪的步骤中利用每个人的位置。首先，从所有检测到的姿态中筛选出可用的姿态，删除那些姿态信心分数小于T11或脚踝信心分数都小于T12的姿态。然后，估计每个筛选出来的手术成员的位置，并将其映射到世界地平面上，我们估计其在图像坐标系中的位置，Cp = (xp, yp)，其两个脚踝坐标在同一帧中，这意味着我们利用一个人的两个脚踝的平均坐标来表示他们的位置:<img src="/../images/image-20230124221312362-16745703216817.png" alt="image-20230124221312362"></p><p>，其中Cla和Cra分别表示左右脚踝的坐标，然后我们将Cp映射到世界地平面上，使用预先估计的摄像机单应性矩阵H: <img src="/../images/image-20230124221340476-16745703326399.png" alt="image-20230124221340476"></p><p>其中(Xp, Yp)表示这个人在地坐标系中的坐标。</p><p>最后，我们逐帧跟踪每个筛选的手术成员，如果他们在当前帧和下一帧的位置距离小于预先设定的阈值T13(人在地坐标系中可以跑的最大距离)，则在下一帧中跟踪他们。此外，为了处理某人的姿势可能在某些帧中丢失或被过滤掉的情况，我们设置了另一个参数T14，称为“记忆帧数”，这意味着我们考虑到一个人可能会丢失几帧，然后再次出现的可能性，我们保留消失的外科成员的跟踪，并在他们最后一次出现后保留他们的id，最多保留一些帧数。通过这种方法，我们获得了每个摄像机的所有手术成员的初始轨迹。</p><h5 id="3-3-相机间重识别"><a href="#3-3-相机间重识别" class="headerlink" title="3.3 相机间重识别"></a>3.3 相机间重识别</h5><p>由于闭塞和一些其他干扰，获得的轨迹将被分成几个段为每个手术成员如上所述。在单目跟踪中，这个问题很难解决。幸运的是，在手术室场景中放置多个摄像机是很方便的，这意味着我们可以通过其他摄像机更完整的轨迹来克服这些挑战。我们引入了基于密度的空间聚类方法(DBSCAN[31])来总结不同摄像机的轨迹，为属于同一人的摄像机分配相同的ID。</p><p>原始DBSCAN的思想非常简洁:它从一个没有访问过的任意核心点开始;检索这个点的e邻域，如果它包含足够多的点，则启动一个集群;否则，这个点被标记为噪声。受此思想启发，我们设计了我们的聚类算法:在原始的DBSCAN中，将核心对象定义为其邻域内的高密度点，基于轨迹的完整性，我们将时间长度(单位:帧)大于T21的轨迹定义为我们的案例中的核心对象;对于核心轨迹p，如果p与另一个轨迹q的平均距离小于T22，且p与q的平均目标框相似度(本文采用直方图相关系数)大于T23，则认为q是p的邻域对象。对于聚类后标记为“噪声”的轨迹，我们根据平均距离和平均包围盒相似度为每个“噪声”轨迹寻找最可能的聚类。此外，为了避免逻辑错误，对应于同一行人并来自同一摄像机的轨迹不应该在时域重叠，因为行人只能在一个摄像机帧中出现一次。最后，我们获得了所有检测到的手术成员和对应的轨迹，每个摄像机的id。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>基于自定位智能摄像机网络的分布式目标跟踪</title>
      <link href="/2023/01/24/%E5%9F%BA%E4%BA%8E%E8%87%AA%E5%AE%9A%E4%BD%8D%E6%99%BA%E8%83%BD%E6%91%84%E5%83%8F%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/"/>
      <url>/2023/01/24/%E5%9F%BA%E4%BA%8E%E8%87%AA%E5%AE%9A%E4%BD%8D%E6%99%BA%E8%83%BD%E6%91%84%E5%83%8F%E6%9C%BA%E7%BD%91%E7%BB%9C%E7%9A%84%E5%88%86%E5%B8%83%E5%BC%8F%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA/</url>
      
        <content type="html"><![CDATA[<h3 id="基于自定位智能摄像机网络的分布式目标跟踪"><a href="#基于自定位智能摄像机网络的分布式目标跟踪" class="headerlink" title="基于自定位智能摄像机网络的分布式目标跟踪"></a>基于自定位智能摄像机网络的分布式目标跟踪</h3><blockquote><p>Distributed Target Tracking using Self Localizing Smart<br>Camera Networks</p></blockquote><h4 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h4><p>针对分布式智能摄像机，提出了一种新的分散目标跟踪方案。该方法建立在分布式定位协议之上，该协议允许智能摄像头节点自动识别具有重叠域的相邻传感器，并建立一个通信图，反映节点如何相互作用以融合网络中的测量。新协议将检测和跟踪问题均匀地分布在整个网络中，以无缝的方式计算传感器的切换。该方法还在网络中的节点之间分配有关被跟踪对象状态的知识。然后，这些信息可以通过分布式查询获得，该查询允许网络参与者订阅他们可能感兴趣的不同类型的事件。所提出的方案已用于使用自定义设计的智能摄像机节点的集合实时跟踪目标。给出了实验结果。</p><h4 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h4><p>嵌入式智能摄像头系统成本的降低和性能的提高使得考虑将其应用于各种监视和跟踪应用程序具有吸引力。在不久的将来，将有可能像部署灯泡一样部署小型、不显眼的智能摄像头，对广阔的区域提供无所不在的覆盖。我们可以想象使用这样一个系统来跟踪机场的乘客，从他们到达路边值机到他们登机的时间。同样地，我们可以使用这样一个系统来监控老人或体弱多病的人在他们家中的活动，以提高他们的护理质量。</p><p>为了实现我们的愿景，一个强大的态势感知感知从分布式摄像机的集合，我们将需要解决分布式传感和跟踪的问题。更具体地说，挑战将是可靠地检测、定位和跟踪目标，因为他们在一个由多个分布式智能摄像机覆盖的扩展区域移动。</p><p>为了部署这些系统，我们需要开发检测和跟踪的方法，这些方法可以分布在多个传感器上，而不需要过多的通信。这些系统必须是可扩展的，以允许部署可能涉及分布在扩展区域的数千个摄像头，并且必须对故障具有健壮性，以便在异步添加或删除单个传感器时，整个系统能够优雅地响应。</p><p>本文描述了一种基于去中心化的智能摄像机网络检测与跟踪的新方法。这种方法建立在先前的自定位工作的基础上，该工作允许智能摄像机自动检测和定位具有重叠字段的其他摄像机节点，并建立反映节点如何相互作用以融合网络中的测量的通信图。我们开发了具有有限通信要求的新型网络协议，允许系统通过网络均匀分布检测和跟踪问题，以无缝的方式计算传感器的切换。</p><h4 id="相关工作"><a href="#相关工作" class="headerlink" title="相关工作"></a>相关工作</h4>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>adaptive_pose</title>
      <link href="/2023/01/22/adaptive-pose/"/>
      <url>/2023/01/22/adaptive-pose/</url>
      
        <content type="html"><![CDATA[<blockquote><p><a href="https://arxiv.org/pdf/2210.04014.pdf">AdaptivePose++</a> : <a href="https://github.com/buptxyb666/AdaptivePose">github</a></p></blockquote><p><img src="/../images/framework.jpg" alt="img"></p><h4 id="问题"><a href="#问题" class="headerlink" title="问题"></a>问题</h4><ol><li>z[‘ap’] 虽然输出了，但是没有参与最终的计算， 这个ap更像是网络自己学的一个中间级的过程，这个ap没有监督 能学到像论文的示意么。</li><li>Resample2D的作用是啥，在flownet2中搜到了，但还是不太清楚， 是将不同位置的特征进行融合么。 self.gradient_mul是为了控制ap回传的梯度范围么，这个是经验值吗。</li><li>我之前在centernet中加入oks 替换掉了原来的RegWeightedL1Loss_coco()， 你这是额外加了oks的loss，这没有重复学这个回归信息么， 如果为了加快收敛可不可以先RegWeightedL1Loss_coco()，再oks。我一直很好奇如果是自己的数据集，oks中的sigma一般怎么估计呀。 还有oks 的应该是以绝对位置作为计算吧， 我看代码里好像是相对中心点的偏移。</li></ol><h4 id="回答"><a href="#回答" class="headerlink" title="回答"></a>回答</h4><ol><li>z[‘ap’] 只是用于可视化，没有显示的监督，adaptivepose使用中心特征预测ap偏移，再取出ap位置的特征第二跳偏移，整个两跳path是梯度可回传的，所以相当于隐式监督的。</li><li>Resample2D就是warp操作，通过双线性插值取ap位置特征。self.gradient_mul这块意思跟降低该层的学习率一个意思。</li><li>我这边实验效果 oks+L1 &gt; oks &gt; L1。先RegWeightedL1Loss_coco()，再oks这个操作你可以自己试试。自己的数据集如果是人体关键点你直接按着coco取对应位置的sigma就可以了，sigma跟数据集无关。off_to_pose中将中心坐标加到偏移上。首先认为标注过程符合高斯分布，sigma 跟 scale这俩参数乘积，就是高斯分布的方差，直觉上理解就是对偏差的容忍度，比如同样偏移五个像素，可能对于eye的预测误差就是不可容忍的，对于hip的预测误差是可容忍的，对于large scale是可容忍的，对small scale是不可容忍的。coco上提供的标注，也是脸部关键点的sigma最小，其他的大一些，你可以按着这个思路来估算下你所估计的点的sigma。</li></ol><h4 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h4><p><img src="/../images/adaptive-pose/image-20230123160115331.png" alt="image-20230123160115331"></p><p><img src="/../images/adaptive-pose/image-20230123160148686.png" alt="image-20230123160148686"></p><p><img src="/../images/adaptive-pose/image-20230123160203161.png" alt="image-20230123160203161"></p><p><img src="/../images/adaptive-pose/image-20230123160217277.png" alt="image-20230123160217277"></p><p><img src="/../images/adaptive-pose/image-20230123160227108.png" alt="image-20230123160227108"></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>20220612讨论</title>
      <link href="/2023/01/22/20220612/"/>
      <url>/2023/01/22/20220612/</url>
      
        <content type="html"><![CDATA[<h3 id="20220612"><a href="#20220612" class="headerlink" title="20220612"></a>20220612</h3><h4 id="用docker例子"><a href="#用docker例子" class="headerlink" title="用docker例子"></a>用docker例子</h4><ul><li>怎么把你的image构建的很好</li><li>容量小，并且满足大部分的应用</li><li>shm怎么配置</li><li>dockrfile怎么写</li><li>挂载怎么做</li><li>gpu怎么选卡</li><li>怎么在容器里联调， 容器出了问题怎么解决</li><li>应用在k8s上有啥问题，需要怎么优化。</li></ul><blockquote><p>每个点都有深度可以做，依据自己的怪话，选择自己的切入点</p></blockquote><h4 id="假如模型都是过拟合，怎么定位这个问题"><a href="#假如模型都是过拟合，怎么定位这个问题" class="headerlink" title="假如模型都是过拟合，怎么定位这个问题"></a>假如模型都是过拟合，怎么定位这个问题</h4><ul><li>请问怎么知道它过拟合的类型</li><li>因为什么过拟合的</li><li>模型原因还是数据原因</li><li>训练策略原因</li><li>还是说提前没有预热</li></ul><blockquote><p>花点时间研究比较底层的东西， 别人为什么这么设计算法</p></blockquote><h4 id="本质例如多类分类多标签分类分割，这些他们的本质是在做什么事"><a href="#本质例如多类分类多标签分类分割，这些他们的本质是在做什么事" class="headerlink" title="本质例如多类分类多标签分类分割，这些他们的本质是在做什么事"></a>本质例如多类分类多标签分类分割，这些他们的本质是在做什么事</h4><ul><li>凸优化大概的一个原理</li><li>最优化函数的一个原理</li><li>最小二乘，偏最小2乘等等一系列的</li></ul><h4 id="找到自己擅长的点。"><a href="#找到自己擅长的点。" class="headerlink" title="找到自己擅长的点。"></a>找到自己擅长的点。</h4>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>2022年工作暂时性总结</title>
      <link href="/2023/01/22/2022%E5%B9%B4%E5%B7%A5%E4%BD%9C%E6%9A%82%E6%97%B6%E6%80%A7%E6%80%BB%E7%BB%93/"/>
      <url>/2023/01/22/2022%E5%B9%B4%E5%B7%A5%E4%BD%9C%E6%9A%82%E6%97%B6%E6%80%A7%E6%80%BB%E7%BB%93/</url>
      
        <content type="html"><![CDATA[<h3 id="2022年工作暂时性总结"><a href="#2022年工作暂时性总结" class="headerlink" title="2022年工作暂时性总结"></a>2022年工作暂时性总结</h3><h4 id="1-车路协同目标检测项目"><a href="#1-车路协同目标检测项目" class="headerlink" title="1. 车路协同目标检测项目"></a>1. 车路协同目标检测项目</h4><p>参与车路协同 目标检测项目中的开发支持，人机非等指标达到项目要求</p><h4 id="2-多目融合代码开发"><a href="#2-多目融合代码开发" class="headerlink" title="2.多目融合代码开发"></a>2.多目融合代码开发</h4><p>实现多相机目标融合， 相机切换利用位置进行轨迹平滑， 利用瞩目目标过来了雷视徐建目标，确保无重复id输出，以下沙的数据为例子， 标注了一个融合的数据集</p><h4 id="3-路侧停车泊位划线"><a href="#3-路侧停车泊位划线" class="headerlink" title="3. 路侧停车泊位划线"></a>3. 路侧停车泊位划线</h4><p>支持3559a,3519a,ax630 ax620a,sd3403等平台，发测指标P90+ R 90+</p><h4 id="4-全图目标定位和朝向"><a href="#4-全图目标定位和朝向" class="headerlink" title="4.全图目标定位和朝向"></a>4.全图目标定位和朝向</h4><p>朝向：有限的数据集标注(2-3w)，目标高大于1/30，朝向角度误差11°以内，正确率为88.5%，且检测指标浮动在2%以内</p><p>定位：在nx平台上工程化实现，工程耗时由86ms减少到19ms， 指标差异不大，在车顶点、路侧停车数据集以及贴地点伪标签数据上探索了定位不同的实现方式，验证哪些是可行的哪些对参数依赖比较大，减少对人为设定参数的依赖。</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>RANSAC算法</title>
      <link href="/2023/01/22/RANSAC%E7%AE%97%E6%B3%95/"/>
      <url>/2023/01/22/RANSAC%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<h3 id="RANSAC算法"><a href="#RANSAC算法" class="headerlink" title="RANSAC算法"></a>RANSAC算法</h3><hr><p><strong>RANSAC</strong>(<strong>RA</strong>ndom <strong>SA</strong>mple <strong>C</strong>onsensus,随机采样一致)算法</p><hr><p><strong>RANSAC</strong>(<strong>RA</strong>ndom <strong>SA</strong>mple <strong>C</strong>onsensus,随机采样一致)算法是从一组含有“外点”(outliers)的数据中正确估计数学模型参数的迭代算法。“外点”一般指的的数据中的噪声，比如说匹配中的误匹配和估计曲线中的离群点。所以，RANSAC也是一种“外点”检测算法。RANSAC算法是一种不确定算法，它只能在一种概率下产生结果，并且这个概率会随着迭代次数的增加而加大（之后会解释为什么这个算法是这样的）。RANSAC算最早是由Fischler和Bolles在SRI上提出用来解决LDP(Location Determination Proble)问题的。</p><p>对于RANSAC算法来说一个<strong>基本的假设</strong>就是数据是由“内点”和“外点”组成的。“内点”就是组成模型参数的数据，“外点”就是不适合模型的数据。同时RANSAC假设：在给定一组含有少部分“内点”的数据，存在一个程序可以估计出符合“内点”的模型。</p><h4 id="算法基本思想和流程"><a href="#算法基本思想和流程" class="headerlink" title="算法基本思想和流程"></a>算法基本思想和流程</h4><p>RANSAC是通过反复选择数据集去估计出模型，一直迭代到估计出认为比较好的模型。<br>具体的实现步骤可以分为以下几步：</p><ol><li>选择出可以估计出模型的最小数据集；(对于直线拟合来说就是两个点，对于计算Homography矩阵就是4个点)</li><li>使用这个数据集来计算出数据模型；</li><li>将所有数据带入这个模型，计算出“内点”的数目；(累加在一定误差范围内的适合当前迭代推出模型的数据)</li><li>比较当前模型和之前推出的最好的模型的“内点“的数量，记录最大“内点”数的模型参数和“内点”数；</li><li>重复1-4步，直到迭代结束或者当前模型已经足够好了(“内点数目大于一定数量”)。</li></ol><h4 id="迭代次数推导"><a href="#迭代次数推导" class="headerlink" title="迭代次数推导"></a>迭代次数推导</h4><p>假设“内点”在数据中的占比为 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"></p><p><img src="https://www.zhihu.com/equation?tex=t=%5Cfrac%7Bn_%7Bi+n+l+i+e+r+s%7D%7D%7Bn_%7Bi+n+l+i+e+r+s%7D+n_%7Bo+u+t+l+i+e+r+s%7D%7D+%5C%5C" alt="[公式]"></p><p>那么我们每次计算模型使用 <img src="https://www.zhihu.com/equation?tex=N" alt="[公式]"> 个点的情况下，选取的点至少有一个外点的情况就是</p><p><img src="https://www.zhihu.com/equation?tex=+1+-+t%5EN+%5C%5C" alt="[公式]"></p><p>也就是说，在迭代 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 次的情况下， <img src="https://www.zhihu.com/equation?tex=(1-t_n)%5Ek" alt="[公式]"> 就是 <img src="https://www.zhihu.com/equation?tex=k" alt="[公式]"> 次迭代计算模型都至少采样到一个“外点”去计算模型的概率。那么能采样到正确的 <img src="https://www.zhihu.com/equation?tex=N" alt="[公式]"> 个点去计算出正确模型的概率就是</p><p><img src="https://www.zhihu.com/equation?tex=P=1-%5Cleft(1-t%5E%7Bn%7D%5Cright)%5E%7Bk%7D+%5C%5C" alt="[公式]"></p><p>通过上式，可以求得</p><p><img src="https://www.zhihu.com/equation?tex=k=%5Cfrac%7B%5Clog+(1-P)%7D%7B%5Clog+%5Cleft(1-t%5E%7Bn%7D%5Cright)%7D++%5C%5C" alt="[公式]"></p><p>内点”的概率 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 通常是一个先验值。然后 <img src="https://www.zhihu.com/equation?tex=P" alt="[公式]"> 是我们希望RANSAC得到正确模型的概率。如果事先不知道 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 的值，可以使用自适应迭代次数的方法。也就是一开始设定一个无穷大的迭代次数，然后每次更新模型参数估计的时候，用当前的“内点”比值当成 <img src="https://www.zhihu.com/equation?tex=t" alt="[公式]"> 来估算出迭代次数。</p><h5 id="用Python实现直线拟合"><a href="#用Python实现直线拟合" class="headerlink" title="用Python实现直线拟合"></a>用Python实现直线拟合</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据量。</span></span><br><span class="line">SIZE = <span class="number">50</span></span><br><span class="line"><span class="comment"># 产生数据。np.linspace 返回一个一维数组，SIZE指定数组长度。</span></span><br><span class="line"><span class="comment"># 数组最小值是0，最大值是10。所有元素间隔相等。</span></span><br><span class="line">X = np.linspace(<span class="number">0</span>, <span class="number">10</span>, SIZE)</span><br><span class="line">Y = <span class="number">3</span> * X + <span class="number">10</span></span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line"><span class="comment"># 画图区域分成1行1列。选择第一块区域。</span></span><br><span class="line">ax1 = fig.add_subplot(<span class="number">1</span>,<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line"><span class="comment"># 标题</span></span><br><span class="line">ax1.set_title(<span class="string">"RANSAC"</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># 让散点图的数据更加随机并且添加一些噪声。</span></span><br><span class="line">random_x = []</span><br><span class="line">random_y = []</span><br><span class="line"><span class="comment"># 添加直线随机噪声</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(SIZE):</span><br><span class="line">    random_x.append(X[i] + random.uniform(-<span class="number">0.5</span>, <span class="number">0.5</span>)) </span><br><span class="line">    random_y.append(Y[i] + random.uniform(-<span class="number">0.5</span>, <span class="number">0.5</span>)) </span><br><span class="line"><span class="comment"># 添加随机噪声</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(SIZE):</span><br><span class="line">    random_x.append(random.uniform(<span class="number">0</span>,<span class="number">10</span>))</span><br><span class="line">    random_y.append(random.uniform(<span class="number">10</span>,<span class="number">40</span>))</span><br><span class="line">RANDOM_X = np.array(random_x) <span class="comment"># 散点图的横轴。</span></span><br><span class="line">RANDOM_Y = np.array(random_y) <span class="comment"># 散点图的纵轴。</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 画散点图。</span></span><br><span class="line">ax1.scatter(RANDOM_X, RANDOM_Y)</span><br><span class="line"><span class="comment"># 横轴名称。</span></span><br><span class="line">ax1.set_xlabel(<span class="string">"x"</span>)</span><br><span class="line"><span class="comment"># 纵轴名称。</span></span><br><span class="line">ax1.set_ylabel(<span class="string">"y"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用RANSAC算法估算模型</span></span><br><span class="line"><span class="comment"># 迭代最大次数，每次得到更好的估计会优化iters的数值</span></span><br><span class="line">iters = <span class="number">100000</span></span><br><span class="line"><span class="comment"># 数据和模型之间可接受的差值</span></span><br><span class="line">sigma = <span class="number">0.25</span></span><br><span class="line"><span class="comment"># 最好模型的参数估计和内点数目</span></span><br><span class="line">best_a = <span class="number">0</span></span><br><span class="line">best_b = <span class="number">0</span></span><br><span class="line">pretotal = <span class="number">0</span></span><br><span class="line"><span class="comment"># 希望的得到正确模型的概率</span></span><br><span class="line">P = <span class="number">0.99</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(iters):</span><br><span class="line">    <span class="comment"># 随机在数据中红选出两个点去求解模型</span></span><br><span class="line">    sample_index = random.sample(<span class="built_in">range</span>(SIZE * <span class="number">2</span>),<span class="number">2</span>)</span><br><span class="line">    x_1 = RANDOM_X[sample_index[<span class="number">0</span>]]</span><br><span class="line">    x_2 = RANDOM_X[sample_index[<span class="number">1</span>]]</span><br><span class="line">    y_1 = RANDOM_Y[sample_index[<span class="number">0</span>]]</span><br><span class="line">    y_2 = RANDOM_Y[sample_index[<span class="number">1</span>]]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># y = ax + b 求解出a，b</span></span><br><span class="line">    a = (y_2 - y_1) / (x_2 - x_1)</span><br><span class="line">    b = y_1 - a * x_1</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 算出内点数目</span></span><br><span class="line">    total_inlier = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(SIZE * <span class="number">2</span>):</span><br><span class="line">        y_estimate = a * RANDOM_X[index] + b</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">abs</span>(y_estimate - RANDOM_Y[index]) &lt; sigma:</span><br><span class="line">            total_inlier = total_inlier + <span class="number">1</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 判断当前的模型是否比之前估算的模型好</span></span><br><span class="line">    <span class="keyword">if</span> total_inlier &gt; pretotal:</span><br><span class="line">        iters = math.log(<span class="number">1</span> - P) / math.log(<span class="number">1</span> - <span class="built_in">pow</span>(total_inlier / (SIZE * <span class="number">2</span>), <span class="number">2</span>))</span><br><span class="line">        pretotal = total_inlier</span><br><span class="line">        best_a = a</span><br><span class="line">        best_b = b</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 判断是否当前模型已经符合超过一半的点</span></span><br><span class="line">    <span class="keyword">if</span> total_inlier &gt; SIZE:</span><br><span class="line">        <span class="keyword">break</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 用我们得到的最佳估计画图</span></span><br><span class="line">Y = best_a * RANDOM_X + best_b</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直线图</span></span><br><span class="line">ax1.plot(RANDOM_X, Y)</span><br><span class="line">text = <span class="string">"best_a = "</span> + <span class="built_in">str</span>(best_a) + <span class="string">"\nbest_b = "</span> + <span class="built_in">str</span>(best_b)</span><br><span class="line">plt.text(<span class="number">5</span>,<span class="number">10</span>, text,</span><br><span class="line">         fontdict={<span class="string">'size'</span>: <span class="number">8</span>, <span class="string">'color'</span>: <span class="string">'r'</span>})</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/image-20220718002303153.png" alt="image-20220718002303153"></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>shell学习</title>
      <link href="/2023/01/22/shell%E5%AD%A6%E4%B9%A0/"/>
      <url>/2023/01/22/shell%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h2 id="shell学习"><a href="#shell学习" class="headerlink" title="shell学习"></a>shell学习</h2><h3 id="1-shell概述"><a href="#1-shell概述" class="headerlink" title="1. shell概述"></a>1. shell概述</h3><p>Shell 是一个命令解释器， 用于接收应用程序/用户命令， 然后调用操作系统内核。</p><p><img src="/images/image-20220710001433868.png" alt="image-20220710001433868"></p><p> shell 还是一个功能相当强大的编程语言， 易编写，易调试，灵活性强。</p><h3 id="2-shell解析器"><a href="#2-shell解析器" class="headerlink" title="2. shell解析器"></a>2. shell解析器</h3><figure class="highlight sh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">/bin/sh</span><br><span class="line">/bin/bash</span><br><span class="line">/usr/bin/bash</span><br><span class="line">/bin/rbash</span><br><span class="line">/usr/bin/rbash</span><br><span class="line">/usr/bin/sh</span><br><span class="line">/bin/dash</span><br><span class="line">/usr/bin/dash</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">root@2a63e139ac66:/bin# echo $SHELL</span><br><span class="line">/bin/bash</span><br></pre></td></tr></tbody></table></figure><p>系统默认的是bash</p><h3 id="3-Shell-脚本入门"><a href="#3-Shell-脚本入门" class="headerlink" title="3. Shell 脚本入门"></a>3. Shell 脚本入门</h3><h4 id="3-1-脚本格式"><a href="#3-1-脚本格式" class="headerlink" title="3.1 脚本格式"></a>3.1 脚本格式</h4><p>脚本以#!/bin/bash 开头（指定解析器） </p><h4 id="3-2-第一个shell脚本：-helloworld"><a href="#3-2-第一个shell脚本：-helloworld" class="headerlink" title="3.2 第一个shell脚本： helloworld"></a>3.2 第一个shell脚本： helloworld</h4><h5 id="3-2-1-需求：-创建一个shell脚本，-输出helloworld"><a href="#3-2-1-需求：-创建一个shell脚本，-输出helloworld" class="headerlink" title="3.2.1 需求： 创建一个shell脚本， 输出helloworld"></a>3.2.1 需求： 创建一个shell脚本， 输出helloworld</h5><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">echo "helloworld shell"</span><br></pre></td></tr></tbody></table></figure><figure class="highlight sh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"> <span class="built_in">cd</span> shelldata</span><br><span class="line"><span class="comment"># ls</span></span><br><span class="line"><span class="comment"># pwd</span></span><br><span class="line">/root/shelldata</span><br><span class="line"><span class="comment"># touch helloworld.sh</span></span><br><span class="line"><span class="comment"># vim helloworld.sh</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment"># sh helloworld.sh</span></span><br><span class="line">helloworld shell</span><br></pre></td></tr></tbody></table></figure><p>权限不够</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># ./ helloworld.sh</span><br><span class="line">/bin/sh: 33: ./: Permission denied</span><br></pre></td></tr></tbody></table></figure><h5 id="3-2-2-脚本的常用执行方式"><a href="#3-2-2-脚本的常用执行方式" class="headerlink" title="3.2.2 脚本的常用执行方式"></a>3.2.2 脚本的常用执行方式</h5><p>第一种： 采用bash或者sh+脚本的相对路径或者绝对路径（不用赋予脚本+x权限）</p><p>第二种： 采用输入脚本的绝对路径或者相对路径执行脚本（必须具有可执行权限+x）</p><blockquote><p>注意： 第一种执行方法， 本质是bash解析器帮你执行脚本， 所以脚本本身不需要执行权限， 第二种执行方法， 本质是脚本自己执行，所以需要执行权限。</p></blockquote><h4 id="3-3-第二个shell脚本：-多命令处理"><a href="#3-3-第二个shell脚本：-多命令处理" class="headerlink" title="3.3 第二个shell脚本： 多命令处理"></a>3.3 第二个shell脚本： 多命令处理</h4><h5 id="3-3-1-需求："><a href="#3-3-1-需求：" class="headerlink" title="3.3.1 需求："></a>3.3.1 需求：</h5><p>在/home/atguigu/目录下创建一个bangzhang.txt, 在banzhang.txt中增加“I Love  cls”</p><h5 id="3-3-2-案例实操"><a href="#3-3-2-案例实操" class="headerlink" title="3.3.2 案例实操"></a>3.3.2 案例实操</h5><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">cd /root/shelldata/</span><br><span class="line">touch banzhang.txt</span><br><span class="line">echo "I Love cls"&gt;&gt; banzhang.txt</span><br></pre></td></tr></tbody></table></figure><p>结果显示</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">sh-5.1# cat banzhang.txt</span><br><span class="line">I Love cls</span><br><span class="line">I Love cls</span><br><span class="line">sh-5.1# rm banzhang.txt</span><br><span class="line">sh-5.1# bash hadoop101.sh</span><br><span class="line">sh-5.1# cat banzhang.txt</span><br><span class="line">I Love cls</span><br><span class="line">sh-5.1# vim hadoop101.sh</span><br></pre></td></tr></tbody></table></figure><h3 id="4-Shell中的变量"><a href="#4-Shell中的变量" class="headerlink" title="4. Shell中的变量"></a>4. Shell中的变量</h3><h4 id="4-1-系统变量"><a href="#4-1-系统变量" class="headerlink" title="4.1 系统变量"></a>4.1 系统变量</h4><h5 id="4-1-1-常用系统变量"><a href="#4-1-1-常用系统变量" class="headerlink" title="4.1.1 常用系统变量"></a>4.1.1 常用系统变量</h5><p>$HOME、$PWD、$SHELL、$USER等</p><h5 id="4-1-2-案例实操"><a href="#4-1-2-案例实操" class="headerlink" title="4.1.2 案例实操"></a>4.1.2 案例实操</h5><p>查看系统变量的值</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sh-5.1# echo $HOME</span><br><span class="line">/root</span><br><span class="line">sh-5.1# echo $PWD</span><br><span class="line">/root/shelldata</span><br><span class="line">sh-5.1#</span><br></pre></td></tr></tbody></table></figure><p>显示当前shell中所有变量</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line">BASH=/bin/sh</span><br><span class="line">BASHOPTS=checkwinsize:cmdhist:complete_fullquote:expand_aliases:extquote:force_fignore:globasciiranges:hostcomplete:interactive_comments:progcomp:promptvars:sourcepath</span><br><span class="line">BASH_ALIASES=()</span><br><span class="line">BASH_ARGC=()</span><br><span class="line">BASH_ARGV=()</span><br><span class="line">BASH_CMDS=()</span><br><span class="line">BASH_LINENO=()</span><br><span class="line">BASH_SOURCE=()</span><br><span class="line">BASH_VERSINFO=([0]="5" [1]="1" [2]="16" [3]="1" [4]="release" [5]="x86_64-pc-linux-gnu")</span><br><span class="line">BASH_VERSION='5.1.16(1)-release'</span><br><span class="line">COLUMNS=177</span><br><span class="line">DIRSTACK=()</span><br><span class="line">EUID=0</span><br><span class="line">GROUPS=()</span><br><span class="line">HISTFILE=/root/.bash_history</span><br><span class="line">HISTFILESIZE=500</span><br><span class="line">HISTSIZE=500</span><br><span class="line">HOME=/root</span><br><span class="line">HOSTNAME=8b25e507bcca</span><br><span class="line">HOSTTYPE=x86_64</span><br><span class="line">IFS='</span><br><span class="line">'</span><br><span class="line">LINES=50</span><br><span class="line">MACHTYPE=x86_64-pc-linux-gnu</span><br><span class="line">MAILCHECK=60</span><br><span class="line">OLDPWD=/root</span><br><span class="line">OPTERR=1</span><br><span class="line">OPTIND=1</span><br><span class="line">OSTYPE=linux-gnu</span><br><span class="line">PATH=/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</span><br><span class="line">PIPESTATUS=([0]="0")</span><br><span class="line">POSIXLY_CORRECT=y</span><br><span class="line">PPID=0</span><br><span class="line">PS1='\s-\v\$ '</span><br><span class="line">PS2='&gt; '</span><br><span class="line">PS4='+ '</span><br><span class="line">PWD=/root/shelldata</span><br><span class="line">SHELL=/bin/bash</span><br><span class="line">SHELLOPTS=braceexpand:emacs:hashall:histexpand:history:interactive-comments:monitor:posix</span><br><span class="line">SHLVL=1</span><br><span class="line">TERM=xterm</span><br><span class="line">UID=0</span><br><span class="line">_=/root/shelldata</span><br></pre></td></tr></tbody></table></figure><h4 id="4-2-自定义变量"><a href="#4-2-自定义变量" class="headerlink" title="4.2 自定义变量"></a>4.2 自定义变量</h4><h5 id="4-2-1-基本语法"><a href="#4-2-1-基本语法" class="headerlink" title="4.2.1 基本语法"></a>4.2.1 基本语法</h5><p>定义变量： 变量=值</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sh-5.1# A=1</span><br><span class="line">sh-5.1# echo A</span><br><span class="line">A</span><br><span class="line">sh-5.1# echo $A</span><br><span class="line">1</span><br></pre></td></tr></tbody></table></figure><p>撤销变量： unset变量</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sh-5.1# unset A</span><br><span class="line">sh-5.1# echo $A</span><br><span class="line"></span><br><span class="line">sh-5.1#</span><br></pre></td></tr></tbody></table></figure><p>声明一个静态变量： readonly变量， 注意：不能unset</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sh-5.1# readonly B=3</span><br><span class="line">sh-5.1# echo $B</span><br><span class="line">3</span><br><span class="line">sh-5.1# unset B</span><br><span class="line">sh: unset: B: cannot unset: readonly variable</span><br><span class="line">sh-5.1#</span><br></pre></td></tr></tbody></table></figure><h5 id="4-2-2-变量定义规则"><a href="#4-2-2-变量定义规则" class="headerlink" title="4.2.2 变量定义规则"></a>4.2.2 变量定义规则</h5><ol><li>变量名称可以由字母、数字和下划线构成， 但是不能以数字开头， 环境变量名建议大写</li><li>等号两侧不能有空格</li><li>在bash中， 变量默认类型都是字符串类型， 无法直接进行数值运算。</li></ol><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sh-5.1# C=1+1</span><br><span class="line">sh-5.1#</span><br><span class="line">Display all 505 possibilities? (y or n)</span><br><span class="line">sh-5.1# echo $C</span><br><span class="line">1+1</span><br><span class="line">sh-5.1#</span><br></pre></td></tr></tbody></table></figure><ol start="4"><li>变量的值如果有空格，需要用双引号或者单引号括起来。</li></ol><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sh-5.1# D=bangzhang love mm</span><br><span class="line">sh: love: command not found</span><br><span class="line">sh-5.1# D='bangzhang love mm'</span><br><span class="line">sh-5.1# echo $D</span><br><span class="line">bangzhang love mm</span><br><span class="line">sh-5.1#</span><br></pre></td></tr></tbody></table></figure><ol start="5"><li>可以把变量提升为全局环境变量， 可供其他shell程序使用</li></ol><p>export 变量名</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sh: ./helloworld.sh: Permission denied</span><br><span class="line">sh-5.1# chmod +x helloworld.sh</span><br><span class="line">sh-5.1# ./helloworld.sh</span><br><span class="line">helloworld shell</span><br><span class="line"></span><br><span class="line">sh-5.1# export D</span><br><span class="line">sh-5.1# ./helloworld.sh</span><br><span class="line">helloworld shell</span><br><span class="line">bangzhang love mm</span><br></pre></td></tr></tbody></table></figure><h4 id="4-3-特殊变量：-n"><a href="#4-3-特殊变量：-n" class="headerlink" title="4.3 特殊变量：$n"></a>4.3 特殊变量：$n</h4><h5 id="4-3-1-基本语法"><a href="#4-3-1-基本语法" class="headerlink" title="4.3.1 基本语法"></a>4.3.1 基本语法</h5><p>$n（功能描述：n为数字，$0代表该脚本名称，$1-$9代表第一到第九个参数， 十以上的参数需用大括号包含，如${10}）</p><h5 id="4-3-2-案例实操"><a href="#4-3-2-案例实操" class="headerlink" title="4.3.2  案例实操"></a>4.3.2  案例实操</h5><ol><li>输出该脚本文件名称、输入参数1和输入参数2的值</li></ol><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo "$0 $1 $2"</span><br><span class="line"></span><br><span class="line">~                                                                                                           sh-5.1# vim parameter.sh</span><br><span class="line">sh-5.1# bash parameter.sh</span><br><span class="line">parameter.sh</span><br><span class="line">sh-5.1# bash parameter.sh banzhang</span><br><span class="line">parameter.sh banzhang</span><br><span class="line">sh-5.1# bash parameter.sh banzhang lobve</span><br><span class="line">parameter.sh banzhang lobve</span><br><span class="line">sh-5.1# bash parameter.sh banzhang lobve mm</span><br><span class="line">parameter.sh banzhang lobve</span><br><span class="line">sh-5.1# vim parameter.sh              ~       </span><br></pre></td></tr></tbody></table></figure><h4 id="4-4-特殊变量："><a href="#4-4-特殊变量：" class="headerlink" title="4.4 特殊变量：$#"></a>4.4 特殊变量：$#</h4><h5 id="4-4-1-基本语法"><a href="#4-4-1-基本语法" class="headerlink" title="4.4.1 基本语法"></a>4.4.1 基本语法</h5><p>$# (功能描述： 获取所有输入参数的个数， 常用于循环）</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo "$0 $1 $2"</span><br><span class="line">echo $#</span><br><span class="line">~        </span><br><span class="line">·································</span><br><span class="line">sh-5.1# vim parameter.sh</span><br><span class="line">sh-5.1# ls</span><br><span class="line">banzhang.txt  hadoop101.bahs  hadoop101.sh  helloworld.sh  parameter.sh</span><br><span class="line">sh-5.1# chmod +x helloworld.sh</span><br><span class="line">sh-5.1# chmod 777 parameter.sh</span><br><span class="line">sh-5.1# ./parameter.sh cls xyz 111</span><br><span class="line">./parameter.sh cls xyz</span><br><span class="line">3</span><br><span class="line">sh-5.1#</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h4 id="4-5-特殊变量：-、"><a href="#4-5-特殊变量：-、" class="headerlink" title="4.5 特殊变量：$*、$@"></a>4.5 特殊变量：$*、$@</h4><p>$* （功能描述： 这个变量代表命令行中所有的参数，$*把所有的参数看成一个整体）</p><p>$@（功能描述： 这个变量也代表命令行中所有的参数， 只不过$@把每个参数区分对待）</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">echo "$0 $1 $2"</span><br><span class="line">echo $#</span><br><span class="line">echo $*</span><br><span class="line">echo $@</span><br><span class="line">····························</span><br><span class="line">sh-5.1# ./parameter.sh banhh lobe  1111</span><br><span class="line">./parameter.sh banhh lobe</span><br><span class="line">3</span><br><span class="line">banhh lobe 1111</span><br><span class="line">banhh lobe 1111</span><br></pre></td></tr></tbody></table></figure><h4 id="4-6-特殊变量"><a href="#4-6-特殊变量" class="headerlink" title="4.6 特殊变量$?"></a>4.6 特殊变量$?</h4><p>$? (功能描述： 最后一次执行的命令的返回状态。如果这个变量的值为0表示上个命令执行正确；如果为非零（具体哪个值，由命令自己决定)，证明上一个命令执行不正确了。</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">证明helloworld脚本 是否正确执行</span><br><span class="line">sh-5.1# ./helloworld.sh</span><br><span class="line">helloworld shell</span><br><span class="line"></span><br><span class="line">sh-5.1# echo $?</span><br><span class="line">0</span><br><span class="line">sh-5.1#</span><br></pre></td></tr></tbody></table></figure><h3 id="第5章-运算符"><a href="#第5章-运算符" class="headerlink" title="第5章 运算符"></a>第5章 运算符</h3><h4 id="5-1-基本语法"><a href="#5-1-基本语法" class="headerlink" title="5.1 基本语法"></a>5.1 基本语法</h4><p>“$((运算式))” 或“$[运算式]”</p><p>expr +,- ,\*,/,% 加、减、乘、除，取余</p><p>注意： expr运算符间要有空格</p><ol><li>计算3+2的值</li></ol><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sh-5.1# expr 2 + 3</span><br><span class="line">5</span><br></pre></td></tr></tbody></table></figure><ol start="2"><li>计算3-2 的值</li></ol><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sh-5.1# expr 3 - 2</span><br><span class="line">1</span><br><span class="line">sh-5.1#</span><br></pre></td></tr></tbody></table></figure><ol start="3"><li>计算（2+3）*4的值</li></ol><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">sh: 1: command not found</span><br><span class="line">sh-5.1# expr 'expr 2 + 3' \* 4</span><br><span class="line"></span><br><span class="line">expr: non-integer argument</span><br><span class="line"></span><br><span class="line">采用 $[运算式]方式</span><br><span class="line">sh-5.1#  s=$[(2+3)*4]</span><br><span class="line">sh-5.1# echo $s</span><br><span class="line">20</span><br></pre></td></tr></tbody></table></figure><h3 id="第六章-条件判断"><a href="#第六章-条件判断" class="headerlink" title="第六章 条件判断"></a>第六章 条件判断</h3><h4 id="6-1-基本语法"><a href="#6-1-基本语法" class="headerlink" title="6.1 基本语法"></a>6.1 基本语法</h4><p>[ condition ]  （注意 condition 前后要有空格）</p><p>注意： 条件非空即为true， [ atguigu ] 返回true， []返回false</p><h4 id="6-2-常用判断条件"><a href="#6-2-常用判断条件" class="headerlink" title="6.2 常用判断条件"></a>6.2 常用判断条件</h4><ol><li><p>两个整数之间比较</p><p>= 字符串比较</p><p>-lt 小于（less than)                                        -le 小于等于(less equal)</p><p>-eq 等于（equal）                                        -gt 大于 (greater than)</p><p>-ge 大于等于（greater equal）                   -ne 不等于(Not equal)</p></li><li><p>按照文件权限进行判断</p></li></ol><p>​    -r 有读的权限(read)                                            -w 有写的权限(write)</p><p>​    -x 有执行的权限(execute)                </p><ol start="3"><li><p>按照文件类型进行判断</p><p>-f  文件存在 并且是一个常规的文件(file)</p><p>-e 文件存在（existence）                                     -d 文件存在并且是一个目录(directory)</p></li></ol><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">23 是否大于等于22</span><br><span class="line">sh-5.1# [ 23 -ge 22 ]</span><br><span class="line">sh-5.1# echo $?</span><br><span class="line">0</span><br><span class="line"></span><br><span class="line">helloworld.sh 是否具有写权限</span><br><span class="line">sh-5.1# [ -w helloworld.sh ]</span><br><span class="line">sh-5.1# echo $?</span><br><span class="line">0</span><br></pre></td></tr></tbody></table></figure><ol start="4"><li>多条件判断(&amp;&amp; 表示前面一条命令执行成功时候，才执行后一条命令， || 表示上一条命令执行失败后，才执行下一条命令)</li></ol><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">sh-5.1# [ condition ] &amp;&amp; echo OK || echo notok</span><br><span class="line">OK</span><br><span class="line">sh-5.1# [  ] &amp;&amp; echo OK || echo notok</span><br><span class="line">notok</span><br></pre></td></tr></tbody></table></figure><h4 id="第七章-流程控制"><a href="#第七章-流程控制" class="headerlink" title="第七章 流程控制"></a>第七章 流程控制</h4><h5 id="7-1-if-判断"><a href="#7-1-if-判断" class="headerlink" title="7.1 if 判断"></a>7.1 if 判断</h5><p>if [ 条件判断式 ]:then</p><p>​     程序</p><p>fi</p><p>或者</p><p>if [ 条件判断式 ]</p><p>​    then</p><p>​            程序</p><p>fi</p><p>注意事项：</p><ol><li>[ 条件判断式 ]， 中括号和条件判断式之间必须有空格</li><li>if 后要有空格</li></ol><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">输入一个数字， 如果是1 ， 则输出banzhang zhenshuai， 如果是2，则输出shell mei，如果是其他，则什么也不输出</span><br><span class="line"><span class="meta prompt_"></span></span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">if [ $1 -eq 1 ]</span><br><span class="line">then</span><br><span class="line">        echo "banzhangzhenshuai"</span><br><span class="line"></span><br><span class="line">elif [ $1 -eq 2 ]</span><br><span class="line">then</span><br><span class="line">        echo "shell mei"</span><br><span class="line"></span><br><span class="line">fi</span><br><span class="line"></span><br><span class="line">sh-5.1# bash if.sh</span><br><span class="line">if.sh: line 3: [: -eq: unary operator expected</span><br><span class="line">if.sh: line 7: [: -eq: unary operator expected</span><br><span class="line">sh-5.1# bash if.sh 1</span><br><span class="line">banzhangzhenshuai</span><br><span class="line">sh-5.1# bash if.sh \2</span><br><span class="line">shell mei</span><br><span class="line">sh-5.1# bash if.sh 2</span><br><span class="line">shell mei</span><br></pre></td></tr></tbody></table></figure><h5 id="7-2-case-断句"><a href="#7-2-case-断句" class="headerlink" title="7.2 case 断句"></a>7.2 case 断句</h5><p>case $变量名 in</p><p>“值1”）</p><p>​       如果变量的值等于1，则执行程序1</p><p>   ;;</p><p>“值2”）</p><p>​       如果变量的值等于2，则执行程序2</p><p>   ;;</p><p>…省略其他分支…</p><p>*）</p><p>如果变量的值都不是以上的值，则执行次程序</p><p>;;</p><p>esac</p><p>注意事项：</p><ol><li>case行结尾必须为单词“in” ， 每一个模式匹配必须以右括号“）” 结束。</li><li>双分号“;;”表示命令序列结束，相当于java中的break</li><li>最后的“*）”表示默认模式， 相当于java中的default</li></ol><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">输入一个数字，如果是1 则输出bangzag ，如果是2 则输出java，如果是其他则输出python</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">case $1 in</span><br><span class="line">1) echo "c++"</span><br><span class="line">;;</span><br><span class="line">2) echo "java"</span><br><span class="line">;;</span><br><span class="line">*) echo "python"</span><br><span class="line">;;</span><br><span class="line">esac</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">sh-5.1# vim case.sh</span><br><span class="line">sh-5.1# bash case.sh 2</span><br><span class="line">case.sh: line 10: syntax error: unexpected end of file</span><br><span class="line">sh-5.1# vim case.sh</span><br><span class="line">sh-5.1# bash case.sh 2</span><br><span class="line">java</span><br><span class="line">sh-5.1# bash case.sh 3</span><br><span class="line">python</span><br><span class="line">sh-5.1# bash case.sh 1</span><br><span class="line">c++</span><br></pre></td></tr></tbody></table></figure><h5 id="7-3-for循环"><a href="#7-3-for循环" class="headerlink" title="7.3 for循环"></a>7.3 for循环</h5><p>for (( 初始值;循环控制条件;变量变化 ))</p><p>   do</p><p>​        程序</p><p>   done</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">从1 加到100</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">s=0</span><br><span class="line">for(( i=0;i&lt;=100;i++ ))</span><br><span class="line">do</span><br><span class="line">        s=$[$s+$i]</span><br><span class="line">done</span><br><span class="line">echo $s</span><br><span class="line"></span><br><span class="line">sh-5.1# vim for1.sh</span><br><span class="line">sh-5.1# bash for1.sh</span><br><span class="line">5050</span><br></pre></td></tr></tbody></table></figure><p>语法2</p><p>for 变量 in 值1 值2 值3…</p><p>do </p><p>​       程序</p><p>done</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">打印所有输入参数</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line">for i in $*</span><br><span class="line">do</span><br><span class="line">        echo "shell love $i"</span><br><span class="line">done</span><br><span class="line">for k in "$*"</span><br><span class="line">do</span><br><span class="line">                echo "shell love $k"</span><br><span class="line">done</span><br><span class="line"></span><br><span class="line">for j in "$@"</span><br><span class="line">do</span><br><span class="line">        echo "shell love $j"</span><br><span class="line">done</span><br><span class="line">sh-5.1# touch for2.sh</span><br><span class="line">sh-5.1# vim for2.sh</span><br><span class="line">sh-5.1# sh for2.sh 1 2 3</span><br><span class="line">shell love 1</span><br><span class="line">shell love 2</span><br><span class="line">shell love 3sh-5.1# sh for2.sh 1 2 3</span><br><span class="line">shell love 1</span><br><span class="line">shell love 2</span><br><span class="line">shell love 3</span><br><span class="line">shell love 1 2 3</span><br><span class="line">shell love 1</span><br><span class="line">shell love 2</span><br><span class="line">shell love 3</span><br></pre></td></tr></tbody></table></figure><h5 id="7-4-while循环"><a href="#7-4-while循环" class="headerlink" title="7.4 while循环"></a>7.4 while循环</h5><p>while [ 条件判断式 ]</p><p>do</p><p>   程序</p><p>done</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">从1+到100</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">s=0</span><br><span class="line">i=1</span><br><span class="line">while [ $i -le 100 ]</span><br><span class="line">do</span><br><span class="line">        s=$[$s + $i]</span><br><span class="line">        i=$[$i + 1]</span><br><span class="line">done</span><br><span class="line">echo $s</span><br><span class="line">~        </span><br><span class="line">sh-5.1# bash while.sh</span><br><span class="line">5050</span><br></pre></td></tr></tbody></table></figure><h4 id="第八章-read-读取控制台输入"><a href="#第八章-read-读取控制台输入" class="headerlink" title="第八章 read 读取控制台输入"></a>第八章 read 读取控制台输入</h4><p>read(选项)(参数)</p><p>选项：</p><p>-p： 指定读取值时的提示符：</p><p>-t :   指定读取值时等待的时间（秒）</p><p>参数</p><p>​       变量：指定读取值得变量名</p><ol><li>提示7s内读取控制台输入的名称</li></ol><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">read -t 7 -p "Enter your name i 7 seconds " NAME</span><br><span class="line">echo $NAME</span><br><span class="line">~          </span><br><span class="line">sh-5.1# bash read.sh shell</span><br><span class="line">Enter your name i 7 seconds shell</span><br><span class="line">shell</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h4 id="第九章-函数"><a href="#第九章-函数" class="headerlink" title="第九章 函数"></a>第九章 函数</h4><h5 id="9-1-系统函数"><a href="#9-1-系统函数" class="headerlink" title="9.1 系统函数"></a>9.1 系统函数</h5><p>basename基本语法</p><p>basename [string/pathname] [suffix] (功能描述： basename 的命令会删掉所有前缀包括最后一个（’/‘)字符，然后将字符串显示出来</p><p>选项：</p><p>suffix为后缀， 如果suffix被指定了， basenname会将pathname或者string中的suffix去掉。</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">截取该/homeshell/banzhang.txt路径的文件名称</span><br><span class="line">sh-5.1# basename /homeshell/banzhang.txt .txt</span><br><span class="line">banzhang</span><br></pre></td></tr></tbody></table></figure><p>dirname   文件绝对路径 ( 功能描述： 从给定的包含绝对路径中取出文件名（非目录的部分)，然后返回剩下的路径)</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">获取截取该/homeshell/banzhang.txt路径的文件路径</span><br><span class="line">sh-5.1# dirname /homeshell/banzhang.txt</span><br><span class="line">/homeshell</span><br></pre></td></tr></tbody></table></figure><h5 id="9-2-自定义函数"><a href="#9-2-自定义函数" class="headerlink" title="9.2 自定义函数"></a>9.2 自定义函数</h5><p>基本语法</p><p>[ function ] funname[()]</p><p>{</p><p>​Action；</p><p>​[return int;]</p><p>}</p><ol start="2"><li>经验技巧</li></ol><p>2.1 必须在调用地方之前，先声明函数， shell脚本是逐行运行的， 不会像其他语言一样 先编译。</p><p>2.2 函数返回值， 只能通过$?系统变量或得，可以显示加:return 返回，如果不加，将以最后一条命令运行结果作为返回值，return后跟数值n(0-255)</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">计算两个输入参数的和</span><br><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">function sum()</span><br><span class="line">{</span><br><span class="line">        s=0;</span><br><span class="line">        s=$[$1+$2]</span><br><span class="line">        echo $s</span><br><span class="line">}</span><br><span class="line">read -p "input your paramter1: " P1</span><br><span class="line">read -p "input your paramter1: " P2</span><br><span class="line">sum $P1 $P2</span><br><span class="line"></span><br><span class="line">sh-5.1# vim sum.sh</span><br><span class="line">sh-5.1# bash sum.sh</span><br><span class="line">input your paramter1:</span><br><span class="line">input your paramter1:</span><br><span class="line">sum.sh: line 6: +: syntax error: operand expected (error token is "+")</span><br><span class="line">sh-5.1# bash sum.sh</span><br><span class="line">input your paramter1: 10</span><br><span class="line">input your paramter1: 20</span><br><span class="line">30</span><br></pre></td></tr></tbody></table></figure><h4 id="第十章-shell工具（重点）"><a href="#第十章-shell工具（重点）" class="headerlink" title="第十章 shell工具（重点）"></a>第十章 shell工具（重点）</h4><h5 id="10-1-cut"><a href="#10-1-cut" class="headerlink" title="10.1 cut"></a>10.1 cut</h5><p>cut的工作就是“剪”， 具体的说就是在文件中负责剪切数据用的。cut命令从文件的每一行剪切字节、字符和字段并将这些值输出</p><ol><li>基本用法</li></ol><p>​     cut[选项参数]  filenames</p><p>​    说明: 默认分割符是制表符</p><ol start="2"><li>选项参数说明</li></ol><p><img src="/images/image-20220710225214935.png" alt="image-20220710225214935"></p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">数据准备</span><br><span class="line">sh-5.1# touch cut.txt</span><br><span class="line">sh-5.1# vim cut.txt</span><br><span class="line">sh-5.1# cat cut.txt</span><br><span class="line">dong sheng</span><br><span class="line">guan zheng</span><br><span class="line">wo wo</span><br><span class="line">la   la</span><br><span class="line">lei lei</span><br><span class="line"></span><br><span class="line">sh-5.1# cut -d " " -f 1 cut.txt</span><br><span class="line">dong</span><br><span class="line">guan</span><br><span class="line">wo</span><br><span class="line">la</span><br><span class="line">lei</span><br><span class="line"></span><br><span class="line">在cut文件中切割出guan</span><br><span class="line">h-5.1# cat cut.txt | grep guan</span><br><span class="line">guan zheng</span><br><span class="line">sh-5.1# cat cut.txt | grep guan | cut -d " " -f 1</span><br><span class="line">guan</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">选取系统PATH变量值， 第二个“：”开始后的所有路径：</span><br><span class="line">sh-5.1# echo $PATH</span><br><span class="line">/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin</span><br><span class="line">sh-5.1# echo $PATH | cut -d : -f 3-</span><br><span class="line">/usr/sbin:/usr/bin:/sbin:/bin</span><br><span class="line">切割ifconfig后打印的ip地址</span><br><span class="line">失败了。</span><br><span class="line">sh-5.1# ifconfig eth0 | grep "inet" | cut -d '' -f 2</span><br><span class="line">        inet 172.17.0.2  netmask 255.255.0.0  broadcast 172.17.255.255</span><br><span class="line">sh-5.1#</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/image-20220710230520164.png" alt="image-20220710230520164"></p><h5 id="10-2-sed"><a href="#10-2-sed" class="headerlink" title="10.2 sed"></a>10.2 sed</h5><p>  sed是一种流编辑器， 它一次处理一行内容， 处理时，吧当前处理的行存储在临时缓存区中， 称为：模式空间，接着用sed命令处理缓冲区中的内容， 处理完成后，把缓冲区的内容送往屏幕， 接着处理下一行， 这样不断重复， 直到文件末尾。文件内容没有改变， 除非你使用重定向存储输出。</p><p>基本用法</p><p>sed[选项参数] “command”  filename</p><p><img src="/images/image-20220710230902184.png" alt="image-20220710230902184"></p><h4 id="10-2-2-实战"><a href="#10-2-2-实战" class="headerlink" title="10.2.2 实战"></a>10.2.2 实战</h4><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br></pre></td><td class="code"><pre><span class="line">将mei nv 这个单词插入到sed.txt 第二行下， 打印</span><br><span class="line">sh-5.1# touch sed.txt</span><br><span class="line">sh-5.1# vim sed.txt</span><br><span class="line">sh-5.1# sed '2a mei nv' sed.txt</span><br><span class="line">dong shen</span><br><span class="line">ot python</span><br><span class="line">mei nv</span><br><span class="line">ll ll</span><br><span class="line">ko ok</span><br><span class="line">删除文件中包含所有wo的行</span><br><span class="line">sh-5.1# sed "/m/e" sed.txt</span><br><span class="line">dong shen</span><br><span class="line">ot python</span><br><span class="line">ll ll</span><br><span class="line">ko ok</span><br><span class="line">将sed中wo替换为ni</span><br><span class="line">sh-5.1# sed "s/ot/to/g" sed.txt</span><br><span class="line">dong shen</span><br><span class="line">to python</span><br><span class="line">ll ll</span><br><span class="line">ko ok</span><br><span class="line">注意：</span><br><span class="line">g表示global ，全部替换</span><br><span class="line"></span><br><span class="line">将sed中第二行删除并将wo替换为ni</span><br><span class="line">sh-5.1# sed -e "2d" -e "s/to/ot/g" sed.txt</span><br><span class="line">dong shen</span><br><span class="line">ll ll</span><br><span class="line">ko ok</span><br></pre></td></tr></tbody></table></figure><h4 id="10-3-awk"><a href="#10-3-awk" class="headerlink" title="10.3 awk"></a>10.3 awk</h4><p>一个强大的文本分析工具， 把文件逐行的读入，以空格为默认分隔符将每行切片， 切开的部分再进分析处理。</p><ol><li>基本用法</li><li>awk [选项参数] ‘pattern1{action1} pattern2{action2} ….’ filename</li><li>pattern: 表示AWK在数据中查找的内容，就是匹配模式</li><li>action:在找到匹配时，所执行的一系列命令</li></ol><p><img src="/images/image-20220710231854660.png" alt="image-20220710231854660"></p><h5 id="实操"><a href="#实操" class="headerlink" title="实操"></a>实操</h5><p>数据准备</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sh-5.1# cp /etc/passwd ./</span><br><span class="line">sh-5.1# ls</span><br><span class="line">banzhang.txt  case.sh  cut.txt  for1.sh  for2.sh  hadoop101.bahs  hadoop101.sh  helloworld.sh  if.sh  parameter.sh  passwd  read.sh  sed.txt  sum.sh  while.sh</span><br><span class="line">sh-5.1# awk -F: '/^root/{print $7}' passwd /bin/bash</span><br><span class="line">/bin/bash</span><br><span class="line">sh-5.1#</span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><ol><li><p>搜索passwd文件以root关键词开头的所有行，并输出该行的第7列</p></li><li><p>搜索passwd文件以root关键词开头的所有行，并输出该行的第7列和第一列，中间以“，” 分隔</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sh-5.1# awk -F: '/^root/{print  $1“，”$7}' passwd /bin/bash</span><br></pre></td></tr></tbody></table></figure><p>注意只有匹配了pattern的行才会执行action</p></li><li><p>只显示、etc/passwd的第一列和第七列，以逗号分隔，且在所有行前面铁建列名user， shell在最后一行叠加“dahaige , /bin/zuishuai”.</p>  <figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">awk -F: 'BEGIN{print "user,shell"}{print $1","$7} END{print "dahaiuge,/bin/zuishuai"}' passwd</span><br></pre></td></tr></tbody></table></figure></li></ol><p>注意LBEGIN在所有数据读取行之前执行， END在所有数据执行完之后执行</p><ol start="4"><li><p>将passwd中的用户id增加数值1并输出</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">awk -vi1-F: '{print $3+i}' passwd</span><br></pre></td></tr></tbody></table></figure></li></ol><h5 id="awk的内置变量"><a href="#awk的内置变量" class="headerlink" title="awk的内置变量"></a>awk的内置变量</h5><p><img src="/images/image-20220710232835635.png" alt="image-20220710232835635"></p><h5 id="实操-1"><a href="#实操-1" class="headerlink" title="实操"></a>实操</h5><ol><li>统计passwd文件名，每行的行号，每行的列数</li></ol><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">awk -F: '{print "filename:" FILENAME ", linenumer:" NR ",columns:" NF}' passwd</span><br></pre></td></tr></tbody></table></figure><ol start="2"><li><p>切割ip</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ifconfig eth0 | grep "inet addr" | awk -F : '{print $2}' | awk -F " " '{print $1}'</span><br></pre></td></tr></tbody></table></figure></li><li><p>查询sed.txt中空行所在的行号</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">awk '/^$/{print NR}' sed.txt</span><br></pre></td></tr></tbody></table></figure></li></ol><h4 id="10-4-sort"><a href="#10-4-sort" class="headerlink" title="10.4 sort"></a>10.4 sort</h4><p>sort 命令在LInux里非常游泳，它将文件排序，并将排序结果标准输出</p><p><img src="/images/image-20220711002337565.png" alt="image-20220711002337565"></p><h5 id="实操-2"><a href="#实操-2" class="headerlink" title="实操"></a>实操</h5><ol><li><p>按照“：” 分隔之后排序</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sort-t ： -nrk 2 sort.sh</span><br></pre></td></tr></tbody></table></figure></li></ol><h4 id="第十一章-企业面试题"><a href="#第十一章-企业面试题" class="headerlink" title="第十一章 企业面试题"></a>第十一章 企业面试题</h4><h5 id="11-1-京东"><a href="#11-1-京东" class="headerlink" title="11.1 京东"></a>11.1 京东</h5><p>问题1 ：使用linux命令查询file1中空行所在的行号</p><p>答案：</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">awk '/^$/{print NR}' sed.txt</span><br></pre></td></tr></tbody></table></figure><p> 问题2： 有文件chengji.txt内容如下：</p><p>张三 40</p><p>李四 50</p><p>王五 60</p><p>使用linux命令计算第二列的和并输出</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cat chenji.TXT| AWK -F " " '{sum+=$2} END{print sum}'</span><br></pre></td></tr></tbody></table></figure><h5 id="11-2-搜狐-amp-和讯网"><a href="#11-2-搜狐-amp-和讯网" class="headerlink" title="11.2 搜狐&amp;和讯网"></a>11.2 搜狐&amp;和讯网</h5><p>问题1： shell脚本里如何检查一个文件是否存在，如果不存在该如何处理？</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta prompt_">#</span><span class="language-bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line">if [ -f file.txt ]; then</span><br><span class="line">   echo "文件存在！"</span><br><span class="line">else</span><br><span class="line">   echo "文件不存在！"</span><br><span class="line">fi</span><br></pre></td></tr></tbody></table></figure><h5 id="11-3-新浪"><a href="#11-3-新浪" class="headerlink" title="11.3 新浪"></a>11.3 新浪</h5><p>问题1： 用shell写一个脚本，对文本中无序的一列数据排序</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sort -n  test.txt|awk '{a+=$0;print$0}END{print "SUM-"a}'</span><br></pre></td></tr></tbody></table></figure><h5 id="11-4-金和网络"><a href="#11-4-金和网络" class="headerlink" title="11.4 金和网络"></a>11.4 金和网络</h5><p>问题1： 请用shell脚本写出 查找当前文件夹，(/home) 下所有文本文件内容包含有字符“shen”的文件名称</p><figure class="highlight shell"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">grep -r "shen" /home |cut -d ":" -f 1 &lt;留下文件名&gt;</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>单应矩阵H求解</title>
      <link href="/2023/01/22/%E5%8D%95%E5%BA%94%E7%9F%A9%E9%98%B5H%E6%B1%82%E8%A7%A3/"/>
      <url>/2023/01/22/%E5%8D%95%E5%BA%94%E7%9F%A9%E9%98%B5H%E6%B1%82%E8%A7%A3/</url>
      
        <content type="html"><![CDATA[<h2 id="单应矩阵H求解"><a href="#单应矩阵H求解" class="headerlink" title="单应矩阵H求解"></a>单应矩阵H求解</h2><p>所需求解的单应矩阵：<br>$$<br>H_{3 \times 3}=\left[\begin{array}{lll}<br>h_{11} &amp; h_{12} &amp; h_{13} \<br>h_{21} &amp; h_{22} &amp; h_{23} \<br>h_{31} &amp; h_{32} &amp; h_{33}<br>\end{array}\right]<br>$$<br>单应变换关系：<br>$$<br>\mathrm{s}\left[\begin{array}{c}<br>x^{\prime} \<br>y^{\prime} \<br>1<br>\end{array}\right]=H\left[\begin{array}{l}<br>x \<br>y \<br>1<br>\end{array}\right]=\left[\begin{array}{lll}<br>h_{11} &amp; h_{12} &amp; h_{13} \<br>h_{21} &amp; h_{22} &amp; h_{23} \<br>h_{31} &amp; h_{32} &amp; h_{33}<br>\end{array}\right]\left[\begin{array}{l}<br>x \<br>y \<br>1<br>\end{array}\right]<br>$$<br>为减少自由度， 令h<sub>33</sub> =1, s为尺度因子。<br>$$<br>h_{31} x+h_{32} y+h_{33}=h_{31} x+h_{32} y+1<br>$$</p><p>$$<br>\begin{aligned}<br>x_{i}^{\prime} &amp;=\frac{h_{11} x_{i}+h_{12} y_{i}+h_{13}}{h_{31} x_{i}+h_{32} y_{i}+h_{33}} \<br>y_{i}^{\prime} &amp;=\frac{h_{21} x_{i}+h_{22} y_{i}+h_{23}}{h_{31} x_{i}+h_{32} y_{i}+h_{33}}<br>\end{aligned}<br>$$</p><p>在图像上取在真实世界构成矩形的顺时针的四个角点。</p><p>(x1,y1),(x2,y2),(x3,y3),(x4,y4)以及构成直角的三个点</p><p>(x5,y5),(x6,y6),(x7,y7), 其中（x6,y6)为直角点</p><p>预设期望变换后的矩形，第一个焦点的位置（x1’,y1’)(x2’,y2’) 设计方程组：</p><p>令（x1，y1) (x2,y2) 变换到预设位置（x1’,y1’)(x2’,y2’)<br>$$<br>\begin{aligned}<br>x_{1}^{\prime} &amp;=\frac{h_{11} x_{1}+h_{12} y_{1}+h_{13}}{h_{31} x_{1}+h_{32} y_{1}+h_{33}} \<br>x_{2}^{\prime} &amp;=\frac{h_{11} x_{2}+h_{12} y_{2}+h_{13}}{h_{31} x_{2}+h_{32} y_{2}+h_{33}} \<br>y_{1}^{\prime} &amp;=\frac{h_{21} x_{1}+h_{22} y_{1}+h_{23}}{h_{31} x_{1}+h_{32} y_{1}+h_{33}} \<br>y_{2}^{\prime} &amp;=\frac{h_{21} x_{2}+h_{22} y_{2}+h_{23}}{h_{31} x_{2}+h_{32} y_{2}+h_{33}}<br>\end{aligned}<br>$$<br>令（x1’,y1’)和(x4’,y4’)构成的直线垂直与（x1’,y1’)(x2’,y2’) 构成的直线<br>$$<br>\frac{y_{2}^{\prime}-y_{1}^{\prime}}{x_{2}^{\prime}-x_{1}^{\prime}} \cdot \frac{y_{4}^{\prime}-y_{1}^{\prime}}{x_{4}^{\prime}-x_{1}^{\prime}}=-1<br>$$<br>令x’4到x1’的距离等于x3’ 到x2’的距离， y4’到y1’的距离等于y3’到y2‘的距离，使得(x4’,y4’)和(x3’,y3’)在（x1’,y1’)和(x2’,y2’) 构成的直线的同一侧， 且结合上一个约束， 使得(x2’,y2’) 和(x3’,y3’)直线平行与（x1’,y1’)和(x4’,y4’) 构成的直线，即(x2’,y2’) 和(x3’,y3’) 垂直与</p><p>（x1’,y1’)和(x2’,y2’) 构成的直线<br>$$<br>\begin{aligned}<br>&amp;x_{4}^{\prime}-x_{3}^{\prime}=x_{1}^{\prime}-x_{2}^{\prime} \<br>&amp;y_{4}^{\prime}-y_{3}^{\prime}=y_{1}^{\prime}-y_{2}^{\prime}<br>\end{aligned}<br>$$<br>令(x5’,y5’) 和(x6’,y6’) 垂直与（x6’,y6’)和(x7’,y7’) 构成的直线<br>$$<br>\frac{y_{5}^{\prime}-y_{6}^{\prime}}{x_{5}^{\prime}-x_{6}^{\prime}} \cdot \frac{y_{7}^{\prime}-y_{6}^{\prime}}{x_{7}^{\prime}-x_{6}^{\prime}}=-1<br>$$<br>联立上述8个方程和h33=1可求出单应矩阵H</p><p>此外为了简化求解， 可使得预设期望的变换后的矩形第一个角点（x1’,y1’)，第二个角点的位置（x2’,y2’)的y1和y2 或者x1和x2 相等， 只需要x3=x2 ,x4=x1 即可以保证垂直关系， 计算将大大简化</p><p>通过多选几组点， 结合RANSAC等算法可进行全局优化。</p><p><img src="/images/image-20220717235918026.png" alt="image-20220717235918026"></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>智慧交通项目</title>
      <link href="/2023/01/22/%E6%99%BA%E6%85%A7%E4%BA%A4%E9%80%9A%E9%A1%B9%E7%9B%AE/"/>
      <url>/2023/01/22/%E6%99%BA%E6%85%A7%E4%BA%A4%E9%80%9A%E9%A1%B9%E7%9B%AE/</url>
      
        <content type="html"><![CDATA[<h2 id="智慧交通项目"><a href="#智慧交通项目" class="headerlink" title="智慧交通项目"></a>智慧交通项目</h2><h3 id="1-项目简介"><a href="#1-项目简介" class="headerlink" title="1. 项目简介"></a>1. 项目简介</h3><p><strong>学习目标：</strong></p><ul><li>了解智慧交通项目的架构</li><li>了解智慧交通项目中的模块</li><li>完成智慧交通项目的环境搭建</li></ul><p>主要讲解计算机视觉在交通领域的相关应用， 包括车道线检测、多目标车辆追踪以及流量统计方法</p><ul><li>多目标车辆追踪和计数：SORT方法、匈牙利算法。卡尔曼滤波、虚拟线圈等</li><li>车道线检测：张氏相机校正、仿射变换等。</li></ul><p><img src="/images/image-20220819003607691.png" alt="image-20220819003607691"></p><p>项目的架构</p><ul><li>用户层</li><li>服务层</li><li><img src="/images/image-20220819003735927.png" alt="image-20220819003735927"></li></ul><h3 id="2-环境安装"><a href="#2-环境安装" class="headerlink" title="2. 环境安装"></a>2. 环境安装</h3><p>工具包如下：</p><p>Numpy</p><p>Numba</p><p>Scipy</p><p>h5py</p><p>pandas</p><p>opencv-python</p><p>moviepy</p><p>Filterpy</p><p><strong>安装方法</strong></p><figure class="highlight sh"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><h3 id="3-车流量检测实现"><a href="#3-车流量检测实现" class="headerlink" title="3.车流量检测实现"></a>3.车流量检测实现</h3><h4 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h4><ul><li>了解多目标跟跟踪的实现方法</li><li>知道车流量统计的方法</li></ul><hr><p>车流量统计主要有以下几种方式：</p><ul><li>人工统计， 耗时耗力，且统计结果不具有可验证性</li><li>传感器计数， 容易收到外界环境因素干扰</li><li>基于视频的车流量统计， 本项目所使用的方法</li></ul><p>主要包括以下几个步骤：</p><ol><li>使用yolov3模型进行目标检测</li><li>使用sort算法进行目标跟踪， 使用卡尔曼滤波进行目标位置预测，并利用匈牙利算法对比目标的相似度， 完成车辆目标跟踪</li><li>利用虚拟线圈的思想来实现车辆目标的技术，完成车流量的统计。</li></ol><p>项目流程如下图所示：</p><p><img src="/images/image-20220820222005495.png" alt="image-20220820222005495"></p><h4 id="3-0-项目介绍"><a href="#3-0-项目介绍" class="headerlink" title="3.0 项目介绍"></a>3.0 项目介绍</h4><h4 id="3-1-多目标跟踪"><a href="#3-1-多目标跟踪" class="headerlink" title="3.1 多目标跟踪"></a>3.1 多目标跟踪</h4><h5 id="学习目标："><a href="#学习目标：" class="headerlink" title="学习目标："></a>学习目标：</h5><ul><li>了解多目标跟着的常见分类方法</li><li>了解多目标跟踪中常用的运动模型</li><li>知道多目标跟着的常用算法</li></ul><h5 id="1-多目标跟踪分类"><a href="#1-多目标跟踪分类" class="headerlink" title="1. 多目标跟踪分类"></a>1. 多目标跟踪分类</h5><p>多目标跟踪，即MOT，也就是在一段视频中同时跟踪多个目标。MOT主要应用在安防监控和自动驾驶等领域中。</p><p><img src="/images/image-20220820223032905.png" alt="image-20220820223032905"></p><p>多目标跟踪可以看做多变量估计问题， 即给定一个图像序列</p><p><img src="/images/image-20220820223139570.png" alt="image-20220820223139570"></p><p>1.1 初始化方法</p><p>多目标跟踪问题，并不是所有目标都会在第一帧出现，也并不是所有目标都会在出现在每一帧，。那如何对出现的目标进行初始化， 可以作为跟踪算法的分类表针， 常见的初始化方法分为两大类，一个是Detection-based-tracking（DBT），另一个是Detection-free-tracking(DFT)。</p><p><img src="/images/image-20220820223509408.png" alt="image-20220820223509408"></p><p><img src="/images/image-20220820223550971.png" alt="image-20220820223550971"></p><p>1.2 处理模式</p><p>MOT也存在着不同的处理模式， Online和Offline两大类， 其主要区别在于是否用了后续帧信息， 下图形象的解释了Online和offline的区别</p><p><img src="/images/image-20220820223736305.png" alt="image-20220820223736305"></p><ul><li>Online Tracking</li><li>Online Tracking 是对视频帧进行逐帧进行处理， 当前帧的跟踪仅仅利用过去的信息。</li><li>Offline tracking</li></ul><p>不同于Online Tracking， Offline Tracking会利用前后视频帧对当前帧进行目标跟踪， 这种方式只适合视频， 如果应用于摄像头，则会有滞后效应， 通常采用时间窗的方式进行处理，节省内存和加速</p><h5 id="2-运动模型"><a href="#2-运动模型" class="headerlink" title="2. 运动模型"></a>2. 运动模型</h5><p>为了简化多目标跟踪的难度， 引入运动模型类简化求解过程，运动模型捕捉目标的动态行为，它估计目标在未来帧中的潜在位置， 从而减少搜索空间。在大多数情况下， 假设目标在现实中是平缓运动的， 那么图像空间也是如此，对于车辆的运动， 大致可以分为线性和非线性两种运动：</p><ul><li>线性运动： 线性运动是目标最主流的模型，假设目标的运动属性平稳（速度，加速度，位置）</li><li>非线性运动：虽然线性运动模型比较常用， 但由于存在它解决不了的问题，非线性模型随之产生，它可以使tracklets间运动相似度计算得更加准确。</li></ul><h5 id="3-跟踪方法"><a href="#3-跟踪方法" class="headerlink" title="3. 跟踪方法"></a>3. 跟踪方法</h5><p>多目标跟踪基于神经网络的算法， 端到端的算法并不多，主要还在实验室刷榜阶段，模型复杂，速度慢， 效果不好。 主要介绍两种主流的算法：</p><p><img src="/images/image-20220820233546906.png" alt="image-20220820233546906"></p><p>3.1 基于Kalman和KM算法的后端优化算法</p><p>该类算法能达到实时性， 但依赖于检测算法效果摇号，特征区分要好， （输出最终结果的好坏，依赖于较强的检测算法，而基于卡尔曼加匈牙利匹配的跟踪算法作用于能够输出检测目标的id，其次能够保证追踪算法的实时性，这样追踪效果会好，id切换少代表的算法是SORT/deepSort</p><p><img src="/images/image-20220820233921267.png" alt="image-20220820233921267"></p><p>SORT是一种实用的多目标跟踪算法，引入线性速度模型于卡尔曼滤波来进行位置预测，再无合适匹配检测框的情况下，使用运动模型来预测物体的位置。匈牙利算法是一种寻找二分图的最大匹配的算法，在多目标跟踪问题中可以简单理解为寻找前后两帧的若干目标的匹配最优解的一种算法。而卡尔曼滤波可以看做是一种运动模型，用来解决对目标轨迹预测，并使用置信度比较高的跟踪结果进行预测结果的修正。</p><p>多目标跟踪一般接在目标检测后， 在工业界目标检测采用比较多的是yolo检测算法。先实现目标检测网络， 检测的输出结果主要是将检测框的位置信息输入到多目标跟踪算法中。</p><p>3.2 基于多线程的单目标跟踪的多目标跟踪算法</p><p>这类算法特点是跟踪效果很好，因为每一个类的物体都单独分配了一个跟踪器， 但该算法对尺度变换的要求大，参数调试需要合理，同时耗费cpu资源，实时性不高， 代表算法是利用KCF进行目标跟踪。</p><p>多目标跟踪本质上是多个目标同时运动的问题， 所以有提出将单目标跟踪器引入到多目标追踪的问题，为每一个目标分配一个跟踪器。然后检测地使用匹配算法来修正那些跟踪失败或者新出现的目标，代表性的单目标跟踪算法为核相关滤波算法（KCF），在精度核速度上同时达到很高的水平，是当时单目标跟踪最优秀的算法之一，后来的很多算法都是基于此做的改进。</p><p>实际应用过程会为每个目标分配一个KCF跟踪器并采用多线程的方式来组织这些跟踪器，同时因为实际硬件条件的限制，不可能提供强大的计算资源，会采用检测器与跟踪交替策略，由于检测帧率不高， 跟踪效果滞后性， 实用效果不大。</p><p><img src="/images/image-20220820235435060.png" alt="image-20220820235435060"></p><h4 id="3-2-辅助功能"><a href="#3-2-辅助功能" class="headerlink" title="3.2 辅助功能"></a>3.2 辅助功能</h4><hr><p>yolo的数据格式</p><p>iou的详解</p><h4 id="3-3-卡尔曼滤波器"><a href="#3-3-卡尔曼滤波器" class="headerlink" title="3.3 卡尔曼滤波器"></a>3.3 卡尔曼滤波器</h4><h4 id="3-4-卡尔曼滤波器实战"><a href="#3-4-卡尔曼滤波器实战" class="headerlink" title="3.4 卡尔曼滤波器实战"></a>3.4 卡尔曼滤波器实战</h4><h4 id="3-5-目标估计模型-卡尔曼滤波"><a href="#3-5-目标估计模型-卡尔曼滤波" class="headerlink" title="3.5 目标估计模型-卡尔曼滤波"></a>3.5 目标估计模型-卡尔曼滤波</h4><h4 id="3-6-匈牙利算法"><a href="#3-6-匈牙利算法" class="headerlink" title="3.6 匈牙利算法"></a>3.6 匈牙利算法</h4><h4 id="3-7-数据关联"><a href="#3-7-数据关联" class="headerlink" title="3.7 数据关联"></a>3.7 数据关联</h4><h4 id="3-8-SORT-x2F-deepSORT"><a href="#3-8-SORT-x2F-deepSORT" class="headerlink" title="3.8 SORT/deepSORT"></a>3.8 SORT/deepSORT</h4><h4 id="3-9-多目标跟着实现"><a href="#3-9-多目标跟着实现" class="headerlink" title="3.9 多目标跟着实现"></a>3.9 多目标跟着实现</h4><h4 id="3-10-yolo模型"><a href="#3-10-yolo模型" class="headerlink" title="3.10 yolo模型"></a>3.10 yolo模型</h4><h4 id="3-12-基于yolo的目标检测"><a href="#3-12-基于yolo的目标检测" class="headerlink" title="3.12 基于yolo的目标检测"></a>3.12 基于yolo的目标检测</h4><h4 id="3-12-车流量统计"><a href="#3-12-车流量统计" class="headerlink" title="3.12 车流量统计"></a>3.12 车流量统计</h4><h4 id="3-13-视频中的车流量统计"><a href="#3-13-视频中的车流量统计" class="headerlink" title="3.13 视频中的车流量统计"></a>3.13 视频中的车流量统计</h4>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>目标检测中的匹配机制</title>
      <link href="/2023/01/22/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E5%8C%B9%E9%85%8D%E6%9C%BA%E5%88%B6/"/>
      <url>/2023/01/22/%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E4%B8%AD%E7%9A%84%E5%8C%B9%E9%85%8D%E6%9C%BA%E5%88%B6/</url>
      
        <content type="html"><![CDATA[<h1 id="目标检测中的匹配机制"><a href="#目标检测中的匹配机制" class="headerlink" title="目标检测中的匹配机制"></a>目标检测中的匹配机制</h1><h3 id="1-FCOS"><a href="#1-FCOS" class="headerlink" title="1. FCOS"></a>1. FCOS</h3><p>相较于reatinnet  FOs 的正负样本匹配机制会产生更多的正样本</p><h3 id="2-ATSS"><a href="#2-ATSS" class="headerlink" title="2. ATSS"></a>2. ATSS</h3><h4 id="匹配步骤"><a href="#匹配步骤" class="headerlink" title="匹配步骤"></a>匹配步骤</h4><ol><li>计算每个gt box与多尺度输出层 anchor之间的IoU</li><li>计算每个gt box与多尺度输出层anchor中心坐标的L2距离</li><li>遍历层 ， 遍历gt box ，topk(k 默认是9 ）L2 最小距离的anchor， 一共有L层个 L*K, 每个gt</li><li>对于每个gt box， 计算anchor与box之间的（IoU）的（mean， var） 均值和方差，相加之和作为阈值</li><li>对于每个gt box 大于阈值的作为正样本， 其他作为负样本</li><li>若topk过大，导致不在内部， 则过滤掉</li></ol><h3 id="3-OTA-Optimal-Transport-Assignment"><a href="#3-OTA-Optimal-Transport-Assignment" class="headerlink" title="3. OTA(Optimal Transport Assignment)"></a>3. OTA(Optimal Transport Assignment)</h3><h3 id="4-simOTA"><a href="#4-simOTA" class="headerlink" title="4. simOTA"></a>4. simOTA</h3><h4 id="优点"><a href="#优点" class="headerlink" title="优点"></a>优点</h4><ol><li>simOTA能够自动分析每个gt、所需要多少正样本</li><li>能自动决定每个gt 从哪个特征图来负责检测</li><li>相比OTA, simOTA更快</li><li>相比于OTA， 避免额外超参数</li></ol><h4 id="代码逻辑"><a href="#代码逻辑" class="headerlink" title="代码逻辑"></a>代码逻辑</h4><ol><li>计算正区域</li><li>计算anchor与gt的iou</li><li>在候选区域计算cost</li><li>利用iou确定每个gt的dynamic_k</li><li>gt选取cost最小dynamic_k个anchor正样本， 其余为负</li><li>使用正负样本计算loss</li></ol>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从视频标定到SLAM</title>
      <link href="/2023/01/22/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E8%A7%86%E8%A7%89SLAM%E5%85%AC%E5%BC%80%E8%AF%BE-%E4%BB%8E%E8%A7%86%E9%A2%91%E6%A0%87%E5%AE%9A%E5%88%B0SLAM/"/>
      <url>/2023/01/22/%E6%B5%99%E6%B1%9F%E5%A4%A7%E5%AD%A6%E8%A7%86%E8%A7%89SLAM%E5%85%AC%E5%BC%80%E8%AF%BE-%E4%BB%8E%E8%A7%86%E9%A2%91%E6%A0%87%E5%AE%9A%E5%88%B0SLAM/</url>
      
        <content type="html"><![CDATA[<h2 id="浙江大学视觉SLAM公开课-从视频标定到SLAM"><a href="#浙江大学视觉SLAM公开课-从视频标定到SLAM" class="headerlink" title="浙江大学视觉SLAM公开课-从视频标定到SLAM"></a>浙江大学视觉SLAM公开课-从视频标定到SLAM</h2><h3 id="CAMERA"><a href="#CAMERA" class="headerlink" title="CAMERA"></a>CAMERA</h3><p><img src="/images/image-20220720231448867.png" alt="image-20220720231448867"></p><p>Pinhole Camera （针孔摄像）</p><p><img src="/images/image-20220720231603837.png" alt="image-20220720231603837"></p><ul><li>角度 距离消失</li><li>平行不保留，长度不在绝对</li><li>维度压缩，丢失了非常多的信息</li></ul><p><img src="/images/image-20220720231642379.png" alt="image-20220720231642379"></p><p>制作一个相机</p><p><img src="/images/image-20220720231722622.png" alt="image-20220720231722622"></p><p><img src="/images/image-20220720231739458.png" alt="image-20220720231739458"></p><p>如何做一个相机， </p><p><img src="/images/image-20220720231928295.png" alt="image-20220720231928295"></p><p>Shringking the Aperture（收缩光圈）</p><p>孔太小， 曝光时间太少， 孔太小， 光的波长不能被忽略会发生光的干涉现象。</p><p><img src="/images/image-20220720232242050.png" alt="image-20220720232242050"></p><p>镜头的作用是啥呢？</p><p>用凸透镜 把光聚集起来，聚焦起来到一个点上， 曝光时间简短，</p><p><img src="/images/image-20220720232428472.png" alt="image-20220720232428472"></p><p>还是会散开， 镜面的角度有关</p><p><img src="/images/image-20220720232454754.png" alt="image-20220720232454754"></p><p><img src="/images/image-20220720232621407.png" alt="image-20220720232621407"></p><p>只有到镜头固定距离，才能清晰成像。，只有一定范围内才清晰。</p><p><img src="/images/image-20220720232813073.png" alt="image-20220720232813073"></p><p>depth of filed。 光圈的作用</p><p>光圈的速度， 光圈越小， 比较长时间曝光， 光圈越大F = 1/直径 </p><p>filed of view 跟相机的焦距直接相关。</p><p><img src="/images/image-20220720233522832.png" alt="image-20220720233522832"></p><p><img src="/images/image-20220720233848624.png" alt="image-20220720233848624"></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>matlab基础</title>
      <link href="/2020/02/05/20200205-practise-matlab/"/>
      <url>/2020/02/05/20200205-practise-matlab/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">%%</span><br><span class="line">% 第一题</span><br><span class="line">% 给定N和A，N是一个整数，A是一个向量，例如N = 3; A = [ 4 5 6 7]，请使用matlab命令将A中的每一个</span><br><span class="line">% 元素重复N次，然后形成一个向量，示例计算结果如下：</span><br><span class="line">N = 3; A = [ 4 5 6 7];</span><br><span class="line">B=A(ones(1,N),:); % 注意这里下标的实验，可以使用相同的下标多次</span><br><span class="line">B(:)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20200205_practise-matlab_an1.png" alt="an1">  </p><figure class="highlight matlab"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%%</span></span><br><span class="line"><span class="comment">% 假设x是一个向量，例如</span></span><br><span class="line"><span class="comment">% x = [ 4 4 5 5 5 6 7 7 8 8 8 8 ]</span></span><br><span class="line"><span class="comment">% 现在我们想得到如下两个向量</span></span><br><span class="line"><span class="comment">% l = [ 2 3 1 2 4 ]; % x每个元素重复的个数</span></span><br><span class="line"><span class="comment">% v = [ 4 5 6 7 8 ]; % x中重复元素的值</span></span><br><span class="line">x = [ <span class="number">4</span> <span class="number">4</span> <span class="number">5</span> <span class="number">5</span> <span class="number">5</span> <span class="number">6</span> <span class="number">7</span> <span class="number">7</span> <span class="number">8</span> <span class="number">8</span> <span class="number">8</span> <span class="number">8</span> ]</span><br><span class="line"><span class="built_in">i</span> = [<span class="built_in">find</span>(x(<span class="number">1</span>:<span class="keyword">end</span><span class="number">-1</span>) ~= x(<span class="number">2</span>:<span class="keyword">end</span>)) <span class="built_in">length</span>(x)]</span><br><span class="line">l = diff([<span class="number">0</span> <span class="built_in">i</span>])</span><br><span class="line">v = x(<span class="built_in">i</span>)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20200205_practise-matlab_an2.png" alt="an2">  </p><figure class="highlight matlab"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%%</span></span><br><span class="line"><span class="comment">% 求建立以下tables数组</span></span><br><span class="line"><span class="comment">% table N riqi xianxing Ndanhao Nshuanghao</span></span><br><span class="line"><span class="comment">% 第一列，行编号</span></span><br><span class="line"><span class="comment">% 第二列，2017年每天的日期，datetime格式</span></span><br><span class="line"><span class="comment">% 第三列，如果日期为单号，那么写文本“单号通行”，如果日期是双号，那么写文本“双号通行”，如果是周</span></span><br><span class="line"><span class="comment">% 末，则写“单双通行”</span></span><br><span class="line"><span class="comment">% 第四列，写当前日期单号车一共通行了多少天</span></span><br><span class="line"><span class="comment">% 第五列，写当前日期双号车一共通行了多少天</span></span><br><span class="line">NDays = yeardays(<span class="number">2017</span>) <span class="comment">%2017年的天数</span></span><br><span class="line">N=[<span class="number">1</span>:NDays]';</span><br><span class="line">riqi=datetime(<span class="number">2017</span>,<span class="number">1</span>,<span class="number">1</span>)+N<span class="number">-1</span>;</span><br><span class="line"><span class="comment">%注意这里，很多时候可以采用先预置一种统一答案，然后其中某些结果可以依次覆盖原有答案</span></span><br><span class="line">xianxing = <span class="built_in">repmat</span>(<span class="string">'单号通行'</span>,NDays,<span class="number">1</span>);</span><br><span class="line">a = logical(<span class="built_in">mod</span>(riqi.Day,<span class="number">2</span>)); <span class="comment">%日期是否为单数</span></span><br><span class="line">xianxing(~a,:)= <span class="built_in">repmat</span>(<span class="string">'双号通行'</span>,sum(~a),<span class="number">1</span>);</span><br><span class="line">tf = isweekend(riqi);</span><br><span class="line">xianxing(tf,:) = <span class="built_in">repmat</span>(<span class="string">'单双通行'</span>,sum(tf),<span class="number">1</span>);</span><br><span class="line">idxdanhao=all(xianxing==<span class="string">'单号通行'</span>,<span class="number">2</span>)|all(xianxing==<span class="string">'单双通行'</span>,<span class="number">2</span>)</span><br><span class="line">idxshuanghao=all(xianxing==<span class="string">'双号通行'</span>,<span class="number">2</span>)|all(xianxing==<span class="string">'单双通行'</span>,<span class="number">2</span>)</span><br><span class="line">Ndanhao=cumsum(idxdanhao);</span><br><span class="line">Nshuanghao=cumsum(idxshuanghao);</span><br><span class="line">XianXiangData = <span class="built_in">table</span>(N,riqi,xianxing,Ndanhao,Nshuanghao);</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20200205_practise-matlab_an3.png" alt="an3"></p><figure class="highlight matlab"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%%</span></span><br><span class="line"><span class="comment">% 通过load('data.mat')命令载入当前目录下的数据文件，data.mat，然后进行以下处理：</span></span><br><span class="line"><span class="comment">% data数据是一个手写数字图像的灰度数据，一共2000行785列，2000表示是2000副图片，第一列是当前图片</span></span><br><span class="line"><span class="comment">% 的数字是几（0-9范围，单个整数），第2列至第785列是图像的灰度数据，图片原始大小为28×28=784个像素</span></span><br><span class="line"><span class="comment">% 点，然后按照以下编号方式(图片未显示完全)，将784各点转换为了1行，作为了data数据中的一行</span></span><br><span class="line"><span class="comment">% 现在要求将data第一列提取为一个列向量，命名为trainT，另外第2列至第785列数据转换为28×28×2000的</span></span><br><span class="line"><span class="comment">% 三维数组，第一页为第一个数字的图像，要求按照图片中的顺序，将第一行中的灰度数据存成新的28×28的数</span></span><br><span class="line"><span class="comment">% 组，命名为trainD</span></span><br><span class="line">data=load(<span class="string">'data.mat'</span>);</span><br><span class="line">data1 = data.data(:,<span class="number">2</span>:<span class="keyword">end</span>);</span><br><span class="line">trainId = <span class="built_in">reshape</span>(data1',<span class="number">28</span>,<span class="number">28</span>,<span class="number">2000</span>);</span><br><span class="line">trainId = <span class="built_in">permute</span>(trainId,[<span class="number">2</span> <span class="number">1</span> <span class="number">3</span>]); <span class="comment">%维数的转换</span></span><br><span class="line">trianT = data.data(:,<span class="number">1</span>);</span><br><span class="line"><span class="built_in">figure</span></span><br><span class="line">imshow(uint8(trainId(:,:,<span class="number">7</span>)))</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20200205_practise-matlab_an4.png" alt="an4"></p><figure class="highlight matlab"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%%</span></span><br><span class="line"><span class="comment">% 在同一个图内绘制两个圆的曲线，一个半径为1，一个半径为2，在右方外侧中部添加图例，“小圆”和”大</span></span><br><span class="line"><span class="comment">% 圆“，绘图区域设置为正方形，标题设置为“两个圆”，横轴标签为x，纵轴标签为y</span></span><br><span class="line"><span class="comment">% 双坐标轴图</span></span><br><span class="line"><span class="comment">% 载入数据datahis.mat，</span></span><br><span class="line"><span class="comment">% 然后绘制datahis0.t_his为横坐标，datahis0.temp为左侧纵坐标，datahis0.hum为右侧纵坐标的双坐标轴图形</span></span><br><span class="line"><span class="comment">% 在图像下方外侧添加图例“温度”，“湿度”，横轴标签设置为时间，左侧y轴设置标签为温度，右侧y轴标签</span></span><br><span class="line"><span class="comment">% 设置为湿度</span></span><br><span class="line"><span class="comment">% datahis0.t_his为时间，需要转换为datetime类型，然后选取1月10日的数据进行画图</span></span><br><span class="line">clear</span><br><span class="line"><span class="built_in">figure</span></span><br><span class="line">load(<span class="string">'datahis.mat'</span>)</span><br><span class="line">t = datetime(datevec(datahis0.t_his));</span><br><span class="line">idx=t&gt;=datetime(<span class="number">2017</span>,<span class="number">1</span>,<span class="number">10</span>)&amp;t&lt;=datetime(<span class="number">2017</span>,<span class="number">1</span>,<span class="number">11</span>);</span><br><span class="line">t1 = t(idx);</span><br><span class="line">temp1 =  datahis0.temp(idx);</span><br><span class="line">hum1 = datahis0.hum(idx);</span><br><span class="line">yyaxis left</span><br><span class="line"><span class="built_in">plot</span>(t1,temp1)</span><br><span class="line">xlabel(<span class="string">'时间'</span>)</span><br><span class="line">ylabel(<span class="string">'温度'</span>)</span><br><span class="line">yyaxis right</span><br><span class="line"><span class="built_in">plot</span>(t1,hum1)</span><br><span class="line">ylabel(<span class="string">'温度'</span>)</span><br><span class="line"><span class="built_in">legend</span>(<span class="string">'温度'</span>,<span class="string">'湿度'</span>,<span class="string">'Location'</span>,<span class="string">'southoutside'</span>,<span class="string">'Orientation'</span>,<span class="string">'horizontal'</span>)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20200205_practise-matlab_an5.png" alt="an5"></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">%%</span><br><span class="line">% 多子图的绘制</span><br><span class="line">% 给定以下数据</span><br><span class="line">% x = 0:0.01:20; % x坐标</span><br><span class="line">% y1 = 200*exp(-0.05*x).*sin(x); % Y1</span><br><span class="line">% y2 = 0.8*exp(-0.5*x).*sin(10*x); % Y2</span><br><span class="line">% y3 = 100*exp(-0.5*x).*sin(5*x); % Y3</span><br><span class="line">% y1，y2，y3需要分别绘制一个子图，其中y1占据左侧一半位置，y2占据右侧上方，y3占据右侧下方</span><br><span class="line">clear</span><br><span class="line">figure</span><br><span class="line">x= 0:0.01:20; % x坐标</span><br><span class="line">y1 = 200*exp(-0.05*x).*sin(x); % Y1</span><br><span class="line">y2 = 0.8*exp(-0.5*x).*sin(10*x); % Y2</span><br><span class="line">y3 = 100*exp(-0.5*x).*sin(5*x); % Y3</span><br><span class="line">subplot(2,2,[1 3])</span><br><span class="line">plot(x,y1)</span><br><span class="line">subplot(2,2,2)</span><br><span class="line">plot(x, y2)</span><br><span class="line">subplot(2,2,4)</span><br><span class="line">plot(x,y3)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20200205_practise-matlab_an6.png" alt="an6"></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">%%</span><br><span class="line">% 曲面图</span><br><span class="line">% 绘制函数的网格图,x,y的取值范围为-2到2</span><br><span class="line">figure</span><br><span class="line">clear</span><br><span class="line">x = -2:0.1:2;</span><br><span class="line">y=x;</span><br><span class="line">[X,Y]=meshgrid(x,y)</span><br><span class="line">Z = sin(X.^2+Y.^2);</span><br><span class="line">mesh(X,Y,Z)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20200205_practise-matlab_an7.png" alt="an7">  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">%%</span><br><span class="line">% 三维饼形图</span><br><span class="line">% 绘制三维饼形图，各元素所占数值为： [6 3 7 5 1 2 4]，突出显示第1，3个元素，7个元素的标签分别为</span><br><span class="line">% '周一'到'周日'</span><br><span class="line">figure</span><br><span class="line">x =[6 3 7 5 1 2 4];</span><br><span class="line">labels = {'周一','周二','周三','周四','周五','周六','周日'};</span><br><span class="line">explode=[1 0 1 0 0 0 0] %突出显示向量x的元素</span><br><span class="line">pie3(x,explode,labels)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20200205_practise-matlab_an8.png" alt="an8"></p><figure class="highlight matlab"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">%%</span></span><br><span class="line"><span class="comment">% 绘制如下双纵轴柱形图。数据为：</span></span><br><span class="line"><span class="comment">% x=1:20;</span></span><br><span class="line"><span class="comment">% y1=sin(x)+2;</span></span><br><span class="line"><span class="comment">% y2=(x-10).^2;</span></span><br><span class="line"><span class="comment">% 注意柱形图等图形也是可以使用双坐标轴绘制的</span></span><br><span class="line">clear</span><br><span class="line"><span class="built_in">figure</span></span><br><span class="line">x=<span class="number">1</span>:<span class="number">20</span>;</span><br><span class="line">y1=<span class="built_in">sin</span>(x)+<span class="number">2</span>;</span><br><span class="line">y2=(x<span class="number">-10</span>).^<span class="number">2</span>;</span><br><span class="line">yyaxis left</span><br><span class="line">bar(x+<span class="number">0.2</span>,y1,<span class="number">0.3</span>,<span class="string">'b'</span>) <span class="comment">%注意图形叠加位置的调整</span></span><br><span class="line">ylabel(<span class="string">'sin'</span>)</span><br><span class="line">yyaxis right</span><br><span class="line">bar(x<span class="number">-0.2</span>,y2,<span class="number">0.3</span>,<span class="string">'r'</span>)</span><br><span class="line">ylabel(<span class="string">'x^2'</span>)</span><br><span class="line"><span class="built_in">legend</span>(<span class="string">'sin'</span>,<span class="string">'x^2'</span>,<span class="string">'Location'</span>,<span class="string">'southoutside'</span>,<span class="string">'Orientation'</span>,<span class="string">'horizontal'</span>)</span><br><span class="line">title(<span class="string">'双纵轴柱形图'</span>)</span><br><span class="line">xlabel(<span class="string">'x'</span>)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20200205_practise-matlab_an9.png" alt="an9"></p><figure class="highlight bash"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">%%</span><br><span class="line">% 绘制以下数据对应曲线，并增加横轴标签及对应曲线上的数值做标记。并在图中写标记公式文本</span><br><span class="line">% x=2:20;</span><br><span class="line">% alpha=x.^2;</span><br><span class="line">% beta=<span class="built_in">log</span>(x);</span><br><span class="line">% y=alpha./beta;</span><br><span class="line">clear</span><br><span class="line">figure</span><br><span class="line">x=2:20;</span><br><span class="line">alpha=x.^2;</span><br><span class="line">beta=<span class="built_in">log</span>(x);</span><br><span class="line">y=alpha./beta;</span><br><span class="line">plot(x,y)</span><br><span class="line">hold on</span><br><span class="line">plot(sqrt(23),sqrt(23).^2./log(sqrt(23)),<span class="string">'o'</span>)</span><br><span class="line">text(6,100,<span class="string">'$$\frac{\alpha}{\beta}$$'</span>,<span class="string">'Interpreter'</span>,<span class="string">'latex'</span>);%字符的输出</span><br><span class="line">xticks(<span class="built_in">sort</span>([2:2:20 sqrt(23)]))</span><br><span class="line">h1=gca;</span><br><span class="line">h1.XTickLabel{3}=<span class="string">'$$\sqrt{23}$$'</span>; %字符的输出</span><br><span class="line">h1.TickLabelInterpreter=<span class="string">'latex'</span>;</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20200205_practise-matlab_an10.png" alt="an10"></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">%%</span><br><span class="line">% 绘制如下图形</span><br><span class="line">% t1 = datetime(2014,1:12,1);</span><br><span class="line">% temp = [0 2 12 11 15 25 23 27 25 24 12 8];</span><br><span class="line">clear</span><br><span class="line">figure</span><br><span class="line">t1 = datetime(2014,1:12,1);</span><br><span class="line">temp = [0 2 12 11 15 25 23 27 25 24 12 8];</span><br><span class="line">h = plot(t1,temp,':*');</span><br><span class="line">ax = h.Parent;</span><br><span class="line">title('A Year of Temperatures on the 1st of the Month')</span><br><span class="line">ylabel('Degrees Celcius ^{\circ}') % 上角标的使用，摄氏度符号</span><br><span class="line">yt1 = ax.YTickLabel %坐标轴标签为元胞数组，在此基础上进行修改</span><br><span class="line">ytld = strcat(yt1,'^{\circ}')  %上角标</span><br><span class="line">ax.YTickLabel = ytld;</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20200205_practise-matlab_an11.png" alt="an11"></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">%%</span><br><span class="line">% 按照要求绘图</span><br><span class="line">% t=(pi*(0:1000)/1000)';</span><br><span class="line">% y=sin(t);</span><br><span class="line">% 以t为横轴数据，y为纵轴数据绘制图形，要求：</span><br><span class="line">% 1，将纵坐标轴设置在右方，</span><br><span class="line">% 2，将横轴标签设置为[0 1/4*pi 1/2*pi 3/4*pi pi]，坐标轴范围设置为0～pi</span><br><span class="line">% 3，输出图形的figure窗口位置坐标</span><br><span class="line">% 4，输出图形axes的位置坐标</span><br><span class="line">clear</span><br><span class="line">figure</span><br><span class="line">t=(pi*(0:1000)/1000)';</span><br><span class="line">y=sin(t);</span><br><span class="line">h = plot(t,y)</span><br><span class="line">set(gca,'YAxisLocation','right')</span><br><span class="line">xticks([0 1/4*pi 1/2*pi 3/4*pi pi])</span><br><span class="line">xticklabels({'0','1/4 \pi','1/2 \pi','3/4 \pi','\pi'})</span><br><span class="line">xlim([0 pi])</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20200205_practise-matlab_an12.png" alt="an12"></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">%%</span><br><span class="line">% 按照以下位置绘制图形</span><br><span class="line">% 其中需要用到的数据如下，另外图片坐标的位置与示例图中大致相等即可</span><br><span class="line">% % 图1</span><br><span class="line">% x=linspace(0.2*pi,20);</span><br><span class="line">% y=sin(x);</span><br><span class="line">% % 图2</span><br><span class="line">% t=0:pi/100:20*pi;</span><br><span class="line">% x=sin(t);</span><br><span class="line">% y=cos(t);</span><br><span class="line">% z=t.*sin(t).*cos(t);</span><br><span class="line">% % 图3</span><br><span class="line">% [x,y]=meshgrid(-8:0.5:8);</span><br><span class="line">% z=sin(sqrt(x.^2+y.^2))./sqrt(x.^2+y.^2+eps);</span><br><span class="line">clear</span><br><span class="line">figure</span><br><span class="line">% 图1</span><br><span class="line">x=linspace(0.2*pi,20);</span><br><span class="line">y=sin(x);</span><br><span class="line">axes('Position',[0.6,0.2,0.2,0.7],'GridLineStyle','-');</span><br><span class="line">plot(y,x);</span><br><span class="line">grid on</span><br><span class="line">% 图2</span><br><span class="line">axes('Position',[0.1,0.2,0.5,0.5]);</span><br><span class="line">t=0:pi/100:20*pi;</span><br><span class="line">x=sin(t);</span><br><span class="line">y=cos(t);</span><br><span class="line">z=t.*sin(t).*cos(t);</span><br><span class="line">plot3(x,y,z);</span><br><span class="line">% 图3</span><br><span class="line">axes('Position',[0.1,0.6,0.25,0.3]);</span><br><span class="line">[x,y]=meshgrid(-8:0.5:8);</span><br><span class="line">z=sin(sqrt(x.^2+y.^2))./sqrt(x.^2+y.^2+eps);</span><br><span class="line">mesh(x,y,z)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20200205_practise-matlab_an13.png" alt="an13">  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">%%</span><br><span class="line">% 以下数据进行拟合</span><br><span class="line">% load ex1.mat</span><br><span class="line">% cftool</span><br><span class="line">load ex1.mat</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20200205_practise-matlab_an14.png" alt="an14"></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">%%</span><br><span class="line">% 使用命令导入文件2016010600.txt中的数据，提取第4列日期数据和第5列时刻数据数据，合并生成一个</span><br><span class="line">% datetime数组，命名为T</span><br><span class="line">clear</span><br><span class="line">a = readtable('2016010600.txt');</span><br><span class="line">t1 = a.Var4;</span><br><span class="line">t2 = a.Var5;</span><br><span class="line">TPvec_y = floor(t1/10000);</span><br><span class="line">TPvec_mon = floor((t1-TPvec_y*10000)/100);</span><br><span class="line">TPvec_d = mod(t1,100)</span><br><span class="line">TPvec_h = floor(t2/100);</span><br><span class="line">TPvec_min=mod(t2,100);</span><br><span class="line">TPvec_s = zeros(size(t1,1),1);</span><br><span class="line">t_vec=[TPvec_y TPvec_mon TPvec_d TPvec_h TPvec_min TPvec_s];</span><br><span class="line">T = datetime(t_vec)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20200205_practise-matlab_an15.png" alt="an15"></p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">%%</span><br><span class="line">% 生成与2016010600.txt文件中第6列数据概率分布相同的随机数，数量10000个.</span><br><span class="line">clear</span><br><span class="line">figure</span><br><span class="line">a = readtable('2016010600.txt')</span><br><span class="line">v = a.Var6;</span><br><span class="line">histogram(v,20)</span><br><span class="line">[N,edges] = histcounts(v,20);</span><br><span class="line">rm = RDrnd(N,10000);</span><br><span class="line">figure</span><br><span class="line">histogram(rm,20)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20200205_practise-matlab_an16.png" alt="an16"></p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> MATLAB </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>matlab基础</title>
      <link href="/2020/02/05/practise-matlab/"/>
      <url>/2020/02/05/practise-matlab/</url>
      
        <content type="html"><![CDATA[<pre><code>1  2  3  4  5  6  7  </code></pre><p>| </p><pre><code>%%  % 第一题  % 给定N和A，N是一个整数，A是一个向量，例如N = 3; A = [ 4 5 6 7]，请使用matlab命令将A中的每一个  % 元素重复N次，然后形成一个向量，示例计算结果如下：  N = 3; A = [ 4 5 6 7];  B=A(ones(1,N),:); % 注意这里下标的实验，可以使用相同的下标多次  B(:)    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2020/02/05/practise-matlab/images/20200205_practise-matlab_an1.png" alt="an1">  </p><pre><code>1  2  3  4  5  6  7  8  9  10  </code></pre><p>| </p><pre><code>%%  % 假设x是一个向量，例如  % x = [ 4 4 5 5 5 6 7 7 8 8 8 8 ]  % 现在我们想得到如下两个向量  % l = [ 2 3 1 2 4 ]; % x每个元素重复的个数  % v = [ 4 5 6 7 8 ]; % x中重复元素的值  x = [ 4 4 5 5 5 6 7 7 8 8 8 8 ]  i = [find(x(1:end-1) ~= x(2:end)) length(x)]  l = diff([0 i])  v = x(i)    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2020/02/05/practise-matlab/images/20200205_practise-matlab_an2.png" alt="an2">  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  </code></pre><p>| </p><pre><code>%%  % 求建立以下tables数组  % table N riqi xianxing Ndanhao Nshuanghao  % 第一列，行编号  % 第二列，2017年每天的日期，datetime格式  % 第三列，如果日期为单号，那么写文本“单号通行”，如果日期是双号，那么写文本“双号通行”，如果是周  % 末，则写“单双通行”  % 第四列，写当前日期单号车一共通行了多少天  % 第五列，写当前日期双号车一共通行了多少天  NDays = yeardays(2017) %2017年的天数  N=[1:NDays]';  riqi=datetime(2017,1,1)+N-1;  %注意这里，很多时候可以采用先预置一种统一答案，然后其中某些结果可以依次覆盖原有答案  xianxing = repmat('单号通行',NDays,1);  a = logical(mod(riqi.Day,2)); %日期是否为单数  xianxing(~a,:)= repmat('双号通行',sum(~a),1);  tf = isweekend(riqi);  xianxing(tf,:) = repmat('单双通行',sum(tf),1);  idxdanhao=all(xianxing=='单号通行',2)|all(xianxing=='单双通行',2)  idxshuanghao=all(xianxing=='双号通行',2)|all(xianxing=='单双通行',2)  Ndanhao=cumsum(idxdanhao);  Nshuanghao=cumsum(idxshuanghao);  XianXiangData = table(N,riqi,xianxing,Ndanhao,Nshuanghao);    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2020/02/05/practise-matlab/images/20200205_practise-matlab_an3.png" alt="an3"></p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  </code></pre><p>| </p><pre><code>%%  % 通过load('data.mat')命令载入当前目录下的数据文件，data.mat，然后进行以下处理：  % data数据是一个手写数字图像的灰度数据，一共2000行785列，2000表示是2000副图片，第一列是当前图片  % 的数字是几（0-9范围，单个整数），第2列至第785列是图像的灰度数据，图片原始大小为28×28=784个像素  % 点，然后按照以下编号方式(图片未显示完全)，将784各点转换为了1行，作为了data数据中的一行  % 现在要求将data第一列提取为一个列向量，命名为trainT，另外第2列至第785列数据转换为28×28×2000的  % 三维数组，第一页为第一个数字的图像，要求按照图片中的顺序，将第一行中的灰度数据存成新的28×28的数  % 组，命名为trainD  data=load('data.mat');  data1 = data.data(:,2:end);  trainId = reshape(data1',28,28,2000);  trainId = permute(trainId,[2 1 3]); %维数的转换  trianT = data.data(:,1);  figure  imshow(uint8(trainId(:,:,7)))    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2020/02/05/practise-matlab/images/20200205_practise-matlab_an4.png" alt="an4"></p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  </code></pre><p>| </p><pre><code>%%  % 在同一个图内绘制两个圆的曲线，一个半径为1，一个半径为2，在右方外侧中部添加图例，“小圆”和”大  % 圆“，绘图区域设置为正方形，标题设置为“两个圆”，横轴标签为x，纵轴标签为y  % 双坐标轴图  % 载入数据datahis.mat，  % 然后绘制datahis0.t_his为横坐标，datahis0.temp为左侧纵坐标，datahis0.hum为右侧纵坐标的双坐标轴图形  % 在图像下方外侧添加图例“温度”，“湿度”，横轴标签设置为时间，左侧y轴设置标签为温度，右侧y轴标签  % 设置为湿度  % datahis0.t_his为时间，需要转换为datetime类型，然后选取1月10日的数据进行画图  clear  figure  load('datahis.mat')  t = datetime(datevec(datahis0.t_his));  idx=t&gt;=datetime(2017,1,10)&amp;t&lt;=datetime(2017,1,11);  t1 = t(idx);  temp1 =  datahis0.temp(idx);  hum1 = datahis0.hum(idx);  yyaxis left  plot(t1,temp1)  xlabel('时间')  ylabel('温度')    yyaxis right  plot(t1,hum1)  ylabel('温度')  legend('温度','湿度','Location','southoutside','Orientation','horizontal')    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2020/02/05/practise-matlab/images/20200205_practise-matlab_an5.png" alt="an5"></p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  </code></pre><p>| </p><pre><code>%%  % 多子图的绘制  % 给定以下数据  % x = 0:0.01:20; % x坐标  % y1 = 200*exp(-0.05*x).*sin(x); % Y1  % y2 = 0.8*exp(-0.5*x).*sin(10*x); % Y2  % y3 = 100*exp(-0.5*x).*sin(5*x); % Y3  % y1，y2，y3需要分别绘制一个子图，其中y1占据左侧一半位置，y2占据右侧上方，y3占据右侧下方  clear  figure  x= 0:0.01:20; % x坐标  y1 = 200*exp(-0.05*x).*sin(x); % Y1  y2 = 0.8*exp(-0.5*x).*sin(10*x); % Y2  y3 = 100*exp(-0.5*x).*sin(5*x); % Y3  subplot(2,2,[1 3])  plot(x,y1)  subplot(2,2,2)  plot(x, y2)  subplot(2,2,4)  plot(x,y3)    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2020/02/05/practise-matlab/images/20200205_practise-matlab_an6.png" alt="an6"></p><pre><code>1  2  3  4  5  6  7  8  9  10  </code></pre><p>| </p><pre><code>%%  % 曲面图  % 绘制函数的网格图,x,y的取值范围为-2到2  figure  clear  x = -2:0.1:2;  y=x;  [X,Y]=meshgrid(x,y)  Z = sin(X.^2+Y.^2);  mesh(X,Y,Z)    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2020/02/05/practise-matlab/images/20200205_practise-matlab_an7.png" alt="an7">  </p><pre><code>1  2  3  4  5  6  7  8  9  10  </code></pre><p>| </p><pre><code>%%  % 三维饼形图  % 绘制三维饼形图，各元素所占数值为： [6 3 7 5 1 2 4]，突出显示第1，3个元素，7个元素的标签分别为  % '周一'到'周日'    figure  x =[6 3 7 5 1 2 4];  labels = {'周一','周二','周三','周四','周五','周六','周日'};  explode=[1 0 1 0 0 0 0] %突出显示向量x的元素  pie3(x,explode,labels)    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2020/02/05/practise-matlab/images/20200205_practise-matlab_an8.png" alt="an8"></p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  </code></pre><p>| </p><pre><code>%%  % 绘制如下双纵轴柱形图。数据为：  % x=1:20;  % y1=sin(x)+2;  % y2=(x-10).^2;  % 注意柱形图等图形也是可以使用双坐标轴绘制的  clear  figure  x=1:20;  y1=sin(x)+2;  y2=(x-10).^2;  yyaxis left  bar(x+0.2,y1,0.3,'b') %注意图形叠加位置的调整  ylabel('sin')  yyaxis right  bar(x-0.2,y2,0.3,'r')  ylabel('x^2')  legend('sin','x^2','Location','southoutside','Orientation','horizontal')  title('双纵轴柱形图')  xlabel('x')    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2020/02/05/practise-matlab/images/20200205_practise-matlab_an9.png" alt="an9"></p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  </code></pre><p>| </p><pre><code>%%  % 绘制以下数据对应曲线，并增加横轴标签及对应曲线上的数值做标记。并在图中写标记公式文本  % x=2:20;  % alpha=x.^2;  % beta=log(x);  % y=alpha./beta;  clear  figure  x=2:20;  alpha=x.^2;  beta=log(x);  y=alpha./beta;    plot(x,y)  hold on    plot(sqrt(23),sqrt(23).^2./log(sqrt(23)),'o')  text(6,100,'$$\frac{\alpha}{\beta}$$','Interpreter','latex');%字符的输出  xticks(sort([2:2:20 sqrt(23)]))  h1=gca;  h1.XTickLabel{3}='$$\sqrt{23}$$'; %字符的输出  h1.TickLabelInterpreter='latex';    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2020/02/05/practise-matlab/images/20200205_practise-matlab_an10.png" alt="an10"></p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  </code></pre><p>| </p><pre><code>%%  % 绘制如下图形  % t1 = datetime(2014,1:12,1);  % temp = [0 2 12 11 15 25 23 27 25 24 12 8];  clear  figure  t1 = datetime(2014,1:12,1);  temp = [0 2 12 11 15 25 23 27 25 24 12 8];  h = plot(t1,temp,':*');  ax = h.Parent;  title('A Year of Temperatures on the 1st of the Month')  ylabel('Degrees Celcius ^{\circ}') % 上角标的使用，摄氏度符号  yt1 = ax.YTickLabel %坐标轴标签为元胞数组，在此基础上进行修改  ytld = strcat(yt1,'^{\circ}')  %上角标  ax.YTickLabel = ytld;    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2020/02/05/practise-matlab/images/20200205_practise-matlab_an11.png" alt="an11"></p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  </code></pre><p>| </p><pre><code>%%  % 按照要求绘图  % t=(pi*(0:1000)/1000)';  % y=sin(t);  % 以t为横轴数据，y为纵轴数据绘制图形，要求：  % 1，将纵坐标轴设置在右方，  % 2，将横轴标签设置为[0 1/4*pi 1/2*pi 3/4*pi pi]，坐标轴范围设置为0～pi  % 3，输出图形的figure窗口位置坐标  % 4，输出图形axes的位置坐标    clear  figure  t=(pi*(0:1000)/1000)';  y=sin(t);  h = plot(t,y)  set(gca,'YAxisLocation','right')  xticks([0 1/4*pi 1/2*pi 3/4*pi pi])  xticklabels({'0','1/4 \pi','1/2 \pi','3/4 \pi','\pi'})  xlim([0 pi])    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2020/02/05/practise-matlab/images/20200205_practise-matlab_an12.png" alt="an12"></p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  </code></pre><p>| </p><pre><code>%%  % 按照以下位置绘制图形  % 其中需要用到的数据如下，另外图片坐标的位置与示例图中大致相等即可  % % 图1  % x=linspace(0.2*pi,20);  % y=sin(x);  % % 图2  % t=0:pi/100:20*pi;  % x=sin(t);  % y=cos(t);  % z=t.*sin(t).*cos(t);  % % 图3  % [x,y]=meshgrid(-8:0.5:8);  % z=sin(sqrt(x.^2+y.^2))./sqrt(x.^2+y.^2+eps);  clear  figure  % 图1  x=linspace(0.2*pi,20);  y=sin(x);    axes('Position',[0.6,0.2,0.2,0.7],'GridLineStyle','-');  plot(y,x);  grid on  % 图2  axes('Position',[0.1,0.2,0.5,0.5]);  t=0:pi/100:20*pi;  x=sin(t);  y=cos(t);  z=t.*sin(t).*cos(t);  plot3(x,y,z);    % 图3  axes('Position',[0.1,0.6,0.25,0.3]);  [x,y]=meshgrid(-8:0.5:8);  z=sin(sqrt(x.^2+y.^2))./sqrt(x.^2+y.^2+eps);  mesh(x,y,z)    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2020/02/05/practise-matlab/images/20200205_practise-matlab_an13.png" alt="an13">  </p><pre><code>1  2  3  4  5  </code></pre><p>| </p><pre><code>%%  % 以下数据进行拟合  % load ex1.mat  % cftool  load ex1.mat    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2020/02/05/practise-matlab/images/20200205_practise-matlab_an14.png" alt="an14"></p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  </code></pre><p>| </p><pre><code>%%  % 使用命令导入文件2016010600.txt中的数据，提取第4列日期数据和第5列时刻数据数据，合并生成一个  % datetime数组，命名为T  clear  a = readtable('2016010600.txt');  t1 = a.Var4;  t2 = a.Var5;  TPvec_y = floor(t1/10000);  TPvec_mon = floor((t1-TPvec_y*10000)/100);  TPvec_d = mod(t1,100)  TPvec_h = floor(t2/100);  TPvec_min=mod(t2,100);  TPvec_s = zeros(size(t1,1),1);  t_vec=[TPvec_y TPvec_mon TPvec_d TPvec_h TPvec_min TPvec_s];  T = datetime(t_vec)    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2020/02/05/practise-matlab/images/20200205_practise-matlab_an15.png" alt="an15"></p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  </code></pre><p>| </p><pre><code>%%  % 生成与2016010600.txt文件中第6列数据概率分布相同的随机数，数量10000个.  clear  figure  a = readtable('2016010600.txt')  v = a.Var6;  histogram(v,20)    [N,edges] = histcounts(v,20);  rm = RDrnd(N,10000);  figure  histogram(rm,20)    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2020/02/05/practise-matlab/images/20200205_practise-matlab_an16.png" alt="an16"></p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpatialSense</title>
      <link href="/2019/12/27/20191227-SpatialSense/"/>
      <url>/2019/12/27/20191227-SpatialSense/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>理解图像中物体之间的空间关系是一项具有惊人挑战性的任务(图1)。椅子可能“在”一个人的“后面”，即使它出现在人的左边(取决于人面对的方向)。如果有第三个学生在他们之间，那么两个看起来很近的学生实际上可能并不“挨着”。<br>我们介绍了spatial alsense，这是一个专门研究空间关系识别的数据集，它捕捉了广泛的此类挑战，允许对计算机视觉技术进行适当的基准测试。空间感知是通过对抗性的众包来构建的，在众包中，人类注释者的任务是发现空间关系，这些关系很难用简单的线索来预测，比如二维空间结构或语言先验。与现有的数据集相比，对抗性众包大大减少了数据集的偏倚，并在长尾抽取了更有趣的关系样本。在空间感方面，最先进的识别模型与简单的基线相比，表现得更为出色，这表明它们依赖于直接的线索，而不是对这个复杂的任务进行充分的推理。空间感觉基准测试为提高计算机视觉系统的空间推理能力提供了一条途径。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Attention机制中SEnet CBAM以及Dual pooling的pytorch实现</title>
      <link href="/2019/12/27/20191227-SENet-code/"/>
      <url>/2019/12/27/20191227-SENet-code/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本来自己写了，关于SENet的注意力截止，但是在准备写其他注意力机制代码的时候，看到一篇文章总结的很好，所以对此篇文章进行搬运，以供自己查阅，并加上自己的理解。</p></blockquote><p>[TOC]</p><h2 id="1-SENET中的channel-wise加权的实现"><a href="#1-SENET中的channel-wise加权的实现" class="headerlink" title="1.SENET中的channel-wise加权的实现"></a>1.SENET中的channel-wise加权的实现</h2><p>实现代码参考自：<a href="https://github.com/moskomule/senet.pytorch">senet.pytorch</a><br><img src="/images/20191227_SENet-code_senet.png" alt="senet"><br>代码如下：<br>SEnet 模块  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"> <span class="keyword">from</span> torch <span class="keyword">import</span> nn</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">SELayer</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, channel, reduction=<span class="number">16</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(SELayer, self).__init__()</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.fc = nn.Sequential(</span><br><span class="line">            nn.Linear(channel, channel // reduction, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.ReLU(inplace=<span class="literal">True</span>),</span><br><span class="line">            nn.Linear(channel // reduction, channel, bias=<span class="literal">False</span>),</span><br><span class="line">            nn.Sigmoid()</span><br><span class="line">        )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        b, c, _, _ = x.size()</span><br><span class="line">        y = self.avg_pool(x).view(b, c)</span><br><span class="line">        y = self.fc(y).view(b, c, <span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">        <span class="keyword">return</span> x * y. (x)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20191227_SENet-code_senet2.png" alt="senet2"><br>以上代码设计到的API：</p><ul><li>AdaptiveAvgPool2d: 自适应平均池化，参数为（n,m）则将原来的feature（w,h）通过pooling得到（n,m）的feature，如果是（n）,则将原来的feature从（w,h）通过pooling得到（n,n）</li><li>Sequential: torch容器，存放网络层等内容。</li><li>Linear: 线性层，参数为（in, out）,将原有的in个feature转为out个feature</li><li>ReLU: 激活层， inplace进行原地操作，节省内存</li><li>Sigmoid: 激活层，将输入压缩到0-1<br>分析forward进行模型的构建：</li><li>x是输入的feature,一般各个通道意义如下：（batch size，channel, width , height）,这里获取了batch(b), channel</li><li>x通过AdaptiveAvgPool2d(1)以后将得到（batch size, channel, 1, 1）, 然后view（b,c）意思是按照b,c进行展开</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">```python</span><br><span class="line">In [<span class="number">1</span>]: <span class="keyword">import</span> torch</span><br><span class="line">In [<span class="number">2</span>]:  x = torch.zeros((<span class="number">16</span>,<span class="number">256</span>,<span class="number">256</span>,<span class="number">256</span>))</span><br><span class="line">In [<span class="number">3</span>]:  <span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">In [<span class="number">4</span>]: avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">In [<span class="number">5</span>]: avg_pool(x).shape</span><br><span class="line">Out[<span class="number">5</span>]: torch.Size([<span class="number">16</span>, <span class="number">256</span>, <span class="number">1</span>, <span class="number">1</span>])</span><br><span class="line">In [<span class="number">6</span>]: avg_pool(x).view((<span class="number">16</span>,<span class="number">256</span>)).shape</span><br><span class="line">Out[<span class="number">6</span>]: torch.Size([<span class="number">16</span>, <span class="number">256</span>])</span><br><span class="line">In [<span class="number">7</span>]: avg_pool(x).squeeze().shape <span class="comment"># squeeze()函数也可以将所有通道个数为1的进行挤压</span></span><br><span class="line">Out[<span class="number">7</span>]: torch.Size([<span class="number">16</span>, <span class="number">256</span>])</span><br></pre></td></tr></tbody></table></figure><ul><li>然后形状为【16, 256】的tensor经过fc:</li><li>(1) Linear: from 256(channel) to 256/16</li><li>(2) ReLu：进行一次激活函数</li><li>(3) Linear: from 256/16 to 256(channel)</li><li>(4) Sigmoid: 激活到0-1，代表每个通道的重要性</li><li>然后通过view操作转化为【16,256,1,1】形状的tensor</li><li>现在y得到的是每一个通道对应的分数（0-1），然后需要将其与通道内容相乘，具体操作使用到了tensor的内置函数expand_as(把一个tensor变成和函数括号内一样形状的tensor，用法与expand类似，相当于expand(tensor.size())</li><li>x是【16,256,256,256】形状的特征图，y是【16,256,1,1】大小的channel-wise分数，然后需要将其相乘</li><li>b.expand_as(a)就是将b进行扩充，扩充到a的维度，需要说明的是a的低维度需要比b大，例如b的shape是3<em>1，如果a的shape是3</em>2不会出错，但是是2*2就会报错了。<br>就是必须有一个维度是1，然后用于扩展：<br>1<br>2<br>3<br>In [8]: tensor1 = torch.ones((3,4,1,1))<br>In [9]: tensor1.expand([3,4,5,5]).shape<br>Out[9]: torch.Size([3, 4, 5, 5])</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">这样通过expand_as就能得到【16,256,256,256】大小的tensor，其中256*256都是对应通道的1分数，然后与原先的feature相乘，就能得到channel-wise分数计算后的feature。</span><br><span class="line"></span><br><span class="line">在resetnet中的block插入senet模块  </span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">    class CifarSEBasicBlock(nn.Module):</span><br><span class="line">       def __init__(self, inplanes, planes, stride=1, reduction=16):</span><br><span class="line">           super(CifarSEBasicBlock, self).__init__()</span><br><span class="line">           self.conv1 = conv3x3(inplanes, planes, stride)</span><br><span class="line">           self.bn1 = nn.BatchNorm2d(planes)</span><br><span class="line">           self.relu = nn.ReLU(inplace=True)</span><br><span class="line">           self.conv2 = conv3x3(planes, planes)</span><br><span class="line">           self.bn2 = nn.BatchNorm2d(planes)</span><br><span class="line">           self.se = SELayer(planes, reduction)</span><br><span class="line">           if inplanes != planes:</span><br><span class="line">               self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False),</span><br><span class="line">                                               nn.BatchNorm2d(planes))</span><br><span class="line">           else:</span><br><span class="line">               self.downsample = lambda x: x</span><br><span class="line">           self.stride = stride</span><br><span class="line">       def forward(self, x):</span><br><span class="line">           residual = self.downsample(x)</span><br><span class="line">           out = self.conv1(x)</span><br><span class="line">           out = self.bn1(out)</span><br><span class="line">           out = self.relu(out)</span><br><span class="line">           out = self.conv2(out)</span><br><span class="line">           out = self.bn2(out)</span><br><span class="line">           out = self.se(out)</span><br><span class="line">           out += residual</span><br><span class="line">           out = self.relu(out)</span><br><span class="line">           return out</span><br></pre></td></tr></tbody></table></figure><p>正常的resent的BasicBlock  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes, planes, stride=<span class="number">1</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line">        self.conv1 = conv3x3(inplanes, planes, stride)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.conv2 = conv3x3(planes, planes)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes)</span><br><span class="line">        <span class="keyword">if</span> inplanes != planes:</span><br><span class="line">            self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=<span class="number">1</span>, stride=stride, bias=<span class="literal">False</span>),</span><br><span class="line">                                            nn.BatchNorm2d(planes))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            self.downsample = <span class="keyword">lambda</span> x: x</span><br><span class="line">        self.stride = stride</span><br></pre></td></tr></tbody></table></figure><p>baseline:0.888<br>se+baseline:0.892</p><h2 id="2-CBAM中的通道注意力机制"><a href="#2-CBAM中的通道注意力机制" class="headerlink" title="2.CBAM中的通道注意力机制"></a>2.CBAM中的通道注意力机制</h2><p>channel-attention-module跟以上内容想法有一点像，给每个channel进行打分，具体实现如下：<br>参考来源：<a href="https://github.com/luuuyi/CBAM.PyTorch/blob/master/model/resnet_cbam.py">CBMA.pytorch</a>  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">ChannelAttention</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, in_planes, ratio=<span class="number">16</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(ChannelAttention, self).__init__()</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.max_pool = nn.AdaptiveMaxPool2d(<span class="number">1</span>)</span><br><span class="line">        self.fc1   = nn.Conv2d(in_planes, in_planes // <span class="number">16</span>, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.relu1 = nn.ReLU()</span><br><span class="line">        self.fc2   = nn.Conv2d(in_planes // <span class="number">16</span>, in_planes, <span class="number">1</span>, bias=<span class="literal">False</span>)</span><br><span class="line">        self.sigmoid = nn.Sigmoid()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))</span><br><span class="line">        max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))</span><br><span class="line">        out = avg_out + max_out</span><br><span class="line">        <span class="keyword">return</span> self.sigmoid(out)</span><br></pre></td></tr></tbody></table></figure><p>API跟上边类似，只添加了卷积，也很简单。需要说明的是貌似Linear和Conv2d中的参数很相似，但是实际上，两者还是很不一样的，Linear接受的是线性的2维数组（batch, 一维特征），Con2d接受的是4维数组（batch, 通道，w, h）。<br><img src="/images/20191227_SENet-code_ch.png" alt="ch"><br>forward函数：</p><ul><li>第一行，进行了adaptiveAvgPooling， conv2d, relu, conv2d</li><li>第二行，进行了AdaptiveMaxPooling, conv2d, relu, conv2d</li><li>第三行，将两个向量进行相加</li><li>第四行，将对应结果进行激活，得到通道注意力分数</li></ul><h2 id="3-CBAM中的空间注意力机制"><a href="#3-CBAM中的空间注意力机制" class="headerlink" title="3.CBAM中的空间注意力机制"></a>3.CBAM中的空间注意力机制</h2><p>参考来源：<a href="https://github.com/luuuyi/CBAM.PyTorch/blob/master/model/resnet_cbam.py">CBMA.pytorch</a></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">SpatialAttention</span>(nn.Module):</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, kernel_size=<span class="number">7</span></span>):</span><br><span class="line"><span class="built_in">super</span>(SpatialAttention, self).__init__()</span><br><span class="line"><span class="keyword">assert</span> kernel_size <span class="keyword">in</span> (<span class="number">3</span>, <span class="number">7</span>), <span class="string">'kernel size must be 3 or 7'</span></span><br><span class="line">padding = <span class="number">3</span> <span class="keyword">if</span> kernel_size == <span class="number">7</span> <span class="keyword">else</span> <span class="number">1</span></span><br><span class="line">self.conv1 = nn.Conv2d(<span class="number">2</span>, <span class="number">1</span>, kernel_size, padding=padding, bias=<span class="literal">False</span>)</span><br><span class="line">self.sigmoid = nn.Sigmoid()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">avg_out = torch.mean(x, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">max_out, _ = torch.<span class="built_in">max</span>(x, dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">x = torch.cat([avg_out, max_out], dim=<span class="number">1</span>)</span><br><span class="line">x = self.conv1(x)</span><br><span class="line"><span class="keyword">return</span> self.sigmoid(x)</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20191227_SENet-code_sa.png" alt="sa"><br>Spatial attention module中支持kernel_size=3或者7，默认设置为7。<br>以上涉及到的API:</p><ul><li>torch.mean: 求平均值，dim指的是沿着某一个通道进行计算平均值。这里dim=1，说明沿着通道channel进行平均，对所有channel的feature上相应的像素进行求平均值。</li><li>torch.max: 同上，进行求最大值。<br>forward函数：</li><li>第一行：沿着通道维度进行进行平均，得到一个（batch, 1, w, h）的feature</li><li>第二行：沿着通道维度进行求最大值，得到一个（batch, 1, w, h）的feature</li><li>第三行：将两个feature通过cat的方式拼接起来，得到一个（batch, 2, w, h）的feature</li><li>第四行：对这个feature进行卷积之所以设置如果kernel_size=7的时候padding=3是因为需要将out_feature和in_feature相等，可以带入公式进行计算。</li><li>第五行：进行激活，将得分约束至[0-1]</li></ul><h2 id="4-CBAM中的融合"><a href="#4-CBAM中的融合" class="headerlink" title="4.CBAM中的融合"></a>4.CBAM中的融合</h2><p>参考代码：<a href="https://github.com/luuuyi/CBAM.PyTorch/blob/master/model/resnet_cbam.py">CBMA.pytorch</a><br>在resnet中主要是用在basicBlock中，代码如下：</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">BasicBlock</span>(nn.Module):</span><br><span class="line">    expansion = <span class="number">1</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes, planes, stride=<span class="number">1</span>, downsample=<span class="literal">None</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(BasicBlock, self).__init__()</span><br><span class="line">        self.conv1 = conv3x3(inplanes, planes, stride)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.relu = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.conv2 = conv3x3(planes, planes)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(planes)</span><br><span class="line">        self.ca = ChannelAttention(planes)</span><br><span class="line">        self.sa = SpatialAttention()</span><br><span class="line">        self.downsample = downsample</span><br><span class="line">        self.stride = stride</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        residual = x</span><br><span class="line">        out = self.conv1(x)</span><br><span class="line">        out = self.bn1(out)</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        out = self.conv2(out)</span><br><span class="line">        out = self.bn2(out)</span><br><span class="line">        out = self.ca(out) * out <span class="comment"># 广播机制</span></span><br><span class="line">        out = self.sa(out) * out <span class="comment"># 广播机制</span></span><br><span class="line">        <span class="keyword">if</span> self.downsample <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">            residual = self.downsample(x)</span><br><span class="line">        out += residual</span><br><span class="line">        out = self.relu(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20191227_SENet-code_cbam.png" alt="cbam"><br>resnet50+cbam: 0.902</p><h2 id="5-dual-pooling的pytorch实现"><a href="#5-dual-pooling的pytorch实现" class="headerlink" title="5.dual pooling的pytorch实现"></a>5.dual pooling的pytorch实现</h2><p>max pooling更注重重要的局部特征, average pooling更关注全局特征.两者concat可以丰富特征层.<br>参考链接:<a href="https://zhuanlan.zhihu.com/p/93806755">GaryLIU</a></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">res18</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes</span>):</span><br><span class="line">        <span class="built_in">super</span>(res18, self).__init__()</span><br><span class="line">        self.base = resnet18(pretrained=<span class="literal">True</span>)</span><br><span class="line">        self.feature = nn.Sequential(</span><br><span class="line">            self.base.conv1,</span><br><span class="line">            self.base.bn1,</span><br><span class="line">            self.base.relu,</span><br><span class="line">            self.base.maxpool,</span><br><span class="line">            self.base.layer1,</span><br><span class="line">            self.base.layer2,</span><br><span class="line">            self.base.layer3,</span><br><span class="line">            self.base.layer4</span><br><span class="line">        )</span><br><span class="line">        self.avg_pool = nn.AdaptiveAvgPool2d(<span class="number">1</span>)</span><br><span class="line">        self.max_pool = nn.AdaptiveMaxPool2d(<span class="number">1</span>)</span><br><span class="line">        self.reduce_layer = nn.Conv2d(<span class="number">1024</span>, <span class="number">512</span>, <span class="number">1</span>)</span><br><span class="line">        self.fc  = nn.Sequential(</span><br><span class="line">            nn.Dropout(<span class="number">0.5</span>),</span><br><span class="line">            nn.Linear(<span class="number">512</span>, num_classes)</span><br><span class="line">            )</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        bs = x.shape[<span class="number">0</span>]</span><br><span class="line">        x = self.feature(x)</span><br><span class="line">        x1 = self.avg_pool(x)</span><br><span class="line">        x2 = self.max_pool(x)</span><br><span class="line">        x = torch.cat([x1, x2], dim=<span class="number">1</span>)</span><br><span class="line">        x = self.reduce_layer(x).view(bs, -<span class="number">1</span>)</span><br><span class="line">        logits = self.fc(x)</span><br><span class="line">        <span class="keyword">return</span> logits</span><br></pre></td></tr></tbody></table></figure><blockquote><p>这种是在模型层进行改造的一种小trick了，常见的做法：global max/average pooling + fc layer，这里试concat(global max-pooling, global average pooling) + fc layer，其实就是为了丰富特征层，max pooling更加关注重要的局部特征，而average pooling试更加关注全局的特征。不一定有效，我试过不少次，有效的次数比较少，但不少人喜欢这样用.<br>-gray<br>以上就是dual pooling的实现，具体分析如下：</p><ul><li><p>第一行：得到batch-size</p></li><li><p>第二行：得到feature, gray大佬这里用的是一个sequential将所有的模块装载进来，其实也可以用这种方法：</p><pre><code>1  &gt;</code></pre></li></ul><p>| </p><pre><code>    self.base_model = nn.Sequential(*list(model_ft.children())[:-3]) # 取除了后三个全部的层      &gt;   </code></pre><p>—|—  </p></blockquote><p>children方法里就是返回当前模型子模块的迭代器，可以查看源代码，然后选择将其中一部分去掉，比如fc层等，也可以使用gray大佬的这种方法。<br>查找的过程中找到一个中间层可视化的简单代码：<a href="https://www.jianshu.com/p/0a23db1df55a">https://www.jianshu.com/p/0a23db1df55a</a></p><ul><li>第四，五行，通过avg_pool,max_pool得到对应的feature</li><li>第六行，进行concate操作，进行拼接</li><li>第七行，使用了一个卷积层进行降维通道，并进行view展开成一维向量。</li><li>第八层，进行全连接层的分类。<br>参考链接：<br><a href="https://blog.csdn.net/DD_PP_JJ/article/details/103318617">https://blog.csdn.net/DD_PP_JJ/article/details/103318617</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Attention机制中SEnet CBAM以及Dual pooling的pytorch实现</title>
      <link href="/2019/12/27/SENet-code/"/>
      <url>/2019/12/27/SENet-code/</url>
      
        <content type="html"><![CDATA[<blockquote><p>本来自己写了，关于SENet的注意力截止，但是在准备写其他注意力机制代码的时候，看到一篇文章总结的很好，所以对此篇文章进行搬运，以供自己查阅，并加上自己的理解。</p></blockquote><p>[TOC]</p><h2 id="1-SENET中的channel-wise加权的实现"><a href="#1-SENET中的channel-wise加权的实现" class="headerlink" title="1.SENET中的channel-wise加权的实现"></a>1.SENET中的channel-wise加权的实现</h2><p>实现代码参考自：<a href="https://github.com/moskomule/senet.pytorch">senet.pytorch</a><br><img src="//caius-lu.github.io/2019/12/27/SENet-code/images/20191227_SENet-code_senet.png" alt="senet"><br>代码如下：<br>SEnet 模块  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  </code></pre><p>| </p><pre><code> from torch import nn  class SELayer(nn.Module):      def __init__(self, channel, reduction=16):          super(SELayer, self).__init__()          self.avg_pool = nn.AdaptiveAvgPool2d(1)          self.fc = nn.Sequential(              nn.Linear(channel, channel // reduction, bias=False),              nn.ReLU(inplace=True),              nn.Linear(channel // reduction, channel, bias=False),              nn.Sigmoid()          )        def forward(self, x):          b, c, _, _ = x.size()          y = self.avg_pool(x).view(b, c)          y = self.fc(y).view(b, c, 1, 1)          return x * y. (x)    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2019/12/27/SENet-code/images/20191227_SENet-code_senet2.png" alt="senet2"><br>以上代码设计到的API：</p><ul><li><p>AdaptiveAvgPool2d: 自适应平均池化，参数为（n,m）则将原来的feature（w,h）通过pooling得到（n,m）的feature，如果是（n）,则将原来的feature从（w,h）通过pooling得到（n,n）</p></li><li><p>Sequential: torch容器，存放网络层等内容。</p></li><li><p>Linear: 线性层，参数为（in, out）,将原有的in个feature转为out个feature</p></li><li><p>ReLU: 激活层， inplace进行原地操作，节省内存</p></li><li><p>Sigmoid: 激活层，将输入压缩到0-1<br>分析forward进行模型的构建：</p></li><li><p>x是输入的feature,一般各个通道意义如下：（batch size，channel, width , height）,这里获取了batch(b), channel</p></li><li><p>x通过AdaptiveAvgPool2d(1)以后将得到（batch size, channel, 1, 1）, 然后view（b,c）意思是按照b,c进行展开</p><pre><code>1  2  3  4  5  6  7  8  9  10</code></pre></li></ul><p>| </p><pre><code>    In [1]: import torch      In [2]:  x = torch.zeros((16,256,256,256))      In [3]:  import torch.nn as nn      In [4]: avg_pool = nn.AdaptiveAvgPool2d(1)      In [5]: avg_pool(x).shape      Out[5]: torch.Size([16, 256, 1, 1])      In [6]: avg_pool(x).view((16,256)).shape      Out[6]: torch.Size([16, 256])      In [7]: avg_pool(x).squeeze().shape # squeeze()函数也可以将所有通道个数为1的进行挤压      Out[7]: torch.Size([16, 256])        </code></pre><p>—|—  </p><ul><li><p>然后形状为【16, 256】的tensor经过fc:</p></li><li><p>(1) Linear: from 256(channel) to 256/16</p></li><li><p>(2) ReLu：进行一次激活函数</p></li><li><p>(3) Linear: from 256/16 to 256(channel)</p></li><li><p>(4) Sigmoid: 激活到0-1，代表每个通道的重要性</p></li><li><p>然后通过view操作转化为【16,256,1,1】形状的tensor</p></li><li><p>现在y得到的是每一个通道对应的分数（0-1），然后需要将其与通道内容相乘，具体操作使用到了tensor的内置函数expand_as(把一个tensor变成和函数括号内一样形状的tensor，用法与expand类似，相当于expand(tensor.size())</p></li><li><p>x是【16,256,256,256】形状的特征图，y是【16,256,1,1】大小的channel-wise分数，然后需要将其相乘</p></li><li><p>b.expand_as(a)就是将b进行扩充，扩充到a的维度，需要说明的是a的低维度需要比b大，例如b的shape是3<em>1，如果a的shape是3</em>2不会出错，但是是2*2就会报错了。<br>就是必须有一个维度是1，然后用于扩展：</p><pre><code>1  2  3</code></pre></li></ul><p>| </p><pre><code>    In [8]: tensor1 = torch.ones((3,4,1,1))      In [9]: tensor1.expand([3,4,5,5]).shape      Out[9]: torch.Size([3, 4, 5, 5])        </code></pre><p>—|—  </p><p>这样通过expand_as就能得到【16,256,256,256】大小的tensor，其中256*256都是对应通道的1分数，然后与原先的feature相乘，就能得到channel-wise分数计算后的feature。</p><p>在resetnet中的block插入senet模块  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  </code></pre><p>| </p><pre><code>class CifarSEBasicBlock(nn.Module):     def __init__(self, inplanes, planes, stride=1, reduction=16):         super(CifarSEBasicBlock, self).__init__()         self.conv1 = conv3x3(inplanes, planes, stride)         self.bn1 = nn.BatchNorm2d(planes)         self.relu = nn.ReLU(inplace=True)         self.conv2 = conv3x3(planes, planes)         self.bn2 = nn.BatchNorm2d(planes)         self.se = SELayer(planes, reduction)         if inplanes != planes:             self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False),                                             nn.BatchNorm2d(planes))         else:             self.downsample = lambda x: x         self.stride = stride       def forward(self, x):         residual = self.downsample(x)         out = self.conv1(x)         out = self.bn1(out)         out = self.relu(out)           out = self.conv2(out)         out = self.bn2(out)         out = self.se(out)           out += residual         out = self.relu(out)           return out    </code></pre><p>—|—  </p><p>正常的resent的BasicBlock  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  </code></pre><p>| </p><pre><code>class BasicBlock(nn.Module):      def __init__(self, inplanes, planes, stride=1):          super(BasicBlock, self).__init__()          self.conv1 = conv3x3(inplanes, planes, stride)          self.bn1 = nn.BatchNorm2d(planes)          self.relu = nn.ReLU(inplace=True)          self.conv2 = conv3x3(planes, planes)          self.bn2 = nn.BatchNorm2d(planes)          if inplanes != planes:              self.downsample = nn.Sequential(nn.Conv2d(inplanes, planes, kernel_size=1, stride=stride, bias=False),                                              nn.BatchNorm2d(planes))          else:              self.downsample = lambda x: x          self.stride = stride    </code></pre><p>—|—  </p><p>baseline:0.888<br>se+baseline:0.892</p><h2 id="2-CBAM中的通道注意力机制"><a href="#2-CBAM中的通道注意力机制" class="headerlink" title="2.CBAM中的通道注意力机制"></a>2.CBAM中的通道注意力机制</h2><p>channel-attention-module跟以上内容想法有一点像，给每个channel进行打分，具体实现如下：<br>参考来源：<a href="https://github.com/luuuyi/CBAM.PyTorch/blob/master/model/resnet_cbam.py">CBMA.pytorch</a>  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  </code></pre><p>| </p><pre><code>class ChannelAttention(nn.Module):      def __init__(self, in_planes, ratio=16):          super(ChannelAttention, self).__init__()          self.avg_pool = nn.AdaptiveAvgPool2d(1)          self.max_pool = nn.AdaptiveMaxPool2d(1)            self.fc1   = nn.Conv2d(in_planes, in_planes // 16, 1, bias=False)          self.relu1 = nn.ReLU()          self.fc2   = nn.Conv2d(in_planes // 16, in_planes, 1, bias=False)            self.sigmoid = nn.Sigmoid()        def forward(self, x):          avg_out = self.fc2(self.relu1(self.fc1(self.avg_pool(x))))          max_out = self.fc2(self.relu1(self.fc1(self.max_pool(x))))          out = avg_out + max_out          return self.sigmoid(out)    </code></pre><p>—|—  </p><p>API跟上边类似，只添加了卷积，也很简单。需要说明的是貌似Linear和Conv2d中的参数很相似，但是实际上，两者还是很不一样的，Linear接受的是线性的2维数组（batch, 一维特征），Con2d接受的是4维数组（batch, 通道，w, h）。<br><img src="//caius-lu.github.io/2019/12/27/SENet-code/images/20191227_SENet-code_ch.png" alt="ch"><br>forward函数：</p><ul><li>第一行，进行了adaptiveAvgPooling， conv2d, relu, conv2d</li><li>第二行，进行了AdaptiveMaxPooling, conv2d, relu, conv2d</li><li>第三行，将两个向量进行相加</li><li>第四行，将对应结果进行激活，得到通道注意力分数</li></ul><h2 id="3-CBAM中的空间注意力机制"><a href="#3-CBAM中的空间注意力机制" class="headerlink" title="3.CBAM中的空间注意力机制"></a>3.CBAM中的空间注意力机制</h2><p>参考来源：<a href="https://github.com/luuuyi/CBAM.PyTorch/blob/master/model/resnet_cbam.py">CBMA.pytorch</a></p><pre><code>    1      2      3      4      5      6      7      8      9      10      11      12      13      14      15      16      </code></pre><p>| </p><pre><code>    class SpatialAttention(nn.Module):          def __init__(self, kernel_size=7):              super(SpatialAttention, self).__init__()                    assert kernel_size in (3, 7), 'kernel size must be 3 or 7'              padding = 3 if kernel_size == 7 else 1                    self.conv1 = nn.Conv2d(2, 1, kernel_size, padding=padding, bias=False)              self.sigmoid = nn.Sigmoid()                def forward(self, x):              avg_out = torch.mean(x, dim=1, keepdim=True)              max_out, _ = torch.max(x, dim=1, keepdim=True)              x = torch.cat([avg_out, max_out], dim=1)              x = self.conv1(x)              return self.sigmoid(x)        </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2019/12/27/SENet-code/images/20191227_SENet-code_sa.png" alt="sa"><br>Spatial attention module中支持kernel_size=3或者7，默认设置为7。<br>以上涉及到的API:</p><ul><li>torch.mean: 求平均值，dim指的是沿着某一个通道进行计算平均值。这里dim=1，说明沿着通道channel进行平均，对所有channel的feature上相应的像素进行求平均值。</li><li>torch.max: 同上，进行求最大值。<br>forward函数：</li><li>第一行：沿着通道维度进行进行平均，得到一个（batch, 1, w, h）的feature</li><li>第二行：沿着通道维度进行求最大值，得到一个（batch, 1, w, h）的feature</li><li>第三行：将两个feature通过cat的方式拼接起来，得到一个（batch, 2, w, h）的feature</li><li>第四行：对这个feature进行卷积之所以设置如果kernel_size=7的时候padding=3是因为需要将out_feature和in_feature相等，可以带入公式进行计算。</li><li>第五行：进行激活，将得分约束至[0-1]</li></ul><h2 id="4-CBAM中的融合"><a href="#4-CBAM中的融合" class="headerlink" title="4.CBAM中的融合"></a>4.CBAM中的融合</h2><p>参考代码：<a href="https://github.com/luuuyi/CBAM.PyTorch/blob/master/model/resnet_cbam.py">CBMA.pytorch</a><br>在resnet中主要是用在basicBlock中，代码如下：</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  </code></pre><p>| </p><pre><code>class BasicBlock(nn.Module):      expansion = 1        def __init__(self, inplanes, planes, stride=1, downsample=None):          super(BasicBlock, self).__init__()          self.conv1 = conv3x3(inplanes, planes, stride)          self.bn1 = nn.BatchNorm2d(planes)          self.relu = nn.ReLU(inplace=True)          self.conv2 = conv3x3(planes, planes)          self.bn2 = nn.BatchNorm2d(planes)            self.ca = ChannelAttention(planes)          self.sa = SpatialAttention()            self.downsample = downsample          self.stride = stride        def forward(self, x):          residual = x            out = self.conv1(x)          out = self.bn1(out)          out = self.relu(out)            out = self.conv2(out)          out = self.bn2(out)            out = self.ca(out) * out # 广播机制          out = self.sa(out) * out # 广播机制            if self.downsample is not None:              residual = self.downsample(x)          out += residual          out = self.relu(out)          return out    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2019/12/27/SENet-code/images/20191227_SENet-code_cbam.png" alt="cbam"><br>resnet50+cbam: 0.902</p><h2 id="5-dual-pooling的pytorch实现"><a href="#5-dual-pooling的pytorch实现" class="headerlink" title="5.dual pooling的pytorch实现"></a>5.dual pooling的pytorch实现</h2><p>max pooling更注重重要的局部特征, average pooling更关注全局特征.两者concat可以丰富特征层.<br>参考链接:<a href="https://zhuanlan.zhihu.com/p/93806755">GaryLIU</a></p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  </code></pre><p>| </p><pre><code>class res18(nn.Module):      def __init__(self, num_classes):          super(res18, self).__init__()          self.base = resnet18(pretrained=True)          self.feature = nn.Sequential(              self.base.conv1,              self.base.bn1,              self.base.relu,              self.base.maxpool,              self.base.layer1,              self.base.layer2,              self.base.layer3,              self.base.layer4          )          self.avg_pool = nn.AdaptiveAvgPool2d(1)          self.max_pool = nn.AdaptiveMaxPool2d(1)          self.reduce_layer = nn.Conv2d(1024, 512, 1)          self.fc  = nn.Sequential(              nn.Dropout(0.5),              nn.Linear(512, num_classes)              )      def forward(self, x):          bs = x.shape[0]          x = self.feature(x)          x1 = self.avg_pool(x)          x2 = self.max_pool(x)          x = torch.cat([x1, x2], dim=1)          x = self.reduce_layer(x).view(bs, -1)          logits = self.fc(x)          return logits    </code></pre><p>—|—  </p><blockquote><p>这种是在模型层进行改造的一种小trick了，常见的做法：global max/average pooling + fc layer，这里试concat(global max-pooling, global average pooling) + fc layer，其实就是为了丰富特征层，max pooling更加关注重要的局部特征，而average pooling试更加关注全局的特征。不一定有效，我试过不少次，有效的次数比较少，但不少人喜欢这样用.<br>-gray<br>以上就是dual pooling的实现，具体分析如下：</p><ul><li><p>第一行：得到batch-size</p></li><li><p>第二行：得到feature, gray大佬这里用的是一个sequential将所有的模块装载进来，其实也可以用这种方法：</p><pre><code>1  &gt;</code></pre></li></ul><p>| </p><pre><code>    self.base_model = nn.Sequential(*list(model_ft.children())[:-3]) # 取除了后三个全部的层      &gt;   </code></pre><p>—|—  </p></blockquote><p>children方法里就是返回当前模型子模块的迭代器，可以查看源代码，然后选择将其中一部分去掉，比如fc层等，也可以使用gray大佬的这种方法。<br>查找的过程中找到一个中间层可视化的简单代码：<a href="https://www.jianshu.com/p/0a23db1df55a">https://www.jianshu.com/p/0a23db1df55a</a></p><ul><li>第四，五行，通过avg_pool,max_pool得到对应的feature</li><li>第六行，进行concate操作，进行拼接</li><li>第七行，使用了一个卷积层进行降维通道，并进行view展开成一维向量。</li><li>第八层，进行全连接层的分类。<br>参考链接：<br><a href="https://blog.csdn.net/DD_PP_JJ/article/details/103318617">https://blog.csdn.net/DD_PP_JJ/article/details/103318617</a></li></ul>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SpatialSense</title>
      <link href="/2019/12/27/SpatialSense/"/>
      <url>/2019/12/27/SpatialSense/</url>
      
        <content type="html"><![CDATA[<h2 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h2><p>理解图像中物体之间的空间关系是一项具有惊人挑战性的任务(图1)。椅子可能“在”一个人的“后面”，即使它出现在人的左边(取决于人面对的方向)。如果有第三个学生在他们之间，那么两个看起来很近的学生实际上可能并不“挨着”。<br>我们介绍了spatial alsense，这是一个专门研究空间关系识别的数据集，它捕捉了广泛的此类挑战，允许对计算机视觉技术进行适当的基准测试。空间感知是通过对抗性的众包来构建的，在众包中，人类注释者的任务是发现空间关系，这些关系很难用简单的线索来预测，比如二维空间结构或语言先验。与现有的数据集相比，对抗性众包大大减少了数据集的偏倚，并在长尾抽取了更有趣的关系样本。在空间感方面，最先进的识别模型与简单的基线相比，表现得更为出色，这表明它们依赖于直接的线索，而不是对这个复杂的任务进行充分的推理。空间感觉基准测试为提高计算机视觉系统的空间推理能力提供了一条途径。</p><h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ADVERSARIALAL_AUTOAUGMENT</title>
      <link href="/2019/12/26/20191226-ADVERSARIALAL-AUTOAUGMENT/"/>
      <url>/2019/12/26/20191226-ADVERSARIALAL-AUTOAUGMENT/</url>
      
        <content type="html"><![CDATA[<p>数据增广（DA，data augmentation）已被广泛用于改善训练深度神经网络的泛化性。最近，人为设计的数据增广已逐渐被自动学习的增广策略所取代。通过在精心设计的数据增广搜索空间中找到最佳策略，AutoAugment可以显著提高图像分类任务的验证准确性。但是，这种方法在大规模问题上在计算上并不实用。在本文中，我们开发了一种对抗方法，以得出一种计算上可行的解决方案，称为Adversarial AutoAugment（对抗自动增广），可以同时优化目标相关对象和增广策略搜索损失。增广策略网络试图通过生成对抗性增广策略来增加目标网络的训练损失，而目标网络可以从较难的示例中学习更强大的功能，以提高通用性。与先前的工作相反，我们在目标网络训练中重新使用计算以进行策略评估，而无需对目标网络进行再训练。与AutoAugment相比，这使ImageNet的计算成本降低了约12倍，时间开销缩短了11倍。我们在ImageNet上显示了我们在CIFAR-10 / CIFAR-100上的方法的实验结果，并展示了相对于最新技术的显著性能改进。在CIFAR-10上，我们实现了top-1测试误差为1.36％，这是目前表现最佳的单一模型。在ImageNet上，在没有额外数据的情况下，我们在ResNet-50上达到了top-1精度的领先性能，在ResNet-50-D上达到了80.00％。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ADVERSARIALAL_AUTOAUGMENT</title>
      <link href="/2019/12/26/ADVERSARIALAL-AUTOAUGMENT/"/>
      <url>/2019/12/26/ADVERSARIALAL-AUTOAUGMENT/</url>
      
        <content type="html"><![CDATA[<p>数据增广（DA，data augmentation）已被广泛用于改善训练深度神经网络的泛化性。最近，人为设计的数据增广已逐渐被自动学习的增广策略所取代。通过在精心设计的数据增广搜索空间中找到最佳策略，AutoAugment可以显著提高图像分类任务的验证准确性。但是，这种方法在大规模问题上在计算上并不实用。在本文中，我们开发了一种对抗方法，以得出一种计算上可行的解决方案，称为Adversarial AutoAugment（对抗自动增广），可以同时优化目标相关对象和增广策略搜索损失。增广策略网络试图通过生成对抗性增广策略来增加目标网络的训练损失，而目标网络可以从较难的示例中学习更强大的功能，以提高通用性。与先前的工作相反，我们在目标网络训练中重新使用计算以进行策略评估，而无需对目标网络进行再训练。与AutoAugment相比，这使ImageNet的计算成本降低了约12倍，时间开销缩短了11倍。我们在ImageNet上显示了我们在CIFAR-10 / CIFAR-100上的方法的实验结果，并展示了相对于最新技术的显著性能改进。在CIFAR-10上，我们实现了top-1测试误差为1.36％，这是目前表现最佳的单一模型。在ImageNet上，在没有额外数据的情况下，我们在ResNet-50上达到了top-1精度的领先性能，在ResNet-50-D上达到了80.00％。</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scene_Text_Detection</title>
      <link href="/2019/12/25/20191225-Scene-Text-Detection/"/>
      <url>/2019/12/25/20191225-Scene-Text-Detection/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Scene_Text_Detection</title>
      <link href="/2019/12/25/Scene-Text-Detection/"/>
      <url>/2019/12/25/Scene-Text-Detection/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Semantic_segmentation_overview</title>
      <link href="/2019/12/24/20191224-Semantic-segmentation-overview/"/>
      <url>/2019/12/24/20191224-Semantic-segmentation-overview/</url>
      
        <content type="html"><![CDATA[<p>语义分割是近年来出现的基本问题之一，因此成为计算机视觉和机器学习领域的热门话题。</p><h4 id="通用语义分割图像集"><a href="#通用语义分割图像集" class="headerlink" title="通用语义分割图像集"></a>通用语义分割图像集</h4><ul><li>PASCAL Visual Object Classes (VOC)</li><li>Common Objects in Context (COCO)<br>With 200K labelled images, 1.5 million object instances, and 80<br>object categories</li><li>Other General Purpose Semantic Segmentation Image Sets</li><li>YouTube-Objects</li><li>SIFT-flow</li></ul><h4 id="Urban-Street-Semantic-Segmentation-Image-Sets"><a href="#Urban-Street-Semantic-Segmentation-Image-Sets" class="headerlink" title="Urban Street Semantic Segmentation Image Sets"></a>Urban Street Semantic Segmentation Image Sets</h4><ul><li>Cityscapes</li><li>CamVid</li><li>KITTI</li><li>SYNTHIA</li></ul><h2 id="Before-Fully-Convolutional-Networks"><a href="#Before-Fully-Convolutional-Networks" class="headerlink" title="Before Fully Convolutional Networks"></a>Before Fully Convolutional Networks</h2><h3 id="Pre-Deep-Learning-Approaches"><a href="#Pre-Deep-Learning-Approaches" class="headerlink" title="Pre-Deep Learning Approaches"></a>Pre-Deep Learning Approaches</h3><p>传统图像分割与语义分割的区别在于语义特征在图像分割过程中的应用。传统的图像分割方法，如阈值、聚类和区域增长等(有关传统图像分割技术的调查，请参阅[29])使用手工制作的低级特征(即在图像中定位物体的边界。因此，在需要图像语义信息进行像素级分割的情况下，例如在相似物体相互遮挡的情况下，这些方法是必要的。<br>关于深度CNNs流行之前的语义分割工作，有多种方法[30,31，<br>32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]使用的图形模型，如马尔可夫随机域(MRF)，条件<br>随机场(CRF)或基于森林(有时被称为“整体”)的方法，以便在像素级找到场景标签。其主要思想是通过观察相邻像素之间的依赖关系来找到一个推论。换句话说，这些方法将图像的语义建模为相邻像素之间的一种“先验”信息。<br>另一组研究，有时被称为“分层模型”[44,45,46]，使用一个预先训练的和独立的对象探测器的组合，以便从图像中提取语义信息。由于单个的对象检测器未能正确地对区域进行分类，或者由于这些方法受到“手工选择”的检测器库所提供的对象类的有限数量的限制，因此与当今最先进的方法相比，它们的性能相对较低。<br>虽然上述的前深度学习时代的方法不再是首选的分割方法，一些图形模型，特别是CRFs，目前正在使用的最先进的方法作为后处理(细化)层，目的是提高语义分割的性能，具体细节将在下一节中讨论</p><h3 id="Refinement-Methods"><a href="#Refinement-Methods" class="headerlink" title="Refinement Methods"></a>Refinement Methods</h3><p>深度神经网络具有很强的局部特征提取能力。然而，它们缺乏利用全局上下文信息的能力，因此无法对相邻像素预测之间的交互进行建模。另一方面，前深度学习时代流行的分割方法，图形模型，非常适合这类任务。这就是为什么它们目前被用作许多深度基于cnn的语义分割架构的细化层。<br>正如在前一节中提到的，使用图形模型进行分割背后的思想是通过观察相邻像素之间的低层次关系来寻找一个推论。在图2中，可以看到使用基于图形模型的细分对分割结果的影响。分类器(见图2.b)不能正确分割不同类标签相邻的像素。在本例中，我们使用基于crf的细分[42]来改进像素级的分割结果。基于crf的方法被广泛用于深度语义分割方法的细化<br>CRFs[50]是一种有区别的无向概率图形模型。它们被用来对观测之间已知的关系进行编码，并构建一致的解释。它们用作细化层的原因是，与不考虑相邻像素相似性的离散分类器不同，CRF可以利用这些信息。与其他图形化模型(如隐马尔科夫模型)相比，CRFs的主要优点是它们的条件性质和避免标签偏差[50]问题的能力。尽管有相当数量的方法(见表1)使用CRFs进行细化，但这些模型在相对较新的方法中开始变得不受欢迎，因为它们的速度非常慢，而且非常难以优化。</p><h2 id="Early-Deep-Learning-Approaches"><a href="#Early-Deep-Learning-Approaches" class="headerlink" title="Early Deep Learning Approaches"></a>Early Deep Learning Approaches</h2><p>FCN在2014年出现，使用tanh 相较于proposal of a ReLU layer 很难去区分，因此，训练这样的系统被认为是不适合计算的，甚至对大规模数据是不可行的。然而，第一个成熟的方法只是简单地尝试转换分类网络，如AlexNet和VGG通过微调全连接层来细分网络。他们在训练阶段遭受了过度拟合和完全连接层的时间限制。此外，使用的CNNs不够深，无法创建抽象的特征，这与图像的语义有关。在一些早期的深度学习研究中，研究人员拒绝使用完全连接的层来进行决策，而是使用不同的结构，如周期性的架构[57]或使用来自一个单独计算的分段家族的标记。通过提出全连接层FCN这样的结构的必要性的第一个迹象，不出所料，它们被FCN取代。<br>由于他们的分割结果被认为是不令人满意的，这些研究通常使用一个细化的过程，要么作为一个后处理层[52,53,54,56]，或作为一个替代架构，以完全连接的决策层<br>Refinement methods varied such as Markov random fields，nearest neighbour-based approach，使用校准层[54]，使用超级像素[55,56]，或普通CNNs的递归网络。细化层仍然被后fcn方法所使用，其目的是提高类交叉区域的像素级标记性能。<br>tips: 4FCN [11] ] was officially published in 2017. However the same group first shared the idea online as pre-printed literature in 014 [51].</p><h2 id="Fully-Convolutional-Networks-for-Semantic-Segmentation"><a href="#Fully-Convolutional-Networks-for-Semantic-Segmentation" class="headerlink" title="Fully Convolutional Networks for Semantic Segmentation"></a>Fully Convolutional Networks for Semantic Segmentation</h2><p><img src="/images/20191224_Semantic-segmentation-overview_1.png" alt="1"><br>FCN(2017) 提出了从CNNs (DCNN)中拆除全连通层的设想.‘FCN-32s’, ‘FCN16s’, and ‘FCN8s’ all transfer-learnt using the VGG architecture<br>FCN架构在很多方面都被认为是革命性的,</p><ol><li>FCN不包括全连接层</li><li>该结构允许为任何分辨率的图像生成分割图。使用反卷积层，可以将粗深卷积层输出提升到任意分辨率的稠密像素。</li><li>提出了DCNNs的skip架构。跳过架构(或连接)在DCNNs中提供不相邻层之间的链接。仅仅通过对未连接层的输出进行求和或连接，这些连接就可以使信息流动，否则，由于体系结构的选择(如最大池化层或辍学)，这些信息就会丢失。最常见的做法是在max-pooling层之前使用skip连接，它通过选择特定区域的最大值对层输出进行采样。池化层有助于架构创建特性层次，但也会导致局部信息的丢失，而这些局部信息对于语义分割是有价值的，特别是在对象边界。跳过连接通过绕过池化层来保存这些信息并将其转发到更深层。实际上，在[11]中使用跳转连接被认为是相当原始的。“FCN-8s”和“FCN-16s”网络在不同的层包含这些跳过连接。对于相同的架构，即“FCN-4s”和“更密集的跳过连接”。“FCN-2s”也被用于各种应用[61,62]。这一思想最终演变为用于语义分割的编码器-解码器结构[63,27]，下文将对此进行介绍。</li></ol><h2 id="Post-FCN-Approaches"><a href="#Post-FCN-Approaches" class="headerlink" title="Post-FCN Approaches"></a>Post-FCN Approaches</h2><p>drawbacks of FCNs： 特性层次结构中标签本地化的低效丢失、无法处理全局上下文知识以及缺乏多尺度处理机制。我们还讨论了语义分割上下文中的尺度不变性，最后讨论了基于对象检测的方法，这是一种新的解决方案，旨在解决同时检测对象实例的语义分割问题。</p><h3 id="Techniques-for-Fine-grained-Localisation"><a href="#Techniques-for-Fine-grained-Localisation" class="headerlink" title="Techniques for Fine-grained Localisation"></a>Techniques for Fine-grained Localisation</h3><p>根据定义，语义分割是一个密集的过程，因此它需要在像素级对类标签进行细粒度的本地化。例如，在机器人手术中，语义分割中的像素错误可能会导致生存或死亡的情况。层次特性创建的池(即。，最大池)层可以部分失去本地化。此外，由于他们FCNs完全是卷积性质的，它本身并不具备在图像中对全局上下文信息建模的能力，这在类标签本地化方面也非常有效。因此，这两个问题在本质上和本质上是相互交织的下面我们将讨论旨在克服这些问题和提供更好的本地化的不同方法类的标签。</p><h4 id="Encoder-Decoder-Architecture"><a href="#Encoder-Decoder-Architecture" class="headerlink" title="Encoder-Decoder Architecture"></a>Encoder-Decoder Architecture</h4><p><img src="/images/20191224_Semantic-segmentation-overview_ed.png" alt="ed"><br>编译码器， 类似U-net，具有开创新的研究。编码器使用池化层逐渐缩减输入数据的空间维度，而解码器通过反卷积层等网络层逐步恢复目标的细节和相应的空间维度。从编码器到解码器之间，通常存在直接的信息连接，来帮助解码器更好地恢复目标细节。<br>U-Net，Seg-Net 都是非常出名的网络。在这种结构中，由编码器部分相邻的低分辨率特征映射提供的强相关语义信息必须经过额外的中间层才能到达相同的译码层。这通常会导致一定程度的信息衰减。<br>然而，U-Net架构已经被证明对于不同应用的分割非常有用，例如卫星图像。</p><h4 id="Spatial-Pyramid-Pooling-空间金字塔池化"><a href="#Spatial-Pyramid-Pooling-空间金字塔池化" class="headerlink" title="Spatial Pyramid Pooling(空间金字塔池化)"></a>Spatial Pyramid Pooling(空间金字塔池化)</h4><p><img src="/images/20191224_Semantic-segmentation-overview_spp.png" alt="spp"><br>Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. 在2006年首次被提出， 为的是解决单词袋系统失去了特征之间的空间关系。首次应用在深度学习是2015年的SPPNet这篇文章。无论输入大小如何，都可以在空间金字塔汇聚网络中创建深度特征的空间金字塔表示。SPP-Net最重要的贡献是它允许输入不同大小的数据。不同大小的图像输入到卷积层中，不可避免地会产生不同大小的特征图。然而,如果池化层刚好在决策层之前，具有与输入大小成比例的步长值，则创建特征映射这一层将被固定。CNN中的SPP层构建了不同层次特征之间的关系。因此，它与ED结构中的跳过连接非常相似，后者也允许特性层次结构之间的信息流。：SPP层用于语义分割最常见的用法是在[67]中提出的，比如SPP层被附加到最后一个卷积层，并反馈给像素级分类器。</p><h4 id="Feature-Concatenation-特征连接"><a href="#Feature-Concatenation-特征连接" class="headerlink" title="Feature Concatenation(特征连接)"></a>Feature Concatenation(特征连接)</h4><p>这个想法是基于融合从不同来源提取的特征。</p><h4 id="Dilated-Convolution-扩张卷积，空洞卷积"><a href="#Dilated-Convolution-扩张卷积，空洞卷积" class="headerlink" title="Dilated Convolution(扩张卷积，空洞卷积)"></a>Dilated Convolution(扩张卷积，空洞卷积)</h4><p><img src="/images/20191224_Semantic-segmentation-overview_dc.png" alt="dc"><br>扩展卷积的思想实际上很简单:使用连续的卷积滤波器，一个有效的接收域只能随层线性增长;然而，如果使用在滤波器中有间隙的膨胀卷积(见图4.c)，有效接受域将增长得更快[70]。因此，在没有池或子采样的情况下，创建了卷积层的矩形棱镜。扩张卷积是一种非常有效和强大的方法来详细保存特征图分辨率。缺点在于对GPU存储和计算的要求更高，因为特征图分辨率不会在特征层次结构中缩小。</p><h4 id="Conditional-Random-Fields-条件随机场"><a href="#Conditional-Random-Fields-条件随机场" class="headerlink" title="Conditional Random Fields(条件随机场)"></a>Conditional Random Fields(条件随机场)</h4><p>cnn自然缺乏特别的‘关注’类交叉区域的机制。在这些区域周围，通过观察CNN层的相邻feature maps之间的低层关系，使用图形化模型进行推理。因此，图形模型(主要是crf)被用作深度语义分割架构的细化层。与在[72]中一样，CRFs将低级交互与来自多类交互的输出连接起来，并以这种方式构建全局上下文知识。<br>CRFs作为一种细化层，目前存在多种利用CRFs对CNNs进行深度处理的方法，如卷积CRFs[47]、稠密CRF[42]、CRN-as-RNN等[73]。尽管CRFs有助于构建上下文知识，从而在类标签中更好地本地化，表1显示了在“CRF模型”选项卡下分类的CRFs，以便将它们与实际的CNN架构扩展区分开。</p><h4 id="Recurrent-Approaches"><a href="#Recurrent-Approaches" class="headerlink" title="Recurrent Approaches"></a>Recurrent Approaches</h4><p>递归神经网络处理时间信息的能力有助于提高分割精度。例如，[74]使用ConvLSTM层来改进图像序列中的语义分割结果。<br>然而，也有一些方法在静态图像上使用循环结构。在[13]中，研究人员利用LSTMchains来缠绕多个尺度，从而得到像素级的分割改进。也有将CNNs和RNNs融合的混合方法。这方面的一个很好的例子是所谓的ReSeg模型[75]，其中，输入图像被馈送到一个类似于vgg的CNN编码器，然后通过递归层(即ReNet架构)进行处理，以便更好地定位像素标签。据我们所知，语义分割不存在单纯的递归结构，这主要是因为语义分割需要一个初步的基于cnn的特征编码方案。<br>目前，有一种特定类型的RNN，即“注意模块”，有增长的趋势。在这些模块中，RNN在技术上融合了注意力[76]，在预测输出序列的某个部分时，将注意力集中在输入的某个区域。因此，它们也被用于语义分割[77,78,79]。</p><h3 id="Scale-Invariance-尺度变化"><a href="#Scale-Invariance-尺度变化" class="headerlink" title="Scale-Invariance(尺度变化)"></a>Scale-Invariance(尺度变化)</h3><p>根据定义，尺度不变性是指一个方法处理输入时不依赖于相对尺度的能力。或图像分辨率。尽管它对于某些应用程序来说是极其重要的，但是这种能力通常被忽视，或者与方法包含多尺度信息的能力相混淆。一种方法可以使用多尺度信息来提高其像素级分割能力，但仍然依赖于尺度或分辨率。</p><p>文献： A SURVEY ON DEEP LEARNING-BASED ARCHITECTURES FOR SEMANTIC SEGMENTATION ON 2D IMAGES</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Semantic_segmentation_overview</title>
      <link href="/2019/12/24/Semantic-segmentation-overview/"/>
      <url>/2019/12/24/Semantic-segmentation-overview/</url>
      
        <content type="html"><![CDATA[<p>语义分割是近年来出现的基本问题之一，因此成为计算机视觉和机器学习领域的热门话题。</p><h4 id="通用语义分割图像集"><a href="#通用语义分割图像集" class="headerlink" title="通用语义分割图像集"></a>通用语义分割图像集</h4><ul><li>PASCAL Visual Object Classes (VOC)</li><li>Common Objects in Context (COCO)<br>With 200K labelled images, 1.5 million object instances, and 80<br>object categories</li><li>Other General Purpose Semantic Segmentation Image Sets</li><li>YouTube-Objects</li><li>SIFT-flow</li></ul><h4 id="Urban-Street-Semantic-Segmentation-Image-Sets"><a href="#Urban-Street-Semantic-Segmentation-Image-Sets" class="headerlink" title="Urban Street Semantic Segmentation Image Sets"></a>Urban Street Semantic Segmentation Image Sets</h4><ul><li>Cityscapes</li><li>CamVid</li><li>KITTI</li><li>SYNTHIA</li></ul><h2 id="Before-Fully-Convolutional-Networks"><a href="#Before-Fully-Convolutional-Networks" class="headerlink" title="Before Fully Convolutional Networks"></a>Before Fully Convolutional Networks</h2><h3 id="Pre-Deep-Learning-Approaches"><a href="#Pre-Deep-Learning-Approaches" class="headerlink" title="Pre-Deep Learning Approaches"></a>Pre-Deep Learning Approaches</h3><p>传统图像分割与语义分割的区别在于语义特征在图像分割过程中的应用。传统的图像分割方法，如阈值、聚类和区域增长等(有关传统图像分割技术的调查，请参阅[29])使用手工制作的低级特征(即在图像中定位物体的边界。因此，在需要图像语义信息进行像素级分割的情况下，例如在相似物体相互遮挡的情况下，这些方法是必要的。<br>关于深度CNNs流行之前的语义分割工作，有多种方法[30,31，<br>32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42]使用的图形模型，如马尔可夫随机域(MRF)，条件<br>随机场(CRF)或基于森林(有时被称为“整体”)的方法，以便在像素级找到场景标签。其主要思想是通过观察相邻像素之间的依赖关系来找到一个推论。换句话说，这些方法将图像的语义建模为相邻像素之间的一种“先验”信息。<br>另一组研究，有时被称为“分层模型”[44,45,46]，使用一个预先训练的和独立的对象探测器的组合，以便从图像中提取语义信息。由于单个的对象检测器未能正确地对区域进行分类，或者由于这些方法受到“手工选择”的检测器库所提供的对象类的有限数量的限制，因此与当今最先进的方法相比，它们的性能相对较低。<br>虽然上述的前深度学习时代的方法不再是首选的分割方法，一些图形模型，特别是CRFs，目前正在使用的最先进的方法作为后处理(细化)层，目的是提高语义分割的性能，具体细节将在下一节中讨论</p><h3 id="Refinement-Methods"><a href="#Refinement-Methods" class="headerlink" title="Refinement Methods"></a>Refinement Methods</h3><p>深度神经网络具有很强的局部特征提取能力。然而，它们缺乏利用全局上下文信息的能力，因此无法对相邻像素预测之间的交互进行建模。另一方面，前深度学习时代流行的分割方法，图形模型，非常适合这类任务。这就是为什么它们目前被用作许多深度基于cnn的语义分割架构的细化层。<br>正如在前一节中提到的，使用图形模型进行分割背后的思想是通过观察相邻像素之间的低层次关系来寻找一个推论。在图2中，可以看到使用基于图形模型的细分对分割结果的影响。分类器(见图2.b)不能正确分割不同类标签相邻的像素。在本例中，我们使用基于crf的细分[42]来改进像素级的分割结果。基于crf的方法被广泛用于深度语义分割方法的细化<br>CRFs[50]是一种有区别的无向概率图形模型。它们被用来对观测之间已知的关系进行编码，并构建一致的解释。它们用作细化层的原因是，与不考虑相邻像素相似性的离散分类器不同，CRF可以利用这些信息。与其他图形化模型(如隐马尔科夫模型)相比，CRFs的主要优点是它们的条件性质和避免标签偏差[50]问题的能力。尽管有相当数量的方法(见表1)使用CRFs进行细化，但这些模型在相对较新的方法中开始变得不受欢迎，因为它们的速度非常慢，而且非常难以优化。</p><h2 id="Early-Deep-Learning-Approaches"><a href="#Early-Deep-Learning-Approaches" class="headerlink" title="Early Deep Learning Approaches"></a>Early Deep Learning Approaches</h2><p>FCN在2014年出现，使用tanh 相较于proposal of a ReLU layer 很难去区分，因此，训练这样的系统被认为是不适合计算的，甚至对大规模数据是不可行的。然而，第一个成熟的方法只是简单地尝试转换分类网络，如AlexNet和VGG通过微调全连接层来细分网络。他们在训练阶段遭受了过度拟合和完全连接层的时间限制。此外，使用的CNNs不够深，无法创建抽象的特征，这与图像的语义有关。在一些早期的深度学习研究中，研究人员拒绝使用完全连接的层来进行决策，而是使用不同的结构，如周期性的架构[57]或使用来自一个单独计算的分段家族的标记。通过提出全连接层FCN这样的结构的必要性的第一个迹象，不出所料，它们被FCN取代。<br>由于他们的分割结果被认为是不令人满意的，这些研究通常使用一个细化的过程，要么作为一个后处理层[52,53,54,56]，或作为一个替代架构，以完全连接的决策层<br>Refinement methods varied such as Markov random fields，nearest neighbour-based approach，使用校准层[54]，使用超级像素[55,56]，或普通CNNs的递归网络。细化层仍然被后fcn方法所使用，其目的是提高类交叉区域的像素级标记性能。<br>tips: 4FCN [11] ] was officially published in 2017. However the same group first shared the idea online as pre-printed literature in 014 [51].</p><h2 id="Fully-Convolutional-Networks-for-Semantic-Segmentation"><a href="#Fully-Convolutional-Networks-for-Semantic-Segmentation" class="headerlink" title="Fully Convolutional Networks for Semantic Segmentation"></a>Fully Convolutional Networks for Semantic Segmentation</h2><p><img src="//caius-lu.github.io/2019/12/24/Semantic-segmentation-overview/images/20191224_Semantic-segmentation-overview_1.png" alt="1"><br>FCN(2017) 提出了从CNNs (DCNN)中拆除全连通层的设想.‘FCN-32s’, ‘FCN16s’, and ‘FCN8s’ all transfer-learnt using the VGG architecture<br>FCN架构在很多方面都被认为是革命性的,</p><ol><li>FCN不包括全连接层</li><li>该结构允许为任何分辨率的图像生成分割图。使用反卷积层，可以将粗深卷积层输出提升到任意分辨率的稠密像素。</li><li>提出了DCNNs的skip架构。跳过架构(或连接)在DCNNs中提供不相邻层之间的链接。仅仅通过对未连接层的输出进行求和或连接，这些连接就可以使信息流动，否则，由于体系结构的选择(如最大池化层或辍学)，这些信息就会丢失。最常见的做法是在max-pooling层之前使用skip连接，它通过选择特定区域的最大值对层输出进行采样。池化层有助于架构创建特性层次，但也会导致局部信息的丢失，而这些局部信息对于语义分割是有价值的，特别是在对象边界。跳过连接通过绕过池化层来保存这些信息并将其转发到更深层。实际上，在[11]中使用跳转连接被认为是相当原始的。“FCN-8s”和“FCN-16s”网络在不同的层包含这些跳过连接。对于相同的架构，即“FCN-4s”和“更密集的跳过连接”。“FCN-2s”也被用于各种应用[61,62]。这一思想最终演变为用于语义分割的编码器-解码器结构[63,27]，下文将对此进行介绍。</li></ol><h2 id="Post-FCN-Approaches"><a href="#Post-FCN-Approaches" class="headerlink" title="Post-FCN Approaches"></a>Post-FCN Approaches</h2><p>drawbacks of FCNs： 特性层次结构中标签本地化的低效丢失、无法处理全局上下文知识以及缺乏多尺度处理机制。我们还讨论了语义分割上下文中的尺度不变性，最后讨论了基于对象检测的方法，这是一种新的解决方案，旨在解决同时检测对象实例的语义分割问题。</p><h3 id="Techniques-for-Fine-grained-Localisation"><a href="#Techniques-for-Fine-grained-Localisation" class="headerlink" title="Techniques for Fine-grained Localisation"></a>Techniques for Fine-grained Localisation</h3><p>根据定义，语义分割是一个密集的过程，因此它需要在像素级对类标签进行细粒度的本地化。例如，在机器人手术中，语义分割中的像素错误可能会导致生存或死亡的情况。层次特性创建的池(即。，最大池)层可以部分失去本地化。此外，由于他们FCNs完全是卷积性质的，它本身并不具备在图像中对全局上下文信息建模的能力，这在类标签本地化方面也非常有效。因此，这两个问题在本质上和本质上是相互交织的下面我们将讨论旨在克服这些问题和提供更好的本地化的不同方法类的标签。</p><h4 id="Encoder-Decoder-Architecture"><a href="#Encoder-Decoder-Architecture" class="headerlink" title="Encoder-Decoder Architecture"></a>Encoder-Decoder Architecture</h4><p><img src="//caius-lu.github.io/2019/12/24/Semantic-segmentation-overview/images/20191224_Semantic-segmentation-overview_ed.png" alt="ed"><br>编译码器， 类似U-net，具有开创新的研究。编码器使用池化层逐渐缩减输入数据的空间维度，而解码器通过反卷积层等网络层逐步恢复目标的细节和相应的空间维度。从编码器到解码器之间，通常存在直接的信息连接，来帮助解码器更好地恢复目标细节。<br>U-Net，Seg-Net 都是非常出名的网络。在这种结构中，由编码器部分相邻的低分辨率特征映射提供的强相关语义信息必须经过额外的中间层才能到达相同的译码层。这通常会导致一定程度的信息衰减。<br>然而，U-Net架构已经被证明对于不同应用的分割非常有用，例如卫星图像。</p><h4 id="Spatial-Pyramid-Pooling-空间金字塔池化"><a href="#Spatial-Pyramid-Pooling-空间金字塔池化" class="headerlink" title="Spatial Pyramid Pooling(空间金字塔池化)"></a>Spatial Pyramid Pooling(空间金字塔池化)</h4><p><img src="//caius-lu.github.io/2019/12/24/Semantic-segmentation-overview/images/20191224_Semantic-segmentation-overview_spp.png" alt="spp"><br>Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. 在2006年首次被提出， 为的是解决单词袋系统失去了特征之间的空间关系。首次应用在深度学习是2015年的SPPNet这篇文章。无论输入大小如何，都可以在空间金字塔汇聚网络中创建深度特征的空间金字塔表示。SPP-Net最重要的贡献是它允许输入不同大小的数据。不同大小的图像输入到卷积层中，不可避免地会产生不同大小的特征图。然而,如果池化层刚好在决策层之前，具有与输入大小成比例的步长值，则创建特征映射这一层将被固定。CNN中的SPP层构建了不同层次特征之间的关系。因此，它与ED结构中的跳过连接非常相似，后者也允许特性层次结构之间的信息流。：SPP层用于语义分割最常见的用法是在[67]中提出的，比如SPP层被附加到最后一个卷积层，并反馈给像素级分类器。</p><h4 id="Feature-Concatenation-特征连接"><a href="#Feature-Concatenation-特征连接" class="headerlink" title="Feature Concatenation(特征连接)"></a>Feature Concatenation(特征连接)</h4><p>这个想法是基于融合从不同来源提取的特征。</p><h4 id="Dilated-Convolution-扩张卷积，空洞卷积"><a href="#Dilated-Convolution-扩张卷积，空洞卷积" class="headerlink" title="Dilated Convolution(扩张卷积，空洞卷积)"></a>Dilated Convolution(扩张卷积，空洞卷积)</h4><p><img src="//caius-lu.github.io/2019/12/24/Semantic-segmentation-overview/images/20191224_Semantic-segmentation-overview_dc.png" alt="dc"><br>扩展卷积的思想实际上很简单:使用连续的卷积滤波器，一个有效的接收域只能随层线性增长;然而，如果使用在滤波器中有间隙的膨胀卷积(见图4.c)，有效接受域将增长得更快[70]。因此，在没有池或子采样的情况下，创建了卷积层的矩形棱镜。扩张卷积是一种非常有效和强大的方法来详细保存特征图分辨率。缺点在于对GPU存储和计算的要求更高，因为特征图分辨率不会在特征层次结构中缩小。</p><h4 id="Conditional-Random-Fields-条件随机场"><a href="#Conditional-Random-Fields-条件随机场" class="headerlink" title="Conditional Random Fields(条件随机场)"></a>Conditional Random Fields(条件随机场)</h4><p>cnn自然缺乏特别的‘关注’类交叉区域的机制。在这些区域周围，通过观察CNN层的相邻feature maps之间的低层关系，使用图形化模型进行推理。因此，图形模型(主要是crf)被用作深度语义分割架构的细化层。与在[72]中一样，CRFs将低级交互与来自多类交互的输出连接起来，并以这种方式构建全局上下文知识。<br>CRFs作为一种细化层，目前存在多种利用CRFs对CNNs进行深度处理的方法，如卷积CRFs[47]、稠密CRF[42]、CRN-as-RNN等[73]。尽管CRFs有助于构建上下文知识，从而在类标签中更好地本地化，表1显示了在“CRF模型”选项卡下分类的CRFs，以便将它们与实际的CNN架构扩展区分开。</p><h4 id="Recurrent-Approaches"><a href="#Recurrent-Approaches" class="headerlink" title="Recurrent Approaches"></a>Recurrent Approaches</h4><p>递归神经网络处理时间信息的能力有助于提高分割精度。例如，[74]使用ConvLSTM层来改进图像序列中的语义分割结果。<br>然而，也有一些方法在静态图像上使用循环结构。在[13]中，研究人员利用LSTMchains来缠绕多个尺度，从而得到像素级的分割改进。也有将CNNs和RNNs融合的混合方法。这方面的一个很好的例子是所谓的ReSeg模型[75]，其中，输入图像被馈送到一个类似于vgg的CNN编码器，然后通过递归层(即ReNet架构)进行处理，以便更好地定位像素标签。据我们所知，语义分割不存在单纯的递归结构，这主要是因为语义分割需要一个初步的基于cnn的特征编码方案。<br>目前，有一种特定类型的RNN，即“注意模块”，有增长的趋势。在这些模块中，RNN在技术上融合了注意力[76]，在预测输出序列的某个部分时，将注意力集中在输入的某个区域。因此，它们也被用于语义分割[77,78,79]。</p><h3 id="Scale-Invariance-尺度变化"><a href="#Scale-Invariance-尺度变化" class="headerlink" title="Scale-Invariance(尺度变化)"></a>Scale-Invariance(尺度变化)</h3><p>根据定义，尺度不变性是指一个方法处理输入时不依赖于相对尺度的能力。或图像分辨率。尽管它对于某些应用程序来说是极其重要的，但是这种能力通常被忽视，或者与方法包含多尺度信息的能力相混淆。一种方法可以使用多尺度信息来提高其像素级分割能力，但仍然依赖于尺度或分辨率。</p><p>文献： A SURVEY ON DEEP LEARNING-BASED ARCHITECTURES FOR SEMANTIC SEGMENTATION ON 2D IMAGES</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二叉树打印</title>
      <link href="/2019/12/19/20191219-%E4%BA%8C%E5%8F%89%E6%A0%91%E6%89%93%E5%8D%B0/"/>
      <url>/2019/12/19/20191219-%E4%BA%8C%E5%8F%89%E6%A0%91%E6%89%93%E5%8D%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="二叉树按层遍历"><a href="#二叉树按层遍历" class="headerlink" title="二叉树按层遍历"></a>二叉树按层遍历</h2><ol><li>针对二叉树的宽度优先遍历</li><li>宽度优先遍历常使用队列结构</li><li>面试中，该类题目常常对换行有所要求<br><img src="/images/20191219_%E4%BA%8C%E5%8F%89%E6%A0%91%E6%89%93%E5%8D%B0_1.png" alt="1"><br>last: 表示正在打印的当前行的最右节点<br>nlast：表示下一行的最右节点<br><img src="/images/20191219_%E4%BA%8C%E5%8F%89%E6%A0%91%E6%89%93%E5%8D%B0_2.png" alt="2"></li></ol><h2 id="二叉树的序列化和反序列化"><a href="#二叉树的序列化和反序列化" class="headerlink" title="二叉树的序列化和反序列化"></a>二叉树的序列化和反序列化</h2><ol start="4"><li>二叉树-&gt;字符串（序列化</li><li>字符串-&gt;二叉树（反序列化</li></ol><h3 id="序列化的方式："><a href="#序列化的方式：" class="headerlink" title="序列化的方式："></a>序列化的方式：</h3><ol start="6"><li>根据先序遍历序列化</li><li>根据中序遍历序列化</li><li>根据后序遍历序列化</li><li>按层序列化</li></ol>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二叉树打印</title>
      <link href="/2019/12/19/%E4%BA%8C%E5%8F%89%E6%A0%91%E6%89%93%E5%8D%B0/"/>
      <url>/2019/12/19/%E4%BA%8C%E5%8F%89%E6%A0%91%E6%89%93%E5%8D%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="二叉树按层遍历"><a href="#二叉树按层遍历" class="headerlink" title="二叉树按层遍历"></a>二叉树按层遍历</h2><ol><li>针对二叉树的宽度优先遍历</li><li>宽度优先遍历常使用队列结构</li><li>面试中，该类题目常常对换行有所要求<br><img src="//caius-lu.github.io/2019/12/19/%E4%BA%8C%E5%8F%89%E6%A0%91%E6%89%93%E5%8D%B0/images/20191219_%E4%BA%8C%E5%8F%89%E6%A0%91%E6%89%93%E5%8D%B0_1.png" alt="1"><br>last: 表示正在打印的当前行的最右节点<br>nlast：表示下一行的最右节点<br><img src="//caius-lu.github.io/2019/12/19/%E4%BA%8C%E5%8F%89%E6%A0%91%E6%89%93%E5%8D%B0/images/20191219_%E4%BA%8C%E5%8F%89%E6%A0%91%E6%89%93%E5%8D%B0_2.png" alt="2"></li></ol><h2 id="二叉树的序列化和反序列化"><a href="#二叉树的序列化和反序列化" class="headerlink" title="二叉树的序列化和反序列化"></a>二叉树的序列化和反序列化</h2><ol start="4"><li>二叉树-&gt;字符串（序列化</li><li>字符串-&gt;二叉树（反序列化</li></ol><h3 id="序列化的方式："><a href="#序列化的方式：" class="headerlink" title="序列化的方式："></a>序列化的方式：</h3><ol start="6"><li>根据先序遍历序列化</li><li>根据中序遍历序列化</li><li>根据后序遍历序列化</li><li>按层序列化</li></ol>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>隐马尔可夫HMM</title>
      <link href="/2019/11/28/20191128-HMM/"/>
      <url>/2019/11/28/20191128-HMM/</url>
      
        <content type="html"><![CDATA[<h4 id="马尔科夫模型"><a href="#马尔科夫模型" class="headerlink" title="马尔科夫模型"></a>马尔科夫模型</h4><ul><li><p>状态之间可以发生转换，昨天和今天转换的情况：</p></li><li><p>今天能得到明天的情况，明天能得到后天的情况，以此类推可以无限的玩下去</p></li><li><p>这里我们就定义好了一个一阶马尔科夫模型：<br>状态：晴天，多云，雷雨<br>状态转换概率：三种天气状态间的转换概率<br>初始概率：晴天</p></li><li><p>计算今天(t=1)的天气状况：<br>今天为晴天的概率=初始晴天概率X晴天转晴天概率</p><pre><code>+初始多云概率X多云转晴天概率       +初始雷雨概率X雷雨转晴天概率。</code></pre></li></ul><h4 id="隐马尔科夫模型"><a href="#隐马尔科夫模型" class="headerlink" title="隐马尔科夫模型"></a>隐马尔科夫模型</h4><p><img src="/images/20191128_HMM_HMM.jpg" alt="HMM"></p><ul><li>当前的状态只和前一状态有关：</li><li>某个观测只和生成它的状态有关：<img src="/images/20191128_HMM_HMM2.jpg" alt="HMM2"></li></ul><h5 id="隐马尔科夫模型的组成"><a href="#隐马尔科夫模型的组成" class="headerlink" title="隐马尔科夫模型的组成"></a>隐马尔科夫模型的组成</h5><ul><li>三个必备：初始概率(π)，隐藏状态转移概率矩阵(A)，生成观测状态概率矩阵(B)。</li><li>隐藏状态与观察状态（B矩阵）:</li></ul><h4 id="要解决的问题-模型为"><a href="#要解决的问题-模型为" class="headerlink" title="要解决的问题: 模型为"></a>要解决的问题: 模型为</h4><ol><li>给定模型及观测序列 计算其出现的概率</li><li>给定观测序列求解参数使得最大</li><li>已知模型和观测序列求状态序列，使得最大</li></ol><h4 id="求观测序列的概率"><a href="#求观测序列的概率" class="headerlink" title="求观测序列的概率"></a>求观测序列的概率</h4><ul><li>暴力求解：我们要求的是在给定模型下观测序列出现的概率，那如果我能把<br>所有的隐藏序列都给列出来，也就可以知道联合概率分布</li><li>现在要求的目标就很明确了。在给定模型下，一个隐藏序列出现的概率，那就由初始状态慢慢转换嘛。出现的概率为：</li></ul><h5 id="前向算法"><a href="#前向算法" class="headerlink" title="前向算法"></a>前向算法</h5><p>给定t时刻的隐藏状态为i，观测序列为o1,o2…ot的概率叫做前向概率：<img src="/images/20191128_HMM_HMM3.jpg" alt="HMM3"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>隐马尔可夫HMM</title>
      <link href="/2019/11/28/HMM/"/>
      <url>/2019/11/28/HMM/</url>
      
        <content type="html"><![CDATA[<h4 id="马尔科夫模型"><a href="#马尔科夫模型" class="headerlink" title="马尔科夫模型"></a>马尔科夫模型</h4><ul><li><p>状态之间可以发生转换，昨天和今天转换的情况：</p></li><li><p>今天能得到明天的情况，明天能得到后天的情况，以此类推可以无限的玩下去</p></li><li><p>这里我们就定义好了一个一阶马尔科夫模型：<br>状态：晴天，多云，雷雨<br>状态转换概率：三种天气状态间的转换概率<br>初始概率：晴天</p></li><li><p>计算今天(t=1)的天气状况：<br>今天为晴天的概率=初始晴天概率X晴天转晴天概率</p><pre><code>+初始多云概率X多云转晴天概率       +初始雷雨概率X雷雨转晴天概率。</code></pre></li></ul><h4 id="隐马尔科夫模型"><a href="#隐马尔科夫模型" class="headerlink" title="隐马尔科夫模型"></a>隐马尔科夫模型</h4><p><img src="//caius-lu.github.io/2019/11/28/HMM/images/20191128_HMM_HMM.jpg" alt="HMM"></p><ul><li>当前的状态只和前一状态有关：</li><li>某个观测只和生成它的状态有关：<img src="//caius-lu.github.io/2019/11/28/HMM/images/20191128_HMM_HMM2.jpg" alt="HMM2"></li></ul><h5 id="隐马尔科夫模型的组成"><a href="#隐马尔科夫模型的组成" class="headerlink" title="隐马尔科夫模型的组成"></a>隐马尔科夫模型的组成</h5><ul><li>三个必备：初始概率(π)，隐藏状态转移概率矩阵(A)，生成观测状态概率矩阵(B)。</li><li>隐藏状态与观察状态（B矩阵）:</li></ul><h4 id="要解决的问题-模型为"><a href="#要解决的问题-模型为" class="headerlink" title="要解决的问题: 模型为"></a>要解决的问题: 模型为</h4><ol><li>给定模型及观测序列 计算其出现的概率</li><li>给定观测序列求解参数使得最大</li><li>已知模型和观测序列求状态序列，使得最大</li></ol><h4 id="求观测序列的概率"><a href="#求观测序列的概率" class="headerlink" title="求观测序列的概率"></a>求观测序列的概率</h4><ul><li>暴力求解：我们要求的是在给定模型下观测序列出现的概率，那如果我能把<br>所有的隐藏序列都给列出来，也就可以知道联合概率分布</li><li>现在要求的目标就很明确了。在给定模型下，一个隐藏序列出现的概率，那就由初始状态慢慢转换嘛。出现的概率为：</li></ul><h5 id="前向算法"><a href="#前向算法" class="headerlink" title="前向算法"></a>前向算法</h5><p>给定t时刻的隐藏状态为i，观测序列为o1,o2…ot的概率叫做前向概率：<img src="//caius-lu.github.io/2019/11/28/HMM/images/20191128_HMM_HMM3.jpg" alt="HMM3"></p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAN</title>
      <link href="/2019/11/26/20191126-GAN/"/>
      <url>/2019/11/26/20191126-GAN/</url>
      
        <content type="html"><![CDATA[<h3 id="对抗生成网络-GAN-Generative-Adversarial-Nets"><a href="#对抗生成网络-GAN-Generative-Adversarial-Nets" class="headerlink" title="对抗生成网络 GAN(Generative Adversarial Nets)"></a>对抗生成网络 GAN(Generative Adversarial Nets)</h3><p><img src="/images/20191126_GAN_%E5%9B%BE%E7%89%871.png" alt="图片1"></p><h3 id="Adversarial-Nets-Framework"><a href="#Adversarial-Nets-Framework" class="headerlink" title="Adversarial Nets Framework"></a><strong>Adversarial Nets Framework</strong></h3><p><img src="/images/20191126_GAN_%E5%9B%BE%E7%89%872.png" alt="图片2"></p><p>生成器与判别器状态相等</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>它做的是去最大化 D的区分度，最小化G和real数据集的数据分布</p><p>判别模型：</p><p>D1 和 D2 相同的，是判别器，G是生成器</p><p>生成模型：</p><p>先训练判别器，在训练生成器。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Author: Your name</span></span><br><span class="line"><span class="comment"># @Date:   2019-11-26 09:12:52</span></span><br><span class="line"><span class="comment"># @Last Modified by:   Your name</span></span><br><span class="line"><span class="comment"># @Last Modified time: 2019-11-26 09:12:52</span></span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> scipy.stats <span class="keyword">import</span> norm</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> animation</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">sns.<span class="built_in">set</span>(color_codes=<span class="literal">True</span>)</span><br><span class="line">seed = <span class="number">42</span></span><br><span class="line">np.random.seed(seed)</span><br><span class="line">tf.set_random_seed(seed)</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataDistribution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        self.mu = <span class="number">4</span></span><br><span class="line">        self.sigma = <span class="number">0.5</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self, N</span>):</span><br><span class="line">        samples = np.random.normal(self.mu, self.sigma, N) <span class="comment"># 生成高斯分布的概率密度随机数 均值，标准差</span></span><br><span class="line">        samples.sort()</span><br><span class="line">        <span class="keyword">return</span> samples</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GeneratorDistribution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, <span class="built_in">range</span></span>):</span><br><span class="line">        self.<span class="built_in">range</span> = <span class="built_in">range</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">sample</span>(<span class="params">self, N</span>):</span><br><span class="line">        <span class="keyword">return</span> np.linspace(-self.<span class="built_in">range</span>, self.<span class="built_in">range</span>, N) + \</span><br><span class="line">            np.random.random(N) * <span class="number">0.01</span>  <span class="comment"># 生成随机数</span></span><br><span class="line"><span class="comment"># 对线性相乘进行初始化</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">linear</span>(<span class="params"><span class="built_in">input</span>, output_dim, scope=<span class="literal">None</span>, stddev=<span class="number">1.0</span></span>):</span><br><span class="line">    norm = tf.random_normal_initializer(stddev=stddev)</span><br><span class="line">    const = tf.constant_initializer(<span class="number">0.0</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(scope <span class="keyword">or</span> <span class="string">'linear'</span>): <span class="comment"># 定义命名空间</span></span><br><span class="line">        w = tf.get_variable(<span class="string">'w'</span>, [<span class="built_in">input</span>.get_shape()[<span class="number">1</span>], output_dim], initializer=norm)</span><br><span class="line">        b = tf.get_variable(<span class="string">'b'</span>, [output_dim], initializer=const)</span><br><span class="line">        <span class="keyword">return</span> tf.matmul(<span class="built_in">input</span>, w) + b</span><br><span class="line"><span class="comment"># 生成器</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generator</span>(<span class="params"><span class="built_in">input</span>, h_dim</span>):</span><br><span class="line">    <span class="comment"># 这个函数的作用是计算激活函数softplus，即log( exp( features ) + 1)</span></span><br><span class="line">    h0 = tf.nn.softplus(linear(<span class="built_in">input</span>, h_dim, <span class="string">'g0'</span>))</span><br><span class="line">    h1 = linear(h0, <span class="number">1</span>, <span class="string">'g1'</span>)</span><br><span class="line">    <span class="keyword">return</span> h1</span><br><span class="line"><span class="comment"># 判别器</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">discriminator</span>(<span class="params"><span class="built_in">input</span>, h_dim</span>):</span><br><span class="line">    h0 = tf.tanh(linear(<span class="built_in">input</span>, h_dim * <span class="number">2</span>, <span class="string">'d0'</span>))</span><br><span class="line">    h1 = tf.tanh(linear(h0, h_dim * <span class="number">2</span>, <span class="string">'d1'</span>))</span><br><span class="line">    h2 = tf.tanh(linear(h1, h_dim * <span class="number">2</span>, scope=<span class="string">'d2'</span>))</span><br><span class="line">    h3 = tf.sigmoid(linear(h2, <span class="number">1</span>, scope=<span class="string">'d3'</span>))</span><br><span class="line">    <span class="keyword">return</span> h3</span><br><span class="line"><span class="comment"># 优化器</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">optimizer</span>(<span class="params">loss, var_list, initial_learning_rate</span>):</span><br><span class="line">    decay = <span class="number">0.95</span></span><br><span class="line">    num_decay_steps = <span class="number">150</span></span><br><span class="line">    batch = tf.Variable(<span class="number">0</span>)</span><br><span class="line">    learning_rate = tf.train.exponential_decay(</span><br><span class="line">        initial_learning_rate,</span><br><span class="line">        batch,</span><br><span class="line">        num_decay_steps,</span><br><span class="line">        decay,</span><br><span class="line">        staircase=<span class="literal">True</span></span><br><span class="line">    )</span><br><span class="line">    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(</span><br><span class="line">        loss,</span><br><span class="line">        global_step=batch,</span><br><span class="line">        var_list=var_list</span><br><span class="line">    )</span><br><span class="line">    <span class="keyword">return</span> optimizer</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">GAN</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, data, gen, num_steps, batch_size, log_every</span>):</span><br><span class="line">        self.data = data</span><br><span class="line">        self.gen = gen</span><br><span class="line">        self.num_steps = num_steps</span><br><span class="line">        self.batch_size = batch_size</span><br><span class="line">        self.log_every = log_every</span><br><span class="line">        self.mlp_hidden_size = <span class="number">4</span></span><br><span class="line">        self.learning_rate = <span class="number">0.03</span></span><br><span class="line">        self._create_model()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_create_model</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'D_pre'</span>):</span><br><span class="line">            self.pre_input = tf.placeholder(tf.float32, shape=(self.batch_size, <span class="number">1</span>))</span><br><span class="line">            self.pre_labels = tf.placeholder(tf.float32, shape=(self.batch_size, <span class="number">1</span>))</span><br><span class="line">            D_pre = discriminator(self.pre_input, self.mlp_hidden_size)</span><br><span class="line">            self.pre_loss = tf.reduce_mean(tf.square(D_pre - self.pre_labels))</span><br><span class="line">            self.pre_opt = optimizer(self.pre_loss, <span class="literal">None</span>, self.learning_rate)</span><br><span class="line">        <span class="comment"># This defines the generator network - it takes samples from a noise</span></span><br><span class="line">        <span class="comment"># distribution as input, and passes them through an MLP.</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'Gen'</span>):</span><br><span class="line">            self.z = tf.placeholder(tf.float32, shape=(self.batch_size, <span class="number">1</span>))</span><br><span class="line">            self.G = generator(self.z, self.mlp_hidden_size)</span><br><span class="line">        <span class="comment"># The discriminator tries to tell the difference between samples from the</span></span><br><span class="line">        <span class="comment"># true data distribution (self.x) and the generated samples (self.z).</span></span><br><span class="line">        <span class="comment">#</span></span><br><span class="line">        <span class="comment"># Here we create two copies of the discriminator network (that share parameters),</span></span><br><span class="line">        <span class="comment"># as you cannot use the same network with different inputs in TensorFlow.</span></span><br><span class="line">        <span class="keyword">with</span> tf.variable_scope(<span class="string">'Disc'</span>) <span class="keyword">as</span> scope:</span><br><span class="line">            self.x = tf.placeholder(tf.float32, shape=(self.batch_size, <span class="number">1</span>))</span><br><span class="line">            self.D1 = discriminator(self.x, self.mlp_hidden_size)</span><br><span class="line">            scope.reuse_variables()</span><br><span class="line">            self.D2 = discriminator(self.G, self.mlp_hidden_size)</span><br><span class="line">        <span class="comment"># Define the loss for discriminator and generator networks (see the original</span></span><br><span class="line">        <span class="comment"># paper for details), and create optimizers for both</span></span><br><span class="line">        self.loss_d = tf.reduce_mean(-tf.log(self.D1) - tf.log(<span class="number">1</span> - self.D2))</span><br><span class="line">        self.loss_g = tf.reduce_mean(-tf.log(self.D2))</span><br><span class="line">        self.d_pre_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=<span class="string">'D_pre'</span>)</span><br><span class="line">        self.d_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=<span class="string">'Disc'</span>)</span><br><span class="line">        self.g_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope=<span class="string">'Gen'</span>)</span><br><span class="line">        self.opt_d = optimizer(self.loss_d, self.d_params, self.learning_rate)</span><br><span class="line">        self.opt_g = optimizer(self.loss_g, self.g_params, self.learning_rate)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">with</span> tf.Session() <span class="keyword">as</span> session:</span><br><span class="line">            tf.global_variables_initializer().run()</span><br><span class="line">            <span class="comment"># pretraining discriminator</span></span><br><span class="line">            num_pretrain_steps = <span class="number">1000</span></span><br><span class="line">            <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(num_pretrain_steps):</span><br><span class="line">                d = (np.random.random(self.batch_size) - <span class="number">0.5</span>) * <span class="number">10.0</span></span><br><span class="line">                labels = norm.pdf(d, loc=self.data.mu, scale=self.data.sigma) <span class="comment"># norm.pdf:正态概率密度函数</span></span><br><span class="line">                pretrain_loss, _ = session.run([self.pre_loss, self.pre_opt], {</span><br><span class="line">                    self.pre_input: np.reshape(d, (self.batch_size, <span class="number">1</span>)),</span><br><span class="line">                    self.pre_labels: np.reshape(labels, (self.batch_size, <span class="number">1</span>))</span><br><span class="line">                })</span><br><span class="line">            self.weightsD = session.run(self.d_pre_params)</span><br><span class="line">            <span class="comment"># copy weights from pre-training over to new D network</span></span><br><span class="line">            <span class="keyword">for</span> i, v <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.d_params):</span><br><span class="line">                session.run(v.assign(self.weightsD[i]))</span><br><span class="line">            <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(self.num_steps):</span><br><span class="line">                <span class="comment"># update discriminator</span></span><br><span class="line">                x = self.data.sample(self.batch_size)</span><br><span class="line">                z = self.gen.sample(self.batch_size)</span><br><span class="line">                loss_d, _ = session.run([self.loss_d, self.opt_d], {</span><br><span class="line">                    self.x: np.reshape(x, (self.batch_size, <span class="number">1</span>)),</span><br><span class="line">                    self.z: np.reshape(z, (self.batch_size, <span class="number">1</span>))</span><br><span class="line">                })</span><br><span class="line">                <span class="comment"># update generator</span></span><br><span class="line">                z = self.gen.sample(self.batch_size)</span><br><span class="line">                loss_g, _ = session.run([self.loss_g, self.opt_g], {</span><br><span class="line">                    self.z: np.reshape(z, (self.batch_size, <span class="number">1</span>))</span><br><span class="line">                })</span><br><span class="line">                <span class="keyword">if</span> step % self.log_every == <span class="number">0</span>:</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">'{}: {}\t{}'</span>.<span class="built_in">format</span>(step, loss_d, loss_g))</span><br><span class="line">                <span class="keyword">if</span> step % <span class="number">100</span> == <span class="number">0</span> <span class="keyword">or</span> step==<span class="number">0</span> <span class="keyword">or</span> step == self.num_steps -<span class="number">1</span> :</span><br><span class="line">                    self._plot_distributions(session)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_samples</span>(<span class="params">self, session, num_points=<span class="number">10000</span>, num_bins=<span class="number">100</span></span>):</span><br><span class="line">        xs = np.linspace(-self.gen.<span class="built_in">range</span>, self.gen.<span class="built_in">range</span>, num_points)</span><br><span class="line">        bins = np.linspace(-self.gen.<span class="built_in">range</span>, self.gen.<span class="built_in">range</span>, num_bins)</span><br><span class="line">        <span class="comment"># data distribution</span></span><br><span class="line">        d = self.data.sample(num_points)</span><br><span class="line">        pd, _ = np.histogram(d, bins=bins, density=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># generated samples</span></span><br><span class="line">        zs = np.linspace(-self.gen.<span class="built_in">range</span>, self.gen.<span class="built_in">range</span>, num_points)</span><br><span class="line">        g = np.zeros((num_points, <span class="number">1</span>))</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(num_points // self.batch_size):</span><br><span class="line">            g[self.batch_size * i:self.batch_size * (i + <span class="number">1</span>)] = session.run(self.G, {</span><br><span class="line">                self.z: np.reshape(</span><br><span class="line">                    zs[self.batch_size * i:self.batch_size * (i + <span class="number">1</span>)],</span><br><span class="line">                    (self.batch_size, <span class="number">1</span>)</span><br><span class="line">                )</span><br><span class="line">            })</span><br><span class="line">        pg, _ = np.histogram(g, bins=bins, density=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> pd, pg</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_plot_distributions</span>(<span class="params">self, session</span>):</span><br><span class="line">        pd, pg = self._samples(session)</span><br><span class="line">        p_x = np.linspace(-self.gen.<span class="built_in">range</span>, self.gen.<span class="built_in">range</span>, <span class="built_in">len</span>(pd))</span><br><span class="line">        f, ax = plt.subplots(<span class="number">1</span>)</span><br><span class="line">        ax.set_ylim(<span class="number">0</span>, <span class="number">1</span>)</span><br><span class="line">        plt.plot(p_x, pd, label=<span class="string">'real data'</span>)</span><br><span class="line">        plt.plot(p_x, pg, label=<span class="string">'generated data'</span>)</span><br><span class="line">        plt.title(<span class="string">'1D Generative Adversarial Network'</span>)</span><br><span class="line">        plt.xlabel(<span class="string">'Data values'</span>)</span><br><span class="line">        plt.ylabel(<span class="string">'Probability density'</span>)</span><br><span class="line">        plt.legend()</span><br><span class="line">        plt.show()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">args</span>):</span><br><span class="line">    model = GAN(</span><br><span class="line">        DataDistribution(),</span><br><span class="line">        GeneratorDistribution(<span class="built_in">range</span>=<span class="number">8</span>),</span><br><span class="line">        args.num_steps,</span><br><span class="line">        args.batch_size,</span><br><span class="line">        args.log_every,</span><br><span class="line">    )</span><br><span class="line">    model.train()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">parse_args</span>():</span><br><span class="line">    parser = argparse.ArgumentParser()</span><br><span class="line">    parser.add_argument(<span class="string">'--num-steps'</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">12000</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">'the number of training steps to take'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--batch-size'</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">12</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">'the batch size'</span>)</span><br><span class="line">    parser.add_argument(<span class="string">'--log-every'</span>, <span class="built_in">type</span>=<span class="built_in">int</span>, default=<span class="number">10</span>,</span><br><span class="line">                        <span class="built_in">help</span>=<span class="string">'print loss after this many steps'</span>)</span><br><span class="line">    <span class="keyword">return</span> parser.parse_args()</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main(parse_args())</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20191126_GAN_myplot.png" alt="myplot"></p><h4 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h4><ol><li><p>将pooling层convolutions替代 </p><ul><li><p>对于判别模型：容许网络学习自己的空间下采样</p></li><li><p>对于生成模型：容许它学习自己的空间上采样</p></li></ul></li><li><p>在generator和discriminator上都使用batchnorm </p><ul><li>解决初始化差的问题</li><li>帮助梯度传播到每一层</li><li>防止generator把所有的样本都收敛到同一个点。</li></ul></li><li><p>在CNN中移除全连接层</p></li><li><p>在generator的除了输出层外的所有层使用ReLU，输出层采用tanh。</p></li><li><p>在discriminator的所有层上使用LeakyReLU</p></li></ol><p><img src="/images/20191126_GAN_33.png" alt="33"></p><p>100维的向量转为为特征图相似的东西， 再将这个向量reshape 。使用反卷积操作。</p><p><img src="/images/20191126_GAN_a22.png" alt="a22"></p><p>输入图片，得到一个值是0或者1，这个是判别网络</p><p><img src="/images/20191126_GAN_aa1.png" alt="aa1"></p><p>这个是生成网络。<br>model.py  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br><span class="line">282</span><br><span class="line">283</span><br><span class="line">284</span><br><span class="line">285</span><br><span class="line">286</span><br><span class="line">287</span><br><span class="line">288</span><br><span class="line">289</span><br><span class="line">290</span><br><span class="line">291</span><br><span class="line">292</span><br><span class="line">293</span><br><span class="line">294</span><br><span class="line">295</span><br><span class="line">296</span><br><span class="line">297</span><br><span class="line">298</span><br><span class="line">299</span><br><span class="line">300</span><br><span class="line">301</span><br><span class="line">302</span><br><span class="line">303</span><br><span class="line">304</span><br><span class="line">305</span><br><span class="line">306</span><br><span class="line">307</span><br><span class="line">308</span><br><span class="line">309</span><br><span class="line">310</span><br><span class="line">311</span><br><span class="line">312</span><br><span class="line">313</span><br><span class="line">314</span><br><span class="line">315</span><br><span class="line">316</span><br><span class="line">317</span><br><span class="line">318</span><br><span class="line">319</span><br><span class="line">320</span><br><span class="line">321</span><br><span class="line">322</span><br><span class="line">323</span><br><span class="line">324</span><br><span class="line">325</span><br><span class="line">326</span><br><span class="line">327</span><br><span class="line">328</span><br><span class="line">329</span><br><span class="line">330</span><br><span class="line">331</span><br><span class="line">332</span><br><span class="line">333</span><br><span class="line">334</span><br><span class="line">335</span><br><span class="line">336</span><br><span class="line">337</span><br><span class="line">338</span><br><span class="line">339</span><br><span class="line">340</span><br><span class="line">341</span><br><span class="line">342</span><br><span class="line">343</span><br><span class="line">344</span><br><span class="line">345</span><br><span class="line">346</span><br><span class="line">347</span><br><span class="line">348</span><br><span class="line">349</span><br><span class="line">350</span><br><span class="line">351</span><br><span class="line">352</span><br><span class="line">353</span><br><span class="line">354</span><br><span class="line">355</span><br><span class="line">356</span><br><span class="line">357</span><br><span class="line">358</span><br><span class="line">359</span><br><span class="line">360</span><br><span class="line">361</span><br><span class="line">362</span><br><span class="line">363</span><br><span class="line">364</span><br><span class="line">365</span><br><span class="line">366</span><br><span class="line">367</span><br><span class="line">368</span><br><span class="line">369</span><br><span class="line">370</span><br><span class="line">371</span><br><span class="line">372</span><br><span class="line">373</span><br><span class="line">374</span><br><span class="line">375</span><br><span class="line">376</span><br><span class="line">377</span><br><span class="line">378</span><br><span class="line">379</span><br><span class="line">380</span><br><span class="line">381</span><br><span class="line">382</span><br><span class="line">383</span><br><span class="line">384</span><br><span class="line">385</span><br><span class="line">386</span><br><span class="line">387</span><br><span class="line">388</span><br><span class="line">389</span><br><span class="line">390</span><br><span class="line">391</span><br><span class="line">392</span><br><span class="line">393</span><br><span class="line">394</span><br><span class="line">395</span><br><span class="line">396</span><br><span class="line">397</span><br><span class="line">398</span><br><span class="line">399</span><br><span class="line">400</span><br><span class="line">401</span><br><span class="line">402</span><br><span class="line">403</span><br><span class="line">404</span><br><span class="line">405</span><br><span class="line">406</span><br><span class="line">407</span><br><span class="line">408</span><br><span class="line">409</span><br><span class="line">410</span><br><span class="line">411</span><br><span class="line">412</span><br><span class="line">413</span><br><span class="line">414</span><br><span class="line">415</span><br><span class="line">416</span><br><span class="line">417</span><br><span class="line">418</span><br><span class="line">419</span><br><span class="line">420</span><br><span class="line">421</span><br><span class="line">422</span><br><span class="line">423</span><br><span class="line">424</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> glob <span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> six.moves <span class="keyword">import</span> xrange</span><br><span class="line"><span class="keyword">from</span> ops <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">conv_out_size_same</span>(<span class="params">size, stride</span>):</span><br><span class="line">  <span class="keyword">return</span> math.ceil(<span class="built_in">float</span>(size) / <span class="built_in">float</span>(stride))</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DCGAN</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, sess, input_height=<span class="number">108</span>, input_width=<span class="number">108</span>, is_crop=<span class="literal">True</span>,</span></span><br><span class="line"><span class="params">         batch_size=<span class="number">64</span>, sample_num = <span class="number">64</span>, output_height=<span class="number">64</span>, output_width=<span class="number">64</span>,</span></span><br><span class="line"><span class="params">         y_dim=<span class="literal">None</span>, z_dim=<span class="number">100</span>, gf_dim=<span class="number">64</span>, df_dim=<span class="number">64</span>,</span></span><br><span class="line"><span class="params">         gfc_dim=<span class="number">1024</span>, dfc_dim=<span class="number">1024</span>, c_dim=<span class="number">3</span>, dataset_name=<span class="string">'default'</span>,</span></span><br><span class="line"><span class="params">         input_fname_pattern=<span class="string">'*.jpg'</span>, checkpoint_dir=<span class="literal">None</span>, sample_dir=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    # sample number  测试噪音的输出，y代表label</span></span><br><span class="line"><span class="string">    Args:</span></span><br><span class="line"><span class="string">      sess: TensorFlow session</span></span><br><span class="line"><span class="string">      batch_size: The size of batch. Should be specified before training.</span></span><br><span class="line"><span class="string">      y_dim: (optional) Dimension of dim for y. [None]</span></span><br><span class="line"><span class="string">      z_dim: (optional) Dimension of dim for Z. [100]</span></span><br><span class="line"><span class="string">      gf_dim: (optional) Dimension of gen filters in first conv layer. [64]</span></span><br><span class="line"><span class="string">      df_dim: (optional) Dimension of discrim filters in first conv layer. [64]</span></span><br><span class="line"><span class="string">      gfc_dim: (optional) Dimension of gen units for for fully connected layer. [1024]</span></span><br><span class="line"><span class="string">      dfc_dim: (optional) Dimension of discrim units for fully connected layer. [1024]</span></span><br><span class="line"><span class="string">      c_dim: (optional) Dimension of image color. For grayscale input, set to 1. [3]</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    self.sess = sess</span><br><span class="line">    self.is_crop = is_crop</span><br><span class="line">    self.is_grayscale = (c_dim == <span class="number">1</span>)</span><br><span class="line">    self.batch_size = batch_size</span><br><span class="line">    self.sample_num = sample_num</span><br><span class="line">    self.input_height = input_height</span><br><span class="line">    self.input_width = input_width</span><br><span class="line">    self.output_height = output_height</span><br><span class="line">    self.output_width = output_width</span><br><span class="line">    self.y_dim = y_dim <span class="comment"># null</span></span><br><span class="line">    self.z_dim = z_dim <span class="comment"># 噪音点的维度 100</span></span><br><span class="line">    self.gf_dim = gf_dim <span class="comment"># 最终多少个filter的个数 基数</span></span><br><span class="line">    self.df_dim = df_dim <span class="comment"># 64</span></span><br><span class="line">    self.gfc_dim = gfc_dim<span class="comment"># 生成和判别的全连接 1024</span></span><br><span class="line">    self.dfc_dim = dfc_dim <span class="comment"># 1024</span></span><br><span class="line">    self.c_dim = c_dim<span class="comment"># 生成的是彩色图 3</span></span><br><span class="line">    <span class="comment"># batch normalization : deals with poor initialization helps gradient flow</span></span><br><span class="line">    self.d_bn1 = batch_norm(name=<span class="string">'d_bn1'</span>)<span class="comment"># bacth在relu之前卷积之后</span></span><br><span class="line">    self.d_bn2 = batch_norm(name=<span class="string">'d_bn2'</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.y_dim:</span><br><span class="line">      self.d_bn3 = batch_norm(name=<span class="string">'d_bn3'</span>)</span><br><span class="line">    self.g_bn0 = batch_norm(name=<span class="string">'g_bn0'</span>)</span><br><span class="line">    self.g_bn1 = batch_norm(name=<span class="string">'g_bn1'</span>)</span><br><span class="line">    self.g_bn2 = batch_norm(name=<span class="string">'g_bn2'</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> self.y_dim:</span><br><span class="line">      self.g_bn3 = batch_norm(name=<span class="string">'g_bn3'</span>)</span><br><span class="line">    self.dataset_name = dataset_name</span><br><span class="line">    self.input_fname_pattern = input_fname_pattern</span><br><span class="line">    self.checkpoint_dir = checkpoint_dir</span><br><span class="line">    self.build_model()</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">build_model</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">if</span> self.y_dim:</span><br><span class="line">      self.y= tf.placeholder(tf.float32, [self.batch_size, self.y_dim], name=<span class="string">'y'</span>)</span><br><span class="line">    <span class="keyword">if</span> self.is_crop:</span><br><span class="line">      image_dims = [self.output_height, self.output_width, self.c_dim]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      image_dims = [self.input_height, self.input_height, self.c_dim]</span><br><span class="line">    self.inputs = tf.placeholder(</span><br><span class="line">      tf.float32, [self.batch_size] + image_dims, name=<span class="string">'real_images'</span>)</span><br><span class="line">    self.sample_inputs = tf.placeholder(  <span class="comment"># 64 108 108 3，iamge_dim 108 108 3</span></span><br><span class="line">      tf.float32, [self.sample_num] + image_dims, name=<span class="string">'sample_inputs'</span>)</span><br><span class="line">    inputs = self.inputs  <span class="comment"># 64 108 108 3</span></span><br><span class="line">    sample_inputs = self.sample_inputs</span><br><span class="line">    self.z = tf.placeholder(</span><br><span class="line">      tf.float32, [<span class="literal">None</span>, self.z_dim], name=<span class="string">'z'</span>)  <span class="comment">## 生成网络组最开始的输入，float32  # B， 100</span></span><br><span class="line">    self.z_sum = histogram_summary(<span class="string">"z"</span>, self.z)  <span class="comment"># 在训练神经网络时，当需要查看一个张量在训练过程中值的分布情况时，可通过tf.summary.histogram()将其分布情况以直方图的形式在TensorBoard直方图仪表板上显示．</span></span><br><span class="line">    <span class="keyword">if</span> self.y_dim:</span><br><span class="line">      self.G = self.generator(self.z, self.y)</span><br><span class="line">      self.D, self.D_logits = \</span><br><span class="line">          self.discriminator(inputs, self.y, reuse=<span class="literal">False</span>)</span><br><span class="line">      self.sampler = self.sampler(self.z, self.y)</span><br><span class="line">      self.D_, self.D_logits_ = \</span><br><span class="line">          self.discriminator(self.G, self.y, reuse=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      self.G = self.generator(self.z)  <span class="comment"># 64 64 64 3</span></span><br><span class="line">      self.D, self.D_logits = self.discriminator(inputs)  <span class="comment"># 64 108 108 3</span></span><br><span class="line">      self.sampler = self.sampler(self.z)</span><br><span class="line">      self.D_, self.D_logits_ = self.discriminator(self.G, reuse=<span class="literal">True</span>)</span><br><span class="line">    self.d_sum = histogram_summary(<span class="string">"d"</span>, self.D)</span><br><span class="line">    self.d__sum = histogram_summary(<span class="string">"d_"</span>, self.D_)</span><br><span class="line">    self.G_sum = image_summary(<span class="string">"G"</span>, self.G)</span><br><span class="line">    <span class="comment"># tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits,l)</span></span><br><span class="line">    self.d_loss_real = tf.reduce_mean(</span><br><span class="line">      tf.nn.sigmoid_cross_entropy_with_logits(</span><br><span class="line">        logits=self.D_logits, labels=tf.ones_like(self.D)))</span><br><span class="line">    self.d_loss_fake = tf.reduce_mean(</span><br><span class="line">      tf.nn.sigmoid_cross_entropy_with_logits(</span><br><span class="line">        logits=self.D_logits_, labels=tf.zeros_like(self.D_)))</span><br><span class="line">    self.g_loss = tf.reduce_mean(</span><br><span class="line">      tf.nn.sigmoid_cross_entropy_with_logits(</span><br><span class="line">        logits=self.D_logits_, labels=tf.ones_like(self.D_)))</span><br><span class="line">    self.d_loss_real_sum = scalar_summary(<span class="string">"d_loss_real"</span>, self.d_loss_real)</span><br><span class="line">    self.d_loss_fake_sum = scalar_summary(<span class="string">"d_loss_fake"</span>, self.d_loss_fake)</span><br><span class="line">    self.d_loss = self.d_loss_real + self.d_loss_fake</span><br><span class="line">    self.g_loss_sum = scalar_summary(<span class="string">"g_loss"</span>, self.g_loss)</span><br><span class="line">    self.d_loss_sum = scalar_summary(<span class="string">"d_loss"</span>, self.d_loss)</span><br><span class="line">    t_vars = tf.trainable_variables()</span><br><span class="line">    self.d_vars = [var <span class="keyword">for</span> var <span class="keyword">in</span> t_vars <span class="keyword">if</span> <span class="string">'d_'</span> <span class="keyword">in</span> var.name]</span><br><span class="line">    self.g_vars = [var <span class="keyword">for</span> var <span class="keyword">in</span> t_vars <span class="keyword">if</span> <span class="string">'g_'</span> <span class="keyword">in</span> var.name]</span><br><span class="line">    self.saver = tf.train.Saver()</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">self, config</span>):</span><br><span class="line">    <span class="string">"""Train DCGAN"""</span></span><br><span class="line">    <span class="keyword">if</span> config.dataset == <span class="string">'mnist'</span>:</span><br><span class="line">      data_X, data_y = self.load_mnist()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      data = glob(os.path.join(<span class="string">"./data"</span>, config.dataset, self.input_fname_pattern))</span><br><span class="line">    <span class="comment">#np.random.shuffle(data)</span></span><br><span class="line">    d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \</span><br><span class="line">              .minimize(self.d_loss, var_list=self.d_vars)</span><br><span class="line">    g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \</span><br><span class="line">              .minimize(self.g_loss, var_list=self.g_vars)</span><br><span class="line">    <span class="keyword">try</span>:</span><br><span class="line">      tf.global_variables_initializer().run()</span><br><span class="line">    <span class="keyword">except</span>:</span><br><span class="line">      tf.initialize_all_variables().run()</span><br><span class="line">    self.g_sum = merge_summary([self.z_sum, self.d__sum,</span><br><span class="line">      self.G_sum, self.d_loss_fake_sum, self.g_loss_sum])</span><br><span class="line">    self.d_sum = merge_summary(</span><br><span class="line">        [self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum])</span><br><span class="line">    self.writer = SummaryWriter(<span class="string">"./logs"</span>, self.sess.graph)</span><br><span class="line">    sample_z = np.random.uniform(-<span class="number">1</span>, <span class="number">1</span>, size=(self.sample_num , self.z_dim))</span><br><span class="line">    <span class="keyword">if</span> config.dataset == <span class="string">'mnist'</span>:</span><br><span class="line">      sample_inputs = data_X[<span class="number">0</span>:self.sample_num]</span><br><span class="line">      sample_labels = data_y[<span class="number">0</span>:self.sample_num]</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      sample_files = data[<span class="number">0</span>:self.sample_num]</span><br><span class="line">      sample = [</span><br><span class="line">          get_image(sample_file,</span><br><span class="line">                    input_height=self.input_height,</span><br><span class="line">                    input_width=self.input_width,</span><br><span class="line">                    resize_height=self.output_height,</span><br><span class="line">                    resize_width=self.output_width,</span><br><span class="line">                    is_crop=self.is_crop,</span><br><span class="line">                    is_grayscale=self.is_grayscale) <span class="keyword">for</span> sample_file <span class="keyword">in</span> sample_files]</span><br><span class="line">      <span class="keyword">if</span> (self.is_grayscale):</span><br><span class="line">        sample_inputs = np.array(sample).astype(np.float32)[:, :, :, <span class="literal">None</span>]</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        sample_inputs = np.array(sample).astype(np.float32)</span><br><span class="line">    counter = <span class="number">1</span></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    <span class="keyword">if</span> self.load(self.checkpoint_dir):</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">" [*] Load SUCCESS"</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">" [!] Load failed..."</span>)</span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> xrange(config.epoch):</span><br><span class="line">      <span class="keyword">if</span> config.dataset == <span class="string">'mnist'</span>:</span><br><span class="line">        batch_idxs = <span class="built_in">min</span>(<span class="built_in">len</span>(data_X), config.train_size) // config.batch_size</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        data = glob(os.path.join(</span><br><span class="line">          <span class="string">"./data"</span>, config.dataset, self.input_fname_pattern))</span><br><span class="line">        batch_idxs = <span class="built_in">min</span>(<span class="built_in">len</span>(data), config.train_size) // config.batch_size</span><br><span class="line">      <span class="keyword">for</span> idx <span class="keyword">in</span> xrange(<span class="number">0</span>, batch_idxs):</span><br><span class="line">        <span class="keyword">if</span> config.dataset == <span class="string">'mnist'</span>:</span><br><span class="line">          batch_images = data_X[idx*config.batch_size:(idx+<span class="number">1</span>)*config.batch_size]</span><br><span class="line">          batch_labels = data_y[idx*config.batch_size:(idx+<span class="number">1</span>)*config.batch_size]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          batch_files = data[idx*config.batch_size:(idx+<span class="number">1</span>)*config.batch_size]</span><br><span class="line">          batch = [</span><br><span class="line">              get_image(batch_file,</span><br><span class="line">                        input_height=self.input_height,</span><br><span class="line">                        input_width=self.input_width,</span><br><span class="line">                        resize_height=self.output_height,</span><br><span class="line">                        resize_width=self.output_width,</span><br><span class="line">                        is_crop=self.is_crop,</span><br><span class="line">                        is_grayscale=self.is_grayscale) <span class="keyword">for</span> batch_file <span class="keyword">in</span> batch_files]</span><br><span class="line">          <span class="keyword">if</span> (self.is_grayscale):</span><br><span class="line">            batch_images = np.array(batch).astype(np.float32)[:, :, :, <span class="literal">None</span>]</span><br><span class="line">          <span class="keyword">else</span>:</span><br><span class="line">            batch_images = np.array(batch).astype(np.float32)</span><br><span class="line">        <span class="comment">#  一个均匀分布[low,high)中随机采样 从+1和-1之间随才采样</span></span><br><span class="line">        batch_z = np.random.uniform(-<span class="number">1</span>, <span class="number">1</span>, [config.batch_size, self.z_dim]) \</span><br><span class="line">          .astype(np.float32)</span><br><span class="line">        <span class="keyword">if</span> config.dataset == <span class="string">'mnist'</span>:</span><br><span class="line">          <span class="comment"># Update D network</span></span><br><span class="line">          _, summary_str = self.sess.run([d_optim, self.d_sum],</span><br><span class="line">            feed_dict={</span><br><span class="line">              self.inputs: batch_images,</span><br><span class="line">              self.z: batch_z,</span><br><span class="line">              self.y:batch_labels,</span><br><span class="line">            })</span><br><span class="line">          self.writer.add_summary(summary_str, counter)</span><br><span class="line">          <span class="comment"># Update G network</span></span><br><span class="line">          _, summary_str = self.sess.run([g_optim, self.g_sum],</span><br><span class="line">            feed_dict={</span><br><span class="line">              self.z: batch_z,</span><br><span class="line">              self.y:batch_labels,</span><br><span class="line">            })</span><br><span class="line">          self.writer.add_summary(summary_str, counter)</span><br><span class="line">          <span class="comment"># Run g_optim twice to make sure that d_loss does not go to zero (different from paper)</span></span><br><span class="line">          _, summary_str = self.sess.run([g_optim, self.g_sum],</span><br><span class="line">            feed_dict={ self.z: batch_z, self.y:batch_labels })</span><br><span class="line">          self.writer.add_summary(summary_str, counter)</span><br><span class="line">          errD_fake = self.d_loss_fake.<span class="built_in">eval</span>({</span><br><span class="line">              self.z: batch_z,</span><br><span class="line">              self.y:batch_labels</span><br><span class="line">          })</span><br><span class="line">          errD_real = self.d_loss_real.<span class="built_in">eval</span>({</span><br><span class="line">              self.inputs: batch_images,</span><br><span class="line">              self.y:batch_labels</span><br><span class="line">          })</span><br><span class="line">          errG = self.g_loss.<span class="built_in">eval</span>({</span><br><span class="line">              self.z: batch_z,</span><br><span class="line">              self.y: batch_labels</span><br><span class="line">          })</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">          <span class="comment"># Update D network</span></span><br><span class="line">          _, summary_str = self.sess.run([d_optim, self.d_sum],</span><br><span class="line">            feed_dict={ self.inputs: batch_images, self.z: batch_z })</span><br><span class="line">          self.writer.add_summary(summary_str, counter)</span><br><span class="line">          <span class="comment"># Update G network</span></span><br><span class="line">          _, summary_str = self.sess.run([g_optim, self.g_sum],</span><br><span class="line">            feed_dict={ self.z: batch_z })</span><br><span class="line">          self.writer.add_summary(summary_str, counter)</span><br><span class="line">          <span class="comment"># Run g_optim twice to make sure that d_loss does not go to zero (different from paper)</span></span><br><span class="line">          _, summary_str = self.sess.run([g_optim, self.g_sum],</span><br><span class="line">            feed_dict={ self.z: batch_z })</span><br><span class="line">          self.writer.add_summary(summary_str, counter)</span><br><span class="line">          errD_fake = self.d_loss_fake.<span class="built_in">eval</span>({ self.z: batch_z })</span><br><span class="line">          errD_real = self.d_loss_real.<span class="built_in">eval</span>({ self.inputs: batch_images })</span><br><span class="line">          errG = self.g_loss.<span class="built_in">eval</span>({self.z: batch_z})</span><br><span class="line">        counter += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f"</span> \</span><br><span class="line">          % (epoch, idx, batch_idxs,</span><br><span class="line">            time.time() - start_time, errD_fake+errD_real, errG))</span><br><span class="line">        <span class="keyword">if</span> np.mod(counter, <span class="number">100</span>) == <span class="number">1</span>:</span><br><span class="line">          <span class="keyword">if</span> config.dataset == <span class="string">'mnist'</span>:</span><br><span class="line">            samples, d_loss, g_loss = self.sess.run(</span><br><span class="line">              [self.sampler, self.d_loss, self.g_loss],</span><br><span class="line">              feed_dict={</span><br><span class="line">                  self.z: sample_z,</span><br><span class="line">                  self.inputs: sample_inputs,</span><br><span class="line">                  self.y:sample_labels,</span><br><span class="line">              }</span><br><span class="line">            )</span><br><span class="line">            save_images(samples, [<span class="number">8</span>, <span class="number">8</span>],</span><br><span class="line">                  <span class="string">'./{}/train_{:02d}_{:04d}.png'</span>.<span class="built_in">format</span>(config.sample_dir, epoch, idx))</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"[Sample] d_loss: %.8f, g_loss: %.8f"</span> % (d_loss, g_loss))</span><br><span class="line">          <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">              samples, d_loss, g_loss = self.sess.run(</span><br><span class="line">                [self.sampler, self.d_loss, self.g_loss],</span><br><span class="line">                feed_dict={</span><br><span class="line">                    self.z: sample_z,</span><br><span class="line">                    self.inputs: sample_inputs,</span><br><span class="line">                },</span><br><span class="line">              )</span><br><span class="line">              save_images(samples, [<span class="number">8</span>, <span class="number">8</span>],</span><br><span class="line">                    <span class="string">'./{}/train_{:02d}_{:04d}.png'</span>.<span class="built_in">format</span>(config.sample_dir, epoch, idx))</span><br><span class="line">              <span class="built_in">print</span>(<span class="string">"[Sample] d_loss: %.8f, g_loss: %.8f"</span> % (d_loss, g_loss))</span><br><span class="line">            <span class="keyword">except</span>:</span><br><span class="line">              <span class="built_in">print</span>(<span class="string">"one pic error!..."</span>)</span><br><span class="line">        <span class="keyword">if</span> counter//<span class="number">10</span> == <span class="number">2</span>:</span><br><span class="line">          self.save(config.checkpoint_dir, counter)</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">discriminator</span>(<span class="params">self, image, y=<span class="literal">None</span>, reuse=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"discriminator"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">      <span class="keyword">if</span> reuse:</span><br><span class="line">        scope.reuse_variables()</span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> self.y_dim:</span><br><span class="line">        h0 = lrelu(conv2d(image, self.df_dim, name=<span class="string">'d_h0_conv'</span>))</span><br><span class="line">        h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*<span class="number">2</span>, name=<span class="string">'d_h1_conv'</span>)))</span><br><span class="line">        h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*<span class="number">4</span>, name=<span class="string">'d_h2_conv'</span>)))</span><br><span class="line">        h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*<span class="number">8</span>, name=<span class="string">'d_h3_conv'</span>))) <span class="comment"># 64 14 14 256 -&gt;64 7 7 512</span></span><br><span class="line">        aa = tf.reshape(h3, [self.batch_size, -<span class="number">1</span>])</span><br><span class="line">        h4 = linear(aa, <span class="number">1</span>, <span class="string">'d_h3_lin'</span>)</span><br><span class="line">        <span class="keyword">return</span> tf.nn.sigmoid(h4), h4</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        yb = tf.reshape(y, [self.batch_size, <span class="number">1</span>, <span class="number">1</span>, self.y_dim])</span><br><span class="line">        x = conv_cond_concat(image, yb)</span><br><span class="line">        h0 = lrelu(conv2d(x, self.c_dim + self.y_dim, name=<span class="string">'d_h0_conv'</span>))</span><br><span class="line">        h0 = conv_cond_concat(h0, yb)</span><br><span class="line">        h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim + self.y_dim, name=<span class="string">'d_h1_conv'</span>)))</span><br><span class="line">        h1 = tf.reshape(h1, [self.batch_size, -<span class="number">1</span>])</span><br><span class="line">        h1 = concat([h1, y], <span class="number">1</span>)</span><br><span class="line">        h2 = lrelu(self.d_bn2(linear(h1, self.dfc_dim, <span class="string">'d_h2_lin'</span>)))</span><br><span class="line">        h2 = concat([h2, y], <span class="number">1</span>)</span><br><span class="line">        h3 = linear(h2, <span class="number">1</span>, <span class="string">'d_h3_lin'</span>)</span><br><span class="line">        <span class="keyword">return</span> tf.nn.sigmoid(h3), h3</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">generator</span>(<span class="params">self, z, y=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"generator"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> self.y_dim:</span><br><span class="line">        s_h, s_w = self.output_height, self.output_width</span><br><span class="line">        s_h2, s_w2 = conv_out_size_same(s_h, <span class="number">2</span>), conv_out_size_same(s_w, <span class="number">2</span>) <span class="comment"># 先把特征图大小确定出来</span></span><br><span class="line">        s_h4, s_w4 = conv_out_size_same(s_h2, <span class="number">2</span>), conv_out_size_same(s_w2, <span class="number">2</span>)</span><br><span class="line">        s_h8, s_w8 = conv_out_size_same(s_h4, <span class="number">2</span>), conv_out_size_same(s_w4, <span class="number">2</span>)</span><br><span class="line">        s_h16, s_w16 = conv_out_size_same(s_h8, <span class="number">2</span>), conv_out_size_same(s_w8, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># project `z` and reshape</span></span><br><span class="line">        self.z_, self.h0_w, self.h0_b = linear(</span><br><span class="line">            z, self.gf_dim*<span class="number">8</span>*s_h16*s_w16, <span class="string">'g_h0_lin'</span>, with_w=<span class="literal">True</span>)</span><br><span class="line">        self.h0 = tf.reshape(</span><br><span class="line">            self.z_, [-<span class="number">1</span>, s_h16, s_w16, self.gf_dim * <span class="number">8</span>])</span><br><span class="line">        h0 = tf.nn.relu(self.g_bn0(self.h0))</span><br><span class="line">        self.h1, self.h1_w, self.h1_b = deconv2d(</span><br><span class="line">            h0, [self.batch_size, s_h8, s_w8, self.gf_dim*<span class="number">4</span>], name=<span class="string">'g_h1'</span>, with_w=<span class="literal">True</span>)</span><br><span class="line">        h1 = tf.nn.relu(self.g_bn1(self.h1))</span><br><span class="line">        h2, self.h2_w, self.h2_b = deconv2d(</span><br><span class="line">            h1, [self.batch_size, s_h4, s_w4, self.gf_dim*<span class="number">2</span>], name=<span class="string">'g_h2'</span>, with_w=<span class="literal">True</span>)</span><br><span class="line">        h2 = tf.nn.relu(self.g_bn2(h2))</span><br><span class="line">        h3, self.h3_w, self.h3_b = deconv2d(</span><br><span class="line">            h2, [self.batch_size, s_h2, s_w2, self.gf_dim*<span class="number">1</span>], name=<span class="string">'g_h3'</span>, with_w=<span class="literal">True</span>)</span><br><span class="line">        h3 = tf.nn.relu(self.g_bn3(h3))</span><br><span class="line">        h4, self.h4_w, self.h4_b = deconv2d(</span><br><span class="line">            h3, [self.batch_size, s_h, s_w, self.c_dim], name=<span class="string">'g_h4'</span>, with_w=<span class="literal">True</span>)</span><br><span class="line">        <span class="keyword">return</span> tf.nn.tanh(h4)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        s_h, s_w = self.output_height, self.output_width</span><br><span class="line">        s_h2, s_h4 = <span class="built_in">int</span>(s_h/<span class="number">2</span>), <span class="built_in">int</span>(s_h/<span class="number">4</span>)</span><br><span class="line">        s_w2, s_w4 = <span class="built_in">int</span>(s_w/<span class="number">2</span>), <span class="built_in">int</span>(s_w/<span class="number">4</span>)</span><br><span class="line">        <span class="comment"># yb = tf.expand_dims(tf.expand_dims(y, 1),2)</span></span><br><span class="line">        yb = tf.reshape(y, [self.batch_size, <span class="number">1</span>, <span class="number">1</span>, self.y_dim])</span><br><span class="line">        z = concat([z, y], <span class="number">1</span>)</span><br><span class="line">        h0 = tf.nn.relu(</span><br><span class="line">            self.g_bn0(linear(z, self.gfc_dim, <span class="string">'g_h0_lin'</span>)))</span><br><span class="line">        h0 = concat([h0, y], <span class="number">1</span>)</span><br><span class="line">        h1 = tf.nn.relu(self.g_bn1(</span><br><span class="line">            linear(h0, self.gf_dim*<span class="number">2</span>*s_h4*s_w4, <span class="string">'g_h1_lin'</span>)))</span><br><span class="line">        h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * <span class="number">2</span>])</span><br><span class="line">        h1 = conv_cond_concat(h1, yb)</span><br><span class="line">        h2 = tf.nn.relu(self.g_bn2(deconv2d(h1,</span><br><span class="line">            [self.batch_size, s_h2, s_w2, self.gf_dim * <span class="number">2</span>], name=<span class="string">'g_h2'</span>)))</span><br><span class="line">        h2 = conv_cond_concat(h2, yb)</span><br><span class="line">        <span class="keyword">return</span> tf.nn.sigmoid(</span><br><span class="line">            deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name=<span class="string">'g_h3'</span>))</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">sampler</span>(<span class="params">self, z, y=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">"generator"</span>) <span class="keyword">as</span> scope:</span><br><span class="line">      scope.reuse_variables()</span><br><span class="line">      <span class="keyword">if</span> <span class="keyword">not</span> self.y_dim:</span><br><span class="line">        s_h, s_w = self.output_height, self.output_width</span><br><span class="line">        s_h2, s_w2 = conv_out_size_same(s_h, <span class="number">2</span>), conv_out_size_same(s_w, <span class="number">2</span>)</span><br><span class="line">        s_h4, s_w4 = conv_out_size_same(s_h2, <span class="number">2</span>), conv_out_size_same(s_w2, <span class="number">2</span>)</span><br><span class="line">        s_h8, s_w8 = conv_out_size_same(s_h4, <span class="number">2</span>), conv_out_size_same(s_w4, <span class="number">2</span>)</span><br><span class="line">        s_h16, s_w16 = conv_out_size_same(s_h8, <span class="number">2</span>), conv_out_size_same(s_w8, <span class="number">2</span>)</span><br><span class="line">        <span class="comment"># project `z` and reshape</span></span><br><span class="line">        h0 = tf.reshape(</span><br><span class="line">            linear(z, self.gf_dim*<span class="number">8</span>*s_h16*s_w16, <span class="string">'g_h0_lin'</span>),</span><br><span class="line">            [-<span class="number">1</span>, s_h16, s_w16, self.gf_dim * <span class="number">8</span>])</span><br><span class="line">        h0 = tf.nn.relu(self.g_bn0(h0, train=<span class="literal">False</span>))</span><br><span class="line">        h1 = deconv2d(h0, [self.batch_size, s_h8, s_w8, self.gf_dim*<span class="number">4</span>], name=<span class="string">'g_h1'</span>)</span><br><span class="line">        h1 = tf.nn.relu(self.g_bn1(h1, train=<span class="literal">False</span>))</span><br><span class="line">        h2 = deconv2d(h1, [self.batch_size, s_h4, s_w4, self.gf_dim*<span class="number">2</span>], name=<span class="string">'g_h2'</span>)</span><br><span class="line">        h2 = tf.nn.relu(self.g_bn2(h2, train=<span class="literal">False</span>))</span><br><span class="line">        h3 = deconv2d(h2, [self.batch_size, s_h2, s_w2, self.gf_dim*<span class="number">1</span>], name=<span class="string">'g_h3'</span>)</span><br><span class="line">        h3 = tf.nn.relu(self.g_bn3(h3, train=<span class="literal">False</span>))</span><br><span class="line">        h4 = deconv2d(h3, [self.batch_size, s_h, s_w, self.c_dim], name=<span class="string">'g_h4'</span>)</span><br><span class="line">        <span class="keyword">return</span> tf.nn.tanh(h4)</span><br><span class="line">      <span class="keyword">else</span>:</span><br><span class="line">        s_h, s_w = self.output_height, self.output_width</span><br><span class="line">        s_h2, s_h4 = <span class="built_in">int</span>(s_h/<span class="number">2</span>), <span class="built_in">int</span>(s_h/<span class="number">4</span>)</span><br><span class="line">        s_w2, s_w4 = <span class="built_in">int</span>(s_w/<span class="number">2</span>), <span class="built_in">int</span>(s_w/<span class="number">4</span>)</span><br><span class="line">        <span class="comment"># yb = tf.reshape(y, [-1, 1, 1, self.y_dim])</span></span><br><span class="line">        yb = tf.reshape(y, [self.batch_size, <span class="number">1</span>, <span class="number">1</span>, self.y_dim])</span><br><span class="line">        z = concat([z, y], <span class="number">1</span>)</span><br><span class="line">        h0 = tf.nn.relu(self.g_bn0(linear(z, self.gfc_dim, <span class="string">'g_h0_lin'</span>)))</span><br><span class="line">        h0 = concat([h0, y], <span class="number">1</span>)</span><br><span class="line">        h1 = tf.nn.relu(self.g_bn1(</span><br><span class="line">            linear(h0, self.gf_dim*<span class="number">2</span>*s_h4*s_w4, <span class="string">'g_h1_lin'</span>), train=<span class="literal">False</span>))</span><br><span class="line">        h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * <span class="number">2</span>])</span><br><span class="line">        h1 = conv_cond_concat(h1, yb)</span><br><span class="line">        h2 = tf.nn.relu(self.g_bn2(</span><br><span class="line">            deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim * <span class="number">2</span>], name=<span class="string">'g_h2'</span>), train=<span class="literal">False</span>))</span><br><span class="line">        h2 = conv_cond_concat(h2, yb)</span><br><span class="line">        <span class="keyword">return</span> tf.nn.sigmoid(deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name=<span class="string">'g_h3'</span>))</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">load_mnist</span>(<span class="params">self</span>):</span><br><span class="line">    data_dir = os.path.join(<span class="string">"./data"</span>, self.dataset_name)</span><br><span class="line">    fd = <span class="built_in">open</span>(os.path.join(data_dir,<span class="string">'train-images-idx3-ubyte'</span>))</span><br><span class="line">    loaded = np.fromfile(file=fd,dtype=np.uint8)</span><br><span class="line">    trX = loaded[<span class="number">16</span>:].reshape((<span class="number">60000</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)).astype(np.<span class="built_in">float</span>)</span><br><span class="line">    fd = <span class="built_in">open</span>(os.path.join(data_dir,<span class="string">'train-labels-idx1-ubyte'</span>))</span><br><span class="line">    loaded = np.fromfile(file=fd,dtype=np.uint8)</span><br><span class="line">    trY = loaded[<span class="number">8</span>:].reshape((<span class="number">60000</span>)).astype(np.<span class="built_in">float</span>)</span><br><span class="line">    fd = <span class="built_in">open</span>(os.path.join(data_dir,<span class="string">'t10k-images-idx3-ubyte'</span>))</span><br><span class="line">    loaded = np.fromfile(file=fd,dtype=np.uint8)</span><br><span class="line">    teX = loaded[<span class="number">16</span>:].reshape((<span class="number">10000</span>,<span class="number">28</span>,<span class="number">28</span>,<span class="number">1</span>)).astype(np.<span class="built_in">float</span>)</span><br><span class="line">    fd = <span class="built_in">open</span>(os.path.join(data_dir,<span class="string">'t10k-labels-idx1-ubyte'</span>))</span><br><span class="line">    loaded = np.fromfile(file=fd,dtype=np.uint8)</span><br><span class="line">    teY = loaded[<span class="number">8</span>:].reshape((<span class="number">10000</span>)).astype(np.<span class="built_in">float</span>)</span><br><span class="line">    trY = np.asarray(trY)</span><br><span class="line">    teY = np.asarray(teY)</span><br><span class="line">    X = np.concatenate((trX, teX), axis=<span class="number">0</span>)</span><br><span class="line">    y = np.concatenate((trY, teY), axis=<span class="number">0</span>).astype(np.<span class="built_in">int</span>)</span><br><span class="line">    seed = <span class="number">547</span></span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    np.random.shuffle(X)</span><br><span class="line">    np.random.seed(seed)</span><br><span class="line">    np.random.shuffle(y)</span><br><span class="line">    y_vec = np.zeros((<span class="built_in">len</span>(y), self.y_dim), dtype=np.<span class="built_in">float</span>)</span><br><span class="line">    <span class="keyword">for</span> i, label <span class="keyword">in</span> <span class="built_in">enumerate</span>(y):</span><br><span class="line">      y_vec[i,y[i]] = <span class="number">1.0</span></span><br><span class="line">    <span class="keyword">return</span> X/<span class="number">255.</span>,y_vec</span><br><span class="line"><span class="meta">  @property</span></span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">model_dir</span>(<span class="params">self</span>):</span><br><span class="line">    <span class="keyword">return</span> <span class="string">"{}_{}_{}_{}"</span>.<span class="built_in">format</span>(</span><br><span class="line">        self.dataset_name, self.batch_size,</span><br><span class="line">        self.output_height, self.output_width)</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">save</span>(<span class="params">self, checkpoint_dir, step</span>):</span><br><span class="line">    model_name = <span class="string">"DCGAN.model"</span></span><br><span class="line">    checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(checkpoint_dir):</span><br><span class="line">      os.makedirs(checkpoint_dir)</span><br><span class="line">    self.saver.save(self.sess,</span><br><span class="line">            os.path.join(checkpoint_dir, model_name),</span><br><span class="line">            global_step=step)</span><br><span class="line">  <span class="keyword">def</span> <span class="title function_">load</span>(<span class="params">self, checkpoint_dir</span>):</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">" [*] Reading checkpoints..."</span>)</span><br><span class="line">    checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)</span><br><span class="line">    ckpt = tf.train.get_checkpoint_state(checkpoint_dir)</span><br><span class="line">    <span class="keyword">if</span> ckpt <span class="keyword">and</span> ckpt.model_checkpoint_path:</span><br><span class="line">      ckpt_name = os.path.basename(ckpt.model_checkpoint_path)</span><br><span class="line">      self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">" [*] Success to read {}"</span>.<span class="built_in">format</span>(ckpt_name))</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">      <span class="built_in">print</span>(<span class="string">" [*] Failed to find a checkpoint"</span>)</span><br><span class="line">      <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>GAN</title>
      <link href="/2019/11/26/GAN/"/>
      <url>/2019/11/26/GAN/</url>
      
        <content type="html"><![CDATA[<h3 id="对抗生成网络-GAN-Generative-Adversarial-Nets"><a href="#对抗生成网络-GAN-Generative-Adversarial-Nets" class="headerlink" title="对抗生成网络 GAN(Generative Adversarial Nets)"></a>对抗生成网络 GAN(Generative Adversarial Nets)</h3><p><img src="//caius-lu.github.io/2019/11/26/GAN/images/20191126_GAN_%E5%9B%BE%E7%89%871.png" alt="图片1"></p><h3 id="Adversarial-Nets-Framework"><a href="#Adversarial-Nets-Framework" class="headerlink" title="Adversarial Nets Framework"></a><strong>Adversarial Nets Framework</strong></h3><p><img src="//caius-lu.github.io/2019/11/26/GAN/images/20191126_GAN_%E5%9B%BE%E7%89%872.png" alt="图片2"></p><p>生成器与判别器状态相等</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>它做的是去最大化 D的区分度，最小化G和real数据集的数据分布</p><p>判别模型：</p><p>D1 和 D2 相同的，是判别器，G是生成器</p><p>生成模型：</p><p>先训练判别器，在训练生成器。</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160  161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176  177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192  193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208  209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224  225  226  227  228  229  230  </code></pre><p>| </p><pre><code># -*- coding: utf-8 -*-  # @Author: Your name  # @Date:   2019-11-26 09:12:52  # @Last Modified by:   Your name  # @Last Modified time: 2019-11-26 09:12:52  import argparse  import numpy as np  from scipy.stats import norm  import tensorflow as tf  import matplotlib.pyplot as plt  from matplotlib import animation  import seaborn as sns    sns.set(color_codes=True)    seed = 42  np.random.seed(seed)  tf.set_random_seed(seed)      class DataDistribution(object):      def __init__(self):          self.mu = 4          self.sigma = 0.5        def sample(self, N):          samples = np.random.normal(self.mu, self.sigma, N) # 生成高斯分布的概率密度随机数 均值，标准差          samples.sort()          return samples      class GeneratorDistribution(object):      def __init__(self, range):          self.range = range        def sample(self, N):          return np.linspace(-self.range, self.range, N) + \              np.random.random(N) * 0.01  # 生成随机数    # 对线性相乘进行初始化  def linear(input, output_dim, scope=None, stddev=1.0):      norm = tf.random_normal_initializer(stddev=stddev)      const = tf.constant_initializer(0.0)      with tf.variable_scope(scope or 'linear'): # 定义命名空间          w = tf.get_variable('w', [input.get_shape()[1], output_dim], initializer=norm)          b = tf.get_variable('b', [output_dim], initializer=const)          return tf.matmul(input, w) + b    # 生成器  def generator(input, h_dim):      # 这个函数的作用是计算激活函数softplus，即log( exp( features ) + 1)      h0 = tf.nn.softplus(linear(input, h_dim, 'g0'))      h1 = linear(h0, 1, 'g1')      return h1    # 判别器  def discriminator(input, h_dim):      h0 = tf.tanh(linear(input, h_dim * 2, 'd0'))      h1 = tf.tanh(linear(h0, h_dim * 2, 'd1'))      h2 = tf.tanh(linear(h1, h_dim * 2, scope='d2'))        h3 = tf.sigmoid(linear(h2, 1, scope='d3'))      return h3    # 优化器  def optimizer(loss, var_list, initial_learning_rate):      decay = 0.95      num_decay_steps = 150      batch = tf.Variable(0)      learning_rate = tf.train.exponential_decay(          initial_learning_rate,          batch,          num_decay_steps,          decay,          staircase=True      )      optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(          loss,          global_step=batch,          var_list=var_list      )      return optimizer      class GAN(object):      def __init__(self, data, gen, num_steps, batch_size, log_every):          self.data = data          self.gen = gen          self.num_steps = num_steps          self.batch_size = batch_size          self.log_every = log_every          self.mlp_hidden_size = 4          self.learning_rate = 0.03            self._create_model()        def _create_model(self):            with tf.variable_scope('D_pre'):              self.pre_input = tf.placeholder(tf.float32, shape=(self.batch_size, 1))              self.pre_labels = tf.placeholder(tf.float32, shape=(self.batch_size, 1))              D_pre = discriminator(self.pre_input, self.mlp_hidden_size)              self.pre_loss = tf.reduce_mean(tf.square(D_pre - self.pre_labels))              self.pre_opt = optimizer(self.pre_loss, None, self.learning_rate)            # This defines the generator network - it takes samples from a noise          # distribution as input, and passes them through an MLP.          with tf.variable_scope('Gen'):              self.z = tf.placeholder(tf.float32, shape=(self.batch_size, 1))              self.G = generator(self.z, self.mlp_hidden_size)            # The discriminator tries to tell the difference between samples from the          # true data distribution (self.x) and the generated samples (self.z).          #          # Here we create two copies of the discriminator network (that share parameters),          # as you cannot use the same network with different inputs in TensorFlow.          with tf.variable_scope('Disc') as scope:              self.x = tf.placeholder(tf.float32, shape=(self.batch_size, 1))              self.D1 = discriminator(self.x, self.mlp_hidden_size)              scope.reuse_variables()              self.D2 = discriminator(self.G, self.mlp_hidden_size)            # Define the loss for discriminator and generator networks (see the original          # paper for details), and create optimizers for both          self.loss_d = tf.reduce_mean(-tf.log(self.D1) - tf.log(1 - self.D2))          self.loss_g = tf.reduce_mean(-tf.log(self.D2))            self.d_pre_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='D_pre')          self.d_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Disc')          self.g_params = tf.get_collection(tf.GraphKeys.TRAINABLE_VARIABLES, scope='Gen')            self.opt_d = optimizer(self.loss_d, self.d_params, self.learning_rate)          self.opt_g = optimizer(self.loss_g, self.g_params, self.learning_rate)        def train(self):          with tf.Session() as session:              tf.global_variables_initializer().run()                # pretraining discriminator              num_pretrain_steps = 1000              for step in range(num_pretrain_steps):                  d = (np.random.random(self.batch_size) - 0.5) * 10.0                  labels = norm.pdf(d, loc=self.data.mu, scale=self.data.sigma) # norm.pdf:正态概率密度函数                  pretrain_loss, _ = session.run([self.pre_loss, self.pre_opt], {                      self.pre_input: np.reshape(d, (self.batch_size, 1)),                      self.pre_labels: np.reshape(labels, (self.batch_size, 1))                  })              self.weightsD = session.run(self.d_pre_params)              # copy weights from pre-training over to new D network              for i, v in enumerate(self.d_params):                  session.run(v.assign(self.weightsD[i]))                for step in range(self.num_steps):                  # update discriminator                  x = self.data.sample(self.batch_size)                  z = self.gen.sample(self.batch_size)                  loss_d, _ = session.run([self.loss_d, self.opt_d], {                      self.x: np.reshape(x, (self.batch_size, 1)),                      self.z: np.reshape(z, (self.batch_size, 1))                  })                    # update generator                  z = self.gen.sample(self.batch_size)                  loss_g, _ = session.run([self.loss_g, self.opt_g], {                      self.z: np.reshape(z, (self.batch_size, 1))                  })                    if step % self.log_every == 0:                      print('{}: {}\t{}'.format(step, loss_d, loss_g))                  if step % 100 == 0 or step==0 or step == self.num_steps -1 :                      self._plot_distributions(session)        def _samples(self, session, num_points=10000, num_bins=100):          xs = np.linspace(-self.gen.range, self.gen.range, num_points)          bins = np.linspace(-self.gen.range, self.gen.range, num_bins)            # data distribution          d = self.data.sample(num_points)          pd, _ = np.histogram(d, bins=bins, density=True)            # generated samples          zs = np.linspace(-self.gen.range, self.gen.range, num_points)          g = np.zeros((num_points, 1))          for i in range(num_points // self.batch_size):              g[self.batch_size * i:self.batch_size * (i + 1)] = session.run(self.G, {                  self.z: np.reshape(                      zs[self.batch_size * i:self.batch_size * (i + 1)],                      (self.batch_size, 1)                  )              })          pg, _ = np.histogram(g, bins=bins, density=True)            return pd, pg        def _plot_distributions(self, session):          pd, pg = self._samples(session)          p_x = np.linspace(-self.gen.range, self.gen.range, len(pd))          f, ax = plt.subplots(1)          ax.set_ylim(0, 1)          plt.plot(p_x, pd, label='real data')          plt.plot(p_x, pg, label='generated data')          plt.title('1D Generative Adversarial Network')          plt.xlabel('Data values')          plt.ylabel('Probability density')          plt.legend()          plt.show()  def main(args):      model = GAN(          DataDistribution(),          GeneratorDistribution(range=8),          args.num_steps,          args.batch_size,          args.log_every,      )      model.train()      def parse_args():      parser = argparse.ArgumentParser()      parser.add_argument('--num-steps', type=int, default=12000,                          help='the number of training steps to take')      parser.add_argument('--batch-size', type=int, default=12,                          help='the batch size')      parser.add_argument('--log-every', type=int, default=10,                          help='print loss after this many steps')      return parser.parse_args()      if __name__ == '__main__':      main(parse_args())    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2019/11/26/GAN/images/20191126_GAN_myplot.png" alt="myplot"></p><h4 id="DCGAN"><a href="#DCGAN" class="headerlink" title="DCGAN"></a>DCGAN</h4><ol><li><p>将pooling层convolutions替代 </p><ul><li><p>对于判别模型：容许网络学习自己的空间下采样</p></li><li><p>对于生成模型：容许它学习自己的空间上采样</p></li></ul></li><li><p>在generator和discriminator上都使用batchnorm </p><ul><li>解决初始化差的问题</li><li>帮助梯度传播到每一层</li><li>防止generator把所有的样本都收敛到同一个点。</li></ul></li><li><p>在CNN中移除全连接层</p></li><li><p>在generator的除了输出层外的所有层使用ReLU，输出层采用tanh。</p></li><li><p>在discriminator的所有层上使用LeakyReLU</p></li></ol><p><img src="//caius-lu.github.io/2019/11/26/GAN/images/20191126_GAN_33.png" alt="33"></p><p>100维的向量转为为特征图相似的东西， 再将这个向量reshape 。使用反卷积操作。</p><p><img src="//caius-lu.github.io/2019/11/26/GAN/images/20191126_GAN_a22.png" alt="a22"></p><p>输入图片，得到一个值是0或者1，这个是判别网络</p><p><img src="//caius-lu.github.io/2019/11/26/GAN/images/20191126_GAN_aa1.png" alt="aa1"></p><p>这个是生成网络。<br>model.py  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160  161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176  177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192  193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208  209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224  225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240  241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256  257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272  273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288  289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304  305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320  321  322  323  324  325  326  327  328  329  330  331  332  333  334  335  336  337  338  339  340  341  342  343  344  345  346  347  348  349  350  351  352  353  354  355  356  357  358  359  360  361  362  363  364  365  366  367  368  369  370  371  372  373  374  375  376  377  378  379  380  381  382  383  384  385  386  387  388  389  390  391  392  393  394  395  396  397  398  399  400  401  402  403  404  405  406  407  408  409  410  411  412  413  414  415  416  417  418  419  420  421  422  423  424  425  426  427  428  429  430  431  432  433  434  435  436  437  438  439  440  441  442  443  444  445  446  447  448  449  450  451  452  453  454  455  456  457  458  459  460  461  462  463  464  465  466  467  468  469  470  471  472  473  474  475  476  477  478  479  480  481  482  483  484  485  486  487  488  489  490  491  492  493  494  495  496  497  498  499  500  501  502  503  504  505  506  507  508  509  510  511  512  513  514  515  516  517  518  519  520  521  522  523  </code></pre><p>| </p><pre><code>from __future__ import division  import os  import time  import math  from glob import glob  import tensorflow as tf  import numpy as np  from six.moves import xrange    from ops import *  from utils import *    def conv_out_size_same(size, stride):    return math.ceil(float(size) / float(stride))    class DCGAN(object):    def __init__(self, sess, input_height=108, input_width=108, is_crop=True,           batch_size=64, sample_num = 64, output_height=64, output_width=64,           y_dim=None, z_dim=100, gf_dim=64, df_dim=64,           gfc_dim=1024, dfc_dim=1024, c_dim=3, dataset_name='default',           input_fname_pattern='*.jpg', checkpoint_dir=None, sample_dir=None):      """      # sample number  测试噪音的输出，y代表label      Args:        sess: TensorFlow session        batch_size: The size of batch. Should be specified before training.        y_dim: (optional) Dimension of dim for y. [None]        z_dim: (optional) Dimension of dim for Z. [100]        gf_dim: (optional) Dimension of gen filters in first conv layer. [64]        df_dim: (optional) Dimension of discrim filters in first conv layer. [64]        gfc_dim: (optional) Dimension of gen units for for fully connected layer. [1024]        dfc_dim: (optional) Dimension of discrim units for fully connected layer. [1024]        c_dim: (optional) Dimension of image color. For grayscale input, set to 1. [3]      """      self.sess = sess      self.is_crop = is_crop      self.is_grayscale = (c_dim == 1)        self.batch_size = batch_size      self.sample_num = sample_num        self.input_height = input_height      self.input_width = input_width      self.output_height = output_height      self.output_width = output_width        self.y_dim = y_dim # null      self.z_dim = z_dim # 噪音点的维度 100        self.gf_dim = gf_dim # 最终多少个filter的个数 基数      self.df_dim = df_dim # 64        self.gfc_dim = gfc_dim# 生成和判别的全连接 1024      self.dfc_dim = dfc_dim # 1024        self.c_dim = c_dim# 生成的是彩色图 3        # batch normalization : deals with poor initialization helps gradient flow      self.d_bn1 = batch_norm(name='d_bn1')# bacth在relu之前卷积之后      self.d_bn2 = batch_norm(name='d_bn2')        if not self.y_dim:        self.d_bn3 = batch_norm(name='d_bn3')        self.g_bn0 = batch_norm(name='g_bn0')      self.g_bn1 = batch_norm(name='g_bn1')      self.g_bn2 = batch_norm(name='g_bn2')        if not self.y_dim:        self.g_bn3 = batch_norm(name='g_bn3')        self.dataset_name = dataset_name      self.input_fname_pattern = input_fname_pattern      self.checkpoint_dir = checkpoint_dir      self.build_model()      def build_model(self):      if self.y_dim:        self.y= tf.placeholder(tf.float32, [self.batch_size, self.y_dim], name='y')        if self.is_crop:        image_dims = [self.output_height, self.output_width, self.c_dim]      else:        image_dims = [self.input_height, self.input_height, self.c_dim]        self.inputs = tf.placeholder(        tf.float32, [self.batch_size] + image_dims, name='real_images')      self.sample_inputs = tf.placeholder(  # 64 108 108 3，iamge_dim 108 108 3        tf.float32, [self.sample_num] + image_dims, name='sample_inputs')        inputs = self.inputs  # 64 108 108 3      sample_inputs = self.sample_inputs        self.z = tf.placeholder(        tf.float32, [None, self.z_dim], name='z')  ## 生成网络组最开始的输入，float32  # B， 100      self.z_sum = histogram_summary("z", self.z)  # 在训练神经网络时，当需要查看一个张量在训练过程中值的分布情况时，可通过tf.summary.histogram()将其分布情况以直方图的形式在TensorBoard直方图仪表板上显示．        if self.y_dim:        self.G = self.generator(self.z, self.y)        self.D, self.D_logits = \            self.discriminator(inputs, self.y, reuse=False)          self.sampler = self.sampler(self.z, self.y)        self.D_, self.D_logits_ = \            self.discriminator(self.G, self.y, reuse=True)      else:        self.G = self.generator(self.z)  # 64 64 64 3        self.D, self.D_logits = self.discriminator(inputs)  # 64 108 108 3          self.sampler = self.sampler(self.z)        self.D_, self.D_logits_ = self.discriminator(self.G, reuse=True)        self.d_sum = histogram_summary("d", self.D)      self.d__sum = histogram_summary("d_", self.D_)      self.G_sum = image_summary("G", self.G)      # tf.nn.sigmoid_cross_entropy_with_logits(logits=self.D_logits,l)      self.d_loss_real = tf.reduce_mean(        tf.nn.sigmoid_cross_entropy_with_logits(          logits=self.D_logits, labels=tf.ones_like(self.D)))       self.d_loss_fake = tf.reduce_mean(        tf.nn.sigmoid_cross_entropy_with_logits(          logits=self.D_logits_, labels=tf.zeros_like(self.D_)))      self.g_loss = tf.reduce_mean(        tf.nn.sigmoid_cross_entropy_with_logits(          logits=self.D_logits_, labels=tf.ones_like(self.D_)))        self.d_loss_real_sum = scalar_summary("d_loss_real", self.d_loss_real)      self.d_loss_fake_sum = scalar_summary("d_loss_fake", self.d_loss_fake)                                  self.d_loss = self.d_loss_real + self.d_loss_fake        self.g_loss_sum = scalar_summary("g_loss", self.g_loss)      self.d_loss_sum = scalar_summary("d_loss", self.d_loss)        t_vars = tf.trainable_variables()        self.d_vars = [var for var in t_vars if 'd_' in var.name]      self.g_vars = [var for var in t_vars if 'g_' in var.name]        self.saver = tf.train.Saver()      def train(self, config):      """Train DCGAN"""      if config.dataset == 'mnist':        data_X, data_y = self.load_mnist()      else:        data = glob(os.path.join("./data", config.dataset, self.input_fname_pattern))      #np.random.shuffle(data)        d_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \                .minimize(self.d_loss, var_list=self.d_vars)      g_optim = tf.train.AdamOptimizer(config.learning_rate, beta1=config.beta1) \                .minimize(self.g_loss, var_list=self.g_vars)      try:        tf.global_variables_initializer().run()      except:        tf.initialize_all_variables().run()        self.g_sum = merge_summary([self.z_sum, self.d__sum,        self.G_sum, self.d_loss_fake_sum, self.g_loss_sum])      self.d_sum = merge_summary(          [self.z_sum, self.d_sum, self.d_loss_real_sum, self.d_loss_sum])      self.writer = SummaryWriter("./logs", self.sess.graph)        sample_z = np.random.uniform(-1, 1, size=(self.sample_num , self.z_dim))            if config.dataset == 'mnist':        sample_inputs = data_X[0:self.sample_num]        sample_labels = data_y[0:self.sample_num]      else:        sample_files = data[0:self.sample_num]        sample = [            get_image(sample_file,                      input_height=self.input_height,                      input_width=self.input_width,                      resize_height=self.output_height,                      resize_width=self.output_width,                      is_crop=self.is_crop,                      is_grayscale=self.is_grayscale) for sample_file in sample_files]        if (self.is_grayscale):          sample_inputs = np.array(sample).astype(np.float32)[:, :, :, None]        else:          sample_inputs = np.array(sample).astype(np.float32)          counter = 1      start_time = time.time()        if self.load(self.checkpoint_dir):        print(" [*] Load SUCCESS")      else:        print(" [!] Load failed...")        for epoch in xrange(config.epoch):        if config.dataset == 'mnist':          batch_idxs = min(len(data_X), config.train_size) // config.batch_size        else:                data = glob(os.path.join(            "./data", config.dataset, self.input_fname_pattern))          batch_idxs = min(len(data), config.train_size) // config.batch_size          for idx in xrange(0, batch_idxs):          if config.dataset == 'mnist':            batch_images = data_X[idx*config.batch_size:(idx+1)*config.batch_size]            batch_labels = data_y[idx*config.batch_size:(idx+1)*config.batch_size]          else:            batch_files = data[idx*config.batch_size:(idx+1)*config.batch_size]            batch = [                get_image(batch_file,                          input_height=self.input_height,                          input_width=self.input_width,                          resize_height=self.output_height,                          resize_width=self.output_width,                          is_crop=self.is_crop,                          is_grayscale=self.is_grayscale) for batch_file in batch_files]            if (self.is_grayscale):              batch_images = np.array(batch).astype(np.float32)[:, :, :, None]            else:              batch_images = np.array(batch).astype(np.float32)          #  一个均匀分布[low,high)中随机采样 从+1和-1之间随才采样          batch_z = np.random.uniform(-1, 1, [config.batch_size, self.z_dim]) \            .astype(np.float32)            if config.dataset == 'mnist':            # Update D network            _, summary_str = self.sess.run([d_optim, self.d_sum],              feed_dict={                 self.inputs: batch_images,                self.z: batch_z,                self.y:batch_labels,              })            self.writer.add_summary(summary_str, counter)              # Update G network            _, summary_str = self.sess.run([g_optim, self.g_sum],              feed_dict={                self.z: batch_z,                 self.y:batch_labels,              })            self.writer.add_summary(summary_str, counter)              # Run g_optim twice to make sure that d_loss does not go to zero (different from paper)            _, summary_str = self.sess.run([g_optim, self.g_sum],              feed_dict={ self.z: batch_z, self.y:batch_labels })            self.writer.add_summary(summary_str, counter)                        errD_fake = self.d_loss_fake.eval({                self.z: batch_z,                 self.y:batch_labels            })            errD_real = self.d_loss_real.eval({                self.inputs: batch_images,                self.y:batch_labels            })            errG = self.g_loss.eval({                self.z: batch_z,                self.y: batch_labels            })          else:            # Update D network            _, summary_str = self.sess.run([d_optim, self.d_sum],              feed_dict={ self.inputs: batch_images, self.z: batch_z })            self.writer.add_summary(summary_str, counter)              # Update G network            _, summary_str = self.sess.run([g_optim, self.g_sum],              feed_dict={ self.z: batch_z })            self.writer.add_summary(summary_str, counter)              # Run g_optim twice to make sure that d_loss does not go to zero (different from paper)            _, summary_str = self.sess.run([g_optim, self.g_sum],              feed_dict={ self.z: batch_z })            self.writer.add_summary(summary_str, counter)                        errD_fake = self.d_loss_fake.eval({ self.z: batch_z })            errD_real = self.d_loss_real.eval({ self.inputs: batch_images })            errG = self.g_loss.eval({self.z: batch_z})            counter += 1          print("Epoch: [%2d] [%4d/%4d] time: %4.4f, d_loss: %.8f, g_loss: %.8f" \            % (epoch, idx, batch_idxs,              time.time() - start_time, errD_fake+errD_real, errG))            if np.mod(counter, 100) == 1:            if config.dataset == 'mnist':              samples, d_loss, g_loss = self.sess.run(                [self.sampler, self.d_loss, self.g_loss],                feed_dict={                    self.z: sample_z,                    self.inputs: sample_inputs,                    self.y:sample_labels,                }              )              save_images(samples, [8, 8],                    './{}/train_{:02d}_{:04d}.png'.format(config.sample_dir, epoch, idx))              print("[Sample] d_loss: %.8f, g_loss: %.8f" % (d_loss, g_loss))             else:              try:                samples, d_loss, g_loss = self.sess.run(                  [self.sampler, self.d_loss, self.g_loss],                  feed_dict={                      self.z: sample_z,                      self.inputs: sample_inputs,                  },                )                save_images(samples, [8, 8],                      './{}/train_{:02d}_{:04d}.png'.format(config.sample_dir, epoch, idx))                print("[Sample] d_loss: %.8f, g_loss: %.8f" % (d_loss, g_loss))               except:                print("one pic error!...")            if counter//10 == 2:            self.save(config.checkpoint_dir, counter)      def discriminator(self, image, y=None, reuse=False):      with tf.variable_scope("discriminator") as scope:        if reuse:          scope.reuse_variables()          if not self.y_dim:          h0 = lrelu(conv2d(image, self.df_dim, name='d_h0_conv'))          h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim*2, name='d_h1_conv')))          h2 = lrelu(self.d_bn2(conv2d(h1, self.df_dim*4, name='d_h2_conv')))          h3 = lrelu(self.d_bn3(conv2d(h2, self.df_dim*8, name='d_h3_conv'))) # 64 14 14 256 -&gt;64 7 7 512          aa = tf.reshape(h3, [self.batch_size, -1])          h4 = linear(aa, 1, 'd_h3_lin')            return tf.nn.sigmoid(h4), h4        else:          yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])          x = conv_cond_concat(image, yb)            h0 = lrelu(conv2d(x, self.c_dim + self.y_dim, name='d_h0_conv'))          h0 = conv_cond_concat(h0, yb)            h1 = lrelu(self.d_bn1(conv2d(h0, self.df_dim + self.y_dim, name='d_h1_conv')))          h1 = tf.reshape(h1, [self.batch_size, -1])                h1 = concat([h1, y], 1)                    h2 = lrelu(self.d_bn2(linear(h1, self.dfc_dim, 'd_h2_lin')))          h2 = concat([h2, y], 1)            h3 = linear(h2, 1, 'd_h3_lin')                    return tf.nn.sigmoid(h3), h3      def generator(self, z, y=None):      with tf.variable_scope("generator") as scope:        if not self.y_dim:          s_h, s_w = self.output_height, self.output_width          s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2) # 先把特征图大小确定出来          s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2)          s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2)          s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2)            # project `z` and reshape          self.z_, self.h0_w, self.h0_b = linear(              z, self.gf_dim*8*s_h16*s_w16, 'g_h0_lin', with_w=True)            self.h0 = tf.reshape(              self.z_, [-1, s_h16, s_w16, self.gf_dim * 8])          h0 = tf.nn.relu(self.g_bn0(self.h0))            self.h1, self.h1_w, self.h1_b = deconv2d(              h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name='g_h1', with_w=True)          h1 = tf.nn.relu(self.g_bn1(self.h1))            h2, self.h2_w, self.h2_b = deconv2d(              h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name='g_h2', with_w=True)          h2 = tf.nn.relu(self.g_bn2(h2))            h3, self.h3_w, self.h3_b = deconv2d(              h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name='g_h3', with_w=True)          h3 = tf.nn.relu(self.g_bn3(h3))            h4, self.h4_w, self.h4_b = deconv2d(              h3, [self.batch_size, s_h, s_w, self.c_dim], name='g_h4', with_w=True)            return tf.nn.tanh(h4)        else:          s_h, s_w = self.output_height, self.output_width          s_h2, s_h4 = int(s_h/2), int(s_h/4)          s_w2, s_w4 = int(s_w/2), int(s_w/4)            # yb = tf.expand_dims(tf.expand_dims(y, 1),2)          yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])          z = concat([z, y], 1)            h0 = tf.nn.relu(              self.g_bn0(linear(z, self.gfc_dim, 'g_h0_lin')))          h0 = concat([h0, y], 1)            h1 = tf.nn.relu(self.g_bn1(              linear(h0, self.gf_dim*2*s_h4*s_w4, 'g_h1_lin')))          h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2])            h1 = conv_cond_concat(h1, yb)            h2 = tf.nn.relu(self.g_bn2(deconv2d(h1,              [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name='g_h2')))          h2 = conv_cond_concat(h2, yb)            return tf.nn.sigmoid(              deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name='g_h3'))      def sampler(self, z, y=None):      with tf.variable_scope("generator") as scope:        scope.reuse_variables()          if not self.y_dim:          s_h, s_w = self.output_height, self.output_width          s_h2, s_w2 = conv_out_size_same(s_h, 2), conv_out_size_same(s_w, 2)          s_h4, s_w4 = conv_out_size_same(s_h2, 2), conv_out_size_same(s_w2, 2)          s_h8, s_w8 = conv_out_size_same(s_h4, 2), conv_out_size_same(s_w4, 2)          s_h16, s_w16 = conv_out_size_same(s_h8, 2), conv_out_size_same(s_w8, 2)            # project `z` and reshape          h0 = tf.reshape(              linear(z, self.gf_dim*8*s_h16*s_w16, 'g_h0_lin'),              [-1, s_h16, s_w16, self.gf_dim * 8])          h0 = tf.nn.relu(self.g_bn0(h0, train=False))            h1 = deconv2d(h0, [self.batch_size, s_h8, s_w8, self.gf_dim*4], name='g_h1')          h1 = tf.nn.relu(self.g_bn1(h1, train=False))            h2 = deconv2d(h1, [self.batch_size, s_h4, s_w4, self.gf_dim*2], name='g_h2')          h2 = tf.nn.relu(self.g_bn2(h2, train=False))            h3 = deconv2d(h2, [self.batch_size, s_h2, s_w2, self.gf_dim*1], name='g_h3')          h3 = tf.nn.relu(self.g_bn3(h3, train=False))            h4 = deconv2d(h3, [self.batch_size, s_h, s_w, self.c_dim], name='g_h4')            return tf.nn.tanh(h4)        else:          s_h, s_w = self.output_height, self.output_width          s_h2, s_h4 = int(s_h/2), int(s_h/4)          s_w2, s_w4 = int(s_w/2), int(s_w/4)            # yb = tf.reshape(y, [-1, 1, 1, self.y_dim])          yb = tf.reshape(y, [self.batch_size, 1, 1, self.y_dim])          z = concat([z, y], 1)            h0 = tf.nn.relu(self.g_bn0(linear(z, self.gfc_dim, 'g_h0_lin')))          h0 = concat([h0, y], 1)            h1 = tf.nn.relu(self.g_bn1(              linear(h0, self.gf_dim*2*s_h4*s_w4, 'g_h1_lin'), train=False))          h1 = tf.reshape(h1, [self.batch_size, s_h4, s_w4, self.gf_dim * 2])          h1 = conv_cond_concat(h1, yb)            h2 = tf.nn.relu(self.g_bn2(              deconv2d(h1, [self.batch_size, s_h2, s_w2, self.gf_dim * 2], name='g_h2'), train=False))          h2 = conv_cond_concat(h2, yb)            return tf.nn.sigmoid(deconv2d(h2, [self.batch_size, s_h, s_w, self.c_dim], name='g_h3'))      def load_mnist(self):      data_dir = os.path.join("./data", self.dataset_name)            fd = open(os.path.join(data_dir,'train-images-idx3-ubyte'))      loaded = np.fromfile(file=fd,dtype=np.uint8)      trX = loaded[16:].reshape((60000,28,28,1)).astype(np.float)        fd = open(os.path.join(data_dir,'train-labels-idx1-ubyte'))      loaded = np.fromfile(file=fd,dtype=np.uint8)      trY = loaded[8:].reshape((60000)).astype(np.float)        fd = open(os.path.join(data_dir,'t10k-images-idx3-ubyte'))      loaded = np.fromfile(file=fd,dtype=np.uint8)      teX = loaded[16:].reshape((10000,28,28,1)).astype(np.float)        fd = open(os.path.join(data_dir,'t10k-labels-idx1-ubyte'))      loaded = np.fromfile(file=fd,dtype=np.uint8)      teY = loaded[8:].reshape((10000)).astype(np.float)        trY = np.asarray(trY)      teY = np.asarray(teY)            X = np.concatenate((trX, teX), axis=0)      y = np.concatenate((trY, teY), axis=0).astype(np.int)            seed = 547      np.random.seed(seed)      np.random.shuffle(X)      np.random.seed(seed)      np.random.shuffle(y)            y_vec = np.zeros((len(y), self.y_dim), dtype=np.float)      for i, label in enumerate(y):        y_vec[i,y[i]] = 1.0            return X/255.,y_vec      @property    def model_dir(self):      return "{}_{}_{}_{}".format(          self.dataset_name, self.batch_size,          self.output_height, self.output_width)            def save(self, checkpoint_dir, step):      model_name = "DCGAN.model"      checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)        if not os.path.exists(checkpoint_dir):        os.makedirs(checkpoint_dir)        self.saver.save(self.sess,              os.path.join(checkpoint_dir, model_name),              global_step=step)      def load(self, checkpoint_dir):      print(" [*] Reading checkpoints...")      checkpoint_dir = os.path.join(checkpoint_dir, self.model_dir)        ckpt = tf.train.get_checkpoint_state(checkpoint_dir)      if ckpt and ckpt.model_checkpoint_path:        ckpt_name = os.path.basename(ckpt.model_checkpoint_path)        self.saver.restore(self.sess, os.path.join(checkpoint_dir, ckpt_name))        print(" [*] Success to read {}".format(ckpt_name))        return True      else:        print(" [*] Failed to find a checkpoint")        return False    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DFS_BFS</title>
      <link href="/2019/11/17/20191117-DFS-BFS/"/>
      <url>/2019/11/17/20191117-DFS-BFS/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time : 2019/11/17 10:28</span></span><br><span class="line"><span class="comment"># @Author : caius</span></span><br><span class="line"><span class="comment"># @Site :</span></span><br><span class="line"><span class="comment"># @File : BFS.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line">graph = {</span><br><span class="line">    <span class="string">"A"</span>:{<span class="string">"B"</span>, <span class="string">"C"</span>},</span><br><span class="line">    <span class="string">"B"</span>:{<span class="string">"A"</span>,<span class="string">"C"</span>,<span class="string">"D"</span>},</span><br><span class="line">    <span class="string">"C"</span>:{<span class="string">"A"</span>,<span class="string">"B"</span>,<span class="string">"D"</span>,<span class="string">"E"</span>},</span><br><span class="line">    <span class="string">"D"</span>:{<span class="string">"B"</span>,<span class="string">"C"</span>,<span class="string">"E"</span>,<span class="string">"F"</span>},</span><br><span class="line">    <span class="string">"E"</span>:{<span class="string">"C"</span>,<span class="string">"D"</span>},</span><br><span class="line">    <span class="string">"F"</span>:{<span class="string">"D"</span>}</span><br><span class="line">}</span><br><span class="line"><span class="comment"># 字典的基本用法</span></span><br><span class="line"><span class="comment"># keys: A B C D E F</span></span><br><span class="line"><span class="comment"># graph["E"} "c". "D"</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">BFS</span>(<span class="params">graph, s</span>):</span><br><span class="line">    <span class="comment"># 队列先进先出</span></span><br><span class="line">    queue=[]</span><br><span class="line">    queue.append(s)</span><br><span class="line">    seen = <span class="built_in">set</span>()<span class="comment"># 代表这个东西是个set</span></span><br><span class="line">    seen.add(s)</span><br><span class="line">    parrent ={}</span><br><span class="line">    parrent={s:<span class="literal">None</span>}</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">len</span>(queue)&gt;<span class="number">0</span>):</span><br><span class="line">        vertex = queue.pop(<span class="number">0</span>)</span><br><span class="line">        nodes = graph[vertex]</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> nodes:</span><br><span class="line">            <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> seen:</span><br><span class="line">                queue.append(w)</span><br><span class="line">                seen.add(w)</span><br><span class="line">                parrent[w] = vertex</span><br><span class="line">        <span class="built_in">print</span>(vertex)</span><br><span class="line">    <span class="keyword">return</span> parrent</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">DFS</span>(<span class="params">graph, s</span>):</span><br><span class="line">    <span class="comment"># 队列先进先出</span></span><br><span class="line">    stack=[]</span><br><span class="line">    stack.append(s)</span><br><span class="line">    seen = <span class="built_in">set</span>()<span class="comment"># 代表这个东西是个set</span></span><br><span class="line">    seen.add(s)</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">len</span>(stack)&gt;<span class="number">0</span>):</span><br><span class="line">        vertex = stack.pop()</span><br><span class="line">        nodes = graph[vertex]</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> nodes:</span><br><span class="line">            <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> seen:</span><br><span class="line">                stack.append(w)</span><br><span class="line">                seen.add(w)</span><br><span class="line">        <span class="built_in">print</span>(vertex)</span><br><span class="line">DFS(graph,<span class="string">"E"</span>)</span><br><span class="line">parrent = BFS(graph,<span class="string">'E'</span>)</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> parrent:</span><br><span class="line">    <span class="built_in">print</span>(key,parrent[key])</span><br><span class="line">v = <span class="string">"B"</span></span><br><span class="line">count=-<span class="number">1</span></span><br><span class="line"><span class="keyword">while</span> v!= <span class="literal">None</span>:</span><br><span class="line">    <span class="built_in">print</span>(v)</span><br><span class="line">    v = parrent[v]</span><br><span class="line">    count+=<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"count: {} 次"</span>.<span class="built_in">format</span>(count))</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Duplicate</title>
      <link href="/2019/11/17/20191117-Duplicate/"/>
      <url>/2019/11/17/20191117-Duplicate/</url>
      
        <content type="html"><![CDATA[<p>是否存在相同元素，python3用字典的方式解决  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">containsDuplicate</span>(<span class="params">self, nums</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        :type nums: List[int]</span></span><br><span class="line"><span class="string">        :rtype: bool</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># 方法3：数字存字典</span></span><br><span class="line">        dic = {}</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> nums:</span><br><span class="line">            dic[i] = dic.get(i, <span class="number">0</span>) + <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> dic[i] &gt; <span class="number">1</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><p>最长回文子串  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">longestPalindrome</span>(<span class="params">self, s: <span class="built_in">str</span></span>) -&gt; <span class="built_in">str</span>:</span><br><span class="line">        <span class="comment"># 两种判断条件</span></span><br><span class="line">        <span class="comment"># DP 动态规划</span></span><br><span class="line">        <span class="comment"># CABAC</span></span><br><span class="line">        <span class="comment"># B</span></span><br><span class="line">        <span class="comment"># ABA</span></span><br><span class="line">        <span class="comment"># CABAC</span></span><br><span class="line">        palindrome = <span class="string">''</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(s)):</span><br><span class="line">            aa1 = self.getlongestpalindrome(s,i,i)</span><br><span class="line">            len1 = <span class="built_in">len</span>(aa1)</span><br><span class="line">            <span class="keyword">if</span> len1&gt;<span class="built_in">len</span>(palindrome):</span><br><span class="line">                palindrome = aa1</span><br><span class="line">            aa2 = self.getlongestpalindrome(s,i,i+<span class="number">1</span>)</span><br><span class="line">            len2 = <span class="built_in">len</span>(aa2)</span><br><span class="line">            <span class="keyword">if</span> len2&gt;<span class="built_in">len</span>(palindrome):</span><br><span class="line">                palindrome = aa2</span><br><span class="line">        <span class="keyword">return</span> palindrome</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">getlongestpalindrome</span>(<span class="params">self, s, l, r</span>):</span><br><span class="line">        <span class="keyword">while</span> l &gt;= <span class="number">0</span> <span class="keyword">and</span> r&lt;<span class="built_in">len</span>(s) <span class="keyword">and</span> s[l]==s[r]:</span><br><span class="line">            l -= <span class="number">1</span></span><br><span class="line">            r += <span class="number">1</span></span><br><span class="line">        <span class="keyword">return</span> s[l+<span class="number">1</span>:r]</span><br><span class="line">~</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">containsNearbyDuplicate</span>(<span class="params">self, nums: <span class="type">List</span>[<span class="built_in">int</span>], k: <span class="built_in">int</span></span>) -&gt; <span class="built_in">bool</span>:</span><br><span class="line">        <span class="comment"># 用dictionary来做比较容易一点</span></span><br><span class="line">        lookup = {}</span><br><span class="line">        <span class="keyword">for</span> i , num <span class="keyword">in</span> <span class="built_in">enumerate</span>(nums):</span><br><span class="line">            <span class="keyword">if</span> num <span class="keyword">not</span> <span class="keyword">in</span> lookup:</span><br><span class="line">                lookup[num] = i <span class="comment"># 将num存到dic里面</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">if</span> i-lookup[num]&lt;=k:</span><br><span class="line">                    <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">                lookup[num] = i</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">~</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>dynamic planning</title>
      <link href="/2019/11/17/20191117-dynamic/"/>
      <url>/2019/11/17/20191117-dynamic/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">arr= [<span class="number">1</span>,<span class="number">2</span>,<span class="number">4</span>,<span class="number">1</span>,<span class="number">7</span>,<span class="number">8</span>,<span class="number">3</span>]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rec_opt</span>(<span class="params">arr,i</span>):</span><br><span class="line">    <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> arr[<span class="number">0</span>]</span><br><span class="line">    <span class="keyword">elif</span> i == <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">max</span>(arr[<span class="number">0</span>],arr[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        A = rec_opt(arr,i-<span class="number">2</span>)+arr[i]</span><br><span class="line">        B = rec_opt(arr,i-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">max</span>(A,B)</span><br><span class="line"><span class="built_in">print</span>(rec_opt(arr,<span class="number">6</span>))</span><br></pre></td></tr></tbody></table></figure><h3 id="递归会产生很多的重叠的子问题，运算规模2-n"><a href="#递归会产生很多的重叠的子问题，运算规模2-n" class="headerlink" title="递归会产生很多的重叠的子问题，运算规模2^n"></a>递归会产生很多的重叠的子问题，运算规模2^n</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#非递归的方法</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dp_opt</span>(<span class="params">arr</span>):</span><br><span class="line">    opt = np.zeros(<span class="built_in">len</span>(arr))</span><br><span class="line">    opt[<span class="number">0</span>] = arr[<span class="number">0</span>]</span><br><span class="line">    opt[<span class="number">1</span>] = <span class="built_in">max</span>(arr[<span class="number">0</span>], arr[<span class="number">1</span>])</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, <span class="built_in">len</span>(arr)):</span><br><span class="line">        A = opt[i-<span class="number">2</span>] +arr[i]</span><br><span class="line">        B = opt[i-<span class="number">1</span>]</span><br><span class="line">        opt[i] = <span class="built_in">max</span>(A,B)</span><br><span class="line">    <span class="keyword">return</span> opt[<span class="built_in">len</span>(arr)-<span class="number">1</span>]</span><br><span class="line">dp_opt(arr)</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">arr = [<span class="number">3</span>,<span class="number">34</span>,<span class="number">4</span>,<span class="number">12</span>,<span class="number">5</span>,<span class="number">2</span>]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">rec_subset</span>(<span class="params">arr, i,s</span>):</span><br><span class="line">    <span class="keyword">if</span> s == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">elif</span> i == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> arr[<span class="number">0</span>] == s</span><br><span class="line">    <span class="keyword">elif</span> arr[i]&gt;s:</span><br><span class="line">        <span class="keyword">return</span> rec_subset(arr,i-<span class="number">1</span>,s)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        A= rec_subset(arr,i-<span class="number">1</span>, s-arr[i])</span><br><span class="line">        B = rec_subset(arr, i-<span class="number">1</span>,s)</span><br><span class="line">        <span class="keyword">return</span> A <span class="keyword">or</span> B</span><br><span class="line">rec_subset(arr, <span class="built_in">len</span>(arr)-<span class="number">1</span>,<span class="number">9</span>)</span><br><span class="line">~</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 非递归的方法，用二维数组来保存</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dp_subset</span>(<span class="params">arr,s</span>):</span><br><span class="line">    subset = np.zeros((<span class="built_in">len</span>(arr),s+<span class="number">1</span>),dtype=<span class="built_in">bool</span>)</span><br><span class="line">    subset[:,<span class="number">0</span>] = <span class="literal">True</span></span><br><span class="line">    subset[<span class="number">0</span>,:] = Fasle</span><br><span class="line">    subset[<span class="number">0</span>,arr[<span class="number">0</span>]] = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,<span class="built_in">len</span>(arr)):</span><br><span class="line">        <span class="keyword">for</span> ss <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1</span>,s+<span class="number">1</span>):</span><br><span class="line">            <span class="keyword">if</span> arr[i]&gt; ss:</span><br><span class="line">                subset[i,ss] = subset[i-<span class="number">1</span>,ss]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                A= subset(i-<span class="number">1</span>, s-arr[i])</span><br><span class="line">                B = subset(i-<span class="number">1</span>,s)</span><br><span class="line">                subset[i,s] = A <span class="keyword">or</span> B</span><br><span class="line">    r ,c  =subset.shape</span><br><span class="line">    <span class="keyword">return</span> subset[r-<span class="number">1</span>,c-<span class="number">1</span>]</span><br><span class="line">rec_subset(arr, <span class="built_in">len</span>(arr)-<span class="number">1</span>,<span class="number">9</span>)</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rnn</title>
      <link href="/2019/11/17/20191117-rnn/"/>
      <url>/2019/11/17/20191117-rnn/</url>
      
        <content type="html"><![CDATA[<h3 id="RNN及RNN的几个变体"><a href="#RNN及RNN的几个变体" class="headerlink" title="RNN及RNN的几个变体"></a>RNN及RNN的几个变体</h3><p>序列形的数据不太好用原始的神经网络处理，为了建模序列问题，RNN引入了隐状态h（hidden）的概念，h可以对序列形的数据提取特征，接着转换为输出。</p><p><img src="/images/20191117_rnn_h1.jpg" alt="h1"></p><ul><li><p>圆圈或方块表示的是向量</p></li><li><p>一个箭头就表示对该向量做一次变换。如上图中h0和x1分别有一个箭头连接，就表示对h0和x1各做了一次变换</p></li></ul><p><strong>Tips：U、W、b都是一样的，每个步骤的参数都是共享的，这是RNN的重要特点。</strong></p><p><img src="/images/20191117_rnn_h2.jpg" alt="h2"><br>依次计算剩下来的（使用相同的参数U、W、b）：<br><img src="/images/20191117_rnn_h3.jpg" alt="h3"><br>输出值的方法就是直接通过h进行计算：<br><img src="/images/20191117_rnn_h4.jpg" alt="h4"><br><strong>一个箭头就表示对对应的向量做一次类似于f(Wx+b)的变换，这里的这个箭头就表示对h1进行一次变换，得到输出y1</strong><br><img src="/images/20191117_rnn_h5.jpg" alt="h5"><br>输入是x1, x2, …..xn，输出为y1, y2, …yn，也就是说，<strong>输入和输出序列必须要是等长的。</strong><br>一些问题适合用经典的RNN结构建模，如：</p><ul><li>计算视频中每一帧的分类标签。因为要对每一帧进行计算，因此输入和输出序列等长。</li><li>输入为字符，输出为下一个字符的概率。这就是著名的Char RNN（详细介绍请参考：The Unreasonable Effectiveness of Recurrent Neural Networks）。</li></ul><h3 id="N-vs-1-多输入单输出"><a href="#N-vs-1-多输入单输出" class="headerlink" title="N vs 1 多输入单输出"></a>N vs 1 多输入单输出</h3><p><img src="/images/20191117_rnn_h6.jpg" alt="h6"><br>这种结构通常用来处理序列分类问题。如输入一段文字判别它所属的类别，输入一个句子判断其情感倾向，输入一段视频并判断它的类别等等。</p><h3 id="1-VS-N-单输入多输出"><a href="#1-VS-N-单输入多输出" class="headerlink" title="1 VS N 单输入多输出"></a>1 VS N 单输入多输出</h3><p>输入不是序列而输出为序列的情况怎么处理？我们可以只在序列开始进行输入计算：<br><img src="/images/20191117_rnn_h7.jpg" alt="h7"><br>还有一种结构是把输入x作为每个阶段的输入：<br><img src="/images/20191117_rnn_h8.jpg" alt="h8"><br>这种结构处理的问题：</p><ul><li>从图像生成文字（image caption），此时输入的X就是图像的特征，而输出的y序列就是一段句子</li><li>从类别生成语音或音乐等</li></ul><h3 id="N-vs-M"><a href="#N-vs-M" class="headerlink" title="N vs M"></a>N vs M</h3><p>RNN最重要的一个变种：N vs M。这种结构又叫Encoder-Decoder模型，也可以称之为Seq2Seq模型。<br>为此，Encoder-Decoder结构先将输入数据编码成一个上下文向量c：<br><img src="/images/20191117_rnn_h9.jpg" alt="h9"><br>c有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给c，还可以对最后的隐状态做一个变换得到c，也可以对所有的隐状态做变换。</p><p>拿到c之后，就用另一个RNN网络对其进行解码，这部分RNN网络被称为Decoder。具体做法就是将c当做之前的初始状态h0输入到Decoder中：<br><img src="/images/20191117_rnn_h10.jpg" alt="h10"><br>还有一种做法是将c当做每一步的输入：<br><img src="//caius-lu.github.io/2019/11/17/rnn/h11.jpg" alt="h11"><br>Encoder-Decoder结构不限制输入和输出的序列长度，因此应用的范围非常广泛，比如：</p><ul><li>机器翻译。Encoder-Decoder的最经典应用，事实上这一结构就是在机器翻译领域最先提出的</li><li>文本摘要。输入是一段文本序列，输出是这段文本序列的摘要序列</li><li>阅读理解。将输入的文章和问题分别编码，再对其进行解码得到问题的答案。</li><li>语音识别。输入是语音信号序列，输出是文字序列。</li><li>………</li></ul><h2 id="tensorflow-实现RNN"><a href="#tensorflow-实现RNN" class="headerlink" title="tensorflow 实现RNN"></a>tensorflow 实现RNN</h2><h3 id="RNNCell"><a href="#RNNCell" class="headerlink" title="RNNCell"></a>RNNCell</h3><p>RNNCell 是TensorFlow中实现RNN的基本单元，每个RNNCell都有一个call方法，使用方式是：(output, next_state) = call(input, state)<br>借助图片来说可能更容易理解。假设我们有一个初始状态h0，还有输入x1，调用call(x1, h0)后就可以得到(output1, h1)：<br><img src="/images/20191117_rnn_h12.jpg" alt="h12"><br>再调用一次call(x2, h1)就可以得到(output2, h2)：<br><img src="/images/20191117_rnn_h13.jpg" alt="h13"><br>也就是说，每调用一次RNNCell的call方法，就相当于在时间上“推进了一步”，这就是RNNCell的基本功能。<br>RNNCell只是一个抽象类，其他的RNNcell都会继承该方法，然后具体实现其中的call()函数。主要有state_size和output_size两个属性，分别代表了隐藏层和输出层的维度。然后就是zero_state()和call()两个函数，分别用于初始化初始状态h0为全零向量和定义实际的RNNCell的操作（比如RNN就是一个激活，GRU的两个门，LSTM的三个门控等，不同的RNN的区别主要体现在这个函数）。</p><h4 id="BasicRNNCell"><a href="#BasicRNNCell" class="headerlink" title="BasicRNNCell"></a>BasicRNNCell</h4><p>把state_size和output_size定义成相同，而且ht和output也是相同的<br>最普通的RNN定义方式。也就是说output = new_state = f(W <em>input + U</em> state + B)</p><h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h4><p>相比BasicRNNCell只改变了call函数部分，增加了重置门和更新门两部分，分别由r和u表示。然后c表示要更新的状态值。其对应的公式如如下所示：</p><p>r = f(W1 <em>input + U1</em> state + B1)r=f(W1∗input+U1∗state+B1)<br>u = f(W2 <em>input + U2</em> state + B2)u=f(W2∗input+U2∗state+B2)<br>c = f(W3 <em>input + U3</em> r _state + B3)c=f(W3∗input+U3∗r∗state+B3)<br>h_new = u _ h + (1 - u) * chn​ew=u∗h+(1−u)∗c</p><h4 id="BasicLSTMCell"><a href="#BasicLSTMCell" class="headerlink" title="BasicLSTMCell"></a>BasicLSTMCell</h4><p>相比GRU，LSTM又多了一个输出门，而且又新增添了一个C表示其内部状态，然后将h和c以tuple的形式返回作为LSTM内部的状态变量。</p><p>我们用的时候都是用的它的两个子类BasicRNNCell和BasicLSTMCell。顾名思义，前者是RNN的基础类，后者是LSTM的基础类。看下RNNCell、BasicRNNCell、BasicLSTMCell这三个类的注释部分，应该就可以理解它们的功能了。<br>除了call方法外，对于RNNCell，还有两个类属性比较重要：</p><ul><li>state_size</li><li>output_size<br>前者是隐层的大小，后者是输出的大小。比如我们通常是将一个batch送入模型计算，设输入数据的形状为(batch_size, input_size)，那么计算时得到的隐层状态就是(batch_size, state_size)，输出就是(batch_size, output_size)。<br><img src="/images/20191117_rnn_h14.jpg" alt="h14"></li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">2</span>]: <span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">In [<span class="number">3</span>]: cell = tf.nn.rnn_cell.BasicRNNCell(num_units=<span class="number">128</span>) <span class="comment"># state_size = 128</span></span><br><span class="line">In [<span class="number">4</span>]: <span class="built_in">print</span>(cell.state_size) <span class="comment"># 128</span></span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">    In [<span class="number">5</span>]: inputs = tf.placeholder(np.float32, shape=(<span class="number">32</span>, <span class="number">100</span>)) <span class="comment"># 32 是 batch_size</span></span><br><span class="line">    In [<span class="number">6</span>]: h0 = cell.zero_state(<span class="number">32</span>, np.float32) <span class="comment"># 通过zero_state得到一个全0的初始状态，形状为(batch_size, state_size)</span></span><br><span class="line">    In [<span class="number">7</span>]: output, h1 = cell.call(inputs, h0) <span class="comment">#调用call函数</span></span><br><span class="line">    ---------------------------------------------------------------------------</span><br><span class="line">    AttributeError                            Traceback (most recent call last)</span><br><span class="line">    &lt;ipython-<span class="built_in">input</span>-<span class="number">7</span>-378fe3b1c400&gt; <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    ----&gt; <span class="number">1</span> output, h1 = cell.call(inputs, h0) <span class="comment">#调用call函数</span></span><br><span class="line">    ~/anaconda3/envs/tensorflow/lib/python3<span class="number">.6</span>/site-packages/tensorflow/python/ops/rnn_cell_impl.py <span class="keyword">in</span> call(self, inputs, state)</span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">        <span class="number">350</span>     gate_inputs = math_ops.matmul(</span><br><span class="line">    --&gt; <span class="number">351</span>         array_ops.concat([inputs, state], <span class="number">1</span>), self._kernel)</span><br><span class="line">        <span class="number">352</span>     gate_inputs = nn_ops.bias_add(gate_inputs, self._bias)</span><br><span class="line">        <span class="number">353</span>     output = self._activation(gate_inputs)</span><br><span class="line">    AttributeError: <span class="string">'BasicRNNCell'</span> <span class="built_in">object</span> has no attribute <span class="string">'_kernel'</span></span><br><span class="line">    In [<span class="number">8</span>]: output, h1 = cell.__call__(inputs, h0) <span class="comment">#调用call函数</span></span><br><span class="line">    In [<span class="number">9</span>]: <span class="built_in">print</span>(h1.shape) <span class="comment"># (32, 128)</span></span><br><span class="line">    (<span class="number">32</span>, <span class="number">128</span>)</span><br><span class="line">    In [<span class="number">10</span>]:</span><br></pre></td></tr></tbody></table></figure><p>BasicLSTMCell</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">10</span>]: lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=<span class="number">128</span>)</span><br><span class="line">In [<span class="number">11</span>]: inputs = tf.placeholder(np.float32, shape=(<span class="number">32</span>, <span class="number">100</span>)) <span class="comment"># 32 是 batch_size</span></span><br><span class="line">In [<span class="number">12</span>]: h0 = lstm_cell.zero_state(<span class="number">32</span>, np.float32) <span class="comment"># 通过zero_state得到一个全0的初始状态</span></span><br><span class="line">In [<span class="number">13</span>]: output, h1 = lstm_cell.__call__(inputs, h0)</span><br><span class="line">In [<span class="number">14</span>]: <span class="built_in">print</span>(h1.h)  <span class="comment"># shape=(32, 128)</span></span><br><span class="line">Tensor(<span class="string">"basic_lstm_cell/Mul_2:0"</span>, shape=(<span class="number">32</span>, <span class="number">128</span>), dtype=float32)</span><br><span class="line">In [<span class="number">15</span>]: <span class="built_in">print</span>(h1.c)  <span class="comment"># shape=(32, 128)</span></span><br><span class="line">Tensor(<span class="string">"basic_lstm_cell/Add_1:0"</span>, shape=(<span class="number">32</span>, <span class="number">128</span>), dtype=float32)</span><br></pre></td></tr></tbody></table></figure><p>参考链接：<br><a href="https://zhuanlan.zhihu.com/p/28054589">https://zhuanlan.zhihu.com/p/28054589</a><br><a href="https://zhuanlan.zhihu.com/p/28196873">https://zhuanlan.zhihu.com/p/28196873</a></p><pre><code></code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python_learn</title>
      <link href="/2019/11/17/20191117-python-learn/"/>
      <url>/2019/11/17/20191117-python-learn/</url>
      
        <content type="html"><![CDATA[<h2 id="查找图中两个节点的最小的距离"><a href="#查找图中两个节点的最小的距离" class="headerlink" title="查找图中两个节点的最小的距离"></a>查找图中两个节点的最小的距离</h2><p>这里面使用了python的优先队列，这里的队列按照后面的数值大小进行排序，而不是像普通的队列一样先进先出。后面的数值，是节点到出发节点的距离长度。  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time : 2019/11/17 11:50</span></span><br><span class="line"><span class="comment"># @Author : caius</span></span><br><span class="line"><span class="comment"># @Site :</span></span><br><span class="line"><span class="comment"># @File : find_min_bfs.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"><span class="keyword">import</span>  heapq</span><br><span class="line"><span class="keyword">import</span>  math</span><br><span class="line"><span class="comment"># pqueue = []</span></span><br><span class="line"><span class="comment"># heapq.heappush(pqueue,(1,"A"))</span></span><br><span class="line"><span class="comment"># heapq.heappush(pqueue,(7,"B"))</span></span><br><span class="line"><span class="comment"># heapq.heappush(pqueue,(3,"C"))</span></span><br><span class="line"><span class="comment"># heapq.heappush(pqueue,(6,"D"))</span></span><br><span class="line"><span class="comment"># heapq.heappush(pqueue,(2,"E"))</span></span><br><span class="line">graph = {</span><br><span class="line">    <span class="string">"A"</span>:{<span class="string">"B"</span>: <span class="number">5</span>, <span class="string">"C"</span>: <span class="number">1</span>},</span><br><span class="line">    <span class="string">"B"</span>:{<span class="string">"A"</span>: <span class="number">5</span>,<span class="string">"C"</span>: <span class="number">2</span>,<span class="string">"D"</span>: <span class="number">1</span>},</span><br><span class="line">    <span class="string">"C"</span>:{<span class="string">"A"</span>: <span class="number">1</span>,<span class="string">"B"</span>: <span class="number">2</span>,<span class="string">"D"</span>: <span class="number">4</span>,<span class="string">"E"</span>: <span class="number">8</span>},</span><br><span class="line">    <span class="string">"D"</span>:{<span class="string">"B"</span>: <span class="number">1</span>,<span class="string">"C"</span>: <span class="number">4</span>,<span class="string">"E"</span>: <span class="number">3</span>,<span class="string">"F"</span>: <span class="number">6</span>},</span><br><span class="line">    <span class="string">"E"</span>:{<span class="string">"C"</span>: <span class="number">8</span>,<span class="string">"D"</span>: <span class="number">3</span>},</span><br><span class="line">    <span class="string">"F"</span>:{<span class="string">"D"</span>: <span class="number">6</span>}</span><br><span class="line">}</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">init_distance</span>(<span class="params">graph,s</span>):</span><br><span class="line">    distance = {s:<span class="number">0</span>}</span><br><span class="line">    <span class="keyword">for</span> vertex <span class="keyword">in</span> graph:</span><br><span class="line">        <span class="keyword">if</span> vertex != s:</span><br><span class="line">            distance[vertex] = math.inf</span><br><span class="line">    <span class="keyword">return</span> distance</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">dijkstra</span>(<span class="params">graph, s</span>):</span><br><span class="line">    pqueue = []</span><br><span class="line">    heapq.heappush(pqueue,(<span class="number">0</span>,s))</span><br><span class="line">    seen =<span class="built_in">set</span>()</span><br><span class="line">    parent ={s:<span class="literal">None</span>}</span><br><span class="line">    distance = init_distance(graph,s)</span><br><span class="line">    <span class="keyword">while</span>(<span class="built_in">len</span>(pqueue)&gt;<span class="number">0</span>):</span><br><span class="line">        pair = heapq.heappop(pqueue) <span class="comment"># 拿到一对点，pair</span></span><br><span class="line">        dist = pair[<span class="number">0</span>]</span><br><span class="line">        vertex = pair[<span class="number">1</span>]</span><br><span class="line">        seen.add(vertex)</span><br><span class="line">        nodes = graph[vertex].keys()</span><br><span class="line">        <span class="keyword">for</span> w <span class="keyword">in</span> nodes:</span><br><span class="line">            <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> seen:</span><br><span class="line">                <span class="keyword">if</span> dist+graph[vertex][w] &lt;distance[w]:</span><br><span class="line">                    heapq.heappush(pqueue,(dist+graph[vertex][w],w))</span><br><span class="line">                    parent[w] = vertex</span><br><span class="line">                    distance[w] = dist+graph[vertex][w]</span><br><span class="line">    <span class="keyword">return</span> parent,distance</span><br><span class="line">parent, distance  = dijkstra(graph,<span class="string">"A"</span>)</span><br><span class="line"><span class="built_in">print</span>(parent)</span><br><span class="line"><span class="built_in">print</span>(distance)</span><br></pre></td></tr></tbody></table></figure><h3 id="python-装饰器"><a href="#python-装饰器" class="headerlink" title="python 装饰器"></a>python 装饰器</h3><p>装饰器(Decorators)是 Python 的一个重要部分。简单地说：他们是修改其他函数的功能的函数。他们有助于让我们的代码更简短，也更Pythonic（Python范儿）。装饰器可以让你的代码更简洁。  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time : 2019/11/17 9:59</span></span><br><span class="line"><span class="comment"># @Author : caius</span></span><br><span class="line"><span class="comment"># @Site :</span></span><br><span class="line"><span class="comment"># @File : deco.py.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"><span class="keyword">import</span>  time</span><br><span class="line"><span class="comment"># 装饰器</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">display_time</span>(<span class="params">func</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">wrapper</span>(<span class="params">*args</span>):</span><br><span class="line">        t1 = time.time()</span><br><span class="line">        result = func(*args)</span><br><span class="line">        t2 = time.time()</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"Total time: {:.4} s"</span>.<span class="built_in">format</span>(t2-t1))</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">return</span> wrapper</span><br><span class="line"><span class="comment"># 输出质数</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">is_prime</span>(<span class="params">num</span>):</span><br><span class="line">    <span class="keyword">if</span> num&lt;<span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">    <span class="keyword">elif</span> num==<span class="number">2</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, num):</span><br><span class="line">            <span class="keyword">if</span> num%i ==<span class="number">0</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line"><span class="meta">@display_time</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">prime_nums</span>():</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span>  <span class="built_in">range</span>(<span class="number">2</span>,<span class="number">10000</span>):</span><br><span class="line">        <span class="keyword">if</span> is_prime(i):</span><br><span class="line">            <span class="built_in">print</span>(i)</span><br><span class="line"><span class="meta">@display_time</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">count_prime_nums</span>(<span class="params">maxnum</span>):</span><br><span class="line">    count = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">2</span>, maxnum):</span><br><span class="line">        <span class="keyword">if</span> is_prime(i):</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> count</span><br><span class="line">count = count_prime_nums(<span class="number">5000</span>)</span><br><span class="line"><span class="built_in">print</span>(count)</span><br><span class="line">~</span><br></pre></td></tr></tbody></table></figure><h3 id="用turtle-画图"><a href="#用turtle-画图" class="headerlink" title="用turtle 画图"></a>用turtle 画图</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time : 2019/11/17 13:27</span></span><br><span class="line"><span class="comment"># @Author : caius</span></span><br><span class="line"><span class="comment"># @Site :</span></span><br><span class="line"><span class="comment"># @File : draw.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"><span class="keyword">from</span> turtle <span class="keyword">import</span>  *</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="comment"># turtle 画图</span></span><br><span class="line"><span class="comment"># forward(100)</span></span><br><span class="line"><span class="comment"># left(90)</span></span><br><span class="line"><span class="comment"># forward(100)</span></span><br><span class="line"><span class="comment"># left(90)</span></span><br><span class="line"><span class="comment"># forward(100)</span></span><br><span class="line"><span class="comment"># left(90)</span></span><br><span class="line"><span class="comment"># forward(100)</span></span><br><span class="line"><span class="comment"># left(90)</span></span><br><span class="line"><span class="comment"># exitonclick()# 不点击窗口的话就不会退出</span></span><br><span class="line"><span class="comment"># # 画等边三角形</span></span><br><span class="line"><span class="comment"># forward(100)</span></span><br><span class="line"><span class="comment"># left(120)</span></span><br><span class="line"><span class="comment"># forward(100)</span></span><br><span class="line"><span class="comment"># left(120)</span></span><br><span class="line"><span class="comment"># forward(100)</span></span><br><span class="line"><span class="comment"># left(120)</span></span><br><span class="line"><span class="comment"># exitonclick()# 不点击窗口的话就不会退出</span></span><br><span class="line"><span class="comment"># 画五角星</span></span><br><span class="line"><span class="comment"># forward(100)</span></span><br><span class="line"><span class="comment"># right(180-36)</span></span><br><span class="line"><span class="comment"># forward(100)</span></span><br><span class="line"><span class="comment"># right(180-36)</span></span><br><span class="line"><span class="comment"># forward(100)</span></span><br><span class="line"><span class="comment"># right(180-36)</span></span><br><span class="line"><span class="comment"># forward(100)</span></span><br><span class="line"><span class="comment"># right(180-36)</span></span><br><span class="line"><span class="comment"># forward(100)</span></span><br><span class="line"><span class="comment"># right(180-36)</span></span><br><span class="line"><span class="comment"># for i in range(5):</span></span><br><span class="line"><span class="comment">#     forward(100)</span></span><br><span class="line"><span class="comment">#     right(180-36)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line">angle = <span class="number">360</span>/<span class="number">8</span></span><br><span class="line">length = <span class="number">100</span></span><br><span class="line">speed(<span class="number">0</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    <span class="keyword">if</span> i %<span class="number">2</span>==<span class="number">0</span>:</span><br><span class="line">        color(<span class="string">'yellow'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        color(<span class="string">'red'</span>)</span><br><span class="line">    begin_fill()</span><br><span class="line">    forward(<span class="number">100</span>)</span><br><span class="line">    left(angle)</span><br><span class="line">    forward(length)</span><br><span class="line">    left(<span class="number">180</span>-angle)</span><br><span class="line">    forward(length)</span><br><span class="line">    left(angle)</span><br><span class="line">    forward(length)</span><br><span class="line">    left(<span class="number">180</span>-angle)</span><br><span class="line">    end_fill()</span><br><span class="line">    left(angle)</span><br><span class="line">forward(length)</span><br><span class="line">left(<span class="number">180</span>-(<span class="number">180</span>-angle)/<span class="number">2</span>)</span><br><span class="line">alpha = angle*<span class="number">3.1415926536</span> /<span class="number">180</span></span><br><span class="line">step = <span class="number">2</span>*length*math.sin(alpha/<span class="number">2</span>)</span><br><span class="line">color(<span class="string">'blue'</span>)</span><br><span class="line">begin_fill()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">8</span>):</span><br><span class="line">    forward(step)</span><br><span class="line">    left(angle)</span><br><span class="line">end_fill()</span><br><span class="line">exitonclick()<span class="comment"># 不点击窗口的话就不会退出</span></span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20191117_python-learn_h1.png" alt="h1">  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time : 2019/11/17 14:20</span></span><br><span class="line"><span class="comment"># @Author : caius</span></span><br><span class="line"><span class="comment"># @Site :</span></span><br><span class="line"><span class="comment"># @File : Lsystem2.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"><span class="keyword">from</span> turtle <span class="keyword">import</span>  *</span><br><span class="line">length = <span class="number">7</span></span><br><span class="line">angle = <span class="number">60</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">split_path</span>(<span class="params">path</span>):</span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="built_in">list</span> = []</span><br><span class="line">    <span class="keyword">while</span> i &lt;<span class="built_in">len</span>(path):</span><br><span class="line">        <span class="keyword">if</span> path[i] == <span class="string">"F"</span>:</span><br><span class="line">            <span class="built_in">list</span>.append(path[i:i+<span class="number">2</span>])</span><br><span class="line">            i = i+<span class="number">2</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="built_in">list</span>.append(path[i])</span><br><span class="line">            i = i+<span class="number">1</span></span><br><span class="line">    <span class="keyword">return</span> <span class="built_in">list</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">apply_rule</span>(<span class="params">path, rules</span>):</span><br><span class="line">    lst = split_path(path)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(lst)):</span><br><span class="line">        symbol = lst[i]</span><br><span class="line">        <span class="keyword">if</span> symbol <span class="keyword">in</span> rules:</span><br><span class="line">            lst[i] = rules[symbol]</span><br><span class="line">    path =<span class="string">""</span>.join(symbol <span class="keyword">for</span> symbol <span class="keyword">in</span> lst)</span><br><span class="line">    <span class="keyword">return</span> path</span><br><span class="line">rules={</span><br><span class="line">    <span class="string">"Fl"</span>: <span class="string">"Fr+Fl+Fr"</span>,</span><br><span class="line">    <span class="string">"Fr"</span>:<span class="string">"Fl-Fr-Fl"</span></span><br><span class="line">}</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">draw_patj</span>(<span class="params">path</span>):</span><br><span class="line">    lst = split_path(path)</span><br><span class="line">    <span class="keyword">for</span> symbol <span class="keyword">in</span> lst:</span><br><span class="line">        <span class="keyword">if</span> symbol ==<span class="string">"Fl"</span>  <span class="keyword">or</span> symbol==<span class="string">'Fr'</span>:</span><br><span class="line">            forward(length)</span><br><span class="line">        <span class="keyword">elif</span> symbol==<span class="string">"-"</span>:</span><br><span class="line">            left(angle)</span><br><span class="line">        <span class="keyword">elif</span> symbol==<span class="string">'+'</span>:</span><br><span class="line">            right(angle)</span><br><span class="line">speed(<span class="number">0</span>)</span><br><span class="line">path = <span class="string">'Fr'</span></span><br><span class="line"><span class="comment"># speed(0)</span></span><br><span class="line"><span class="comment">#</span></span><br><span class="line"><span class="comment">#lst = split_path(path)</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">6</span>):</span><br><span class="line">    path = apply_rule(path,rules)</span><br><span class="line"><span class="built_in">print</span>(path)</span><br><span class="line">draw_patj(path)</span><br><span class="line">exitonclick()</span><br><span class="line">~</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20191117_python-learn_h2.png" alt="h2"></p><h3 id="python-类的构造"><a href="#python-类的构造" class="headerlink" title="python 类的构造"></a>python 类的构造</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time : 2019/11/17 14:52</span></span><br><span class="line"><span class="comment"># @Author : caius</span></span><br><span class="line"><span class="comment"># @Site :</span></span><br><span class="line"><span class="comment"># @File : Bank.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">BankAccount</span>:</span><br><span class="line">    <span class="comment"># Constructor 构造器</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,accountNumber, accountName, balance</span>):</span><br><span class="line">        self.accountNumber = accountNumber</span><br><span class="line">        self.accountName = accountName</span><br><span class="line">        self.balance = balance</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"(name: {},  balance: {})"</span>.<span class="built_in">format</span>(self.accountName,self.balance)</span><br><span class="line"><span class="comment">#!/usr/bin/env python</span></span><br><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="comment"># @Time : 2019/11/17 14:56</span></span><br><span class="line"><span class="comment"># @Author : caius</span></span><br><span class="line"><span class="comment"># @Site :</span></span><br><span class="line"><span class="comment"># @File : main.py</span></span><br><span class="line"><span class="comment"># @Software: PyCharm</span></span><br><span class="line"><span class="keyword">from</span> Bank <span class="keyword">import</span> BankAccount</span><br><span class="line">b1 = BankAccount(<span class="string">"56789"</span>,<span class="string">"Tony"</span>, <span class="number">100.0</span>)</span><br><span class="line"><span class="built_in">print</span>((b1))</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Duplicate</title>
      <link href="/2019/11/17/Duplicate/"/>
      <url>/2019/11/17/Duplicate/</url>
      
        <content type="html"><![CDATA[<p>是否存在相同元素，python3用字典的方式解决  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  </code></pre><p>| </p><pre><code>class Solution(object):      def containsDuplicate(self, nums):          """          :type nums: List[int]          :rtype: bool          """          # 方法3：数字存字典          dic = {}          for i in nums:              dic[i] = dic.get(i, 0) + 1              if dic[i] &gt; 1:                  return True          return False    </code></pre><p>—|—  </p><p>最长回文子串  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  </code></pre><p>| </p><pre><code>class Solution:      def longestPalindrome(self, s: str) -&gt; str:          # 两种判断条件          # DP 动态规划          # CABAC          # B          # ABA          # CABAC          palindrome = ''          for i in range(len(s)):              aa1 = self.getlongestpalindrome(s,i,i)              len1 = len(aa1)              if len1&gt;len(palindrome):                  palindrome = aa1              aa2 = self.getlongestpalindrome(s,i,i+1)              len2 = len(aa2)              if len2&gt;len(palindrome):                  palindrome = aa2                    return palindrome              def getlongestpalindrome(self, s, l, r):          while l &gt;= 0 and r&lt;len(s) and s[l]==s[r]:              l -= 1              r += 1          return s[l+1:r]    ~    </code></pre><p>—|—  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  </code></pre><p>| </p><pre><code>class Solution:      def containsNearbyDuplicate(self, nums: List[int], k: int) -&gt; bool:          # 用dictionary来做比较容易一点          lookup = {}          for i , num in enumerate(nums):              if num not in lookup:                  lookup[num] = i # 将num存到dic里面              else:                  if i-lookup[num]&lt;=k:                      return True                  lookup[num] = i          return False              ~    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>DFS_BFS</title>
      <link href="/2019/11/17/DFS-BFS/"/>
      <url>/2019/11/17/DFS-BFS/</url>
      
        <content type="html"><![CDATA[<pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68    </code></pre><p>| </p><pre><code>#!/usr/bin/env python  # -*- coding: utf-8 -*-  # @Time : 2019/11/17 10:28  # @Author : caius  # @Site :   # @File : BFS.py  # @Software: PyCharm    graph = {      "A":{"B", "C"},      "B":{"A","C","D"},      "C":{"A","B","D","E"},      "D":{"B","C","E","F"},      "E":{"C","D"},      "F":{"D"}  }  # 字典的基本用法  # keys: A B C D E F  # graph["E"} "c". "D"      def BFS(graph, s):      # 队列先进先出      queue=[]      queue.append(s)      seen = set()# 代表这个东西是个set      seen.add(s)      parrent ={}      parrent={s:None}      while(len(queue)&gt;0):          vertex = queue.pop(0)          nodes = graph[vertex]          for w in nodes:              if w not in seen:                  queue.append(w)                  seen.add(w)                  parrent[w] = vertex          print(vertex)      return parrent    def DFS(graph, s):      # 队列先进先出      stack=[]      stack.append(s)      seen = set()# 代表这个东西是个set      seen.add(s)      while(len(stack)&gt;0):          vertex = stack.pop()          nodes = graph[vertex]          for w in nodes:              if w not in seen:                  stack.append(w)                  seen.add(w)          print(vertex)      DFS(graph,"E")  parrent = BFS(graph,'E')  for key in parrent:      print(key,parrent[key])  v = "B"  count=-1  while v!= None:      print(v)      v = parrent[v]      count+=1    print("count: {} 次".format(count))    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>dynamic planning</title>
      <link href="/2019/11/17/dynamic/"/>
      <url>/2019/11/17/dynamic/</url>
      
        <content type="html"><![CDATA[<pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  </code></pre><p>| </p><pre><code>arr= [1,2,4,1,7,8,3]  def rec_opt(arr,i):      if i == 0:          return arr[0]      elif i == 1:          return max(arr[0],arr[1])      else:          A = rec_opt(arr,i-2)+arr[i]          B = rec_opt(arr,i-1)      return max(A,B)          print(rec_opt(arr,6))    </code></pre><p>—|—  </p><h3 id="递归会产生很多的重叠的子问题，运算规模2-n"><a href="#递归会产生很多的重叠的子问题，运算规模2-n" class="headerlink" title="递归会产生很多的重叠的子问题，运算规模2^n"></a>递归会产生很多的重叠的子问题，运算规模2^n</h3><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  </code></pre><p>| </p><pre><code>#非递归的方法  import numpy as np    def dp_opt(arr):      opt = np.zeros(len(arr))      opt[0] = arr[0]      opt[1] = max(arr[0], arr[1])            for i in range(2, len(arr)):          A = opt[i-2] +arr[i]          B = opt[i-1]          opt[i] = max(A,B)            return opt[len(arr)-1]  dp_opt(arr)    </code></pre><p>—|—  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  </code></pre><p>| </p><pre><code>arr = [3,34,4,12,5,2]  def rec_subset(arr, i,s):      if s == 0:          return True      elif i == 0:          return arr[0] == s      elif arr[i]&gt;s:          return rec_subset(arr,i-1,s)      else:          A= rec_subset(arr,i-1, s-arr[i])          B = rec_subset(arr, i-1,s)          return A or B    rec_subset(arr, len(arr)-1,9)  ~    </code></pre><p>—|—  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  </code></pre><p>| </p><pre><code># 非递归的方法，用二维数组来保存  def dp_subset(arr,s):      subset = np.zeros((len(arr),s+1),dtype=bool)      subset[:,0] = True      subset[0,:] = Fasle      subset[0,arr[0]] = True      for i in range(1,len(arr)):          for ss in range(1,s+1):              if arr[i]&gt; ss:                  subset[i,ss] = subset[i-1,ss]              else:                  A= subset(i-1, s-arr[i])                  B = subset(i-1,s)                  subset[i,s] = A or B      r ,c  =subset.shape      return subset[r-1,c-1]    rec_subset(arr, len(arr)-1,9)    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python_learn</title>
      <link href="/2019/11/17/python-learn/"/>
      <url>/2019/11/17/python-learn/</url>
      
        <content type="html"><![CDATA[<h2 id="查找图中两个节点的最小的距离"><a href="#查找图中两个节点的最小的距离" class="headerlink" title="查找图中两个节点的最小的距离"></a>查找图中两个节点的最小的距离</h2><p>这里面使用了python的优先队列，这里的队列按照后面的数值大小进行排序，而不是像普通的队列一样先进先出。后面的数值，是节点到出发节点的距离长度。  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  </code></pre><p>| </p><pre><code>#!/usr/bin/env python  # -*- coding: utf-8 -*-  # @Time : 2019/11/17 11:50  # @Author : caius  # @Site :   # @File : find_min_bfs.py  # @Software: PyCharm    import  heapq  import  math  # pqueue = []  # heapq.heappush(pqueue,(1,"A"))  # heapq.heappush(pqueue,(7,"B"))  # heapq.heappush(pqueue,(3,"C"))  # heapq.heappush(pqueue,(6,"D"))  # heapq.heappush(pqueue,(2,"E"))    graph = {      "A":{"B": 5, "C": 1},      "B":{"A": 5,"C": 2,"D": 1},      "C":{"A": 1,"B": 2,"D": 4,"E": 8},      "D":{"B": 1,"C": 4,"E": 3,"F": 6},      "E":{"C": 8,"D": 3},      "F":{"D": 6}  }    def init_distance(graph,s):      distance = {s:0}      for vertex in graph:          if vertex != s:              distance[vertex] = math.inf      return distance      def dijkstra(graph, s):      pqueue = []      heapq.heappush(pqueue,(0,s))      seen =set()      parent ={s:None}      distance = init_distance(graph,s)      while(len(pqueue)&gt;0):          pair = heapq.heappop(pqueue) # 拿到一对点，pair          dist = pair[0]          vertex = pair[1]          seen.add(vertex)            nodes = graph[vertex].keys()          for w in nodes:              if w not in seen:                  if dist+graph[vertex][w] &lt;distance[w]:                      heapq.heappush(pqueue,(dist+graph[vertex][w],w))                      parent[w] = vertex                      distance[w] = dist+graph[vertex][w]        return parent,distance    parent, distance  = dijkstra(graph,"A")  print(parent)  print(distance)    </code></pre><p>—|—  </p><h3 id="python-装饰器"><a href="#python-装饰器" class="headerlink" title="python 装饰器"></a>python 装饰器</h3><p>装饰器(Decorators)是 Python 的一个重要部分。简单地说：他们是修改其他函数的功能的函数。他们有助于让我们的代码更简短，也更Pythonic（Python范儿）。装饰器可以让你的代码更简洁。  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  </code></pre><p>| </p><pre><code>#!/usr/bin/env python  # -*- coding: utf-8 -*-  # @Time : 2019/11/17 9:59  # @Author : caius  # @Site :   # @File : deco.py.py  # @Software: PyCharm  import  time    # 装饰器  def display_time(func):      def wrapper(*args):          t1 = time.time()          result = func(*args)          t2 = time.time()          print("Total time: {:.4} s".format(t2-t1))          return result      return wrapper    # 输出质数  def is_prime(num):      if num&lt;2:          return False      elif num==2:          return True      else:          for i in range(2, num):              if num%i ==0:                  return False          return True    @display_time  def prime_nums():        for i in  range(2,10000):          if is_prime(i):              print(i)      @display_time  def count_prime_nums(maxnum):      count = 0      for i in range(2, maxnum):          if is_prime(i):              count += 1      return count    count = count_prime_nums(5000)  print(count)  ~    </code></pre><p>—|—  </p><h3 id="用turtle-画图"><a href="#用turtle-画图" class="headerlink" title="用turtle 画图"></a>用turtle 画图</h3><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  </code></pre><p>| </p><pre><code>#!/usr/bin/env python  # -*- coding: utf-8 -*-  # @Time : 2019/11/17 13:27  # @Author : caius  # @Site :   # @File : draw.py  # @Software: PyCharm  from turtle import  *  import math  # turtle 画图  # forward(100)  # left(90)  # forward(100)  # left(90)  # forward(100)  # left(90)  # forward(100)  # left(90)  # exitonclick()# 不点击窗口的话就不会退出    # # 画等边三角形  # forward(100)  # left(120)  # forward(100)  # left(120)  # forward(100)  # left(120)  # exitonclick()# 不点击窗口的话就不会退出    # 画五角星  # forward(100)  # right(180-36)  # forward(100)  # right(180-36)  # forward(100)  # right(180-36)  # forward(100)  # right(180-36)  # forward(100)  # right(180-36)  # for i in range(5):  #     forward(100)  #     right(180-36)  #  angle = 360/8  length = 100  speed(0)  for i in range(8):      if i %2==0:          color('yellow')      else:          color('red')      begin_fill()      forward(100)      left(angle)      forward(length)      left(180-angle)      forward(length)      left(angle)      forward(length)      left(180-angle)      end_fill()      left(angle)  forward(length)  left(180-(180-angle)/2)    alpha = angle*3.1415926536 /180  step = 2*length*math.sin(alpha/2)  color('blue')  begin_fill()  for i in range(8):      forward(step)      left(angle)  end_fill()    exitonclick()# 不点击窗口的话就不会退出    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2019/11/17/python-learn/images/20191117_python-learn_h1.png" alt="h1">  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  </code></pre><p>| </p><pre><code>#!/usr/bin/env python  # -*- coding: utf-8 -*-  # @Time : 2019/11/17 14:20  # @Author : caius  # @Site :   # @File : Lsystem2.py  # @Software: PyCharm  from turtle import  *  length = 7  angle = 60      def split_path(path):      i = 0      list = []      while i &lt;len(path):          if path[i] == "F":              list.append(path[i:i+2])              i = i+2          else:              list.append(path[i])              i = i+1      return list    def apply_rule(path, rules):      lst = split_path(path)      for i in range(len(lst)):          symbol = lst[i]          if symbol in rules:              lst[i] = rules[symbol]      path ="".join(symbol for symbol in lst)      return path  rules={      "Fl": "Fr+Fl+Fr",      "Fr":"Fl-Fr-Fl"  }  def draw_patj(path):      lst = split_path(path)      for symbol in lst:          if symbol =="Fl"  or symbol=='Fr':              forward(length)          elif symbol=="-":              left(angle)          elif symbol=='+':              right(angle)    speed(0)  path = 'Fr'  # speed(0)  #  #lst = split_path(path)  for i in range(6):      path = apply_rule(path,rules)  print(path)  draw_patj(path)  exitonclick()  ~    </code></pre><p>—|—  </p><p><img src="//caius-lu.github.io/2019/11/17/python-learn/images/20191117_python-learn_h2.png" alt="h2"></p><h3 id="python-类的构造"><a href="#python-类的构造" class="headerlink" title="python 类的构造"></a>python 类的构造</h3><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  </code></pre><p>| </p><pre><code>#!/usr/bin/env python  # -*- coding: utf-8 -*-  # @Time : 2019/11/17 14:52  # @Author : caius  # @Site :   # @File : Bank.py  # @Software: PyCharm  class BankAccount:      # Constructor 构造器      def __init__(self,accountNumber, accountName, balance):          self.accountNumber = accountNumber          self.accountName = accountName          self.balance = balance        def __str__(self):          return "(name: {},  balance: {})".format(self.accountName,self.balance)    #!/usr/bin/env python  # -*- coding: utf-8 -*-  # @Time : 2019/11/17 14:56  # @Author : caius  # @Site :   # @File : main.py  # @Software: PyCharm  from Bank import BankAccount    b1 = BankAccount("56789","Tony", 100.0)  print((b1))    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>rnn</title>
      <link href="/2019/11/17/rnn/"/>
      <url>/2019/11/17/rnn/</url>
      
        <content type="html"><![CDATA[<h3 id="RNN及RNN的几个变体"><a href="#RNN及RNN的几个变体" class="headerlink" title="RNN及RNN的几个变体"></a>RNN及RNN的几个变体</h3><p>序列形的数据不太好用原始的神经网络处理，为了建模序列问题，RNN引入了隐状态h（hidden）的概念，h可以对序列形的数据提取特征，接着转换为输出。</p><p><img src="//caius-lu.github.io/2019/11/17/rnn/images/20191117_rnn_h1.jpg" alt="h1"></p><ul><li><p>圆圈或方块表示的是向量</p></li><li><p>一个箭头就表示对该向量做一次变换。如上图中h0和x1分别有一个箭头连接，就表示对h0和x1各做了一次变换</p></li></ul><p><strong>Tips：U、W、b都是一样的，每个步骤的参数都是共享的，这是RNN的重要特点。</strong></p><p><img src="//caius-lu.github.io/2019/11/17/rnn/images/20191117_rnn_h2.jpg" alt="h2"><br>依次计算剩下来的（使用相同的参数U、W、b）：<br><img src="//caius-lu.github.io/2019/11/17/rnn/images/20191117_rnn_h3.jpg" alt="h3"><br>输出值的方法就是直接通过h进行计算：<br><img src="//caius-lu.github.io/2019/11/17/rnn/images/20191117_rnn_h4.jpg" alt="h4"><br><strong>一个箭头就表示对对应的向量做一次类似于f(Wx+b)的变换，这里的这个箭头就表示对h1进行一次变换，得到输出y1</strong><br><img src="//caius-lu.github.io/2019/11/17/rnn/images/20191117_rnn_h5.jpg" alt="h5"><br>输入是x1, x2, …..xn，输出为y1, y2, …yn，也就是说，<strong>输入和输出序列必须要是等长的。</strong><br>一些问题适合用经典的RNN结构建模，如：</p><ul><li>计算视频中每一帧的分类标签。因为要对每一帧进行计算，因此输入和输出序列等长。</li><li>输入为字符，输出为下一个字符的概率。这就是著名的Char RNN（详细介绍请参考：The Unreasonable Effectiveness of Recurrent Neural Networks）。</li></ul><h3 id="N-vs-1-多输入单输出"><a href="#N-vs-1-多输入单输出" class="headerlink" title="N vs 1 多输入单输出"></a>N vs 1 多输入单输出</h3><p><img src="//caius-lu.github.io/2019/11/17/rnn/images/20191117_rnn_h6.jpg" alt="h6"><br>这种结构通常用来处理序列分类问题。如输入一段文字判别它所属的类别，输入一个句子判断其情感倾向，输入一段视频并判断它的类别等等。</p><h3 id="1-VS-N-单输入多输出"><a href="#1-VS-N-单输入多输出" class="headerlink" title="1 VS N 单输入多输出"></a>1 VS N 单输入多输出</h3><p>输入不是序列而输出为序列的情况怎么处理？我们可以只在序列开始进行输入计算：<br><img src="//caius-lu.github.io/2019/11/17/rnn/images/20191117_rnn_h7.jpg" alt="h7"><br>还有一种结构是把输入x作为每个阶段的输入：<br><img src="//caius-lu.github.io/2019/11/17/rnn/images/20191117_rnn_h8.jpg" alt="h8"><br>这种结构处理的问题：</p><ul><li>从图像生成文字（image caption），此时输入的X就是图像的特征，而输出的y序列就是一段句子</li><li>从类别生成语音或音乐等</li></ul><h3 id="N-vs-M"><a href="#N-vs-M" class="headerlink" title="N vs M"></a>N vs M</h3><p>RNN最重要的一个变种：N vs M。这种结构又叫Encoder-Decoder模型，也可以称之为Seq2Seq模型。<br>为此，Encoder-Decoder结构先将输入数据编码成一个上下文向量c：<br><img src="//caius-lu.github.io/2019/11/17/rnn/images/20191117_rnn_h9.jpg" alt="h9"><br>c有多种方式，最简单的方法就是把Encoder的最后一个隐状态赋值给c，还可以对最后的隐状态做一个变换得到c，也可以对所有的隐状态做变换。</p><p>拿到c之后，就用另一个RNN网络对其进行解码，这部分RNN网络被称为Decoder。具体做法就是将c当做之前的初始状态h0输入到Decoder中：<br><img src="//caius-lu.github.io/2019/11/17/rnn/images/20191117_rnn_h10.jpg" alt="h10"><br>还有一种做法是将c当做每一步的输入：<br><img src="//caius-lu.github.io/2019/11/17/rnn/h11.jpg" alt="h11"><br>Encoder-Decoder结构不限制输入和输出的序列长度，因此应用的范围非常广泛，比如：</p><ul><li>机器翻译。Encoder-Decoder的最经典应用，事实上这一结构就是在机器翻译领域最先提出的</li><li>文本摘要。输入是一段文本序列，输出是这段文本序列的摘要序列</li><li>阅读理解。将输入的文章和问题分别编码，再对其进行解码得到问题的答案。</li><li>语音识别。输入是语音信号序列，输出是文字序列。</li><li>………</li></ul><h2 id="tensorflow-实现RNN"><a href="#tensorflow-实现RNN" class="headerlink" title="tensorflow 实现RNN"></a>tensorflow 实现RNN</h2><h3 id="RNNCell"><a href="#RNNCell" class="headerlink" title="RNNCell"></a>RNNCell</h3><p>RNNCell 是TensorFlow中实现RNN的基本单元，每个RNNCell都有一个call方法，使用方式是：(output, next_state) = call(input, state)<br>借助图片来说可能更容易理解。假设我们有一个初始状态h0，还有输入x1，调用call(x1, h0)后就可以得到(output1, h1)：<br><img src="//caius-lu.github.io/2019/11/17/rnn/images/20191117_rnn_h12.jpg" alt="h12"><br>再调用一次call(x2, h1)就可以得到(output2, h2)：<br><img src="//caius-lu.github.io/2019/11/17/rnn/images/20191117_rnn_h13.jpg" alt="h13"><br>也就是说，每调用一次RNNCell的call方法，就相当于在时间上“推进了一步”，这就是RNNCell的基本功能。<br>RNNCell只是一个抽象类，其他的RNNcell都会继承该方法，然后具体实现其中的call()函数。主要有state_size和output_size两个属性，分别代表了隐藏层和输出层的维度。然后就是zero_state()和call()两个函数，分别用于初始化初始状态h0为全零向量和定义实际的RNNCell的操作（比如RNN就是一个激活，GRU的两个门，LSTM的三个门控等，不同的RNN的区别主要体现在这个函数）。</p><h4 id="BasicRNNCell"><a href="#BasicRNNCell" class="headerlink" title="BasicRNNCell"></a>BasicRNNCell</h4><p>把state_size和output_size定义成相同，而且ht和output也是相同的<br>最普通的RNN定义方式。也就是说output = new_state = f(W <em>input + U</em> state + B)</p><h4 id="GRU"><a href="#GRU" class="headerlink" title="GRU"></a>GRU</h4><p>相比BasicRNNCell只改变了call函数部分，增加了重置门和更新门两部分，分别由r和u表示。然后c表示要更新的状态值。其对应的公式如如下所示：</p><p>r = f(W1 <em>input + U1</em> state + B1)r=f(W1∗input+U1∗state+B1)<br>u = f(W2 <em>input + U2</em> state + B2)u=f(W2∗input+U2∗state+B2)<br>c = f(W3 <em>input + U3</em> r _state + B3)c=f(W3∗input+U3∗r∗state+B3)<br>h_new = u _ h + (1 - u) * chn​ew=u∗h+(1−u)∗c</p><h4 id="BasicLSTMCell"><a href="#BasicLSTMCell" class="headerlink" title="BasicLSTMCell"></a>BasicLSTMCell</h4><p>相比GRU，LSTM又多了一个输出门，而且又新增添了一个C表示其内部状态，然后将h和c以tuple的形式返回作为LSTM内部的状态变量。</p><p>我们用的时候都是用的它的两个子类BasicRNNCell和BasicLSTMCell。顾名思义，前者是RNN的基础类，后者是LSTM的基础类。看下RNNCell、BasicRNNCell、BasicLSTMCell这三个类的注释部分，应该就可以理解它们的功能了。<br>除了call方法外，对于RNNCell，还有两个类属性比较重要：</p><ul><li><p>state_size</p></li><li><p>output_size<br>前者是隐层的大小，后者是输出的大小。比如我们通常是将一个batch送入模型计算，设输入数据的形状为(batch_size, input_size)，那么计算时得到的隐层状态就是(batch_size, state_size)，输出就是(batch_size, output_size)。<br><img src="//caius-lu.github.io/2019/11/17/rnn/images/20191117_rnn_h14.jpg" alt="h14"></p><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19<br>20<br>21<br>22<br>23<br>24<br>25<br>26<br>27<br>28<br>29<br>30<br>31<br>32</p></li></ul><p>| </p><pre><code>In [2]: import numpy as np                                                                                                                                            In [3]: cell = tf.nn.rnn_cell.BasicRNNCell(num_units=128) # state_size = 128                                                                                          In [4]: print(cell.state_size) # 128                                                                                                                                128    In [5]: inputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 是 batch_size                                                                                       In [6]: h0 = cell.zero_state(32, np.float32) # 通过zero_state得到一个全0的初始状态，形状为(batch_size, state_size)                                                    In [7]: output, h1 = cell.call(inputs, h0) #调用call函数                                                                                                            ---------------------------------------------------------------------------  AttributeError                            Traceback (most recent call last)  &lt;ipython-input-7-378fe3b1c400&gt; in &lt;module&gt;  ----&gt; 1 output, h1 = cell.call(inputs, h0) #调用call函数    ~/anaconda3/envs/tensorflow/lib/python3.6/site-packages/tensorflow/python/ops/rnn_cell_impl.py in call(self, inputs, state)      349       350     gate_inputs = math_ops.matmul(  --&gt; 351         array_ops.concat([inputs, state], 1), self._kernel)      352     gate_inputs = nn_ops.bias_add(gate_inputs, self._bias)      353     output = self._activation(gate_inputs)    AttributeError: 'BasicRNNCell' object has no attribute '_kernel'    In [8]: output, h1 = cell.__call__(inputs, h0) #调用call函数                                                                                                          In [9]: print(h1.shape) # (32, 128)                                                                                                                                 (32, 128)    In [10]:    </code></pre><p>—|—  </p><p>BasicLSTMCell</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  </code></pre><p>| </p><pre><code>In [10]: lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units=128)                                               In [11]: inputs = tf.placeholder(np.float32, shape=(32, 100)) # 32 是 batch_size                               In [12]: h0 = lstm_cell.zero_state(32, np.float32) # 通过zero_state得到一个全0的初始状态                       In [13]: output, h1 = lstm_cell.__call__(inputs, h0)                                                           In [14]: print(h1.h)  # shape=(32, 128)                                                                      Tensor("basic_lstm_cell/Mul_2:0", shape=(32, 128), dtype=float32)    In [15]: print(h1.c)  # shape=(32, 128)                                                                      Tensor("basic_lstm_cell/Add_1:0", shape=(32, 128), dtype=float32)    </code></pre><p>—|—  </p><p>参考链接：<br><a href="https://zhuanlan.zhihu.com/p/28054589">https://zhuanlan.zhihu.com/p/28054589</a><br><a href="https://zhuanlan.zhihu.com/p/28196873">https://zhuanlan.zhihu.com/p/28196873</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文本检测调研</title>
      <link href="/2019/11/13/20191113-%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E8%B0%83%E7%A0%94/"/>
      <url>/2019/11/13/20191113-%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E8%B0%83%E7%A0%94/</url>
      
        <content type="html"><![CDATA[<p>文本检测问题，广义上来说可以看做是一个目标检测的问题，但是相当于目标检测要简单的多。因为目标检测往往除了背景还有其他的类，而文本检测，只需要检测背景和文本类两个问题。因此可以采用目标检测或者分割的方法来进行文本检测。<br>而视频中的文本检测，也可以看做是视频中的目标检测中的一种，感觉应该也可以用视频中的目标检测+跟踪来做。</p><h4 id="通常目标跟踪面临的极大难点："><a href="#通常目标跟踪面临的极大难点：" class="headerlink" title="通常目标跟踪面临的极大难点："></a>通常目标跟踪面临的极大难点：</h4><p>物体变形、亮度变化、快速移动、背景干扰覆盖。其中最主要的三个难题分别是目标背景的变化，物体本身的变化，光照强度的变化。</p><h5 id="光流法"><a href="#光流法" class="headerlink" title="光流法"></a>光流法</h5><h5 id="帧间差分法"><a href="#帧间差分法" class="headerlink" title="帧间差分法"></a>帧间差分法</h5><h5 id="背景差分法"><a href="#背景差分法" class="headerlink" title="背景差分法"></a>背景差分法</h5>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python：logging模块</title>
      <link href="/2019/11/04/20191104-Python%EF%BC%9Alogging%E6%A8%A1%E5%9D%97/"/>
      <url>/2019/11/04/20191104-Python%EF%BC%9Alogging%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">12</span>]: <span class="keyword">import</span> logging</span><br><span class="line">    ...:</span><br><span class="line">    ...: logging.basicConfig(level=logging.DEBUG,</span><br><span class="line">    ...:                     filename=<span class="string">'output.log'</span>,</span><br><span class="line">    ...:                     datefmt=<span class="string">'%Y/%m/%d %H:%M:%S'</span>,</span><br><span class="line">    ...:                     <span class="built_in">format</span>=<span class="string">'%(asctime)s - %(name)s - %(levelname)s - %(lineno)d - %(module)s - %(message)s'</span>)</span><br><span class="line">    ...: logger = logging.getLogger(__name__)</span><br><span class="line">    ...:</span><br><span class="line">    ...: logger.info(<span class="string">'This is a log info'</span>)</span><br><span class="line">    ...: logger.debug(<span class="string">'Debugging'</span>)</span><br><span class="line">    ...: logger.warning(<span class="string">'Warning exists'</span>)</span><br><span class="line">    ...: logger.info(<span class="string">'Finish'</span>)</span><br></pre></td></tr></tbody></table></figure><p>2019-11-04 13:00:45,976 - <strong>main</strong> - INFO - This is a log info<br>2019-11-04 13:00:45,977 - <strong>main</strong> - WARNING - Warning exists<br>2019-11-04 13:00:45,977 - <strong>main</strong> - INFO - Finish</p><p>设置level等级，从而控制log输出的级别。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">In [<span class="number">13</span>]: <span class="keyword">import</span> logging</span><br><span class="line">    ...:</span><br><span class="line">    ...: logging.basicConfig(level=logging.DEBUG,</span><br><span class="line">    ...:                     filename=<span class="string">'output.log'</span>,</span><br><span class="line">    ...:                     datefmt=<span class="string">'%Y/%m/%d %H:%M:%S'</span>,</span><br><span class="line">    ...:                     <span class="built_in">format</span>=<span class="string">'%(asctime)s - %(name)s - %(levelname)s - %(lineno)d - %(module)s - %(message)s'</span>)</span><br><span class="line">    ...: logger = logging.getLogger(__name__)</span><br><span class="line">    ...: logger.setLevel(level=logging.DEBUG)</span><br><span class="line">    ...: logger.info(<span class="string">'This is a log info'</span>)</span><br><span class="line">    ...: logger.debug(<span class="string">'Debugging'</span>)</span><br><span class="line">    ...: logger.warning(<span class="string">'Warning exists'</span>)</span><br><span class="line">    ...: logger.info(<span class="string">'Finish'</span>)</span><br></pre></td></tr></tbody></table></figure><p>如果不设置logger的Level的话， debug’的信息也不会被输出。</p><p>需要设置 logger.setLevel(level=logging.DEBUG)，然后信息就可以正常的显示出来了。</p><p>2019-11-04 13:10:01,634 - <strong>main</strong> - INFO - This is a log info<br>2019-11-04 13:10:01,634 - <strong>main</strong> - DEBUG - Debugging<br>2019-11-04 13:10:01,635 - <strong>main</strong> - WARNING - Warning exists<br>2019-11-04 13:10:01,639 - <strong>main</strong> - INFO - Finish</p><p>CSDN博客地址：<br><a href="https://blog.csdn.net/eilot_c/article/details/102894687">https://blog.csdn.net/eilot_c/article/details/102894687</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Python：logging模块</title>
      <link href="/2019/11/04/Python%EF%BC%9Alogging%E6%A8%A1%E5%9D%97/"/>
      <url>/2019/11/04/Python%EF%BC%9Alogging%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  </code></pre><p>| </p><pre><code>In [12]: import logging      ...:      ...: logging.basicConfig(level=logging.DEBUG,      ...:                     filename='output.log',      ...:                     datefmt='%Y/%m/%d %H:%M:%S',      ...:                     format='%(asctime)s - %(name)s - %(levelname)s - %(lineno)d - %(module)s - %(message)s')      ...: logger = logging.getLogger(__name__)      ...:      ...: logger.info('This is a log info')      ...: logger.debug('Debugging')      ...: logger.warning('Warning exists')      ...: logger.info('Finish')    </code></pre><p>—|—  </p><p>2019-11-04 13:00:45,976 - <strong>main</strong> - INFO - This is a log info<br>2019-11-04 13:00:45,977 - <strong>main</strong> - WARNING - Warning exists<br>2019-11-04 13:00:45,977 - <strong>main</strong> - INFO - Finish</p><p>设置level等级，从而控制log输出的级别。</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  </code></pre><p>| </p><pre><code>In [13]: import logging      ...:      ...: logging.basicConfig(level=logging.DEBUG,      ...:                     filename='output.log',      ...:                     datefmt='%Y/%m/%d %H:%M:%S',      ...:                     format='%(asctime)s - %(name)s - %(levelname)s - %(lineno)d - %(module)s - %(message)s')      ...: logger = logging.getLogger(__name__)      ...: logger.setLevel(level=logging.DEBUG)      ...: logger.info('This is a log info')      ...: logger.debug('Debugging')      ...: logger.warning('Warning exists')      ...: logger.info('Finish')    </code></pre><p>—|—  </p><p>如果不设置logger的Level的话， debug’的信息也不会被输出。</p><p>需要设置 logger.setLevel(level=logging.DEBUG)，然后信息就可以正常的显示出来了。</p><p>2019-11-04 13:10:01,634 - <strong>main</strong> - INFO - This is a log info<br>2019-11-04 13:10:01,634 - <strong>main</strong> - DEBUG - Debugging<br>2019-11-04 13:10:01,635 - <strong>main</strong> - WARNING - Warning exists<br>2019-11-04 13:10:01,639 - <strong>main</strong> - INFO - Finish</p><p>CSDN博客地址：<br><a href="https://blog.csdn.net/eilot_c/article/details/102894687">https://blog.csdn.net/eilot_c/article/details/102894687</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>attention_mechanism_xmind</title>
      <link href="/2019/10/31/20191031-attention-mechanism-xmind/"/>
      <url>/2019/10/31/20191031-attention-mechanism-xmind/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/20191031_attention-mechanism-xmind_AttentionMechanism.png"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>object_detection_xmind</title>
      <link href="/2019/10/31/20191031-object-dectection-xmind/"/>
      <url>/2019/10/31/20191031-object-dectection-xmind/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/20191031_object-dectection-xmind_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B.png"></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch基础导图</title>
      <link href="/2019/10/31/20191031-pytorchchch%E2%80%94%E2%80%94-xmind/"/>
      <url>/2019/10/31/20191031-pytorchchch%E2%80%94%E2%80%94-xmind/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/20191031_pytorchchch%E2%80%94%E2%80%94-xmind_PyTorch%E5%AD%A6%E4%B9%A0.png"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>object_track_xmind</title>
      <link href="/2019/10/31/20191031-pyobject-track-xmind/"/>
      <url>/2019/10/31/20191031-pyobject-track-xmind/</url>
      
        <content type="html"><![CDATA[<p><img src="/images/20191031_pyobject-track-xmind_%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA.png"></p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>attention_mechanism_xmind</title>
      <link href="/2019/10/31/attention-mechanism-xmind/"/>
      <url>/2019/10/31/attention-mechanism-xmind/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/10/31/attention-mechanism-xmind/images/20191031_attention-mechanism-xmind_AttentionMechanism.png"></p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Attention </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>object_detection_xmind</title>
      <link href="/2019/10/31/object-dectection-xmind/"/>
      <url>/2019/10/31/object-dectection-xmind/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/10/31/object-dectection-xmind/images/20191031_object-dectection-xmind_%E7%9B%AE%E6%A0%87%E6%A3%80%E6%B5%8B%E6%A8%A1%E5%9E%8B.png"></p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>object_track_xmind</title>
      <link href="/2019/10/31/pyobject-track-xmind/"/>
      <url>/2019/10/31/pyobject-track-xmind/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/10/31/pyobject-track-xmind/images/20191031_pyobject-track-xmind_%E7%9B%AE%E6%A0%87%E8%B7%9F%E8%B8%AA.png"></p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标跟踪 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>pytorch基础导图</title>
      <link href="/2019/10/31/pytorchchch%E2%80%94%E2%80%94-xmind/"/>
      <url>/2019/10/31/pytorchchch%E2%80%94%E2%80%94-xmind/</url>
      
        <content type="html"><![CDATA[<p><img src="/2019/10/31/pytorchchch%E2%80%94%E2%80%94-xmind/images/20191031_pytorchchch%E2%80%94%E2%80%94-xmind_PyTorch%E5%AD%A6%E4%B9%A0.png"></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>knn手写数字识别</title>
      <link href="/2019/10/20/20191020-knn%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/"/>
      <url>/2019/10/20/20191020-knn%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<h2 id="K近邻算法"><a href="#K近邻算法" class="headerlink" title="K近邻算法"></a>K近邻算法</h2><p>算法的核心思想是，给定一个训练数据集，对于新的输入实例，在训练集中找到与该实例最近的K个实例，这K个实例的多数属于某个类，就把这个输入归为哪个类中。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 1 重要</span></span><br><span class="line"><span class="comment"># 2 KNN CNN 2种</span></span><br><span class="line"><span class="comment"># 3 样本</span></span><br><span class="line"><span class="comment"># 4 旧瓶装新酒 ：数字识别的不同</span></span><br><span class="line"><span class="comment"># 4.1 网络 4。2 每一级 4.3 先原理 后代码</span></span><br><span class="line"><span class="comment"># 本质：knn test 样本 K个 max4 3个1 -》1</span></span><br><span class="line"><span class="comment"># 1 load Data  1.1 随机数 1.2 4组 训练 测试 （图片 和 标签）</span></span><br><span class="line"><span class="comment"># 2 knn test train distance 5*500 = 2500 784=28*28</span></span><br><span class="line"><span class="comment"># 3 knn k个最近的图片5 500 1-》500train （4）</span></span><br><span class="line"><span class="comment"># 4 k个最近的图片-&gt; parse centent label</span></span><br><span class="line"><span class="comment"># 5 label -》 数字 p9 测试图片-》数据</span></span><br><span class="line"><span class="comment"># 6 检测概率统计</span></span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="comment"># load data 2 one_hot : 1 0000 1 fileName</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'MNIST_data'</span>,one_hot=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 属性设置</span></span><br><span class="line">trainNum = <span class="number">55000</span></span><br><span class="line">testNum = <span class="number">10000</span></span><br><span class="line">trainSize = <span class="number">500</span></span><br><span class="line">testSize = <span class="number">5</span></span><br><span class="line">k = <span class="number">4</span></span><br><span class="line"><span class="comment"># data 分解 1 trainSize   2范围0-trainNum 3 replace=False</span></span><br><span class="line">trainIndex = np.random.choice(trainNum,trainSize,replace=<span class="literal">False</span>)</span><br><span class="line">testIndex = np.random.choice(testNum,testSize,replace=<span class="literal">False</span>)</span><br><span class="line">trainData = mnist.train.images[trainIndex]<span class="comment"># 训练图片</span></span><br><span class="line">trainLabel = mnist.train.labels[trainIndex]<span class="comment"># 训练标签</span></span><br><span class="line">testData = mnist.test.images[testIndex]</span><br><span class="line">testLabel = mnist.test.labels[testIndex]</span><br><span class="line"><span class="comment"># 28*28 = 784</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'trainData.shape='</span>,trainData.shape)<span class="comment">#500*784 1 图片个数 2 784?</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'trainLabel.shape='</span>,trainLabel.shape)<span class="comment">#500*10</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'testData.shape='</span>,testData.shape)<span class="comment">#5*784</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'testLabel.shape='</span>,testLabel.shape)<span class="comment">#5*10</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'testLabel='</span>,testLabel)<span class="comment"># 4 :testData [0]  3:testData[1] 6</span></span><br><span class="line"><span class="comment"># tf input  784-&gt;image</span></span><br><span class="line">trainDataInput = tf.placeholder(shape=[<span class="literal">None</span>,<span class="number">784</span>],dtype=tf.float32)</span><br><span class="line">trainLabelInput = tf.placeholder(shape=[<span class="literal">None</span>,<span class="number">10</span>],dtype=tf.float32)</span><br><span class="line">testDataInput = tf.placeholder(shape=[<span class="literal">None</span>,<span class="number">784</span>],dtype=tf.float32)</span><br><span class="line">testLabelInput = tf.placeholder(shape=[<span class="literal">None</span>,<span class="number">10</span>],dtype=tf.float32)</span><br><span class="line"><span class="comment">#knn distance 5*785.  5*1*784</span></span><br><span class="line"><span class="comment"># 5 500 784 (3D) 2500*784</span></span><br><span class="line">f1 = tf.expand_dims(testDataInput,<span class="number">1</span>) <span class="comment"># 维度扩展  (?, 1, 784)  # 0其实代表的第一维度，那么1代表第二维度，2代表第三维度。以此类推。</span></span><br><span class="line"><span class="built_in">print</span>()</span><br><span class="line">f2 = tf.subtract(trainDataInput,f1)<span class="comment"># 784 sum(784)  返回 x-y 的元素.</span></span><br><span class="line">f3 = tf.reduce_sum(tf.<span class="built_in">abs</span>(f2),reduction_indices=<span class="number">2</span>)<span class="comment"># 完成数据累加 784 abs     # axis=0时，按第一个维度求和，</span></span><br><span class="line"><span class="comment"># 5*500</span></span><br><span class="line">f4 = tf.negative(f3)<span class="comment"># 取反 取负值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'f1.shape='</span>,f1.shape)<span class="comment">#500*784 1 图片个数 2 784?</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'f2.shape='</span>,f2.shape)<span class="comment">#500*10  (?, ?, 784)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'f3.shape='</span>,f3.shape)<span class="comment">#5*784  (?, ?)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'f4.shape='</span>,f4.shape)<span class="comment">#5*10   (?, ?)</span></span><br><span class="line">f5,f6 = tf.nn.top_k(f4,k=<span class="number">4</span>) <span class="comment"># 选取f4 最大的四个值   返回值和其索引位置</span></span><br><span class="line"><span class="comment"># f3 最小的四个值</span></span><br><span class="line"><span class="comment"># f6 index-&gt;trainLabelInput</span></span><br><span class="line">f7 = tf.gather(trainLabelInput,f6)  <span class="comment"># 根据索引抽取其中的张量</span></span><br><span class="line"><span class="comment"># f8 num reduce_sum  reduction_indices=1 '竖直'</span></span><br><span class="line">f8 = tf.reduce_sum(f7,reduction_indices=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># tf.argmax 选取在某一个最大的值 index</span></span><br><span class="line">f9 = tf.argmax(f8,dimension=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># f9 -&gt; test5 image -&gt; 5 num</span></span><br><span class="line"><span class="keyword">with</span> tf.Session() <span class="keyword">as</span> sess:</span><br><span class="line">    <span class="comment"># f1 &lt;- testData 5张图片</span></span><br><span class="line">    p1 = sess.run(f1,feed_dict={testDataInput:testData[<span class="number">0</span>:<span class="number">5</span>]})</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'p1='</span>,p1.shape)<span class="comment"># p1= (5, 1, 784)</span></span><br><span class="line">    p2 = sess.run(f2,feed_dict={trainDataInput:trainData,testDataInput:testData[<span class="number">0</span>:<span class="number">5</span>]})</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'p2='</span>,p2.shape)<span class="comment">#p2= (5, 500, 784) (1,100)</span></span><br><span class="line">    p3 = sess.run(f3,feed_dict={trainDataInput:trainData,testDataInput:testData[<span class="number">0</span>:<span class="number">5</span>]})</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'p3='</span>,p3.shape)<span class="comment">#p3= (5, 500)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'p3[0,0]='</span>,p3[<span class="number">0</span>,<span class="number">0</span>]) <span class="comment">#130.451 knn distance p3[0,0]= 155.812</span></span><br><span class="line">    p4 = sess.run(f4,feed_dict={trainDataInput:trainData,testDataInput:testData[<span class="number">0</span>:<span class="number">5</span>]})</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'p4='</span>,p4.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'p4[0,0]'</span>,p4[<span class="number">0</span>,<span class="number">0</span>])</span><br><span class="line">    p5,p6 = sess.run((f5,f6),feed_dict={trainDataInput:trainData,testDataInput:testData[<span class="number">0</span>:<span class="number">5</span>]})</span><br><span class="line">    <span class="comment">#p5= (5, 4) 每一张测试图片（5张）分别对应4张最近训练图片</span></span><br><span class="line">    <span class="comment">#p6= (5, 4)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'p5='</span>,p5.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'p6='</span>,p6.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'p5[0,0]'</span>,p5[<span class="number">0</span>])</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'p6[0,0]'</span>,p6[<span class="number">0</span>])<span class="comment"># p6 index</span></span><br><span class="line">    p7 = sess.run(f7,feed_dict={trainDataInput:trainData,testDataInput:testData[<span class="number">0</span>:<span class="number">5</span>],trainLabelInput:trainLabel})</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'p7='</span>,p7.shape)<span class="comment">#p7= (5, 4, 10)</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'p7[]'</span>,p7)</span><br><span class="line">    p8 = sess.run(f8,feed_dict={trainDataInput:trainData,testDataInput:testData[<span class="number">0</span>:<span class="number">5</span>],trainLabelInput:trainLabel})</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'p8='</span>,p8.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'p8[]='</span>,p8)</span><br><span class="line">    p9 = sess.run(f9,feed_dict={trainDataInput:trainData,testDataInput:testData[<span class="number">0</span>:<span class="number">5</span>],trainLabelInput:trainLabel})</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'p9='</span>,p9.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'p9[]='</span>,p9)</span><br><span class="line">    p10 = np.argmax(testLabel[<span class="number">0</span>:<span class="number">5</span>],axis=<span class="number">1</span>)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'p10[]='</span>,p10)</span><br><span class="line">j = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>,<span class="number">5</span>):</span><br><span class="line">    <span class="keyword">if</span> p10[i] == p9[i]:</span><br><span class="line">        j = j+<span class="number">1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'ac='</span>,j*<span class="number">100</span>/<span class="number">5</span>)</span><br></pre></td></tr></tbody></table></figure><p>参考链接：<br>OpenCV+TensorFlow 入门人工智能图像处理 手写数字识别</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>knn手写数字识别</title>
      <link href="/2019/10/20/knn%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/"/>
      <url>/2019/10/20/knn%E6%89%8B%E5%86%99%E6%95%B0%E5%AD%97%E8%AF%86%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<h2 id="K近邻算法"><a href="#K近邻算法" class="headerlink" title="K近邻算法"></a>K近邻算法</h2><p>算法的核心思想是，给定一个训练数据集，对于新的输入实例，在训练集中找到与该实例最近的K个实例，这K个实例的多数属于某个类，就把这个输入归为哪个类中。</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  </code></pre><p>| </p><pre><code># 1 重要  # 2 KNN CNN 2种  # 3 样本   # 4 旧瓶装新酒 ：数字识别的不同  # 4.1 网络 4。2 每一级 4.3 先原理 后代码   # 本质：knn test 样本 K个 max4 3个1 -》1    # 1 load Data  1.1 随机数 1.2 4组 训练 测试 （图片 和 标签）  # 2 knn test train distance 5*500 = 2500 784=28*28  # 3 knn k个最近的图片5 500 1-》500train （4）  # 4 k个最近的图片-&gt; parse centent label  # 5 label -》 数字 p9 测试图片-》数据  # 6 检测概率统计  import tensorflow as tf  import numpy as np  import random   from tensorflow.examples.tutorials.mnist import input_data  # load data 2 one_hot : 1 0000 1 fileName   mnist = input_data.read_data_sets('MNIST_data',one_hot=True)  # 属性设置  trainNum = 55000  testNum = 10000  trainSize = 500  testSize = 5  k = 4  # data 分解 1 trainSize   2范围0-trainNum 3 replace=False   trainIndex = np.random.choice(trainNum,trainSize,replace=False)  testIndex = np.random.choice(testNum,testSize,replace=False)  trainData = mnist.train.images[trainIndex]# 训练图片  trainLabel = mnist.train.labels[trainIndex]# 训练标签  testData = mnist.test.images[testIndex]  testLabel = mnist.test.labels[testIndex]  # 28*28 = 784  print('trainData.shape=',trainData.shape)#500*784 1 图片个数 2 784?  print('trainLabel.shape=',trainLabel.shape)#500*10  print('testData.shape=',testData.shape)#5*784  print('testLabel.shape=',testLabel.shape)#5*10  print('testLabel=',testLabel)# 4 :testData [0]  3:testData[1] 6   # tf input  784-&gt;image  trainDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32)  trainLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32)  testDataInput = tf.placeholder(shape=[None,784],dtype=tf.float32)  testLabelInput = tf.placeholder(shape=[None,10],dtype=tf.float32)  #knn distance 5*785.  5*1*784  # 5 500 784 (3D) 2500*784  f1 = tf.expand_dims(testDataInput,1) # 维度扩展  (?, 1, 784)  # 0其实代表的第一维度，那么1代表第二维度，2代表第三维度。以此类推。  print()  f2 = tf.subtract(trainDataInput,f1)# 784 sum(784)  返回 x-y 的元素.  f3 = tf.reduce_sum(tf.abs(f2),reduction_indices=2)# 完成数据累加 784 abs     # axis=0时，按第一个维度求和，  # 5*500  f4 = tf.negative(f3)# 取反 取负值  print('f1.shape=',f1.shape)#500*784 1 图片个数 2 784?  print('f2.shape=',f2.shape)#500*10  (?, ?, 784)  print('f3.shape=',f3.shape)#5*784  (?, ?)  print('f4.shape=',f4.shape)#5*10   (?, ?)  f5,f6 = tf.nn.top_k(f4,k=4) # 选取f4 最大的四个值   返回值和其索引位置  # f3 最小的四个值  # f6 index-&gt;trainLabelInput  f7 = tf.gather(trainLabelInput,f6)  # 根据索引抽取其中的张量  # f8 num reduce_sum  reduction_indices=1 '竖直'  f8 = tf.reduce_sum(f7,reduction_indices=1)  # tf.argmax 选取在某一个最大的值 index  f9 = tf.argmax(f8,dimension=1)  # f9 -&gt; test5 image -&gt; 5 num  with tf.Session() as sess:      # f1 &lt;- testData 5张图片      p1 = sess.run(f1,feed_dict={testDataInput:testData[0:5]})      print('p1=',p1.shape)# p1= (5, 1, 784)      p2 = sess.run(f2,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]})      print('p2=',p2.shape)#p2= (5, 500, 784) (1,100)        p3 = sess.run(f3,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]})      print('p3=',p3.shape)#p3= (5, 500)      print('p3[0,0]=',p3[0,0]) #130.451 knn distance p3[0,0]= 155.812            p4 = sess.run(f4,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]})      print('p4=',p4.shape)      print('p4[0,0]',p4[0,0])            p5,p6 = sess.run((f5,f6),feed_dict={trainDataInput:trainData,testDataInput:testData[0:5]})      #p5= (5, 4) 每一张测试图片（5张）分别对应4张最近训练图片      #p6= (5, 4)      print('p5=',p5.shape)      print('p6=',p6.shape)      print('p5[0,0]',p5[0])      print('p6[0,0]',p6[0])# p6 index            p7 = sess.run(f7,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel})      print('p7=',p7.shape)#p7= (5, 4, 10)      print('p7[]',p7)            p8 = sess.run(f8,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel})      print('p8=',p8.shape)      print('p8[]=',p8)            p9 = sess.run(f9,feed_dict={trainDataInput:trainData,testDataInput:testData[0:5],trainLabelInput:trainLabel})      print('p9=',p9.shape)      print('p9[]=',p9)            p10 = np.argmax(testLabel[0:5],axis=1)      print('p10[]=',p10)  j = 0  for i in range(0,5):      if p10[i] == p9[i]:          j = j+1  print('ac=',j*100/5)    </code></pre><p>—|—  </p><p>参考链接：<br>OpenCV+TensorFlow 入门人工智能图像处理 手写数字识别</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow自定义网络模型</title>
      <link href="/2019/10/17/20191017-tensorflow%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"/>
      <url>/2019/10/17/20191017-tensorflow%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="Slim"><a href="#Slim" class="headerlink" title="Slim"></a>Slim</h2><p>TF-Slim 模块是 TensorFlow 中最好用的 API 之一。尤其是里面引入的 arg_scope、model_variables、repeat、stack。<br>TF-Slim 是 TensorFlow 中一个用来构建、训练、评估复杂模型的轻量化库。TF-Slim 模块可以和 TensorFlow 中其它API混合使用。</p><h3 id="Slim模块的导入"><a href="#Slim模块的导入" class="headerlink" title="Slim模块的导入"></a>Slim模块的导入</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> tensorflow.contrib.slim <span class="keyword">as</span> slim</span><br></pre></td></tr></tbody></table></figure><h3 id="Slim-构建模型"><a href="#Slim-构建模型" class="headerlink" title="Slim 构建模型"></a>Slim 构建模型</h3><p>可以用 slim、variables、layers 和 scopes 来十分简洁地定义模型。下面对各个部分进行了详细描述：</p><h4 id="Slim变量（Variables）"><a href="#Slim变量（Variables）" class="headerlink" title="Slim变量（Variables）"></a>Slim变量（Variables）</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">weights = slim.variable('weights',</span><br><span class="line">                        shape=[10, 10, 3 , 3],</span><br><span class="line">                        initializer=tf.truncated_normal_initializer(stddev=0.1),</span><br><span class="line">                        regularizer=slim.l2_regularizer(0.05),</span><br><span class="line">                        device='/CPU:0')</span><br><span class="line">~</span><br></pre></td></tr></tbody></table></figure><h4 id="Slim-层（Layers）"><a href="#Slim-层（Layers）" class="headerlink" title="Slim 层（Layers）"></a>Slim 层（Layers）</h4><p>使用基础（plain）的 TensorFlow 代码：  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">input = ...</span><br><span class="line">with tf.name_scope('conv1_1') as scope:</span><br><span class="line">  kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,</span><br><span class="line">                                           stddev=1e-1), name='weights')</span><br><span class="line">  conv = tf.nn.conv2d(input, kernel, [1, 1, 1, 1], padding='SAME')</span><br><span class="line">  biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),</span><br><span class="line">                       trainable=True, name='biases')</span><br><span class="line">  bias = tf.nn.bias_add(conv, biases)</span><br><span class="line">  conv1 = tf.nn.relu(bias, name=scope)</span><br></pre></td></tr></tbody></table></figure><p>为了避免代码的重复。Slim 提供了很多方便的神经网络 layers 的高层 op。例如：与上面的代码对应的 Slim 版的代码：  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">input = ...</span><br><span class="line">net = slim.conv2d(input, 128, [3, 3], scope='conv1_1')</span><br></pre></td></tr></tbody></table></figure><h3 id="slim-arg-scope（）-函数的使用"><a href="#slim-arg-scope（）-函数的使用" class="headerlink" title="slim.arg_scope（） 函数的使用"></a>slim.arg_scope（） 函数的使用</h3><p>这个函数的作用是给list_ops中的内容设置默认值。但是每个list_ops中的每个成员需要用@add_arg_scope修饰才行。所以使用slim.arg_scope（）有两个步骤：</p><ul><li>使用@slim.add_arg_scope修饰目标函数</li><li>用 slim.arg_scope（）为目标函数设置默认参数.<br>例如如下代码；首先用@slim.add_arg_scope修饰目标函数fun1（），然后利用slim.arg_scope（）为它设置默认参数。</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">```python</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line">slim =tf.contrib.slim</span><br><span class="line"><span class="meta">@slim.add_arg_scope</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">fun1</span>(<span class="params">a=<span class="number">0</span>,b=<span class="number">0</span></span>):</span><br><span class="line"><span class="keyword">return</span> (a+b)</span><br><span class="line"><span class="keyword">with</span> slim.arg_scope([fun1],a=<span class="number">10</span>):</span><br><span class="line">x=fun1(b=<span class="number">30</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></tbody></table></figure><p>运行结果:40<br>参考链接：<br><a href="https://blog.csdn.net/u013921430/article/details/80915696">https://blog.csdn.net/u013921430/article/details/80915696</a></p><h3 id="其他用法见参考链接"><a href="#其他用法见参考链接" class="headerlink" title="其他用法见参考链接"></a>其他用法见参考链接</h3><p><a href="https://blog.csdn.net/wanttifa/article/details/90208398">https://blog.csdn.net/wanttifa/article/details/90208398</a></p><h2 id="查看ckpt中变量的几种方法"><a href="#查看ckpt中变量的几种方法" class="headerlink" title="查看ckpt中变量的几种方法"></a>查看ckpt中变量的几种方法</h2><p>查看ckpt中变量的方法有三种：</p><ul><li>在有model的情况下，使用tf.train.Saver进行restore</li><li>使用tf.train.NewCheckpointReader直接读取ckpt文件，这种方法不需要model。</li><li>使用tools里的freeze_graph来读取ckpt<br>Tips:</li><li>如果模型保存为.ckpt的文件，则使用该文件就可以查看.ckpt文件里的变量。ckpt路径为 model.ckpt</li><li>如果模型保存为.ckpt-xxx-data (图结构)、.ckpt-xxx.index (参数名)、.ckpt-xxx-meta (参数值)文件，则需要同时拥有这三个文件才行。并且ckpt的路径为 model.ckpt-xxx</li></ul><h3 id="1-基于model来读取ckpt文件里的变量"><a href="#1-基于model来读取ckpt文件里的变量" class="headerlink" title="1.基于model来读取ckpt文件里的变量"></a>1.基于model来读取ckpt文件里的变量</h3><p>1.首先建立起model<br>2.从ckpt中恢复变量<br>        1<br>        2<br>        3<br>        4<br>        5<br>        6<br>        7<br>        8<br>        9<br>        10<br>        with tf.Graph().as_default() as g:<br>          #建立model<br>          images, labels = cifar10.inputs(eval_data=eval_data)<br>          logits = cifar10.inference(images)<br>          top_k_op = tf.nn.in_top_k(logits, labels, 1)<br>          #从ckpt中恢复变量<br>          sess = tf.Session()<br>          saver = tf.train.Saver() #saver = tf.train.Saver(…variables…) # 恢复部分变量时，只需要在Saver里指定要恢复的变量<br>          save_path = ‘ckpt的路径’<br>          saver.restore(sess, save_path) # 从ckpt中恢复变量</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">注意：基于model来读取ckpt中变量时，model和ckpt必须匹配。</span><br><span class="line"></span><br><span class="line">### 2.使用tf.train.NewCheckpointReader直接读取ckpt文件里的变量，使用tools.inspect_checkpoint里的print_tensors_in_checkpoint_file函数打印ckpt里的东西</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">    #使用NewCheckpointReader来读取ckpt里的变量</span><br><span class="line">    from tensorflow.python import pywrap_tensorflow</span><br><span class="line">    checkpoint_path = os.path.join(model_dir, "model.ckpt")</span><br><span class="line">    reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path) #tf.train.NewCheckpointReader</span><br><span class="line">    var_to_shape_map = reader.get_variable_to_shape_map()</span><br><span class="line">    for key in var_to_shape_map:</span><br><span class="line">      print("tensor_name: ", key)</span><br><span class="line">      #print(reader.get_tensor(key))</span><br><span class="line">    #使用print_tensors_in_checkpoint_file打印ckpt里的内容</span><br><span class="line">    from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file</span><br><span class="line">    print_tensors_in_checkpoint_file(file_name, #ckpt文件名字</span><br><span class="line">                     tensor_name, # 如果为None,则默认为ckpt里的所有变量</span><br><span class="line">                     all_tensors, # bool 是否打印所有的tensor，这里打印出的是tensor的值，一般不推荐这里设置为False</span><br><span class="line">                     all_tensor_names) # bool 是否打印所有的tensor的name</span><br><span class="line">    #上面的打印ckpt的内部使用的是pywrap_tensorflow.NewCheckpointReader所以要掌握NewCheckpointReader</span><br></pre></td></tr></tbody></table></figure><h3 id="3-使用tools里的freeze-graph来读取ckpt"><a href="#3-使用tools里的freeze-graph来读取ckpt" class="headerlink" title="3.使用tools里的freeze_graph来读取ckpt"></a>3.使用tools里的freeze_graph来读取ckpt</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.python.tools <span class="keyword">import</span> freeze_graph</span><br><span class="line">freeze_graph(input_graph, <span class="comment">#=some_graph_def.pb</span></span><br><span class="line">       input_saver,</span><br><span class="line">       input_binary,</span><br><span class="line">       input_checkpoint, <span class="comment">#=model.ckpt</span></span><br><span class="line">       output_node_names, <span class="comment">#=softmax</span></span><br><span class="line">       restore_op_name,</span><br><span class="line">       filename_tensor_name,</span><br><span class="line">       output_graph, <span class="comment">#='./tmp/frozen_graph.pb'</span></span><br><span class="line">       clear_devices,</span><br><span class="line">       initializer_nodes,</span><br><span class="line">       variable_names_whitelist=<span class="string">''</span>,</span><br><span class="line">       variable_names_blacklist=<span class="string">''</span>,</span><br><span class="line">       input_meta_graph=<span class="literal">None</span>,</span><br><span class="line">       input_saved_model_dir=<span class="literal">None</span>,</span><br><span class="line">       saved_model_tags=<span class="string">'serve'</span>,</span><br><span class="line">       checkpoint_version=<span class="number">2</span>)</span><br><span class="line"><span class="comment">#freeze_graph_test.py讲述了怎么使用freeze_grapg。</span></span><br></pre></td></tr></tbody></table></figure><p>参考链接：<br><a href="https://www.jb51.net/article/142183.htm">https://www.jb51.net/article/142183.htm</a></p><h2 id="control-dependencies"><a href="#control-dependencies" class="headerlink" title="control_dependencies"></a>control_dependencies</h2><p>tf.control_dependencies(control_inputs)<br>Wrapper for Graph.control_dependencies() using the default graph.<br>See Graph.control_dependencies() for more details.<br>此函数指定某些操作执行的依赖关系<br>返回一个控制依赖的上下文管理器，使用 with 关键字可以让在这个上下文环境中的操作都在 control_inputs 执行  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">1 with tf.control_dependencies([a, b]):</span><br><span class="line">2     c = ....</span><br><span class="line">3     d = ...</span><br></pre></td></tr></tbody></table></figure><p>在执行完 a，b 操作之后，才能执行 c，d 操作。意思就是 c，d 操作依赖 a，b 操作  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">1 with tf.control_dependencies([train_step, variable_averages_op]):</span><br><span class="line">2     train_op = tf.no_op(name='train')</span><br></pre></td></tr></tbody></table></figure><p>tf.no_op()表示执行完 train_step, variable_averages_op 操作之后什么都不做<br>参考链接：<br><a href="http://www.tensorfly.cn/tfdoc/api_docs/python/framework.html#Graph.control_dependencies">http://www.tensorfly.cn/tfdoc/api_docs/python/framework.html#Graph.control_dependencies</a></p><h2 id="TensorBoard"><a href="#TensorBoard" class="headerlink" title="TensorBoard"></a>TensorBoard</h2><h3 id="在TensorBoard中可视化图形"><a href="#在TensorBoard中可视化图形" class="headerlink" title="在TensorBoard中可视化图形"></a>在TensorBoard中可视化图形</h3><p>构建您的网络，创建一个会话(session)，然后创建一个TensorFlow File Writer对象<br>File Writer定义存储TensorBoard文件的路径，以及TensorFlow graph对象sess.graph是第二个参数。  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">writer = tf.summary.FileWriter(STORE_PATH, sess.graph)</span><br></pre></td></tr></tbody></table></figure><p>当创建一个TensorFlow网络后，定义并运行File Writer时，就可以启动TensorBoard来可视化图形。要定义File Writer并将图形发送给它，运行以下命令:  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># start the session</span><br><span class="line">with tf.Session() as sess:</span><br><span class="line">writer = tf.summary.FileWriter(STORE_PATH, sess.graph)</span><br></pre></td></tr></tbody></table></figure><h3 id="启动TensorBoard"><a href="#启动TensorBoard" class="headerlink" title="启动TensorBoard"></a>启动TensorBoard</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tensorboard --logdir=STORE_PATH</span><br></pre></td></tr></tbody></table></figure><h3 id="名称空间（Namespaces）"><a href="#名称空间（Namespaces）" class="headerlink" title="名称空间（Namespaces）"></a>名称空间（Namespaces）</h3><p>名称空间是一种作用域，可以用它来包围图形组件，以便将它们组合在一起。通过这样的操作，名称空间中的细节将被折叠成TensorBoard计算图形可视化中的单个名称空间节点。要在TensorFlow中创建名称空间，可以使用Python with功能，如下所示：  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">with tf.name_scope("layer_1"):</span><br><span class="line"># now declare the weights connecting the input to the hidden  layer</span><br><span class="line"> W1 = tf.Variable(tf.random_normal([784, 300], stddev=0.01), name='W')</span><br><span class="line">         b1 = tf.Variable(tf.random_normal([300]), name='b')</span><br><span class="line">         hidden_logits = tf.add(tf.matmul(x_sc, W1), b1)</span><br><span class="line">         hidden_out = tf.nn.sigmoid(hidden_logits)</span><br></pre></td></tr></tbody></table></figure><p>还可以使用tf.variable_scope()代替tf.name_scope()。变量作用域是TensorFlow中的get_variable()变量共享机制的一部分。</p><h3 id="标量总结（Scalar-summaries）"><a href="#标量总结（Scalar-summaries）" class="headerlink" title="标量总结（Scalar summaries）"></a>标量总结（Scalar summaries）</h3><p>在网络中的任何位置，都可以记录标量(即单个实值)数量，以便在TensorBoard中显示。这对于跟踪诸如训练准确率的提高或损失函数的减少，或研究分布的标准差等方面都很有用。执行起来很容易。例如，下面的代码展示了如何在这个图中记录accuracy标量:  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># add a summary to store the</span><br><span class="line">accuracytf.summary.scalar('acc_summary', accuracy)</span><br></pre></td></tr></tbody></table></figure><p>第一个参数是要在TensorBoard可视化中给出标量的名称，第二个参数是要记录的操作(必须返回一个实值)。scalar()调用的输出是一个操作。在上面的代码中，我没有将这个操作分配给Python中的任何变量，但是如果用户愿意，可以这样做。然而，与TensorFlow中的其他操作一样，这些汇总操作在运行之前不会执行任何操作。根据开发人员想要观察的内容，在任何给定的图中通常都会运行许多可视化函数，因此有一个方便的助手函数merge_all()。这将把图中的所有函数调用合并在一起，这样您只需调用merge操作，它将为您收集所有其他函数操作并记录数据。它是这样的:  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">merged = tf.summary.merge_all()</span><br></pre></td></tr></tbody></table></figure><h3 id="图像可视化"><a href="#图像可视化" class="headerlink" title="图像可视化"></a>图像可视化</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"># add summary</span><br><span class="line">if reuse_variables is None:</span><br><span class="line">    tf.summary.image('input', images)</span><br><span class="line">    tf.summary.image('score_map', score_maps)</span><br><span class="line">    tf.summary.image('score_map_pred', f_score * 255)</span><br><span class="line">    tf.summary.image('geo_map_0', geo_maps[:, :, :, 0:1])</span><br><span class="line">    tf.summary.image('geo_map_0_pred', f_geometry[:, :, :, 0:1])</span><br><span class="line">    tf.summary.image('training_masks', training_masks)</span><br><span class="line">    tf.summary.scalar('model_loss', model_loss)</span><br><span class="line">    tf.summary.scalar('total_loss', total_loss)</span><br></pre></td></tr></tbody></table></figure><h2 id="文本检测模型EAST的搭建"><a href="#文本检测模型EAST的搭建" class="headerlink" title="文本检测模型EAST的搭建"></a>文本检测模型EAST的搭建</h2><h3 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">load_annoataion</span>(<span class="params">p</span>):</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    load annotation from the text file</span></span><br><span class="line"><span class="string">    :param p:</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    text_polys = []</span><br><span class="line">    text_tags = []</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(p):</span><br><span class="line">        <span class="keyword">return</span> np.array(text_polys, dtype=np.float32)</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(p, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        reader = csv.reader(f)</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> reader:</span><br><span class="line">            label = line[-<span class="number">1</span>]</span><br><span class="line">            <span class="comment"># strip BOM. \ufeff for python3,  \xef\xbb\bf for python2</span></span><br><span class="line">            line = [i.strip(<span class="string">'\ufeff'</span>).strip(<span class="string">'\xef\xbb\xbf'</span>) <span class="keyword">for</span> i <span class="keyword">in</span> line]</span><br><span class="line">            x1, y1, x2, y2, x3, y3, x4, y4 = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="built_in">float</span>, line[:<span class="number">8</span>]))</span><br><span class="line">            text_polys.append([[x1, y1], [x2, y2], [x3, y3], [x4, y4]])</span><br><span class="line">            <span class="keyword">if</span> label == <span class="string">'*'</span> <span class="keyword">or</span> label == <span class="string">'###'</span>:</span><br><span class="line">                text_tags.append(<span class="literal">True</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                text_tags.append(<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">return</span> np.array(text_polys, dtype=np.float32), np.array(text_tags, dtype=np.<span class="built_in">bool</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">generator</span>(<span class="params">input_size=<span class="number">512</span>, batch_size=<span class="number">32</span>,</span></span><br><span class="line"><span class="params">            background_ratio=<span class="number">3.</span>/<span class="number">8</span>,</span></span><br><span class="line"><span class="params">            random_scale=np.array(<span class="params">[<span class="number">0.5</span>, <span class="number">1</span>, <span class="number">2.0</span>, <span class="number">3.0</span>]</span>),</span></span><br><span class="line"><span class="params">            vis=<span class="literal">False</span></span>):</span><br><span class="line">    image_list = np.array(get_images())</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'{} training images in {}'</span>.<span class="built_in">format</span>(</span><br><span class="line">        image_list.shape[<span class="number">0</span>], FLAGS.training_data_path))</span><br><span class="line">    index = np.arange(<span class="number">0</span>, image_list.shape[<span class="number">0</span>])</span><br><span class="line">    <span class="keyword">while</span> <span class="literal">True</span>:</span><br><span class="line">        np.random.shuffle(index)</span><br><span class="line">        images = []</span><br><span class="line">        image_fns = []</span><br><span class="line">        score_maps = []</span><br><span class="line">        geo_maps = []</span><br><span class="line">        training_masks = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> index:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                im_fn = image_list[i]</span><br><span class="line">                im = cv2.imread(im_fn)</span><br><span class="line">                <span class="comment"># print im_fn</span></span><br><span class="line">                h, w, _ = im.shape</span><br><span class="line">                txt_fn = im_fn.replace(os.path.basename(im_fn).split(<span class="string">'.'</span>)[<span class="number">1</span>], <span class="string">'txt'</span>)</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(txt_fn):</span><br><span class="line">                    <span class="built_in">print</span>(<span class="string">'text file {} does not exists'</span>.<span class="built_in">format</span>(txt_fn))</span><br><span class="line">                    <span class="keyword">continue</span></span><br><span class="line">                text_polys, text_tags = load_annoataion(txt_fn)</span><br><span class="line">                text_polys, text_tags = check_and_validate_polys(text_polys, text_tags, (h, w))</span><br><span class="line">                <span class="comment"># if text_polys.shape[0] == 0:</span></span><br><span class="line">                <span class="comment">#     continue</span></span><br><span class="line">                <span class="comment"># random scale this image</span></span><br><span class="line">                rd_scale = np.random.choice(random_scale)</span><br><span class="line">                im = cv2.resize(im, dsize=<span class="literal">None</span>, fx=rd_scale, fy=rd_scale)</span><br><span class="line">                text_polys *= rd_scale</span><br><span class="line">                <span class="comment"># print rd_scale</span></span><br><span class="line">                <span class="comment"># random crop a area from image</span></span><br><span class="line">                <span class="keyword">if</span> np.random.rand() &lt; background_ratio:</span><br><span class="line">                    <span class="comment"># crop background</span></span><br><span class="line">                    im, text_polys, text_tags = crop_area(im, text_polys, text_tags, crop_background=<span class="literal">True</span>)</span><br><span class="line">                    <span class="keyword">if</span> text_polys.shape[<span class="number">0</span>] &gt; <span class="number">0</span>:</span><br><span class="line">                        <span class="comment"># cannot find background</span></span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    <span class="comment"># pad and resize image</span></span><br><span class="line">                    new_h, new_w, _ = im.shape</span><br><span class="line">                    max_h_w_i = np.<span class="built_in">max</span>([new_h, new_w, input_size])</span><br><span class="line">                    im_padded = np.zeros((max_h_w_i, max_h_w_i, <span class="number">3</span>), dtype=np.uint8)</span><br><span class="line">                    im_padded[:new_h, :new_w, :] = im.copy()</span><br><span class="line">                    im = cv2.resize(im_padded, dsize=(input_size, input_size))</span><br><span class="line">                    score_map = np.zeros((input_size, input_size), dtype=np.uint8)</span><br><span class="line">                    geo_map_channels = <span class="number">5</span> <span class="keyword">if</span> FLAGS.geometry == <span class="string">'RBOX'</span> <span class="keyword">else</span> <span class="number">8</span></span><br><span class="line">                    geo_map = np.zeros((input_size, input_size, geo_map_channels), dtype=np.float32)</span><br><span class="line">                    training_mask = np.ones((input_size, input_size), dtype=np.uint8)</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    im, text_polys, text_tags = crop_area(im, text_polys, text_tags, crop_background=<span class="literal">False</span>)</span><br><span class="line">                    <span class="keyword">if</span> text_polys.shape[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">                        <span class="keyword">continue</span></span><br><span class="line">                    h, w, _ = im.shape</span><br><span class="line">                    <span class="comment"># pad the image to the training input size or the longer side of image</span></span><br><span class="line">                    new_h, new_w, _ = im.shape</span><br><span class="line">                    max_h_w_i = np.<span class="built_in">max</span>([new_h, new_w, input_size])</span><br><span class="line">                    im_padded = np.zeros((max_h_w_i, max_h_w_i, <span class="number">3</span>), dtype=np.uint8)</span><br><span class="line">                    im_padded[:new_h, :new_w, :] = im.copy()</span><br><span class="line">                    im = im_padded</span><br><span class="line">                    <span class="comment"># resize the image to input size</span></span><br><span class="line">                    new_h, new_w, _ = im.shape</span><br><span class="line">                    resize_h = input_size</span><br><span class="line">                    resize_w = input_size</span><br><span class="line">                    im = cv2.resize(im, dsize=(resize_w, resize_h))</span><br><span class="line">                    resize_ratio_3_x = resize_w/<span class="built_in">float</span>(new_w)</span><br><span class="line">                    resize_ratio_3_y = resize_h/<span class="built_in">float</span>(new_h)</span><br><span class="line">                    text_polys[:, :, <span class="number">0</span>] *= resize_ratio_3_x</span><br><span class="line">                    text_polys[:, :, <span class="number">1</span>] *= resize_ratio_3_y</span><br><span class="line">                    new_h, new_w, _ = im.shape</span><br><span class="line">                    score_map, geo_map, training_mask = generate_rbox((new_h, new_w), text_polys, text_tags)</span><br><span class="line">                <span class="keyword">if</span> vis:</span><br><span class="line">                    fig, axs = plt.subplots(<span class="number">3</span>, <span class="number">2</span>, figsize=(<span class="number">20</span>, <span class="number">30</span>))</span><br><span class="line">                    <span class="comment"># axs[0].imshow(im[:, :, ::-1])</span></span><br><span class="line">                    <span class="comment"># axs[0].set_xticks([])</span></span><br><span class="line">                    <span class="comment"># axs[0].set_yticks([])</span></span><br><span class="line">                    <span class="comment"># for poly in text_polys:</span></span><br><span class="line">                    <span class="comment">#     poly_h = min(abs(poly[3, 1] - poly[0, 1]), abs(poly[2, 1] - poly[1, 1]))</span></span><br><span class="line">                    <span class="comment">#     poly_w = min(abs(poly[1, 0] - poly[0, 0]), abs(poly[2, 0] - poly[3, 0]))</span></span><br><span class="line">                    <span class="comment">#     axs[0].add_artist(Patches.Polygon(</span></span><br><span class="line">                    <span class="comment">#         poly * 4, facecolor='none', edgecolor='green', linewidth=2, linestyle='-', fill=True))</span></span><br><span class="line">                    <span class="comment">#     axs[0].text(poly[0, 0] * 4, poly[0, 1] * 4, '{:.0f}-{:.0f}'.format(poly_h * 4, poly_w * 4),</span></span><br><span class="line">                    <span class="comment">#                    color='purple')</span></span><br><span class="line">                    <span class="comment"># axs[1].imshow(score_map)</span></span><br><span class="line">                    <span class="comment"># axs[1].set_xticks([])</span></span><br><span class="line">                    <span class="comment"># axs[1].set_yticks([])</span></span><br><span class="line">                    axs[<span class="number">0</span>, <span class="number">0</span>].imshow(im[:, :, ::-<span class="number">1</span>])</span><br><span class="line">                    axs[<span class="number">0</span>, <span class="number">0</span>].set_xticks([])</span><br><span class="line">                    axs[<span class="number">0</span>, <span class="number">0</span>].set_yticks([])</span><br><span class="line">                    <span class="keyword">for</span> poly <span class="keyword">in</span> text_polys:</span><br><span class="line">                        poly_h = <span class="built_in">min</span>(<span class="built_in">abs</span>(poly[<span class="number">3</span>, <span class="number">1</span>] - poly[<span class="number">0</span>, <span class="number">1</span>]), <span class="built_in">abs</span>(poly[<span class="number">2</span>, <span class="number">1</span>] - poly[<span class="number">1</span>, <span class="number">1</span>]))</span><br><span class="line">                        poly_w = <span class="built_in">min</span>(<span class="built_in">abs</span>(poly[<span class="number">1</span>, <span class="number">0</span>] - poly[<span class="number">0</span>, <span class="number">0</span>]), <span class="built_in">abs</span>(poly[<span class="number">2</span>, <span class="number">0</span>] - poly[<span class="number">3</span>, <span class="number">0</span>]))</span><br><span class="line">                        axs[<span class="number">0</span>, <span class="number">0</span>].add_artist(Patches.Polygon(</span><br><span class="line">                            poly, facecolor=<span class="string">'none'</span>, edgecolor=<span class="string">'green'</span>, linewidth=<span class="number">2</span>, linestyle=<span class="string">'-'</span>, fill=<span class="literal">True</span>))</span><br><span class="line">                        axs[<span class="number">0</span>, <span class="number">0</span>].text(poly[<span class="number">0</span>, <span class="number">0</span>], poly[<span class="number">0</span>, <span class="number">1</span>], <span class="string">'{:.0f}-{:.0f}'</span>.<span class="built_in">format</span>(poly_h, poly_w), color=<span class="string">'purple'</span>)</span><br><span class="line">                    axs[<span class="number">0</span>, <span class="number">1</span>].imshow(score_map[::, ::])</span><br><span class="line">                    axs[<span class="number">0</span>, <span class="number">1</span>].set_xticks([])</span><br><span class="line">                    axs[<span class="number">0</span>, <span class="number">1</span>].set_yticks([])</span><br><span class="line">                    axs[<span class="number">1</span>, <span class="number">0</span>].imshow(geo_map[::, ::, <span class="number">0</span>])</span><br><span class="line">                    axs[<span class="number">1</span>, <span class="number">0</span>].set_xticks([])</span><br><span class="line">                    axs[<span class="number">1</span>, <span class="number">0</span>].set_yticks([])</span><br><span class="line">                    axs[<span class="number">1</span>, <span class="number">1</span>].imshow(geo_map[::, ::, <span class="number">1</span>])</span><br><span class="line">                    axs[<span class="number">1</span>, <span class="number">1</span>].set_xticks([])</span><br><span class="line">                    axs[<span class="number">1</span>, <span class="number">1</span>].set_yticks([])</span><br><span class="line">                    axs[<span class="number">2</span>, <span class="number">0</span>].imshow(geo_map[::, ::, <span class="number">2</span>])</span><br><span class="line">                    axs[<span class="number">2</span>, <span class="number">0</span>].set_xticks([])</span><br><span class="line">                    axs[<span class="number">2</span>, <span class="number">0</span>].set_yticks([])</span><br><span class="line">                    axs[<span class="number">2</span>, <span class="number">1</span>].imshow(training_mask[::, ::])</span><br><span class="line">                    axs[<span class="number">2</span>, <span class="number">1</span>].set_xticks([])</span><br><span class="line">                    axs[<span class="number">2</span>, <span class="number">1</span>].set_yticks([])</span><br><span class="line">                    plt.tight_layout()</span><br><span class="line">                    plt.show()</span><br><span class="line">                    plt.close()</span><br><span class="line">                images.append(im[:, :, ::-<span class="number">1</span>].astype(np.float32))</span><br><span class="line">                image_fns.append(im_fn)</span><br><span class="line">                score_maps.append(score_map[::<span class="number">4</span>, ::<span class="number">4</span>, np.newaxis].astype(np.float32))</span><br><span class="line">                geo_maps.append(geo_map[::<span class="number">4</span>, ::<span class="number">4</span>, :].astype(np.float32))</span><br><span class="line">                training_masks.append(training_mask[::<span class="number">4</span>, ::<span class="number">4</span>, np.newaxis].astype(np.float32))</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">len</span>(images) == batch_size:</span><br><span class="line">                    <span class="keyword">yield</span> images, image_fns, score_maps, geo_maps, training_masks</span><br><span class="line">                    images = []</span><br><span class="line">                    image_fns = []</span><br><span class="line">                    score_maps = []</span><br><span class="line">                    geo_maps = []</span><br><span class="line">                    training_masks = []</span><br><span class="line">            <span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">                <span class="keyword">import</span> traceback</span><br><span class="line">                traceback.print_exc()</span><br><span class="line">                <span class="keyword">continue</span></span><br></pre></td></tr></tbody></table></figure><h3 id="网络模型的搭建"><a href="#网络模型的搭建" class="headerlink" title="网络模型的搭建"></a>网络模型的搭建</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">model</span>(<span class="params">images, weight_decay=<span class="number">1e-5</span>, is_training=<span class="literal">True</span></span>):</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    define the model, we use slim's implemention of resnet</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    images = mean_image_subtraction(images)</span><br><span class="line">    <span class="keyword">with</span> slim.arg_scope(resnet_v1.resnet_arg_scope(weight_decay=weight_decay)):</span><br><span class="line">        logits, end_points = resnet_v1.resnet_v1_50(images, is_training=is_training, scope=<span class="string">'resnet_v1_50'</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.variable_scope(<span class="string">'feature_fusion'</span>, values=[end_points.values]):</span><br><span class="line">        batch_norm_params = {</span><br><span class="line">        <span class="string">'decay'</span>: <span class="number">0.997</span>,</span><br><span class="line">        <span class="string">'epsilon'</span>: <span class="number">1e-5</span>,</span><br><span class="line">        <span class="string">'scale'</span>: <span class="literal">True</span>,</span><br><span class="line">        <span class="string">'is_training'</span>: is_training</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">with</span> slim.arg_scope([slim.conv2d],</span><br><span class="line">                            activation_fn=tf.nn.relu,</span><br><span class="line">                            normalizer_fn=slim.batch_norm,</span><br><span class="line">                            normalizer_params=batch_norm_params,</span><br><span class="line">                            weights_regularizer=slim.l2_regularizer(weight_decay)):</span><br><span class="line">            f = [end_points[<span class="string">'pool5'</span>], end_points[<span class="string">'pool4'</span>],</span><br><span class="line">                 end_points[<span class="string">'pool3'</span>], end_points[<span class="string">'pool2'</span>]]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">'Shape of f_{} {}'</span>.<span class="built_in">format</span>(i, f[i].shape))</span><br><span class="line">            g = [<span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">            h = [<span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>]</span><br><span class="line">            num_outputs = [<span class="literal">None</span>, <span class="number">128</span>, <span class="number">64</span>, <span class="number">32</span>]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">4</span>):</span><br><span class="line">                <span class="keyword">if</span> i == <span class="number">0</span>:</span><br><span class="line">                    h[i] = f[i]</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    c1_1 = slim.conv2d(tf.concat([g[i-<span class="number">1</span>], f[i]], axis=-<span class="number">1</span>), num_outputs[i], <span class="number">1</span>)</span><br><span class="line">                    h[i] = slim.conv2d(c1_1, num_outputs[i], <span class="number">3</span>)</span><br><span class="line">                <span class="keyword">if</span> i &lt;= <span class="number">2</span>:</span><br><span class="line">                    g[i] = unpool(h[i])</span><br><span class="line">                <span class="keyword">else</span>:</span><br><span class="line">                    g[i] = slim.conv2d(h[i], num_outputs[i], <span class="number">3</span>)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">'Shape of h_{} {}, g_{} {}'</span>.<span class="built_in">format</span>(i, h[i].shape, i, g[i].shape))</span><br><span class="line">            <span class="comment"># here we use a slightly different way for regression part,</span></span><br><span class="line">            <span class="comment"># we first use a sigmoid to limit the regression range, and also</span></span><br><span class="line">            <span class="comment"># this is do with the angle map</span></span><br><span class="line">            F_score = slim.conv2d(g[<span class="number">3</span>], <span class="number">1</span>, <span class="number">1</span>, activation_fn=tf.nn.sigmoid, normalizer_fn=<span class="literal">None</span>)</span><br><span class="line">            <span class="comment"># 4 channel of axis aligned bbox and 1 channel rotation angle</span></span><br><span class="line">            geo_map = slim.conv2d(g[<span class="number">3</span>], <span class="number">4</span>, <span class="number">1</span>, activation_fn=tf.nn.sigmoid, normalizer_fn=<span class="literal">None</span>) * FLAGS.text_scale</span><br><span class="line">            angle_map = (slim.conv2d(g[<span class="number">3</span>], <span class="number">1</span>, <span class="number">1</span>, activation_fn=tf.nn.sigmoid, normalizer_fn=<span class="literal">None</span>) - <span class="number">0.5</span>) * np.pi/<span class="number">2</span> <span class="comment"># angle is between [-45, 45]</span></span><br><span class="line">            F_geometry = tf.concat([geo_map, angle_map], axis=-<span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> F_score, F_geometry</span><br></pre></td></tr></tbody></table></figure><h3 id="loss函数的设计"><a href="#loss函数的设计" class="headerlink" title="loss函数的设计"></a>loss函数的设计</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">loss</span>(<span class="params">y_true_cls, y_pred_cls,</span></span><br><span class="line"><span class="params">         y_true_geo, y_pred_geo,</span></span><br><span class="line"><span class="params">         training_mask</span>):</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    define the loss used for training, contraning two part,</span></span><br><span class="line"><span class="string">    the first part we use dice loss instead of weighted logloss,</span></span><br><span class="line"><span class="string">    the second part is the iou loss defined in the paper</span></span><br><span class="line"><span class="string">    :param y_true_cls: ground truth of text</span></span><br><span class="line"><span class="string">    :param y_pred_cls: prediction os text</span></span><br><span class="line"><span class="string">    :param y_true_geo: ground truth of geometry</span></span><br><span class="line"><span class="string">    :param y_pred_geo: prediction of geometry</span></span><br><span class="line"><span class="string">    :param training_mask: mask used in training, to ignore some text annotated by ###</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    classification_loss = dice_coefficient(y_true_cls, y_pred_cls, training_mask)</span><br><span class="line">    <span class="comment"># scale classification loss to match the iou loss part</span></span><br><span class="line">    classification_loss *= <span class="number">0.01</span></span><br><span class="line">    <span class="comment"># d1 -&gt; top, d2-&gt;right, d3-&gt;bottom, d4-&gt;left</span></span><br><span class="line">    d1_gt, d2_gt, d3_gt, d4_gt, theta_gt = tf.split(value=y_true_geo, num_or_size_splits=<span class="number">5</span>, axis=<span class="number">3</span>)</span><br><span class="line">    d1_pred, d2_pred, d3_pred, d4_pred, theta_pred = tf.split(value=y_pred_geo, num_or_size_splits=<span class="number">5</span>, axis=<span class="number">3</span>)</span><br><span class="line">    area_gt = (d1_gt + d3_gt) * (d2_gt + d4_gt)</span><br><span class="line">    area_pred = (d1_pred + d3_pred) * (d2_pred + d4_pred)</span><br><span class="line">    w_union = tf.minimum(d2_gt, d2_pred) + tf.minimum(d4_gt, d4_pred)</span><br><span class="line">    h_union = tf.minimum(d1_gt, d1_pred) + tf.minimum(d3_gt, d3_pred)</span><br><span class="line">    area_intersect = w_union * h_union</span><br><span class="line">    area_union = area_gt + area_pred - area_intersect</span><br><span class="line">    L_AABB = -tf.log((area_intersect + <span class="number">1.0</span>)/(area_union + <span class="number">1.0</span>))</span><br><span class="line">    L_theta = <span class="number">1</span> - tf.cos(theta_pred - theta_gt)</span><br><span class="line">    tf.summary.scalar(<span class="string">'geometry_AABB'</span>, tf.reduce_mean(L_AABB * y_true_cls * training_mask))</span><br><span class="line">    tf.summary.scalar(<span class="string">'geometry_theta'</span>, tf.reduce_mean(L_theta * y_true_cls * training_mask))</span><br><span class="line">    L_g = L_AABB + <span class="number">20</span> * L_theta</span><br><span class="line">    <span class="keyword">return</span> tf.reduce_mean(L_g * y_true_cls * training_mask) + classification_loss</span><br></pre></td></tr></tbody></table></figure><h3 id="train"><a href="#train" class="headerlink" title="train"></a>train</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>(<span class="params">argv=<span class="literal">None</span></span>):</span><br><span class="line">    <span class="keyword">import</span> os</span><br><span class="line">    os.environ[<span class="string">'CUDA_VISIBLE_DEVICES'</span>] = FLAGS.gpu_list</span><br><span class="line">    config = <span class="literal">None</span></span><br><span class="line">    config.batch_size = FLAGS.batch_size_per_gpu * FLAGS.num_gpus</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> tf.gfile.Exists(FLAGS.checkpoint_path):</span><br><span class="line">        tf.gfile.MkDir(FLAGS.checkpoint_path)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">if</span> <span class="keyword">not</span> FLAGS.restore:</span><br><span class="line">            tf.gfile.DeleteRecursively(FLAGS.checkpoint_path)</span><br><span class="line">            tf.gfile.MkDir(FLAGS.checkpoint_path)</span><br><span class="line">    input_images = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="number">3</span>], name=<span class="string">'input_images'</span>)</span><br><span class="line">    input_score_maps = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="number">1</span>], name=<span class="string">'input_score_maps'</span>)</span><br><span class="line">    <span class="keyword">if</span> FLAGS.geometry == <span class="string">'RBOX'</span>:</span><br><span class="line">        input_geo_maps = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="number">5</span>], name=<span class="string">'input_geo_maps'</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        input_geo_maps = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="number">8</span>], name=<span class="string">'input_geo_maps'</span>)</span><br><span class="line">    input_training_masks = tf.placeholder(tf.float32, shape=[<span class="literal">None</span>, <span class="literal">None</span>, <span class="literal">None</span>, <span class="number">1</span>], name=<span class="string">'input_training_masks'</span>)</span><br><span class="line">    global_step = tf.get_variable(<span class="string">'global_step'</span>, [], initializer=tf.constant_initializer(<span class="number">0</span>), trainable=<span class="literal">False</span>)</span><br><span class="line">    learning_rate = tf.train.exponential_decay(FLAGS.learning_rate, global_step, decay_steps=<span class="number">10000</span>, decay_rate=<span class="number">0.94</span>, staircase=<span class="literal">True</span>)</span><br><span class="line">    <span class="comment"># add summary</span></span><br><span class="line">    tf.summary.scalar(<span class="string">'learning_rate'</span>, learning_rate)</span><br><span class="line">    opt = tf.train.AdamOptimizer(learning_rate)</span><br><span class="line">    <span class="comment"># opt = tf.train.MomentumOptimizer(learning_rate, 0.9)</span></span><br><span class="line">    <span class="comment"># split</span></span><br><span class="line">    input_images_split = tf.split(input_images, <span class="built_in">len</span>(gpus))</span><br><span class="line">    input_score_maps_split = tf.split(input_score_maps, <span class="built_in">len</span>(gpus))</span><br><span class="line">    input_geo_maps_split = tf.split(input_geo_maps, <span class="built_in">len</span>(gpus))</span><br><span class="line">    input_training_masks_split = tf.split(input_training_masks, <span class="built_in">len</span>(gpus))</span><br><span class="line">    tower_grads = []</span><br><span class="line">    reuse_variables = <span class="literal">None</span></span><br><span class="line">    <span class="keyword">for</span> i, gpu_id <span class="keyword">in</span> <span class="built_in">enumerate</span>(gpus):</span><br><span class="line">        <span class="keyword">with</span> tf.device(<span class="string">'/gpu:%d'</span> % gpu_id):</span><br><span class="line">            <span class="keyword">with</span> tf.name_scope(<span class="string">'model_%d'</span> % gpu_id) <span class="keyword">as</span> scope:</span><br><span class="line">                iis = input_images_split[i]</span><br><span class="line">                isms = input_score_maps_split[i]</span><br><span class="line">                igms = input_geo_maps_split[i]</span><br><span class="line">                itms = input_training_masks_split[i]</span><br><span class="line">                total_loss, model_loss = tower_loss(iis, isms, igms, itms, reuse_variables)</span><br><span class="line">                batch_norm_updates_op = tf.group(*tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope))</span><br><span class="line">                reuse_variables = <span class="literal">True</span></span><br><span class="line">                grads = opt.compute_gradients(total_loss)</span><br><span class="line">                tower_grads.append(grads)</span><br><span class="line">    grads = average_gradients(tower_grads)</span><br><span class="line">    apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)</span><br><span class="line">    summary_op = tf.summary.merge_all()</span><br><span class="line">    <span class="comment"># save moving average</span></span><br><span class="line">    variable_averages = tf.train.ExponentialMovingAverage(</span><br><span class="line">        FLAGS.moving_average_decay, global_step)</span><br><span class="line">    variables_averages_op = variable_averages.apply(tf.trainable_variables())</span><br><span class="line">    <span class="comment"># batch norm updates</span></span><br><span class="line">    <span class="keyword">with</span> tf.control_dependencies([variables_averages_op, apply_gradient_op, batch_norm_updates_op]):</span><br><span class="line">        train_op = tf.no_op(name=<span class="string">'train_op'</span>)</span><br><span class="line">    saver = tf.train.Saver(tf.global_variables())</span><br><span class="line">    summary_writer = tf.summary.FileWriter(FLAGS.checkpoint_path, tf.get_default_graph())</span><br><span class="line">    init = tf.global_variables_initializer()</span><br><span class="line">    <span class="keyword">if</span> FLAGS.pretrained_model_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        variable_restore_op = slim.assign_from_checkpoint_fn(FLAGS.pretrained_model_path, slim.get_trainable_variables(),</span><br><span class="line">                                                            ignore_missing_vars=<span class="literal">True</span>)</span><br><span class="line">    <span class="keyword">with</span> tf.Session(config=tf.ConfigProto(allow_soft_placement=<span class="literal">True</span>)) <span class="keyword">as</span> sess:</span><br><span class="line">        <span class="keyword">if</span> FLAGS.restore:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">'continue training from previous checkpoint'</span>)</span><br><span class="line">            ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_path)</span><br><span class="line">            saver.restore(sess, ckpt)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            sess.run(init)</span><br><span class="line">            <span class="keyword">if</span> FLAGS.pretrained_model_path <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                variable_restore_op(sess)</span><br><span class="line">        <span class="comment"># data_generator = icdar.get_batch(num_workers=FLAGS.num_readers,</span></span><br><span class="line">        <span class="comment">#                                 input_size=FLAGS.input_size,</span></span><br><span class="line">        <span class="comment">#                                 batch_size=FLAGS.batch_size_per_gpu * len(gpus))</span></span><br><span class="line">        train_data_generator = icdar_single.get_batch_seq(num_workers=FLAGS.num_readers, config=config, is_training=<span class="literal">True</span>)</span><br><span class="line">        start = time.time()</span><br><span class="line">        <span class="keyword">for</span> step <span class="keyword">in</span> <span class="built_in">range</span>(FLAGS.max_steps):</span><br><span class="line">            data = <span class="built_in">next</span>(train_data_generator)</span><br><span class="line">            ml, tl, _ = sess.run([model_loss, total_loss, train_op], feed_dict={input_images: data[<span class="number">0</span>],</span><br><span class="line">                                                                                input_score_maps: data[<span class="number">2</span>],</span><br><span class="line">                                                                                input_geo_maps: data[<span class="number">3</span>],</span><br><span class="line">                                                                                input_training_masks: data[<span class="number">4</span>]})</span><br><span class="line">            <span class="keyword">if</span> np.isnan(tl):</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">'Loss diverged, stop training'</span>)</span><br><span class="line">                <span class="keyword">break</span></span><br><span class="line">            <span class="keyword">if</span> step % <span class="number">10</span> == <span class="number">0</span>:</span><br><span class="line">                avg_time_per_step = (time.time() - start)/<span class="number">10</span></span><br><span class="line">                avg_examples_per_second = (<span class="number">10</span> * FLAGS.batch_size_per_gpu * <span class="built_in">len</span>(gpus))/(time.time() - start)</span><br><span class="line">                start = time.time()</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">'Step {:06d}, model loss {:.4f}, total loss {:.4f}, {:.2f} seconds/step, {:.2f} examples/second'</span>.<span class="built_in">format</span>(</span><br><span class="line">                    step, ml, tl, avg_time_per_step, avg_examples_per_second))</span><br><span class="line">            <span class="keyword">if</span> step % FLAGS.save_checkpoint_steps == <span class="number">0</span>:</span><br><span class="line">                saver.save(sess, FLAGS.checkpoint_path + <span class="string">'model.ckpt'</span>, global_step=global_step)</span><br><span class="line">            <span class="keyword">if</span> step % FLAGS.save_summary_steps == <span class="number">0</span>:</span><br><span class="line">                _, tl, summary_str = sess.run([train_op, total_loss, summary_op], feed_dict={input_images: data[<span class="number">0</span>],</span><br><span class="line">                                                                                            input_score_maps: data[<span class="number">2</span>],</span><br><span class="line">                                                                                            input_geo_maps: data[<span class="number">3</span>],</span><br><span class="line">                                                                                            input_training_masks: data[<span class="number">4</span>]})</span><br><span class="line">                summary_writer.add_summary(summary_str, global_step=step)</span><br></pre></td></tr></tbody></table></figure><p>参考链接：<br><a href="https://github.com/argman/EAST">https://github.com/argman/EAST</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>tensorflow自定义网络模型</title>
      <link href="/2019/10/17/tensorflow%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/"/>
      <url>/2019/10/17/tensorflow%E8%87%AA%E5%AE%9A%E4%B9%89%E7%BD%91%E7%BB%9C%E6%A8%A1%E5%9E%8B/</url>
      
        <content type="html"><![CDATA[<h2 id="Slim"><a href="#Slim" class="headerlink" title="Slim"></a>Slim</h2><p>TF-Slim 模块是 TensorFlow 中最好用的 API 之一。尤其是里面引入的 arg_scope、model_variables、repeat、stack。<br>TF-Slim 是 TensorFlow 中一个用来构建、训练、评估复杂模型的轻量化库。TF-Slim 模块可以和 TensorFlow 中其它API混合使用。</p><h3 id="Slim模块的导入"><a href="#Slim模块的导入" class="headerlink" title="Slim模块的导入"></a>Slim模块的导入</h3><pre><code>1  </code></pre><p>| </p><pre><code>import tensorflow.contrib.slim as slim    </code></pre><p>—|—  </p><h3 id="Slim-构建模型"><a href="#Slim-构建模型" class="headerlink" title="Slim 构建模型"></a>Slim 构建模型</h3><p>可以用 slim、variables、layers 和 scopes 来十分简洁地定义模型。下面对各个部分进行了详细描述：</p><h4 id="Slim变量（Variables）"><a href="#Slim变量（Variables）" class="headerlink" title="Slim变量（Variables）"></a>Slim变量（Variables）</h4><pre><code>1  2  3  4  5  6  </code></pre><p>| </p><pre><code>weights = slim.variable('weights',                          shape=[10, 10, 3 , 3],                          initializer=tf.truncated_normal_initializer(stddev=0.1),                          regularizer=slim.l2_regularizer(0.05),                          device='/CPU:0')  ~    </code></pre><p>—|—  </p><h4 id="Slim-层（Layers）"><a href="#Slim-层（Layers）" class="headerlink" title="Slim 层（Layers）"></a>Slim 层（Layers）</h4><p>使用基础（plain）的 TensorFlow 代码：  </p><pre><code>1  2  3  4  5  6  7  8  9  </code></pre><p>| </p><pre><code>input = ...  with tf.name_scope('conv1_1') as scope:    kernel = tf.Variable(tf.truncated_normal([3, 3, 64, 128], dtype=tf.float32,                                             stddev=1e-1), name='weights')    conv = tf.nn.conv2d(input, kernel, [1, 1, 1, 1], padding='SAME')    biases = tf.Variable(tf.constant(0.0, shape=[128], dtype=tf.float32),                         trainable=True, name='biases')    bias = tf.nn.bias_add(conv, biases)    conv1 = tf.nn.relu(bias, name=scope)    </code></pre><p>—|—  </p><p>为了避免代码的重复。Slim 提供了很多方便的神经网络 layers 的高层 op。例如：与上面的代码对应的 Slim 版的代码：  </p><pre><code>1  2  </code></pre><p>| </p><pre><code>input = ...  net = slim.conv2d(input, 128, [3, 3], scope='conv1_1')    </code></pre><p>—|—  </p><h3 id="slim-arg-scope（）-函数的使用"><a href="#slim-arg-scope（）-函数的使用" class="headerlink" title="slim.arg_scope（） 函数的使用"></a>slim.arg_scope（） 函数的使用</h3><p>这个函数的作用是给list_ops中的内容设置默认值。但是每个list_ops中的每个成员需要用@add_arg_scope修饰才行。所以使用slim.arg_scope（）有两个步骤：</p><ul><li><p>使用@slim.add_arg_scope修饰目标函数</p></li><li><p>用 slim.arg_scope（）为目标函数设置默认参数.<br>例如如下代码；首先用@slim.add_arg_scope修饰目标函数fun1（），然后利用slim.arg_scope（）为它设置默认参数。</p><pre><code>1  2  3  4  5  6  7  8  9  10</code></pre></li></ul><p>| </p><pre><code>    import tensorflow as tf      slim =tf.contrib.slim             @slim.add_arg_scope      def fun1(a=0,b=0):          return (a+b)             with slim.arg_scope([fun1],a=10):          x=fun1(b=30)          print(x)        </code></pre><p>—|—  </p><p>运行结果:40<br>参考链接：<br><a href="https://blog.csdn.net/u013921430/article/details/80915696">https://blog.csdn.net/u013921430/article/details/80915696</a></p><h3 id="其他用法见参考链接"><a href="#其他用法见参考链接" class="headerlink" title="其他用法见参考链接"></a>其他用法见参考链接</h3><p><a href="https://blog.csdn.net/wanttifa/article/details/90208398">https://blog.csdn.net/wanttifa/article/details/90208398</a></p><h2 id="查看ckpt中变量的几种方法"><a href="#查看ckpt中变量的几种方法" class="headerlink" title="查看ckpt中变量的几种方法"></a>查看ckpt中变量的几种方法</h2><p>查看ckpt中变量的方法有三种：</p><ul><li>在有model的情况下，使用tf.train.Saver进行restore</li><li>使用tf.train.NewCheckpointReader直接读取ckpt文件，这种方法不需要model。</li><li>使用tools里的freeze_graph来读取ckpt<br>Tips:</li><li>如果模型保存为.ckpt的文件，则使用该文件就可以查看.ckpt文件里的变量。ckpt路径为 model.ckpt</li><li>如果模型保存为.ckpt-xxx-data (图结构)、.ckpt-xxx.index (参数名)、.ckpt-xxx-meta (参数值)文件，则需要同时拥有这三个文件才行。并且ckpt的路径为 model.ckpt-xxx</li></ul><h3 id="1-基于model来读取ckpt文件里的变量"><a href="#1-基于model来读取ckpt文件里的变量" class="headerlink" title="1.基于model来读取ckpt文件里的变量"></a>1.基于model来读取ckpt文件里的变量</h3><p>1.首先建立起model<br>2.从ckpt中恢复变量</p><pre><code>    1      2      3      4      5      6      7      8      9      10      </code></pre><p>| </p><pre><code>    with tf.Graph().as_default() as g:         #建立model        images, labels = cifar10.inputs(eval_data=eval_data)         logits = cifar10.inference(images)         top_k_op = tf.nn.in_top_k(logits, labels, 1)         #从ckpt中恢复变量        sess = tf.Session()        saver = tf.train.Saver() #saver = tf.train.Saver(...variables...) # 恢复部分变量时，只需要在Saver里指定要恢复的变量        save_path = 'ckpt的路径'        saver.restore(sess, save_path) # 从ckpt中恢复变量        </code></pre><p>—|—  </p><p>注意：基于model来读取ckpt中变量时，model和ckpt必须匹配。</p><h3 id="2-使用tf-train-NewCheckpointReader直接读取ckpt文件里的变量，使用tools-inspect-checkpoint里的print-tensors-in-checkpoint-file函数打印ckpt里的东西"><a href="#2-使用tf-train-NewCheckpointReader直接读取ckpt文件里的变量，使用tools-inspect-checkpoint里的print-tensors-in-checkpoint-file函数打印ckpt里的东西" class="headerlink" title="2.使用tf.train.NewCheckpointReader直接读取ckpt文件里的变量，使用tools.inspect_checkpoint里的print_tensors_in_checkpoint_file函数打印ckpt里的东西"></a>2.使用tf.train.NewCheckpointReader直接读取ckpt文件里的变量，使用tools.inspect_checkpoint里的print_tensors_in_checkpoint_file函数打印ckpt里的东西</h3><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  </code></pre><p>| </p><pre><code>#使用NewCheckpointReader来读取ckpt里的变量  from tensorflow.python import pywrap_tensorflow  checkpoint_path = os.path.join(model_dir, "model.ckpt")  reader = pywrap_tensorflow.NewCheckpointReader(checkpoint_path) #tf.train.NewCheckpointReader  var_to_shape_map = reader.get_variable_to_shape_map()  for key in var_to_shape_map:    print("tensor_name: ", key)    #print(reader.get_tensor(key))  #使用print_tensors_in_checkpoint_file打印ckpt里的内容  from tensorflow.python.tools.inspect_checkpoint import print_tensors_in_checkpoint_file    print_tensors_in_checkpoint_file(file_name, #ckpt文件名字                   tensor_name, # 如果为None,则默认为ckpt里的所有变量                   all_tensors, # bool 是否打印所有的tensor，这里打印出的是tensor的值，一般不推荐这里设置为False                   all_tensor_names) # bool 是否打印所有的tensor的name  #上面的打印ckpt的内部使用的是pywrap_tensorflow.NewCheckpointReader所以要掌握NewCheckpointReader    </code></pre><p>—|—  </p><h3 id="3-使用tools里的freeze-graph来读取ckpt"><a href="#3-使用tools里的freeze-graph来读取ckpt" class="headerlink" title="3.使用tools里的freeze_graph来读取ckpt"></a>3.使用tools里的freeze_graph来读取ckpt</h3><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  </code></pre><p>| </p><pre><code>from tensorflow.python.tools import freeze_graph    freeze_graph(input_graph, #=some_graph_def.pb         input_saver,          input_binary,          input_checkpoint, #=model.ckpt         output_node_names, #=softmax         restore_op_name,          filename_tensor_name,          output_graph, #='./tmp/frozen_graph.pb'         clear_devices,          initializer_nodes,          variable_names_whitelist='',          variable_names_blacklist='',          input_meta_graph=None,          input_saved_model_dir=None,          saved_model_tags='serve',          checkpoint_version=2)  #freeze_graph_test.py讲述了怎么使用freeze_grapg。    </code></pre><p>—|—  </p><p>参考链接：<br><a href="https://www.jb51.net/article/142183.htm">https://www.jb51.net/article/142183.htm</a></p><h2 id="control-dependencies"><a href="#control-dependencies" class="headerlink" title="control_dependencies"></a>control_dependencies</h2><p>tf.control_dependencies(control_inputs)<br>Wrapper for Graph.control_dependencies() using the default graph.<br>See Graph.control_dependencies() for more details.<br>此函数指定某些操作执行的依赖关系<br>返回一个控制依赖的上下文管理器，使用 with 关键字可以让在这个上下文环境中的操作都在 control_inputs 执行  </p><pre><code>1  2  3  </code></pre><p>| </p><pre><code>1 with tf.control_dependencies([a, b]):  2     c = ....  3     d = ...    </code></pre><p>—|—  </p><p>在执行完 a，b 操作之后，才能执行 c，d 操作。意思就是 c，d 操作依赖 a，b 操作  </p><pre><code>1  2  </code></pre><p>| </p><pre><code>1 with tf.control_dependencies([train_step, variable_averages_op]):  2     train_op = tf.no_op(name='train')    </code></pre><p>—|—  </p><p>tf.no_op()表示执行完 train_step, variable_averages_op 操作之后什么都不做<br>参考链接：<br><a href="http://www.tensorfly.cn/tfdoc/api_docs/python/framework.html#Graph.control_dependencies">http://www.tensorfly.cn/tfdoc/api_docs/python/framework.html#Graph.control_dependencies</a></p><h2 id="TensorBoard"><a href="#TensorBoard" class="headerlink" title="TensorBoard"></a>TensorBoard</h2><h3 id="在TensorBoard中可视化图形"><a href="#在TensorBoard中可视化图形" class="headerlink" title="在TensorBoard中可视化图形"></a>在TensorBoard中可视化图形</h3><p>构建您的网络，创建一个会话(session)，然后创建一个TensorFlow File Writer对象<br>File Writer定义存储TensorBoard文件的路径，以及TensorFlow graph对象sess.graph是第二个参数。  </p><pre><code>1  </code></pre><p>| </p><pre><code>writer = tf.summary.FileWriter(STORE_PATH, sess.graph)    </code></pre><p>—|—  </p><p>当创建一个TensorFlow网络后，定义并运行File Writer时，就可以启动TensorBoard来可视化图形。要定义File Writer并将图形发送给它，运行以下命令:  </p><pre><code>1  2  3  </code></pre><p>| </p><pre><code># start the session  with tf.Session() as sess:   writer = tf.summary.FileWriter(STORE_PATH, sess.graph)    </code></pre><p>—|—  </p><h3 id="启动TensorBoard"><a href="#启动TensorBoard" class="headerlink" title="启动TensorBoard"></a>启动TensorBoard</h3><pre><code>1  </code></pre><p>| </p><pre><code>tensorboard --logdir=STORE_PATH    </code></pre><p>—|—  </p><h3 id="名称空间（Namespaces）"><a href="#名称空间（Namespaces）" class="headerlink" title="名称空间（Namespaces）"></a>名称空间（Namespaces）</h3><p>名称空间是一种作用域，可以用它来包围图形组件，以便将它们组合在一起。通过这样的操作，名称空间中的细节将被折叠成TensorBoard计算图形可视化中的单个名称空间节点。要在TensorFlow中创建名称空间，可以使用Python with功能，如下所示：  </p><pre><code>1  2  3  4  5  6  </code></pre><p>| </p><pre><code>with tf.name_scope("layer_1"):  # now declare the weights connecting the input to the hidden  layer   W1 = tf.Variable(tf.random_normal([784, 300], stddev=0.01), name='W')           b1 = tf.Variable(tf.random_normal([300]), name='b')           hidden_logits = tf.add(tf.matmul(x_sc, W1), b1)           hidden_out = tf.nn.sigmoid(hidden_logits)    </code></pre><p>—|—  </p><p>还可以使用tf.variable_scope()代替tf.name_scope()。变量作用域是TensorFlow中的get_variable()变量共享机制的一部分。</p><h3 id="标量总结（Scalar-summaries）"><a href="#标量总结（Scalar-summaries）" class="headerlink" title="标量总结（Scalar summaries）"></a>标量总结（Scalar summaries）</h3><p>在网络中的任何位置，都可以记录标量(即单个实值)数量，以便在TensorBoard中显示。这对于跟踪诸如训练准确率的提高或损失函数的减少，或研究分布的标准差等方面都很有用。执行起来很容易。例如，下面的代码展示了如何在这个图中记录accuracy标量:  </p><pre><code>1  2  </code></pre><p>| </p><pre><code># add a summary to store the   accuracytf.summary.scalar('acc_summary', accuracy)    </code></pre><p>—|—  </p><p>第一个参数是要在TensorBoard可视化中给出标量的名称，第二个参数是要记录的操作(必须返回一个实值)。scalar()调用的输出是一个操作。在上面的代码中，我没有将这个操作分配给Python中的任何变量，但是如果用户愿意，可以这样做。然而，与TensorFlow中的其他操作一样，这些汇总操作在运行之前不会执行任何操作。根据开发人员想要观察的内容，在任何给定的图中通常都会运行许多可视化函数，因此有一个方便的助手函数merge_all()。这将把图中的所有函数调用合并在一起，这样您只需调用merge操作，它将为您收集所有其他函数操作并记录数据。它是这样的:  </p><pre><code>1  </code></pre><p>| </p><pre><code>merged = tf.summary.merge_all()    </code></pre><p>—|—  </p><h3 id="图像可视化"><a href="#图像可视化" class="headerlink" title="图像可视化"></a>图像可视化</h3><pre><code>1  2  3  4  5  6  7  8  9  10  </code></pre><p>| </p><pre><code># add summary  if reuse_variables is None:      tf.summary.image('input', images)      tf.summary.image('score_map', score_maps)      tf.summary.image('score_map_pred', f_score * 255)      tf.summary.image('geo_map_0', geo_maps[:, :, :, 0:1])      tf.summary.image('geo_map_0_pred', f_geometry[:, :, :, 0:1])      tf.summary.image('training_masks', training_masks)      tf.summary.scalar('model_loss', model_loss)      tf.summary.scalar('total_loss', total_loss)    </code></pre><p>—|—  </p><h2 id="文本检测模型EAST的搭建"><a href="#文本检测模型EAST的搭建" class="headerlink" title="文本检测模型EAST的搭建"></a>文本检测模型EAST的搭建</h2><h3 id="数据加载"><a href="#数据加载" class="headerlink" title="数据加载"></a>数据加载</h3><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160  161  162  </code></pre><p>| </p><pre><code>def load_annoataion(p):      '''      load annotation from the text file      :param p:      :return:      '''      text_polys = []      text_tags = []      if not os.path.exists(p):          return np.array(text_polys, dtype=np.float32)      with open(p, 'r') as f:          reader = csv.reader(f)          for line in reader:              label = line[-1]              # strip BOM. \ufeff for python3,  \xef\xbb\bf for python2              line = [i.strip('\ufeff').strip('\xef\xbb\xbf') for i in line]                x1, y1, x2, y2, x3, y3, x4, y4 = list(map(float, line[:8]))              text_polys.append([[x1, y1], [x2, y2], [x3, y3], [x4, y4]])              if label == '*' or label == '###':                  text_tags.append(True)              else:                  text_tags.append(False)          return np.array(text_polys, dtype=np.float32), np.array(text_tags, dtype=np.bool)            def generator(input_size=512, batch_size=32,              background_ratio=3./8,              random_scale=np.array([0.5, 1, 2.0, 3.0]),              vis=False):      image_list = np.array(get_images())      print('{} training images in {}'.format(          image_list.shape[0], FLAGS.training_data_path))      index = np.arange(0, image_list.shape[0])      while True:          np.random.shuffle(index)          images = []          image_fns = []          score_maps = []          geo_maps = []          training_masks = []          for i in index:              try:                  im_fn = image_list[i]                  im = cv2.imread(im_fn)                  # print im_fn                  h, w, _ = im.shape                  txt_fn = im_fn.replace(os.path.basename(im_fn).split('.')[1], 'txt')                  if not os.path.exists(txt_fn):                      print('text file {} does not exists'.format(txt_fn))                      continue                    text_polys, text_tags = load_annoataion(txt_fn)                    text_polys, text_tags = check_and_validate_polys(text_polys, text_tags, (h, w))                  # if text_polys.shape[0] == 0:                  #     continue                  # random scale this image                  rd_scale = np.random.choice(random_scale)                  im = cv2.resize(im, dsize=None, fx=rd_scale, fy=rd_scale)                  text_polys *= rd_scale                  # print rd_scale                  # random crop a area from image                  if np.random.rand() &lt; background_ratio:                      # crop background                      im, text_polys, text_tags = crop_area(im, text_polys, text_tags, crop_background=True)                      if text_polys.shape[0] &gt; 0:                          # cannot find background                          continue                      # pad and resize image                      new_h, new_w, _ = im.shape                      max_h_w_i = np.max([new_h, new_w, input_size])                      im_padded = np.zeros((max_h_w_i, max_h_w_i, 3), dtype=np.uint8)                      im_padded[:new_h, :new_w, :] = im.copy()                      im = cv2.resize(im_padded, dsize=(input_size, input_size))                      score_map = np.zeros((input_size, input_size), dtype=np.uint8)                      geo_map_channels = 5 if FLAGS.geometry == 'RBOX' else 8                      geo_map = np.zeros((input_size, input_size, geo_map_channels), dtype=np.float32)                      training_mask = np.ones((input_size, input_size), dtype=np.uint8)                  else:                      im, text_polys, text_tags = crop_area(im, text_polys, text_tags, crop_background=False)                      if text_polys.shape[0] == 0:                          continue                      h, w, _ = im.shape                        # pad the image to the training input size or the longer side of image                      new_h, new_w, _ = im.shape                      max_h_w_i = np.max([new_h, new_w, input_size])                      im_padded = np.zeros((max_h_w_i, max_h_w_i, 3), dtype=np.uint8)                      im_padded[:new_h, :new_w, :] = im.copy()                      im = im_padded                      # resize the image to input size                      new_h, new_w, _ = im.shape                      resize_h = input_size                      resize_w = input_size                      im = cv2.resize(im, dsize=(resize_w, resize_h))                      resize_ratio_3_x = resize_w/float(new_w)                      resize_ratio_3_y = resize_h/float(new_h)                      text_polys[:, :, 0] *= resize_ratio_3_x                      text_polys[:, :, 1] *= resize_ratio_3_y                      new_h, new_w, _ = im.shape                      score_map, geo_map, training_mask = generate_rbox((new_h, new_w), text_polys, text_tags)                    if vis:                      fig, axs = plt.subplots(3, 2, figsize=(20, 30))                      # axs[0].imshow(im[:, :, ::-1])                      # axs[0].set_xticks([])                      # axs[0].set_yticks([])                      # for poly in text_polys:                      #     poly_h = min(abs(poly[3, 1] - poly[0, 1]), abs(poly[2, 1] - poly[1, 1]))                      #     poly_w = min(abs(poly[1, 0] - poly[0, 0]), abs(poly[2, 0] - poly[3, 0]))                      #     axs[0].add_artist(Patches.Polygon(                      #         poly * 4, facecolor='none', edgecolor='green', linewidth=2, linestyle='-', fill=True))                      #     axs[0].text(poly[0, 0] * 4, poly[0, 1] * 4, '{:.0f}-{:.0f}'.format(poly_h * 4, poly_w * 4),                      #                    color='purple')                      # axs[1].imshow(score_map)                      # axs[1].set_xticks([])                      # axs[1].set_yticks([])                      axs[0, 0].imshow(im[:, :, ::-1])                      axs[0, 0].set_xticks([])                      axs[0, 0].set_yticks([])                      for poly in text_polys:                          poly_h = min(abs(poly[3, 1] - poly[0, 1]), abs(poly[2, 1] - poly[1, 1]))                          poly_w = min(abs(poly[1, 0] - poly[0, 0]), abs(poly[2, 0] - poly[3, 0]))                          axs[0, 0].add_artist(Patches.Polygon(                              poly, facecolor='none', edgecolor='green', linewidth=2, linestyle='-', fill=True))                          axs[0, 0].text(poly[0, 0], poly[0, 1], '{:.0f}-{:.0f}'.format(poly_h, poly_w), color='purple')                      axs[0, 1].imshow(score_map[::, ::])                      axs[0, 1].set_xticks([])                      axs[0, 1].set_yticks([])                      axs[1, 0].imshow(geo_map[::, ::, 0])                      axs[1, 0].set_xticks([])                      axs[1, 0].set_yticks([])                      axs[1, 1].imshow(geo_map[::, ::, 1])                      axs[1, 1].set_xticks([])                      axs[1, 1].set_yticks([])                      axs[2, 0].imshow(geo_map[::, ::, 2])                      axs[2, 0].set_xticks([])                      axs[2, 0].set_yticks([])                      axs[2, 1].imshow(training_mask[::, ::])                      axs[2, 1].set_xticks([])                      axs[2, 1].set_yticks([])                      plt.tight_layout()                      plt.show()                      plt.close()                    images.append(im[:, :, ::-1].astype(np.float32))                  image_fns.append(im_fn)                  score_maps.append(score_map[::4, ::4, np.newaxis].astype(np.float32))                  geo_maps.append(geo_map[::4, ::4, :].astype(np.float32))                  training_masks.append(training_mask[::4, ::4, np.newaxis].astype(np.float32))                    if len(images) == batch_size:                      yield images, image_fns, score_maps, geo_maps, training_masks                      images = []                      image_fns = []                      score_maps = []                      geo_maps = []                      training_masks = []              except Exception as e:                  import traceback                  traceback.print_exc()                  continue    </code></pre><p>—|—  </p><h3 id="网络模型的搭建"><a href="#网络模型的搭建" class="headerlink" title="网络模型的搭建"></a>网络模型的搭建</h3><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  </code></pre><p>| </p><pre><code>def model(images, weight_decay=1e-5, is_training=True):      '''      define the model, we use slim's implemention of resnet      '''      images = mean_image_subtraction(images)        with slim.arg_scope(resnet_v1.resnet_arg_scope(weight_decay=weight_decay)):          logits, end_points = resnet_v1.resnet_v1_50(images, is_training=is_training, scope='resnet_v1_50')        with tf.variable_scope('feature_fusion', values=[end_points.values]):          batch_norm_params = {          'decay': 0.997,          'epsilon': 1e-5,          'scale': True,          'is_training': is_training          }          with slim.arg_scope([slim.conv2d],                              activation_fn=tf.nn.relu,                              normalizer_fn=slim.batch_norm,                              normalizer_params=batch_norm_params,                              weights_regularizer=slim.l2_regularizer(weight_decay)):              f = [end_points['pool5'], end_points['pool4'],                   end_points['pool3'], end_points['pool2']]              for i in range(4):                  print('Shape of f_{} {}'.format(i, f[i].shape))              g = [None, None, None, None]              h = [None, None, None, None]              num_outputs = [None, 128, 64, 32]              for i in range(4):                  if i == 0:                      h[i] = f[i]                  else:                      c1_1 = slim.conv2d(tf.concat([g[i-1], f[i]], axis=-1), num_outputs[i], 1)                      h[i] = slim.conv2d(c1_1, num_outputs[i], 3)                  if i &lt;= 2:                      g[i] = unpool(h[i])                  else:                      g[i] = slim.conv2d(h[i], num_outputs[i], 3)                  print('Shape of h_{} {}, g_{} {}'.format(i, h[i].shape, i, g[i].shape))                # here we use a slightly different way for regression part,              # we first use a sigmoid to limit the regression range, and also              # this is do with the angle map              F_score = slim.conv2d(g[3], 1, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None)              # 4 channel of axis aligned bbox and 1 channel rotation angle              geo_map = slim.conv2d(g[3], 4, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None) * FLAGS.text_scale              angle_map = (slim.conv2d(g[3], 1, 1, activation_fn=tf.nn.sigmoid, normalizer_fn=None) - 0.5) * np.pi/2 # angle is between [-45, 45]              F_geometry = tf.concat([geo_map, angle_map], axis=-1)        return F_score, F_geometry    </code></pre><p>—|—  </p><h3 id="loss函数的设计"><a href="#loss函数的设计" class="headerlink" title="loss函数的设计"></a>loss函数的设计</h3><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  </code></pre><p>| </p><pre><code>def loss(y_true_cls, y_pred_cls,           y_true_geo, y_pred_geo,           training_mask):      '''      define the loss used for training, contraning two part,      the first part we use dice loss instead of weighted logloss,      the second part is the iou loss defined in the paper      :param y_true_cls: ground truth of text      :param y_pred_cls: prediction os text      :param y_true_geo: ground truth of geometry      :param y_pred_geo: prediction of geometry      :param training_mask: mask used in training, to ignore some text annotated by ###      :return:      '''      classification_loss = dice_coefficient(y_true_cls, y_pred_cls, training_mask)      # scale classification loss to match the iou loss part      classification_loss *= 0.01        # d1 -&gt; top, d2-&gt;right, d3-&gt;bottom, d4-&gt;left      d1_gt, d2_gt, d3_gt, d4_gt, theta_gt = tf.split(value=y_true_geo, num_or_size_splits=5, axis=3)      d1_pred, d2_pred, d3_pred, d4_pred, theta_pred = tf.split(value=y_pred_geo, num_or_size_splits=5, axis=3)      area_gt = (d1_gt + d3_gt) * (d2_gt + d4_gt)      area_pred = (d1_pred + d3_pred) * (d2_pred + d4_pred)      w_union = tf.minimum(d2_gt, d2_pred) + tf.minimum(d4_gt, d4_pred)      h_union = tf.minimum(d1_gt, d1_pred) + tf.minimum(d3_gt, d3_pred)      area_intersect = w_union * h_union      area_union = area_gt + area_pred - area_intersect      L_AABB = -tf.log((area_intersect + 1.0)/(area_union + 1.0))      L_theta = 1 - tf.cos(theta_pred - theta_gt)      tf.summary.scalar('geometry_AABB', tf.reduce_mean(L_AABB * y_true_cls * training_mask))      tf.summary.scalar('geometry_theta', tf.reduce_mean(L_theta * y_true_cls * training_mask))      L_g = L_AABB + 20 * L_theta        return tf.reduce_mean(L_g * y_true_cls * training_mask) + classification_loss    </code></pre><p>—|—  </p><h3 id="train"><a href="#train" class="headerlink" title="train"></a>train</h3><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  </code></pre><p>| </p><pre><code>def main(argv=None):      import os      os.environ['CUDA_VISIBLE_DEVICES'] = FLAGS.gpu_list      config = None      config.batch_size = FLAGS.batch_size_per_gpu * FLAGS.num_gpus      if not tf.gfile.Exists(FLAGS.checkpoint_path):          tf.gfile.MkDir(FLAGS.checkpoint_path)      else:          if not FLAGS.restore:              tf.gfile.DeleteRecursively(FLAGS.checkpoint_path)              tf.gfile.MkDir(FLAGS.checkpoint_path)        input_images = tf.placeholder(tf.float32, shape=[None, None, None, 3], name='input_images')      input_score_maps = tf.placeholder(tf.float32, shape=[None, None, None, 1], name='input_score_maps')      if FLAGS.geometry == 'RBOX':          input_geo_maps = tf.placeholder(tf.float32, shape=[None, None, None, 5], name='input_geo_maps')      else:          input_geo_maps = tf.placeholder(tf.float32, shape=[None, None, None, 8], name='input_geo_maps')      input_training_masks = tf.placeholder(tf.float32, shape=[None, None, None, 1], name='input_training_masks')        global_step = tf.get_variable('global_step', [], initializer=tf.constant_initializer(0), trainable=False)      learning_rate = tf.train.exponential_decay(FLAGS.learning_rate, global_step, decay_steps=10000, decay_rate=0.94, staircase=True)      # add summary      tf.summary.scalar('learning_rate', learning_rate)      opt = tf.train.AdamOptimizer(learning_rate)      # opt = tf.train.MomentumOptimizer(learning_rate, 0.9)          # split      input_images_split = tf.split(input_images, len(gpus))      input_score_maps_split = tf.split(input_score_maps, len(gpus))      input_geo_maps_split = tf.split(input_geo_maps, len(gpus))      input_training_masks_split = tf.split(input_training_masks, len(gpus))        tower_grads = []      reuse_variables = None      for i, gpu_id in enumerate(gpus):          with tf.device('/gpu:%d' % gpu_id):              with tf.name_scope('model_%d' % gpu_id) as scope:                  iis = input_images_split[i]                  isms = input_score_maps_split[i]                  igms = input_geo_maps_split[i]                  itms = input_training_masks_split[i]                  total_loss, model_loss = tower_loss(iis, isms, igms, itms, reuse_variables)                  batch_norm_updates_op = tf.group(*tf.get_collection(tf.GraphKeys.UPDATE_OPS, scope))                  reuse_variables = True                    grads = opt.compute_gradients(total_loss)                  tower_grads.append(grads)        grads = average_gradients(tower_grads)      apply_gradient_op = opt.apply_gradients(grads, global_step=global_step)        summary_op = tf.summary.merge_all()      # save moving average      variable_averages = tf.train.ExponentialMovingAverage(          FLAGS.moving_average_decay, global_step)      variables_averages_op = variable_averages.apply(tf.trainable_variables())      # batch norm updates      with tf.control_dependencies([variables_averages_op, apply_gradient_op, batch_norm_updates_op]):          train_op = tf.no_op(name='train_op')        saver = tf.train.Saver(tf.global_variables())      summary_writer = tf.summary.FileWriter(FLAGS.checkpoint_path, tf.get_default_graph())        init = tf.global_variables_initializer()        if FLAGS.pretrained_model_path is not None:          variable_restore_op = slim.assign_from_checkpoint_fn(FLAGS.pretrained_model_path, slim.get_trainable_variables(),                                                              ignore_missing_vars=True)      with tf.Session(config=tf.ConfigProto(allow_soft_placement=True)) as sess:          if FLAGS.restore:              print('continue training from previous checkpoint')              ckpt = tf.train.latest_checkpoint(FLAGS.checkpoint_path)              saver.restore(sess, ckpt)          else:              sess.run(init)              if FLAGS.pretrained_model_path is not None:                  variable_restore_op(sess)            # data_generator = icdar.get_batch(num_workers=FLAGS.num_readers,          #                                 input_size=FLAGS.input_size,          #                                 batch_size=FLAGS.batch_size_per_gpu * len(gpus))          train_data_generator = icdar_single.get_batch_seq(num_workers=FLAGS.num_readers, config=config, is_training=True)            start = time.time()          for step in range(FLAGS.max_steps):              data = next(train_data_generator)              ml, tl, _ = sess.run([model_loss, total_loss, train_op], feed_dict={input_images: data[0],                                                                                  input_score_maps: data[2],                                                                                  input_geo_maps: data[3],                                                                                  input_training_masks: data[4]})              if np.isnan(tl):                  print('Loss diverged, stop training')                  break                if step % 10 == 0:                  avg_time_per_step = (time.time() - start)/10                  avg_examples_per_second = (10 * FLAGS.batch_size_per_gpu * len(gpus))/(time.time() - start)                  start = time.time()                  print('Step {:06d}, model loss {:.4f}, total loss {:.4f}, {:.2f} seconds/step, {:.2f} examples/second'.format(                      step, ml, tl, avg_time_per_step, avg_examples_per_second))                if step % FLAGS.save_checkpoint_steps == 0:                  saver.save(sess, FLAGS.checkpoint_path + 'model.ckpt', global_step=global_step)                if step % FLAGS.save_summary_steps == 0:                  _, tl, summary_str = sess.run([train_op, total_loss, summary_op], feed_dict={input_images: data[0],                                                                                              input_score_maps: data[2],                                                                                              input_geo_maps: data[3],                                                                                              input_training_masks: data[4]})                  summary_writer.add_summary(summary_str, global_step=step)    </code></pre><p>—|—  </p><p>参考链接：<br><a href="https://github.com/argman/EAST">https://github.com/argman/EAST</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> TensorFlow </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像增强常用函数</title>
      <link href="/2019/10/14/20191014-%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/"/>
      <url>/2019/10/14/20191014-%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>对目标检测一些常用的数据增强函数</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding: utf-8 -*-</span></span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numbers</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> skimage.util <span class="keyword">import</span> random_noise</span><br><span class="line"><span class="comment"># 在原图上画出目标框</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">show_pic</span>(<span class="params">img, bboxes=<span class="literal">None</span>, name=<span class="string">'pic'</span></span>):</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    输入:</span></span><br><span class="line"><span class="string">        img:图像array</span></span><br><span class="line"><span class="string">        bboxes:图像的所有boudning box list, 格式为[[x_min, y_min, x_max, y_max]....]</span></span><br><span class="line"><span class="string">        names:每个box对应的名称</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    show_img = img.copy()</span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> <span class="built_in">isinstance</span>(bboxes, np.ndarray):</span><br><span class="line">        bboxes = np.array(bboxes)</span><br><span class="line">    <span class="keyword">for</span> point <span class="keyword">in</span> bboxes.astype(np.<span class="built_in">int</span>):</span><br><span class="line">        cv2.line(show_img, <span class="built_in">tuple</span>(point[<span class="number">0</span>]), <span class="built_in">tuple</span>(point[<span class="number">1</span>]), (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>), <span class="number">2</span>)  <span class="comment"># tuple 是在原有数据上加小括号</span></span><br><span class="line">        cv2.line(show_img, <span class="built_in">tuple</span>(point[<span class="number">1</span>]), <span class="built_in">tuple</span>(point[<span class="number">2</span>]), (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>), <span class="number">2</span>)</span><br><span class="line">        cv2.line(show_img, <span class="built_in">tuple</span>(point[<span class="number">2</span>]), <span class="built_in">tuple</span>(point[<span class="number">3</span>]), (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>), <span class="number">2</span>)</span><br><span class="line">        cv2.line(show_img, <span class="built_in">tuple</span>(point[<span class="number">3</span>]), <span class="built_in">tuple</span>(point[<span class="number">0</span>]), (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>), <span class="number">2</span>)</span><br><span class="line">    <span class="comment"># cv2.namedWindow(name, 0)  # 1表示原图</span></span><br><span class="line">    <span class="comment"># cv2.moveWindow(name, 0, 0)</span></span><br><span class="line">    <span class="comment"># cv2.resizeWindow(name, 1200, 800)  # 可视化的图片大小</span></span><br><span class="line">    cv2.imshow(name, show_img)</span><br><span class="line"><span class="comment"># 图像均为cv2读取</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">DataAugment</span>():</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">add_noise</span>(<span class="params">self, im: np.ndarray</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        对图片加噪声</span></span><br><span class="line"><span class="string">        :param img: 图像array</span></span><br><span class="line"><span class="string">        :return: 加噪声后的图像array,由于输出的像素是在[0,1]之间,所以得乘以255</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">return</span> (random_noise(im, mode=<span class="string">'gaussian'</span>, clip=<span class="literal">True</span>) * <span class="number">255</span>).astype(im.dtype)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">random_scale</span>(<span class="params">self, im: np.ndarray, text_polys: np.ndarray, scales: np.ndarray <span class="keyword">or</span> <span class="built_in">list</span></span>) -&gt; <span class="built_in">tuple</span>:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        从scales中随机选择一个尺度，对图片和文本框进行缩放</span></span><br><span class="line"><span class="string">        :param im: 原图</span></span><br><span class="line"><span class="string">        :param text_polys: 文本框</span></span><br><span class="line"><span class="string">        :param scales: 尺度</span></span><br><span class="line"><span class="string">        :return: 经过缩放的图片和文本</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        tmp_text_polys = text_polys.copy()</span><br><span class="line">        rd_scale = <span class="built_in">float</span>(np.random.choice(scales))</span><br><span class="line">        im = cv2.resize(im, dsize=<span class="literal">None</span>, fx=rd_scale, fy=rd_scale)</span><br><span class="line">        tmp_text_polys *= rd_scale</span><br><span class="line">        <span class="keyword">return</span> im, tmp_text_polys</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">random_rotate_img_bbox</span>(<span class="params">self, img, text_polys, degrees: numbers.Number <span class="keyword">or</span> <span class="built_in">list</span> <span class="keyword">or</span> <span class="built_in">tuple</span> <span class="keyword">or</span> np.ndarray,</span></span><br><span class="line"><span class="params">                            same_size=<span class="literal">False</span></span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        从给定的角度中选择一个角度，对图片和文本框进行旋转</span></span><br><span class="line"><span class="string">        :param img: 图片</span></span><br><span class="line"><span class="string">        :param text_polys: 文本框</span></span><br><span class="line"><span class="string">        :param degrees: 角度，可以是一个数值或者list</span></span><br><span class="line"><span class="string">        :param same_size: 是否保持和原图一样大</span></span><br><span class="line"><span class="string">        :return: 旋转后的图片和角度</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(degrees, numbers.Number):</span><br><span class="line">            <span class="keyword">if</span> degrees &lt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">"If degrees is a single number, it must be positive."</span>)</span><br><span class="line">            degrees = (-degrees, degrees)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(degrees, <span class="built_in">list</span>) <span class="keyword">or</span> <span class="built_in">isinstance</span>(degrees, <span class="built_in">tuple</span>) <span class="keyword">or</span> <span class="built_in">isinstance</span>(degrees, np.ndarray):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(degrees) != <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">"If degrees is a sequence, it must be of len 2."</span>)</span><br><span class="line">            degrees = degrees</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">'degrees must in Number or list or tuple or np.ndarray'</span>)</span><br><span class="line">        <span class="comment"># ---------------------- 旋转图像 ----------------------</span></span><br><span class="line">        w = img.shape[<span class="number">1</span>]</span><br><span class="line">        h = img.shape[<span class="number">0</span>]</span><br><span class="line">        angle = np.random.uniform(degrees[<span class="number">0</span>], degrees[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">if</span> same_size:</span><br><span class="line">            nw = w</span><br><span class="line">            nh = h</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="comment"># 角度变弧度</span></span><br><span class="line">            rangle = np.deg2rad(angle)</span><br><span class="line">            <span class="comment"># 计算旋转之后图像的w, h</span></span><br><span class="line">            nw = (<span class="built_in">abs</span>(np.sin(rangle) * h) + <span class="built_in">abs</span>(np.cos(rangle) * w))</span><br><span class="line">            nh = (<span class="built_in">abs</span>(np.cos(rangle) * h) + <span class="built_in">abs</span>(np.sin(rangle) * w))</span><br><span class="line">        <span class="comment"># 构造仿射矩阵</span></span><br><span class="line">        rot_mat = cv2.getRotationMatrix2D((nw * <span class="number">0.5</span>, nh * <span class="number">0.5</span>), angle, <span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 计算原图中心点到新图中心点的偏移量</span></span><br><span class="line">        rot_move = np.dot(rot_mat, np.array([(nw - w) * <span class="number">0.5</span>, (nh - h) * <span class="number">0.5</span>, <span class="number">0</span>]))</span><br><span class="line">        <span class="comment"># 更新仿射矩阵</span></span><br><span class="line">        rot_mat[<span class="number">0</span>, <span class="number">2</span>] += rot_move[<span class="number">0</span>]</span><br><span class="line">        rot_mat[<span class="number">1</span>, <span class="number">2</span>] += rot_move[<span class="number">1</span>]</span><br><span class="line">        <span class="comment"># 仿射变换</span></span><br><span class="line">        rot_img = cv2.warpAffine(img, rot_mat, (<span class="built_in">int</span>(math.ceil(nw)), <span class="built_in">int</span>(math.ceil(nh))), flags=cv2.INTER_LANCZOS4)</span><br><span class="line">        <span class="comment"># ---------------------- 矫正bbox坐标 ----------------------</span></span><br><span class="line">        <span class="comment"># rot_mat是最终的旋转矩阵</span></span><br><span class="line">        <span class="comment"># 获取原始bbox的四个中点，然后将这四个点转换到旋转后的坐标系下</span></span><br><span class="line">        rot_text_polys = <span class="built_in">list</span>()</span><br><span class="line">        <span class="keyword">for</span> bbox <span class="keyword">in</span> text_polys:</span><br><span class="line">            point1 = np.dot(rot_mat, np.array([bbox[<span class="number">0</span>, <span class="number">0</span>], bbox[<span class="number">0</span>, <span class="number">1</span>], <span class="number">1</span>]))</span><br><span class="line">            point2 = np.dot(rot_mat, np.array([bbox[<span class="number">1</span>, <span class="number">0</span>], bbox[<span class="number">1</span>, <span class="number">1</span>], <span class="number">1</span>]))</span><br><span class="line">            point3 = np.dot(rot_mat, np.array([bbox[<span class="number">2</span>, <span class="number">0</span>], bbox[<span class="number">2</span>, <span class="number">1</span>], <span class="number">1</span>]))</span><br><span class="line">            point4 = np.dot(rot_mat, np.array([bbox[<span class="number">3</span>, <span class="number">0</span>], bbox[<span class="number">3</span>, <span class="number">1</span>], <span class="number">1</span>]))</span><br><span class="line">            rot_text_polys.append([point1, point2, point3, point4])</span><br><span class="line">        <span class="keyword">return</span> rot_img, np.array(rot_text_polys, dtype=np.float32)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">random_crop</span>(<span class="params">self, imgs, img_size</span>):</span><br><span class="line">        h, w = imgs[<span class="number">0</span>].shape[<span class="number">0</span>:<span class="number">2</span>]</span><br><span class="line">        th, tw = img_size</span><br><span class="line">        <span class="keyword">if</span> w == tw <span class="keyword">and</span> h == th:</span><br><span class="line">            <span class="keyword">return</span> imgs</span><br><span class="line">        <span class="comment"># label中存在文本实例，并且按照概率进行裁剪</span></span><br><span class="line">        <span class="keyword">if</span> np.<span class="built_in">max</span>(imgs[<span class="number">1</span>][:, :, -<span class="number">1</span>]) &gt; <span class="number">0</span> <span class="keyword">and</span> random.random() &gt; <span class="number">3.0</span> / <span class="number">8.0</span>:</span><br><span class="line">            <span class="comment"># 文本实例的top left点</span></span><br><span class="line">            tl = np.<span class="built_in">min</span>(np.where(imgs[<span class="number">1</span>][:, :, -<span class="number">1</span>] &gt; <span class="number">0</span>), axis=<span class="number">1</span>) - img_size</span><br><span class="line">            tl[tl &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">            <span class="comment"># 文本实例的 bottom right 点</span></span><br><span class="line">            br = np.<span class="built_in">max</span>(np.where(imgs[<span class="number">1</span>][:, :, -<span class="number">1</span>] &gt; <span class="number">0</span>), axis=<span class="number">1</span>) - img_size</span><br><span class="line">            br[br &lt; <span class="number">0</span>] = <span class="number">0</span></span><br><span class="line">            <span class="comment"># 保证选到右下角点是，有足够的距离进行crop</span></span><br><span class="line">            br[<span class="number">0</span>] = <span class="built_in">min</span>(br[<span class="number">0</span>], h - th)</span><br><span class="line">            br[<span class="number">1</span>] = <span class="built_in">min</span>(br[<span class="number">1</span>], w - tw)</span><br><span class="line">            i = random.randint(tl[<span class="number">0</span>], br[<span class="number">0</span>])</span><br><span class="line">            j = random.randint(tl[<span class="number">1</span>], br[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            i = random.randint(<span class="number">0</span>, h - th)</span><br><span class="line">            j = random.randint(<span class="number">0</span>, w - tw)</span><br><span class="line">        <span class="comment"># return i, j, th, tw</span></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(imgs)):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(imgs[idx].shape) == <span class="number">3</span>:</span><br><span class="line">                imgs[idx] = imgs[idx][i:i + th, j:j + tw, :]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                imgs[idx] = imgs[idx][i:i + th, j:j + tw]</span><br><span class="line">        <span class="keyword">return</span> imgs</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">resize</span>(<span class="params">self, im: np.ndarray, text_polys: np.ndarray,</span></span><br><span class="line"><span class="params">            input_size: numbers.Number <span class="keyword">or</span> <span class="built_in">list</span> <span class="keyword">or</span> <span class="built_in">tuple</span> <span class="keyword">or</span> np.ndarray, keep_ratio: <span class="built_in">bool</span> = <span class="literal">False</span></span>) -&gt; <span class="built_in">tuple</span>:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        对图片和文本框进行resize</span></span><br><span class="line"><span class="string">        :param im: 图片</span></span><br><span class="line"><span class="string">        :param text_polys: 文本框</span></span><br><span class="line"><span class="string">        :param input_size: resize尺寸,数字或者list的形式，如果为list形式，就是[w,h]</span></span><br><span class="line"><span class="string">        :param keep_ratio: 是否保持长宽比</span></span><br><span class="line"><span class="string">        :return: resize后的图片和文本框</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">isinstance</span>(input_size, numbers.Number):</span><br><span class="line">            <span class="keyword">if</span> input_size &lt; <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">"If input_size is a single number, it must be positive."</span>)</span><br><span class="line">            input_size = (input_size, input_size)</span><br><span class="line">        <span class="keyword">elif</span> <span class="built_in">isinstance</span>(input_size, <span class="built_in">list</span>) <span class="keyword">or</span> <span class="built_in">isinstance</span>(input_size, <span class="built_in">tuple</span>) <span class="keyword">or</span> <span class="built_in">isinstance</span>(input_size, np.ndarray):</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(input_size) != <span class="number">2</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">"If input_size is a sequence, it must be of len 2."</span>)</span><br><span class="line">            input_size = (input_size[<span class="number">0</span>], input_size[<span class="number">1</span>])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">raise</span> Exception(<span class="string">'input_size must in Number or list or tuple or np.ndarray'</span>)</span><br><span class="line">        <span class="keyword">if</span> keep_ratio:</span><br><span class="line">            <span class="comment"># 将图片短边pad到和长边一样</span></span><br><span class="line">            h, w, c = im.shape</span><br><span class="line">            max_h = <span class="built_in">max</span>(h, input_size[<span class="number">0</span>])</span><br><span class="line">            max_w = <span class="built_in">max</span>(w, input_size[<span class="number">1</span>])</span><br><span class="line">            im_padded = np.zeros((max_h, max_w, c), dtype=np.uint8)</span><br><span class="line">            im_padded[:h, :w] = im.copy()</span><br><span class="line">            im = im_padded</span><br><span class="line">        text_polys = text_polys.astype(np.float32)</span><br><span class="line">        h, w, _ = im.shape</span><br><span class="line">        im = cv2.resize(im, input_size)</span><br><span class="line">        w_scale = input_size[<span class="number">0</span>] / <span class="built_in">float</span>(w)</span><br><span class="line">        h_scale = input_size[<span class="number">1</span>] / <span class="built_in">float</span>(h)</span><br><span class="line">        text_polys[:, :, <span class="number">0</span>] *= w_scale</span><br><span class="line">        text_polys[:, :, <span class="number">1</span>] *= h_scale</span><br><span class="line">        <span class="keyword">return</span> im, text_polys</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">horizontal_flip</span>(<span class="params">self, im: np.ndarray, text_polys: np.ndarray</span>) -&gt; <span class="built_in">tuple</span>:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        对图片和文本框进行水平翻转</span></span><br><span class="line"><span class="string">        :param im: 图片</span></span><br><span class="line"><span class="string">        :param text_polys: 文本框</span></span><br><span class="line"><span class="string">        :return: 水平翻转之后的图片和文本框</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        flip_text_polys = text_polys.copy()</span><br><span class="line">        flip_im = cv2.flip(im, <span class="number">1</span>)</span><br><span class="line">        h, w, _ = flip_im.shape</span><br><span class="line">        flip_text_polys[:, :, <span class="number">0</span>] = w - flip_text_polys[:, :, <span class="number">0</span>]</span><br><span class="line">        <span class="keyword">return</span> flip_im, flip_text_polys</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">vertical_flip</span>(<span class="params">self, im: np.ndarray, text_polys: np.ndarray</span>) -&gt; <span class="built_in">tuple</span>:</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">        对图片和文本框进行竖直翻转</span></span><br><span class="line"><span class="string">        :param im: 图片</span></span><br><span class="line"><span class="string">        :param text_polys: 文本框</span></span><br><span class="line"><span class="string">        :return: 竖直翻转之后的图片和文本框</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        flip_text_polys = text_polys.copy()</span><br><span class="line">        flip_im = cv2.flip(im, <span class="number">0</span>)</span><br><span class="line">        h, w, _ = flip_im.shape</span><br><span class="line">        flip_text_polys[:, :, <span class="number">1</span>] = h - flip_text_polys[:, :, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">return</span> flip_im, flip_text_polys</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">test</span>(<span class="params">self, im: np.ndarray, text_polys: np.ndarray</span>):</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'随机尺度缩放'</span>)</span><br><span class="line">        t_im, t_text_polys = self.random_scale(im, text_polys, [<span class="number">0.5</span>, <span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>])</span><br><span class="line">        <span class="built_in">print</span>(t_im.shape, t_text_polys.dtype)</span><br><span class="line">        show_pic(t_im, t_text_polys, <span class="string">'random_scale'</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'随机旋转'</span>)</span><br><span class="line">        t_im, t_text_polys = self.random_rotate_img_bbox(im, text_polys, <span class="number">10</span>)</span><br><span class="line">        <span class="built_in">print</span>(t_im.shape, t_text_polys.dtype)</span><br><span class="line">        show_pic(t_im, t_text_polys, <span class="string">'random_rotate_img_bbox'</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'随机裁剪'</span>)</span><br><span class="line">        t_im, t_text_polys = self.random_crop_img_bboxes(im, text_polys)</span><br><span class="line">        <span class="built_in">print</span>(t_im.shape, t_text_polys.dtype)</span><br><span class="line">        show_pic(t_im, t_text_polys, <span class="string">'random_crop_img_bboxes'</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'水平翻转'</span>)</span><br><span class="line">        t_im, t_text_polys = self.horizontal_flip(im, text_polys)</span><br><span class="line">        <span class="built_in">print</span>(t_im.shape, t_text_polys.dtype)</span><br><span class="line">        show_pic(t_im, t_text_polys, <span class="string">'horizontal_flip'</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'竖直翻转'</span>)</span><br><span class="line">        t_im, t_text_polys = self.vertical_flip(im, text_polys)</span><br><span class="line">        <span class="built_in">print</span>(t_im.shape, t_text_polys.dtype)</span><br><span class="line">        show_pic(t_im, t_text_polys, <span class="string">'vertical_flip'</span>)</span><br><span class="line">        show_pic(im, text_polys, <span class="string">'vertical_flip_ori'</span>)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'加噪声'</span>)</span><br><span class="line">        t_im = self.add_noise(im)</span><br><span class="line">        <span class="built_in">print</span>(t_im.shape)</span><br><span class="line">        show_pic(t_im, text_polys, <span class="string">'add_noise'</span>)</span><br><span class="line">        show_pic(im, text_polys, <span class="string">'add_noise_ori'</span>)</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像增强常用函数</title>
      <link href="/2019/10/14/%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/"/>
      <url>/2019/10/14/%E5%9B%BE%E5%83%8F%E5%A2%9E%E5%BC%BA%E5%B8%B8%E7%94%A8%E5%87%BD%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>对目标检测一些常用的数据增强函数</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160  161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176  177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192  193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208  209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224  225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240  241  </code></pre><p>| </p><pre><code># -*- coding: utf-8 -*-    import cv2  import numbers  import math  import random  import numpy as np  from skimage.util import random_noise    # 在原图上画出目标框  def show_pic(img, bboxes=None, name='pic'):      '''      输入:          img:图像array          bboxes:图像的所有boudning box list, 格式为[[x_min, y_min, x_max, y_max]....]          names:每个box对应的名称      '''      show_img = img.copy()      if not isinstance(bboxes, np.ndarray):          bboxes = np.array(bboxes)      for point in bboxes.astype(np.int):          cv2.line(show_img, tuple(point[0]), tuple(point[1]), (255, 0, 0), 2)  # tuple 是在原有数据上加小括号          cv2.line(show_img, tuple(point[1]), tuple(point[2]), (255, 0, 0), 2)          cv2.line(show_img, tuple(point[2]), tuple(point[3]), (255, 0, 0), 2)          cv2.line(show_img, tuple(point[3]), tuple(point[0]), (255, 0, 0), 2)      # cv2.namedWindow(name, 0)  # 1表示原图      # cv2.moveWindow(name, 0, 0)      # cv2.resizeWindow(name, 1200, 800)  # 可视化的图片大小      cv2.imshow(name, show_img)      # 图像均为cv2读取  class DataAugment():      def __init__(self):          pass        def add_noise(self, im: np.ndarray):          """          对图片加噪声          :param img: 图像array          :return: 加噪声后的图像array,由于输出的像素是在[0,1]之间,所以得乘以255          """          return (random_noise(im, mode='gaussian', clip=True) * 255).astype(im.dtype)        def random_scale(self, im: np.ndarray, text_polys: np.ndarray, scales: np.ndarray or list) -&gt; tuple:          """          从scales中随机选择一个尺度，对图片和文本框进行缩放          :param im: 原图          :param text_polys: 文本框          :param scales: 尺度          :return: 经过缩放的图片和文本          """          tmp_text_polys = text_polys.copy()          rd_scale = float(np.random.choice(scales))          im = cv2.resize(im, dsize=None, fx=rd_scale, fy=rd_scale)          tmp_text_polys *= rd_scale          return im, tmp_text_polys        def random_rotate_img_bbox(self, img, text_polys, degrees: numbers.Number or list or tuple or np.ndarray,                              same_size=False):          """          从给定的角度中选择一个角度，对图片和文本框进行旋转          :param img: 图片          :param text_polys: 文本框          :param degrees: 角度，可以是一个数值或者list          :param same_size: 是否保持和原图一样大          :return: 旋转后的图片和角度          """          if isinstance(degrees, numbers.Number):              if degrees &lt; 0:                  raise ValueError("If degrees is a single number, it must be positive.")              degrees = (-degrees, degrees)          elif isinstance(degrees, list) or isinstance(degrees, tuple) or isinstance(degrees, np.ndarray):              if len(degrees) != 2:                  raise ValueError("If degrees is a sequence, it must be of len 2.")              degrees = degrees          else:              raise Exception('degrees must in Number or list or tuple or np.ndarray')          # ---------------------- 旋转图像 ----------------------          w = img.shape[1]          h = img.shape[0]          angle = np.random.uniform(degrees[0], degrees[1])            if same_size:              nw = w              nh = h          else:              # 角度变弧度              rangle = np.deg2rad(angle)              # 计算旋转之后图像的w, h              nw = (abs(np.sin(rangle) * h) + abs(np.cos(rangle) * w))              nh = (abs(np.cos(rangle) * h) + abs(np.sin(rangle) * w))          # 构造仿射矩阵          rot_mat = cv2.getRotationMatrix2D((nw * 0.5, nh * 0.5), angle, 1)          # 计算原图中心点到新图中心点的偏移量          rot_move = np.dot(rot_mat, np.array([(nw - w) * 0.5, (nh - h) * 0.5, 0]))          # 更新仿射矩阵          rot_mat[0, 2] += rot_move[0]          rot_mat[1, 2] += rot_move[1]          # 仿射变换          rot_img = cv2.warpAffine(img, rot_mat, (int(math.ceil(nw)), int(math.ceil(nh))), flags=cv2.INTER_LANCZOS4)            # ---------------------- 矫正bbox坐标 ----------------------          # rot_mat是最终的旋转矩阵          # 获取原始bbox的四个中点，然后将这四个点转换到旋转后的坐标系下          rot_text_polys = list()          for bbox in text_polys:              point1 = np.dot(rot_mat, np.array([bbox[0, 0], bbox[0, 1], 1]))              point2 = np.dot(rot_mat, np.array([bbox[1, 0], bbox[1, 1], 1]))              point3 = np.dot(rot_mat, np.array([bbox[2, 0], bbox[2, 1], 1]))              point4 = np.dot(rot_mat, np.array([bbox[3, 0], bbox[3, 1], 1]))              rot_text_polys.append([point1, point2, point3, point4])          return rot_img, np.array(rot_text_polys, dtype=np.float32)        def random_crop(self, imgs, img_size):          h, w = imgs[0].shape[0:2]          th, tw = img_size          if w == tw and h == th:              return imgs            # label中存在文本实例，并且按照概率进行裁剪          if np.max(imgs[1][:, :, -1]) &gt; 0 and random.random() &gt; 3.0 / 8.0:              # 文本实例的top left点              tl = np.min(np.where(imgs[1][:, :, -1] &gt; 0), axis=1) - img_size              tl[tl &lt; 0] = 0              # 文本实例的 bottom right 点              br = np.max(np.where(imgs[1][:, :, -1] &gt; 0), axis=1) - img_size              br[br &lt; 0] = 0              # 保证选到右下角点是，有足够的距离进行crop              br[0] = min(br[0], h - th)              br[1] = min(br[1], w - tw)                i = random.randint(tl[0], br[0])              j = random.randint(tl[1], br[1])          else:              i = random.randint(0, h - th)              j = random.randint(0, w - tw)            # return i, j, th, tw          for idx in range(len(imgs)):              if len(imgs[idx].shape) == 3:                  imgs[idx] = imgs[idx][i:i + th, j:j + tw, :]              else:                  imgs[idx] = imgs[idx][i:i + th, j:j + tw]          return imgs        def resize(self, im: np.ndarray, text_polys: np.ndarray,              input_size: numbers.Number or list or tuple or np.ndarray, keep_ratio: bool = False) -&gt; tuple:          """          对图片和文本框进行resize          :param im: 图片          :param text_polys: 文本框          :param input_size: resize尺寸,数字或者list的形式，如果为list形式，就是[w,h]          :param keep_ratio: 是否保持长宽比          :return: resize后的图片和文本框          """          if isinstance(input_size, numbers.Number):              if input_size &lt; 0:                  raise ValueError("If input_size is a single number, it must be positive.")              input_size = (input_size, input_size)          elif isinstance(input_size, list) or isinstance(input_size, tuple) or isinstance(input_size, np.ndarray):              if len(input_size) != 2:                  raise ValueError("If input_size is a sequence, it must be of len 2.")              input_size = (input_size[0], input_size[1])          else:              raise Exception('input_size must in Number or list or tuple or np.ndarray')          if keep_ratio:              # 将图片短边pad到和长边一样              h, w, c = im.shape              max_h = max(h, input_size[0])              max_w = max(w, input_size[1])              im_padded = np.zeros((max_h, max_w, c), dtype=np.uint8)              im_padded[:h, :w] = im.copy()              im = im_padded          text_polys = text_polys.astype(np.float32)          h, w, _ = im.shape          im = cv2.resize(im, input_size)          w_scale = input_size[0] / float(w)          h_scale = input_size[1] / float(h)          text_polys[:, :, 0] *= w_scale          text_polys[:, :, 1] *= h_scale          return im, text_polys        def horizontal_flip(self, im: np.ndarray, text_polys: np.ndarray) -&gt; tuple:          """          对图片和文本框进行水平翻转          :param im: 图片          :param text_polys: 文本框          :return: 水平翻转之后的图片和文本框          """          flip_text_polys = text_polys.copy()          flip_im = cv2.flip(im, 1)          h, w, _ = flip_im.shape          flip_text_polys[:, :, 0] = w - flip_text_polys[:, :, 0]          return flip_im, flip_text_polys        def vertical_flip(self, im: np.ndarray, text_polys: np.ndarray) -&gt; tuple:          """          对图片和文本框进行竖直翻转          :param im: 图片          :param text_polys: 文本框          :return: 竖直翻转之后的图片和文本框          """          flip_text_polys = text_polys.copy()          flip_im = cv2.flip(im, 0)          h, w, _ = flip_im.shape          flip_text_polys[:, :, 1] = h - flip_text_polys[:, :, 1]          return flip_im, flip_text_polys        def test(self, im: np.ndarray, text_polys: np.ndarray):          print('随机尺度缩放')          t_im, t_text_polys = self.random_scale(im, text_polys, [0.5, 1, 2, 3])          print(t_im.shape, t_text_polys.dtype)          show_pic(t_im, t_text_polys, 'random_scale')            print('随机旋转')          t_im, t_text_polys = self.random_rotate_img_bbox(im, text_polys, 10)          print(t_im.shape, t_text_polys.dtype)          show_pic(t_im, t_text_polys, 'random_rotate_img_bbox')            print('随机裁剪')          t_im, t_text_polys = self.random_crop_img_bboxes(im, text_polys)          print(t_im.shape, t_text_polys.dtype)          show_pic(t_im, t_text_polys, 'random_crop_img_bboxes')            print('水平翻转')          t_im, t_text_polys = self.horizontal_flip(im, text_polys)          print(t_im.shape, t_text_polys.dtype)          show_pic(t_im, t_text_polys, 'horizontal_flip')            print('竖直翻转')          t_im, t_text_polys = self.vertical_flip(im, text_polys)          print(t_im.shape, t_text_polys.dtype)          show_pic(t_im, t_text_polys, 'vertical_flip')          show_pic(im, text_polys, 'vertical_flip_ori')            print('加噪声')          t_im = self.add_noise(im)          print(t_im.shape)          show_pic(t_im, text_polys, 'add_noise')          show_pic(im, text_polys, 'add_noise_ori')    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ocr答题卡识别</title>
      <link href="/2019/10/13/20191013-ocr%E7%AD%94%E9%A2%98%E5%8D%A1%E8%AF%86%E5%88%AB/"/>
      <url>/2019/10/13/20191013-ocr%E7%AD%94%E9%A2%98%E5%8D%A1%E8%AF%86%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">sort_contours</span>(<span class="params">cnts, method=<span class="string">"left-to-right"</span></span>):</span><br><span class="line">    reverse = <span class="literal">False</span></span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">if</span> method == <span class="string">"right-to-left"</span> <span class="keyword">or</span> method == <span class="string">"bottom-to-top"</span>:</span><br><span class="line">        reverse = <span class="literal">True</span></span><br><span class="line">    <span class="keyword">if</span> method == <span class="string">"top-to-bottom"</span> <span class="keyword">or</span> method == <span class="string">"bottom-to-top"</span>:</span><br><span class="line">        i = <span class="number">1</span></span><br><span class="line">    boundingBoxes = [cv2.boundingRect(c) <span class="keyword">for</span> c <span class="keyword">in</span> cnts] <span class="comment">#用一个最小的矩形，把找到的形状包起来x,y,h,w</span></span><br><span class="line">    (cnts, boundingBoxes) = <span class="built_in">zip</span>(*<span class="built_in">sorted</span>(<span class="built_in">zip</span>(cnts, boundingBoxes),</span><br><span class="line">                                        key=<span class="keyword">lambda</span> b: b[<span class="number">1</span>][i], reverse=reverse))</span><br><span class="line">    <span class="keyword">return</span> cnts, boundingBoxes</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resize</span>(<span class="params">image, width=<span class="literal">None</span>, height=<span class="literal">None</span>, inter=cv2.INTER_AREA</span>):</span><br><span class="line">    dim = <span class="literal">None</span></span><br><span class="line">    (h, w) = image.shape[:<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">if</span> width <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> height <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        <span class="keyword">return</span> image</span><br><span class="line">    <span class="keyword">if</span> width <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">        r = height / <span class="built_in">float</span>(h)</span><br><span class="line">        dim = (<span class="built_in">int</span>(w * r), height)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        r = width / <span class="built_in">float</span>(w)</span><br><span class="line">        dim = (width, <span class="built_in">int</span>(h * r))</span><br><span class="line">    resized = cv2.resize(image, dim, interpolation=inter)</span><br><span class="line">    <span class="keyword">return</span> resized</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入工具包</span></span><br><span class="line"><span class="keyword">from</span> imutils <span class="keyword">import</span> contours</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> myutils_beifen</span><br><span class="line"><span class="comment"># # 设置参数</span></span><br><span class="line"><span class="comment"># ap = argparse.ArgumentParser()</span></span><br><span class="line"><span class="comment"># ap.add_argument("-i", "--image", required=True,</span></span><br><span class="line"><span class="comment"># help="path to input image")</span></span><br><span class="line"><span class="comment"># ap.add_argument("-t", "--template", required=True,</span></span><br><span class="line"><span class="comment"># help="path to template OCR-A image")</span></span><br><span class="line"><span class="comment"># args = vars(ap.parse_args())</span></span><br><span class="line"><span class="comment"># 指定信用卡类型</span></span><br><span class="line">FIRST_NUMBER = {</span><br><span class="line"><span class="string">"3"</span>: <span class="string">"American Express"</span>,</span><br><span class="line"><span class="string">"4"</span>: <span class="string">"Visa"</span>,</span><br><span class="line"><span class="string">"5"</span>: <span class="string">"MasterCard"</span>,</span><br><span class="line"><span class="string">"6"</span>: <span class="string">"Discover Card"</span></span><br><span class="line">}</span><br><span class="line"><span class="comment"># 绘图展示</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cv_show</span>(<span class="params">name,img</span>):</span><br><span class="line">cv2.imshow(name, img)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line"><span class="comment"># 读取一个模板图像</span></span><br><span class="line"><span class="comment"># img = cv2.imread(args["template"])</span></span><br><span class="line">img  = cv2.imread(<span class="string">'images/ocr_a_reference.png'</span>)</span><br><span class="line">cv_show(<span class="string">'img'</span>,img)</span><br><span class="line"><span class="comment"># 灰度图</span></span><br><span class="line">ref = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)</span><br><span class="line">cv_show(<span class="string">'ref'</span>,ref)</span><br><span class="line"><span class="comment"># 二值图像</span></span><br><span class="line">ref = cv2.threshold(ref, <span class="number">10</span>, <span class="number">255</span>, cv2.THRESH_BINARY_INV)[<span class="number">1</span>]</span><br><span class="line">cv_show(<span class="string">'ref'</span>,ref)</span><br><span class="line"><span class="comment"># 计算轮廓</span></span><br><span class="line"><span class="comment">#cv2.findContours()函数接受的参数为二值图，即黑白的（不是灰度图）,cv2.RETR_EXTERNAL只检测外轮廓，cv2.CHAIN_APPROX_SIMPLE只保留终点坐标</span></span><br><span class="line"><span class="comment">#返回的list中每个元素都是图像中的一个轮廓</span></span><br><span class="line">ref_, refCnts, hierarchy = cv2.findContours(ref.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)</span><br><span class="line">cv2.drawContours(img,refCnts,-<span class="number">1</span>,(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),<span class="number">3</span>)  <span class="comment"># -1表示画所有的轮廓</span></span><br><span class="line">cv_show(<span class="string">'img'</span>,img)</span><br><span class="line"><span class="built_in">print</span> (np.array(refCnts).shape)</span><br><span class="line">refCnts = myutils.sort_contours(refCnts, method=<span class="string">"left-to-right"</span>)[<span class="number">0</span>] <span class="comment">#排序，从左到右，从上到下</span></span><br><span class="line">digits = {}</span><br><span class="line"><span class="comment"># 遍历每一个轮廓</span></span><br><span class="line"><span class="keyword">for</span> (i, c) <span class="keyword">in</span> <span class="built_in">enumerate</span>(refCnts):</span><br><span class="line"><span class="comment"># 计算外接矩形并且resize成合适大小</span></span><br><span class="line">(x, y, w, h) = cv2.boundingRect(c)</span><br><span class="line">roi = ref[y:y + h, x:x + w]</span><br><span class="line">roi = cv2.resize(roi, (<span class="number">57</span>, <span class="number">88</span>))</span><br><span class="line"><span class="comment"># 每一个数字对应每一个模板</span></span><br><span class="line">digits[i] = roi</span><br><span class="line"><span class="comment"># 初始化卷积核</span></span><br><span class="line">rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (<span class="number">9</span>, <span class="number">3</span>))</span><br><span class="line">sqKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (<span class="number">5</span>, <span class="number">5</span>))</span><br><span class="line"><span class="comment">#读取输入图像，预处理</span></span><br><span class="line"><span class="comment"># image = cv2.imread(args["image"])</span></span><br><span class="line">image = cv2.imread(<span class="string">'images/credit_card_02.png'</span>)</span><br><span class="line">cv_show(<span class="string">'image'</span>,image)</span><br><span class="line">image = myutils.resize(image, width=<span class="number">300</span>)</span><br><span class="line">gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br><span class="line">cv_show(<span class="string">'gray'</span>,gray)</span><br><span class="line"><span class="comment">#礼帽操作，突出更明亮的区域</span></span><br><span class="line">tophat = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, rectKernel)</span><br><span class="line">cv_show(<span class="string">'tophat'</span>,tophat)</span><br><span class="line"><span class="comment">#</span></span><br><span class="line">gradX = cv2.Sobel(tophat, ddepth=cv2.CV_32F, dx=<span class="number">1</span>, dy=<span class="number">0</span>, <span class="comment">#ksize=-1相当于用3*3的</span></span><br><span class="line">ksize=-<span class="number">1</span>)</span><br><span class="line">gradX = np.absolute(gradX)</span><br><span class="line">(minVal, maxVal) = (np.<span class="built_in">min</span>(gradX), np.<span class="built_in">max</span>(gradX))</span><br><span class="line">gradX = (<span class="number">255</span> * ((gradX - minVal) / (maxVal - minVal)))</span><br><span class="line">gradX = gradX.astype(<span class="string">"uint8"</span>)</span><br><span class="line"><span class="built_in">print</span> (np.array(gradX).shape)</span><br><span class="line">cv_show(<span class="string">'gradX'</span>,gradX)</span><br><span class="line"><span class="comment">#通过闭操作（先膨胀，再腐蚀）将数字连在一起</span></span><br><span class="line">gradX = cv2.morphologyEx(gradX, cv2.MORPH_CLOSE, rectKernel)</span><br><span class="line">cv_show(<span class="string">'gradX'</span>,gradX)</span><br><span class="line"><span class="comment">#THRESH_OTSU会自动寻找合适的阈值，适合双峰，需把阈值参数设置为0</span></span><br><span class="line">thresh = cv2.threshold(gradX, <span class="number">0</span>, <span class="number">255</span>,</span><br><span class="line">cv2.THRESH_BINARY | cv2.THRESH_OTSU)[<span class="number">1</span>]</span><br><span class="line">cv_show(<span class="string">'thresh'</span>,thresh)</span><br><span class="line"><span class="comment">#再来一个闭操作</span></span><br><span class="line">thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, sqKernel) <span class="comment">#再来一个闭操作</span></span><br><span class="line">cv_show(<span class="string">'thresh'</span>,thresh)</span><br><span class="line"><span class="comment"># 计算轮廓</span></span><br><span class="line">thresh_, threshCnts, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,</span><br><span class="line">cv2.CHAIN_APPROX_SIMPLE)</span><br><span class="line">cnts = threshCnts</span><br><span class="line">cur_img = image.copy()</span><br><span class="line">cv2.drawContours(cur_img,cnts,-<span class="number">1</span>,(<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>),<span class="number">3</span>)</span><br><span class="line">cv_show(<span class="string">'img'</span>,cur_img)</span><br><span class="line">locs = []</span><br><span class="line"><span class="comment"># 遍历轮廓</span></span><br><span class="line"><span class="keyword">for</span> (i, c) <span class="keyword">in</span> <span class="built_in">enumerate</span>(cnts):</span><br><span class="line"><span class="comment"># 计算矩形</span></span><br><span class="line">(x, y, w, h) = cv2.boundingRect(c)</span><br><span class="line">ar = w / <span class="built_in">float</span>(h)</span><br><span class="line"><span class="comment"># 选择合适的区域，根据实际任务来，这里的基本都是四个数字一组</span></span><br><span class="line"><span class="keyword">if</span> ar &gt; <span class="number">2.5</span> <span class="keyword">and</span> ar &lt; <span class="number">4.0</span>:</span><br><span class="line"><span class="keyword">if</span> (w &gt; <span class="number">40</span> <span class="keyword">and</span> w &lt; <span class="number">55</span>) <span class="keyword">and</span> (h &gt; <span class="number">10</span> <span class="keyword">and</span> h &lt; <span class="number">20</span>):</span><br><span class="line"><span class="comment">#符合的留下来</span></span><br><span class="line">locs.append((x, y, w, h))</span><br><span class="line"><span class="comment"># 将符合的轮廓从左到右排序</span></span><br><span class="line">locs = <span class="built_in">sorted</span>(locs, key=<span class="keyword">lambda</span> x:x[<span class="number">0</span>])</span><br><span class="line">output = []</span><br><span class="line"><span class="comment"># 遍历每一个轮廓中的数字</span></span><br><span class="line"><span class="keyword">for</span> (i, (gX, gY, gW, gH)) <span class="keyword">in</span> <span class="built_in">enumerate</span>(locs):</span><br><span class="line"><span class="comment"># initialize the list of group digits</span></span><br><span class="line">groupOutput = []</span><br><span class="line"><span class="comment"># 根据坐标提取每一个组</span></span><br><span class="line">group = gray[gY - <span class="number">5</span>:gY + gH + <span class="number">5</span>, gX - <span class="number">5</span>:gX + gW + <span class="number">5</span>]</span><br><span class="line">cv_show(<span class="string">'group'</span>,group)</span><br><span class="line"><span class="comment"># 预处理</span></span><br><span class="line">group = cv2.threshold(group, <span class="number">0</span>, <span class="number">255</span>,</span><br><span class="line">cv2.THRESH_BINARY | cv2.THRESH_OTSU)[<span class="number">1</span>]</span><br><span class="line">cv_show(<span class="string">'group'</span>,group)</span><br><span class="line"><span class="comment"># 计算每一组的轮廓</span></span><br><span class="line">group_,digitCnts,hierarchy = cv2.findContours(group.copy(), cv2.RETR_EXTERNAL,</span><br><span class="line">cv2.CHAIN_APPROX_SIMPLE)</span><br><span class="line">digitCnts = contours.sort_contours(digitCnts,</span><br><span class="line">method=<span class="string">"left-to-right"</span>)[<span class="number">0</span>]</span><br><span class="line"><span class="comment"># 计算每一组中的每一个数值</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> digitCnts:</span><br><span class="line"><span class="comment"># 找到当前数值的轮廓，resize成合适的的大小</span></span><br><span class="line">(x, y, w, h) = cv2.boundingRect(c)</span><br><span class="line">roi = group[y:y + h, x:x + w]</span><br><span class="line">roi = cv2.resize(roi, (<span class="number">57</span>, <span class="number">88</span>)) <span class="comment"># 模板匹配要求大小一致</span></span><br><span class="line">cv_show(<span class="string">'roi'</span>,roi)</span><br><span class="line"><span class="comment"># 计算匹配得分</span></span><br><span class="line">scores = []</span><br><span class="line"><span class="comment"># 在模板中计算每一个得分</span></span><br><span class="line"><span class="keyword">for</span> (digit, digitROI) <span class="keyword">in</span> digits.items():</span><br><span class="line"><span class="comment"># 模板匹配</span></span><br><span class="line">result = cv2.matchTemplate(roi, digitROI,</span><br><span class="line">cv2.TM_CCOEFF)</span><br><span class="line">(_, score, _, _) = cv2.minMaxLoc(result)</span><br><span class="line">scores.append(score)</span><br><span class="line"><span class="comment"># 得到最合适的数字</span></span><br><span class="line">groupOutput.append(<span class="built_in">str</span>(np.argmax(scores)))</span><br><span class="line"><span class="comment"># 画出来</span></span><br><span class="line">cv2.rectangle(image, (gX - <span class="number">5</span>, gY - <span class="number">5</span>),</span><br><span class="line">(gX + gW + <span class="number">5</span>, gY + gH + <span class="number">5</span>), (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>), <span class="number">1</span>)</span><br><span class="line">cv2.putText(image, <span class="string">""</span>.join(groupOutput), (gX, gY - <span class="number">15</span>),</span><br><span class="line">cv2.FONT_HERSHEY_SIMPLEX, <span class="number">0.65</span>, (<span class="number">0</span>, <span class="number">0</span>, <span class="number">255</span>), <span class="number">2</span>)</span><br><span class="line"><span class="comment"># 得到结果</span></span><br><span class="line">output.extend(groupOutput)</span><br><span class="line"><span class="comment"># 打印结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Credit Card Type: {}"</span>.<span class="built_in">format</span>(FIRST_NUMBER[output[<span class="number">0</span>]]))</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Credit Card #: {}"</span>.<span class="built_in">format</span>(<span class="string">""</span>.join(output)))</span><br><span class="line">cv2.imshow(<span class="string">"Image"</span>, image)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></tbody></table></figure><p>参考资料： 唐宇迪 OpenCV计算机视觉实战(Python版)</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ocr透视变换</title>
      <link href="/2019/10/13/20191013-ocr%E9%80%8F%E8%A7%86%E5%8F%98%E6%8D%A2/"/>
      <url>/2019/10/13/20191013-ocr%E9%80%8F%E8%A7%86%E5%8F%98%E6%8D%A2/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 导入工具包</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> argparse</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="comment"># 设置参数</span></span><br><span class="line">ap = argparse.ArgumentParser()</span><br><span class="line">ap.add_argument(<span class="string">"-i"</span>, <span class="string">"--image"</span>, required = <span class="literal">False</span>,</span><br><span class="line"><span class="built_in">help</span> = <span class="string">"Path to the image to be scanned"</span>,default=<span class="string">'images/page.jpg'</span>)</span><br><span class="line">args = <span class="built_in">vars</span>(ap.parse_args())</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">order_points</span>(<span class="params">pts</span>):</span><br><span class="line"><span class="comment"># 一共4个坐标点</span></span><br><span class="line">rect = np.zeros((<span class="number">4</span>, <span class="number">2</span>), dtype = <span class="string">"float32"</span>)</span><br><span class="line"><span class="comment"># 按顺序找到对应坐标0123分别是 左上，右上，右下，左下</span></span><br><span class="line"><span class="comment"># 计算左上，右下</span></span><br><span class="line">s = pts.<span class="built_in">sum</span>(axis = <span class="number">1</span>)</span><br><span class="line">rect[<span class="number">0</span>] = pts[np.argmin(s)]</span><br><span class="line">rect[<span class="number">2</span>] = pts[np.argmax(s)]</span><br><span class="line"><span class="comment"># 计算右上和左下</span></span><br><span class="line">diff = np.diff(pts, axis = <span class="number">1</span>)</span><br><span class="line">rect[<span class="number">1</span>] = pts[np.argmin(diff)]</span><br><span class="line">rect[<span class="number">3</span>] = pts[np.argmax(diff)]</span><br><span class="line"><span class="keyword">return</span> rect</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">four_point_transform</span>(<span class="params">image, pts</span>):</span><br><span class="line"><span class="comment"># 获取输入坐标点</span></span><br><span class="line">rect = order_points(pts)</span><br><span class="line">(tl, tr, br, bl) = rect</span><br><span class="line"><span class="comment"># 计算输入的w和h值</span></span><br><span class="line">widthA = np.sqrt(((br[<span class="number">0</span>] - bl[<span class="number">0</span>]) ** <span class="number">2</span>) + ((br[<span class="number">1</span>] - bl[<span class="number">1</span>]) ** <span class="number">2</span>))</span><br><span class="line">widthB = np.sqrt(((tr[<span class="number">0</span>] - tl[<span class="number">0</span>]) ** <span class="number">2</span>) + ((tr[<span class="number">1</span>] - tl[<span class="number">1</span>]) ** <span class="number">2</span>))</span><br><span class="line">maxWidth = <span class="built_in">max</span>(<span class="built_in">int</span>(widthA), <span class="built_in">int</span>(widthB))</span><br><span class="line">heightA = np.sqrt(((tr[<span class="number">0</span>] - br[<span class="number">0</span>]) ** <span class="number">2</span>) + ((tr[<span class="number">1</span>] - br[<span class="number">1</span>]) ** <span class="number">2</span>))</span><br><span class="line">heightB = np.sqrt(((tl[<span class="number">0</span>] - bl[<span class="number">0</span>]) ** <span class="number">2</span>) + ((tl[<span class="number">1</span>] - bl[<span class="number">1</span>]) ** <span class="number">2</span>))</span><br><span class="line">maxHeight = <span class="built_in">max</span>(<span class="built_in">int</span>(heightA), <span class="built_in">int</span>(heightB))</span><br><span class="line"><span class="comment"># 变换后对应坐标位置</span></span><br><span class="line">dst = np.array([</span><br><span class="line">[<span class="number">0</span>, <span class="number">0</span>],</span><br><span class="line">[maxWidth - <span class="number">1</span>, <span class="number">0</span>],</span><br><span class="line">[maxWidth - <span class="number">1</span>, maxHeight - <span class="number">1</span>],</span><br><span class="line">[<span class="number">0</span>, maxHeight - <span class="number">1</span>]], dtype = <span class="string">"float32"</span>)</span><br><span class="line"><span class="comment"># 计算变换矩阵</span></span><br><span class="line">M = cv2.getPerspectiveTransform(rect, dst)</span><br><span class="line">warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))</span><br><span class="line"><span class="comment"># 返回变换后结果</span></span><br><span class="line"><span class="keyword">return</span> warped</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">resize</span>(<span class="params">image, width=<span class="literal">None</span>, height=<span class="literal">None</span>, inter=cv2.INTER_AREA</span>):</span><br><span class="line">dim = <span class="literal">None</span></span><br><span class="line">(h, w) = image.shape[:<span class="number">2</span>]</span><br><span class="line"><span class="keyword">if</span> width <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">and</span> height <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line"><span class="keyword">return</span> image</span><br><span class="line"><span class="keyword">if</span> width <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">r = height / <span class="built_in">float</span>(h)</span><br><span class="line">dim = (<span class="built_in">int</span>(w * r), height)</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">r = width / <span class="built_in">float</span>(w)</span><br><span class="line">dim = (width, <span class="built_in">int</span>(h * r))</span><br><span class="line">resized = cv2.resize(image, dim, interpolation=inter)</span><br><span class="line"><span class="keyword">return</span> resized</span><br><span class="line"><span class="comment"># 读取输入</span></span><br><span class="line">image = cv2.imread(args[<span class="string">"image"</span>])</span><br><span class="line"><span class="comment">#坐标也会相同变化</span></span><br><span class="line">ratio = image.shape[<span class="number">0</span>] / <span class="number">500.0</span></span><br><span class="line">orig = image.copy()</span><br><span class="line">image = resize(orig, height = <span class="number">500</span>)</span><br><span class="line"><span class="comment"># 预处理</span></span><br><span class="line">gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)</span><br><span class="line">gray = cv2.GaussianBlur(gray, (<span class="number">5</span>, <span class="number">5</span>), <span class="number">0</span>)</span><br><span class="line">edged = cv2.Canny(gray, <span class="number">75</span>, <span class="number">200</span>)</span><br><span class="line"><span class="comment"># 展示预处理结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"STEP 1: 边缘检测"</span>)</span><br><span class="line">cv2.imshow(<span class="string">"Image"</span>, image)</span><br><span class="line">cv2.imshow(<span class="string">"Edged"</span>, edged)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line"><span class="comment"># 轮廓检测</span></span><br><span class="line">cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[<span class="number">1</span>]</span><br><span class="line">cnts = <span class="built_in">sorted</span>(cnts, key = cv2.contourArea, reverse = <span class="literal">True</span>)[:<span class="number">5</span>]</span><br><span class="line"><span class="comment"># 遍历轮廓</span></span><br><span class="line"><span class="keyword">for</span> c <span class="keyword">in</span> cnts:</span><br><span class="line"><span class="comment"># 计算轮廓近似</span></span><br><span class="line">peri = cv2.arcLength(c, <span class="literal">True</span>)</span><br><span class="line"><span class="comment"># C表示输入的点集</span></span><br><span class="line"><span class="comment"># epsilon表示从原始轮廓到近似轮廓的最大距离，它是一个准确度参数</span></span><br><span class="line"><span class="comment"># True表示封闭的</span></span><br><span class="line">approx = cv2.approxPolyDP(c, <span class="number">0.02</span> * peri, <span class="literal">True</span>)<span class="comment"># 越小越精准，长度的百分之多少作为精度</span></span><br><span class="line"><span class="comment"># 4个点的时候就拿出来</span></span><br><span class="line"><span class="keyword">if</span> <span class="built_in">len</span>(approx) == <span class="number">4</span>:</span><br><span class="line">screenCnt = approx</span><br><span class="line"><span class="keyword">break</span></span><br><span class="line"><span class="comment"># 展示结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"STEP 2: 获取轮廓"</span>)</span><br><span class="line">cv2.drawContours(image, [screenCnt], -<span class="number">1</span>, (<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>), <span class="number">2</span>)</span><br><span class="line">cv2.imshow(<span class="string">"Outline"</span>, image)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line"><span class="comment"># 透视变换</span></span><br><span class="line">warped = four_point_transform(orig, screenCnt.reshape(<span class="number">4</span>, <span class="number">2</span>) * ratio)</span><br><span class="line"><span class="comment"># 二值处理</span></span><br><span class="line">warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)</span><br><span class="line">ref = cv2.threshold(warped, <span class="number">100</span>, <span class="number">255</span>, cv2.THRESH_BINARY)[<span class="number">1</span>]</span><br><span class="line">cv2.imwrite(<span class="string">'scan.jpg'</span>, ref)</span><br><span class="line"><span class="comment"># 展示结果</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"STEP 3: 变换"</span>)</span><br><span class="line">cv2.imshow(<span class="string">"Original"</span>, resize(orig, height = <span class="number">650</span>))</span><br><span class="line">cv2.imshow(<span class="string">"Scanned"</span>, resize(ref, height = <span class="number">650</span>))</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br></pre></td></tr></tbody></table></figure><p>参考资料： 唐宇迪 OpenCV计算机视觉实战(Python版)</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>park停车场项目实战</title>
      <link href="/2019/10/13/20191013-park%E5%81%9C%E8%BD%A6%E5%9C%BA%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/"/>
      <url>/2019/10/13/20191013-park%E5%81%9C%E8%BD%A6%E5%9C%BA%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/</url>
      
        <content type="html"><![CDATA[<h2 id="park-类对象"><a href="#park-类对象" class="headerlink" title="park 类对象"></a>park 类对象</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br><span class="line">204</span><br><span class="line">205</span><br><span class="line">206</span><br><span class="line">207</span><br><span class="line">208</span><br><span class="line">209</span><br><span class="line">210</span><br><span class="line">211</span><br><span class="line">212</span><br><span class="line">213</span><br><span class="line">214</span><br><span class="line">215</span><br><span class="line">216</span><br><span class="line">217</span><br><span class="line">218</span><br><span class="line">219</span><br><span class="line">220</span><br><span class="line">221</span><br><span class="line">222</span><br><span class="line">223</span><br><span class="line">224</span><br><span class="line">225</span><br><span class="line">226</span><br><span class="line">227</span><br><span class="line">228</span><br><span class="line">229</span><br><span class="line">230</span><br><span class="line">231</span><br><span class="line">232</span><br><span class="line">233</span><br><span class="line">234</span><br><span class="line">235</span><br><span class="line">236</span><br><span class="line">237</span><br><span class="line">238</span><br><span class="line">239</span><br><span class="line">240</span><br><span class="line">241</span><br><span class="line">242</span><br><span class="line">243</span><br><span class="line">244</span><br><span class="line">245</span><br><span class="line">246</span><br><span class="line">247</span><br><span class="line">248</span><br><span class="line">249</span><br><span class="line">250</span><br><span class="line">251</span><br><span class="line">252</span><br><span class="line">253</span><br><span class="line">254</span><br><span class="line">255</span><br><span class="line">256</span><br><span class="line">257</span><br><span class="line">258</span><br><span class="line">259</span><br><span class="line">260</span><br><span class="line">261</span><br><span class="line">262</span><br><span class="line">263</span><br><span class="line">264</span><br><span class="line">265</span><br><span class="line">266</span><br><span class="line">267</span><br><span class="line">268</span><br><span class="line">269</span><br><span class="line">270</span><br><span class="line">271</span><br><span class="line">272</span><br><span class="line">273</span><br><span class="line">274</span><br><span class="line">275</span><br><span class="line">276</span><br><span class="line">277</span><br><span class="line">278</span><br><span class="line">279</span><br><span class="line">280</span><br><span class="line">281</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding=utf-8</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> os, glob</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Parking</span>:</span><br><span class="line">    <span class="comment"># 显示图片</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">show_images</span>(<span class="params">self, images, cmap=<span class="literal">None</span></span>):</span><br><span class="line">        cols = <span class="number">2</span></span><br><span class="line">        rows = (<span class="built_in">len</span>(images)+<span class="number">1</span>)//cols</span><br><span class="line">        plt.figure(figsize=(<span class="number">15</span>,<span class="number">12</span>))</span><br><span class="line">        <span class="keyword">for</span> i, image <span class="keyword">in</span> <span class="built_in">enumerate</span>(images):</span><br><span class="line">            plt.subplot(rows, cols, i+<span class="number">1</span>)</span><br><span class="line">            cmap = <span class="string">'gray'</span> <span class="keyword">if</span>  <span class="built_in">len</span>(image.shape)==<span class="number">2</span> <span class="keyword">else</span> cmap</span><br><span class="line">            plt.imshow(image, cmap=cmap)</span><br><span class="line">            plt.xticks([])</span><br><span class="line">            plt.yticks([])</span><br><span class="line">        plt.tight_layout(pad=<span class="number">0</span>, h_pad=<span class="number">0</span>, w_pad=<span class="number">0</span>)</span><br><span class="line">        plt.show()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">cv_show</span>(<span class="params">self, name, img</span>):</span><br><span class="line">        cv2.imshow(name,img)</span><br><span class="line">        cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">        cv2.destroyAllWindows()</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">select_rgb_white_yello</span>(<span class="params">self, image</span>):</span><br><span class="line">        <span class="comment"># 过滤掉背景</span></span><br><span class="line">        lower = np.uint8([<span class="number">120</span>,<span class="number">120</span>,<span class="number">120</span>])</span><br><span class="line">        upper = np.uint8([<span class="number">255</span>,<span class="number">255</span>,<span class="number">255</span>])</span><br><span class="line">        <span class="comment"># lower_red和高于upper_red的部分分别变成0，lower_red～upper_red之间的值变成255,相当于过滤背景</span></span><br><span class="line">        white_mask =  cv2.inRange(image,lower, upper)</span><br><span class="line">        self.cv_show(<span class="string">'white_mask'</span>, white_mask)</span><br><span class="line">        masked = cv2.bitwise_and(image, image, mask = white_mask)</span><br><span class="line">        self.cv_show(<span class="string">'masked'</span>, masked)</span><br><span class="line">        <span class="keyword">return</span> masked</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">convert_gray_sacle</span>(<span class="params">self,image</span>):</span><br><span class="line">        <span class="keyword">return</span> cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)</span><br><span class="line">    <span class="comment"># 检测边缘</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">detect_edges</span>(<span class="params">self, image, low_threshold=<span class="number">50</span>, high_threshold=<span class="number">200</span></span>):</span><br><span class="line">        <span class="keyword">return</span> cv2.Canny(image, low_threshold, high_threshold)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">filter_region</span>(<span class="params">self, image, vertices</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">           剔除不需要的地方</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        mask = np.zeros_like(image)</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(mask.shape) == <span class="number">2</span>:</span><br><span class="line">            cv2.fillPoly(mask, vertices, <span class="number">255</span>)</span><br><span class="line">            self.cv_show(<span class="string">'mask'</span>, mask)</span><br><span class="line">        <span class="keyword">return</span> cv2.bitwise_and(image, mask)</span><br><span class="line">    <span class="comment"># 手动选择区域</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">select_region</span>(<span class="params">self,image</span>):</span><br><span class="line">        <span class="string">"""</span></span><br><span class="line"><span class="string">                手动选择区域</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        <span class="comment"># first, define the polygon by vertices</span></span><br><span class="line">        rows, cols = image.shape[:<span class="number">2</span>]</span><br><span class="line">        pt_1  = [cols*<span class="number">0.05</span>, rows*<span class="number">0.90</span>]</span><br><span class="line">        pt_2 = [cols*<span class="number">0.05</span>, rows*<span class="number">0.70</span>]</span><br><span class="line">        pt_3 = [cols*<span class="number">0.30</span>, rows*<span class="number">0.55</span>]</span><br><span class="line">        pt_4 = [cols*<span class="number">0.6</span>, rows*<span class="number">0.15</span>]</span><br><span class="line">        pt_5 = [cols*<span class="number">0.90</span>, rows*<span class="number">0.15</span>]</span><br><span class="line">        pt_6 = [cols*<span class="number">0.90</span>, rows*<span class="number">0.90</span>]</span><br><span class="line">        vertices = np.array([[pt_1, pt_2, pt_3, pt_4, pt_5, pt_6]], dtype=np.int32)</span><br><span class="line">        point_img = image.copy()</span><br><span class="line">        point_img = cv2.cvtColor(point_img, cv2.COLOR_GRAY2RGB)</span><br><span class="line">        <span class="keyword">for</span> point <span class="keyword">in</span> vertices[<span class="number">0</span>]:</span><br><span class="line">            cv2.circle(point_img, (point[<span class="number">0</span>],point[<span class="number">1</span>]), <span class="number">10</span>, (<span class="number">0</span>,<span class="number">0</span>,<span class="number">255</span>), <span class="number">4</span>)</span><br><span class="line">        self.cv_show(<span class="string">'point_img'</span>,point_img)</span><br><span class="line">        <span class="keyword">return</span> self.filter_region(image, vertices)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">hough_line</span>(<span class="params">self, image</span>):</span><br><span class="line">        <span class="comment"># 输入的图像需要是边缘检测后的结果</span></span><br><span class="line">        <span class="comment"># minLineLengh(线的最短长度，比这个短的都被忽略)和MaxLineCap（两条直线之间的最大间隔，小于此值，认为是一条直线）</span></span><br><span class="line">        <span class="comment"># rho距离精度,theta角度精度,threshod超过设定阈值才被检测出线段</span></span><br><span class="line">        <span class="keyword">return</span> cv2.HoughLinesP(image, rho=<span class="number">0.1</span>, theta=np.pi/<span class="number">10</span>, threshold=<span class="number">15</span>, minLineLength=<span class="number">9</span>, maxLineGap=<span class="number">4</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">draw_lines</span>(<span class="params">self, image, lines, color=[<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>], thickness=<span class="number">2</span>, make_copy=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="comment"># 过滤霍夫变换检测得到直线</span></span><br><span class="line">        <span class="keyword">if</span> make_copy:</span><br><span class="line">            image = np.copy(image)</span><br><span class="line">        cleaned = []</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">            <span class="keyword">for</span> x1, y1, x2, y2 <span class="keyword">in</span> line:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">abs</span>(y2-y1) &lt;=<span class="number">1</span> <span class="keyword">and</span> <span class="built_in">abs</span>(x2-x1) &gt;=<span class="number">25</span> <span class="keyword">and</span> <span class="built_in">abs</span>(x2-x1) &lt;= <span class="number">55</span>:</span><br><span class="line">                    cleaned.append((x1,y1,x2,y2))</span><br><span class="line">                    cv2.line(image, (x1, y1), (x2, y2), color, thickness)</span><br><span class="line">        <span class="built_in">print</span>(<span class="string">'No lines detected: '</span>, <span class="built_in">len</span>(cleaned))</span><br><span class="line">        <span class="keyword">return</span> image</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">identify_blocks</span>(<span class="params">self, image, lines, make_copy=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">if</span> make_copy:</span><br><span class="line">            new_image = np.copy(image)</span><br><span class="line">        <span class="comment"># step 1: 过滤部分直线</span></span><br><span class="line">        cleaned = []</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> lines:</span><br><span class="line">            <span class="keyword">for</span> x1,y1,x2,y2 <span class="keyword">in</span> line:</span><br><span class="line">                <span class="keyword">if</span> <span class="built_in">abs</span>(y2-y1) &lt;=<span class="number">1</span> <span class="keyword">and</span> <span class="built_in">abs</span>(x2-x1) &gt;=<span class="number">25</span> <span class="keyword">and</span> <span class="built_in">abs</span>(x2-x1) &lt;= <span class="number">55</span>:</span><br><span class="line">                    cleaned.append((x1,y1,x2,y2))</span><br><span class="line">        <span class="comment"># step 2: 对直线按照x1进行排序</span></span><br><span class="line">        <span class="keyword">import</span> operator</span><br><span class="line">        list1 = <span class="built_in">sorted</span>(cleaned, key=operator.itemgetter(<span class="number">0</span>,<span class="number">1</span>))</span><br><span class="line">        <span class="comment"># &gt;&gt;&gt; b=operator.itemgetter(1,0)   //定义函数b，获取对象的第1个域和第0个的值</span></span><br><span class="line">        <span class="comment"># &gt;&gt;&gt; b(a)</span></span><br><span class="line">        <span class="comment"># (2, 1)</span></span><br><span class="line">        <span class="comment"># step 3: 找到多个列，相当于每列是一排车</span></span><br><span class="line">        clusters = {}</span><br><span class="line">        dIndex = <span class="number">0</span></span><br><span class="line">        clus_dist = <span class="number">10</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(list1)-<span class="number">1</span>):</span><br><span class="line">            distance = <span class="built_in">abs</span>(list1[i+<span class="number">1</span>][<span class="number">0</span>] - list1[i][<span class="number">0</span>])</span><br><span class="line">            <span class="keyword">if</span> distance &lt;= clus_dist:</span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> dIndex <span class="keyword">in</span> clusters.keys(): clusters[dIndex] = []</span><br><span class="line">                clusters[dIndex].append(list1[i])</span><br><span class="line">                clusters[dIndex].append(list1[i + <span class="number">1</span>])</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                dIndex += <span class="number">1</span></span><br><span class="line">        <span class="comment"># step 4: 得到坐标</span></span><br><span class="line">        rects = {}</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> clusters:</span><br><span class="line">            all_list = clusters[key]</span><br><span class="line">            cleaned = <span class="built_in">list</span>(<span class="built_in">set</span>(all_list))</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">len</span>(cleaned) &gt; <span class="number">5</span>:</span><br><span class="line">                cleaned = <span class="built_in">sorted</span>(cleaned, key=<span class="keyword">lambda</span> tup: tup[<span class="number">1</span>])</span><br><span class="line">                avg_y1 = cleaned[<span class="number">0</span>][<span class="number">1</span>]</span><br><span class="line">                avg_y2 = cleaned[-<span class="number">1</span>][<span class="number">1</span>]</span><br><span class="line">                avg_x1 = <span class="number">0</span></span><br><span class="line">                avg_x2 = <span class="number">0</span></span><br><span class="line">                <span class="keyword">for</span> tup <span class="keyword">in</span> cleaned:</span><br><span class="line">                    avg_x1 += tup[<span class="number">0</span>]</span><br><span class="line">                    avg_x2 += tup[<span class="number">2</span>]</span><br><span class="line">                avg_x1 = avg_x1/<span class="built_in">len</span>(cleaned)</span><br><span class="line">                avg_x2 = avg_x2/<span class="built_in">len</span>(cleaned)</span><br><span class="line">                rects[i] = (avg_x1, avg_y1, avg_x2, avg_y2)</span><br><span class="line">                i += <span class="number">1</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"Num Parking Lanes: "</span>, <span class="built_in">len</span>(rects))</span><br><span class="line">        <span class="comment"># step 5: 把矩形画出来</span></span><br><span class="line">        buff = <span class="number">7</span></span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> rects:</span><br><span class="line">            tup_topLeft = (<span class="built_in">int</span>(rects[key][<span class="number">0</span>] - buff), <span class="built_in">int</span>(rects[key][<span class="number">1</span>]))</span><br><span class="line">            tup_botRight = (<span class="built_in">int</span>(rects[key][<span class="number">2</span>] + buff), <span class="built_in">int</span>(rects[key][<span class="number">3</span>]))</span><br><span class="line">            cv2.rectangle(new_image, tup_topLeft,tup_botRight,(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">3</span>)</span><br><span class="line">        <span class="keyword">return</span> new_image, rects</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">draw_parking</span>(<span class="params">self, image, rects, make_copy=<span class="literal">True</span>, color=[<span class="number">255</span>,<span class="number">0</span>,<span class="number">0</span>],thickness=<span class="number">2</span>, save=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="keyword">if</span> make_copy:</span><br><span class="line">            new_image = np.copy(image)</span><br><span class="line">        gap = <span class="number">15.5</span></span><br><span class="line">        spot_dict = {} <span class="comment"># 字典：一个车位对应一个位置</span></span><br><span class="line">        tot_spots = <span class="number">0</span></span><br><span class="line">        <span class="comment">#微调</span></span><br><span class="line">        adj_y1 = {<span class="number">0</span>: <span class="number">20</span>, <span class="number">1</span>:-<span class="number">10</span>, <span class="number">2</span>:<span class="number">0</span>, <span class="number">3</span>:-<span class="number">11</span>, <span class="number">4</span>:<span class="number">28</span>, <span class="number">5</span>:<span class="number">5</span>, <span class="number">6</span>:-<span class="number">15</span>, <span class="number">7</span>:-<span class="number">15</span>, <span class="number">8</span>:-<span class="number">10</span>, <span class="number">9</span>:-<span class="number">30</span>, <span class="number">10</span>:<span class="number">9</span>, <span class="number">11</span>:-<span class="number">32</span>}</span><br><span class="line">        adj_y2 = {<span class="number">0</span>: <span class="number">30</span>, <span class="number">1</span>: <span class="number">50</span>, <span class="number">2</span>:<span class="number">15</span>, <span class="number">3</span>:<span class="number">10</span>, <span class="number">4</span>:-<span class="number">15</span>, <span class="number">5</span>:<span class="number">15</span>, <span class="number">6</span>:<span class="number">15</span>, <span class="number">7</span>:-<span class="number">20</span>, <span class="number">8</span>:<span class="number">15</span>, <span class="number">9</span>:<span class="number">15</span>, <span class="number">10</span>:<span class="number">0</span>, <span class="number">11</span>:<span class="number">30</span>}</span><br><span class="line">        adj_x1 = {<span class="number">0</span>: -<span class="number">8</span>, <span class="number">1</span>:-<span class="number">15</span>, <span class="number">2</span>:-<span class="number">15</span>, <span class="number">3</span>:-<span class="number">15</span>, <span class="number">4</span>:-<span class="number">15</span>, <span class="number">5</span>:-<span class="number">15</span>, <span class="number">6</span>:-<span class="number">15</span>, <span class="number">7</span>:-<span class="number">15</span>, <span class="number">8</span>:-<span class="number">10</span>, <span class="number">9</span>:-<span class="number">10</span>, <span class="number">10</span>:-<span class="number">10</span>, <span class="number">11</span>:<span class="number">0</span>}</span><br><span class="line">        adj_x2 = {<span class="number">0</span>: <span class="number">0</span>, <span class="number">1</span>: <span class="number">15</span>, <span class="number">2</span>:<span class="number">15</span>, <span class="number">3</span>:<span class="number">15</span>, <span class="number">4</span>:<span class="number">15</span>, <span class="number">5</span>:<span class="number">15</span>, <span class="number">6</span>:<span class="number">15</span>, <span class="number">7</span>:<span class="number">15</span>, <span class="number">8</span>:<span class="number">10</span>, <span class="number">9</span>:<span class="number">10</span>, <span class="number">10</span>:<span class="number">10</span>, <span class="number">11</span>:<span class="number">0</span>}</span><br><span class="line">        <span class="keyword">for</span> key <span class="keyword">in</span> rects:</span><br><span class="line">            tup = rects[key]</span><br><span class="line">            x1 = <span class="built_in">int</span>(tup[<span class="number">0</span>]+ adj_x1[key])</span><br><span class="line">            x2 = <span class="built_in">int</span>(tup[<span class="number">2</span>]+ adj_x2[key])</span><br><span class="line">            y1 = <span class="built_in">int</span>(tup[<span class="number">1</span>] + adj_y1[key])</span><br><span class="line">            y2 = <span class="built_in">int</span>(tup[<span class="number">3</span>] + adj_y2[key])</span><br><span class="line">            cv2.rectangle(new_image, (x1, y1),(x2,y2),(<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>),<span class="number">2</span>)</span><br><span class="line">            num_splits = <span class="built_in">int</span>(<span class="built_in">abs</span>(y2-y1)//gap)</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_splits+<span class="number">1</span>):</span><br><span class="line">                y = <span class="built_in">int</span>(y1 + i*gap)</span><br><span class="line">                cv2.line(new_image, (x1, y), (x2, y), color, thickness)</span><br><span class="line">            <span class="keyword">if</span> key &gt; <span class="number">0</span> <span class="keyword">and</span> key &lt; <span class="built_in">len</span>(rects) -<span class="number">1</span> :</span><br><span class="line">                <span class="comment">#竖直线</span></span><br><span class="line">                x = <span class="built_in">int</span>((x1 + x2)/<span class="number">2</span>)</span><br><span class="line">                cv2.line(new_image, (x, y1), (x, y2), color, thickness)</span><br><span class="line">            <span class="comment"># 计算数量</span></span><br><span class="line">            <span class="keyword">if</span> key == <span class="number">0</span> <span class="keyword">or</span> key == (<span class="built_in">len</span>(rects) -<span class="number">1</span>):</span><br><span class="line">                tot_spots += num_splits +<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                tot_spots += <span class="number">2</span>*(num_splits +<span class="number">1</span>)</span><br><span class="line">            <span class="comment"># 字典对应好</span></span><br><span class="line">            <span class="keyword">if</span> key == <span class="number">0</span> <span class="keyword">or</span> key == (<span class="built_in">len</span>(rects) -<span class="number">1</span>):</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_splits+<span class="number">1</span>):</span><br><span class="line">                    cur_len = <span class="built_in">len</span>(spot_dict)</span><br><span class="line">                    y = <span class="built_in">int</span>(y1 + i*gap)</span><br><span class="line">                    spot_dict[(x1, y, x2, y+gap)] = cur_len +<span class="number">1</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">0</span>, num_splits+<span class="number">1</span>):</span><br><span class="line">                    cur_len = <span class="built_in">len</span>(spot_dict)</span><br><span class="line">                    y = <span class="built_in">int</span>(y1 + i*gap)</span><br><span class="line">                    x = <span class="built_in">int</span>((x1 + x2)/<span class="number">2</span>)</span><br><span class="line">                    spot_dict[(x1, y, x, y+gap)] = cur_len +<span class="number">1</span></span><br><span class="line">                    spot_dict[(x, y, x2, y+gap)] = cur_len +<span class="number">2</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"total parking spaces: "</span>, tot_spots, cur_len)</span><br><span class="line">        <span class="keyword">if</span> save:</span><br><span class="line">            filename = <span class="string">'with_parking.jpg'</span></span><br><span class="line">            cv2.imwrite(filename, new_image)</span><br><span class="line">        <span class="keyword">return</span> new_image, spot_dict</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">assign_spots_map</span>(<span class="params">self,image, spot_dict, make_copy = <span class="literal">True</span>, color=[<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>], thickness=<span class="number">2</span></span>):</span><br><span class="line">        <span class="keyword">if</span> make_copy:</span><br><span class="line">            new_image = np.copy(image)</span><br><span class="line">        <span class="keyword">for</span> spot <span class="keyword">in</span> spot_dict.keys():</span><br><span class="line">            (x1, y1, x2, y2) = spot</span><br><span class="line">            cv2.rectangle(new_image, (<span class="built_in">int</span>(x1),<span class="built_in">int</span>(y1)), (<span class="built_in">int</span>(x2),<span class="built_in">int</span>(y2)), color, thickness)</span><br><span class="line">        <span class="keyword">return</span> new_image</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">save_images_for_cnn</span>(<span class="params">self,image, spot_dict, folder_name =<span class="string">'cnn_data'</span></span>):</span><br><span class="line">        <span class="keyword">for</span> spot <span class="keyword">in</span> spot_dict.keys():</span><br><span class="line">            (x1, y1, x2, y2) = spot</span><br><span class="line">            (x1, y1, x2, y2) = (<span class="built_in">int</span>(x1), <span class="built_in">int</span>(y1), <span class="built_in">int</span>(x2), <span class="built_in">int</span>(y2))</span><br><span class="line">            <span class="comment">#裁剪图像</span></span><br><span class="line">            spot_img = image[y1:y2, x1:x2]</span><br><span class="line">            spot_img = cv2.resize(spot_img, (<span class="number">0</span>,<span class="number">0</span>), fx=<span class="number">2.0</span>, fy=<span class="number">2.0</span>)</span><br><span class="line">            spot_id = spot_dict[spot]</span><br><span class="line">            filename = <span class="string">'spot'</span> + <span class="built_in">str</span>(spot_id) +<span class="string">'.jpg'</span></span><br><span class="line">            <span class="built_in">print</span>(spot_img.shape, filename, (x1,x2,y1,y2))</span><br><span class="line">            cv2.imwrite(os.path.join(folder_name, filename), spot_img)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">make_prediction</span>(<span class="params">self,image, model, class_dictionary</span>):</span><br><span class="line">        <span class="comment"># 预处理</span></span><br><span class="line">        img = image/<span class="number">255.</span></span><br><span class="line">        <span class="comment"># 转换成4D tensor</span></span><br><span class="line">        image = np.expand_dims(img,axis=<span class="number">0</span>)</span><br><span class="line">        <span class="comment"># 用训练好的模型进行训练</span></span><br><span class="line">        class_predicted = model.predict(image)</span><br><span class="line">        inID = np.argmax(class_predicted[<span class="number">0</span>])</span><br><span class="line">        label = class_dictionary[inID]</span><br><span class="line">        <span class="keyword">return</span> label</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict_on_image</span>(<span class="params">self, image, spot_dict, model, class_dictionary, make_copy=<span class="literal">True</span>, color = [<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>], alpha=<span class="number">0.5</span></span>):</span><br><span class="line">        <span class="keyword">if</span> make_copy:</span><br><span class="line">            new_image = np.copy(image)</span><br><span class="line">            overlay = np.copy(image)</span><br><span class="line">        self.cv_show(<span class="string">'new_image'</span>,new_image)</span><br><span class="line">        cnt_empty = <span class="number">0</span></span><br><span class="line">        all_spots = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> spot <span class="keyword">in</span> spot_dict.key():</span><br><span class="line">            all_spots += <span class="number">1</span></span><br><span class="line">            (x1, y1, x2, y2) = spot</span><br><span class="line">            (x1, y1, x2, y2) = (<span class="built_in">int</span>(x1), <span class="built_in">int</span>(y1), <span class="built_in">int</span>(x2), <span class="built_in">int</span>(y2))</span><br><span class="line">            spot_img = image[y1:y2, x1:x2]</span><br><span class="line">            spot_img = cv2.resize(spot_img, (<span class="number">48</span>, <span class="number">48</span>))</span><br><span class="line">            label = self.make_prediction(spot_img,model,class_dictionary)</span><br><span class="line">            <span class="keyword">if</span> label == <span class="string">'empty'</span>:</span><br><span class="line">                cv2.rectangle(overlay, (<span class="built_in">int</span>(x1),<span class="built_in">int</span>(y1)), (<span class="built_in">int</span>(x2),<span class="built_in">int</span>(y2)), color, -<span class="number">1</span>)</span><br><span class="line">                cnt_empty += <span class="number">1</span></span><br><span class="line">        cv2.addWeighted(overlay, alpha, new_image, <span class="number">1</span> - alpha, <span class="number">0</span>, new_image) <span class="comment"># 图像融合</span></span><br><span class="line">        cv2.putText(new_image, <span class="string">"Available: %d spots"</span> %cnt_empty, (<span class="number">30</span>, <span class="number">95</span>),</span><br><span class="line">        cv2.FONT_HERSHEY_SIMPLEX,</span><br><span class="line">        <span class="number">0.7</span>, (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>), <span class="number">2</span>)</span><br><span class="line">        cv2.putText(new_image, <span class="string">"Total: %d spots"</span> %all_spots, (<span class="number">30</span>, <span class="number">125</span>),</span><br><span class="line">        cv2.FONT_HERSHEY_SIMPLEX,</span><br><span class="line">        <span class="number">0.7</span>, (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>), <span class="number">2</span>)</span><br><span class="line">                save = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> save:</span><br><span class="line">            filename = <span class="string">'with_marking.jpg'</span></span><br><span class="line">            cv2.imwrite(filename, new_image)</span><br><span class="line">        self.cv_show(<span class="string">'new_image'</span>,new_image)</span><br><span class="line">        <span class="keyword">return</span> new_image</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">predict_on_video</span>(<span class="params">self,video_name,final_spot_dict, model,class_dictionary,ret=<span class="literal">True</span></span>):</span><br><span class="line">        cap = cv2.VideoCapture(video_name)</span><br><span class="line">        count = <span class="number">0</span></span><br><span class="line">        <span class="keyword">while</span> ret:</span><br><span class="line">            ret, image = cap.read()</span><br><span class="line">            count += <span class="number">1</span></span><br><span class="line">            <span class="keyword">if</span> count == <span class="number">5</span>:</span><br><span class="line">                count = <span class="number">0</span></span><br><span class="line">                new_image = np.copy(image)</span><br><span class="line">                overlay = np.copy(image)</span><br><span class="line">                cnt_empty = <span class="number">0</span></span><br><span class="line">                all_spots = <span class="number">0</span></span><br><span class="line">                color = [<span class="number">0</span>, <span class="number">255</span>, <span class="number">0</span>]</span><br><span class="line">                alpha=<span class="number">0.5</span></span><br><span class="line">                <span class="keyword">for</span> spot <span class="keyword">in</span> final_spot_dict.keys():</span><br><span class="line">                    all_spots += <span class="number">1</span></span><br><span class="line">                    (x1, y1, x2, y2) = spot</span><br><span class="line">                    (x1, y1, x2, y2) = (<span class="built_in">int</span>(x1), <span class="built_in">int</span>(y1), <span class="built_in">int</span>(x2), <span class="built_in">int</span>(y2))</span><br><span class="line">                    spot_img = image[y1:y2, x1:x2]</span><br><span class="line">                    spot_img = cv2.resize(spot_img, (<span class="number">48</span>,<span class="number">48</span>))</span><br><span class="line">                    label = self.make_prediction(spot_img,model,class_dictionary)</span><br><span class="line">                    <span class="keyword">if</span> label == <span class="string">'empty'</span>:</span><br><span class="line">                        cv2.rectangle(overlay, (<span class="built_in">int</span>(x1),<span class="built_in">int</span>(y1)), (<span class="built_in">int</span>(x2),<span class="built_in">int</span>(y2)), color, -<span class="number">1</span>)</span><br><span class="line">                        cnt_empty += <span class="number">1</span></span><br><span class="line">                cv2.addWeighted(overlay, alpha, new_image, <span class="number">1</span> - alpha, <span class="number">0</span>, new_image)</span><br><span class="line">                cv2.putText(new_image, <span class="string">"Available: %d spots"</span> %cnt_empty, (<span class="number">30</span>, <span class="number">95</span>),</span><br><span class="line">                cv2.FONT_HERSHEY_SIMPLEX,</span><br><span class="line">                <span class="number">0.7</span>, (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>), <span class="number">2</span>)</span><br><span class="line">                cv2.putText(new_image, <span class="string">"Total: %d spots"</span> %all_spots, (<span class="number">30</span>, <span class="number">125</span>),</span><br><span class="line">                cv2.FONT_HERSHEY_SIMPLEX,</span><br><span class="line">                <span class="number">0.7</span>, (<span class="number">255</span>, <span class="number">255</span>, <span class="number">255</span>), <span class="number">2</span>)</span><br><span class="line">                cv2.imshow(<span class="string">'frame'</span>, new_image)</span><br><span class="line">                <span class="keyword">if</span> cv2.waitKey(<span class="number">10</span>) &amp; <span class="number">0xFF</span> == <span class="built_in">ord</span>(<span class="string">'q'</span>):</span><br><span class="line">                    <span class="keyword">break</span></span><br><span class="line">        cv2.destroyAllWindows()</span><br><span class="line">        cap.release()</span><br></pre></td></tr></tbody></table></figure><h2 id="test模块"><a href="#test模块" class="headerlink" title="test模块"></a>test模块</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> division</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> os, glob</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">from</span> keras.applications.imagenet_utils <span class="keyword">import</span> preprocess_input</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> load_model</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing <span class="keyword">import</span> image</span><br><span class="line"><span class="keyword">from</span> Parking <span class="keyword">import</span> Parking</span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line">cwd = os.getcwd()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">img_process</span>(<span class="params">test_images,park</span>):</span><br><span class="line">    white_yellow_images = <span class="built_in">list</span>(<span class="built_in">map</span>(park.select_rgb_white_yellow, test_images))</span><br><span class="line">    park.show_images(white_yellow_images)</span><br><span class="line">    gray_images = <span class="built_in">list</span>(<span class="built_in">map</span>(park.convert_gray_scale, white_yellow_images))</span><br><span class="line">    park.show_images(gray_images)</span><br><span class="line">    edge_images = <span class="built_in">list</span>(<span class="built_in">map</span>(<span class="keyword">lambda</span> image: park.detect_edges(image), gray_images))</span><br><span class="line">    park.show_images(edge_images)</span><br><span class="line">    roi_images = <span class="built_in">list</span>(<span class="built_in">map</span>(park.select_region, edge_images))</span><br><span class="line">    park.show_images(roi_images)</span><br><span class="line">    list_of_lines = <span class="built_in">list</span>(<span class="built_in">map</span>(park.hough_lines, roi_images))</span><br><span class="line">    line_images = []</span><br><span class="line">    <span class="keyword">for</span> image, lines <span class="keyword">in</span> <span class="built_in">zip</span>(test_images, list_of_lines):</span><br><span class="line">        line_images.append(park.draw_lines(image, lines))</span><br><span class="line">    park.show_images(line_images)</span><br><span class="line">    rect_images = []</span><br><span class="line">    rect_coords = []</span><br><span class="line">    <span class="keyword">for</span> image, lines <span class="keyword">in</span> <span class="built_in">zip</span>(test_images, list_of_lines):</span><br><span class="line">        new_image, rects = park.identify_blocks(image, lines)</span><br><span class="line">        rect_images.append(new_image)</span><br><span class="line">        rect_coords.append(rects)</span><br><span class="line">    park.show_images(rect_images)</span><br><span class="line">    delineated = []</span><br><span class="line">    spot_pos = []</span><br><span class="line">    <span class="keyword">for</span> image, rects <span class="keyword">in</span> <span class="built_in">zip</span>(test_images, rect_coords):</span><br><span class="line">        new_image, spot_dict = park.draw_parking(image, rects)</span><br><span class="line">        delineated.append(new_image)</span><br><span class="line">        spot_pos.append(spot_dict)</span><br><span class="line">    park.show_images(delineated)</span><br><span class="line">    final_spot_dict = spot_pos[<span class="number">1</span>]</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">len</span>(final_spot_dict))</span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'spot_dict.pickle'</span>, <span class="string">'wb'</span>) <span class="keyword">as</span> handle:</span><br><span class="line">        pickle.dump(final_spot_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)</span><br><span class="line">    park.save_images_for_cnn(test_images[<span class="number">0</span>],final_spot_dict)</span><br><span class="line">    <span class="keyword">return</span> final_spot_dict</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">keras_model</span>(<span class="params">weights_path</span>):</span><br><span class="line">    model = load_model(weights_path)</span><br><span class="line">    <span class="keyword">return</span> model</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">img_test</span>(<span class="params">test_images,final_spot_dict,model,class_dictionary</span>):</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span> (<span class="built_in">len</span>(test_images)):</span><br><span class="line">        predicted_images = park.predict_on_image(test_images[i],final_spot_dict,model,class_dictionary)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">video_test</span>(<span class="params">video_name,final_spot_dict,model,class_dictionary</span>):</span><br><span class="line">    name = video_name</span><br><span class="line">    cap = cv2.VideoCapture(name)</span><br><span class="line">    park.predict_on_video(name,final_spot_dict,model,class_dictionary,ret=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    test_images = [plt.imread(path) <span class="keyword">for</span> path <span class="keyword">in</span> glob.glob(<span class="string">'test_images/*.jpg'</span>)]</span><br><span class="line">    weights_path = <span class="string">'car1.h5'</span></span><br><span class="line">    video_name = <span class="string">'parking_video.mp4'</span></span><br><span class="line">    class_dictionary = {}</span><br><span class="line">    class_dictionary[<span class="number">0</span>] = <span class="string">'empty'</span></span><br><span class="line">    class_dictionary[<span class="number">1</span>] = <span class="string">'occupied'</span></span><br><span class="line">    park = Parking()</span><br><span class="line">    park.show_images(test_images)</span><br><span class="line">    final_spot_dict = img_process(test_images,park)</span><br><span class="line">    model = keras_model(weights_path)</span><br><span class="line">    img_test(test_images,final_spot_dict,model,class_dictionary)</span><br><span class="line">    video_test(video_name,final_spot_dict,model,class_dictionary)</span><br></pre></td></tr></tbody></table></figure><h2 id="利用cnn训练出一个二分类网络"><a href="#利用cnn训练出一个二分类网络" class="headerlink" title="利用cnn训练出一个二分类网络"></a>利用cnn训练出一个二分类网络</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> applications</span><br><span class="line"><span class="keyword">from</span> keras.preprocessing.image <span class="keyword">import</span> ImageDataGenerator</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> optimizers</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential, Model</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dropout, Flatten, Dense, GlobalAveragePooling2D</span><br><span class="line"><span class="keyword">from</span> keras <span class="keyword">import</span> backend <span class="keyword">as</span> k</span><br><span class="line"><span class="keyword">from</span> keras.callbacks <span class="keyword">import</span> ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers.normalization <span class="keyword">import</span> BatchNormalization</span><br><span class="line"><span class="keyword">from</span> keras.layers.convolutional <span class="keyword">import</span> Conv2D</span><br><span class="line"><span class="keyword">from</span> keras.layers.convolutional <span class="keyword">import</span> MaxPooling2D</span><br><span class="line"><span class="keyword">from</span> keras.initializers <span class="keyword">import</span> TruncatedNormal</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Activation</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Flatten</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dropout</span><br><span class="line"><span class="keyword">from</span> keras.layers.core <span class="keyword">import</span> Dense</span><br><span class="line">files_train = <span class="number">0</span></span><br><span class="line">files_validation = <span class="number">0</span></span><br><span class="line">cwd = os.getcwd()</span><br><span class="line">folder = <span class="string">'train_data/train'</span></span><br><span class="line"><span class="keyword">for</span> sub_folder <span class="keyword">in</span> os.listdir(folder):</span><br><span class="line">    path, dirs, files = <span class="built_in">next</span>(os.walk(os.path.join(folder,sub_folder)))</span><br><span class="line">    files_train += <span class="built_in">len</span>(files)</span><br><span class="line">folder = <span class="string">'train_data/test'</span></span><br><span class="line"><span class="keyword">for</span> sub_folder <span class="keyword">in</span> os.listdir(folder):</span><br><span class="line">    path, dirs, files = <span class="built_in">next</span>(os.walk(os.path.join(folder,sub_folder)))</span><br><span class="line">    files_validation += <span class="built_in">len</span>(files)</span><br><span class="line"><span class="built_in">print</span>(files_train,files_validation)</span><br><span class="line">img_width, img_height = <span class="number">48</span>, <span class="number">48</span></span><br><span class="line">train_data_dir = <span class="string">"train_data/train"</span></span><br><span class="line">validation_data_dir = <span class="string">"train_data/test"</span></span><br><span class="line">nb_train_samples = files_train</span><br><span class="line">nb_validation_samples = files_validation</span><br><span class="line">batch_size = <span class="number">32</span></span><br><span class="line">epochs = <span class="number">15</span></span><br><span class="line">num_classes = <span class="number">2</span></span><br><span class="line">model = applications.VGG16(weights=<span class="string">'imagenet'</span>, include_top=<span class="literal">False</span>, input_shape = (img_width, img_height, <span class="number">3</span>))</span><br><span class="line"><span class="keyword">for</span> layer <span class="keyword">in</span> model.layers[:<span class="number">10</span>]:</span><br><span class="line">    layer.trainable = <span class="literal">False</span></span><br><span class="line">x = model.output</span><br><span class="line">x = Flatten()(x)</span><br><span class="line">predictions = Dense(num_classes, activation=<span class="string">"softmax"</span>)(x)</span><br><span class="line">model_final = Model(<span class="built_in">input</span> = model.<span class="built_in">input</span>, output = predictions)</span><br><span class="line">model_final.<span class="built_in">compile</span>(loss = <span class="string">"categorical_crossentropy"</span>,</span><br><span class="line">                    optimizer = optimizers.SGD(lr=<span class="number">0.0001</span>, momentum=<span class="number">0.9</span>),</span><br><span class="line">                    metrics=[<span class="string">"accuracy"</span>])</span><br><span class="line">train_datagen = ImageDataGenerator(</span><br><span class="line">rescale = <span class="number">1.</span>/<span class="number">255</span>,</span><br><span class="line">horizontal_flip = <span class="literal">True</span>,</span><br><span class="line">fill_mode = <span class="string">"nearest"</span>,</span><br><span class="line">zoom_range = <span class="number">0.1</span>,</span><br><span class="line">width_shift_range = <span class="number">0.1</span>,</span><br><span class="line">height_shift_range=<span class="number">0.1</span>,</span><br><span class="line">rotation_range=<span class="number">5</span>)</span><br><span class="line">test_datagen = ImageDataGenerator(</span><br><span class="line">rescale = <span class="number">1.</span>/<span class="number">255</span>,</span><br><span class="line">horizontal_flip = <span class="literal">True</span>,</span><br><span class="line">fill_mode = <span class="string">"nearest"</span>,</span><br><span class="line">zoom_range = <span class="number">0.1</span>,</span><br><span class="line">width_shift_range = <span class="number">0.1</span>,</span><br><span class="line">height_shift_range=<span class="number">0.1</span>,</span><br><span class="line">rotation_range=<span class="number">5</span>)</span><br><span class="line">train_generator = train_datagen.flow_from_directory(</span><br><span class="line">train_data_dir,</span><br><span class="line">target_size = (img_height, img_width),</span><br><span class="line">batch_size = batch_size,</span><br><span class="line">class_mode = <span class="string">"categorical"</span>)</span><br><span class="line">validation_generator = test_datagen.flow_from_directory(</span><br><span class="line">validation_data_dir,</span><br><span class="line">target_size = (img_height, img_width),</span><br><span class="line">class_mode = <span class="string">"categorical"</span>)</span><br><span class="line">checkpoint = ModelCheckpoint(<span class="string">"car1.h5"</span>, monitor=<span class="string">'val_acc'</span>, verbose=<span class="number">1</span>, save_best_only=<span class="literal">True</span>, save_weights_only=<span class="literal">False</span>, mode=<span class="string">'auto'</span>, period=<span class="number">1</span>)</span><br><span class="line">early = EarlyStopping(monitor=<span class="string">'val_acc'</span>, min_delta=<span class="number">0</span>, patience=<span class="number">10</span>, verbose=<span class="number">1</span>, mode=<span class="string">'auto'</span>)</span><br><span class="line">history_object = model_final.fit_generator(</span><br><span class="line">train_generator,</span><br><span class="line">samples_per_epoch = nb_train_samples,</span><br><span class="line">epochs = epochs,</span><br><span class="line">validation_data = validation_generator,</span><br><span class="line">nb_val_samples = nb_validation_samples,</span><br><span class="line">callbacks = [checkpoint, early])</span><br></pre></td></tr></tbody></table></figure><p>参考资料： 唐宇迪 OpenCV计算机视觉实战(Python版)</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像处理-1</title>
      <link href="/2019/10/13/20191013-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86-1/"/>
      <url>/2019/10/13/20191013-%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86-1/</url>
      
        <content type="html"><![CDATA[<h2 id="灰度图"><a href="#灰度图" class="headerlink" title="灰度图"></a>灰度图</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2 <span class="comment">#opencv读取的格式是BGR</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt<span class="comment">#Matplotlib是RGB</span></span><br><span class="line">%matplotlib inline</span><br><span class="line">img=cv2.imread(<span class="string">'cat.jpg'</span>)</span><br><span class="line">img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)</span><br><span class="line">img_gray.shape</span><br><span class="line">cv2.imshow(<span class="string">"img_gray"</span>, img_gray)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></tbody></table></figure><h2 id="HSV"><a href="#HSV" class="headerlink" title="HSV"></a>HSV</h2><ul><li>H - 色调（主波长）。</li><li>S - 饱和度（纯度/颜色的阴影）。</li><li>V值（强度）</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">hsv=cv2.cvtColor(img,cv2.COLOR_BGR2HSV)</span><br><span class="line">cv2.imshow(<span class="string">"hsv"</span>, hsv)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></tbody></table></figure><h2 id="图像阈值"><a href="#图像阈值" class="headerlink" title="图像阈值"></a>图像阈值</h2><p>ret, dst = cv2.threshold(src, thresh, maxval, type)</p><ul><li><p>src： 输入图，只能输入单通道图像，通常来说为灰度图</p></li><li><p>dst： 输出图</p></li><li><p>thresh： 阈值</p></li><li><p>maxval： 当像素值超过了阈值（或者小于阈值，根据type来决定），所赋予的值</p></li><li><p>type：二值化操作的类型，包含以下5种类型： cv2.THRESH_BINARY；</p></li><li><p>cv2.THRESH_BINARY_INV； cv2.THRESH_TRUNC； cv2.THRESH_TOZERO；</p></li><li><p>cv2.THRESH_TOZERO_INV</p></li><li><p>cv2.THRESH_BINARY 超过阈值部分取maxval（最大值），否则取0</p></li><li><p>cv2.THRESH_BINARY_INV THRESH_BINARY的反转</p></li><li><p>cv2.THRESH_TRUNC 大于阈值部分设为阈值，否则不变</p></li><li><p>cv2.THRESH_TOZERO 大于阈值部分不改变，否则设为0</p></li><li><p>cv2.THRESH_TOZERO_INV THRESH_TOZERO的反转</p></li></ul><p>| </p><pre><code>    ret, thresh1 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_BINARY)      ret, thresh2 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_BINARY_INV)  # INV 反转      ret, thresh3 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_TRUNC)      ret, thresh4 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_TOZERO)      ret, thresh5 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_TOZERO_INV)            titles = ['Original Image', 'BINARY', 'BINARY_INV', 'TRUNC', 'TOZERO', 'TOZERO_INV']      images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]            for i in range(6):          plt.subplot(2, 3, i + 1), plt.imshow(images[i], 'gray')          plt.title(titles[i])          plt.xticks([]), plt.yticks([])      plt.show()        </code></pre><p>—|—  </p><h2 id="图像平滑"><a href="#图像平滑" class="headerlink" title="图像平滑"></a>图像平滑</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">'lenaNoise.png'</span>)</span><br><span class="line">cv2.imshow(<span class="string">'img'</span>, img)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line"><span class="comment"># 均值滤波</span></span><br><span class="line"><span class="comment"># 简单的平均卷积操作</span></span><br><span class="line">blur = cv2.blur(img, (<span class="number">3</span>, <span class="number">3</span>))</span><br><span class="line">cv2.imshow(<span class="string">'blur'</span>, blur)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line"><span class="comment"># 方框滤波</span></span><br><span class="line"><span class="comment"># 基本和均值一样，可以选择归一化  ，-1表示颜色通道是一至的</span></span><br><span class="line">box = cv2.boxFilter(img,-<span class="number">1</span>,(<span class="number">3</span>,<span class="number">3</span>), normalize=<span class="literal">True</span>)</span><br><span class="line">cv2.imshow(<span class="string">'box'</span>, box)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line"><span class="comment"># 方框滤波</span></span><br><span class="line"><span class="comment"># 基本和均值一样，可以选择归一化,容易越界  ，做归一化的结果跟均值滤波是一样的</span></span><br><span class="line">box = cv2.boxFilter(img,-<span class="number">1</span>,(<span class="number">3</span>,<span class="number">3</span>), normalize=<span class="literal">False</span>)</span><br><span class="line">cv2.imshow(<span class="string">'box'</span>, box)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line"><span class="comment"># 高斯滤波</span></span><br><span class="line"><span class="comment"># 高斯模糊的卷积核里的数值是满足高斯分布，相当于更重视中间的</span></span><br><span class="line">aussian = cv2.GaussianBlur(img, (<span class="number">5</span>, <span class="number">5</span>), <span class="number">1</span>)</span><br><span class="line">cv2.imshow(<span class="string">'aussian'</span>, aussian)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line"><span class="comment"># 中值滤波</span></span><br><span class="line"><span class="comment"># 相当于用中值代替</span></span><br><span class="line">median = cv2.medianBlur(img, <span class="number">5</span>)  <span class="comment"># 中值滤波</span></span><br><span class="line">cv2.imshow(<span class="string">'median'</span>, median)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line"><span class="comment"># 展示所有的</span></span><br><span class="line">res = np.hstack((blur,aussian,median))</span><br><span class="line"><span class="comment">#print (res)</span></span><br><span class="line">cv2.imshow(<span class="string">'median vs average'</span>, res)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></tbody></table></figure><h2 id="形态学-腐蚀操作"><a href="#形态学-腐蚀操作" class="headerlink" title="形态学-腐蚀操作"></a>形态学-腐蚀操作</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">'dige.png'</span>)</span><br><span class="line">cv2.imshow(<span class="string">'img'</span>, img)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line">kernel = np.ones((<span class="number">3</span>,<span class="number">3</span>),np.uint8)</span><br><span class="line">erosion = cv2.erode(img,kernel,iterations = <span class="number">1</span>) <span class="comment"># erode函数</span></span><br><span class="line">cv2.imshow(<span class="string">'erosion'</span>, erosion)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line">pie = cv2.imread(<span class="string">'pie.png'</span>)</span><br><span class="line">cv2.imshow(<span class="string">'pie'</span>, pie)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line">kernel = np.ones((<span class="number">30</span>,<span class="number">30</span>),np.uint8)</span><br><span class="line">erosion_1 = cv2.erode(pie,kernel,iterations = <span class="number">1</span>)</span><br><span class="line">erosion_2 = cv2.erode(pie,kernel,iterations = <span class="number">2</span>)</span><br><span class="line">erosion_3 = cv2.erode(pie,kernel,iterations = <span class="number">3</span>)</span><br><span class="line">res = np.hstack((erosion_1,erosion_2,erosion_3))</span><br><span class="line">cv2.imshow(<span class="string">'res'</span>, res)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></tbody></table></figure><h2 id="形态学-膨胀操作"><a href="#形态学-膨胀操作" class="headerlink" title="形态学-膨胀操作"></a>形态学-膨胀操作</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(<span class="string">'dige.png'</span>)</span><br><span class="line">cv2.imshow(<span class="string">'img'</span>, img)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line">kernel = np.ones((<span class="number">3</span>,<span class="number">3</span>),np.uint8)</span><br><span class="line">dige_erosion = cv2.erode(img,kernel,iterations = <span class="number">1</span>)</span><br><span class="line">cv2.imshow(<span class="string">'erosion'</span>, erosion)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line">kernel = np.ones((<span class="number">3</span>,<span class="number">3</span>),np.uint8)</span><br><span class="line">dige_dilate = cv2.dilate(dige_erosion,kernel,iterations = <span class="number">1</span>)</span><br><span class="line">cv2.imshow(<span class="string">'dilate'</span>, dige_dilate)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line">pie = cv2.imread(<span class="string">'pie.png'</span>)</span><br><span class="line">kernel = np.ones((<span class="number">30</span>,<span class="number">30</span>),np.uint8)</span><br><span class="line">dilate_1 = cv2.dilate(pie,kernel,iterations = <span class="number">1</span>)</span><br><span class="line">dilate_2 = cv2.dilate(pie,kernel,iterations = <span class="number">2</span>)</span><br><span class="line">dilate_3 = cv2.dilate(pie,kernel,iterations = <span class="number">3</span>)</span><br><span class="line">res = np.hstack((dilate_1,dilate_2,dilate_3))</span><br><span class="line">cv2.imshow(<span class="string">'res'</span>, res)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></tbody></table></figure><h2 id="开运算与闭运算"><a href="#开运算与闭运算" class="headerlink" title="开运算与闭运算"></a>开运算与闭运算</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 开：先腐蚀，再膨胀</span></span><br><span class="line">img = cv2.imread(<span class="string">'dige.png'</span>)</span><br><span class="line">kernel = np.ones((<span class="number">5</span>,<span class="number">5</span>),np.uint8)</span><br><span class="line">opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)</span><br><span class="line">cv2.imshow(<span class="string">'opening'</span>, opening)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line"><span class="comment"># 闭：先膨胀，再腐蚀</span></span><br><span class="line">img = cv2.imread(<span class="string">'dige.png'</span>)</span><br><span class="line">kernel = np.ones((<span class="number">5</span>,<span class="number">5</span>),np.uint8)</span><br><span class="line">closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)</span><br><span class="line">cv2.imshow(<span class="string">'closing'</span>, closing)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></tbody></table></figure><h2 id="梯度运算"><a href="#梯度运算" class="headerlink" title="梯度运算"></a>梯度运算</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 梯度=膨胀-腐蚀</span></span><br><span class="line">pie = cv2.imread(<span class="string">'pie.png'</span>)</span><br><span class="line">kernel = np.ones((<span class="number">7</span>,<span class="number">7</span>),np.uint8)</span><br><span class="line">dilate = cv2.dilate(pie,kernel,iterations = <span class="number">5</span>)</span><br><span class="line">erosion = cv2.erode(pie,kernel,iterations = <span class="number">5</span>)</span><br><span class="line">res = np.hstack((dilate,erosion))</span><br><span class="line">cv2.imshow(<span class="string">'res'</span>, res)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br><span class="line">gradient = cv2.morphologyEx(pie, cv2.MORPH_GRADIENT, kernel)</span><br><span class="line">cv2.imshow(<span class="string">'gradient'</span>, gradient)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyAllWindows()</span><br></pre></td></tr></tbody></table></figure><h2 id="礼帽与黑帽"><a href="#礼帽与黑帽" class="headerlink" title="礼帽与黑帽"></a>礼帽与黑帽</h2><ul><li>礼帽 = 原始输入-开运算结果</li><li>黑帽 = 闭运算-原始输入</li></ul><p>| </p><pre><code>    #礼帽      img = cv2.imread('dige.png')      tophat = cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel)      cv2.imshow('tophat', tophat)      cv2.waitKey(0)      cv2.destroyAllWindows()      #黑帽      img = cv2.imread('dige.png')      blackhat  = cv2.morphologyEx(img,cv2.MORPH_BLACKHAT, kernel)      cv2.imshow('blackhat ', blackhat )      cv2.waitKey(0)      cv2.destroyAllWindows()        </code></pre><p>—|—  </p><p>参考资料： 唐宇迪 OpenCV计算机视觉实战(Python版)</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ocr透视变换</title>
      <link href="/2019/10/13/ocr%E9%80%8F%E8%A7%86%E5%8F%98%E6%8D%A2/"/>
      <url>/2019/10/13/ocr%E9%80%8F%E8%A7%86%E5%8F%98%E6%8D%A2/</url>
      
        <content type="html"><![CDATA[<pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  </code></pre><p>| </p><pre><code># 导入工具包  import numpy as np  import argparse  import cv2    # 设置参数  ap = argparse.ArgumentParser()  ap.add_argument("-i", "--image", required = False,      help = "Path to the image to be scanned",default='images/page.jpg')   args = vars(ap.parse_args())    def order_points(pts):      # 一共4个坐标点      rect = np.zeros((4, 2), dtype = "float32")        # 按顺序找到对应坐标0123分别是 左上，右上，右下，左下      # 计算左上，右下      s = pts.sum(axis = 1)      rect[0] = pts[np.argmin(s)]      rect[2] = pts[np.argmax(s)]        # 计算右上和左下      diff = np.diff(pts, axis = 1)      rect[1] = pts[np.argmin(diff)]      rect[3] = pts[np.argmax(diff)]        return rect    def four_point_transform(image, pts):      # 获取输入坐标点      rect = order_points(pts)      (tl, tr, br, bl) = rect        # 计算输入的w和h值      widthA = np.sqrt(((br[0] - bl[0]) ** 2) + ((br[1] - bl[1]) ** 2))      widthB = np.sqrt(((tr[0] - tl[0]) ** 2) + ((tr[1] - tl[1]) ** 2))      maxWidth = max(int(widthA), int(widthB))        heightA = np.sqrt(((tr[0] - br[0]) ** 2) + ((tr[1] - br[1]) ** 2))      heightB = np.sqrt(((tl[0] - bl[0]) ** 2) + ((tl[1] - bl[1]) ** 2))      maxHeight = max(int(heightA), int(heightB))        # 变换后对应坐标位置      dst = np.array([          [0, 0],          [maxWidth - 1, 0],          [maxWidth - 1, maxHeight - 1],          [0, maxHeight - 1]], dtype = "float32")        # 计算变换矩阵      M = cv2.getPerspectiveTransform(rect, dst)      warped = cv2.warpPerspective(image, M, (maxWidth, maxHeight))        # 返回变换后结果      return warped    def resize(image, width=None, height=None, inter=cv2.INTER_AREA):      dim = None      (h, w) = image.shape[:2]      if width is None and height is None:          return image      if width is None:          r = height / float(h)          dim = (int(w * r), height)      else:          r = width / float(w)          dim = (width, int(h * r))      resized = cv2.resize(image, dim, interpolation=inter)      return resized    # 读取输入  image = cv2.imread(args["image"])  #坐标也会相同变化  ratio = image.shape[0] / 500.0  orig = image.copy()      image = resize(orig, height = 500)    # 预处理  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  gray = cv2.GaussianBlur(gray, (5, 5), 0)  edged = cv2.Canny(gray, 75, 200)    # 展示预处理结果  print("STEP 1: 边缘检测")  cv2.imshow("Image", image)  cv2.imshow("Edged", edged)  cv2.waitKey(0)  cv2.destroyAllWindows()    # 轮廓检测  cnts = cv2.findContours(edged.copy(), cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)[1]  cnts = sorted(cnts, key = cv2.contourArea, reverse = True)[:5]    # 遍历轮廓  for c in cnts:      # 计算轮廓近似      peri = cv2.arcLength(c, True)      # C表示输入的点集      # epsilon表示从原始轮廓到近似轮廓的最大距离，它是一个准确度参数      # True表示封闭的      approx = cv2.approxPolyDP(c, 0.02 * peri, True)# 越小越精准，长度的百分之多少作为精度        # 4个点的时候就拿出来      if len(approx) == 4:          screenCnt = approx          break    # 展示结果  print("STEP 2: 获取轮廓")  cv2.drawContours(image, [screenCnt], -1, (0, 255, 0), 2)  cv2.imshow("Outline", image)  cv2.waitKey(0)  cv2.destroyAllWindows()    # 透视变换  warped = four_point_transform(orig, screenCnt.reshape(4, 2) * ratio)    # 二值处理  warped = cv2.cvtColor(warped, cv2.COLOR_BGR2GRAY)  ref = cv2.threshold(warped, 100, 255, cv2.THRESH_BINARY)[1]  cv2.imwrite('scan.jpg', ref)  # 展示结果  print("STEP 3: 变换")  cv2.imshow("Original", resize(orig, height = 650))  cv2.imshow("Scanned", resize(ref, height = 650))  cv2.waitKey(0)    </code></pre><p>—|—  </p><p>参考资料： 唐宇迪 OpenCV计算机视觉实战(Python版)</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>park停车场项目实战</title>
      <link href="/2019/10/13/park%E5%81%9C%E8%BD%A6%E5%9C%BA%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/"/>
      <url>/2019/10/13/park%E5%81%9C%E8%BD%A6%E5%9C%BA%E9%A1%B9%E7%9B%AE%E5%AE%9E%E6%88%98/</url>
      
        <content type="html"><![CDATA[<h2 id="park-类对象"><a href="#park-类对象" class="headerlink" title="park 类对象"></a>park 类对象</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160  161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176  177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192  193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208  209  210  211  212  213  214  215  216  217  218  219  220  221  222  223  224  225  226  227  228  229  230  231  232  233  234  235  236  237  238  239  240  241  242  243  244  245  246  247  248  249  250  251  252  253  254  255  256  257  258  259  260  261  262  263  264  265  266  267  268  269  270  271  272  273  274  275  276  277  278  279  280  281  282  283  284  285  286  287  288  289  290  291  292  293  294  295  296  297  298  299  300  301  302  303  304  305  306  307  308  309  310  311  312  313  314  315  316  317  318  319  320  321  322  323  324  325  326  </code></pre><p>| </p><pre><code># coding=utf-8  import matplotlib.pyplot as plt  import cv2  import os, glob  import numpy as np      class Parking:            # 显示图片      def show_images(self, images, cmap=None):          cols = 2          rows = (len(images)+1)//cols            plt.figure(figsize=(15,12))          for i, image in enumerate(images):              plt.subplot(rows, cols, i+1)              cmap = 'gray' if  len(image.shape)==2 else cmap              plt.imshow(image, cmap=cmap)              plt.xticks([])              plt.yticks([])          plt.tight_layout(pad=0, h_pad=0, w_pad=0)          plt.show()            def cv_show(self, name, img):          cv2.imshow(name,img)          cv2.waitKey(0)          cv2.destroyAllWindows()            def select_rgb_white_yello(self, image):          # 过滤掉背景          lower = np.uint8([120,120,120])          upper = np.uint8([255,255,255])          # lower_red和高于upper_red的部分分别变成0，lower_red～upper_red之间的值变成255,相当于过滤背景          white_mask =  cv2.inRange(image,lower, upper)            self.cv_show('white_mask', white_mask)            masked = cv2.bitwise_and(image, image, mask = white_mask)          self.cv_show('masked', masked)            return masked      def convert_gray_sacle(self,image):          return cv2.cvtColor(image,cv2.COLOR_RGB2GRAY)      # 检测边缘      def detect_edges(self, image, low_threshold=50, high_threshold=200):          return cv2.Canny(image, low_threshold, high_threshold)        def filter_region(self, image, vertices):          """             剔除不需要的地方          """          mask = np.zeros_like(image)          if len(mask.shape) == 2:              cv2.fillPoly(mask, vertices, 255)              self.cv_show('mask', mask)          return cv2.bitwise_and(image, mask)            # 手动选择区域            def select_region(self,image):          """                  手动选择区域          """          # first, define the polygon by vertices          rows, cols = image.shape[:2]          pt_1  = [cols*0.05, rows*0.90]          pt_2 = [cols*0.05, rows*0.70]          pt_3 = [cols*0.30, rows*0.55]          pt_4 = [cols*0.6, rows*0.15]          pt_5 = [cols*0.90, rows*0.15]           pt_6 = [cols*0.90, rows*0.90]            vertices = np.array([[pt_1, pt_2, pt_3, pt_4, pt_5, pt_6]], dtype=np.int32)           point_img = image.copy()                 point_img = cv2.cvtColor(point_img, cv2.COLOR_GRAY2RGB)          for point in vertices[0]:              cv2.circle(point_img, (point[0],point[1]), 10, (0,0,255), 4)          self.cv_show('point_img',point_img)                              return self.filter_region(image, vertices)            def hough_line(self, image):          # 输入的图像需要是边缘检测后的结果          # minLineLengh(线的最短长度，比这个短的都被忽略)和MaxLineCap（两条直线之间的最大间隔，小于此值，认为是一条直线）          # rho距离精度,theta角度精度,threshod超过设定阈值才被检测出线段          return cv2.HoughLinesP(image, rho=0.1, theta=np.pi/10, threshold=15, minLineLength=9, maxLineGap=4)      def draw_lines(self, image, lines, color=[255,0,0], thickness=2, make_copy=True):          # 过滤霍夫变换检测得到直线          if make_copy:              image = np.copy(image)          cleaned = []          for line in lines:              for x1, y1, x2, y2 in line:                  if abs(y2-y1) &lt;=1 and abs(x2-x1) &gt;=25 and abs(x2-x1) &lt;= 55:                      cleaned.append((x1,y1,x2,y2))                      cv2.line(image, (x1, y1), (x2, y2), color, thickness)          print('No lines detected: ', len(cleaned))          return image        def identify_blocks(self, image, lines, make_copy=True):          if make_copy:              new_image = np.copy(image)          # step 1: 过滤部分直线          cleaned = []          for line in lines:              for x1,y1,x2,y2 in line:                  if abs(y2-y1) &lt;=1 and abs(x2-x1) &gt;=25 and abs(x2-x1) &lt;= 55:                      cleaned.append((x1,y1,x2,y2))                    # step 2: 对直线按照x1进行排序          import operator          list1 = sorted(cleaned, key=operator.itemgetter(0,1))          # &gt;&gt;&gt; b=operator.itemgetter(1,0)   //定义函数b，获取对象的第1个域和第0个的值          # &gt;&gt;&gt; b(a)           # (2, 1)           # step 3: 找到多个列，相当于每列是一排车          clusters = {}          dIndex = 0          clus_dist = 10            for i in range(len(list1)-1):              distance = abs(list1[i+1][0] - list1[i][0])              if distance &lt;= clus_dist:                  if not dIndex in clusters.keys(): clusters[dIndex] = []                  clusters[dIndex].append(list1[i])                  clusters[dIndex].append(list1[i + 1])                     else:                  dIndex += 1                    # step 4: 得到坐标          rects = {}          i = 0          for key in clusters:              all_list = clusters[key]              cleaned = list(set(all_list))              if len(cleaned) &gt; 5:                  cleaned = sorted(cleaned, key=lambda tup: tup[1])                  avg_y1 = cleaned[0][1]                  avg_y2 = cleaned[-1][1]                  avg_x1 = 0                  avg_x2 = 0                  for tup in cleaned:                      avg_x1 += tup[0]                      avg_x2 += tup[2]                  avg_x1 = avg_x1/len(cleaned)                  avg_x2 = avg_x2/len(cleaned)                  rects[i] = (avg_x1, avg_y1, avg_x2, avg_y2)                  i += 1          print("Num Parking Lanes: ", len(rects))          # step 5: 把矩形画出来          buff = 7          for key in rects:              tup_topLeft = (int(rects[key][0] - buff), int(rects[key][1]))              tup_botRight = (int(rects[key][2] + buff), int(rects[key][3]))              cv2.rectangle(new_image, tup_topLeft,tup_botRight,(0,255,0),3)          return new_image, rects            def draw_parking(self, image, rects, make_copy=True, color=[255,0,0],thickness=2, save=True):          if make_copy:              new_image = np.copy(image)          gap = 15.5          spot_dict = {} # 字典：一个车位对应一个位置          tot_spots = 0          #微调          adj_y1 = {0: 20, 1:-10, 2:0, 3:-11, 4:28, 5:5, 6:-15, 7:-15, 8:-10, 9:-30, 10:9, 11:-32}          adj_y2 = {0: 30, 1: 50, 2:15, 3:10, 4:-15, 5:15, 6:15, 7:-20, 8:15, 9:15, 10:0, 11:30}                    adj_x1 = {0: -8, 1:-15, 2:-15, 3:-15, 4:-15, 5:-15, 6:-15, 7:-15, 8:-10, 9:-10, 10:-10, 11:0}          adj_x2 = {0: 0, 1: 15, 2:15, 3:15, 4:15, 5:15, 6:15, 7:15, 8:10, 9:10, 10:10, 11:0}                    for key in rects:              tup = rects[key]              x1 = int(tup[0]+ adj_x1[key])              x2 = int(tup[2]+ adj_x2[key])              y1 = int(tup[1] + adj_y1[key])              y2 = int(tup[3] + adj_y2[key])              cv2.rectangle(new_image, (x1, y1),(x2,y2),(0,255,0),2)              num_splits = int(abs(y2-y1)//gap)              for i in range(0, num_splits+1):                  y = int(y1 + i*gap)                  cv2.line(new_image, (x1, y), (x2, y), color, thickness)              if key &gt; 0 and key &lt; len(rects) -1 :                          #竖直线                  x = int((x1 + x2)/2)                  cv2.line(new_image, (x, y1), (x, y2), color, thickness)              # 计算数量              if key == 0 or key == (len(rects) -1):                  tot_spots += num_splits +1              else:                  tot_spots += 2*(num_splits +1)                                # 字典对应好              if key == 0 or key == (len(rects) -1):                  for i in range(0, num_splits+1):                      cur_len = len(spot_dict)                      y = int(y1 + i*gap)                      spot_dict[(x1, y, x2, y+gap)] = cur_len +1                      else:                  for i in range(0, num_splits+1):                      cur_len = len(spot_dict)                      y = int(y1 + i*gap)                      x = int((x1 + x2)/2)                      spot_dict[(x1, y, x, y+gap)] = cur_len +1                      spot_dict[(x, y, x2, y+gap)] = cur_len +2                       print("total parking spaces: ", tot_spots, cur_len)          if save:              filename = 'with_parking.jpg'              cv2.imwrite(filename, new_image)          return new_image, spot_dict                          def assign_spots_map(self,image, spot_dict, make_copy = True, color=[255, 0, 0], thickness=2):          if make_copy:              new_image = np.copy(image)          for spot in spot_dict.keys():              (x1, y1, x2, y2) = spot              cv2.rectangle(new_image, (int(x1),int(y1)), (int(x2),int(y2)), color, thickness)          return new_image            def save_images_for_cnn(self,image, spot_dict, folder_name ='cnn_data'):          for spot in spot_dict.keys():              (x1, y1, x2, y2) = spot              (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2))              #裁剪图像              spot_img = image[y1:y2, x1:x2]              spot_img = cv2.resize(spot_img, (0,0), fx=2.0, fy=2.0)               spot_id = spot_dict[spot]                            filename = 'spot' + str(spot_id) +'.jpg'              print(spot_img.shape, filename, (x1,x2,y1,y2))                            cv2.imwrite(os.path.join(folder_name, filename), spot_img)      def make_prediction(self,image, model, class_dictionary):          # 预处理          img = image/255.            # 转换成4D tensor          image = np.expand_dims(img,axis=0)            # 用训练好的模型进行训练          class_predicted = model.predict(image)          inID = np.argmax(class_predicted[0])          label = class_dictionary[inID]            return label            def predict_on_image(self, image, spot_dict, model, class_dictionary, make_copy=True, color = [0,255,0], alpha=0.5):          if make_copy:              new_image = np.copy(image)              overlay = np.copy(image)          self.cv_show('new_image',new_image)          cnt_empty = 0          all_spots = 0          for spot in spot_dict.key():              all_spots += 1              (x1, y1, x2, y2) = spot              (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2))              spot_img = image[y1:y2, x1:x2]              spot_img = cv2.resize(spot_img, (48, 48))               label = self.make_prediction(spot_img,model,class_dictionary)              if label == 'empty':                  cv2.rectangle(overlay, (int(x1),int(y1)), (int(x2),int(y2)), color, -1)                  cnt_empty += 1                        cv2.addWeighted(overlay, alpha, new_image, 1 - alpha, 0, new_image) # 图像融合          cv2.putText(new_image, "Available: %d spots" %cnt_empty, (30, 95),          cv2.FONT_HERSHEY_SIMPLEX,          0.7, (255, 255, 255), 2)          cv2.putText(new_image, "Total: %d spots" %all_spots, (30, 125),          cv2.FONT_HERSHEY_SIMPLEX,          0.7, (255, 255, 255), 2)                  save = False                    if save:              filename = 'with_marking.jpg'              cv2.imwrite(filename, new_image)          self.cv_show('new_image',new_image)                    return new_image        def predict_on_video(self,video_name,final_spot_dict, model,class_dictionary,ret=True):             cap = cv2.VideoCapture(video_name)          count = 0          while ret:              ret, image = cap.read()              count += 1              if count == 5:                  count = 0                                    new_image = np.copy(image)                  overlay = np.copy(image)                  cnt_empty = 0                  all_spots = 0                  color = [0, 255, 0]                   alpha=0.5                  for spot in final_spot_dict.keys():                      all_spots += 1                      (x1, y1, x2, y2) = spot                      (x1, y1, x2, y2) = (int(x1), int(y1), int(x2), int(y2))                      spot_img = image[y1:y2, x1:x2]                      spot_img = cv2.resize(spot_img, (48,48))                             label = self.make_prediction(spot_img,model,class_dictionary)                      if label == 'empty':                          cv2.rectangle(overlay, (int(x1),int(y1)), (int(x2),int(y2)), color, -1)                          cnt_empty += 1                        cv2.addWeighted(overlay, alpha, new_image, 1 - alpha, 0, new_image)                        cv2.putText(new_image, "Available: %d spots" %cnt_empty, (30, 95),                  cv2.FONT_HERSHEY_SIMPLEX,                  0.7, (255, 255, 255), 2)                        cv2.putText(new_image, "Total: %d spots" %all_spots, (30, 125),                  cv2.FONT_HERSHEY_SIMPLEX,                  0.7, (255, 255, 255), 2)                  cv2.imshow('frame', new_image)                  if cv2.waitKey(10) &amp; 0xFF == ord('q'):                      break            cv2.destroyAllWindows()          cap.release()    </code></pre><p>—|—  </p><h2 id="test模块"><a href="#test模块" class="headerlink" title="test模块"></a>test模块</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  </code></pre><p>| </p><pre><code>from __future__ import division  import matplotlib.pyplot as plt  import cv2  import os, glob  import numpy as np  from PIL import Image  from keras.applications.imagenet_utils import preprocess_input  from keras.models import load_model  from keras.preprocessing import image  from Parking import Parking  import pickle  cwd = os.getcwd()    def img_process(test_images,park):      white_yellow_images = list(map(park.select_rgb_white_yellow, test_images))      park.show_images(white_yellow_images)            gray_images = list(map(park.convert_gray_scale, white_yellow_images))      park.show_images(gray_images)            edge_images = list(map(lambda image: park.detect_edges(image), gray_images))      park.show_images(edge_images)            roi_images = list(map(park.select_region, edge_images))      park.show_images(roi_images)            list_of_lines = list(map(park.hough_lines, roi_images))            line_images = []      for image, lines in zip(test_images, list_of_lines):          line_images.append(park.draw_lines(image, lines))       park.show_images(line_images)            rect_images = []      rect_coords = []      for image, lines in zip(test_images, list_of_lines):          new_image, rects = park.identify_blocks(image, lines)          rect_images.append(new_image)          rect_coords.append(rects)                park.show_images(rect_images)            delineated = []      spot_pos = []      for image, rects in zip(test_images, rect_coords):          new_image, spot_dict = park.draw_parking(image, rects)          delineated.append(new_image)          spot_pos.append(spot_dict)                park.show_images(delineated)      final_spot_dict = spot_pos[1]      print(len(final_spot_dict))        with open('spot_dict.pickle', 'wb') as handle:          pickle.dump(final_spot_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)            park.save_images_for_cnn(test_images[0],final_spot_dict)            return final_spot_dict  def keras_model(weights_path):          model = load_model(weights_path)      return model  def img_test(test_images,final_spot_dict,model,class_dictionary):      for i in range (len(test_images)):          predicted_images = park.predict_on_image(test_images[i],final_spot_dict,model,class_dictionary)  def video_test(video_name,final_spot_dict,model,class_dictionary):      name = video_name      cap = cv2.VideoCapture(name)      park.predict_on_video(name,final_spot_dict,model,class_dictionary,ret=True)                    if __name__ == '__main__':      test_images = [plt.imread(path) for path in glob.glob('test_images/*.jpg')]      weights_path = 'car1.h5'      video_name = 'parking_video.mp4'      class_dictionary = {}      class_dictionary[0] = 'empty'      class_dictionary[1] = 'occupied'      park = Parking()      park.show_images(test_images)      final_spot_dict = img_process(test_images,park)      model = keras_model(weights_path)      img_test(test_images,final_spot_dict,model,class_dictionary)      video_test(video_name,final_spot_dict,model,class_dictionary)    </code></pre><p>—|—  </p><h2 id="利用cnn训练出一个二分类网络"><a href="#利用cnn训练出一个二分类网络" class="headerlink" title="利用cnn训练出一个二分类网络"></a>利用cnn训练出一个二分类网络</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  </code></pre><p>| </p><pre><code>import numpy  import os  from keras import applications  from keras.preprocessing.image import ImageDataGenerator  from keras import optimizers  from keras.models import Sequential, Model  from keras.layers import Dropout, Flatten, Dense, GlobalAveragePooling2D  from keras import backend as k  from keras.callbacks import ModelCheckpoint, LearningRateScheduler, TensorBoard, EarlyStopping  from keras.models import Sequential  from keras.layers.normalization import BatchNormalization  from keras.layers.convolutional import Conv2D  from keras.layers.convolutional import MaxPooling2D  from keras.initializers import TruncatedNormal  from keras.layers.core import Activation  from keras.layers.core import Flatten  from keras.layers.core import Dropout  from keras.layers.core import Dense      files_train = 0  files_validation = 0    cwd = os.getcwd()  folder = 'train_data/train'  for sub_folder in os.listdir(folder):      path, dirs, files = next(os.walk(os.path.join(folder,sub_folder)))      files_train += len(files)      folder = 'train_data/test'  for sub_folder in os.listdir(folder):      path, dirs, files = next(os.walk(os.path.join(folder,sub_folder)))      files_validation += len(files)    print(files_train,files_validation)    img_width, img_height = 48, 48  train_data_dir = "train_data/train"  validation_data_dir = "train_data/test"  nb_train_samples = files_train  nb_validation_samples = files_validation  batch_size = 32  epochs = 15  num_classes = 2    model = applications.VGG16(weights='imagenet', include_top=False, input_shape = (img_width, img_height, 3))      for layer in model.layers[:10]:      layer.trainable = False      x = model.output  x = Flatten()(x)  predictions = Dense(num_classes, activation="softmax")(x)      model_final = Model(input = model.input, output = predictions)      model_final.compile(loss = "categorical_crossentropy",                       optimizer = optimizers.SGD(lr=0.0001, momentum=0.9),                       metrics=["accuracy"])       train_datagen = ImageDataGenerator(  rescale = 1./255,  horizontal_flip = True,  fill_mode = "nearest",  zoom_range = 0.1,  width_shift_range = 0.1,  height_shift_range=0.1,  rotation_range=5)    test_datagen = ImageDataGenerator(  rescale = 1./255,  horizontal_flip = True,  fill_mode = "nearest",  zoom_range = 0.1,  width_shift_range = 0.1,  height_shift_range=0.1,  rotation_range=5)    train_generator = train_datagen.flow_from_directory(  train_data_dir,  target_size = (img_height, img_width),  batch_size = batch_size,  class_mode = "categorical")    validation_generator = test_datagen.flow_from_directory(  validation_data_dir,  target_size = (img_height, img_width),  class_mode = "categorical")    checkpoint = ModelCheckpoint("car1.h5", monitor='val_acc', verbose=1, save_best_only=True, save_weights_only=False, mode='auto', period=1)  early = EarlyStopping(monitor='val_acc', min_delta=0, patience=10, verbose=1, mode='auto')          history_object = model_final.fit_generator(  train_generator,  samples_per_epoch = nb_train_samples,  epochs = epochs,  validation_data = validation_generator,  nb_val_samples = nb_validation_samples,  callbacks = [checkpoint, early])    </code></pre><p>—|—  </p><p>参考资料： 唐宇迪 OpenCV计算机视觉实战(Python版)</p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ocr答题卡识别</title>
      <link href="/2019/10/13/ocr%E7%AD%94%E9%A2%98%E5%8D%A1%E8%AF%86%E5%88%AB/"/>
      <url>/2019/10/13/ocr%E7%AD%94%E9%A2%98%E5%8D%A1%E8%AF%86%E5%88%AB/</url>
      
        <content type="html"><![CDATA[<pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  </code></pre><p>| </p><pre><code>import cv2    def sort_contours(cnts, method="left-to-right"):      reverse = False      i = 0        if method == "right-to-left" or method == "bottom-to-top":          reverse = True        if method == "top-to-bottom" or method == "bottom-to-top":          i = 1      boundingBoxes = [cv2.boundingRect(c) for c in cnts] #用一个最小的矩形，把找到的形状包起来x,y,h,w      (cnts, boundingBoxes) = zip(*sorted(zip(cnts, boundingBoxes),                                          key=lambda b: b[1][i], reverse=reverse))        return cnts, boundingBoxes  def resize(image, width=None, height=None, inter=cv2.INTER_AREA):      dim = None      (h, w) = image.shape[:2]      if width is None and height is None:          return image      if width is None:          r = height / float(h)          dim = (int(w * r), height)      else:          r = width / float(w)          dim = (width, int(h * r))      resized = cv2.resize(image, dim, interpolation=inter)      return resized    </code></pre><p>—|—  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160  161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176  177  178  179  180  181  182  183  </code></pre><p>| </p><pre><code># 导入工具包  from imutils import contours  import numpy as np  import argparse  import cv2  import myutils_beifen    # # 设置参数  # ap = argparse.ArgumentParser()  # ap.add_argument("-i", "--image", required=True,  # help="path to input image")  # ap.add_argument("-t", "--template", required=True,  # help="path to template OCR-A image")  # args = vars(ap.parse_args())    # 指定信用卡类型  FIRST_NUMBER = {      "3": "American Express",      "4": "Visa",      "5": "MasterCard",      "6": "Discover Card"  }  # 绘图展示  def cv_show(name,img):      cv2.imshow(name, img)      cv2.waitKey(0)      cv2.destroyAllWindows()  # 读取一个模板图像  # img = cv2.imread(args["template"])  img  = cv2.imread('images/ocr_a_reference.png')  cv_show('img',img)  # 灰度图  ref = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)  cv_show('ref',ref)  # 二值图像  ref = cv2.threshold(ref, 10, 255, cv2.THRESH_BINARY_INV)[1]  cv_show('ref',ref)    # 计算轮廓  #cv2.findContours()函数接受的参数为二值图，即黑白的（不是灰度图）,cv2.RETR_EXTERNAL只检测外轮廓，cv2.CHAIN_APPROX_SIMPLE只保留终点坐标  #返回的list中每个元素都是图像中的一个轮廓    ref_, refCnts, hierarchy = cv2.findContours(ref.copy(), cv2.RETR_EXTERNAL,cv2.CHAIN_APPROX_SIMPLE)    cv2.drawContours(img,refCnts,-1,(0,0,255),3)  # -1表示画所有的轮廓  cv_show('img',img)  print (np.array(refCnts).shape)  refCnts = myutils.sort_contours(refCnts, method="left-to-right")[0] #排序，从左到右，从上到下  digits = {}    # 遍历每一个轮廓  for (i, c) in enumerate(refCnts):      # 计算外接矩形并且resize成合适大小      (x, y, w, h) = cv2.boundingRect(c)      roi = ref[y:y + h, x:x + w]      roi = cv2.resize(roi, (57, 88))        # 每一个数字对应每一个模板      digits[i] = roi    # 初始化卷积核  rectKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (9, 3))    sqKernel = cv2.getStructuringElement(cv2.MORPH_RECT, (5, 5))    #读取输入图像，预处理  # image = cv2.imread(args["image"])  image = cv2.imread('images/credit_card_02.png')  cv_show('image',image)  image = myutils.resize(image, width=300)  gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)  cv_show('gray',gray)    #礼帽操作，突出更明亮的区域  tophat = cv2.morphologyEx(gray, cv2.MORPH_TOPHAT, rectKernel)   cv_show('tophat',tophat)   #   gradX = cv2.Sobel(tophat, ddepth=cv2.CV_32F, dx=1, dy=0, #ksize=-1相当于用3*3的      ksize=-1)      gradX = np.absolute(gradX)  (minVal, maxVal) = (np.min(gradX), np.max(gradX))  gradX = (255 * ((gradX - minVal) / (maxVal - minVal)))  gradX = gradX.astype("uint8")    print (np.array(gradX).shape)  cv_show('gradX',gradX)    #通过闭操作（先膨胀，再腐蚀）将数字连在一起  gradX = cv2.morphologyEx(gradX, cv2.MORPH_CLOSE, rectKernel)   cv_show('gradX',gradX)  #THRESH_OTSU会自动寻找合适的阈值，适合双峰，需把阈值参数设置为0  thresh = cv2.threshold(gradX, 0, 255,      cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]   cv_show('thresh',thresh)    #再来一个闭操作    thresh = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, sqKernel) #再来一个闭操作  cv_show('thresh',thresh)    # 计算轮廓    thresh_, threshCnts, hierarchy = cv2.findContours(thresh.copy(), cv2.RETR_EXTERNAL,      cv2.CHAIN_APPROX_SIMPLE)    cnts = threshCnts  cur_img = image.copy()  cv2.drawContours(cur_img,cnts,-1,(0,0,255),3)   cv_show('img',cur_img)  locs = []    # 遍历轮廓  for (i, c) in enumerate(cnts):      # 计算矩形      (x, y, w, h) = cv2.boundingRect(c)      ar = w / float(h)        # 选择合适的区域，根据实际任务来，这里的基本都是四个数字一组      if ar &gt; 2.5 and ar &lt; 4.0:            if (w &gt; 40 and w &lt; 55) and (h &gt; 10 and h &lt; 20):              #符合的留下来              locs.append((x, y, w, h))    # 将符合的轮廓从左到右排序  locs = sorted(locs, key=lambda x:x[0])  output = []    # 遍历每一个轮廓中的数字  for (i, (gX, gY, gW, gH)) in enumerate(locs):      # initialize the list of group digits      groupOutput = []        # 根据坐标提取每一个组      group = gray[gY - 5:gY + gH + 5, gX - 5:gX + gW + 5]      cv_show('group',group)      # 预处理      group = cv2.threshold(group, 0, 255,          cv2.THRESH_BINARY | cv2.THRESH_OTSU)[1]      cv_show('group',group)      # 计算每一组的轮廓      group_,digitCnts,hierarchy = cv2.findContours(group.copy(), cv2.RETR_EXTERNAL,          cv2.CHAIN_APPROX_SIMPLE)      digitCnts = contours.sort_contours(digitCnts,          method="left-to-right")[0]        # 计算每一组中的每一个数值      for c in digitCnts:          # 找到当前数值的轮廓，resize成合适的的大小          (x, y, w, h) = cv2.boundingRect(c)          roi = group[y:y + h, x:x + w]          roi = cv2.resize(roi, (57, 88)) # 模板匹配要求大小一致          cv_show('roi',roi)            # 计算匹配得分          scores = []            # 在模板中计算每一个得分          for (digit, digitROI) in digits.items():              # 模板匹配              result = cv2.matchTemplate(roi, digitROI,                  cv2.TM_CCOEFF)              (_, score, _, _) = cv2.minMaxLoc(result)              scores.append(score)            # 得到最合适的数字          groupOutput.append(str(np.argmax(scores)))        # 画出来      cv2.rectangle(image, (gX - 5, gY - 5),          (gX + gW + 5, gY + gH + 5), (0, 0, 255), 1)      cv2.putText(image, "".join(groupOutput), (gX, gY - 15),          cv2.FONT_HERSHEY_SIMPLEX, 0.65, (0, 0, 255), 2)        # 得到结果      output.extend(groupOutput)    # 打印结果  print("Credit Card Type: {}".format(FIRST_NUMBER[output[0]]))  print("Credit Card #: {}".format("".join(output)))  cv2.imshow("Image", image)  cv2.waitKey(0)    </code></pre><p>—|—  </p><p>参考资料： 唐宇迪 OpenCV计算机视觉实战(Python版)</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>图像处理-1</title>
      <link href="/2019/10/13/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86-1/"/>
      <url>/2019/10/13/%E5%9B%BE%E5%83%8F%E5%A4%84%E7%90%86-1/</url>
      
        <content type="html"><![CDATA[<h2 id="灰度图"><a href="#灰度图" class="headerlink" title="灰度图"></a>灰度图</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  </code></pre><p>| </p><pre><code>import cv2 #opencv读取的格式是BGR  import numpy as np  import matplotlib.pyplot as plt#Matplotlib是RGB  %matplotlib inline     img=cv2.imread('cat.jpg')  img_gray = cv2.cvtColor(img,cv2.COLOR_BGR2GRAY)  img_gray.shape    cv2.imshow("img_gray", img_gray)  cv2.waitKey(0)      cv2.destroyAllWindows()    </code></pre><p>—|—  </p><h2 id="HSV"><a href="#HSV" class="headerlink" title="HSV"></a>HSV</h2><ul><li><p>H - 色调（主波长）。</p></li><li><p>S - 饱和度（纯度/颜色的阴影）。</p></li><li><p>V值（强度）</p><pre><code>1  2  3  4  5</code></pre></li></ul><p>| </p><pre><code>    hsv=cv2.cvtColor(img,cv2.COLOR_BGR2HSV)            cv2.imshow("hsv", hsv)      cv2.waitKey(0)          cv2.destroyAllWindows()        </code></pre><p>—|—  </p><h2 id="图像阈值"><a href="#图像阈值" class="headerlink" title="图像阈值"></a>图像阈值</h2><p>ret, dst = cv2.threshold(src, thresh, maxval, type)</p><ul><li><p>src： 输入图，只能输入单通道图像，通常来说为灰度图</p></li><li><p>dst： 输出图</p></li><li><p>thresh： 阈值</p></li><li><p>maxval： 当像素值超过了阈值（或者小于阈值，根据type来决定），所赋予的值</p></li><li><p>type：二值化操作的类型，包含以下5种类型： cv2.THRESH_BINARY；</p></li><li><p>cv2.THRESH_BINARY_INV； cv2.THRESH_TRUNC； cv2.THRESH_TOZERO；</p></li><li><p>cv2.THRESH_TOZERO_INV</p></li><li><p>cv2.THRESH_BINARY 超过阈值部分取maxval（最大值），否则取0</p></li><li><p>cv2.THRESH_BINARY_INV THRESH_BINARY的反转</p></li><li><p>cv2.THRESH_TRUNC 大于阈值部分设为阈值，否则不变</p></li><li><p>cv2.THRESH_TOZERO 大于阈值部分不改变，否则设为0</p></li><li><p>cv2.THRESH_TOZERO_INV THRESH_TOZERO的反转</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14</code></pre></li></ul><p>| </p><pre><code>    ret, thresh1 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_BINARY)      ret, thresh2 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_BINARY_INV)  # INV 反转      ret, thresh3 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_TRUNC)      ret, thresh4 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_TOZERO)      ret, thresh5 = cv2.threshold(img_gray, 127, 255, cv2.THRESH_TOZERO_INV)            titles = ['Original Image', 'BINARY', 'BINARY_INV', 'TRUNC', 'TOZERO', 'TOZERO_INV']      images = [img, thresh1, thresh2, thresh3, thresh4, thresh5]            for i in range(6):          plt.subplot(2, 3, i + 1), plt.imshow(images[i], 'gray')          plt.title(titles[i])          plt.xticks([]), plt.yticks([])      plt.show()        </code></pre><p>—|—  </p><h2 id="图像平滑"><a href="#图像平滑" class="headerlink" title="图像平滑"></a>图像平滑</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  </code></pre><p>| </p><pre><code>img = cv2.imread('lenaNoise.png')    cv2.imshow('img', img)  cv2.waitKey(0)  cv2.destroyAllWindows()  # 均值滤波  # 简单的平均卷积操作  blur = cv2.blur(img, (3, 3))    cv2.imshow('blur', blur)  cv2.waitKey(0)  cv2.destroyAllWindows()  # 方框滤波  # 基本和均值一样，可以选择归一化  ，-1表示颜色通道是一至的  box = cv2.boxFilter(img,-1,(3,3), normalize=True)      cv2.imshow('box', box)  cv2.waitKey(0)  cv2.destroyAllWindows()  # 方框滤波  # 基本和均值一样，可以选择归一化,容易越界  ，做归一化的结果跟均值滤波是一样的  box = cv2.boxFilter(img,-1,(3,3), normalize=False)      cv2.imshow('box', box)  cv2.waitKey(0)  cv2.destroyAllWindows()  # 高斯滤波  # 高斯模糊的卷积核里的数值是满足高斯分布，相当于更重视中间的  aussian = cv2.GaussianBlur(img, (5, 5), 1)      cv2.imshow('aussian', aussian)  cv2.waitKey(0)  cv2.destroyAllWindows()  # 中值滤波  # 相当于用中值代替  median = cv2.medianBlur(img, 5)  # 中值滤波    cv2.imshow('median', median)  cv2.waitKey(0)  cv2.destroyAllWindows()  # 展示所有的  res = np.hstack((blur,aussian,median))  #print (res)  cv2.imshow('median vs average', res)  cv2.waitKey(0)  cv2.destroyAllWindows()    </code></pre><p>—|—  </p><h2 id="形态学-腐蚀操作"><a href="#形态学-腐蚀操作" class="headerlink" title="形态学-腐蚀操作"></a>形态学-腐蚀操作</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  </code></pre><p>| </p><pre><code>img = cv2.imread('dige.png')    cv2.imshow('img', img)  cv2.waitKey(0)  cv2.destroyAllWindows()  kernel = np.ones((3,3),np.uint8)   erosion = cv2.erode(img,kernel,iterations = 1) # erode函数    cv2.imshow('erosion', erosion)  cv2.waitKey(0)  cv2.destroyAllWindows()  pie = cv2.imread('pie.png')  cv2.imshow('pie', pie)  cv2.waitKey(0)  cv2.destroyAllWindows()  kernel = np.ones((30,30),np.uint8)   erosion_1 = cv2.erode(pie,kernel,iterations = 1)  erosion_2 = cv2.erode(pie,kernel,iterations = 2)  erosion_3 = cv2.erode(pie,kernel,iterations = 3)  res = np.hstack((erosion_1,erosion_2,erosion_3))  cv2.imshow('res', res)  cv2.waitKey(0)  cv2.destroyAllWindows()    </code></pre><p>—|—  </p><h2 id="形态学-膨胀操作"><a href="#形态学-膨胀操作" class="headerlink" title="形态学-膨胀操作"></a>形态学-膨胀操作</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  </code></pre><p>| </p><pre><code>img = cv2.imread('dige.png')  cv2.imshow('img', img)  cv2.waitKey(0)  cv2.destroyAllWindows()  kernel = np.ones((3,3),np.uint8)   dige_erosion = cv2.erode(img,kernel,iterations = 1)    cv2.imshow('erosion', erosion)  cv2.waitKey(0)  cv2.destroyAllWindows()  kernel = np.ones((3,3),np.uint8)   dige_dilate = cv2.dilate(dige_erosion,kernel,iterations = 1)    cv2.imshow('dilate', dige_dilate)  cv2.waitKey(0)  cv2.destroyAllWindows()  pie = cv2.imread('pie.png')    kernel = np.ones((30,30),np.uint8)   dilate_1 = cv2.dilate(pie,kernel,iterations = 1)  dilate_2 = cv2.dilate(pie,kernel,iterations = 2)  dilate_3 = cv2.dilate(pie,kernel,iterations = 3)  res = np.hstack((dilate_1,dilate_2,dilate_3))  cv2.imshow('res', res)  cv2.waitKey(0)  cv2.destroyAllWindows()    </code></pre><p>—|—  </p><h2 id="开运算与闭运算"><a href="#开运算与闭运算" class="headerlink" title="开运算与闭运算"></a>开运算与闭运算</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  </code></pre><p>| </p><pre><code># 开：先腐蚀，再膨胀  img = cv2.imread('dige.png')    kernel = np.ones((5,5),np.uint8)   opening = cv2.morphologyEx(img, cv2.MORPH_OPEN, kernel)    cv2.imshow('opening', opening)  cv2.waitKey(0)  cv2.destroyAllWindows()    # 闭：先膨胀，再腐蚀  img = cv2.imread('dige.png')    kernel = np.ones((5,5),np.uint8)   closing = cv2.morphologyEx(img, cv2.MORPH_CLOSE, kernel)    cv2.imshow('closing', closing)  cv2.waitKey(0)  cv2.destroyAllWindows()    </code></pre><p>—|—  </p><h2 id="梯度运算"><a href="#梯度运算" class="headerlink" title="梯度运算"></a>梯度运算</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  </code></pre><p>| </p><pre><code># 梯度=膨胀-腐蚀  pie = cv2.imread('pie.png')  kernel = np.ones((7,7),np.uint8)   dilate = cv2.dilate(pie,kernel,iterations = 5)  erosion = cv2.erode(pie,kernel,iterations = 5)    res = np.hstack((dilate,erosion))    cv2.imshow('res', res)  cv2.waitKey(0)  cv2.destroyAllWindows()  gradient = cv2.morphologyEx(pie, cv2.MORPH_GRADIENT, kernel)    cv2.imshow('gradient', gradient)  cv2.waitKey(0)  cv2.destroyAllWindows()    </code></pre><p>—|—  </p><h2 id="礼帽与黑帽"><a href="#礼帽与黑帽" class="headerlink" title="礼帽与黑帽"></a>礼帽与黑帽</h2><ul><li><p>礼帽 = 原始输入-开运算结果</p></li><li><p>黑帽 = 闭运算-原始输入</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12</code></pre></li></ul><p>| </p><pre><code>    #礼帽      img = cv2.imread('dige.png')      tophat = cv2.morphologyEx(img, cv2.MORPH_TOPHAT, kernel)      cv2.imshow('tophat', tophat)      cv2.waitKey(0)      cv2.destroyAllWindows()      #黑帽      img = cv2.imread('dige.png')      blackhat  = cv2.morphologyEx(img,cv2.MORPH_BLACKHAT, kernel)      cv2.imshow('blackhat ', blackhat )      cv2.waitKey(0)      cv2.destroyAllWindows()        </code></pre><p>—|—  </p><p>参考资料： 唐宇迪 OpenCV计算机视觉实战(Python版)</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zhongjie</title>
      <link href="/2019/09/30/20190930-zhongjie/"/>
      <url>/2019/09/30/20190930-zhongjie/</url>
      
        <content type="html"><![CDATA[<p>一切终止于今天吧！</p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 其他 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>zhongjie</title>
      <link href="/2019/09/30/zhongjie/"/>
      <url>/2019/09/30/zhongjie/</url>
      
        <content type="html"><![CDATA[<p>一切终止于今天吧！</p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hungary算法</title>
      <link href="/2019/09/16/20190916-Hungary%E7%AE%97%E6%B3%95/"/>
      <url>/2019/09/16/20190916-Hungary%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hungary算法</title>
      <link href="/2019/09/16/Hungary%E7%AE%97%E6%B3%95/"/>
      <url>/2019/09/16/Hungary%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sort</title>
      <link href="/2019/09/15/20190915-Sort/"/>
      <url>/2019/09/15/20190915-Sort/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Sort</title>
      <link href="/2019/09/15/Sort/"/>
      <url>/2019/09/15/Sort/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>合并两个排序的链表</title>
      <link href="/2019/09/14/20190914-%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%8E%92%E5%BA%8F%E7%9A%84%E9%93%BE%E8%A1%A8/"/>
      <url>/2019/09/14/20190914-%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%8E%92%E5%BA%8F%E7%9A%84%E9%93%BE%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：587032<br>本题知识点： 链表</p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line">struct ListNode {</span><br><span class="line"><span class="built_in">int</span> val;</span><br><span class="line">struct ListNode *<span class="built_in">next</span>;</span><br><span class="line">ListNode(<span class="built_in">int</span> x) :</span><br><span class="line">val(x), <span class="built_in">next</span>(NULL) {</span><br><span class="line">}</span><br><span class="line">};*/</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> {</span><br><span class="line">public:</span><br><span class="line">    ListNode* Merge(ListNode* pHead1, ListNode* pHead2)</span><br><span class="line">    {</span><br><span class="line">        <span class="keyword">if</span>(!pHead1)<span class="keyword">return</span> pHead2;</span><br><span class="line">        <span class="keyword">if</span>(!pHead2)<span class="keyword">return</span> pHead1;</span><br><span class="line">        <span class="keyword">if</span>(pHead1-&gt;val&lt;=pHead2-&gt;val){</span><br><span class="line">            pHead1-&gt;<span class="built_in">next</span>= Merge(pHead1-&gt;<span class="built_in">next</span>,pHead2);</span><br><span class="line">            <span class="keyword">return</span> pHead1;</span><br><span class="line">        }<span class="keyword">else</span>{</span><br><span class="line">            pHead2-&gt;<span class="built_in">next</span>= Merge(pHead1,pHead2-&gt;<span class="built_in">next</span>);</span><br><span class="line">            <span class="keyword">return</span> pHead2;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># class ListNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.next = None</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="comment"># 返回合并后列表</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">Merge</span>(<span class="params">self, pHead1, pHead2</span>):</span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> pHead1 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> pHead2</span><br><span class="line">        <span class="keyword">if</span> pHead2 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> pHead1</span><br><span class="line">        <span class="keyword">if</span> pHead1.val &lt; pHead2.val:</span><br><span class="line">            pHead1.<span class="built_in">next</span> = self.Merge(pHead1.<span class="built_in">next</span>,pHead2)</span><br><span class="line">            <span class="keyword">return</span> pHead1</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            pHead2.<span class="built_in">next</span> = self.Merge(pHead1,pHead2.<span class="built_in">next</span>)</span><br><span class="line">            <span class="keyword">return</span> pHead2</span><br></pre></td></tr></tbody></table></figure><p>运行时间：3ms<br>占用内存：492k</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mosse</title>
      <link href="/2019/09/14/20190914-mosse/"/>
      <url>/2019/09/14/20190914-mosse/</url>
      
        <content type="html"><![CDATA[<h2 id="MOSSE"><a href="#MOSSE" class="headerlink" title="MOSSE"></a>MOSSE</h2><p>MOSSE(Minimum Output Sum of Squared Error) 是2010年 的CVPR，它的全名叫做Visual Object Tracking using Adaptive Correlation Filters。 MOSSE 是第一篇将correlation filter(CF) 引入object tracking 的论文，它也是CSK和KCF/DCF等算法的基础。</p><h3 id="CF-相关滤波"><a href="#CF-相关滤波" class="headerlink" title="CF(相关滤波)"></a>CF(相关滤波)</h3><p>相关一般分为自相关和互相关，这里我们一般指的是互相关，假设我们有两个信号f和g</p><p>f∗表示f的共轭，互相关的直接解释就是衡量两个信号在某个时刻τ时的相似程度。<br>假设f和g的形状一样，那么一定是f和g对齐的时候二者的相似程度最大，此时达到最大的输出响应，如下图所示：<br><img src="/images/20190914_mosse_mosse1.png"><br>卷积计算和相关计算的关系</p><ul><li>Two-dimensional correlation is equivalent to two-dimensional convolution with the filter matrix rotated 180 degrees.</li></ul><h2 id="论文解读"><a href="#论文解读" class="headerlink" title="论文解读"></a>论文解读</h2><hr><p>将CF应用在tracking方面最基本的思想就是，设计一个滤波模板，使得该模板与跟踪目标的ROI做卷积运算，得到最大的输出响应。</p><hr><ul><li>g表示输出响应</li><li>f表示输入原始图片的灰度图像</li><li>h表示滤波模板<br>为了简化计算，将时域的卷积转化为频域的点乘积。<br>时域公式表示：频域公式表示：所以目标H的计算为：在跟踪的光照等其他因素的影响下，为了提高滤波模板的鲁棒性，在文章中作者对GroundTruth进行随机仿射变换得到一系列的训练样本fi，gi是由高斯函数产生的并且其峰值位置是在fi的中心,我们同时考虑m帧作为参考，这就是MOSSE模型的思想，最终该模型的目标函数表示为：将目标函数最小化，对上式在频域进行求导（复数域不同于实数域），得到：在跟踪过程中，我们只需要将以上模板与当前帧与滤波模板做相关操作，在输出响应中找到最大值的位置，该位置就是目标在当前帧中的位置。本文的参数更新的策略为：其中，η是一个超参数，为经验值。</li></ul><p>缺点：</p><ul><li>输入的特征为单通道灰度图像，特征表达能力有限</li><li>没有尺度更新，对于尺度变化的跟踪目标不敏感</li></ul><p><img src="/images/20190914_mosse_surfer.gif"></p><h2 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h2><p>这里面主要做的就是 ，初始帧的输入与输出来求出Ai与Bi，从而求出初始的模板Hi，下面将初始的Hi与当前帧所在的上个位置进行卷积，频域也就是进行相乘。然后找到最大值的位置也就是当前目标的中心，由于宽高不变，所以在此基础上更新宽高就可以了，实现目标跟踪。  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">from</span> utils <span class="keyword">import</span> linear_mapping, pre_process, random_warp</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">This module implements the basic correlation filter based tracking algorithm -- MOSSE</span></span><br><span class="line"><span class="string">Date: 2018-05-28</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">mosse</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, args, img_path</span>):</span><br><span class="line">        <span class="comment"># get arguments..</span></span><br><span class="line">        self.args = args</span><br><span class="line">        self.img_path = img_path</span><br><span class="line">        <span class="comment"># get the img lists...</span></span><br><span class="line">        self.frame_lists = self._get_img_lists(self.img_path)</span><br><span class="line">        self.frame_lists.sort()</span><br><span class="line">    <span class="comment"># start to do the object tracking...</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">start_tracking</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment"># get the image of the first frame... (read as gray scale image...)</span></span><br><span class="line">        init_img = cv2.imread(self.frame_lists[<span class="number">0</span>])</span><br><span class="line">        init_frame = cv2.cvtColor(init_img, cv2.COLOR_BGR2GRAY)</span><br><span class="line">        init_frame = init_frame.astype(np.float32)</span><br><span class="line">        <span class="comment"># get the init ground truth.. [x, y, width, height]</span></span><br><span class="line">        init_gt = cv2.selectROI(<span class="string">'demo'</span>, init_img, <span class="literal">False</span>, <span class="literal">False</span>)</span><br><span class="line">        init_gt = np.array(init_gt).astype(np.int64)</span><br><span class="line">        <span class="comment"># start to draw the gaussian response...</span></span><br><span class="line">        response_map = self._get_gauss_response(init_frame, init_gt)</span><br><span class="line">        <span class="comment"># start to create the training set ...</span></span><br><span class="line">        <span class="comment"># get the goal..</span></span><br><span class="line">        <span class="built_in">print</span>(init_gt)</span><br><span class="line">        g = response_map[init_gt[<span class="number">1</span>]:init_gt[<span class="number">1</span>]+init_gt[<span class="number">3</span>], init_gt[<span class="number">0</span>]:init_gt[<span class="number">0</span>]+init_gt[<span class="number">2</span>]]</span><br><span class="line">        <span class="built_in">print</span>(g)</span><br><span class="line">        fi = init_frame[init_gt[<span class="number">1</span>]:init_gt[<span class="number">1</span>]+init_gt[<span class="number">3</span>], init_gt[<span class="number">0</span>]:init_gt[<span class="number">0</span>]+init_gt[<span class="number">2</span>]]</span><br><span class="line">        G = np.fft.fft2(g)</span><br><span class="line">        <span class="comment"># start to do the pre-training...</span></span><br><span class="line">        Ai, Bi = self._pre_training(fi, G)</span><br><span class="line">        <span class="comment"># start the tracking...</span></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(self.frame_lists)):</span><br><span class="line">            current_frame = cv2.imread(self.frame_lists[idx])</span><br><span class="line">            frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)</span><br><span class="line">            frame_gray = frame_gray.astype(np.float32)</span><br><span class="line">            <span class="keyword">if</span> idx == <span class="number">0</span>:</span><br><span class="line">                Ai = self.args.lr * Ai</span><br><span class="line">                Bi = self.args.lr * Bi</span><br><span class="line">                pos = init_gt.copy()</span><br><span class="line">                clip_pos = np.array([pos[<span class="number">0</span>], pos[<span class="number">1</span>], pos[<span class="number">0</span>]+pos[<span class="number">2</span>], pos[<span class="number">1</span>]+pos[<span class="number">3</span>]]).astype(np.int64)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                Hi = Ai / Bi</span><br><span class="line">                fi = frame_gray[clip_pos[<span class="number">1</span>]:clip_pos[<span class="number">3</span>], clip_pos[<span class="number">0</span>]:clip_pos[<span class="number">2</span>]]</span><br><span class="line">                fi = pre_process(cv2.resize(fi, (init_gt[<span class="number">2</span>], init_gt[<span class="number">3</span>])))</span><br><span class="line">                Gi = Hi * np.fft.fft2(fi)</span><br><span class="line">                gi = linear_mapping(np.fft.ifft2(Gi))</span><br><span class="line">                <span class="comment"># find the max pos...</span></span><br><span class="line">                max_value = np.<span class="built_in">max</span>(gi)</span><br><span class="line">                max_pos = np.where(gi == max_value)</span><br><span class="line">                dy = <span class="built_in">int</span>(np.mean(max_pos[<span class="number">0</span>]) - gi.shape[<span class="number">0</span>] / <span class="number">2</span>)</span><br><span class="line">                dx = <span class="built_in">int</span>(np.mean(max_pos[<span class="number">1</span>]) - gi.shape[<span class="number">1</span>] / <span class="number">2</span>)</span><br><span class="line">                <span class="comment"># update the position...</span></span><br><span class="line">                pos[<span class="number">0</span>] = pos[<span class="number">0</span>] + dx</span><br><span class="line">                pos[<span class="number">1</span>] = pos[<span class="number">1</span>] + dy</span><br><span class="line">                <span class="comment"># trying to get the clipped position [xmin, ymin, xmax, ymax]</span></span><br><span class="line">                clip_pos[<span class="number">0</span>] = np.clip(pos[<span class="number">0</span>], <span class="number">0</span>, current_frame.shape[<span class="number">1</span>])</span><br><span class="line">                clip_pos[<span class="number">1</span>] = np.clip(pos[<span class="number">1</span>], <span class="number">0</span>, current_frame.shape[<span class="number">0</span>])</span><br><span class="line">                clip_pos[<span class="number">2</span>] = np.clip(pos[<span class="number">0</span>]+pos[<span class="number">2</span>], <span class="number">0</span>, current_frame.shape[<span class="number">1</span>])</span><br><span class="line">                clip_pos[<span class="number">3</span>] = np.clip(pos[<span class="number">1</span>]+pos[<span class="number">3</span>], <span class="number">0</span>, current_frame.shape[<span class="number">0</span>])</span><br><span class="line">                clip_pos = clip_pos.astype(np.int64)</span><br><span class="line">                <span class="comment"># get the current fi..</span></span><br><span class="line">                fi = frame_gray[clip_pos[<span class="number">1</span>]:clip_pos[<span class="number">3</span>], clip_pos[<span class="number">0</span>]:clip_pos[<span class="number">2</span>]]</span><br><span class="line">                fi = pre_process(cv2.resize(fi, (init_gt[<span class="number">2</span>], init_gt[<span class="number">3</span>])))</span><br><span class="line">                <span class="comment"># online update...</span></span><br><span class="line">                Ai = self.args.lr * (G * np.conjugate(np.fft.fft2(fi))) + (<span class="number">1</span> - self.args.lr) * Ai</span><br><span class="line">                Bi = self.args.lr * (np.fft.fft2(fi) * np.conjugate(np.fft.fft2(fi))) + (<span class="number">1</span> - self.args.lr) * Bi</span><br><span class="line">            <span class="comment"># visualize the tracking process...</span></span><br><span class="line">            cv2.rectangle(current_frame, (pos[<span class="number">0</span>], pos[<span class="number">1</span>]), (pos[<span class="number">0</span>]+pos[<span class="number">2</span>], pos[<span class="number">1</span>]+pos[<span class="number">3</span>]), (<span class="number">255</span>, <span class="number">0</span>, <span class="number">0</span>), <span class="number">2</span>)</span><br><span class="line">            cv2.imshow(<span class="string">'demo'</span>, current_frame)</span><br><span class="line">            cv2.waitKey(<span class="number">100</span>)</span><br><span class="line">            <span class="comment"># if record... save the frames..</span></span><br><span class="line">            <span class="keyword">if</span> self.args.record:</span><br><span class="line">                frame_path = <span class="string">'record_frames/'</span> + self.img_path.split(<span class="string">'/'</span>)[<span class="number">1</span>] + <span class="string">'/'</span></span><br><span class="line">                <span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(frame_path):</span><br><span class="line">                    os.mkdir(frame_path)</span><br><span class="line">                cv2.imwrite(frame_path + <span class="built_in">str</span>(idx).zfill(<span class="number">5</span>) + <span class="string">'.png'</span>, current_frame)</span><br><span class="line">    <span class="comment"># pre train the filter on the first frame...</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_pre_training</span>(<span class="params">self, init_frame, G</span>):</span><br><span class="line">        height, width = G.shape</span><br><span class="line">        fi = cv2.resize(init_frame, (width, height))</span><br><span class="line">        <span class="comment"># pre-process img..</span></span><br><span class="line">        fi = pre_process(fi)</span><br><span class="line">        Ai = G * np.conjugate(np.fft.fft2(fi))</span><br><span class="line">        Bi = np.fft.fft2(init_frame) * np.conjugate(np.fft.fft2(init_frame))</span><br><span class="line">        <span class="keyword">for</span> _ <span class="keyword">in</span> <span class="built_in">range</span>(self.args.num_pretrain):</span><br><span class="line">            <span class="keyword">if</span> self.args.rotate:</span><br><span class="line">                fi = pre_process(random_warp(init_frame))</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                fi = pre_process(init_frame)</span><br><span class="line">            Ai = Ai + G * np.conjugate(np.fft.fft2(fi))</span><br><span class="line">            Bi = Bi + np.fft.fft2(fi) * np.conjugate(np.fft.fft2(fi))</span><br><span class="line">        <span class="keyword">return</span> Ai, Bi</span><br><span class="line">    <span class="comment"># get the ground-truth gaussian reponse...</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_get_gauss_response</span>(<span class="params">self, img, gt</span>):</span><br><span class="line">        <span class="comment"># get the shape of the image..</span></span><br><span class="line">        height, width = img.shape</span><br><span class="line">        <span class="comment"># get the mesh grid...</span></span><br><span class="line">        xx, yy = np.meshgrid(np.arange(width), np.arange(height))</span><br><span class="line">        <span class="comment"># get the center of the object...</span></span><br><span class="line">        center_x = gt[<span class="number">0</span>] + <span class="number">0.5</span> * gt[<span class="number">2</span>]</span><br><span class="line">        center_y = gt[<span class="number">1</span>] + <span class="number">0.5</span> * gt[<span class="number">3</span>]</span><br><span class="line">        <span class="comment"># cal the distance...</span></span><br><span class="line">        dist = (np.square(xx - center_x) + np.square(yy - center_y)) / (<span class="number">2</span> * self.args.sigma)</span><br><span class="line">        <span class="comment"># get the response map...</span></span><br><span class="line">        response = np.exp(-dist)</span><br><span class="line">        <span class="comment"># normalize...</span></span><br><span class="line">        response = linear_mapping(response)</span><br><span class="line">        <span class="keyword">return</span> response</span><br><span class="line">    <span class="comment"># it will extract the image list</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_get_img_lists</span>(<span class="params">self, img_path</span>):</span><br><span class="line">        frame_list = []</span><br><span class="line">        <span class="keyword">for</span> frame <span class="keyword">in</span> os.listdir(img_path):</span><br><span class="line">            <span class="keyword">if</span> os.path.splitext(frame)[<span class="number">1</span>] == <span class="string">'.jpg'</span>:</span><br><span class="line">                frame_list.append(os.path.join(img_path, frame))</span><br><span class="line">        <span class="keyword">return</span> frame_list</span><br><span class="line">    <span class="comment"># it will get the first ground truth of the video..</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">_get_init_ground_truth</span>(<span class="params">self, img_path</span>):</span><br><span class="line">        gt_path = os.path.join(img_path, <span class="string">'groundtruth.txt'</span>)</span><br><span class="line">        <span class="keyword">with</span> <span class="built_in">open</span>(gt_path, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">            <span class="comment"># just read the first frame...</span></span><br><span class="line">            line = f.readline()</span><br><span class="line">            gt_pos = line.split(<span class="string">','</span>)</span><br><span class="line">        <span class="keyword">return</span> [<span class="built_in">float</span>(element) <span class="keyword">for</span> element <span class="keyword">in</span> gt_pos]</span><br></pre></td></tr></tbody></table></figure><p>参考链接：<br><a href="http://simtalk.cn/2017/07/03/Object-Tracking/">http://simtalk.cn/2017/07/03/Object-Tracking/</a><br><a href="https://github.com/TianhongDai/mosse-object-tracking">https://github.com/TianhongDai/mosse-object-tracking</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标跟踪 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>树的子结构</title>
      <link href="/2019/09/14/20190914-%E6%A0%91%E7%9A%84%E5%AD%90%E7%BB%93%E6%9E%84/"/>
      <url>/2019/09/14/20190914-%E6%A0%91%E7%9A%84%E5%AD%90%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：829537<br>本题知识点： 链表</p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>输入一个链表，输出该链表中倒数第k个结点。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># -*- coding:utf-8 -*-</span></span><br><span class="line"><span class="comment"># class TreeNode:</span></span><br><span class="line"><span class="comment">#     def __init__(self, x):</span></span><br><span class="line"><span class="comment">#         self.val = x</span></span><br><span class="line"><span class="comment">#         self.left = None</span></span><br><span class="line"><span class="comment">#         self.right = None</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">HasSubtree</span>(<span class="params">self, pRoot1, pRoot2</span>):</span><br><span class="line">        <span class="comment"># write code here</span></span><br><span class="line">        <span class="keyword">if</span> pRoot1 <span class="keyword">is</span> <span class="literal">None</span> <span class="keyword">or</span> pRoot2 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        result = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> pRoot1.val == pRoot2.val:</span><br><span class="line">            result = self.isSubtree(pRoot1,pRoot2)</span><br><span class="line">        <span class="keyword">if</span> result == <span class="literal">False</span>:</span><br><span class="line">            result = self.HasSubtree(pRoot1.left, pRoot2) | self.HasSubtree(pRoot1.right, pRoot2)</span><br><span class="line">        <span class="keyword">return</span> result</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">isSubtree</span>(<span class="params">self,root1,root2</span>):</span><br><span class="line">        <span class="keyword">if</span> root2 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">True</span></span><br><span class="line">        <span class="keyword">if</span> root1 <span class="keyword">is</span> <span class="literal">None</span>:</span><br><span class="line">            <span class="keyword">return</span> <span class="literal">False</span></span><br><span class="line">        <span class="keyword">if</span> root1.val == root2.val:</span><br><span class="line">            <span class="keyword">return</span> self.isSubtree(root1.left,root2.left) &amp; self.isSubtree(root1.right,root2.right)</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">False</span></span><br></pre></td></tr></tbody></table></figure><p>运行时间：24 ms<br>占用内存：5860K</p>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mosse</title>
      <link href="/2019/09/14/mosse/"/>
      <url>/2019/09/14/mosse/</url>
      
        <content type="html"><![CDATA[<h2 id="MOSSE"><a href="#MOSSE" class="headerlink" title="MOSSE"></a>MOSSE</h2><p>MOSSE(Minimum Output Sum of Squared Error) 是2010年 的CVPR，它的全名叫做Visual Object Tracking using Adaptive Correlation Filters。 MOSSE 是第一篇将correlation filter(CF) 引入object tracking 的论文，它也是CSK和KCF/DCF等算法的基础。</p><h3 id="CF-相关滤波"><a href="#CF-相关滤波" class="headerlink" title="CF(相关滤波)"></a>CF(相关滤波)</h3><p>相关一般分为自相关和互相关，这里我们一般指的是互相关，假设我们有两个信号f和g</p><p>f∗表示f的共轭，互相关的直接解释就是衡量两个信号在某个时刻τ时的相似程度。<br>假设f和g的形状一样，那么一定是f和g对齐的时候二者的相似程度最大，此时达到最大的输出响应，如下图所示：<br><img src="/2019/09/14/mosse/images/20190914_mosse_mosse1.png"><br>卷积计算和相关计算的关系</p><ul><li>Two-dimensional correlation is equivalent to two-dimensional convolution with the filter matrix rotated 180 degrees.</li></ul><h2 id="论文解读"><a href="#论文解读" class="headerlink" title="论文解读"></a>论文解读</h2><hr><p>将CF应用在tracking方面最基本的思想就是，设计一个滤波模板，使得该模板与跟踪目标的ROI做卷积运算，得到最大的输出响应。</p><hr><ul><li>g表示输出响应</li><li>f表示输入原始图片的灰度图像</li><li>h表示滤波模板<br>为了简化计算，将时域的卷积转化为频域的点乘积。<br>时域公式表示：频域公式表示：所以目标H的计算为：在跟踪的光照等其他因素的影响下，为了提高滤波模板的鲁棒性，在文章中作者对GroundTruth进行随机仿射变换得到一系列的训练样本fi，gi是由高斯函数产生的并且其峰值位置是在fi的中心,我们同时考虑m帧作为参考，这就是MOSSE模型的思想，最终该模型的目标函数表示为：将目标函数最小化，对上式在频域进行求导（复数域不同于实数域），得到：在跟踪过程中，我们只需要将以上模板与当前帧与滤波模板做相关操作，在输出响应中找到最大值的位置，该位置就是目标在当前帧中的位置。本文的参数更新的策略为：其中，η是一个超参数，为经验值。</li></ul><p>缺点：</p><ul><li>输入的特征为单通道灰度图像，特征表达能力有限</li><li>没有尺度更新，对于尺度变化的跟踪目标不敏感</li></ul><p><img src="/2019/09/14/mosse/images/20190914_mosse_surfer.gif"></p><h2 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h2><p>这里面主要做的就是 ，初始帧的输入与输出来求出Ai与Bi，从而求出初始的模板Hi，下面将初始的Hi与当前帧所在的上个位置进行卷积，频域也就是进行相乘。然后找到最大值的位置也就是当前目标的中心，由于宽高不变，所以在此基础上更新宽高就可以了，实现目标跟踪。  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  </code></pre><p>| </p><pre><code>import numpy as np  import cv2  import os  from utils import linear_mapping, pre_process, random_warp    """  This module implements the basic correlation filter based tracking algorithm -- MOSSE    Date: 2018-05-28    """    class mosse:      def __init__(self, args, img_path):          # get arguments..          self.args = args          self.img_path = img_path          # get the img lists...          self.frame_lists = self._get_img_lists(self.img_path)          self.frame_lists.sort()            # start to do the object tracking...      def start_tracking(self):          # get the image of the first frame... (read as gray scale image...)          init_img = cv2.imread(self.frame_lists[0])          init_frame = cv2.cvtColor(init_img, cv2.COLOR_BGR2GRAY)          init_frame = init_frame.astype(np.float32)          # get the init ground truth.. [x, y, width, height]          init_gt = cv2.selectROI('demo', init_img, False, False)          init_gt = np.array(init_gt).astype(np.int64)          # start to draw the gaussian response...          response_map = self._get_gauss_response(init_frame, init_gt)          # start to create the training set ...          # get the goal..          print(init_gt)          g = response_map[init_gt[1]:init_gt[1]+init_gt[3], init_gt[0]:init_gt[0]+init_gt[2]]          print(g)          fi = init_frame[init_gt[1]:init_gt[1]+init_gt[3], init_gt[0]:init_gt[0]+init_gt[2]]          G = np.fft.fft2(g)          # start to do the pre-training...          Ai, Bi = self._pre_training(fi, G)            # start the tracking...          for idx in range(len(self.frame_lists)):              current_frame = cv2.imread(self.frame_lists[idx])              frame_gray = cv2.cvtColor(current_frame, cv2.COLOR_BGR2GRAY)              frame_gray = frame_gray.astype(np.float32)              if idx == 0:                  Ai = self.args.lr * Ai                  Bi = self.args.lr * Bi                  pos = init_gt.copy()                  clip_pos = np.array([pos[0], pos[1], pos[0]+pos[2], pos[1]+pos[3]]).astype(np.int64)              else:                  Hi = Ai / Bi                  fi = frame_gray[clip_pos[1]:clip_pos[3], clip_pos[0]:clip_pos[2]]                  fi = pre_process(cv2.resize(fi, (init_gt[2], init_gt[3])))                  Gi = Hi * np.fft.fft2(fi)                  gi = linear_mapping(np.fft.ifft2(Gi))                  # find the max pos...                  max_value = np.max(gi)                  max_pos = np.where(gi == max_value)                  dy = int(np.mean(max_pos[0]) - gi.shape[0] / 2)                  dx = int(np.mean(max_pos[1]) - gi.shape[1] / 2)                                    # update the position...                  pos[0] = pos[0] + dx                  pos[1] = pos[1] + dy                    # trying to get the clipped position [xmin, ymin, xmax, ymax]                  clip_pos[0] = np.clip(pos[0], 0, current_frame.shape[1])                  clip_pos[1] = np.clip(pos[1], 0, current_frame.shape[0])                  clip_pos[2] = np.clip(pos[0]+pos[2], 0, current_frame.shape[1])                  clip_pos[3] = np.clip(pos[1]+pos[3], 0, current_frame.shape[0])                  clip_pos = clip_pos.astype(np.int64)                    # get the current fi..                  fi = frame_gray[clip_pos[1]:clip_pos[3], clip_pos[0]:clip_pos[2]]                  fi = pre_process(cv2.resize(fi, (init_gt[2], init_gt[3])))                  # online update...                  Ai = self.args.lr * (G * np.conjugate(np.fft.fft2(fi))) + (1 - self.args.lr) * Ai                  Bi = self.args.lr * (np.fft.fft2(fi) * np.conjugate(np.fft.fft2(fi))) + (1 - self.args.lr) * Bi                            # visualize the tracking process...              cv2.rectangle(current_frame, (pos[0], pos[1]), (pos[0]+pos[2], pos[1]+pos[3]), (255, 0, 0), 2)              cv2.imshow('demo', current_frame)              cv2.waitKey(100)              # if record... save the frames..              if self.args.record:                  frame_path = 'record_frames/' + self.img_path.split('/')[1] + '/'                  if not os.path.exists(frame_path):                      os.mkdir(frame_path)                  cv2.imwrite(frame_path + str(idx).zfill(5) + '.png', current_frame)          # pre train the filter on the first frame...      def _pre_training(self, init_frame, G):          height, width = G.shape          fi = cv2.resize(init_frame, (width, height))          # pre-process img..          fi = pre_process(fi)          Ai = G * np.conjugate(np.fft.fft2(fi))          Bi = np.fft.fft2(init_frame) * np.conjugate(np.fft.fft2(init_frame))          for _ in range(self.args.num_pretrain):              if self.args.rotate:                  fi = pre_process(random_warp(init_frame))              else:                  fi = pre_process(init_frame)              Ai = Ai + G * np.conjugate(np.fft.fft2(fi))              Bi = Bi + np.fft.fft2(fi) * np.conjugate(np.fft.fft2(fi))                    return Ai, Bi        # get the ground-truth gaussian reponse...      def _get_gauss_response(self, img, gt):          # get the shape of the image..          height, width = img.shape          # get the mesh grid...          xx, yy = np.meshgrid(np.arange(width), np.arange(height))          # get the center of the object...          center_x = gt[0] + 0.5 * gt[2]          center_y = gt[1] + 0.5 * gt[3]          # cal the distance...          dist = (np.square(xx - center_x) + np.square(yy - center_y)) / (2 * self.args.sigma)          # get the response map...          response = np.exp(-dist)          # normalize...          response = linear_mapping(response)          return response        # it will extract the image list       def _get_img_lists(self, img_path):          frame_list = []          for frame in os.listdir(img_path):              if os.path.splitext(frame)[1] == '.jpg':                  frame_list.append(os.path.join(img_path, frame))           return frame_list            # it will get the first ground truth of the video..      def _get_init_ground_truth(self, img_path):          gt_path = os.path.join(img_path, 'groundtruth.txt')          with open(gt_path, 'r') as f:              # just read the first frame...              line = f.readline()              gt_pos = line.split(',')            return [float(element) for element in gt_pos]    </code></pre><p>—|—  </p><p>参考链接：<br><a href="http://simtalk.cn/2017/07/03/Object-Tracking/">http://simtalk.cn/2017/07/03/Object-Tracking/</a><br><a href="https://github.com/TianhongDai/mosse-object-tracking">https://github.com/TianhongDai/mosse-object-tracking</a></p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>合并两个排序的链表</title>
      <link href="/2019/09/14/%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%8E%92%E5%BA%8F%E7%9A%84%E9%93%BE%E8%A1%A8/"/>
      <url>/2019/09/14/%E5%90%88%E5%B9%B6%E4%B8%A4%E4%B8%AA%E6%8E%92%E5%BA%8F%E7%9A%84%E9%93%BE%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：587032<br>本题知识点： 链表</p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>输入两个单调递增的链表，输出两个链表合成后的链表，当然我们需要合成后的链表满足单调不减规则。</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  </code></pre><p>| </p><pre><code>/*  struct ListNode {      int val;      struct ListNode *next;      ListNode(int x) :              val(x), next(NULL) {      }  };*/  class Solution {  public:      ListNode* Merge(ListNode* pHead1, ListNode* pHead2)      {          if(!pHead1)return pHead2;          if(!pHead2)return pHead1;          if(pHead1-&gt;val&lt;=pHead2-&gt;val){              pHead1-&gt;next= Merge(pHead1-&gt;next,pHead2);              return pHead1;          }else{              pHead2-&gt;next= Merge(pHead1,pHead2-&gt;next);               return pHead2;          }      }  };    </code></pre><p>—|—  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  </code></pre><p>| </p><pre><code># -*- coding:utf-8 -*-  # class ListNode:  #     def __init__(self, x):  #         self.val = x  #         self.next = None    class Solution:      # 返回合并后列表      def Merge(self, pHead1, pHead2):          # write code here          if pHead1 is None:              return pHead2          if pHead2 is None:              return pHead1          if pHead1.val &lt; pHead2.val:              pHead1.next = self.Merge(pHead1.next,pHead2)              return pHead1          else:              pHead2.next = self.Merge(pHead1,pHead2.next)              return pHead2    </code></pre><p>—|—  </p><p>运行时间：3ms<br>占用内存：492k</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>树的子结构</title>
      <link href="/2019/09/14/%E6%A0%91%E7%9A%84%E5%AD%90%E7%BB%93%E6%9E%84/"/>
      <url>/2019/09/14/%E6%A0%91%E7%9A%84%E5%AD%90%E7%BB%93%E6%9E%84/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：829537<br>本题知识点： 链表</p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>输入一个链表，输出该链表中倒数第k个结点。</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  </code></pre><p>| </p><pre><code># -*- coding:utf-8 -*-  # class TreeNode:  #     def __init__(self, x):  #         self.val = x  #         self.left = None  #         self.right = None  class Solution:      def HasSubtree(self, pRoot1, pRoot2):          # write code here          if pRoot1 is None or pRoot2 is None:              return False                     result = False          if pRoot1.val == pRoot2.val:              result = self.isSubtree(pRoot1,pRoot2)          if result == False:              result = self.HasSubtree(pRoot1.left, pRoot2) | self.HasSubtree(pRoot1.right, pRoot2)                         return result                               def isSubtree(self,root1,root2):          if root2 is None:              return True          if root1 is None:              return False          if root1.val == root2.val:              return self.isSubtree(root1.left,root2.left) &amp; self.isSubtree(root1.right,root2.right)          return False    </code></pre><p>—|—  </p><p>运行时间：24 ms<br>占用内存：5860K</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>反转链表</title>
      <link href="/2019/09/12/20190912-%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/"/>
      <url>/2019/09/12/20190912-%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：606485<br>本题知识点： 链表</p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>输入一个链表，反转链表后，输出新链表的表头。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line">struct ListNode {</span><br><span class="line"><span class="built_in">int</span> val;</span><br><span class="line">struct ListNode *<span class="built_in">next</span>;</span><br><span class="line">ListNode(<span class="built_in">int</span> x) :</span><br><span class="line">val(x), <span class="built_in">next</span>(NULL) {</span><br><span class="line">}</span><br><span class="line">};*/</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> {</span><br><span class="line">public:</span><br><span class="line">    ListNode* ReverseList(ListNode* pHead) {</span><br><span class="line">        ListNode *prev = NULL;</span><br><span class="line">        ListNode *curr = pHead;</span><br><span class="line">        ListNode *<span class="built_in">next</span> = NULL;</span><br><span class="line">        <span class="keyword">while</span>(curr!=NULL)</span><br><span class="line">        {</span><br><span class="line">            <span class="built_in">next</span> = curr-&gt;<span class="built_in">next</span>;</span><br><span class="line">            curr-&gt;<span class="built_in">next</span> = prev;</span><br><span class="line">            prev = curr;</span><br><span class="line">            curr = <span class="built_in">next</span>;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">return</span> prev;</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><p>运行时间：4ms<br>占用内存：488</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>四边形按照顺时针排序</title>
      <link href="/2019/09/12/20190912-%E5%9B%9B%E8%BE%B9%E5%BD%A2%E6%8C%89%E7%85%A7%E9%A1%BA%E6%97%B6%E9%92%88%E6%8E%92%E5%BA%8F/"/>
      <url>/2019/09/12/20190912-%E5%9B%9B%E8%BE%B9%E5%BD%A2%E6%8C%89%E7%85%A7%E9%A1%BA%E6%97%B6%E9%92%88%E6%8E%92%E5%BA%8F/</url>
      
        <content type="html"><![CDATA[<p>做图像检测的时候处理数据经常遇到给出矩形的四个坐标点，要求找出左上角坐标并对乱序的坐标按顺时针或者逆时针进行排序  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> scipy.spatial <span class="keyword">import</span> distance <span class="keyword">as</span> dist</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">cos_dist</span>(<span class="params">a, b</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(a) != <span class="built_in">len</span>(b):</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    part_up = <span class="number">0.0</span></span><br><span class="line">    a_sq = <span class="number">0.0</span></span><br><span class="line">    b_sq = <span class="number">0.0</span></span><br><span class="line">    <span class="built_in">print</span>(a, b)</span><br><span class="line">    <span class="built_in">print</span>(<span class="built_in">zip</span>(a, b))</span><br><span class="line">    <span class="keyword">for</span> a1, b1 <span class="keyword">in</span> <span class="built_in">zip</span>(a, b):</span><br><span class="line">        part_up += a1*b1</span><br><span class="line">        a_sq += a1**<span class="number">2</span></span><br><span class="line">        b_sq += b1**<span class="number">2</span></span><br><span class="line">    part_down = math.sqrt(a_sq*b_sq)</span><br><span class="line">    <span class="keyword">if</span> part_down == <span class="number">0.0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> part_up / part_down</span><br><span class="line"><span class="comment"># this function is confined to rectangle</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">order_points</span>(<span class="params">pts</span>):</span><br><span class="line">    <span class="comment"># sort the points based on their x-coordinates</span></span><br><span class="line">    xSorted = pts[np.argsort(pts[:, <span class="number">0</span>]), :]</span><br><span class="line">    <span class="comment"># grab the left-most and right-most points from the sorted</span></span><br><span class="line">    <span class="comment"># x-roodinate points</span></span><br><span class="line">    leftMost = xSorted[:<span class="number">2</span>, :]</span><br><span class="line">    rightMost = xSorted[<span class="number">2</span>:, :]</span><br><span class="line">    <span class="comment"># now, sort the left-most coordinates according to their</span></span><br><span class="line">    <span class="comment"># y-coordinates so we can grab the top-left and bottom-left</span></span><br><span class="line">    <span class="comment"># points, respectively</span></span><br><span class="line">    leftMost = leftMost[np.argsort(leftMost[:, <span class="number">1</span>]), :]</span><br><span class="line">    (tl, bl) = leftMost</span><br><span class="line">    <span class="comment"># now that we have the top-left coordinate, use it as an</span></span><br><span class="line">    <span class="comment"># anchor to calculate the Euclidean distance between the</span></span><br><span class="line">    <span class="comment"># top-left and right-most points; by the Pythagorean</span></span><br><span class="line">    <span class="comment"># theorem, the point with the largest distance will be</span></span><br><span class="line">    <span class="comment"># our bottom-right point</span></span><br><span class="line">    D = dist.cdist(tl[np.newaxis], rightMost, <span class="string">"euclidean"</span>)[<span class="number">0</span>]</span><br><span class="line">    (br, tr) = rightMost[np.argsort(D)[::-<span class="number">1</span>], :]</span><br><span class="line">    <span class="comment"># return the coordinates in top-left, top-right,</span></span><br><span class="line">    <span class="comment"># bottom-right, and bottom-left order</span></span><br><span class="line">    <span class="keyword">return</span> np.array([tl, tr, br, bl], dtype=<span class="string">"float32"</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">order_points_quadrangle</span>(<span class="params">pts</span>):</span><br><span class="line">    <span class="comment"># sort the points based on their x-coordinates</span></span><br><span class="line">    xSorted = pts[np.argsort(pts[:, <span class="number">0</span>]), :]</span><br><span class="line">    <span class="comment"># grab the left-most and right-most points from the sorted</span></span><br><span class="line">    <span class="comment"># x-roodinate points</span></span><br><span class="line">    leftMost = xSorted[:<span class="number">2</span>, :]</span><br><span class="line">    rightMost = xSorted[<span class="number">2</span>:, :]</span><br><span class="line">    <span class="comment"># now, sort the left-most coordinates according to their</span></span><br><span class="line">    <span class="comment"># y-coordinates so we can grab the top-left and bottom-left</span></span><br><span class="line">    <span class="comment"># points, respectively</span></span><br><span class="line">    leftMost = leftMost[np.argsort(leftMost[:, <span class="number">1</span>]), :]</span><br><span class="line">    (tl, bl) = leftMost</span><br><span class="line">    <span class="comment"># now that we have the top-left and bottom-left coordinate, use it as an</span></span><br><span class="line">    <span class="comment"># base vector to calculate the angles between the other two vectors</span></span><br><span class="line">    vector_0 = np.array(bl-tl)</span><br><span class="line">    vector_1 = np.array(rightMost[<span class="number">0</span>]-tl)</span><br><span class="line">    vector_2 = np.array(rightMost[<span class="number">1</span>]-tl)</span><br><span class="line">    angle = [np.arccos(cos_dist(vector_0, vector_1)), np.arccos(cos_dist(vector_0, vector_2))]</span><br><span class="line">    (br, tr) = rightMost[np.argsort(angle), :]</span><br><span class="line">    <span class="comment"># return the coordinates in top-left, top-right,</span></span><br><span class="line">    <span class="comment"># bottom-right, and bottom-left order</span></span><br><span class="line">    <span class="keyword">return</span> np.array([tl, tr, br, bl], dtype=<span class="string">"float32"</span>)</span><br><span class="line">testdata =[<span class="number">1074</span>,<span class="number">439</span>,<span class="number">1078</span>,<span class="number">424</span>,<span class="number">991</span>,<span class="number">427</span>,<span class="number">991</span>,<span class="number">411</span>]</span><br><span class="line">points = numpy.array(array).reshape(<span class="number">4</span>,<span class="number">2</span>)</span><br><span class="line">poit = order_points_quadrangle(points)</span><br><span class="line">poit</span><br><span class="line">array([[ <span class="number">991.</span>,  <span class="number">411.</span>],</span><br><span class="line">       [<span class="number">1078.</span>,  <span class="number">424.</span>],</span><br><span class="line">       [<span class="number">1074.</span>,  <span class="number">439.</span>],</span><br><span class="line">       [ <span class="number">991.</span>,  <span class="number">427.</span>]], dtype=float32)</span><br></pre></td></tr></tbody></table></figure><p>参考链接：<br><a href="http://www.bnee.net/article/821708.html">http://www.bnee.net/article/821708.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>反转链表</title>
      <link href="/2019/09/12/%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/"/>
      <url>/2019/09/12/%E5%8F%8D%E8%BD%AC%E9%93%BE%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：606485<br>本题知识点： 链表</p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>输入一个链表，反转链表后，输出新链表的表头。</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  </code></pre><p>| </p><pre><code>/*  struct ListNode {      int val;      struct ListNode *next;      ListNode(int x) :              val(x), next(NULL) {      }  };*/  class Solution {  public:      ListNode* ReverseList(ListNode* pHead) {          ListNode *prev = NULL;          ListNode *curr = pHead;          ListNode *next = NULL;          while(curr!=NULL)          {              next = curr-&gt;next;              curr-&gt;next = prev;              prev = curr;              curr = next;          }          return prev;      }  };    </code></pre><p>—|—  </p><p>运行时间：4ms<br>占用内存：488</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>四边形按照顺时针排序</title>
      <link href="/2019/09/12/%E5%9B%9B%E8%BE%B9%E5%BD%A2%E6%8C%89%E7%85%A7%E9%A1%BA%E6%97%B6%E9%92%88%E6%8E%92%E5%BA%8F/"/>
      <url>/2019/09/12/%E5%9B%9B%E8%BE%B9%E5%BD%A2%E6%8C%89%E7%85%A7%E9%A1%BA%E6%97%B6%E9%92%88%E6%8E%92%E5%BA%8F/</url>
      
        <content type="html"><![CDATA[<p>做图像检测的时候处理数据经常遇到给出矩形的四个坐标点，要求找出左上角坐标并对乱序的坐标按顺时针或者逆时针进行排序  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  </code></pre><p>| </p><pre><code>from scipy.spatial import distance as dist  import numpy as np  import math      def cos_dist(a, b):      if len(a) != len(b):          return None      part_up = 0.0      a_sq = 0.0      b_sq = 0.0      print(a, b)      print(zip(a, b))       for a1, b1 in zip(a, b):          part_up += a1*b1          a_sq += a1**2          b_sq += b1**2      part_down = math.sqrt(a_sq*b_sq)      if part_down == 0.0:          return None      else:          return part_up / part_down      # this function is confined to rectangle  def order_points(pts):      # sort the points based on their x-coordinates      xSorted = pts[np.argsort(pts[:, 0]), :]        # grab the left-most and right-most points from the sorted      # x-roodinate points      leftMost = xSorted[:2, :]      rightMost = xSorted[2:, :]        # now, sort the left-most coordinates according to their      # y-coordinates so we can grab the top-left and bottom-left      # points, respectively      leftMost = leftMost[np.argsort(leftMost[:, 1]), :]      (tl, bl) = leftMost        # now that we have the top-left coordinate, use it as an      # anchor to calculate the Euclidean distance between the      # top-left and right-most points; by the Pythagorean      # theorem, the point with the largest distance will be      # our bottom-right point      D = dist.cdist(tl[np.newaxis], rightMost, "euclidean")[0]      (br, tr) = rightMost[np.argsort(D)[::-1], :]        # return the coordinates in top-left, top-right,      # bottom-right, and bottom-left order      return np.array([tl, tr, br, bl], dtype="float32")      def order_points_quadrangle(pts):      # sort the points based on their x-coordinates      xSorted = pts[np.argsort(pts[:, 0]), :]        # grab the left-most and right-most points from the sorted      # x-roodinate points      leftMost = xSorted[:2, :]      rightMost = xSorted[2:, :]        # now, sort the left-most coordinates according to their      # y-coordinates so we can grab the top-left and bottom-left      # points, respectively      leftMost = leftMost[np.argsort(leftMost[:, 1]), :]      (tl, bl) = leftMost        # now that we have the top-left and bottom-left coordinate, use it as an      # base vector to calculate the angles between the other two vectors        vector_0 = np.array(bl-tl)      vector_1 = np.array(rightMost[0]-tl)      vector_2 = np.array(rightMost[1]-tl)        angle = [np.arccos(cos_dist(vector_0, vector_1)), np.arccos(cos_dist(vector_0, vector_2))]      (br, tr) = rightMost[np.argsort(angle), :]        # return the coordinates in top-left, top-right,      # bottom-right, and bottom-left order      return np.array([tl, tr, br, bl], dtype="float32")  testdata =[1074,439,1078,424,991,427,991,411]  points = numpy.array(array).reshape(4,2)  poit = order_points_quadrangle(points)    poit    array([[ 991.,  411.],         [1078.,  424.],         [1074.,  439.],         [ 991.,  427.]], dtype=float32)    </code></pre><p>—|—  </p><p>参考链接：<br><a href="http://www.bnee.net/article/821708.html">http://www.bnee.net/article/821708.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>链表中倒数第k个结点</title>
      <link href="/2019/09/10/20190910-%E9%93%BE%E8%A1%A8%E4%B8%AD%E5%80%92%E6%95%B0%E7%AC%ACk%E4%B8%AA%E7%BB%93%E7%82%B9/"/>
      <url>/2019/09/10/20190910-%E9%93%BE%E8%A1%A8%E4%B8%AD%E5%80%92%E6%95%B0%E7%AC%ACk%E4%B8%AA%E7%BB%93%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：829537<br>本题知识点： 链表  </p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>输入一个链表，输出该链表中倒数第k个结点。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line">struct ListNode {</span><br><span class="line"><span class="built_in">int</span> val;</span><br><span class="line">struct ListNode *<span class="built_in">next</span>;</span><br><span class="line">ListNode(<span class="built_in">int</span> x) :</span><br><span class="line">val(x), <span class="built_in">next</span>(NULL) {</span><br><span class="line">}</span><br><span class="line">};*/</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> {</span><br><span class="line">public:</span><br><span class="line">    ListNode* FindKthToTail(ListNode* pListHead, unsigned <span class="built_in">int</span> k) {</span><br><span class="line">        <span class="keyword">if</span> (k == <span class="number">0</span>)</span><br><span class="line">          <span class="keyword">return</span> NULL;//如果K为<span class="number">0</span>，返回NULL</span><br><span class="line">        queue&lt;ListNode*&gt; que;</span><br><span class="line">        ListNode *node = pListHead;</span><br><span class="line">        <span class="keyword">while</span> (node != NULL)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span> (que.size() == k)</span><br><span class="line">            {</span><br><span class="line">                que.pop();</span><br><span class="line">            }</span><br><span class="line">            que.push(node);</span><br><span class="line">            node = node-&gt;<span class="built_in">next</span>;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">if</span> (que.size() == k)</span><br><span class="line">            <span class="keyword">return</span> que.front();</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> NULL;//如果k大于链表的最大长度，返回NULL</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><p>运行时间：3ms<br>占用内存：472K</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>链表中倒数第k个结点</title>
      <link href="/2019/09/10/%E9%93%BE%E8%A1%A8%E4%B8%AD%E5%80%92%E6%95%B0%E7%AC%ACk%E4%B8%AA%E7%BB%93%E7%82%B9/"/>
      <url>/2019/09/10/%E9%93%BE%E8%A1%A8%E4%B8%AD%E5%80%92%E6%95%B0%E7%AC%ACk%E4%B8%AA%E7%BB%93%E7%82%B9/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：829537<br>本题知识点： 链表  </p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>输入一个链表，输出该链表中倒数第k个结点。</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  </code></pre><p>| </p><pre><code>/*  struct ListNode {      int val;      struct ListNode *next;      ListNode(int x) :              val(x), next(NULL) {      }  };*/  class Solution {  public:      ListNode* FindKthToTail(ListNode* pListHead, unsigned int k) {              if (k == 0)            return NULL;//如果K为0，返回NULL          queue&lt;ListNode*&gt; que;          ListNode *node = pListHead;          while (node != NULL)          {              if (que.size() == k)              {                  que.pop();              }              que.push(node);              node = node-&gt;next;          }          if (que.size() == k)              return que.front();          else              return NULL;//如果k大于链表的最大长度，返回NULL      }  };    </code></pre><p>—|—  </p><p>运行时间：3ms<br>占用内存：472K</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二进制中1的个数</title>
      <link href="/2019/09/08/20190908-%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0/"/>
      <url>/2019/09/08/20190908-%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：489798<br>本题知识点： 进制转化 补码 反码 原码</p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> {</span><br><span class="line">public:</span><br><span class="line">     <span class="built_in">int</span>  NumberOf1(<span class="built_in">int</span> n) {</span><br><span class="line">         <span class="built_in">int</span> count=<span class="number">0</span>;</span><br><span class="line">         <span class="keyword">while</span>(n!=<span class="number">0</span>)</span><br><span class="line">         {</span><br><span class="line">             count++;</span><br><span class="line">             n = (n-<span class="number">1</span>)&amp;n;</span><br><span class="line">         }</span><br><span class="line">         <span class="keyword">return</span> count;</span><br><span class="line">     }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20190908_%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0_jiexi.png"></p><p>运行时间：3ms<br>占用内存：356k</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从csv中读取数据</title>
      <link href="/2019/09/08/20190908-%E4%BB%8Ecsv%E4%B8%AD%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE/"/>
      <url>/2019/09/08/20190908-%E4%BB%8Ecsv%E4%B8%AD%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loadDataSet</span>(<span class="params">filename</span>):  <span class="comment"># 读取数据</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filename) <span class="keyword">as</span> f:</span><br><span class="line">        dataMat = []</span><br><span class="line">        labelMat = []</span><br><span class="line">        f_csv = csv.reader(f) <span class="comment">## 用csv读取直接是个list</span></span><br><span class="line">        headers = <span class="built_in">next</span>(f_csv)</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> f_csv:</span><br><span class="line">            dataMat.append([<span class="built_in">float</span>(row[<span class="number">0</span>]), <span class="built_in">float</span>(row[<span class="number">1</span>])])</span><br><span class="line">            labelMat.append(<span class="built_in">float</span>(row[<span class="number">2</span>]))</span><br><span class="line">        f.close()</span><br><span class="line">        <span class="keyword">return</span> dataMat, labelMat</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loadData</span>(<span class="params">filename</span>): <span class="comment"># 读取数据</span></span><br><span class="line">    dataMat=[]</span><br><span class="line">    labelMat=[]</span><br><span class="line">    fr=<span class="built_in">open</span>(filename)</span><br><span class="line">    <span class="built_in">next</span>(fr) <span class="comment"># 忽略第一行</span></span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> fr.readlines():</span><br><span class="line">        lineArr=line.strip().split(<span class="string">','</span>)</span><br><span class="line">        dataMat.append([lineArr[<span class="number">0</span>],lineArr[<span class="number">1</span>]])</span><br><span class="line">        labelMat.append(lineArr[<span class="number">2</span>])</span><br><span class="line">    <span class="keyword">return</span> dataMat,labelMat <span class="comment"># 返回数据特征和数据类别</span></span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    dataMat,labelMat = loadData(<span class="string">'test_data.csv'</span>)</span><br><span class="line">    <span class="built_in">print</span>(dataMat,labelMat)</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>支持向量机</title>
      <link href="/2019/09/08/20190908-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
      <url>/2019/09/08/20190908-%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<p>SVM的本质：寻找最大的间隔<br>支持向量：距离超平面最近的那些点<br><strong>SMO算法的原理：</strong> 每次循环中选择两个alpha进行优化处理。一旦找到一对合适的alpha，那么就增大其中一个同时减小另一个。合适：条件一，两个alpha要在间隔边界之外；条件二，这两个alpha还没有进行过区间化处理或不在边界上</p><p><img src="/images/20190908_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA_svm.png"></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> csv</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">loadDataSet</span>(<span class="params">filename</span>):  <span class="comment"># 读取数据</span></span><br><span class="line">    <span class="keyword">with</span> <span class="built_in">open</span>(filename) <span class="keyword">as</span> f:</span><br><span class="line">        dataMat = []</span><br><span class="line">        labelMat = []</span><br><span class="line">        f_csv = csv.reader(f)  <span class="comment">## 用csv读取直接是个list</span></span><br><span class="line">        headers = <span class="built_in">next</span>(f_csv)</span><br><span class="line">        <span class="keyword">for</span> row <span class="keyword">in</span> f_csv:</span><br><span class="line">            dataMat.append([<span class="built_in">float</span>(row[<span class="number">0</span>]), <span class="built_in">float</span>(row[<span class="number">1</span>])])</span><br><span class="line">            labelMat.append(<span class="built_in">float</span>(row[<span class="number">2</span>]))</span><br><span class="line">        f.close()</span><br><span class="line">        <span class="keyword">return</span> dataMat, labelMat</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">selectJrand</span>(<span class="params">i, m</span>):  <span class="comment"># 在0-m中随机选择一个不是i的整数</span></span><br><span class="line">    j = i</span><br><span class="line">    <span class="keyword">while</span> (j == i):</span><br><span class="line">        j = <span class="built_in">int</span>(random.uniform(<span class="number">0</span>, m))</span><br><span class="line">    <span class="keyword">return</span> j</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">clipAlpha</span>(<span class="params">aj, H, L</span>):  <span class="comment"># 保证a在L和H范围内（L &lt;= a &lt;= H）</span></span><br><span class="line">    <span class="keyword">if</span> aj &gt; H:</span><br><span class="line">        aj = H</span><br><span class="line">    <span class="keyword">if</span> L &gt; aj:</span><br><span class="line">        aj = L</span><br><span class="line">    <span class="keyword">return</span> aj</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kernelTrans</span>(<span class="params">X, A, kTup</span>):  <span class="comment"># 核函数，输入参数,X:支持向量的特征树；A：某一行特征数据；kTup：('lin',k1)核函数的类型和参数</span></span><br><span class="line">    m, n = shape(X)</span><br><span class="line">    K = mat(zeros((m, <span class="number">1</span>)))</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"A shape : "</span>,A.shape)</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"X shape : "</span>,X.shape)</span><br><span class="line">    <span class="keyword">if</span> kTup[<span class="number">0</span>] == <span class="string">'lin'</span>:  <span class="comment"># 线性函数</span></span><br><span class="line">        K = X * A.T</span><br><span class="line">    <span class="keyword">elif</span> kTup[<span class="number">0</span>] == <span class="string">'rbf'</span>:  <span class="comment"># 径向基函数(radial bias function)</span></span><br><span class="line">        <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">            deltaRow = X[j, :] - A</span><br><span class="line">            K[j] = deltaRow * deltaRow.T</span><br><span class="line">        K = exp(K / (-<span class="number">1</span> * kTup[<span class="number">1</span>] ** <span class="number">2</span>))  <span class="comment"># 返回生成的结果</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">raise</span> NameError(<span class="string">'Houston We Have a Problem -- That Kernel is not recognized'</span>)</span><br><span class="line">    <span class="keyword">return</span> K</span><br><span class="line"><span class="comment"># 定义类，方便存储数据</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">optStruct</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, dataMatIn, classLabels, C, toler, kTup</span>):  <span class="comment"># 存储各类参数</span></span><br><span class="line">        self.X = dataMatIn  <span class="comment"># 数据特征</span></span><br><span class="line">        self.labelMat = classLabels  <span class="comment"># 数据类别</span></span><br><span class="line">        self.C = C  <span class="comment"># 软间隔参数C，参数越大，非线性拟合能力越强</span></span><br><span class="line">        self.tol = toler  <span class="comment"># 停止阀值</span></span><br><span class="line">        self.m = shape(dataMatIn)[<span class="number">0</span>]  <span class="comment"># 数据 b行数</span></span><br><span class="line">        self.alphas = mat(zeros((self.m, <span class="number">1</span>)))</span><br><span class="line">        self.b = <span class="number">0</span>  <span class="comment"># 初始设为0</span></span><br><span class="line">        self.eCache = mat(zeros((self.m, <span class="number">2</span>)))  <span class="comment"># 缓存</span></span><br><span class="line">        self.K = mat(zeros((self.m, self.m)))  <span class="comment"># 核函数的计算结果</span></span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.m):</span><br><span class="line">            self.K[:, i] = kernelTrans(self.X, self.X[i, :], kTup)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">calcEk</span>(<span class="params">oS, k</span>):  <span class="comment"># 计算Ek（参考《统计学习方法》p127公式7.105）</span></span><br><span class="line">    fXk = <span class="built_in">float</span>(multiply(oS.alphas, oS.labelMat).T * oS.K[:, k] + oS.b)</span><br><span class="line">    Ek = fXk - <span class="built_in">float</span>(oS.labelMat[k])</span><br><span class="line">    <span class="keyword">return</span> Ek</span><br><span class="line"><span class="comment"># 随机选取aj，并返回其E值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">selectJ</span>(<span class="params">i, oS, Ei</span>):</span><br><span class="line">    maxK = -<span class="number">1</span></span><br><span class="line">    maxDeltaE = <span class="number">0</span></span><br><span class="line">    Ej = <span class="number">0</span></span><br><span class="line">    oS.eCache[i] = [<span class="number">1</span>, Ei]</span><br><span class="line">    validEcacheList = nonzero(oS.eCache[:, <span class="number">0</span>].A)[<span class="number">0</span>]  <span class="comment"># 返回矩阵中的非零位置的行数</span></span><br><span class="line">    <span class="keyword">if</span> (<span class="built_in">len</span>(validEcacheList)) &gt; <span class="number">1</span>:</span><br><span class="line">        <span class="keyword">for</span> k <span class="keyword">in</span> validEcacheList:</span><br><span class="line">            <span class="keyword">if</span> k == i:</span><br><span class="line">                <span class="keyword">continue</span></span><br><span class="line">            Ek = calcEk(oS, k)</span><br><span class="line">            deltaE = <span class="built_in">abs</span>(Ei - Ek)</span><br><span class="line">            <span class="keyword">if</span> (deltaE &gt; maxDeltaE):  <span class="comment"># 返回步长最大的aj</span></span><br><span class="line">                maxK = k</span><br><span class="line">                maxDeltaE = deltaE</span><br><span class="line">                Ej = Ek</span><br><span class="line">        <span class="keyword">return</span> maxK, Ej</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        j = selectJrand(i, oS.m)</span><br><span class="line">        Ej = calcEk(oS, j)</span><br><span class="line">    <span class="keyword">return</span> j, Ej</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">updateEk</span>(<span class="params">oS, k</span>):  <span class="comment"># 更新os数据</span></span><br><span class="line">    Ek = calcEk(oS, k)</span><br><span class="line">    oS.eCache[k] = [<span class="number">1</span>, Ek]</span><br><span class="line"><span class="comment"># 首先检验ai是否满足KKT条件，如果不满足，随机选择aj进行优化，更新ai,aj,b值</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">innerL</span>(<span class="params">i, oS</span>):  <span class="comment"># 输入参数i和所有参数数据</span></span><br><span class="line">    Ei = calcEk(oS, i)  <span class="comment"># 计算E值</span></span><br><span class="line">    <span class="keyword">if</span> ((oS.labelMat[i] * Ei &lt; -oS.tol) <span class="keyword">and</span> (oS.alphas[i] &lt; oS.C)) <span class="keyword">or</span> (</span><br><span class="line">            (oS.labelMat[i] * Ei &gt; oS.tol) <span class="keyword">and</span> (oS.alphas[i] &gt; <span class="number">0</span>)):  <span class="comment"># 检验这行数据是否符合KKT条件 参考《统计学习方法》p128公式7.111-113</span></span><br><span class="line">        j, Ej = selectJ(i, oS, Ei)  <span class="comment"># 随机选取aj，并返回其E值</span></span><br><span class="line">        alphaIold = oS.alphas[i].copy()</span><br><span class="line">        alphaJold = oS.alphas[j].copy()</span><br><span class="line">        <span class="keyword">if</span> (oS.labelMat[i] != oS.labelMat[j]):  <span class="comment"># 以下代码的公式参考《统计学习方法》p126</span></span><br><span class="line">            L = <span class="built_in">max</span>(<span class="number">0</span>, oS.alphas[j] - oS.alphas[i])</span><br><span class="line">            H = <span class="built_in">min</span>(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            L = <span class="built_in">max</span>(<span class="number">0</span>, oS.alphas[j] + oS.alphas[i] - oS.C)</span><br><span class="line">            H = <span class="built_in">min</span>(oS.C, oS.alphas[j] + oS.alphas[i])</span><br><span class="line">        <span class="keyword">if</span> L == H:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"L==H"</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        eta = <span class="number">2.0</span> * oS.K[i, j] - oS.K[i, i] - oS.K[j, j]  <span class="comment"># 参考《统计学习方法》p127公式7.107</span></span><br><span class="line">        <span class="keyword">if</span> eta &gt;= <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"eta&gt;=0"</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej) / eta  <span class="comment"># 参考《统计学习方法》p127公式7.106</span></span><br><span class="line">        oS.alphas[j] = clipAlpha(oS.alphas[j], H, L)  <span class="comment"># 参考《统计学习方法》p127公式7.108</span></span><br><span class="line">        updateEk(oS, j)</span><br><span class="line">        <span class="keyword">if</span> (<span class="built_in">abs</span>(oS.alphas[j] - alphaJold) &lt; oS.tol):  <span class="comment"># alpha变化大小阀值（自己设定）</span></span><br><span class="line">            <span class="built_in">print</span>(<span class="string">"j not moving enough"</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">        oS.alphas[i] += oS.labelMat[j] * oS.labelMat[i] * (alphaJold - oS.alphas[j])  <span class="comment"># 参考《统计学习方法》p127公式7.109</span></span><br><span class="line">        updateEk(oS, i)  <span class="comment"># 更新数据</span></span><br><span class="line">        <span class="comment"># 以下求解b的过程，参考《统计学习方法》p129公式7.114-7.116</span></span><br><span class="line">        b1 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, i] - oS.labelMat[j] * (</span><br><span class="line">                    oS.alphas[j] - alphaJold) * oS.K[i, j]</span><br><span class="line">        b2 = oS.b - Ej - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, j] - oS.labelMat[j] * (</span><br><span class="line">                    oS.alphas[j] - alphaJold) * oS.K[j, j]</span><br><span class="line">        <span class="keyword">if</span> (<span class="number">0</span> &lt; oS.alphas[i] &lt; oS.C):</span><br><span class="line">            oS.b = b1</span><br><span class="line">        <span class="keyword">elif</span> (<span class="number">0</span> &lt; oS.alphas[j] &lt; oS.C):</span><br><span class="line">            oS.b = b2</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            oS.b = (b1 + b2) / <span class="number">2.0</span></span><br><span class="line">        <span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line"><span class="comment"># SMO函数，用于快速求解出alpha</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">smoP</span>(<span class="params">dataMatIn, classLabels, C, toler, maxIter, kTup=(<span class="params"><span class="string">'lin'</span>, <span class="number">0</span></span>)</span>):  <span class="comment"># 输入参数：数据特征，数据类别，参数C，阀值toler，最大迭代次数，核函数（默认线性核）</span></span><br><span class="line">    oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler, kTup) <span class="comment"># dataArr, labelArr, 200, 0.0001, 10000, ('rbf', 1.3))  # 通过SMO算法得到b和alpha</span></span><br><span class="line">    <span class="built_in">iter</span> = <span class="number">0</span></span><br><span class="line">    entireSet = <span class="literal">True</span></span><br><span class="line">    alphaPairsChanged = <span class="number">0</span></span><br><span class="line">    <span class="keyword">while</span> (<span class="built_in">iter</span> &lt; maxIter) <span class="keyword">and</span> ((alphaPairsChanged &gt; <span class="number">0</span>) <span class="keyword">or</span> (entireSet)):</span><br><span class="line">        alphaPairsChanged = <span class="number">0</span></span><br><span class="line">        <span class="keyword">if</span> entireSet:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(oS.m):  <span class="comment"># 遍历所有数据</span></span><br><span class="line">                alphaPairsChanged += innerL(i, oS)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">"fullSet, iter: %d i:%d, pairs changed %d"</span> % (</span><br><span class="line">                <span class="built_in">iter</span>, i, alphaPairsChanged))  <span class="comment"># 显示第多少次迭代，那行特征数据使alpha发生了改变，这次改变了多少次alpha</span></span><br><span class="line">            <span class="built_in">iter</span> += <span class="number">1</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            nonBoundIs = nonzero((oS.alphas.A &gt; <span class="number">0</span>) * (oS.alphas.A &lt; C))[<span class="number">0</span>]</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> nonBoundIs:  <span class="comment"># 遍历非边界的数据</span></span><br><span class="line">                alphaPairsChanged += innerL(i, oS)</span><br><span class="line">                <span class="built_in">print</span>(<span class="string">"non-bound, iter: %d i:%d, pairs changed %d"</span> % (<span class="built_in">iter</span>, i, alphaPairsChanged))</span><br><span class="line">            <span class="built_in">iter</span> += <span class="number">1</span></span><br><span class="line">        <span class="keyword">if</span> entireSet:</span><br><span class="line">            entireSet = <span class="literal">False</span></span><br><span class="line">        <span class="keyword">elif</span> (alphaPairsChanged == <span class="number">0</span>):</span><br><span class="line">            entireSet = <span class="literal">True</span></span><br><span class="line">        <span class="built_in">print</span>(<span class="string">"iteration number: %d"</span> % <span class="built_in">iter</span>)</span><br><span class="line">    <span class="keyword">return</span> oS.b, oS.alphas</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">train</span>(<span class="params">data_train, data_test</span>):</span><br><span class="line">    dataArr, labelArr = loadDataSet(data_train)  <span class="comment"># 读取训练数据</span></span><br><span class="line">    b, alphas = smoP(dataArr, labelArr, <span class="number">200</span>, <span class="number">0.0001</span>, <span class="number">10000</span>, (<span class="string">'rbf'</span>, <span class="number">1.3</span>))  <span class="comment"># 通过SMO算法得到b和alpha</span></span><br><span class="line">    datMat = mat(dataArr)</span><br><span class="line">    labelMat = mat(labelArr).transpose()</span><br><span class="line">    svInd = nonzero(alphas)[<span class="number">0</span>]  <span class="comment"># 选取不为0数据的行数（也就是支持向量）</span></span><br><span class="line">    sVs = datMat[svInd]  <span class="comment"># 支持向量的特征数据</span></span><br><span class="line">    labelSV = labelMat[svInd]  <span class="comment"># 支持向量的类别（1或-1）</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"there are %d Support Vectors"</span> % shape(sVs)[<span class="number">0</span>])  <span class="comment"># 打印出共有多少的支持向量</span></span><br><span class="line">    m, n = shape(datMat)  <span class="comment"># 训练数据的行列数</span></span><br><span class="line">    errorCount = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):</span><br><span class="line">        kernelEval = kernelTrans(sVs, datMat[i, :], (<span class="string">'rbf'</span>, <span class="number">1.3</span>))  <span class="comment"># 将支持向量转化为核函数</span></span><br><span class="line">        predict = kernelEval.T * multiply(labelSV, alphas[</span><br><span class="line">            svInd]) + b  <span class="comment"># 这一行的预测结果（代码来源于《统计学习方法》p133里面最后用于预测的公式）注意最后确定的分离平面只有那些支持向量决定。</span></span><br><span class="line">        <span class="keyword">if</span> sign(predict) != sign(labelArr[i]):  <span class="comment"># sign函数 -1 if x &lt; 0, 0 if x==0, 1 if x &gt; 0</span></span><br><span class="line">            errorCount += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"the training error rate is: %f"</span> % (<span class="built_in">float</span>(errorCount) / m))  <span class="comment"># 打印出错误率</span></span><br><span class="line">    dataArr_test, labelArr_test = loadDataSet(data_test)  <span class="comment"># 读取测试数据</span></span><br><span class="line">    errorCount_test = <span class="number">0</span></span><br><span class="line">    datMat_test = mat(dataArr_test)</span><br><span class="line">    labelMat = mat(labelArr_test).transpose()</span><br><span class="line">    m, n = shape(datMat_test)</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(m):  <span class="comment"># 在测试数据上检验错误率</span></span><br><span class="line">        kernelEval = kernelTrans(sVs, datMat_test[i, :], (<span class="string">'rbf'</span>, <span class="number">1.3</span>))</span><br><span class="line">        predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b</span><br><span class="line">        <span class="keyword">if</span> sign(predict) != sign(labelArr_test[i]):</span><br><span class="line">            errorCount_test += <span class="number">1</span></span><br><span class="line">    <span class="built_in">print</span>(<span class="string">"the test error rate is: %f"</span> % (<span class="built_in">float</span>(errorCount_test) / m))</span><br><span class="line"><span class="comment"># 主程序</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">main</span>():</span><br><span class="line">    filename_traindata = <span class="string">'./train_data.csv'</span></span><br><span class="line">    filename_testdata = <span class="string">'./test_data.csv'</span></span><br><span class="line">    train(filename_traindata, filename_testdata)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    main()</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br><span class="line">168</span><br><span class="line">169</span><br><span class="line">170</span><br><span class="line">171</span><br><span class="line">172</span><br><span class="line">173</span><br><span class="line">174</span><br><span class="line">175</span><br><span class="line">176</span><br><span class="line">177</span><br><span class="line">178</span><br><span class="line">179</span><br><span class="line">180</span><br><span class="line">181</span><br><span class="line">182</span><br><span class="line">183</span><br><span class="line">184</span><br><span class="line">185</span><br><span class="line">186</span><br><span class="line">187</span><br><span class="line">188</span><br><span class="line">189</span><br><span class="line">190</span><br><span class="line">191</span><br><span class="line">192</span><br><span class="line">193</span><br><span class="line">194</span><br><span class="line">195</span><br><span class="line">196</span><br><span class="line">197</span><br><span class="line">198</span><br><span class="line">199</span><br><span class="line">200</span><br><span class="line">201</span><br><span class="line">202</span><br><span class="line">203</span><br></pre></td><td class="code"><pre><span class="line">feature1,feature2,label</span><br><span class="line">-0.214824,0.662756,-1.000000</span><br><span class="line">-0.061569,-0.091875,1.000000</span><br><span class="line">0.406933,0.648055,-1.000000</span><br><span class="line">0.223650,0.130142,1.000000</span><br><span class="line">0.231317,0.766906,-1.000000</span><br><span class="line">-0.748800,-0.531637,-1.000000</span><br><span class="line">-0.557789,0.375797,-1.000000</span><br><span class="line">0.207123,-0.019463,1.000000</span><br><span class="line">0.286462,0.719470,-1.000000</span><br><span class="line">0.195300,-0.179039,1.000000</span><br><span class="line">-0.152696,-0.153030,1.000000</span><br><span class="line">0.384471,0.653336,-1.000000</span><br><span class="line">-0.117280,-0.153217,1.000000</span><br><span class="line">-0.238076,0.000583,1.000000</span><br><span class="line">-0.413576,0.145681,1.000000</span><br><span class="line">0.490767,-0.680029,-1.000000</span><br><span class="line">0.199894,-0.199381,1.000000</span><br><span class="line">-0.356048,0.537960,-1.000000</span><br><span class="line">-0.392868,-0.125261,1.000000</span><br><span class="line">0.353588,-0.070617,1.000000</span><br><span class="line">0.020984,0.925720,-1.000000</span><br><span class="line">-0.475167,-0.346247,-1.000000</span><br><span class="line">0.074952,0.042783,1.000000</span><br><span class="line">0.394164,-0.058217,1.000000</span><br><span class="line">0.663418,0.436525,-1.000000</span><br><span class="line">0.402158,0.577744,-1.000000</span><br><span class="line">-0.449349,-0.038074,1.000000</span><br><span class="line">0.619080,-0.088188,-1.000000</span><br><span class="line">0.268066,-0.071621,1.000000</span><br><span class="line">-0.015165,0.359326,1.000000</span><br><span class="line">0.539368,-0.374972,-1.000000</span><br><span class="line">-0.319153,0.629673,-1.000000</span><br><span class="line">0.694424,0.641180,-1.000000</span><br><span class="line">0.079522,0.193198,1.000000</span><br><span class="line">0.253289,-0.285861,1.000000</span><br><span class="line">-0.035558,-0.010086,1.000000</span><br><span class="line">-0.403483,0.474466,-1.000000</span><br><span class="line">-0.034312,0.995685,-1.000000</span><br><span class="line">-0.590657,0.438051,-1.000000</span><br><span class="line">-0.098871,-0.023953,1.000000</span><br><span class="line">-0.250001,0.141621,1.000000</span><br><span class="line">-0.012998,0.525985,-1.000000</span><br><span class="line">0.153738,0.491531,-1.000000</span><br><span class="line">0.388215,-0.656567,-1.000000</span><br><span class="line">0.049008,0.013499,1.000000</span><br><span class="line">0.068286,0.392741,1.000000</span><br><span class="line">0.747800,-0.066630,-1.000000</span><br><span class="line">0.004621,-0.042932,1.000000</span><br><span class="line">-0.701600,0.190983,-1.000000</span><br><span class="line">0.055413,-0.024380,1.000000</span><br><span class="line">0.035398,-0.333682,1.000000</span><br><span class="line">0.211795,0.024689,1.000000</span><br><span class="line">-0.045677,0.172907,1.000000</span><br><span class="line">0.595222,0.209570,-1.000000</span><br><span class="line">0.229465,0.250409,1.000000</span><br><span class="line">-0.089293,0.068198,1.000000</span><br><span class="line">0.384300,-0.176570,1.000000</span><br><span class="line">0.834912,-0.110321,-1.000000</span><br><span class="line">-0.307768,0.503038,-1.000000</span><br><span class="line">-0.777063,-0.348066,-1.000000</span><br><span class="line">0.017390,0.152441,1.000000</span><br><span class="line">-0.293382,-0.139778,1.000000</span><br><span class="line">-0.203272,0.286855,1.000000</span><br><span class="line">0.957812,-0.152444,-1.000000</span><br><span class="line">0.004609,-0.070617,1.000000</span><br><span class="line">-0.755431,0.096711,-1.000000</span><br><span class="line">-0.526487,0.547282,-1.000000</span><br><span class="line">-0.246873,0.833713,-1.000000</span><br><span class="line">0.185639,-0.066162,1.000000</span><br><span class="line">0.851934,0.456603,-1.000000</span><br><span class="line">-0.827912,0.117122,-1.000000</span><br><span class="line">0.233512,-0.106274,1.000000</span><br><span class="line">0.583671,-0.709033,-1.000000</span><br><span class="line">-0.487023,0.625140,-1.000000</span><br><span class="line">-0.448939,0.176725,1.000000</span><br><span class="line">0.155907,-0.166371,1.000000</span><br><span class="line">0.334204,0.381237,-1.000000</span><br><span class="line">0.081536,-0.106212,1.000000</span><br><span class="line">0.227222,0.527437,-1.000000</span><br><span class="line">0.759290,0.330720,-1.000000</span><br><span class="line">0.204177,-0.023516,1.000000</span><br><span class="line">0.577939,0.403784,-1.000000</span><br><span class="line">-0.568534,0.442948,-1.000000</span><br><span class="line">-0.011520,0.021165,1.000000</span><br><span class="line">0.875720,0.422476,-1.000000</span><br><span class="line">0.297885,-0.632874,-1.000000</span><br><span class="line">-0.015821,0.031226,1.000000</span><br><span class="line">0.541359,-0.205969,-1.000000</span><br><span class="line">-0.689946,-0.508674,-1.000000</span><br><span class="line">-0.343049,0.841653,-1.000000</span><br><span class="line">0.523902,-0.436156,-1.000000</span><br><span class="line">0.249281,-0.711840,-1.000000</span><br><span class="line">0.193449,0.574598,-1.000000</span><br><span class="line">-0.257542,-0.753885,-1.000000</span><br><span class="line">-0.021605,0.158080,1.000000</span><br><span class="line">0.601559,-0.727041,-1.000000</span><br><span class="line">-0.791603,0.095651,-1.000000</span><br><span class="line">-0.908298,-0.053376,-1.000000</span><br><span class="line">0.122020,0.850966,-1.000000</span><br><span class="line">-0.725568,-0.292022,-1.000000</span><br><span class="line"> test data</span><br><span class="line">feature1,feature2,label</span><br><span class="line">0.676771,-0.486687,-1.000000</span><br><span class="line">0.008473,0.186070,1.000000</span><br><span class="line">-0.727789,0.594062,-1.000000</span><br><span class="line">0.112367,0.287852,1.000000</span><br><span class="line">0.383633,-0.038068,1.000000</span><br><span class="line">-0.927138,-0.032633,-1.000000</span><br><span class="line">-0.842803,-0.423115,-1.000000</span><br><span class="line">-0.003677,-0.367338,1.000000</span><br><span class="line">0.443211,-0.698469,-1.000000</span><br><span class="line">-0.473835,0.005233,1.000000</span><br><span class="line">0.616741,0.590841,-1.000000</span><br><span class="line">0.557463,-0.373461,-1.000000</span><br><span class="line">-0.498535,-0.223231,-1.000000</span><br><span class="line">-0.246744,0.276413,1.000000</span><br><span class="line">-0.761980,-0.244188,-1.000000</span><br><span class="line">0.641594,-0.479861,-1.000000</span><br><span class="line">-0.659140,0.529830,-1.000000</span><br><span class="line">-0.054873,-0.238900,1.000000</span><br><span class="line">-0.089644,-0.244683,1.000000</span><br><span class="line">-0.431576,-0.481538,-1.000000</span><br><span class="line">-0.099535,0.728679,-1.000000</span><br><span class="line">-0.188428,0.156443,1.000000</span><br><span class="line">0.267051,0.318101,1.000000</span><br><span class="line">0.222114,-0.528887,-1.000000</span><br><span class="line">0.030369,0.113317,1.000000</span><br><span class="line">0.392321,0.026089,1.000000</span><br><span class="line">0.298871,-0.915427,-1.000000</span><br><span class="line">-0.034581,-0.133887,1.000000</span><br><span class="line">0.405956,0.206980,1.000000</span><br><span class="line">0.144902,-0.605762,-1.000000</span><br><span class="line">0.274362,-0.401338,1.000000</span><br><span class="line">0.397998,-0.780144,-1.000000</span><br><span class="line">0.037863,0.155137,1.000000</span><br><span class="line">-0.010363,-0.004170,1.000000</span><br><span class="line">0.506519,0.486619,-1.000000</span><br><span class="line">0.000082,-0.020625,1.000000</span><br><span class="line">0.057761,-0.155140,1.000000</span><br><span class="line">0.027748,-0.553763,-1.000000</span><br><span class="line">-0.413363,-0.746830,-1.000000</span><br><span class="line">0.081500,-0.014264,1.000000</span><br><span class="line">0.047137,-0.491271,1.000000</span><br><span class="line">-0.267459,0.024770,1.000000</span><br><span class="line">-0.148288,-0.532471,-1.000000</span><br><span class="line">-0.225559,-0.201622,1.000000</span><br><span class="line">0.772360,-0.518986,-1.000000</span><br><span class="line">-0.440670,0.688739,-1.000000</span><br><span class="line">0.329064,-0.095349,1.000000</span><br><span class="line">0.970170,-0.010671,-1.000000</span><br><span class="line">-0.689447,-0.318722,-1.000000</span><br><span class="line">-0.465493,-0.227468,-1.000000</span><br><span class="line">-0.049370,0.405711,1.000000</span><br><span class="line">-0.166117,0.274807,1.000000</span><br><span class="line">0.054483,0.012643,1.000000</span><br><span class="line">0.021389,0.076125,1.000000</span><br><span class="line">-0.104404,-0.914042,-1.000000</span><br><span class="line">0.294487,0.440886,-1.000000</span><br><span class="line">0.107915,-0.493703,-1.000000</span><br><span class="line">0.076311,0.438860,1.000000</span><br><span class="line">0.370593,-0.728737,-1.000000</span><br><span class="line">0.409890,0.306851,-1.000000</span><br><span class="line">0.285445,0.474399,-1.000000</span><br><span class="line">-0.870134,-0.161685,-1.000000</span><br><span class="line">-0.654144,-0.675129,-1.000000</span><br><span class="line">0.285278,-0.767310,-1.000000</span><br><span class="line">0.049548,-0.000907,1.000000</span><br><span class="line">0.030014,-0.093265,1.000000</span><br><span class="line">-0.128859,0.278865,1.000000</span><br><span class="line">0.307463,0.085667,1.000000</span><br><span class="line">0.023440,0.298638,1.000000</span><br><span class="line">0.053920,0.235344,1.000000</span><br><span class="line">0.059675,0.533339,-1.000000</span><br><span class="line">0.817125,0.016536,-1.000000</span><br><span class="line">-0.108771,0.477254,1.000000</span><br><span class="line">-0.118106,0.017284,1.000000</span><br><span class="line">0.288339,0.195457,1.000000</span><br><span class="line">0.567309,-0.200203,-1.000000</span><br><span class="line">-0.202446,0.409387,1.000000</span><br><span class="line">-0.330769,-0.240797,1.000000</span><br><span class="line">-0.422377,0.480683,-1.000000</span><br><span class="line">-0.295269,0.326017,1.000000</span><br><span class="line">0.261132,0.046478,1.000000</span><br><span class="line">-0.492244,-0.319998,-1.000000</span><br><span class="line">-0.384419,0.099170,1.000000</span><br><span class="line">0.101882,-0.781145,-1.000000</span><br><span class="line">0.234592,-0.383446,1.000000</span><br><span class="line">-0.020478,-0.901833,-1.000000</span><br><span class="line">0.328449,0.186633,1.000000</span><br><span class="line">-0.150059,-0.409158,1.000000</span><br><span class="line">-0.155876,-0.843413,-1.000000</span><br><span class="line">-0.098134,-0.136786,1.000000</span><br><span class="line">0.110575,-0.197205,1.000000</span><br><span class="line">0.219021,0.054347,1.000000</span><br><span class="line">0.030152,0.251682,1.000000</span><br><span class="line">0.033447,-0.122824,1.000000</span><br><span class="line">-0.686225,-0.020779,-1.000000</span><br><span class="line">-0.911211,-0.262011,-1.000000</span><br><span class="line">0.572557,0.377526,-1.000000</span><br><span class="line">-0.073647,-0.519163,-1.000000</span><br><span class="line">-0.281830,-0.797236,-1.000000</span><br><span class="line">-0.555263,0.126232,-1.000000</span><br></pre></td></tr></tbody></table></figure><p>参考链接：<br><a href="https://blog.csdn.net/csqazwsxedc/article/details/71513197">https://blog.csdn.net/csqazwsxedc/article/details/71513197</a><br><a href="https://blog.csdn.net/zouxy09/article/details/17291543">https://blog.csdn.net/zouxy09/article/details/17291543</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二进制中1的个数</title>
      <link href="/2019/09/08/%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0/"/>
      <url>/2019/09/08/%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：489798<br>本题知识点： 进制转化 补码 反码 原码</p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>输入一个整数，输出该数二进制表示中1的个数。其中负数用补码表示。</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  </code></pre><p>| </p><pre><code>class Solution {  public:       int  NumberOf1(int n) {           int count=0;           while(n!=0)           {               count++;               n = (n-1)&amp;n;           }           return count;       }  };    </code></pre><p>—|—<br><img src="/2019/09/08/%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0/images/20190908_%E4%BA%8C%E8%BF%9B%E5%88%B6%E4%B8%AD1%E7%9A%84%E4%B8%AA%E6%95%B0_jiexi.png"></p><p>运行时间：3ms<br>占用内存：356k</p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从csv中读取数据</title>
      <link href="/2019/09/08/%E4%BB%8Ecsv%E4%B8%AD%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE/"/>
      <url>/2019/09/08/%E4%BB%8Ecsv%E4%B8%AD%E8%AF%BB%E5%8F%96%E6%95%B0%E6%8D%AE/</url>
      
        <content type="html"><![CDATA[<pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  </code></pre><p>| </p><pre><code>import csv  import numpy as np    def loadDataSet(filename):  # 读取数据      with open(filename) as f:          dataMat = []          labelMat = []          f_csv = csv.reader(f) ## 用csv读取直接是个list          headers = next(f_csv)          for row in f_csv:              dataMat.append([float(row[0]), float(row[1])])              labelMat.append(float(row[2]))            f.close()          return dataMat, labelMat    def loadData(filename): # 读取数据      dataMat=[]      labelMat=[]      fr=open(filename)      next(fr) # 忽略第一行      for line in fr.readlines():          lineArr=line.strip().split(',')          dataMat.append([lineArr[0],lineArr[1]])          labelMat.append(lineArr[2])      return dataMat,labelMat # 返回数据特征和数据类别      if __name__ == '__main__':      dataMat,labelMat = loadData('test_data.csv')      print(dataMat,labelMat)    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>支持向量机</title>
      <link href="/2019/09/08/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/"/>
      <url>/2019/09/08/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/</url>
      
        <content type="html"><![CDATA[<p>SVM的本质：寻找最大的间隔<br>支持向量：距离超平面最近的那些点<br><strong>SMO算法的原理：</strong> 每次循环中选择两个alpha进行优化处理。一旦找到一对合适的alpha，那么就增大其中一个同时减小另一个。合适：条件一，两个alpha要在间隔边界之外；条件二，这两个alpha还没有进行过区间化处理或不在边界上</p><p><img src="/2019/09/08/%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA/images/20190908_%E6%94%AF%E6%8C%81%E5%90%91%E9%87%8F%E6%9C%BA_svm.png"></p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160  161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176  177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192  193  194  195  196  197  198  199  200  201  202  203  204  205  206  207  208  209  210  211  212  213  214  </code></pre><p>| </p><pre><code>from numpy import *  import csv      def loadDataSet(filename):  # 读取数据      with open(filename) as f:          dataMat = []          labelMat = []          f_csv = csv.reader(f)  ## 用csv读取直接是个list          headers = next(f_csv)          for row in f_csv:              dataMat.append([float(row[0]), float(row[1])])              labelMat.append(float(row[2]))            f.close()          return dataMat, labelMat      def selectJrand(i, m):  # 在0-m中随机选择一个不是i的整数      j = i      while (j == i):          j = int(random.uniform(0, m))      return j      def clipAlpha(aj, H, L):  # 保证a在L和H范围内（L &lt;= a &lt;= H）      if aj &gt; H:          aj = H      if L &gt; aj:          aj = L      return aj      def kernelTrans(X, A, kTup):  # 核函数，输入参数,X:支持向量的特征树；A：某一行特征数据；kTup：('lin',k1)核函数的类型和参数      m, n = shape(X)      K = mat(zeros((m, 1)))      print("A shape : ",A.shape)      print("X shape : ",X.shape)      if kTup[0] == 'lin':  # 线性函数          K = X * A.T      elif kTup[0] == 'rbf':  # 径向基函数(radial bias function)          for j in range(m):              deltaRow = X[j, :] - A              K[j] = deltaRow * deltaRow.T          K = exp(K / (-1 * kTup[1] ** 2))  # 返回生成的结果      else:          raise NameError('Houston We Have a Problem -- That Kernel is not recognized')      return K      # 定义类，方便存储数据  class optStruct:      def __init__(self, dataMatIn, classLabels, C, toler, kTup):  # 存储各类参数          self.X = dataMatIn  # 数据特征          self.labelMat = classLabels  # 数据类别          self.C = C  # 软间隔参数C，参数越大，非线性拟合能力越强          self.tol = toler  # 停止阀值          self.m = shape(dataMatIn)[0]  # 数据 b行数          self.alphas = mat(zeros((self.m, 1)))          self.b = 0  # 初始设为0          self.eCache = mat(zeros((self.m, 2)))  # 缓存          self.K = mat(zeros((self.m, self.m)))  # 核函数的计算结果          for i in range(self.m):              self.K[:, i] = kernelTrans(self.X, self.X[i, :], kTup)      def calcEk(oS, k):  # 计算Ek（参考《统计学习方法》p127公式7.105）      fXk = float(multiply(oS.alphas, oS.labelMat).T * oS.K[:, k] + oS.b)      Ek = fXk - float(oS.labelMat[k])      return Ek      # 随机选取aj，并返回其E值  def selectJ(i, oS, Ei):      maxK = -1      maxDeltaE = 0      Ej = 0      oS.eCache[i] = [1, Ei]      validEcacheList = nonzero(oS.eCache[:, 0].A)[0]  # 返回矩阵中的非零位置的行数      if (len(validEcacheList)) &gt; 1:          for k in validEcacheList:              if k == i:                  continue              Ek = calcEk(oS, k)              deltaE = abs(Ei - Ek)              if (deltaE &gt; maxDeltaE):  # 返回步长最大的aj                  maxK = k                  maxDeltaE = deltaE                  Ej = Ek          return maxK, Ej      else:          j = selectJrand(i, oS.m)          Ej = calcEk(oS, j)      return j, Ej      def updateEk(oS, k):  # 更新os数据      Ek = calcEk(oS, k)      oS.eCache[k] = [1, Ek]      # 首先检验ai是否满足KKT条件，如果不满足，随机选择aj进行优化，更新ai,aj,b值  def innerL(i, oS):  # 输入参数i和所有参数数据      Ei = calcEk(oS, i)  # 计算E值      if ((oS.labelMat[i] * Ei &lt; -oS.tol) and (oS.alphas[i] &lt; oS.C)) or (              (oS.labelMat[i] * Ei &gt; oS.tol) and (oS.alphas[i] &gt; 0)):  # 检验这行数据是否符合KKT条件 参考《统计学习方法》p128公式7.111-113          j, Ej = selectJ(i, oS, Ei)  # 随机选取aj，并返回其E值          alphaIold = oS.alphas[i].copy()          alphaJold = oS.alphas[j].copy()          if (oS.labelMat[i] != oS.labelMat[j]):  # 以下代码的公式参考《统计学习方法》p126              L = max(0, oS.alphas[j] - oS.alphas[i])              H = min(oS.C, oS.C + oS.alphas[j] - oS.alphas[i])          else:              L = max(0, oS.alphas[j] + oS.alphas[i] - oS.C)              H = min(oS.C, oS.alphas[j] + oS.alphas[i])          if L == H:              print("L==H")              return 0          eta = 2.0 * oS.K[i, j] - oS.K[i, i] - oS.K[j, j]  # 参考《统计学习方法》p127公式7.107          if eta &gt;= 0:              print("eta&gt;=0")              return 0          oS.alphas[j] -= oS.labelMat[j] * (Ei - Ej) / eta  # 参考《统计学习方法》p127公式7.106          oS.alphas[j] = clipAlpha(oS.alphas[j], H, L)  # 参考《统计学习方法》p127公式7.108          updateEk(oS, j)          if (abs(oS.alphas[j] - alphaJold) &lt; oS.tol):  # alpha变化大小阀值（自己设定）              print("j not moving enough")              return 0          oS.alphas[i] += oS.labelMat[j] * oS.labelMat[i] * (alphaJold - oS.alphas[j])  # 参考《统计学习方法》p127公式7.109          updateEk(oS, i)  # 更新数据          # 以下求解b的过程，参考《统计学习方法》p129公式7.114-7.116          b1 = oS.b - Ei - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, i] - oS.labelMat[j] * (                      oS.alphas[j] - alphaJold) * oS.K[i, j]          b2 = oS.b - Ej - oS.labelMat[i] * (oS.alphas[i] - alphaIold) * oS.K[i, j] - oS.labelMat[j] * (                      oS.alphas[j] - alphaJold) * oS.K[j, j]          if (0 &lt; oS.alphas[i] &lt; oS.C):              oS.b = b1          elif (0 &lt; oS.alphas[j] &lt; oS.C):              oS.b = b2          else:              oS.b = (b1 + b2) / 2.0          return 1      else:          return 0      # SMO函数，用于快速求解出alpha  def smoP(dataMatIn, classLabels, C, toler, maxIter, kTup=('lin', 0)):  # 输入参数：数据特征，数据类别，参数C，阀值toler，最大迭代次数，核函数（默认线性核）      oS = optStruct(mat(dataMatIn), mat(classLabels).transpose(), C, toler, kTup) # dataArr, labelArr, 200, 0.0001, 10000, ('rbf', 1.3))  # 通过SMO算法得到b和alpha      iter = 0      entireSet = True      alphaPairsChanged = 0      while (iter &lt; maxIter) and ((alphaPairsChanged &gt; 0) or (entireSet)):          alphaPairsChanged = 0          if entireSet:              for i in range(oS.m):  # 遍历所有数据                  alphaPairsChanged += innerL(i, oS)                  print("fullSet, iter: %d i:%d, pairs changed %d" % (                  iter, i, alphaPairsChanged))  # 显示第多少次迭代，那行特征数据使alpha发生了改变，这次改变了多少次alpha              iter += 1          else:              nonBoundIs = nonzero((oS.alphas.A &gt; 0) * (oS.alphas.A &lt; C))[0]              for i in nonBoundIs:  # 遍历非边界的数据                  alphaPairsChanged += innerL(i, oS)                  print("non-bound, iter: %d i:%d, pairs changed %d" % (iter, i, alphaPairsChanged))              iter += 1          if entireSet:              entireSet = False          elif (alphaPairsChanged == 0):              entireSet = True          print("iteration number: %d" % iter)      return oS.b, oS.alphas      def train(data_train, data_test):      dataArr, labelArr = loadDataSet(data_train)  # 读取训练数据      b, alphas = smoP(dataArr, labelArr, 200, 0.0001, 10000, ('rbf', 1.3))  # 通过SMO算法得到b和alpha      datMat = mat(dataArr)      labelMat = mat(labelArr).transpose()      svInd = nonzero(alphas)[0]  # 选取不为0数据的行数（也就是支持向量）      sVs = datMat[svInd]  # 支持向量的特征数据      labelSV = labelMat[svInd]  # 支持向量的类别（1或-1）      print("there are %d Support Vectors" % shape(sVs)[0])  # 打印出共有多少的支持向量      m, n = shape(datMat)  # 训练数据的行列数      errorCount = 0      for i in range(m):          kernelEval = kernelTrans(sVs, datMat[i, :], ('rbf', 1.3))  # 将支持向量转化为核函数          predict = kernelEval.T * multiply(labelSV, alphas[              svInd]) + b  # 这一行的预测结果（代码来源于《统计学习方法》p133里面最后用于预测的公式）注意最后确定的分离平面只有那些支持向量决定。          if sign(predict) != sign(labelArr[i]):  # sign函数 -1 if x &lt; 0, 0 if x==0, 1 if x &gt; 0              errorCount += 1      print("the training error rate is: %f" % (float(errorCount) / m))  # 打印出错误率      dataArr_test, labelArr_test = loadDataSet(data_test)  # 读取测试数据      errorCount_test = 0      datMat_test = mat(dataArr_test)      labelMat = mat(labelArr_test).transpose()      m, n = shape(datMat_test)      for i in range(m):  # 在测试数据上检验错误率          kernelEval = kernelTrans(sVs, datMat_test[i, :], ('rbf', 1.3))          predict = kernelEval.T * multiply(labelSV, alphas[svInd]) + b          if sign(predict) != sign(labelArr_test[i]):              errorCount_test += 1      print("the test error rate is: %f" % (float(errorCount_test) / m))      # 主程序  def main():      filename_traindata = './train_data.csv'      filename_testdata = './test_data.csv'      train(filename_traindata, filename_testdata)      if __name__ == '__main__':      main()    </code></pre><p>—|—  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160  161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176  177  178  179  180  181  182  183  184  185  186  187  188  189  190  191  192  193  194  195  196  197  198  199  200  201  202  203  </code></pre><p>| </p><pre><code>feature1,feature2,label  -0.214824,0.662756,-1.000000  -0.061569,-0.091875,1.000000  0.406933,0.648055,-1.000000  0.223650,0.130142,1.000000  0.231317,0.766906,-1.000000  -0.748800,-0.531637,-1.000000  -0.557789,0.375797,-1.000000  0.207123,-0.019463,1.000000  0.286462,0.719470,-1.000000  0.195300,-0.179039,1.000000  -0.152696,-0.153030,1.000000  0.384471,0.653336,-1.000000  -0.117280,-0.153217,1.000000  -0.238076,0.000583,1.000000  -0.413576,0.145681,1.000000  0.490767,-0.680029,-1.000000  0.199894,-0.199381,1.000000  -0.356048,0.537960,-1.000000  -0.392868,-0.125261,1.000000  0.353588,-0.070617,1.000000  0.020984,0.925720,-1.000000  -0.475167,-0.346247,-1.000000  0.074952,0.042783,1.000000  0.394164,-0.058217,1.000000  0.663418,0.436525,-1.000000  0.402158,0.577744,-1.000000  -0.449349,-0.038074,1.000000  0.619080,-0.088188,-1.000000  0.268066,-0.071621,1.000000  -0.015165,0.359326,1.000000  0.539368,-0.374972,-1.000000  -0.319153,0.629673,-1.000000  0.694424,0.641180,-1.000000  0.079522,0.193198,1.000000  0.253289,-0.285861,1.000000  -0.035558,-0.010086,1.000000  -0.403483,0.474466,-1.000000  -0.034312,0.995685,-1.000000  -0.590657,0.438051,-1.000000  -0.098871,-0.023953,1.000000  -0.250001,0.141621,1.000000  -0.012998,0.525985,-1.000000  0.153738,0.491531,-1.000000  0.388215,-0.656567,-1.000000  0.049008,0.013499,1.000000  0.068286,0.392741,1.000000  0.747800,-0.066630,-1.000000  0.004621,-0.042932,1.000000  -0.701600,0.190983,-1.000000  0.055413,-0.024380,1.000000  0.035398,-0.333682,1.000000  0.211795,0.024689,1.000000  -0.045677,0.172907,1.000000  0.595222,0.209570,-1.000000  0.229465,0.250409,1.000000  -0.089293,0.068198,1.000000  0.384300,-0.176570,1.000000  0.834912,-0.110321,-1.000000  -0.307768,0.503038,-1.000000  -0.777063,-0.348066,-1.000000  0.017390,0.152441,1.000000  -0.293382,-0.139778,1.000000  -0.203272,0.286855,1.000000  0.957812,-0.152444,-1.000000  0.004609,-0.070617,1.000000  -0.755431,0.096711,-1.000000  -0.526487,0.547282,-1.000000  -0.246873,0.833713,-1.000000  0.185639,-0.066162,1.000000  0.851934,0.456603,-1.000000  -0.827912,0.117122,-1.000000  0.233512,-0.106274,1.000000  0.583671,-0.709033,-1.000000  -0.487023,0.625140,-1.000000  -0.448939,0.176725,1.000000  0.155907,-0.166371,1.000000  0.334204,0.381237,-1.000000  0.081536,-0.106212,1.000000  0.227222,0.527437,-1.000000  0.759290,0.330720,-1.000000  0.204177,-0.023516,1.000000  0.577939,0.403784,-1.000000  -0.568534,0.442948,-1.000000  -0.011520,0.021165,1.000000  0.875720,0.422476,-1.000000  0.297885,-0.632874,-1.000000  -0.015821,0.031226,1.000000  0.541359,-0.205969,-1.000000  -0.689946,-0.508674,-1.000000  -0.343049,0.841653,-1.000000  0.523902,-0.436156,-1.000000  0.249281,-0.711840,-1.000000  0.193449,0.574598,-1.000000  -0.257542,-0.753885,-1.000000  -0.021605,0.158080,1.000000  0.601559,-0.727041,-1.000000  -0.791603,0.095651,-1.000000  -0.908298,-0.053376,-1.000000  0.122020,0.850966,-1.000000  -0.725568,-0.292022,-1.000000   test data  feature1,feature2,label  0.676771,-0.486687,-1.000000  0.008473,0.186070,1.000000  -0.727789,0.594062,-1.000000  0.112367,0.287852,1.000000  0.383633,-0.038068,1.000000  -0.927138,-0.032633,-1.000000  -0.842803,-0.423115,-1.000000  -0.003677,-0.367338,1.000000  0.443211,-0.698469,-1.000000  -0.473835,0.005233,1.000000  0.616741,0.590841,-1.000000  0.557463,-0.373461,-1.000000  -0.498535,-0.223231,-1.000000  -0.246744,0.276413,1.000000  -0.761980,-0.244188,-1.000000  0.641594,-0.479861,-1.000000  -0.659140,0.529830,-1.000000  -0.054873,-0.238900,1.000000  -0.089644,-0.244683,1.000000  -0.431576,-0.481538,-1.000000  -0.099535,0.728679,-1.000000  -0.188428,0.156443,1.000000  0.267051,0.318101,1.000000  0.222114,-0.528887,-1.000000  0.030369,0.113317,1.000000  0.392321,0.026089,1.000000  0.298871,-0.915427,-1.000000  -0.034581,-0.133887,1.000000  0.405956,0.206980,1.000000  0.144902,-0.605762,-1.000000  0.274362,-0.401338,1.000000  0.397998,-0.780144,-1.000000  0.037863,0.155137,1.000000  -0.010363,-0.004170,1.000000  0.506519,0.486619,-1.000000  0.000082,-0.020625,1.000000  0.057761,-0.155140,1.000000  0.027748,-0.553763,-1.000000  -0.413363,-0.746830,-1.000000  0.081500,-0.014264,1.000000  0.047137,-0.491271,1.000000  -0.267459,0.024770,1.000000  -0.148288,-0.532471,-1.000000  -0.225559,-0.201622,1.000000  0.772360,-0.518986,-1.000000  -0.440670,0.688739,-1.000000  0.329064,-0.095349,1.000000  0.970170,-0.010671,-1.000000  -0.689447,-0.318722,-1.000000  -0.465493,-0.227468,-1.000000  -0.049370,0.405711,1.000000  -0.166117,0.274807,1.000000  0.054483,0.012643,1.000000  0.021389,0.076125,1.000000  -0.104404,-0.914042,-1.000000  0.294487,0.440886,-1.000000  0.107915,-0.493703,-1.000000  0.076311,0.438860,1.000000  0.370593,-0.728737,-1.000000  0.409890,0.306851,-1.000000  0.285445,0.474399,-1.000000  -0.870134,-0.161685,-1.000000  -0.654144,-0.675129,-1.000000  0.285278,-0.767310,-1.000000  0.049548,-0.000907,1.000000  0.030014,-0.093265,1.000000  -0.128859,0.278865,1.000000  0.307463,0.085667,1.000000  0.023440,0.298638,1.000000  0.053920,0.235344,1.000000  0.059675,0.533339,-1.000000  0.817125,0.016536,-1.000000  -0.108771,0.477254,1.000000  -0.118106,0.017284,1.000000  0.288339,0.195457,1.000000  0.567309,-0.200203,-1.000000  -0.202446,0.409387,1.000000  -0.330769,-0.240797,1.000000  -0.422377,0.480683,-1.000000  -0.295269,0.326017,1.000000  0.261132,0.046478,1.000000  -0.492244,-0.319998,-1.000000  -0.384419,0.099170,1.000000  0.101882,-0.781145,-1.000000  0.234592,-0.383446,1.000000  -0.020478,-0.901833,-1.000000  0.328449,0.186633,1.000000  -0.150059,-0.409158,1.000000  -0.155876,-0.843413,-1.000000  -0.098134,-0.136786,1.000000  0.110575,-0.197205,1.000000  0.219021,0.054347,1.000000  0.030152,0.251682,1.000000  0.033447,-0.122824,1.000000  -0.686225,-0.020779,-1.000000  -0.911211,-0.262011,-1.000000  0.572557,0.377526,-1.000000  -0.073647,-0.519163,-1.000000  -0.281830,-0.797236,-1.000000  -0.555263,0.126232,-1.000000    </code></pre><p>—|—  </p><p>参考链接：<br><a href="https://blog.csdn.net/csqazwsxedc/article/details/71513197">https://blog.csdn.net/csqazwsxedc/article/details/71513197</a><br><a href="https://blog.csdn.net/zouxy09/article/details/17291543">https://blog.csdn.net/zouxy09/article/details/17291543</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PCA降维</title>
      <link href="/2019/09/07/20190907-PCA%E9%99%8D%E7%BB%B4/"/>
      <url>/2019/09/07/20190907-PCA%E9%99%8D%E7%BB%B4/</url>
      
        <content type="html"><![CDATA[<p>PCA（principal Component Analysis），主成分分析法。顾名思义，就是提取出数据中主要的成分，是一种数据压缩方法，常用于去除噪声、数据预处理，也是机器学习中常见的降维方法。<br><img src="/images/20190907_PCA%E9%99%8D%E7%BB%B4_myplot.png"></p><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><ul><li>对所有样本进行中心化处理<br>即将每个元素减去它的平均值，这样可以增加基向量的正交性。</li><li>计算协方差矩阵及特征值、特征向量</li><li>对特征值进行排序</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># coding:utf-8</span></span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pylab <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">read_file</span>(<span class="params">filename</span>):</span><br><span class="line">f = <span class="built_in">open</span>(filename, <span class="string">'r'</span>)</span><br><span class="line">d = f.readlines()</span><br><span class="line">f.close()</span><br><span class="line"><span class="keyword">return</span> d</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">PCA</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">使用PCA对高维数据进行降维处理</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">data = read_file(<span class="string">'data.txt'</span>)</span><br><span class="line">self.n = <span class="built_in">len</span>(data)  <span class="comment"># 数据的个数</span></span><br><span class="line">self.dim = <span class="number">4</span>  <span class="comment"># 原始数据的维度</span></span><br><span class="line">self.x = np.zeros((self.n, self.dim), dtype=<span class="string">'float64'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n):</span><br><span class="line">data_ = data[i].split(<span class="string">','</span>)</span><br><span class="line">self.x[i][<span class="number">0</span>] = data_[<span class="number">0</span>]</span><br><span class="line">self.x[i][<span class="number">1</span>] = data_[<span class="number">1</span>]</span><br><span class="line">self.x[i][<span class="number">2</span>] = data_[<span class="number">2</span>]</span><br><span class="line">self.x[i][<span class="number">3</span>] = data_[<span class="number">3</span>]</span><br><span class="line">self.k = <span class="number">2</span>  <span class="comment"># 降到二维</span></span><br><span class="line">self.mean_x = np.zeros((self.n, self.dim), dtype=<span class="string">'float64'</span>)  <span class="comment"># 原始数据减去均值以后的x</span></span><br><span class="line">self.mean = np.zeros((self.dim, <span class="number">1</span>), dtype=<span class="string">'float64'</span>)  <span class="comment"># x的均值</span></span><br><span class="line">self.cov = np.zeros((self.dim, self.dim), dtype=<span class="string">'float64'</span>)  <span class="comment"># 协方差矩阵</span></span><br><span class="line">self.pre_x = np.zeros((self.n, self.dim), dtype=<span class="string">'float64'</span>)  <span class="comment"># 预处理之后的数据</span></span><br><span class="line">self.eig_val = np.zeros((<span class="number">1</span>, self.dim), dtype=<span class="string">'float64'</span>)  <span class="comment"># 特征值</span></span><br><span class="line">self.eig_vec = np.zeros((self.dim, self.dim), dtype=<span class="string">'float64'</span>)  <span class="comment"># 特征向量</span></span><br><span class="line">self.final_x = np.zeros((self.n, self.k), dtype=<span class="string">'float64'</span>)  <span class="comment"># 投影后的数据</span></span><br><span class="line">self.pretreatment()</span><br><span class="line">self.pca()</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pretreatment</span>(<span class="params">self</span>):</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">预处理</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 求均值</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.dim):</span><br><span class="line">self.mean[i] = np.mean(self.x[:, i])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n):</span><br><span class="line">self.mean_x[i] = self.x[i] - self.mean.T</span><br><span class="line"><span class="comment"># 求协方差</span></span><br><span class="line"><span class="comment"># self.cov = np.cov(self.mean_x, rowvar=0)</span></span><br><span class="line"><span class="comment"># mean_x已经是x减去均值了，所以直接相乘就是方差</span></span><br><span class="line">self.cov = self.mean_x.T.dot(self.mean_x) / self.n</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.dim):</span><br><span class="line">self.pre_x[:, i] = self.mean_x[:, i] / np.sqrt(self.cov[i][i])   <span class="comment"># x的每个维度都处理一次</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">pca</span>(<span class="params">self</span>):</span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="string">pca的实现</span></span><br><span class="line"><span class="string">"""</span></span><br><span class="line"><span class="comment"># 需要注意的是，在这里需要对预处理之后的数据重新计算协方差</span></span><br><span class="line"><span class="comment"># 计算均值</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.dim):</span><br><span class="line">self.mean[i] = np.mean(self.pre_x[:, i])</span><br><span class="line"><span class="comment"># 计算协方差</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.n):</span><br><span class="line">self.mean_x[i] = self.pre_x[i] - self.mean.T</span><br><span class="line">self.cov = (self.mean_x.T.dot(self.mean_x)) / self.n</span><br><span class="line"><span class="comment"># 求特征值</span></span><br><span class="line">self.eig_val, self.eig_vec = np.linalg.eig(np.mat(self.cov))</span><br><span class="line"><span class="comment"># eig_vec的列向量是特征向量，所以用eig_v存储特征向量，以便后面排序</span></span><br><span class="line">eig_v = np.zeros((self.dim, self.dim), dtype=<span class="string">'float64'</span>)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(self.dim):</span><br><span class="line">eig_v[i] = self.eig_vec[:, i].T</span><br><span class="line"><span class="comment"># 排序</span></span><br><span class="line">eig_list = <span class="built_in">zip</span>(self.eig_val, eig_v)</span><br><span class="line"><span class="built_in">sorted</span>(eig_list,key=<span class="keyword">lambda</span> g: g[<span class="number">0</span>], reverse=<span class="literal">True</span>)</span><br><span class="line"><span class="comment"># 处理排序的结果</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(<span class="built_in">list</span>(eig_list))):</span><br><span class="line">self.eig_val[i] = eig_list[i][<span class="number">0</span>]</span><br><span class="line">self.eig_vec[i] = eig_list[i][<span class="number">1</span>]</span><br><span class="line"><span class="comment"># 最大的k个特征向量，降维</span></span><br><span class="line">f_vec = self.eig_vec[<span class="number">0</span>:self.k, :]</span><br><span class="line">self.final_x = self.pre_x * f_vec.T</span><br><span class="line"><span class="comment"># 显示降维后的数据点</span></span><br><span class="line">plt.plot(self.final_x[<span class="number">0</span>:<span class="number">50</span>, <span class="number">0</span>], self.final_x[<span class="number">0</span>:<span class="number">50</span>, <span class="number">1</span>], <span class="string">'bo'</span>)</span><br><span class="line">plt.plot(self.final_x[<span class="number">50</span>:<span class="number">100</span>, <span class="number">0</span>], self.final_x[<span class="number">50</span>:<span class="number">100</span>, <span class="number">1</span>], <span class="string">'go'</span>)</span><br><span class="line">plt.plot(self.final_x[<span class="number">100</span>:<span class="number">150</span>, <span class="number">0</span>], self.final_x[<span class="number">100</span>:<span class="number">150</span>, <span class="number">1</span>], <span class="string">'ro'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">a = PCA()</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br></pre></td><td class="code"><pre><span class="line">5.1,3.5,1.4,0.2,Iris-setosa</span><br><span class="line">4.9,3.0,1.4,0.2,Iris-setosa</span><br><span class="line">4.7,3.2,1.3,0.2,Iris-setosa</span><br><span class="line">4.6,3.1,1.5,0.2,Iris-setosa</span><br><span class="line">5.0,3.6,1.4,0.2,Iris-setosa</span><br><span class="line">5.4,3.9,1.7,0.4,Iris-setosa</span><br><span class="line">4.6,3.4,1.4,0.3,Iris-setosa</span><br><span class="line">5.0,3.4,1.5,0.2,Iris-setosa</span><br><span class="line">4.4,2.9,1.4,0.2,Iris-setosa</span><br><span class="line">4.9,3.1,1.5,0.1,Iris-setosa</span><br><span class="line">5.4,3.7,1.5,0.2,Iris-setosa</span><br><span class="line">4.8,3.4,1.6,0.2,Iris-setosa</span><br><span class="line">4.8,3.0,1.4,0.1,Iris-setosa</span><br><span class="line">4.3,3.0,1.1,0.1,Iris-setosa</span><br><span class="line">5.8,4.0,1.2,0.2,Iris-setosa</span><br><span class="line">5.7,4.4,1.5,0.4,Iris-setosa</span><br><span class="line">5.4,3.9,1.3,0.4,Iris-setosa</span><br><span class="line">5.1,3.5,1.4,0.3,Iris-setosa</span><br><span class="line">5.7,3.8,1.7,0.3,Iris-setosa</span><br><span class="line">5.1,3.8,1.5,0.3,Iris-setosa</span><br><span class="line">5.4,3.4,1.7,0.2,Iris-setosa</span><br><span class="line">5.1,3.7,1.5,0.4,Iris-setosa</span><br><span class="line">4.6,3.6,1.0,0.2,Iris-setosa</span><br><span class="line">5.1,3.3,1.7,0.5,Iris-setosa</span><br><span class="line">4.8,3.4,1.9,0.2,Iris-setosa</span><br><span class="line">5.0,3.0,1.6,0.2,Iris-setosa</span><br><span class="line">5.0,3.4,1.6,0.4,Iris-setosa</span><br><span class="line">5.2,3.5,1.5,0.2,Iris-setosa</span><br><span class="line">5.2,3.4,1.4,0.2,Iris-setosa</span><br><span class="line">4.7,3.2,1.6,0.2,Iris-setosa</span><br><span class="line">4.8,3.1,1.6,0.2,Iris-setosa</span><br><span class="line">5.4,3.4,1.5,0.4,Iris-setosa</span><br><span class="line">5.2,4.1,1.5,0.1,Iris-setosa</span><br><span class="line">5.5,4.2,1.4,0.2,Iris-setosa</span><br><span class="line">4.9,3.1,1.5,0.1,Iris-setosa</span><br><span class="line">5.0,3.2,1.2,0.2,Iris-setosa</span><br><span class="line">5.5,3.5,1.3,0.2,Iris-setosa</span><br><span class="line">4.9,3.1,1.5,0.1,Iris-setosa</span><br><span class="line">4.4,3.0,1.3,0.2,Iris-setosa</span><br><span class="line">5.1,3.4,1.5,0.2,Iris-setosa</span><br><span class="line">5.0,3.5,1.3,0.3,Iris-setosa</span><br><span class="line">4.5,2.3,1.3,0.3,Iris-setosa</span><br><span class="line">4.4,3.2,1.3,0.2,Iris-setosa</span><br><span class="line">5.0,3.5,1.6,0.6,Iris-setosa</span><br><span class="line">5.1,3.8,1.9,0.4,Iris-setosa</span><br><span class="line">4.8,3.0,1.4,0.3,Iris-setosa</span><br><span class="line">5.1,3.8,1.6,0.2,Iris-setosa</span><br><span class="line">4.6,3.2,1.4,0.2,Iris-setosa</span><br><span class="line">5.3,3.7,1.5,0.2,Iris-setosa</span><br><span class="line">5.0,3.3,1.4,0.2,Iris-setosa</span><br><span class="line">7.0,3.2,4.7,1.4,Iris-versicolor</span><br><span class="line">6.4,3.2,4.5,1.5,Iris-versicolor</span><br><span class="line">6.9,3.1,4.9,1.5,Iris-versicolor</span><br><span class="line">5.5,2.3,4.0,1.3,Iris-versicolor</span><br><span class="line">6.5,2.8,4.6,1.5,Iris-versicolor</span><br><span class="line">5.7,2.8,4.5,1.3,Iris-versicolor</span><br><span class="line">6.3,3.3,4.7,1.6,Iris-versicolor</span><br><span class="line">4.9,2.4,3.3,1.0,Iris-versicolor</span><br><span class="line">6.6,2.9,4.6,1.3,Iris-versicolor</span><br><span class="line">5.2,2.7,3.9,1.4,Iris-versicolor</span><br><span class="line">5.0,2.0,3.5,1.0,Iris-versicolor</span><br><span class="line">5.9,3.0,4.2,1.5,Iris-versicolor</span><br><span class="line">6.0,2.2,4.0,1.0,Iris-versicolor</span><br><span class="line">6.1,2.9,4.7,1.4,Iris-versicolor</span><br><span class="line">5.6,2.9,3.6,1.3,Iris-versicolor</span><br><span class="line">6.7,3.1,4.4,1.4,Iris-versicolor</span><br><span class="line">5.6,3.0,4.5,1.5,Iris-versicolor</span><br><span class="line">5.8,2.7,4.1,1.0,Iris-versicolor</span><br><span class="line">6.2,2.2,4.5,1.5,Iris-versicolor</span><br><span class="line">5.6,2.5,3.9,1.1,Iris-versicolor</span><br><span class="line">5.9,3.2,4.8,1.8,Iris-versicolor</span><br><span class="line">6.1,2.8,4.0,1.3,Iris-versicolor</span><br><span class="line">6.3,2.5,4.9,1.5,Iris-versicolor</span><br><span class="line">6.1,2.8,4.7,1.2,Iris-versicolor</span><br><span class="line">6.4,2.9,4.3,1.3,Iris-versicolor</span><br><span class="line">6.6,3.0,4.4,1.4,Iris-versicolor</span><br><span class="line">6.8,2.8,4.8,1.4,Iris-versicolor</span><br><span class="line">6.7,3.0,5.0,1.7,Iris-versicolor</span><br><span class="line">6.0,2.9,4.5,1.5,Iris-versicolor</span><br><span class="line">5.7,2.6,3.5,1.0,Iris-versicolor</span><br><span class="line">5.5,2.4,3.8,1.1,Iris-versicolor</span><br><span class="line">5.5,2.4,3.7,1.0,Iris-versicolor</span><br><span class="line">5.8,2.7,3.9,1.2,Iris-versicolor</span><br><span class="line">6.0,2.7,5.1,1.6,Iris-versicolor</span><br><span class="line">5.4,3.0,4.5,1.5,Iris-versicolor</span><br><span class="line">6.0,3.4,4.5,1.6,Iris-versicolor</span><br><span class="line">6.7,3.1,4.7,1.5,Iris-versicolor</span><br><span class="line">6.3,2.3,4.4,1.3,Iris-versicolor</span><br><span class="line">5.6,3.0,4.1,1.3,Iris-versicolor</span><br><span class="line">5.5,2.5,4.0,1.3,Iris-versicolor</span><br><span class="line">5.5,2.6,4.4,1.2,Iris-versicolor</span><br><span class="line">6.1,3.0,4.6,1.4,Iris-versicolor</span><br><span class="line">5.8,2.6,4.0,1.2,Iris-versicolor</span><br><span class="line">5.0,2.3,3.3,1.0,Iris-versicolor</span><br><span class="line">5.6,2.7,4.2,1.3,Iris-versicolor</span><br><span class="line">5.7,3.0,4.2,1.2,Iris-versicolor</span><br><span class="line">5.7,2.9,4.2,1.3,Iris-versicolor</span><br><span class="line">6.2,2.9,4.3,1.3,Iris-versicolor</span><br><span class="line">5.1,2.5,3.0,1.1,Iris-versicolor</span><br><span class="line">5.7,2.8,4.1,1.3,Iris-versicolor</span><br><span class="line">6.3,3.3,6.0,2.5,Iris-virginica</span><br><span class="line">5.8,2.7,5.1,1.9,Iris-virginica</span><br><span class="line">7.1,3.0,5.9,2.1,Iris-virginica</span><br><span class="line">6.3,2.9,5.6,1.8,Iris-virginica</span><br><span class="line">6.5,3.0,5.8,2.2,Iris-virginica</span><br><span class="line">7.6,3.0,6.6,2.1,Iris-virginica</span><br><span class="line">4.9,2.5,4.5,1.7,Iris-virginica</span><br><span class="line">7.3,2.9,6.3,1.8,Iris-virginica</span><br><span class="line">6.7,2.5,5.8,1.8,Iris-virginica</span><br><span class="line">7.2,3.6,6.1,2.5,Iris-virginica</span><br><span class="line">6.5,3.2,5.1,2.0,Iris-virginica</span><br><span class="line">6.4,2.7,5.3,1.9,Iris-virginica</span><br><span class="line">6.8,3.0,5.5,2.1,Iris-virginica</span><br><span class="line">5.7,2.5,5.0,2.0,Iris-virginica</span><br><span class="line">5.8,2.8,5.1,2.4,Iris-virginica</span><br><span class="line">6.4,3.2,5.3,2.3,Iris-virginica</span><br><span class="line">6.5,3.0,5.5,1.8,Iris-virginica</span><br><span class="line">7.7,3.8,6.7,2.2,Iris-virginica</span><br><span class="line">7.7,2.6,6.9,2.3,Iris-virginica</span><br><span class="line">6.0,2.2,5.0,1.5,Iris-virginica</span><br><span class="line">6.9,3.2,5.7,2.3,Iris-virginica</span><br><span class="line">5.6,2.8,4.9,2.0,Iris-virginica</span><br><span class="line">7.7,2.8,6.7,2.0,Iris-virginica</span><br><span class="line">6.3,2.7,4.9,1.8,Iris-virginica</span><br><span class="line">6.7,3.3,5.7,2.1,Iris-virginica</span><br><span class="line">7.2,3.2,6.0,1.8,Iris-virginica</span><br><span class="line">6.2,2.8,4.8,1.8,Iris-virginica</span><br><span class="line">6.1,3.0,4.9,1.8,Iris-virginica</span><br><span class="line">6.4,2.8,5.6,2.1,Iris-virginica</span><br><span class="line">7.2,3.0,5.8,1.6,Iris-virginica</span><br><span class="line">7.4,2.8,6.1,1.9,Iris-virginica</span><br><span class="line">7.9,3.8,6.4,2.0,Iris-virginica</span><br><span class="line">6.4,2.8,5.6,2.2,Iris-virginica</span><br><span class="line">6.3,2.8,5.1,1.5,Iris-virginica</span><br><span class="line">6.1,2.6,5.6,1.4,Iris-virginica</span><br><span class="line">7.7,3.0,6.1,2.3,Iris-virginica</span><br><span class="line">6.3,3.4,5.6,2.4,Iris-virginica</span><br><span class="line">6.4,3.1,5.5,1.8,Iris-virginica</span><br><span class="line">6.0,3.0,4.8,1.8,Iris-virginica</span><br><span class="line">6.9,3.1,5.4,2.1,Iris-virginica</span><br><span class="line">6.7,3.1,5.6,2.4,Iris-virginica</span><br><span class="line">6.9,3.1,5.1,2.3,Iris-virginica</span><br><span class="line">5.8,2.7,5.1,1.9,Iris-virginica</span><br><span class="line">6.8,3.2,5.9,2.3,Iris-virginica</span><br><span class="line">6.7,3.3,5.7,2.5,Iris-virginica</span><br><span class="line">6.7,3.0,5.2,2.3,Iris-virginica</span><br><span class="line">6.3,2.5,5.0,1.9,Iris-virginica</span><br><span class="line">6.5,3.0,5.2,2.0,Iris-virginica</span><br><span class="line">6.2,3.4,5.4,2.3,Iris-virginica</span><br><span class="line">5.9,3.0,5.1,1.8,Iris-virginica</span><br></pre></td></tr></tbody></table></figure><p>参考链接：<a href="https://github.com/eva-n27/PCA">https://github.com/eva-n27/PCA</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kmeans算法</title>
      <link href="/2019/09/07/20190907-kmeans%E7%AE%97%E6%B3%95/"/>
      <url>/2019/09/07/20190907-kmeans%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<hr><p>K-Means的思想十分简单，首先随机指定类中心，根据样本与类中心的远近划分类簇，接着重新计算类中心，迭代直至收敛。但是其中迭代的过程并不是主观地想象得出，事实上，若将样本的类别看做为“隐变量”（latent variable），类中心看作样本的分布参数，这一过程正是通过EM算法的两步走策略而计算出，其根本的目的是为了最小化平方误差函数E。</p><h2 id="kmeans算法的最大弱点：只能处理球形的簇（理论）"><a href="#kmeans算法的最大弱点：只能处理球形的簇（理论）" class="headerlink" title="kmeans算法的最大弱点：只能处理球形的簇（理论）"></a>kmeans算法的最大弱点：只能处理球形的簇（理论）</h2><h2 id="kmeans-计算步骤"><a href="#kmeans-计算步骤" class="headerlink" title="kmeans 计算步骤"></a>kmeans 计算步骤</h2><ul><li>1.随机选取K个聚类中心，这里的k值可以自己设定</li><li>2.先设置一个聚类标志，用来保存当前的 样本与第几个聚类中心最近</li><li>3.计算每个样例与每个聚类中心的距离，保存最小距离的k以及距离</li><li>4.更新聚类中心，为当前类别所有样本的均值大小<img src="/images/20190907_kmeans%E7%AE%97%E6%B3%95_myplot.png"></li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># calculate Euclidean distance</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">euclDistance</span>(<span class="params">vector1, vector2</span>):</span><br><span class="line"><span class="keyword">return</span> sqrt(<span class="built_in">sum</span>(power(vector2 - vector1, <span class="number">2</span>)))  <span class="comment"># 求这两个矩阵的距离， vector1, vector2 均为矩阵</span></span><br><span class="line"><span class="comment"># init centroids with random samples</span></span><br><span class="line"><span class="comment"># 在样本集中随机选取k个样本点作为初始质心</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">initCentroids</span>(<span class="params">dataSet, k</span>):</span><br><span class="line">numSamples, dim = dataSet.shape  <span class="comment"># 矩阵的行数、列数</span></span><br><span class="line">centroids = zeros((k, dim))  <span class="comment"># 感觉要不要你都可以</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">index = <span class="built_in">int</span>(random.uniform(<span class="number">0</span>, numSamples))  <span class="comment"># 随机产生一个浮点数，然后将其转化为int型</span></span><br><span class="line">centroids[i, :] = dataSet[index, :]</span><br><span class="line"><span class="keyword">return</span> centroids</span><br><span class="line"><span class="comment"># k-means cluster</span></span><br><span class="line"><span class="comment"># dataSet为一个矩阵</span></span><br><span class="line"><span class="comment"># k为将dataSet矩阵中的样本分成k个类</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">kmeans</span>(<span class="params">dataSet, k</span>):</span><br><span class="line">numSamples = dataSet.shape[<span class="number">0</span>]  <span class="comment"># 读取矩阵dataSet的第一维度的长度,即获得有多少个样本数据</span></span><br><span class="line"><span class="comment"># first column stores which cluster this sample belongs to,</span></span><br><span class="line"><span class="comment"># second column stores the error between this sample and its centroid</span></span><br><span class="line">clusterAssment = mat(zeros((numSamples, <span class="number">2</span>)))  <span class="comment"># 得到一个N*2的零矩阵</span></span><br><span class="line">clusterChanged = <span class="literal">True</span></span><br><span class="line"><span class="comment">## step 1: init centroids</span></span><br><span class="line">centroids = initCentroids(dataSet, k)  <span class="comment"># 在样本集中随机选取k个样本点作为初始质心</span></span><br><span class="line"><span class="keyword">while</span> clusterChanged:</span><br><span class="line">clusterChanged = <span class="literal">False</span></span><br><span class="line"><span class="comment">## for each sample</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numSamples):  <span class="comment"># range</span></span><br><span class="line">minDist = <span class="number">100000.0</span></span><br><span class="line">minIndex = <span class="number">0</span></span><br><span class="line"><span class="comment">## for each centroid</span></span><br><span class="line"><span class="comment">## step 2: find the centroid who is closest</span></span><br><span class="line"><span class="comment"># 计算每个样本点与质点之间的距离，将其归内到距离最小的那一簇</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">distance = euclDistance(centroids[j, :], dataSet[i, :])</span><br><span class="line"><span class="keyword">if</span> distance &lt; minDist:</span><br><span class="line">minDist = distance</span><br><span class="line">minIndex = j</span><br><span class="line"><span class="comment">## step 3: update its cluster</span></span><br><span class="line"><span class="comment"># k个簇里面与第i个样本距离最小的的标号和距离保存在clusterAssment中</span></span><br><span class="line"><span class="comment"># 若所有的样本不在变化，则退出while循环</span></span><br><span class="line"><span class="keyword">if</span> clusterAssment[i, <span class="number">0</span>] != minIndex:</span><br><span class="line">clusterChanged = <span class="literal">True</span></span><br><span class="line">clusterAssment[i, :] = minIndex, minDist ** <span class="number">2</span>  <span class="comment"># 两个**表示的是minDist的平方</span></span><br><span class="line"><span class="comment">## step 4: update centroids</span></span><br><span class="line"><span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line"><span class="comment"># clusterAssment[:,0].A==j是找出矩阵clusterAssment中第一列元素中等于j的行的下标，返回的是一个以array的列表，第一个array为等于j的下标</span></span><br><span class="line">pointsInCluster = dataSet[nonzero(clusterAssment[:, <span class="number">0</span>].A == j)[<span class="number">0</span>]]  <span class="comment"># 将dataSet矩阵中相对应的样本提取出来</span></span><br><span class="line">centroids[j, :] = mean(pointsInCluster, axis=<span class="number">0</span>)  <span class="comment"># 计算标注为j的所有样本的平均值</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Congratulations, cluster complete!'</span>)</span><br><span class="line"><span class="keyword">return</span> centroids, clusterAssment</span><br><span class="line"><span class="comment"># show your cluster only available with 2-D data</span></span><br><span class="line"><span class="comment"># centroids为k个类别，其中保存着每个类别的质心</span></span><br><span class="line"><span class="comment"># clusterAssment为样本的标记，第一列为此样本的类别号，第二列为到此类别质心的距离</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">showCluster</span>(<span class="params">dataSet, k, centroids, clusterAssment</span>):</span><br><span class="line">numSamples, dim = dataSet.shape</span><br><span class="line"><span class="keyword">if</span> dim != <span class="number">2</span>:</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Sorry! I can not draw because the dimension of your data is not 2!"</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span></span><br><span class="line">mark = [<span class="string">'or'</span>, <span class="string">'ob'</span>, <span class="string">'og'</span>, <span class="string">'ok'</span>, <span class="string">'^r'</span>, <span class="string">'+r'</span>, <span class="string">'sr'</span>, <span class="string">'dr'</span>, <span class="string">'&lt;r'</span>, <span class="string">'pr'</span>]</span><br><span class="line"><span class="keyword">if</span> k &gt; <span class="built_in">len</span>(mark):</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"Sorry! Your k is too large! please contact wojiushimogui"</span>)</span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span></span><br><span class="line"><span class="comment"># draw all samples</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(numSamples):</span><br><span class="line">markIndex = <span class="built_in">int</span>(clusterAssment[i, <span class="number">0</span>])  <span class="comment"># 为样本指定颜色</span></span><br><span class="line">plt.plot(dataSet[i, <span class="number">0</span>], dataSet[i, <span class="number">1</span>], mark[markIndex])</span><br><span class="line">mark = [<span class="string">'Dr'</span>, <span class="string">'Db'</span>, <span class="string">'Dg'</span>, <span class="string">'Dk'</span>, <span class="string">'^b'</span>, <span class="string">'+b'</span>, <span class="string">'sb'</span>, <span class="string">'db'</span>, <span class="string">'&lt;b'</span>, <span class="string">'pb'</span>]</span><br><span class="line"><span class="comment"># draw the centroids</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(k):</span><br><span class="line">plt.plot(centroids[i, <span class="number">0</span>], centroids[i, <span class="number">1</span>], mark[i], markersize=<span class="number">12</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="keyword">import</span> time</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> kmeans</span><br><span class="line"><span class="comment">## step 1: load data</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"step 1: load data..."</span>)</span><br><span class="line">dataSet = []  <span class="comment"># 列表，用来表示，列表中的每个元素也是一个二维的列表；这个二维列表就是一个样本，样本中包含有我们的属性值和类别号。</span></span><br><span class="line"><span class="comment"># 与我们所熟悉的矩阵类似，最终我们将获得N*2的矩阵，每行元素构成了我们的训练样本的属性值和类别号</span></span><br><span class="line">fileIn = <span class="built_in">open</span>(<span class="string">"./testSet.txt"</span>)  <span class="comment"># 是正斜杠</span></span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> fileIn.readlines():</span><br><span class="line">    temp = []</span><br><span class="line">    lineArr = line.strip().split(<span class="string">'\t'</span>)  <span class="comment"># line.strip()把末尾的'\n'去掉</span></span><br><span class="line">    temp.append(<span class="built_in">float</span>(lineArr[<span class="number">0</span>]))</span><br><span class="line">    temp.append(<span class="built_in">float</span>(lineArr[<span class="number">1</span>]))</span><br><span class="line">    dataSet.append(temp)</span><br><span class="line"><span class="comment"># dataSet.append([float(lineArr[0]), float(lineArr[1])])</span></span><br><span class="line">fileIn.close()</span><br><span class="line"><span class="comment">## step 2: clustering...</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"step 2: clustering..."</span>)</span><br><span class="line">dataSet = mat(dataSet)  <span class="comment"># mat()函数是Numpy中的库函数，将数组转化为矩阵</span></span><br><span class="line">k = <span class="number">4</span></span><br><span class="line">centroids, clusterAssment = kmeans.kmeans(dataSet, k)  <span class="comment"># 调用KMeans文件中定义的kmeans方法。</span></span><br><span class="line"><span class="comment">## step 3: show the result</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">"step 3: show the result..."</span>)</span><br><span class="line">kmeans.showCluster(dataSet, k, centroids, clusterAssment)</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br></pre></td><td class="code"><pre><span class="line">1.6589854.285136</span><br><span class="line">-3.4536873.424321</span><br><span class="line">4.8381381.151539</span><br><span class="line">-5.379713-3.362104</span><br><span class="line">0.9725642.924086</span><br><span class="line">-3.5679191.531611</span><br><span class="line">0.450614-3.302219</span><br><span class="line">-3.487105-1.724432</span><br><span class="line">2.6687591.594842</span><br><span class="line">-3.1564853.191137</span><br><span class="line">3.165506-3.999838</span><br><span class="line">-2.786837-3.099354</span><br><span class="line">4.2081872.984927</span><br><span class="line">-2.1233372.943366</span><br><span class="line">0.704199-0.479481</span><br><span class="line">-0.392370-3.963704</span><br><span class="line">2.8316671.574018</span><br><span class="line">-0.7901533.343144</span><br><span class="line">2.943496-3.357075</span><br><span class="line">-3.195883-2.283926</span><br><span class="line">2.3364452.875106</span><br><span class="line">-1.7863452.554248</span><br><span class="line">2.190101-1.906020</span><br><span class="line">-3.403367-2.778288</span><br><span class="line">1.7781243.880832</span><br><span class="line">-1.6883462.230267</span><br><span class="line">2.592976-2.054368</span><br><span class="line">-4.007257-3.207066</span><br><span class="line">2.2577343.387564</span><br><span class="line">-2.6790110.785119</span><br><span class="line">0.939512-4.023563</span><br><span class="line">-3.674424-2.261084</span><br><span class="line">2.0462592.735279</span><br><span class="line">-3.1894701.780269</span><br><span class="line">4.372646-0.822248</span><br><span class="line">-2.579316-3.497576</span><br><span class="line">1.8890345.190400</span><br><span class="line">-0.7987472.185588</span><br><span class="line">2.836520-2.658556</span><br><span class="line">-3.837877-3.253815</span><br><span class="line">2.0967013.886007</span><br><span class="line">-2.7090342.923887</span><br><span class="line">3.367037-3.184789</span><br><span class="line">-2.121479-4.232586</span><br><span class="line">2.3295463.179764</span><br><span class="line">-3.2848163.273099</span><br><span class="line">3.091414-3.815232</span><br><span class="line">-3.762093-2.432191</span><br><span class="line">3.5420562.778832</span><br><span class="line">-1.7368224.241041</span><br><span class="line">2.127073-2.983680</span><br><span class="line">-4.323818-3.938116</span><br><span class="line">3.7921215.135768</span><br><span class="line">-4.7864733.358547</span><br><span class="line">2.624081-3.260715</span><br><span class="line">-4.009299-2.978115</span><br><span class="line">2.4935251.963710</span><br><span class="line">-2.5136612.642162</span><br><span class="line">1.864375-3.176309</span><br><span class="line">-3.171184-3.572452</span><br><span class="line">2.8942202.489128</span><br><span class="line">-2.5625392.884438</span><br><span class="line">3.491078-3.947487</span><br><span class="line">-2.565729-2.012114</span><br><span class="line">3.3329483.983102</span><br><span class="line">-1.6168053.573188</span><br><span class="line">2.280615-2.559444</span><br><span class="line">-2.651229-3.103198</span><br><span class="line">2.3213953.154987</span><br><span class="line">-1.6857032.939697</span><br><span class="line">3.031012-3.620252</span><br><span class="line">-4.599622-2.185829</span><br><span class="line">4.1962231.126677</span><br><span class="line">-2.1338633.093686</span><br><span class="line">4.668892-2.562705</span><br><span class="line">-2.793241-2.149706</span><br><span class="line">2.8841053.043438</span><br><span class="line">-2.9676472.848696</span><br><span class="line">4.479332-1.764772</span><br><span class="line">-4.905566-2.911070</span><br></pre></td></tr></tbody></table></figure><p>参考链接：<a href="https://github.com/wojiushimogui/kmeans">https://github.com/wojiushimogui/kmeans</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 机器学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>xml生成</title>
      <link href="/2019/09/07/20190907-xml%E7%94%9F%E6%88%90/"/>
      <url>/2019/09/07/20190907-xml%E7%94%9F%E6%88%90/</url>
      
        <content type="html"><![CDATA[<p>先创建 root Element, 然后创建 SubElement, 最后将 root 传入 ElementTree(element), 创建 tree, 调用 tree.write() 方法写入文件,创建 XML 类型的数据文件  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">subElement</span>(<span class="params">root, tag, text</span>):</span><br><span class="line">    ele = ET.SubElement(root, tag)</span><br><span class="line">    ele.text = text</span><br><span class="line">    ele.tail = <span class="string">'\n'</span></span><br><span class="line">root = ET.Element(<span class="string">"note"</span>)</span><br><span class="line">to = root.makeelement(<span class="string">"to"</span>, {})</span><br><span class="line">to.text = <span class="string">"peter"</span></span><br><span class="line">to.tail = <span class="string">'\n'</span></span><br><span class="line">root.append(to)</span><br><span class="line">subElement(root, <span class="string">"from"</span>, <span class="string">"marry"</span>)</span><br><span class="line">subElement(root, <span class="string">"heading"</span>, <span class="string">"Reminder"</span>)</span><br><span class="line">subElement(root, <span class="string">"body"</span>, <span class="string">"Don't forget the meeting!"</span>)</span><br><span class="line">tree = ET.ElementTree(root)</span><br><span class="line">tree.write(<span class="string">"note.xml"</span>, encoding=<span class="string">"utf-8"</span>, xml_declaration=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">&lt;?xml version='1.0' encoding='utf-8'?&gt;</span><br><span class="line">&lt;note&gt;</span><br><span class="line">&lt;to&gt;peter&lt;/to&gt;</span><br><span class="line">&lt;from&gt;marry&lt;/from&gt;</span><br><span class="line">&lt;heading&gt;Reminder&lt;/heading&gt;</span><br><span class="line">&lt;body&gt;Don't forget the meeting!&lt;/body&gt;</span><br><span class="line">&lt;/note&gt;</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>xml解析</title>
      <link href="/2019/09/07/20190907-xml%E8%A7%A3%E6%9E%90/"/>
      <url>/2019/09/07/20190907-xml%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">&lt;annotation&gt;</span><br><span class="line">&lt;folder&gt;ILSVRC2015_VID_train_0002/ILSVRC2015_train_00555002&lt;/folder&gt;</span><br><span class="line">&lt;filename&gt;000000&lt;/filename&gt;</span><br><span class="line">&lt;source&gt;</span><br><span class="line">&lt;database&gt;ILSVRC_2015&lt;/database&gt;</span><br><span class="line">&lt;/source&gt;</span><br><span class="line">&lt;size&gt;</span><br><span class="line">&lt;width&gt;1280&lt;/width&gt;</span><br><span class="line">&lt;height&gt;720&lt;/height&gt;</span><br><span class="line">&lt;/size&gt;</span><br><span class="line">&lt;object&gt;</span><br><span class="line">&lt;trackid&gt;0&lt;/trackid&gt;</span><br><span class="line">&lt;name&gt;n02691156&lt;/name&gt;</span><br><span class="line">&lt;bndbox&gt;</span><br><span class="line">&lt;xmax&gt;659&lt;/xmax&gt;</span><br><span class="line">&lt;xmin&gt;592&lt;/xmin&gt;</span><br><span class="line">&lt;ymax&gt;375&lt;/ymax&gt;</span><br><span class="line">&lt;ymin&gt;334&lt;/ymin&gt;</span><br><span class="line">&lt;/bndbox&gt;</span><br><span class="line">&lt;occluded&gt;0&lt;/occluded&gt;</span><br><span class="line">&lt;generated&gt;0&lt;/generated&gt;</span><br><span class="line">&lt;/object&gt;</span><br><span class="line">&lt;/annotation&gt;</span><br></pre></td></tr></tbody></table></figure><p>ElementTree生来就是为了处理XML, 它在Python标准库中有两种实现：一种是纯Python实现的, 如xml.etree.ElementTree, 另一种是速度快一点的xml.etree.cElementTree. 注意：尽量使用C语言实现的那种, 因为它速度更快, 而且消耗的内存更少.</p><ul><li>a. 遍历根节点的下一层 </li><li>b. 下标访问各个标签、属性、文本</li><li>c. 查找root下的指定标签</li><li>d. 遍历XML文件</li><li>e. 修改XML文件</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">```python</span><br><span class="line"><span class="keyword">import</span>  os, sys</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line"><span class="keyword">import</span> xml.etree.cElementTree <span class="keyword">as</span> ET</span><br><span class="line"><span class="keyword">except</span>:</span><br><span class="line"><span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</span><br></pre></td></tr></tbody></table></figure><h2 id="解析xml文件"><a href="#解析xml文件" class="headerlink" title="解析xml文件"></a>解析xml文件</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">xmlFilePath = os.path.abspath(<span class="string">'000000.xml'</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    tree = ET.parse(xmlFilePath) <span class="comment"># 或者 tree = ET.ElementTree(xmlFilePath)</span></span><br><span class="line">    root = tree.getroot() <span class="comment"># 获取根节点</span></span><br><span class="line"><span class="keyword">except</span> Exception <span class="keyword">as</span> e:</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'parse xml failed!'</span>)</span><br><span class="line">    sys.exit()</span><br></pre></td></tr></tbody></table></figure><h2 id="逐层遍历"><a href="#逐层遍历" class="headerlink" title="逐层遍历"></a>逐层遍历</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(root.tag, root.attrib, root.text)</span><br><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> root:</span><br><span class="line">    <span class="built_in">print</span>(child.tag, child.attrib, child.text)</span><br></pre></td></tr></tbody></table></figure><h2 id="递归遍历全部"><a href="#递归遍历全部" class="headerlink" title="递归遍历全部:"></a>递归遍历全部:</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">traverseXml</span>(<span class="params">element</span>):</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(element) &gt; <span class="number">0</span>: <span class="comment"># 叶节点的len为0</span></span><br><span class="line">        <span class="keyword">for</span> child <span class="keyword">in</span> element:</span><br><span class="line">            <span class="built_in">print</span>(child.tag, child.attrib)</span><br><span class="line">            traverseXml(child)</span><br><span class="line">traverseXml(root)</span><br></pre></td></tr></tbody></table></figure><h2 id="根据签名查找需要的标签"><a href="#根据签名查找需要的标签" class="headerlink" title="根据签名查找需要的标签"></a>根据签名查找需要的标签</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">item_lists = root.findall(<span class="string">'item'</span>) <span class="comment"># 只能找到儿子, 不能找到孙子, 返回的是儿子们组成的列表</span></span><br><span class="line">item = root.find(<span class="string">'item'</span>) <span class="comment"># 返回的是单个的儿子</span></span><br><span class="line"><span class="built_in">print</span>(root)</span><br><span class="line"><span class="built_in">print</span>(item_lists)</span><br><span class="line"><span class="built_in">print</span>(item)</span><br></pre></td></tr></tbody></table></figure><h2 id="获取叶子节点的值"><a href="#获取叶子节点的值" class="headerlink" title="获取叶子节点的值"></a>获取叶子节点的值</h2><h2 id="当访问到叶子节点时-就可以利用-text-来得到相应的标签了"><a href="#当访问到叶子节点时-就可以利用-text-来得到相应的标签了" class="headerlink" title="当访问到叶子节点时, 就可以利用 text 来得到相应的标签了"></a>当访问到叶子节点时, 就可以利用 text 来得到相应的标签了</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">obj_bbox_set =[]</span><br><span class="line">objects = root.findall(<span class="string">'object'</span>)</span><br><span class="line"><span class="keyword">for</span> obj <span class="keyword">in</span> objects:</span><br><span class="line">    obj_name = obj.find(<span class="string">'name'</span>).text</span><br><span class="line">    bbox = obj.find(<span class="string">'bndbox'</span>)</span><br><span class="line">    x1 = <span class="built_in">int</span>(bbox.find(<span class="string">'xmin'</span>).text)</span><br><span class="line">    y1 = <span class="built_in">int</span>(bbox.find(<span class="string">'ymin'</span>).text)</span><br><span class="line">    x2 = <span class="built_in">int</span>(bbox.find(<span class="string">'xmax'</span>).text)</span><br><span class="line">    y2 = <span class="built_in">int</span>(bbox.find(<span class="string">'ymax'</span>).text)</span><br><span class="line">    obj_bbox_set.append([x1, x2, y1, y2, obj_name])</span><br><span class="line"><span class="built_in">print</span>(obj_bbox_set)</span><br></pre></td></tr></tbody></table></figure><pre><code></code></pre>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>矩形覆盖</title>
      <link href="/2019/09/07/20190907-%E7%9F%A9%E5%BD%A2%E8%A6%86%E7%9B%96/"/>
      <url>/2019/09/07/20190907-%E7%9F%A9%E5%BD%A2%E8%A6%86%E7%9B%96/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：363840<br>本题知识点： 递归  </p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>我们可以用2<em>1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2</em>1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> {</span><br><span class="line">public:</span><br><span class="line">    <span class="built_in">int</span> rectCover(<span class="built_in">int</span> number) {</span><br><span class="line">    <span class="keyword">if</span>(number &lt;= <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">if</span>(number == <span class="number">1</span> || number == <span class="number">2</span>) <span class="keyword">return</span> number;</span><br><span class="line">    <span class="keyword">return</span> rectCover(number - <span class="number">1</span>) + rectCover(number - <span class="number">2</span>);</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><p>运行时间：538ms<br>占用内存：484k</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PCA降维</title>
      <link href="/2019/09/07/PCA%E9%99%8D%E7%BB%B4/"/>
      <url>/2019/09/07/PCA%E9%99%8D%E7%BB%B4/</url>
      
        <content type="html"><![CDATA[<p>PCA（principal Component Analysis），主成分分析法。顾名思义，就是提取出数据中主要的成分，是一种数据压缩方法，常用于去除噪声、数据预处理，也是机器学习中常见的降维方法。<br><img src="/2019/09/07/PCA%E9%99%8D%E7%BB%B4/images/20190907_PCA%E9%99%8D%E7%BB%B4_myplot.png"></p><h2 id="步骤"><a href="#步骤" class="headerlink" title="步骤"></a>步骤</h2><ul><li><p>对所有样本进行中心化处理<br>即将每个元素减去它的平均值，这样可以增加基向量的正交性。</p></li><li><p>计算协方差矩阵及特征值、特征向量</p></li><li><p>对特征值进行排序</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101</code></pre></li></ul><p>| </p><pre><code>    # coding:utf-8      from matplotlib import pylab as plt      import numpy as np            def read_file(filename):          f = open(filename, 'r')          d = f.readlines()          f.close()          return d            class PCA(object):          """          使用PCA对高维数据进行降维处理          """          def __init__(self):              data = read_file('data.txt')                    self.n = len(data)  # 数据的个数              self.dim = 4  # 原始数据的维度              self.x = np.zeros((self.n, self.dim), dtype='float64')              for i in range(self.n):                  data_ = data[i].split(',')                  self.x[i][0] = data_[0]                  self.x[i][1] = data_[1]                  self.x[i][2] = data_[2]                  self.x[i][3] = data_[3]                    self.k = 2  # 降到二维              self.mean_x = np.zeros((self.n, self.dim), dtype='float64')  # 原始数据减去均值以后的x              self.mean = np.zeros((self.dim, 1), dtype='float64')  # x的均值              self.cov = np.zeros((self.dim, self.dim), dtype='float64')  # 协方差矩阵              self.pre_x = np.zeros((self.n, self.dim), dtype='float64')  # 预处理之后的数据              self.eig_val = np.zeros((1, self.dim), dtype='float64')  # 特征值              self.eig_vec = np.zeros((self.dim, self.dim), dtype='float64')  # 特征向量              self.final_x = np.zeros((self.n, self.k), dtype='float64')  # 投影后的数据              self.pretreatment()              self.pca()                def pretreatment(self):              """              预处理              """              # 求均值              for i in range(self.dim):                  self.mean[i] = np.mean(self.x[:, i])                    for i in range(self.n):                  self.mean_x[i] = self.x[i] - self.mean.T                    # 求协方差              # self.cov = np.cov(self.mean_x, rowvar=0)              # mean_x已经是x减去均值了，所以直接相乘就是方差              self.cov = self.mean_x.T.dot(self.mean_x) / self.n                    for i in range(self.dim):                  self.pre_x[:, i] = self.mean_x[:, i] / np.sqrt(self.cov[i][i])   # x的每个维度都处理一次                def pca(self):              """              pca的实现              """              # 需要注意的是，在这里需要对预处理之后的数据重新计算协方差              # 计算均值              for i in range(self.dim):                  self.mean[i] = np.mean(self.pre_x[:, i])                    # 计算协方差              for i in range(self.n):                  self.mean_x[i] = self.pre_x[i] - self.mean.T              self.cov = (self.mean_x.T.dot(self.mean_x)) / self.n                    # 求特征值              self.eig_val, self.eig_vec = np.linalg.eig(np.mat(self.cov))                    # eig_vec的列向量是特征向量，所以用eig_v存储特征向量，以便后面排序              eig_v = np.zeros((self.dim, self.dim), dtype='float64')              for i in range(self.dim):                  eig_v[i] = self.eig_vec[:, i].T                    # 排序              eig_list = zip(self.eig_val, eig_v)              sorted(eig_list,key=lambda g: g[0], reverse=True)                    # 处理排序的结果              for i in range(len(list(eig_list))):                  self.eig_val[i] = eig_list[i][0]                  self.eig_vec[i] = eig_list[i][1]                    # 最大的k个特征向量，降维              f_vec = self.eig_vec[0:self.k, :]              self.final_x = self.pre_x * f_vec.T                    # 显示降维后的数据点              plt.plot(self.final_x[0:50, 0], self.final_x[0:50, 1], 'bo')              plt.plot(self.final_x[50:100, 0], self.final_x[50:100, 1], 'go')              plt.plot(self.final_x[100:150, 0], self.final_x[100:150, 1], 'ro')              plt.show()                  if __name__ == '__main__':          a = PCA()        </code></pre><p>—|—  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  </code></pre><p>| </p><pre><code>5.1,3.5,1.4,0.2,Iris-setosa  4.9,3.0,1.4,0.2,Iris-setosa  4.7,3.2,1.3,0.2,Iris-setosa  4.6,3.1,1.5,0.2,Iris-setosa  5.0,3.6,1.4,0.2,Iris-setosa  5.4,3.9,1.7,0.4,Iris-setosa  4.6,3.4,1.4,0.3,Iris-setosa  5.0,3.4,1.5,0.2,Iris-setosa  4.4,2.9,1.4,0.2,Iris-setosa  4.9,3.1,1.5,0.1,Iris-setosa  5.4,3.7,1.5,0.2,Iris-setosa  4.8,3.4,1.6,0.2,Iris-setosa  4.8,3.0,1.4,0.1,Iris-setosa  4.3,3.0,1.1,0.1,Iris-setosa  5.8,4.0,1.2,0.2,Iris-setosa  5.7,4.4,1.5,0.4,Iris-setosa  5.4,3.9,1.3,0.4,Iris-setosa  5.1,3.5,1.4,0.3,Iris-setosa  5.7,3.8,1.7,0.3,Iris-setosa  5.1,3.8,1.5,0.3,Iris-setosa  5.4,3.4,1.7,0.2,Iris-setosa  5.1,3.7,1.5,0.4,Iris-setosa  4.6,3.6,1.0,0.2,Iris-setosa  5.1,3.3,1.7,0.5,Iris-setosa  4.8,3.4,1.9,0.2,Iris-setosa  5.0,3.0,1.6,0.2,Iris-setosa  5.0,3.4,1.6,0.4,Iris-setosa  5.2,3.5,1.5,0.2,Iris-setosa  5.2,3.4,1.4,0.2,Iris-setosa  4.7,3.2,1.6,0.2,Iris-setosa  4.8,3.1,1.6,0.2,Iris-setosa  5.4,3.4,1.5,0.4,Iris-setosa  5.2,4.1,1.5,0.1,Iris-setosa  5.5,4.2,1.4,0.2,Iris-setosa  4.9,3.1,1.5,0.1,Iris-setosa  5.0,3.2,1.2,0.2,Iris-setosa  5.5,3.5,1.3,0.2,Iris-setosa  4.9,3.1,1.5,0.1,Iris-setosa  4.4,3.0,1.3,0.2,Iris-setosa  5.1,3.4,1.5,0.2,Iris-setosa  5.0,3.5,1.3,0.3,Iris-setosa  4.5,2.3,1.3,0.3,Iris-setosa  4.4,3.2,1.3,0.2,Iris-setosa  5.0,3.5,1.6,0.6,Iris-setosa  5.1,3.8,1.9,0.4,Iris-setosa  4.8,3.0,1.4,0.3,Iris-setosa  5.1,3.8,1.6,0.2,Iris-setosa  4.6,3.2,1.4,0.2,Iris-setosa  5.3,3.7,1.5,0.2,Iris-setosa  5.0,3.3,1.4,0.2,Iris-setosa  7.0,3.2,4.7,1.4,Iris-versicolor  6.4,3.2,4.5,1.5,Iris-versicolor  6.9,3.1,4.9,1.5,Iris-versicolor  5.5,2.3,4.0,1.3,Iris-versicolor  6.5,2.8,4.6,1.5,Iris-versicolor  5.7,2.8,4.5,1.3,Iris-versicolor  6.3,3.3,4.7,1.6,Iris-versicolor  4.9,2.4,3.3,1.0,Iris-versicolor  6.6,2.9,4.6,1.3,Iris-versicolor  5.2,2.7,3.9,1.4,Iris-versicolor  5.0,2.0,3.5,1.0,Iris-versicolor  5.9,3.0,4.2,1.5,Iris-versicolor  6.0,2.2,4.0,1.0,Iris-versicolor  6.1,2.9,4.7,1.4,Iris-versicolor  5.6,2.9,3.6,1.3,Iris-versicolor  6.7,3.1,4.4,1.4,Iris-versicolor  5.6,3.0,4.5,1.5,Iris-versicolor  5.8,2.7,4.1,1.0,Iris-versicolor  6.2,2.2,4.5,1.5,Iris-versicolor  5.6,2.5,3.9,1.1,Iris-versicolor  5.9,3.2,4.8,1.8,Iris-versicolor  6.1,2.8,4.0,1.3,Iris-versicolor  6.3,2.5,4.9,1.5,Iris-versicolor  6.1,2.8,4.7,1.2,Iris-versicolor  6.4,2.9,4.3,1.3,Iris-versicolor  6.6,3.0,4.4,1.4,Iris-versicolor  6.8,2.8,4.8,1.4,Iris-versicolor  6.7,3.0,5.0,1.7,Iris-versicolor  6.0,2.9,4.5,1.5,Iris-versicolor  5.7,2.6,3.5,1.0,Iris-versicolor  5.5,2.4,3.8,1.1,Iris-versicolor  5.5,2.4,3.7,1.0,Iris-versicolor  5.8,2.7,3.9,1.2,Iris-versicolor  6.0,2.7,5.1,1.6,Iris-versicolor  5.4,3.0,4.5,1.5,Iris-versicolor  6.0,3.4,4.5,1.6,Iris-versicolor  6.7,3.1,4.7,1.5,Iris-versicolor  6.3,2.3,4.4,1.3,Iris-versicolor  5.6,3.0,4.1,1.3,Iris-versicolor  5.5,2.5,4.0,1.3,Iris-versicolor  5.5,2.6,4.4,1.2,Iris-versicolor  6.1,3.0,4.6,1.4,Iris-versicolor  5.8,2.6,4.0,1.2,Iris-versicolor  5.0,2.3,3.3,1.0,Iris-versicolor  5.6,2.7,4.2,1.3,Iris-versicolor  5.7,3.0,4.2,1.2,Iris-versicolor  5.7,2.9,4.2,1.3,Iris-versicolor  6.2,2.9,4.3,1.3,Iris-versicolor  5.1,2.5,3.0,1.1,Iris-versicolor  5.7,2.8,4.1,1.3,Iris-versicolor  6.3,3.3,6.0,2.5,Iris-virginica  5.8,2.7,5.1,1.9,Iris-virginica  7.1,3.0,5.9,2.1,Iris-virginica  6.3,2.9,5.6,1.8,Iris-virginica  6.5,3.0,5.8,2.2,Iris-virginica  7.6,3.0,6.6,2.1,Iris-virginica  4.9,2.5,4.5,1.7,Iris-virginica  7.3,2.9,6.3,1.8,Iris-virginica  6.7,2.5,5.8,1.8,Iris-virginica  7.2,3.6,6.1,2.5,Iris-virginica  6.5,3.2,5.1,2.0,Iris-virginica  6.4,2.7,5.3,1.9,Iris-virginica  6.8,3.0,5.5,2.1,Iris-virginica  5.7,2.5,5.0,2.0,Iris-virginica  5.8,2.8,5.1,2.4,Iris-virginica  6.4,3.2,5.3,2.3,Iris-virginica  6.5,3.0,5.5,1.8,Iris-virginica  7.7,3.8,6.7,2.2,Iris-virginica  7.7,2.6,6.9,2.3,Iris-virginica  6.0,2.2,5.0,1.5,Iris-virginica  6.9,3.2,5.7,2.3,Iris-virginica  5.6,2.8,4.9,2.0,Iris-virginica  7.7,2.8,6.7,2.0,Iris-virginica  6.3,2.7,4.9,1.8,Iris-virginica  6.7,3.3,5.7,2.1,Iris-virginica  7.2,3.2,6.0,1.8,Iris-virginica  6.2,2.8,4.8,1.8,Iris-virginica  6.1,3.0,4.9,1.8,Iris-virginica  6.4,2.8,5.6,2.1,Iris-virginica  7.2,3.0,5.8,1.6,Iris-virginica  7.4,2.8,6.1,1.9,Iris-virginica  7.9,3.8,6.4,2.0,Iris-virginica  6.4,2.8,5.6,2.2,Iris-virginica  6.3,2.8,5.1,1.5,Iris-virginica  6.1,2.6,5.6,1.4,Iris-virginica  7.7,3.0,6.1,2.3,Iris-virginica  6.3,3.4,5.6,2.4,Iris-virginica  6.4,3.1,5.5,1.8,Iris-virginica  6.0,3.0,4.8,1.8,Iris-virginica  6.9,3.1,5.4,2.1,Iris-virginica  6.7,3.1,5.6,2.4,Iris-virginica  6.9,3.1,5.1,2.3,Iris-virginica  5.8,2.7,5.1,1.9,Iris-virginica  6.8,3.2,5.9,2.3,Iris-virginica  6.7,3.3,5.7,2.5,Iris-virginica  6.7,3.0,5.2,2.3,Iris-virginica  6.3,2.5,5.0,1.9,Iris-virginica  6.5,3.0,5.2,2.0,Iris-virginica  6.2,3.4,5.4,2.3,Iris-virginica  5.9,3.0,5.1,1.8,Iris-virginica    </code></pre><p>—|—  </p><p>参考链接：<a href="https://github.com/eva-n27/PCA">https://github.com/eva-n27/PCA</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>kmeans算法</title>
      <link href="/2019/09/07/kmeans%E7%AE%97%E6%B3%95/"/>
      <url>/2019/09/07/kmeans%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<hr><p>K-Means的思想十分简单，首先随机指定类中心，根据样本与类中心的远近划分类簇，接着重新计算类中心，迭代直至收敛。但是其中迭代的过程并不是主观地想象得出，事实上，若将样本的类别看做为“隐变量”（latent variable），类中心看作样本的分布参数，这一过程正是通过EM算法的两步走策略而计算出，其根本的目的是为了最小化平方误差函数E。</p><h2 id="kmeans算法的最大弱点：只能处理球形的簇（理论）"><a href="#kmeans算法的最大弱点：只能处理球形的簇（理论）" class="headerlink" title="kmeans算法的最大弱点：只能处理球形的簇（理论）"></a>kmeans算法的最大弱点：只能处理球形的簇（理论）</h2><h2 id="kmeans-计算步骤"><a href="#kmeans-计算步骤" class="headerlink" title="kmeans 计算步骤"></a>kmeans 计算步骤</h2><ul><li><p>1.随机选取K个聚类中心，这里的k值可以自己设定</p></li><li><p>2.先设置一个聚类标志，用来保存当前的 样本与第几个聚类中心最近</p></li><li><p>3.计算每个样例与每个聚类中心的距离，保存最小距离的k以及距离</p></li><li><p>4.更新聚类中心，为当前类别所有样本的均值大小<img src="/2019/09/07/kmeans%E7%AE%97%E6%B3%95/images/20190907_kmeans%E7%AE%97%E6%B3%95_myplot.png"></p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92</code></pre></li></ul><p>| </p><pre><code>    from numpy import *      import time      import matplotlib.pyplot as plt                  # calculate Euclidean distance      def euclDistance(vector1, vector2):          return sqrt(sum(power(vector2 - vector1, 2)))  # 求这两个矩阵的距离， vector1, vector2 均为矩阵                  # init centroids with random samples      # 在样本集中随机选取k个样本点作为初始质心                  def initCentroids(dataSet, k):          numSamples, dim = dataSet.shape  # 矩阵的行数、列数          centroids = zeros((k, dim))  # 感觉要不要你都可以          for i in range(k):              index = int(random.uniform(0, numSamples))  # 随机产生一个浮点数，然后将其转化为int型              centroids[i, :] = dataSet[index, :]          return centroids            # k-means cluster      # dataSet为一个矩阵      # k为将dataSet矩阵中的样本分成k个类      def kmeans(dataSet, k):          numSamples = dataSet.shape[0]  # 读取矩阵dataSet的第一维度的长度,即获得有多少个样本数据          # first column stores which cluster this sample belongs to,          # second column stores the error between this sample and its centroid          clusterAssment = mat(zeros((numSamples, 2)))  # 得到一个N*2的零矩阵          clusterChanged = True                ## step 1: init centroids          centroids = initCentroids(dataSet, k)  # 在样本集中随机选取k个样本点作为初始质心                while clusterChanged:              clusterChanged = False              ## for each sample              for i in range(numSamples):  # range                  minDist = 100000.0                  minIndex = 0                  ## for each centroid                  ## step 2: find the centroid who is closest                  # 计算每个样本点与质点之间的距离，将其归内到距离最小的那一簇                  for j in range(k):                      distance = euclDistance(centroids[j, :], dataSet[i, :])                      if distance &lt; minDist:                          minDist = distance                          minIndex = j                                ## step 3: update its cluster                  # k个簇里面与第i个样本距离最小的的标号和距离保存在clusterAssment中                  # 若所有的样本不在变化，则退出while循环                  if clusterAssment[i, 0] != minIndex:                      clusterChanged = True                      clusterAssment[i, :] = minIndex, minDist ** 2  # 两个**表示的是minDist的平方                    ## step 4: update centroids              for j in range(k):                  # clusterAssment[:,0].A==j是找出矩阵clusterAssment中第一列元素中等于j的行的下标，返回的是一个以array的列表，第一个array为等于j的下标                  pointsInCluster = dataSet[nonzero(clusterAssment[:, 0].A == j)[0]]  # 将dataSet矩阵中相对应的样本提取出来                  centroids[j, :] = mean(pointsInCluster, axis=0)  # 计算标注为j的所有样本的平均值                print('Congratulations, cluster complete!')          return centroids, clusterAssment                  # show your cluster only available with 2-D data      # centroids为k个类别，其中保存着每个类别的质心      # clusterAssment为样本的标记，第一列为此样本的类别号，第二列为到此类别质心的距离      def showCluster(dataSet, k, centroids, clusterAssment):          numSamples, dim = dataSet.shape          if dim != 2:              print("Sorry! I can not draw because the dimension of your data is not 2!")              return 1                mark = ['or', 'ob', 'og', 'ok', '^r', '+r', 'sr', 'dr', '&lt;r', 'pr']          if k &gt; len(mark):              print("Sorry! Your k is too large! please contact wojiushimogui")              return 1                    # draw all samples          for i in range(numSamples):              markIndex = int(clusterAssment[i, 0])  # 为样本指定颜色              plt.plot(dataSet[i, 0], dataSet[i, 1], mark[markIndex])                mark = ['Dr', 'Db', 'Dg', 'Dk', '^b', '+b', 'sb', 'db', '&lt;b', 'pb']          # draw the centroids          for i in range(k):              plt.plot(centroids[i, 0], centroids[i, 1], mark[i], markersize=12)                plt.show()        </code></pre><p>—|—  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  </code></pre><p>| </p><pre><code>from numpy import *  import time  import matplotlib.pyplot as plt  import kmeans    ## step 1: load data  print("step 1: load data...")  dataSet = []  # 列表，用来表示，列表中的每个元素也是一个二维的列表；这个二维列表就是一个样本，样本中包含有我们的属性值和类别号。  # 与我们所熟悉的矩阵类似，最终我们将获得N*2的矩阵，每行元素构成了我们的训练样本的属性值和类别号  fileIn = open("./testSet.txt")  # 是正斜杠  for line in fileIn.readlines():      temp = []      lineArr = line.strip().split('\t')  # line.strip()把末尾的'\n'去掉      temp.append(float(lineArr[0]))      temp.append(float(lineArr[1]))      dataSet.append(temp)  # dataSet.append([float(lineArr[0]), float(lineArr[1])])  fileIn.close()  ## step 2: clustering...  print("step 2: clustering...")  dataSet = mat(dataSet)  # mat()函数是Numpy中的库函数，将数组转化为矩阵  k = 4  centroids, clusterAssment = kmeans.kmeans(dataSet, k)  # 调用KMeans文件中定义的kmeans方法。    ## step 3: show the result  print("step 3: show the result...")  kmeans.showCluster(dataSet, k, centroids, clusterAssment)    </code></pre><p>—|—  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  </code></pre><p>| </p><pre><code>1.6589854.285136    -3.4536873.424321    4.8381381.151539    -5.379713-3.362104    0.9725642.924086    -3.5679191.531611    0.450614-3.302219    -3.487105-1.724432    2.6687591.594842    -3.1564853.191137    3.165506-3.999838    -2.786837-3.099354    4.2081872.984927    -2.1233372.943366    0.704199-0.479481    -0.392370-3.963704    2.8316671.574018    -0.7901533.343144    2.943496-3.357075    -3.195883-2.283926    2.3364452.875106    -1.7863452.554248    2.190101-1.906020    -3.403367-2.778288    1.7781243.880832    -1.6883462.230267    2.592976-2.054368    -4.007257-3.207066    2.2577343.387564    -2.6790110.785119    0.939512-4.023563    -3.674424-2.261084    2.0462592.735279    -3.1894701.780269    4.372646-0.822248    -2.579316-3.497576    1.8890345.190400    -0.7987472.185588    2.836520-2.658556    -3.837877-3.253815    2.0967013.886007    -2.7090342.923887    3.367037-3.184789    -2.121479-4.232586    2.3295463.179764    -3.2848163.273099    3.091414-3.815232    -3.762093-2.432191    3.5420562.778832    -1.7368224.241041    2.127073-2.983680    -4.323818-3.938116    3.7921215.135768    -4.7864733.358547    2.624081-3.260715    -4.009299-2.978115    2.4935251.963710    -2.5136612.642162    1.864375-3.176309    -3.171184-3.572452    2.8942202.489128    -2.5625392.884438    3.491078-3.947487    -2.565729-2.012114    3.3329483.983102    -1.6168053.573188    2.280615-2.559444    -2.651229-3.103198    2.3213953.154987    -1.6857032.939697    3.031012-3.620252    -4.599622-2.185829    4.1962231.126677    -2.1338633.093686    4.668892-2.562705    -2.793241-2.149706    2.8841053.043438    -2.9676472.848696    4.479332-1.764772    -4.905566-2.911070    </code></pre><p>—|—  </p><p>参考链接：<a href="https://github.com/wojiushimogui/kmeans">https://github.com/wojiushimogui/kmeans</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>xml生成</title>
      <link href="/2019/09/07/xml%E7%94%9F%E6%88%90/"/>
      <url>/2019/09/07/xml%E7%94%9F%E6%88%90/</url>
      
        <content type="html"><![CDATA[<p>先创建 root Element, 然后创建 SubElement, 最后将 root 传入 ElementTree(element), 创建 tree, 调用 tree.write() 方法写入文件,创建 XML 类型的数据文件  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  </code></pre><p>| </p><pre><code>import xml.etree.ElementTree as ET  def subElement(root, tag, text):      ele = ET.SubElement(root, tag)      ele.text = text      ele.tail = '\n'    root = ET.Element("note")  to = root.makeelement("to", {})  to.text = "peter"  to.tail = '\n'  root.append(to)  subElement(root, "from", "marry")  subElement(root, "heading", "Reminder")  subElement(root, "body", "Don't forget the meeting!")    tree = ET.ElementTree(root)  tree.write("note.xml", encoding="utf-8", xml_declaration=True)    </code></pre><p>—|—  </p><pre><code>1  2  3  4  5  6  7  </code></pre><p>| </p><pre><code>&lt;?xml version='1.0' encoding='utf-8'?&gt;  &lt;note&gt;  &lt;to&gt;peter&lt;/to&gt;  &lt;from&gt;marry&lt;/from&gt;  &lt;heading&gt;Reminder&lt;/heading&gt;  &lt;body&gt;Don't forget the meeting!&lt;/body&gt;  &lt;/note&gt;    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> XML </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>xml解析</title>
      <link href="/2019/09/07/xml%E8%A7%A3%E6%9E%90/"/>
      <url>/2019/09/07/xml%E8%A7%A3%E6%9E%90/</url>
      
        <content type="html"><![CDATA[<pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  </code></pre><p>| </p><pre><code>&lt;annotation&gt;      &lt;folder&gt;ILSVRC2015_VID_train_0002/ILSVRC2015_train_00555002&lt;/folder&gt;      &lt;filename&gt;000000&lt;/filename&gt;      &lt;source&gt;          &lt;database&gt;ILSVRC_2015&lt;/database&gt;      &lt;/source&gt;      &lt;size&gt;          &lt;width&gt;1280&lt;/width&gt;          &lt;height&gt;720&lt;/height&gt;      &lt;/size&gt;      &lt;object&gt;          &lt;trackid&gt;0&lt;/trackid&gt;          &lt;name&gt;n02691156&lt;/name&gt;          &lt;bndbox&gt;              &lt;xmax&gt;659&lt;/xmax&gt;              &lt;xmin&gt;592&lt;/xmin&gt;              &lt;ymax&gt;375&lt;/ymax&gt;              &lt;ymin&gt;334&lt;/ymin&gt;          &lt;/bndbox&gt;          &lt;occluded&gt;0&lt;/occluded&gt;          &lt;generated&gt;0&lt;/generated&gt;      &lt;/object&gt;  &lt;/annotation&gt;    </code></pre><p>—|—  </p><p>ElementTree生来就是为了处理XML, 它在Python标准库中有两种实现：一种是纯Python实现的, 如xml.etree.ElementTree, 另一种是速度快一点的xml.etree.cElementTree. 注意：尽量使用C语言实现的那种, 因为它速度更快, 而且消耗的内存更少.</p><ul><li><p>a. 遍历根节点的下一层 </p></li><li><p>b. 下标访问各个标签、属性、文本</p></li><li><p>c. 查找root下的指定标签</p></li><li><p>d. 遍历XML文件</p></li><li><p>e. 修改XML文件</p><pre><code>1  2  3  4  5</code></pre></li></ul><p>| </p><pre><code>    import  os, sys      try:          import xml.etree.cElementTree as ET      except:          import xml.etree.ElementTree as ET        </code></pre><p>—|—  </p><h2 id="解析xml文件"><a href="#解析xml文件" class="headerlink" title="解析xml文件"></a>解析xml文件</h2><pre><code>1  2  3  4  5  6  7  8  </code></pre><p>| </p><pre><code>xmlFilePath = os.path.abspath('000000.xml')    try:      tree = ET.parse(xmlFilePath) # 或者 tree = ET.ElementTree(xmlFilePath)      root = tree.getroot() # 获取根节点  except Exception as e:      print('parse xml failed!')      sys.exit()    </code></pre><p>—|—  </p><h2 id="逐层遍历"><a href="#逐层遍历" class="headerlink" title="逐层遍历"></a>逐层遍历</h2><pre><code>1  2  3  </code></pre><p>| </p><pre><code>print(root.tag, root.attrib, root.text)  for child in root:      print(child.tag, child.attrib, child.text)    </code></pre><p>—|—  </p><h2 id="递归遍历全部"><a href="#递归遍历全部" class="headerlink" title="递归遍历全部:"></a>递归遍历全部:</h2><pre><code>1  2  3  4  5  6  7  </code></pre><p>| </p><pre><code>def traverseXml(element):      if len(element) &gt; 0: # 叶节点的len为0          for child in element:              print(child.tag, child.attrib)              traverseXml(child)    traverseXml(root)    </code></pre><p>—|—  </p><h2 id="根据签名查找需要的标签"><a href="#根据签名查找需要的标签" class="headerlink" title="根据签名查找需要的标签"></a>根据签名查找需要的标签</h2><pre><code>1  2  3  4  5  </code></pre><p>| </p><pre><code>item_lists = root.findall('item') # 只能找到儿子, 不能找到孙子, 返回的是儿子们组成的列表  item = root.find('item') # 返回的是单个的儿子  print(root)  print(item_lists)  print(item)    </code></pre><p>—|—  </p><h2 id="获取叶子节点的值"><a href="#获取叶子节点的值" class="headerlink" title="获取叶子节点的值"></a>获取叶子节点的值</h2><h2 id="当访问到叶子节点时-就可以利用-text-来得到相应的标签了"><a href="#当访问到叶子节点时-就可以利用-text-来得到相应的标签了" class="headerlink" title="当访问到叶子节点时, 就可以利用 text 来得到相应的标签了"></a>当访问到叶子节点时, 就可以利用 text 来得到相应的标签了</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  </code></pre><p>| </p><pre><code>obj_bbox_set =[]  objects = root.findall('object')  for obj in objects:      obj_name = obj.find('name').text      bbox = obj.find('bndbox')      x1 = int(bbox.find('xmin').text)      y1 = int(bbox.find('ymin').text)      x2 = int(bbox.find('xmax').text)      y2 = int(bbox.find('ymax').text)      obj_bbox_set.append([x1, x2, y1, y2, obj_name])  print(obj_bbox_set)    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>矩形覆盖</title>
      <link href="/2019/09/07/%E7%9F%A9%E5%BD%A2%E8%A6%86%E7%9B%96/"/>
      <url>/2019/09/07/%E7%9F%A9%E5%BD%A2%E8%A6%86%E7%9B%96/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：363840<br>本题知识点： 递归  </p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>我们可以用2<em>1的小矩形横着或者竖着去覆盖更大的矩形。请问用n个2</em>1的小矩形无重叠地覆盖一个2*n的大矩形，总共有多少种方法？</p><pre><code>1  2  3  4  5  6  7  8  </code></pre><p>| </p><pre><code>class Solution {  public:      int rectCover(int number) {      if(number &lt;= 0) return 0;      if(number == 1 || number == 2) return number;      return rectCover(number - 1) + rectCover(number - 2);      }  };    </code></pre><p>—|—  </p><p>运行时间：538ms<br>占用内存：484k</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker使用笔记</title>
      <link href="/2019/09/06/20190906-docker%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"/>
      <url>/2019/09/06/20190906-docker%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="0-查看需要的Docker列表"><a href="#0-查看需要的Docker列表" class="headerlink" title="0. 查看需要的Docker列表"></a>0. 查看需要的Docker列表</h2><p><a href="https://hub.docker.com/r/nvidia/cuda/">https://hub.docker.com/r/nvidia/cuda/</a><br>列出已经存在的镜像列表</p><h2 id="1-创建容器"><a href="#1-创建容器" class="headerlink" title="1. 创建容器"></a>1. 创建容器</h2><p>(没有镜像, 会自动下载)docker run -it -v /media/data2/dh:/media/data2/dh —name=cloud —runtime=nvidia nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04 /bin/bash<br>(已有镜像)docker run -it -v /media/data2/dh:/media/data2/dh —name=cloud —runtime=nvidia afc5ab1e9a0d /bin/bash</p><h2 id="2-启动容器"><a href="#2-启动容器" class="headerlink" title="2. 启动容器"></a>2. 启动容器</h2><p>dockerstart cloud（容器名）</p><h2 id="3-进入容器"><a href="#3-进入容器" class="headerlink" title="3. 进入容器"></a>3. 进入容器</h2><p>docker exec -it cloud /bin/bash</p><h2 id="4-离开容器"><a href="#4-离开容器" class="headerlink" title="4. 离开容器"></a>4. 离开容器</h2><p>exit</p><h2 id="5-删除容器"><a href="#5-删除容器" class="headerlink" title="5. 删除容器"></a>5. 删除容器</h2><pre><code>A. 停止容器docker stop CONTAINER_IDB. 删除容器docker rm CONTAINER_ID</code></pre><p>启动 systemctl start docker</p><p>守护进程重启 sudo systemctl daemon-reload</p><p>重启docker服务 systemctl restart docker</p><p>重启docker服务 sudo service docker restart </p><p>关闭docker service docker stop</p><p>关闭docker systemctl stop docker </p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mx-maskrcnn环境搭建</title>
      <link href="/2019/09/06/20190906-mx-maskrcnn%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
      <url>/2019/09/06/20190906-mx-maskrcnn%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<p>从<a href="https://hub.docker.com/">https://hub.docker.com/</a> 选取所需的镜像</p><p>下载caffe2 镜像<br>docker pull caffe2/caffe2 :snapshot-py2-cuda8.0-cudnn7-ubuntu16.04<br>Cannot open your terminal ‘/dev/pts/0’错误原因解决<br>可以使用script命令来记录这个终端会话,执行<br>script /dev/null<br>screen -S caius<br>docker 分配<br><a href="http://www.cnblogs.com/codeaaa/p/9041533.html">http://www.cnblogs.com/codeaaa/p/9041533.html</a><br><a href="https://blog.csdn.net/u013948858/article/details/78429954%EF%BC%88%E6%9C%89%E6%95%88%EF%BC%89">https://blog.csdn.net/u013948858/article/details/78429954（有效）</a><br>docker run -it -v /media/:/media/ —name=mxcaius —runtime=nvidia 89f57a4ade86 /bin/bash<br>docker ubuntu源卡主，解决措施：<br>mv source<br>改完之后改回去<br>mv sources.list.d.odd sources.list.d<br>需要BLAS库，可以安装ATLAS、OpenBLAS、MKL，我安装的是atlas<br>sudo apt-get install libatlas-base-dev<br>安装opencv库<br>pip install opencv-python<br>sudo apt-get install libopencv-dev<br>安装Python包<br>cd python;<br>python setup.py install<br>apt-get install python-numpy<br>odules/imgproc/src/resize.cpp:3596: error: (-215:Assertion failed) func != 0 in function ‘resize’</p><p>numpy 1.14<br>setuptools和numpy(sudo apt-get install python-numpy)</p><p>git clone —recursive <a href="https://github.com/apache/incubator-mxnet.git">https://github.com/apache/incubator-mxnet.git</a> incubator-mxnet —branch 0.11.0<br>cp rcnn/CXX_OP/* incubator-mxnet/src/operator/<br>cd incubator-mxnet<br>make -j USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1<br>cd ..<br>make<br>bash scripts/train_alternate.sh<br>make<br>caius@sugon:~$ echo -n “cvlab1205” |md5sum<br>d0599e86d6134fee87bcf017ddca1990</p><p>然后我们使用docker ps查看到该容器信息，接下来就使用docker attach进入该容器<br>可形变卷积</p><p>IndexError: list index out of range </p><p>self.class_id = [0, 1] </p><p>imdb = eval(dataset)(image_set, root_path, dataset_path)</p><p>[‘train’]imagesetIcdar2015/media/data1/caius/dataset<br>[‘train’]imagesetIcdar2015/media/data1/caius/dataset</p><p>icdar2015_train gt roidb loaded from model/res50-fpn/icdar2015/alternate/cache/icdar2015_train_gt_roidb.pkl<br>OpenCV Error: Assertion failed (func != 0) in resize, file /io/opencv/modules/imgproc/src/imgwarp.cpp, line 3370<br>Traceback (most recent call last):<br>File “train_alternate_mask_fpn.py”, line 118, in<br>main()<br>File “train_alternate_mask_fpn.py”, line 115, in main<br>args.rcnn_epoch, args.rcnn_lr, args.rcnn_lr_step)<br>File “train_alternate_mask_fpn.py”, line 61, in alternate_train<br>vis=False, shuffle=False, thresh=0)<br>File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/tools/test_rpn.py”, line 63, in test_rpn<br>imdb_boxes = generate_proposals(predictor, test_data, imdb, vis=vis, thresh=thresh)<br>File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/core/tester.py”, line 61, in generate_proposals<br>for im_info, data_batch in test_data:<br>File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/core/loader.py”, line 60, in next<br>self.get_batch()<br>File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/core/loader.py”, line 83, in get_batch<br>data, label, im_info = get_rpn_testbatch(roidb)<br>File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/io/rpn.py”, line 32, in get_rpn_testbatch<br>imgs, roidb,masks = get_image(roidb)<br>File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/io/image.py”, line 99, in get_image<br>mask, _ = resize(mask, target_size, max_size)<br>File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/io/image.py”, line 138, in resize<br>im = cv2.resize(im, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR)<br>cv2.error: /io/opencv/modules/imgproc/src/imgwarp.cpp:3370: error: (-215) func != 0 in function resize</p><p>root@d59236d7a683:/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss# </p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>变态跳台阶</title>
      <link href="/2019/09/06/20190906-%E5%8F%98%E6%80%81%E8%B7%B3%E5%8F%B0%E9%98%B6/"/>
      <url>/2019/09/06/20190906-%E5%8F%98%E6%80%81%E8%B7%B3%E5%8F%B0%E9%98%B6/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：396047<br>本题知识点： 递归</p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。</p><h2 id="解析"><a href="#解析" class="headerlink" title="解析"></a>解析</h2><p>f(n) = f(n-1)+f(n-2)+…+f(1)<br>f(n-1) = f(n-2)+f(n-3)+…f(1)</p><p>f(n) = 2*f(n-1)</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> {</span><br><span class="line">public:</span><br><span class="line">    <span class="built_in">int</span> jumpFloorII(<span class="built_in">int</span> number) {</span><br><span class="line">        <span class="keyword">if</span>(number == <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> <span class="number">2</span>*jumpFloorII(number-<span class="number">1</span>);</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><p>运行时间：4ms<br>占用内存：480k</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>docker使用笔记</title>
      <link href="/2019/09/06/docker%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/"/>
      <url>/2019/09/06/docker%E4%BD%BF%E7%94%A8%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h2 id="0-查看需要的Docker列表"><a href="#0-查看需要的Docker列表" class="headerlink" title="0. 查看需要的Docker列表"></a>0. 查看需要的Docker列表</h2><p><a href="https://hub.docker.com/r/nvidia/cuda/">https://hub.docker.com/r/nvidia/cuda/</a><br>列出已经存在的镜像列表</p><h2 id="1-创建容器"><a href="#1-创建容器" class="headerlink" title="1. 创建容器"></a>1. 创建容器</h2><p>(没有镜像, 会自动下载)docker run -it -v /media/data2/dh:/media/data2/dh —name=cloud —runtime=nvidia nvidia/cuda:8.0-cudnn5-devel-ubuntu16.04 /bin/bash<br>(已有镜像)docker run -it -v /media/data2/dh:/media/data2/dh —name=cloud —runtime=nvidia afc5ab1e9a0d /bin/bash</p><h2 id="2-启动容器"><a href="#2-启动容器" class="headerlink" title="2. 启动容器"></a>2. 启动容器</h2><p>dockerstart cloud（容器名）</p><h2 id="3-进入容器"><a href="#3-进入容器" class="headerlink" title="3. 进入容器"></a>3. 进入容器</h2><p>docker exec -it cloud /bin/bash</p><h2 id="4-离开容器"><a href="#4-离开容器" class="headerlink" title="4. 离开容器"></a>4. 离开容器</h2><p>exit</p><h2 id="5-删除容器"><a href="#5-删除容器" class="headerlink" title="5. 删除容器"></a>5. 删除容器</h2><pre><code>A. 停止容器docker stop CONTAINER_IDB. 删除容器docker rm CONTAINER_ID</code></pre><p>启动 systemctl start docker</p><p>守护进程重启 sudo systemctl daemon-reload</p><p>重启docker服务 systemctl restart docker</p><p>重启docker服务 sudo service docker restart </p><p>关闭docker service docker stop</p><p>关闭docker systemctl stop docker </p>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Docker </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>mx-maskrcnn环境搭建</title>
      <link href="/2019/09/06/mx-maskrcnn%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/"/>
      <url>/2019/09/06/mx-maskrcnn%E7%8E%AF%E5%A2%83%E6%90%AD%E5%BB%BA/</url>
      
        <content type="html"><![CDATA[<p>从<a href="https://hub.docker.com/">https://hub.docker.com/</a> 选取所需的镜像</p><p>下载caffe2 镜像<br>docker pull caffe2/caffe2 :snapshot-py2-cuda8.0-cudnn7-ubuntu16.04<br>Cannot open your terminal ‘/dev/pts/0’错误原因解决<br>可以使用script命令来记录这个终端会话,执行<br>script /dev/null<br>screen -S caius<br>docker 分配<br><a href="http://www.cnblogs.com/codeaaa/p/9041533.html">http://www.cnblogs.com/codeaaa/p/9041533.html</a><br><a href="https://blog.csdn.net/u013948858/article/details/78429954%EF%BC%88%E6%9C%89%E6%95%88%EF%BC%89">https://blog.csdn.net/u013948858/article/details/78429954（有效）</a><br>docker run -it -v /media/:/media/ —name=mxcaius —runtime=nvidia 89f57a4ade86 /bin/bash<br>docker ubuntu源卡主，解决措施：<br>mv source<br>改完之后改回去<br>mv sources.list.d.odd sources.list.d<br>需要BLAS库，可以安装ATLAS、OpenBLAS、MKL，我安装的是atlas<br>sudo apt-get install libatlas-base-dev<br>安装opencv库<br>pip install opencv-python<br>sudo apt-get install libopencv-dev<br>安装Python包<br>cd python;<br>python setup.py install<br>apt-get install python-numpy<br>odules/imgproc/src/resize.cpp:3596: error: (-215:Assertion failed) func != 0 in function ‘resize’</p><p>numpy 1.14<br>setuptools和numpy(sudo apt-get install python-numpy)</p><p>git clone —recursive <a href="https://github.com/apache/incubator-mxnet.git">https://github.com/apache/incubator-mxnet.git</a> incubator-mxnet —branch 0.11.0<br>cp rcnn/CXX_OP/* incubator-mxnet/src/operator/<br>cd incubator-mxnet<br>make -j USE_BLAS=openblas USE_CUDA=1 USE_CUDA_PATH=/usr/local/cuda USE_CUDNN=1<br>cd ..<br>make<br>bash scripts/train_alternate.sh<br>make<br>caius@sugon:~$ echo -n “cvlab1205” |md5sum<br>d0599e86d6134fee87bcf017ddca1990</p><p>然后我们使用docker ps查看到该容器信息，接下来就使用docker attach进入该容器<br>可形变卷积</p><p>IndexError: list index out of range </p><p>self.class_id = [0, 1] </p><p>imdb = eval(dataset)(image_set, root_path, dataset_path)</p><p>[‘train’]imagesetIcdar2015/media/data1/caius/dataset<br>[‘train’]imagesetIcdar2015/media/data1/caius/dataset</p><p>icdar2015_train gt roidb loaded from model/res50-fpn/icdar2015/alternate/cache/icdar2015_train_gt_roidb.pkl<br>OpenCV Error: Assertion failed (func != 0) in resize, file /io/opencv/modules/imgproc/src/imgwarp.cpp, line 3370<br>Traceback (most recent call last):<br>File “train_alternate_mask_fpn.py”, line 118, in<br>main()<br>File “train_alternate_mask_fpn.py”, line 115, in main<br>args.rcnn_epoch, args.rcnn_lr, args.rcnn_lr_step)<br>File “train_alternate_mask_fpn.py”, line 61, in alternate_train<br>vis=False, shuffle=False, thresh=0)<br>File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/tools/test_rpn.py”, line 63, in test_rpn<br>imdb_boxes = generate_proposals(predictor, test_data, imdb, vis=vis, thresh=thresh)<br>File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/core/tester.py”, line 61, in generate_proposals<br>for im_info, data_batch in test_data:<br>File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/core/loader.py”, line 60, in next<br>self.get_batch()<br>File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/core/loader.py”, line 83, in get_batch<br>data, label, im_info = get_rpn_testbatch(roidb)<br>File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/io/rpn.py”, line 32, in get_rpn_testbatch<br>imgs, roidb,masks = get_image(roidb)<br>File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/io/image.py”, line 99, in get_image<br>mask, _ = resize(mask, target_size, max_size)<br>File “/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss/rcnn/io/image.py”, line 138, in resize<br>im = cv2.resize(im, None, None, fx=im_scale, fy=im_scale, interpolation=cv2.INTER_LINEAR)<br>cv2.error: /io/opencv/modules/imgproc/src/imgwarp.cpp:3370: error: (-215) func != 0 in function resize</p><p>root@d59236d7a683:/media/data1/caius/mx-maskrcnn-original-std-broadcast-2-maskloss/mx-maskrcnn-original-std-broadcast-2-maskloss# </p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>变态跳台阶</title>
      <link href="/2019/09/06/%E5%8F%98%E6%80%81%E8%B7%B3%E5%8F%B0%E9%98%B6/"/>
      <url>/2019/09/06/%E5%8F%98%E6%80%81%E8%B7%B3%E5%8F%B0%E9%98%B6/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：396047<br>本题知识点： 递归</p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>一只青蛙一次可以跳上1级台阶，也可以跳上2级……它也可以跳上n级。求该青蛙跳上一个n级的台阶总共有多少种跳法。</p><h2 id="解析"><a href="#解析" class="headerlink" title="解析"></a>解析</h2><p>f(n) = f(n-1)+f(n-2)+…+f(1)<br>f(n-1) = f(n-2)+f(n-3)+…f(1)</p><p>f(n) = 2*f(n-1)</p><pre><code>1  2  3  4  5  6  7  8  9  </code></pre><p>| </p><pre><code>class Solution {  public:      int jumpFloorII(int number) {          if(number == 1)              return 1;          else               return 2*jumpFloorII(number-1);      }  };    </code></pre><p>—|—  </p><p>运行时间：4ms<br>占用内存：480k</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Terminal 手册</title>
      <link href="/2019/09/05/20190905-cmd%E6%89%8B%E5%86%8C/"/>
      <url>/2019/09/05/20190905-cmd%E6%89%8B%E5%86%8C/</url>
      
        <content type="html"><![CDATA[<h2 id="查看磁盘空间"><a href="#查看磁盘空间" class="headerlink" title="查看磁盘空间"></a>查看磁盘空间</h2><p>df -hl 查看磁盘剩余空间<br>df -h 查看每个根路径的分区大小</p><h2 id="查看文件-x2F-文件夹大小"><a href="#查看文件-x2F-文件夹大小" class="headerlink" title="查看文件/文件夹大小"></a>查看文件/文件夹大小</h2><p>查看指定文件/文件夹大小：du -hs &lt;文件名或文件夹名&gt;<br>查看当前文件夹下所有文件大小（包括子文件夹）：du -sh</p><h2 id="查看文件数量"><a href="#查看文件数量" class="headerlink" title="查看文件数量"></a>查看文件数量</h2><p>统计当前目录下文件的个数（不包括目录）<br>ls -l | grep “^-“ | wc -l</p><h2 id="ls-命令"><a href="#ls-命令" class="headerlink" title="ls 命令"></a>ls 命令</h2><p>ls -a 显示全部的文件及文件夹，包括隐藏的文件或文件夹<br>ls -l 显示较全的文件信息，包括权限、用户、用户组等。<br>ls —color 显示文件及文件夹，并标有不同的颜色。</p><h2 id="tab键"><a href="#tab键" class="headerlink" title="tab键"></a>tab键</h2><p>tab command 用于当你的命令记不全时，输入一部再按一下进行补全，如果有多个前面部分相同命令，则按两次tab键</p><h2 id="cmp"><a href="#cmp" class="headerlink" title="cmp"></a>cmp</h2><p>cmp /bin/ls /bin/dir 用于比较两个文件是否是完全相同的。</p><h2 id="cp"><a href="#cp" class="headerlink" title="cp"></a>cp</h2><p>cp /bin/ls /bin/a 用于复制文件的命令。这时就复制了一个命令文件，就可以运行a命令，与ls用法相同<br>cp命令与操作文件一样是用来复制的，带r表示将其子目录一起复制。</p><h2 id="mv"><a href="#mv" class="headerlink" title="mv"></a>mv</h2><p>格式：mv /home/user1/桌面/ruijie/xrgsu /usr/share/local/bin/xrgsu</p><h2 id="chmod"><a href="#chmod" class="headerlink" title="chmod"></a>chmod</h2><p>用于改为用户对于文件的操作权限。chmod 0+r 添加读的权限。sudo chmod 0-r filename取消读的权限。</p><h2 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h2><ul><li>cat 命令就是用于查看ubuntu中文本文件的内容的命令。</li><li>cat /proc/cpuinfo 用于查看计算机的cpu信息。</li><li>cat /proc/meminfo 用于查看计算机的内在信息。</li><li>cat /etc/issue 查看ubuntu的版本信息。</li><li>查看cuda 版本 cat /usr/local/cuda/version.txt</li><li>查看cudnn 版本 cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</li></ul><h2 id="touch"><a href="#touch" class="headerlink" title="touch"></a>touch</h2><p>格式：touch test1 test2 test3<br>touch命令用于创建文件，可以同一时间创建多个文件。</p><h2 id="pwd"><a href="#pwd" class="headerlink" title="pwd"></a>pwd</h2><p>pwd命令是用来指出当前所在的路径。是print working directory的缩写</p>]]></content>
      
      
      <categories>
          
          <category> 工具 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 工具 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>跳台阶</title>
      <link href="/2019/09/05/20190905-%E8%B7%B3%E5%8F%B0%E9%98%B6/"/>
      <url>/2019/09/05/20190905-%E8%B7%B3%E5%8F%B0%E9%98%B6/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：489122<br>本题知识点： 递归</p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> {</span><br><span class="line">public:</span><br><span class="line">    <span class="built_in">int</span> jumpFloor(<span class="built_in">int</span> number) {</span><br><span class="line">        <span class="keyword">if</span>(number == <span class="number">1</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">else</span> <span class="keyword">if</span>(number==<span class="number">2</span>)</span><br><span class="line">            <span class="keyword">return</span> <span class="number">2</span>;</span><br><span class="line">        <span class="keyword">else</span></span><br><span class="line">            <span class="keyword">return</span> jumpFloor(number-<span class="number">1</span>)+jumpFloor(number-<span class="number">2</span>);</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><p>运行时间：519 ms<br>占用内存：456K</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Terminal 手册</title>
      <link href="/2019/09/05/cmd%E6%89%8B%E5%86%8C/"/>
      <url>/2019/09/05/cmd%E6%89%8B%E5%86%8C/</url>
      
        <content type="html"><![CDATA[<h2 id="查看磁盘空间"><a href="#查看磁盘空间" class="headerlink" title="查看磁盘空间"></a>查看磁盘空间</h2><p>df -hl 查看磁盘剩余空间<br>df -h 查看每个根路径的分区大小</p><h2 id="查看文件-x2F-文件夹大小"><a href="#查看文件-x2F-文件夹大小" class="headerlink" title="查看文件/文件夹大小"></a>查看文件/文件夹大小</h2><p>查看指定文件/文件夹大小：du -hs &lt;文件名或文件夹名&gt;<br>查看当前文件夹下所有文件大小（包括子文件夹）：du -sh</p><h2 id="查看文件数量"><a href="#查看文件数量" class="headerlink" title="查看文件数量"></a>查看文件数量</h2><p>统计当前目录下文件的个数（不包括目录）<br>ls -l | grep “^-“ | wc -l</p><h2 id="ls-命令"><a href="#ls-命令" class="headerlink" title="ls 命令"></a>ls 命令</h2><p>ls -a 显示全部的文件及文件夹，包括隐藏的文件或文件夹<br>ls -l 显示较全的文件信息，包括权限、用户、用户组等。<br>ls —color 显示文件及文件夹，并标有不同的颜色。</p><h2 id="tab键"><a href="#tab键" class="headerlink" title="tab键"></a>tab键</h2><p>tab command 用于当你的命令记不全时，输入一部再按一下进行补全，如果有多个前面部分相同命令，则按两次tab键</p><h2 id="cmp"><a href="#cmp" class="headerlink" title="cmp"></a>cmp</h2><p>cmp /bin/ls /bin/dir 用于比较两个文件是否是完全相同的。</p><h2 id="cp"><a href="#cp" class="headerlink" title="cp"></a>cp</h2><p>cp /bin/ls /bin/a 用于复制文件的命令。这时就复制了一个命令文件，就可以运行a命令，与ls用法相同<br>cp命令与操作文件一样是用来复制的，带r表示将其子目录一起复制。</p><h2 id="mv"><a href="#mv" class="headerlink" title="mv"></a>mv</h2><p>格式：mv /home/user1/桌面/ruijie/xrgsu /usr/share/local/bin/xrgsu</p><h2 id="chmod"><a href="#chmod" class="headerlink" title="chmod"></a>chmod</h2><p>用于改为用户对于文件的操作权限。chmod 0+r 添加读的权限。sudo chmod 0-r filename取消读的权限。</p><h2 id="cat"><a href="#cat" class="headerlink" title="cat"></a>cat</h2><ul><li>cat 命令就是用于查看ubuntu中文本文件的内容的命令。</li><li>cat /proc/cpuinfo 用于查看计算机的cpu信息。</li><li>cat /proc/meminfo 用于查看计算机的内在信息。</li><li>cat /etc/issue 查看ubuntu的版本信息。</li><li>查看cuda 版本 cat /usr/local/cuda/version.txt</li><li>查看cudnn 版本 cat /usr/local/cuda/include/cudnn.h | grep CUDNN_MAJOR -A 2</li></ul><h2 id="touch"><a href="#touch" class="headerlink" title="touch"></a>touch</h2><p>格式：touch test1 test2 test3<br>touch命令用于创建文件，可以同一时间创建多个文件。</p><h2 id="pwd"><a href="#pwd" class="headerlink" title="pwd"></a>pwd</h2><p>pwd命令是用来指出当前所在的路径。是print working directory的缩写</p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>跳台阶</title>
      <link href="/2019/09/05/%E8%B7%B3%E5%8F%B0%E9%98%B6/"/>
      <url>/2019/09/05/%E8%B7%B3%E5%8F%B0%E9%98%B6/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：489122<br>本题知识点： 递归</p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>一只青蛙一次可以跳上1级台阶，也可以跳上2级。求该青蛙跳上一个n级的台阶总共有多少种跳法（先后次序不同算不同的结果）。</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  </code></pre><p>| </p><pre><code>class Solution {  public:      int jumpFloor(int number) {          if(number == 1)              return 1;          else if(number==2)              return 2;          else              return jumpFloor(number-1)+jumpFloor(number-2);      }  };    </code></pre><p>—|—  </p><p>运行时间：519 ms<br>占用内存：456K</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>采坑记录</title>
      <link href="/2019/09/04/20190904-%E9%87%87%E5%9D%91%E8%AE%B0%E5%BD%95/"/>
      <url>/2019/09/04/20190904-%E9%87%87%E5%9D%91%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h3 id="os-getcwd"><a href="#os-getcwd" class="headerlink" title="os.getcwd()"></a>os.getcwd()</h3><p>该函数不需要传递参数，它返回当前的目录。需要说明的是，当前目录并不是指脚本所在的目录，而是所运行脚本的目录。就是说如果你在home的终端运行</p><h3 id="os-listdir-path"><a href="#os-listdir-path" class="headerlink" title="os.listdir(path)"></a>os.listdir(path)</h3><p>其参数path 为要获得内容目录的路径</p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 其他 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>斐波那契数列</title>
      <link href="/2019/09/04/20190904-%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/"/>
      <url>/2019/09/04/20190904-%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：640784<br>本题知识点： 递归</p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项（从0开始，第0项为0）。<br>n&lt;=39</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> {</span><br><span class="line">public:</span><br><span class="line">    <span class="built_in">int</span> Fibonacci(<span class="built_in">int</span> n) {</span><br><span class="line">        <span class="keyword">if</span>(n &lt;= <span class="number">0</span>) <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">if</span>(n==<span class="number">1</span> || n==<span class="number">2</span>) <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">        <span class="keyword">return</span> Fibonacci(n-<span class="number">1</span>)+Fibonacci(n-<span class="number">2</span>);</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><p>运行时间：694 ms<br>占用内存：484K</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>斐波那契数列</title>
      <link href="/2019/09/04/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/"/>
      <url>/2019/09/04/%E6%96%90%E6%B3%A2%E9%82%A3%E5%A5%91%E6%95%B0%E5%88%97/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：640784<br>本题知识点： 递归</p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>大家都知道斐波那契数列，现在要求输入一个整数n，请你输出斐波那契数列的第n项（从0开始，第0项为0）。<br>n&lt;=39</p><pre><code>1  2  3  4  5  6  7  8  </code></pre><p>| </p><pre><code>class Solution {  public:      int Fibonacci(int n) {          if(n &lt;= 0) return 0;          if(n==1 || n==2) return 1;          return Fibonacci(n-1)+Fibonacci(n-2);      }  };    </code></pre><p>—|—  </p><p>运行时间：694 ms<br>占用内存：484K</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>采坑记录</title>
      <link href="/2019/09/04/%E9%87%87%E5%9D%91%E8%AE%B0%E5%BD%95/"/>
      <url>/2019/09/04/%E9%87%87%E5%9D%91%E8%AE%B0%E5%BD%95/</url>
      
        <content type="html"><![CDATA[<h3 id="os-getcwd"><a href="#os-getcwd" class="headerlink" title="os.getcwd()"></a>os.getcwd()</h3><p>该函数不需要传递参数，它返回当前的目录。需要说明的是，当前目录并不是指脚本所在的目录，而是所运行脚本的目录。就是说如果你在home的终端运行</p><h3 id="os-listdir-path"><a href="#os-listdir-path" class="headerlink" title="os.listdir(path)"></a>os.listdir(path)</h3><p>其参数path 为要获得内容目录的路径</p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>遍历文件夹图片</title>
      <link href="/2019/09/03/20190903-%E9%81%8D%E5%8E%86%E6%96%87%E4%BB%B6%E5%A4%B9%E5%9B%BE%E7%89%87/"/>
      <url>/2019/09/03/20190903-%E9%81%8D%E5%8E%86%E6%96%87%E4%BB%B6%E5%A4%B9%E5%9B%BE%E7%89%87/</url>
      
        <content type="html"><![CDATA[<h2 id="题目要求"><a href="#题目要求" class="headerlink" title="题目要求"></a>题目要求</h2><p>要求从下图所示的文件夹遍历子文件夹，并将所在的图片路径保存到txt文件中  </p><h2 id="代码如下"><a href="#代码如下" class="headerlink" title="代码如下"></a>代码如下</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os, glob</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line">root = <span class="string">'/media/data1/lesson/pokemon/pokeman'</span></span><br><span class="line">f_w = <span class="built_in">open</span>(os.path.join(<span class="string">'../'</span>, <span class="string">'train2.txt'</span>), <span class="string">'w'</span>, encoding=<span class="string">'utf8'</span>)</span><br><span class="line">images = []</span><br><span class="line"><span class="keyword">for</span> name <span class="keyword">in</span> <span class="built_in">sorted</span>(os.listdir(os.path.join((root)))): <span class="comment"># 获取子文件夹</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> os.path.isdir(os.path.join(root,name)):</span><br><span class="line">        <span class="keyword">continue</span></span><br><span class="line">    images += glob.glob(os.path.join(root, name, <span class="string">'*.png'</span>))</span><br><span class="line">    images += glob.glob(os.path.join(root, name, <span class="string">'*.jpg'</span>))</span><br><span class="line">    images += glob.glob(os.path.join(root, name, <span class="string">'*.jpeg'</span>))</span><br><span class="line">random.shuffle(images)</span><br><span class="line"><span class="keyword">for</span> line <span class="keyword">in</span> images:</span><br><span class="line">    f_w.write(line + <span class="string">'\n'</span>)</span><br><span class="line">    <span class="comment"># f_w.write(line)</span></span><br><span class="line">f_w.close()</span><br><span class="line"><span class="built_in">print</span>(<span class="string">'Write Done!'</span>)</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 咖喱的代码</span></span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> glob</span><br><span class="line"><span class="keyword">import</span> os.path <span class="keyword">as</span> osp</span><br><span class="line">    dataset_dir = os.getcwd()</span><br><span class="line">    bulbasaur_dir = osp.join(dataset_dir, <span class="string">'pokeman/bulbasaur'</span>)</span><br><span class="line">    charmander_dir = osp.join(dataset_dir, <span class="string">'pokeman/charmander'</span>)</span><br><span class="line">    mewtwo_dir = osp.join(dataset_dir, <span class="string">'pokeman/mewtwo'</span>)</span><br><span class="line">    pikachu_dir = osp.join(dataset_dir, <span class="string">'pokeman/pikachu'</span>)</span><br><span class="line">    squirtle_dir = osp.join(dataset_dir, <span class="string">'pokeman/squirtle'</span>)</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">process</span>(<span class="params">dir_path, save_path</span>):</span><br><span class="line">    img_paths = glob.glob(osp.join(dir_path, <span class="string">'*.jpg'</span>))</span><br><span class="line">    f= <span class="built_in">open</span>(save_path, <span class="string">'w'</span>)</span><br><span class="line">    <span class="keyword">for</span> img_path <span class="keyword">in</span> img_paths:</span><br><span class="line">    f.write(img_path)</span><br><span class="line">    f.write(<span class="string">'\n'</span>)</span><br><span class="line">    f.close()</span><br><span class="line">dataset_dir = os.getcwd()</span><br><span class="line">bulbasaur_dir = osp.join(dataset_dir, <span class="string">'pokeman/bulbasaur'</span>)</span><br><span class="line">charmander_dir = osp.join(dataset_dir, <span class="string">'pokeman/charmander'</span>)</span><br><span class="line">mewtwo_dir = osp.join(dataset_dir, <span class="string">'pokeman/mewtwo'</span>)</span><br><span class="line">pikachu_dir = osp.join(dataset_dir, <span class="string">'pokeman/pikachu'</span>)</span><br><span class="line">squirtle_dir = osp.join(dataset_dir, <span class="string">'pokeman/squirtle'</span>)</span><br><span class="line">save_dir = osp.join(dataset_dir, <span class="string">'imgpath.txt'</span>)</span><br><span class="line">process(test_dir, save_dir)</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>遍历文件夹图片</title>
      <link href="/2019/09/03/%E9%81%8D%E5%8E%86%E6%96%87%E4%BB%B6%E5%A4%B9%E5%9B%BE%E7%89%87/"/>
      <url>/2019/09/03/%E9%81%8D%E5%8E%86%E6%96%87%E4%BB%B6%E5%A4%B9%E5%9B%BE%E7%89%87/</url>
      
        <content type="html"><![CDATA[<h2 id="题目要求"><a href="#题目要求" class="headerlink" title="题目要求"></a>题目要求</h2><p>要求从下图所示的文件夹遍历子文件夹，并将所在的图片路径保存到txt文件中  </p><h2 id="代码如下"><a href="#代码如下" class="headerlink" title="代码如下"></a>代码如下</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  </code></pre><p>| </p><pre><code>import os, glob  import random  root = '/media/data1/lesson/pokemon/pokeman'  f_w = open(os.path.join('../', 'train2.txt'), 'w', encoding='utf8')  images = []  for name in sorted(os.listdir(os.path.join((root)))): # 获取子文件夹      if not os.path.isdir(os.path.join(root,name)):          continue      images += glob.glob(os.path.join(root, name, '*.png'))      images += glob.glob(os.path.join(root, name, '*.jpg'))      images += glob.glob(os.path.join(root, name, '*.jpeg'))  random.shuffle(images)  for line in images:      f_w.write(line + '\n')      # f_w.write(line)  f_w.close()  print('Write Done!')    </code></pre><p>—|—  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  </code></pre><p>| </p><pre><code># 咖喱的代码  import os  import glob  import os.path as osp            dataset_dir = os.getcwd()      bulbasaur_dir = osp.join(dataset_dir, 'pokeman/bulbasaur')      charmander_dir = osp.join(dataset_dir, 'pokeman/charmander')      mewtwo_dir = osp.join(dataset_dir, 'pokeman/mewtwo')      pikachu_dir = osp.join(dataset_dir, 'pokeman/pikachu')      squirtle_dir = osp.join(dataset_dir, 'pokeman/squirtle')    def process(dir_path, save_path):      img_paths = glob.glob(osp.join(dir_path, '*.jpg'))        f= open(save_path, 'w')      for img_path in img_paths:          f.write(img_path)          f.write('\n')      f.close()      dataset_dir = os.getcwd()  bulbasaur_dir = osp.join(dataset_dir, 'pokeman/bulbasaur')  charmander_dir = osp.join(dataset_dir, 'pokeman/charmander')  mewtwo_dir = osp.join(dataset_dir, 'pokeman/mewtwo')  pikachu_dir = osp.join(dataset_dir, 'pokeman/pikachu')  squirtle_dir = osp.join(dataset_dir, 'pokeman/squirtle')    save_dir = osp.join(dataset_dir, 'imgpath.txt')    process(test_dir, save_dir)    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用两个栈实现队列</title>
      <link href="/2019/09/02/20190902-%E7%94%A8%E4%B8%A4%E4%B8%AA%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/"/>
      <url>/2019/09/02/20190902-%E7%94%A8%E4%B8%A4%E4%B8%AA%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：499320<br>本题知识点： 队列 栈  </p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span></span><br><span class="line">{</span><br><span class="line">public:</span><br><span class="line">    void push(<span class="built_in">int</span> node) {</span><br><span class="line">       <span class="keyword">while</span>(!stack2.empty())//入队时要保证 Stack2为空</span><br><span class="line">       {</span><br><span class="line">           stack1.push(stack2.top());</span><br><span class="line">           stack2.pop();</span><br><span class="line">       }</span><br><span class="line">        stack1.push(node);</span><br><span class="line">    }</span><br><span class="line">    <span class="built_in">int</span> pop() {</span><br><span class="line">        <span class="keyword">while</span>(!stack1.empty())//入队时要保证 Stack1为空</span><br><span class="line">       {</span><br><span class="line">           stack2.push(stack1.top());</span><br><span class="line">           stack1.pop();</span><br><span class="line">       }</span><br><span class="line">       <span class="built_in">int</span> temp = stack2.top();</span><br><span class="line">        stack2.pop();</span><br><span class="line">        <span class="keyword">return</span> temp;</span><br><span class="line">    }</span><br><span class="line">private:</span><br><span class="line">    stack&lt;<span class="built_in">int</span>&gt; stack1;</span><br><span class="line">    stack&lt;<span class="built_in">int</span>&gt; stack2;</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><p>运行时间：3ms<br>占用内存：460K</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>用两个栈实现队列</title>
      <link href="/2019/09/02/%E7%94%A8%E4%B8%A4%E4%B8%AA%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/"/>
      <url>/2019/09/02/%E7%94%A8%E4%B8%A4%E4%B8%AA%E6%A0%88%E5%AE%9E%E7%8E%B0%E9%98%9F%E5%88%97/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：499320<br>本题知识点： 队列 栈  </p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>用两个栈来实现一个队列，完成队列的Push和Pop操作。 队列中的元素为int类型。</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  </code></pre><p>| </p><pre><code>class Solution  {  public:      void push(int node) {         while(!stack2.empty())//入队时要保证 Stack2为空         {             stack1.push(stack2.top());             stack2.pop();         }          stack1.push(node);      }        int pop() {          while(!stack1.empty())//入队时要保证 Stack1为空         {             stack2.push(stack1.top());             stack1.pop();         }         int temp = stack2.top();          stack2.pop();          return temp;      }    private:      stack&lt;int&gt; stack1;      stack&lt;int&gt; stack2;  };    </code></pre><p>—|—  </p><p>运行时间：3ms<br>占用内存：460K</p>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从尾到头打印链表</title>
      <link href="/2019/08/29/20190829-%E4%BB%8E%E5%B0%BE%E5%88%B0%E5%A4%B4%E6%89%93%E5%8D%B0%E9%93%BE%E8%A1%A8/"/>
      <url>/2019/08/29/20190829-%E4%BB%8E%E5%B0%BE%E5%88%B0%E5%A4%B4%E6%89%93%E5%8D%B0%E9%93%BE%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：1022710<br>本题知识点： 链表  </p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>输入一个链表，按链表从尾到头的顺序返回一个ArrayList。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line">/**</span><br><span class="line">*  struct ListNode {</span><br><span class="line">*        <span class="built_in">int</span> val;</span><br><span class="line">*        struct ListNode *<span class="built_in">next</span>;</span><br><span class="line">*        ListNode(<span class="built_in">int</span> x) :</span><br><span class="line">*              val(x), <span class="built_in">next</span>(NULL) {</span><br><span class="line">*        }</span><br><span class="line">*  };</span><br><span class="line">*/</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> {</span><br><span class="line">public:</span><br><span class="line">    vector&lt;<span class="built_in">int</span>&gt; printListFromTailToHead(ListNode* head) {</span><br><span class="line">        vector&lt;<span class="built_in">int</span>&gt; value;</span><br><span class="line">        ListNode *p=NULL;</span><br><span class="line">        p=head;</span><br><span class="line">        stack&lt;<span class="built_in">int</span>&gt; stk;</span><br><span class="line">        <span class="keyword">while</span>(p!=NULL){</span><br><span class="line">            stk.push(p-&gt;val);</span><br><span class="line">            p=p-&gt;<span class="built_in">next</span>;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">while</span>(!stk.empty()){</span><br><span class="line">            value.push_back(stk.top());</span><br><span class="line">            stk.pop();</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">return</span> value;</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><p>运行时间：3ms<br>占用内存：480K</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>透视变换</title>
      <link href="/2019/08/29/20190829-%E9%80%8F%E8%A7%86%E5%8F%98%E6%8D%A2/"/>
      <url>/2019/08/29/20190829-%E9%80%8F%E8%A7%86%E5%8F%98%E6%8D%A2/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>替换空格</title>
      <link href="/2019/08/29/20190829-%E6%9B%BF%E6%8D%A2%E7%A9%BA%E6%A0%BC/"/>
      <url>/2019/08/29/20190829-%E6%9B%BF%E6%8D%A2%E7%A9%BA%E6%A0%BC/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：1142927<br>本题知识点： 字符串</p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> {</span><br><span class="line">public:</span><br><span class="line">void replaceSpace(char *<span class="built_in">str</span>,<span class="built_in">int</span> length) {</span><br><span class="line">           <span class="keyword">if</span>(<span class="built_in">str</span>==NULL)</span><br><span class="line">             <span class="keyword">return</span> ;</span><br><span class="line">         <span class="built_in">int</span> CountOfBlanks=<span class="number">0</span>;</span><br><span class="line">         <span class="built_in">int</span> Originallength=<span class="number">0</span>;</span><br><span class="line">         <span class="keyword">for</span>(<span class="built_in">int</span> i=<span class="number">0</span>;<span class="built_in">str</span>[i]!=<span class="string">'\0'</span>;i++)</span><br><span class="line">             {</span><br><span class="line">             Originallength++;</span><br><span class="line">             <span class="keyword">if</span>(<span class="built_in">str</span>[i]==<span class="string">' '</span>)</span><br><span class="line">                 ++CountOfBlanks;</span><br><span class="line">         }</span><br><span class="line">         <span class="built_in">int</span> <span class="built_in">len</span> =Originallength+<span class="number">2</span>*CountOfBlanks;</span><br><span class="line">         <span class="keyword">if</span>(<span class="built_in">len</span>+<span class="number">1</span>&gt;length)</span><br><span class="line">             <span class="keyword">return</span> ;</span><br><span class="line">          /*   <span class="built_in">int</span> pOrignallength=orignallength;</span><br><span class="line">               <span class="built_in">int</span> pNewlength=newlength;</span><br><span class="line">            <span class="keyword">while</span>(pOrignallength&gt;=<span class="number">0</span> &amp;&amp; pNewlength&gt;pOrignallength)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span>(<span class="built_in">str</span>[pOrignallength]==<span class="string">' '</span>)</span><br><span class="line">            {</span><br><span class="line">                <span class="built_in">str</span>[pNewlength--]=<span class="string">'0'</span>;</span><br><span class="line">                <span class="built_in">str</span>[pNewlength--]=<span class="string">'2'</span>;</span><br><span class="line">                <span class="built_in">str</span>[pNewlength--]=<span class="string">'%'</span>;</span><br><span class="line">            }</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">            {</span><br><span class="line">                <span class="built_in">str</span>[pNewlength--]=<span class="built_in">str</span>[pOrignallength];</span><br><span class="line">            }</span><br><span class="line">            pOrignallength--;</span><br><span class="line">        }</span><br><span class="line">          */</span><br><span class="line">         char*pStr1=<span class="built_in">str</span>+Originallength;//复制结束符‘\<span class="number">0</span>’</span><br><span class="line">         char*pStr2=<span class="built_in">str</span>+<span class="built_in">len</span>;</span><br><span class="line">        <span class="keyword">while</span>(pStr1&lt;pStr2)</span><br><span class="line">            {</span><br><span class="line">            <span class="keyword">if</span>(*pStr1==<span class="string">' '</span>)</span><br><span class="line">                {</span><br><span class="line">                *pStr2--=<span class="string">'0'</span>;</span><br><span class="line">                *pStr2--=<span class="string">'2'</span>;</span><br><span class="line">                *pStr2--=<span class="string">'%'</span>;</span><br><span class="line">            }</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">             {</span><br><span class="line">                 *pStr2--=*pStr1;</span><br><span class="line">            }</span><br><span class="line">            --pStr1;</span><br><span class="line">        }</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 技术 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 技术 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从尾到头打印链表</title>
      <link href="/2019/08/29/%E4%BB%8E%E5%B0%BE%E5%88%B0%E5%A4%B4%E6%89%93%E5%8D%B0%E9%93%BE%E8%A1%A8/"/>
      <url>/2019/08/29/%E4%BB%8E%E5%B0%BE%E5%88%B0%E5%A4%B4%E6%89%93%E5%8D%B0%E9%93%BE%E8%A1%A8/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：1022710<br>本题知识点： 链表  </p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>输入一个链表，按链表从尾到头的顺序返回一个ArrayList。</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  </code></pre><p>| </p><pre><code>/**  *  struct ListNode {  *        int val;  *        struct ListNode *next;  *        ListNode(int x) :  *              val(x), next(NULL) {  *        }  *  };  */  class Solution {  public:      vector&lt;int&gt; printListFromTailToHead(ListNode* head) {          vector&lt;int&gt; value;          ListNode *p=NULL;          p=head;          stack&lt;int&gt; stk;          while(p!=NULL){              stk.push(p-&gt;val);              p=p-&gt;next;          }          while(!stk.empty()){              value.push_back(stk.top());              stk.pop();          }          return value;      }    };    </code></pre><p>—|—  </p><p>运行时间：3ms<br>占用内存：480K</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>替换空格</title>
      <link href="/2019/08/29/%E6%9B%BF%E6%8D%A2%E7%A9%BA%E6%A0%BC/"/>
      <url>/2019/08/29/%E6%9B%BF%E6%8D%A2%E7%A9%BA%E6%A0%BC/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：1142927<br>本题知识点： 字符串</p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>请实现一个函数，将一个字符串中的每个空格替换成“%20”。例如，当字符串为We Are Happy.则经过替换之后的字符串为We%20Are%20Happy。</p><h2 id="代码"><a href="#代码" class="headerlink" title="代码"></a>代码</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  </code></pre><p>| </p><pre><code>class Solution {  public:      void replaceSpace(char *str,int length) {             if(str==NULL)               return ;           int CountOfBlanks=0;           int Originallength=0;           for(int i=0;str[i]!='\0';i++)               {               Originallength++;               if(str[i]==' ')                   ++CountOfBlanks;           }           int len =Originallength+2*CountOfBlanks;           if(len+1&gt;length)               return ;            /*   int pOrignallength=orignallength;                 int pNewlength=newlength;              while(pOrignallength&gt;=0 &amp;&amp; pNewlength&gt;pOrignallength)          {              if(str[pOrignallength]==' ')              {                  str[pNewlength--]='0';                  str[pNewlength--]='2';                  str[pNewlength--]='%';              }              else              {                  str[pNewlength--]=str[pOrignallength];              }              pOrignallength--;          }              */           char*pStr1=str+Originallength;//复制结束符‘\0’           char*pStr2=str+len;          while(pStr1&lt;pStr2)              {              if(*pStr1==' ')                  {                  *pStr2--='0';                  *pStr2--='2';                  *pStr2--='%';                  }              else               {                   *pStr2--=*pStr1;              }              --pStr1;          }      }  };    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>透视变换</title>
      <link href="/2019/08/29/%E9%80%8F%E8%A7%86%E5%8F%98%E6%8D%A2/"/>
      <url>/2019/08/29/%E9%80%8F%E8%A7%86%E5%8F%98%E6%8D%A2/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>广度/宽度优先搜索(BFS)</title>
      <link href="/2019/08/28/20190828-BFS/"/>
      <url>/2019/08/28/20190828-BFS/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>广度优先搜索 是最简单的图搜索算法之一， 也是许多重要的图算法的原型。Prime的最小生成树算法和Dijkstra的单源最短路径算法都使用了类似广度优先搜索的思想。<br>给定图G=(V,E) 和一个可以识别的<strong>源</strong> 节点 s，广度优先搜索对图G中的边进行系统性的探索来发现可以从源节点，到达所有的节点。该算法能够计算从源结点s到每个可到达的节点的距离(最小的边数)，同时生成一棵“广度优先搜索树”。该树以源结点s为根节点，包括所有可以从s到达的结点。对于每个从源结点s可以到达的结点v，在广度优先搜索树里从结点s到结点v的简单路径所对应的的就是图G中从结点s到结点v的“最短路径”，即包含最少边数的路径，该算法既可以用于有向图也可以用于无向图。<br>广度优先算法之所以如此得名是因为该算法始终是将已经发现的结点和未发现结点之间的边界，沿其广度方向向外扩展。也就是说，算法需要在发现所有距离源结点s为k的所有结点之后，才会发现距离源结点s为k+1的 其他结点。</p><h2 id="图的概念"><a href="#图的概念" class="headerlink" title="图的概念"></a>图的概念</h2><ul><li>图(graph) 是一种$\textcolor{Blue}{网状数据} $结构， 图是由非空的顶点集合和一个描述顶点之间的关系的集合组成。</li><li>图由顶点和边组成，顶点表示对象，边表示对象之间的连接关系。</li><li>边也可以带权值，称为带权值图。</li></ul><h3 id="无向图术语"><a href="#无向图术语" class="headerlink" title="无向图术语"></a>无向图术语</h3><ul><li>两个顶点之间如果有边连接，视为两个顶点相邻</li><li>相邻顶点间的序列称为路径</li><li>起点和终点重合的路径称为圈</li><li>顶点连接的边数叫做这个顶点的度</li></ul><hr><ul><li>没有圈的连通图，就是树</li><li>没有圈的非连通图，就是森林</li><li>一棵树的边数等于顶点数-1</li><li>边数等于顶点数-1 的连通图，就是树</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">BFS(G,s) \\</span><br><span class="line">for each vertex u \in G.V -{s}\\</span><br><span class="line">u.color = WHITE\\</span><br><span class="line">u.d = \infty\\</span><br><span class="line">u.\pi = NIL\\</span><br><span class="line">s.color = GRAY\\</span><br><span class="line">s.d = 0\\</span><br><span class="line">s.\pi = NIL\\</span><br><span class="line">Q = \emptyset\\</span><br><span class="line">ENQUEUE(Q,s)\\</span><br><span class="line">while Q\neq = \emptyset\\</span><br><span class="line">u = DEQUEUE(Q)\\</span><br><span class="line">for each v \in G.Adj[u]\\</span><br><span class="line">if v.color == WHITE\\</span><br><span class="line">v.color = GRAY\\</span><br><span class="line">v.d = u.d+1\\</span><br><span class="line">v.\pi = u\\</span><br><span class="line">ENQUEUE(Q,v)\\</span><br><span class="line">u.color = BLACK\\</span><br></pre></td></tr></tbody></table></figure><p>广度优先搜索的流程图<br><img src="/images/20190828_BFS_bfs.png"></p><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>POJ3984《迷宫问题》<br>定义一个二维数组：<br>int maze[5][5] = {<br>0, 1, 0, 0, 0,<br>0, 1, 0, 1, 0,<br>0, 0, 0, 0, 0,<br>0, 1, 1, 1, 0,<br>0, 0, 0, 1, 0,<br>};<br>它表示一个迷宫，其中的1表示墙壁，0表示可以走的路，只能横着走或竖着走，不能斜着走，要求编程序找出从左上角到右下角的最短路线。 </p><h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><p>队列是先进后出，后进先出。</p><hr><p>对应于题目的输入数组：<br>0, 1, 0, 0, 0,<br>0, 1, 0, 1, 0,<br>0, 0, 0, 0, 0,<br>0, 1, 1, 1, 0,<br>0, 0, 0, 1, 0,<br>把节点定义为(x,y)，(x,y)表示数组maze的项maze[x][y]。<br>于是起点就是(0,0)，终点是(4,4)。按照刚刚的思路，手工梳理一遍：<br>初始条件：<br>起点Vs为(0,0)<br>终点Vd为(4,4)<br>灰色节点集合Q={}<br>初始化所有节点为白色节点<br>下面开始广度优先搜索：<br>1.起始节点Vs变成灰色，加入队列Q，Q={(0,0)}<br>2.取出队列Q的头一个节点Vn，Vn={0,0}，Q={}<br>3.把Vn={0,0}染成黑色，取出Vn所有相邻的白色节点{(1,0)}<br>4.不包含终点(4,4)，染成灰色，加入队列Q，Q={(1,0)}<br>5.取出队列Q的头一个节点Vn，Vn={1,0}，Q={}<br>6.把Vn={1,0}染成黑色，取出Vn所有相邻的白色节点{(2,0)}<br>7.不包含终点(4,4)，染成灰色，加入队列Q，Q={(2,0)}<br>8.取出队列Q的头一个节点Vn，Vn={2,0}，Q={}<br>9.把Vn={2,0}染成黑色，取出Vn所有相邻的白色节点{(2,1), (3,0)}<br>10.不包含终点(4,4)，染成灰色，加入队列Q，Q={(2,1), (3,0)}<br>11.取出队列Q的头一个节点Vn，Vn={2,1}，Q={(3,0)}<br>12.把Vn={2,1}染成黑色，取出Vn所有相邻的白色节点{(2,2)}<br>13.不包含终点(4,4)，染成灰色，加入队列Q，Q={(3,0), (2,2)}<br>14.持续下去，知道Vn的所有相邻的白色节点中包含了(4,4)……<br>15.此时获得了答案<br><img src="/images/20190828_BFS_mgbfs.png"></p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br></pre></td><td class="code"><pre><span class="line">// BFS.cpp : 定义控制台应用程序的入口点。</span><br><span class="line">//</span><br><span class="line"><span class="comment">#include "stdafx.h"</span></span><br><span class="line"><span class="comment">#include &lt;iostream&gt;</span></span><br><span class="line">using namespace std;</span><br><span class="line"><span class="built_in">int</span> <span class="built_in">map</span>[<span class="number">5</span>][<span class="number">5</span>];</span><br><span class="line">//相邻四个节点</span><br><span class="line"><span class="built_in">int</span> borderUponX[<span class="number">4</span>] = { <span class="number">0</span>, <span class="number">0</span>, <span class="number">1</span>, -<span class="number">1</span> };</span><br><span class="line"><span class="built_in">int</span> borderUponY[<span class="number">4</span>] = { <span class="number">1</span>, -<span class="number">1</span>, <span class="number">0</span>, <span class="number">0</span> };</span><br><span class="line"><span class="built_in">int</span> front = <span class="number">0</span>, rear = <span class="number">1</span>;</span><br><span class="line">struct node {</span><br><span class="line"><span class="built_in">int</span> pre;</span><br><span class="line"><span class="built_in">int</span> x;</span><br><span class="line"><span class="built_in">int</span> y;</span><br><span class="line">} path[<span class="number">100</span>];</span><br><span class="line">void <span class="built_in">print</span>(<span class="built_in">int</span> i) {//当前节点</span><br><span class="line"><span class="keyword">if</span> (path[i].pre != -<span class="number">1</span>) {//找到前面那个节点</span><br><span class="line"><span class="built_in">print</span>(path[i].pre);</span><br><span class="line">cout &lt;&lt; <span class="string">"("</span> &lt;&lt; path[i].x &lt;&lt; <span class="string">","</span> &lt;&lt; path[i].y &lt;&lt; <span class="string">")"</span> &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"><span class="keyword">else</span> {//最前面的那个节点</span><br><span class="line">cout &lt;&lt; <span class="string">"("</span> &lt;&lt; path[i].x &lt;&lt; <span class="string">","</span> &lt;&lt; path[i].y &lt;&lt; <span class="string">")"</span> &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">void bfsSearch(<span class="built_in">int</span> x, <span class="built_in">int</span> y) {</span><br><span class="line">//开始节点（出发），前面没有节点了</span><br><span class="line">path[front].x = x;</span><br><span class="line">path[front].y = y;</span><br><span class="line">path[front].pre = -<span class="number">1</span>;</span><br><span class="line">//当front == rear的时候说明已经走完了所以“相邻”节点</span><br><span class="line">//且都不通</span><br><span class="line"><span class="keyword">while</span> (front &lt; rear) {</span><br><span class="line"><span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i != <span class="number">4</span>; i++) {</span><br><span class="line">//相邻节点坐标</span><br><span class="line"><span class="built_in">int</span> pathX = path[front].x + borderUponX[i];</span><br><span class="line"><span class="built_in">int</span> pathY = path[front].y + borderUponY[i];</span><br><span class="line">//不符合的节点（遇到边界或已经走过了）</span><br><span class="line"><span class="keyword">if</span> (pathY &lt; <span class="number">0</span> || pathX &lt; <span class="number">0</span> || pathX &gt; <span class="number">4</span> || pathY &gt; <span class="number">4</span> || <span class="built_in">map</span>[pathX][pathY])</span><br><span class="line"><span class="keyword">continue</span>;</span><br><span class="line"><span class="keyword">else</span> {//将front的相邻的可以过去的并且是还没有走过的节点加到路径里面</span><br><span class="line"><span class="built_in">map</span>[pathX][pathY] = <span class="number">1</span>;</span><br><span class="line">path[rear].x = pathX;</span><br><span class="line">path[rear].y = pathY;</span><br><span class="line">path[rear].pre = front;</span><br><span class="line">rear++;</span><br><span class="line">}</span><br><span class="line"><span class="keyword">if</span> (pathX == <span class="number">4</span> &amp;&amp; pathY == <span class="number">4</span>) {</span><br><span class="line">//找到了一条路径，又是第一次找到</span><br><span class="line">//那么就是最短路径了</span><br><span class="line"><span class="built_in">print</span>(rear - <span class="number">1</span>);</span><br><span class="line"><span class="keyword">break</span>;</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">front++;</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"><span class="built_in">int</span> main(<span class="built_in">int</span> argc, char const *argv[])</span><br><span class="line">{</span><br><span class="line"><span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++)</span><br><span class="line"><span class="keyword">for</span> (<span class="built_in">int</span> j = <span class="number">0</span>; j &lt; <span class="number">5</span>; j++)</span><br><span class="line">cin &gt;&gt; <span class="built_in">map</span>[i][j];</span><br><span class="line">bfsSearch(<span class="number">0</span>, <span class="number">0</span>);</span><br><span class="line">system(<span class="string">"pause"</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PIL模块</title>
      <link href="/2019/08/28/20190828-PIL%E6%A8%A1%E5%9D%97/"/>
      <url>/2019/08/28/20190828-PIL%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<h2 id="导入"><a href="#导入" class="headerlink" title="导入"></a>导入</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br></pre></td></tr></tbody></table></figure><h2 id="读取"><a href="#读取" class="headerlink" title="读取"></a>读取</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img = Image.open(filepath)</span><br></pre></td></tr></tbody></table></figure><h2 id="显示"><a href="#显示" class="headerlink" title="显示"></a>显示</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img.show()</span><br></pre></td></tr></tbody></table></figure><h2 id="与-numpy-数组的互相转换"><a href="#与-numpy-数组的互相转换" class="headerlink" title="与 numpy 数组的互相转换"></a>与 numpy 数组的互相转换</h2><p>PIL Image 转 numpy 数组  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img_to_array = np.array(img)</span><br></pre></td></tr></tbody></table></figure><p>numpy 数组转 PIL Image (注意要确保数组内的值符合 PIL 的要求)  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">array_to_img = Image.fromarray(img_to_array)</span><br></pre></td></tr></tbody></table></figure><h2 id="PIL-与-cv2-格式互相转换"><a href="#PIL-与-cv2-格式互相转换" class="headerlink" title="PIL 与 cv2 格式互相转换"></a>PIL 与 cv2 格式互相转换</h2><p>PIL.Image读入的图片数据类型不是 numpy 数组, 它的size属性为 (w, h), 利用np.array转换成 numpy 数组后, 它的通道顺序为 (r, g, b)  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># PIL to cv2</span></span><br><span class="line">pil_img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"><span class="built_in">print</span>(pil_img.size) <span class="comment"># (w, h)</span></span><br><span class="line">np_img = np.array(pil_img)</span><br><span class="line">cv2_img = np_img[:, :, ::-<span class="number">1</span>] <span class="comment"># 交换通道</span></span><br><span class="line"><span class="comment"># cv2 to PIL</span></span><br><span class="line">pil_img = Image.fromarray(cv2_img[:, :, ::-<span class="number">1</span>])</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch trick</title>
      <link href="/2019/08/28/20190828-Pytorch-trick/"/>
      <url>/2019/08/28/20190828-Pytorch-trick/</url>
      
        <content type="html"><![CDATA[<p>目录：   </p><ol><li>指定GPU编号</li><li>查看模型每层输出详情</li><li>梯度裁剪</li><li>扩展单张图片维度</li><li>独热编码</li><li>防止验证模型时爆显存</li><li>学习率衰减</li><li>冻结某些层的参数</li><li>对不同层使用不同学习率</li></ol><h2 id="1-指定GPU编号"><a href="#1-指定GPU编号" class="headerlink" title="1. 指定GPU编号"></a>1. 指定GPU编号</h2><ul><li>设置当前使用的GPU设备仅为0号设备，设备名称为 /gpu:0：os.environ[“CUDA_VISIBLE_DEVICES”] = “0”</li><li>设置当前使用的GPU设备为0,1号两个设备，名称依次为 /gpu:0、/gpu:1： os.environ[“CUDA_VISIBLE_DEVICES”] = “0,1” ，根据顺序表示优先使用0号设备,然后使用1号设备。<br>指定GPU的命令需要放在和神经网络相关的一系列操作的前面。</li></ul><h2 id="2-查看模型每层输出详情"><a href="#2-查看模型每层输出详情" class="headerlink" title="2.查看模型每层输出详情"></a>2.查看模型每层输出详情</h2><p>Keras有一个简洁的API来查看模型的每一层输出尺寸，这在调试网络时非常有用。现在在PyTorch中也可以实现这个功能。</p><p>使用很简单，如下用法：  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchsummary <span class="keyword">import</span> summary</span><br><span class="line">summary(your_model, input_size=(channels, H, W))</span><br></pre></td></tr></tbody></table></figure><p>input_size 是根据你自己的网络模型的输入尺寸进行设置。</p><h2 id="3-梯度裁剪（Gradient-Clipping）"><a href="#3-梯度裁剪（Gradient-Clipping）" class="headerlink" title="3.梯度裁剪（Gradient Clipping）"></a>3.梯度裁剪（Gradient Clipping）</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line">outputs = model(data)</span><br><span class="line">loss= loss_fn(outputs, target)</span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line">nn.utils.clip_grad_norm_(model.parameters(), max_norm=<span class="number">20</span>, norm_type=<span class="number">2</span>)</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></tbody></table></figure><p>nn.utils.clip_grad_norm_ 的参数：</p><ul><li>parameters – 一个基于变量的迭代器，会进行梯度归一化</li><li>max_norm – 梯度的最大范数</li><li>max_norm – 梯度的最大范数<br>知乎用户 不椭的椭圆 提出：梯度裁剪在某些任务上会额外消耗大量的计算时间，可移步评论区查看详情。</li></ul><h2 id="4、扩展单张图片维度"><a href="#4、扩展单张图片维度" class="headerlink" title="4、扩展单张图片维度"></a>4、扩展单张图片维度</h2><p>因为在训练时的数据维度一般都是 (batch_size, c, h, w)，而在测试时只输入一张图片，所以需要扩展维度，扩展维度有多个方法：  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">image = cv2.imread(img_path)</span><br><span class="line">image = torch.tensor(image)</span><br><span class="line"><span class="built_in">print</span>(image.size())</span><br><span class="line">img = image.view(<span class="number">1</span>, *image.size())</span><br><span class="line"><span class="built_in">print</span>(img.size())</span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># torch.Size([h, w, c])</span></span><br><span class="line"><span class="comment"># torch.Size([1, h, w, c])</span></span><br></pre></td></tr></tbody></table></figure><p>或者  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">image = cv2.imread(img_path)</span><br><span class="line"><span class="built_in">print</span>(image.shape)</span><br><span class="line">img = image[np.newaxis, :, :, :]</span><br><span class="line"><span class="built_in">print</span>(img.shape)</span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># (h, w, c)</span></span><br><span class="line"><span class="comment"># (1, h, w, c)</span></span><br></pre></td></tr></tbody></table></figure><p>或者  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">image = cv2.imread(img_path)</span><br><span class="line">image = torch.tensor(image)</span><br><span class="line"><span class="built_in">print</span>(image.size())</span><br><span class="line">img = image.unsqueeze(dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(img.size())</span><br><span class="line">img = img.squeeze(dim=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(img.size())</span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line"><span class="comment"># torch.Size([(h, w, c)])</span></span><br><span class="line"><span class="comment"># torch.Size([1, h, w, c])</span></span><br><span class="line"><span class="comment"># torch.Size([h, w, c])</span></span><br></pre></td></tr></tbody></table></figure><p>tensor.unsqueeze(dim)：扩展维度，dim指定扩展哪个维度。</p><p>tensor.squeeze(dim)：去除dim指定的且size为1的维度，维度大于1时，squeeze()不起作用，不指定dim时，去除所有size为1的维度。</p><h2 id="5-独热编码"><a href="#5-独热编码" class="headerlink" title="5.独热编码"></a>5.独热编码</h2><p>在PyTorch中使用交叉熵损失函数的时候会自动把label转化成onehot，所以不用手动转化，而使用MSE需要手动转化成onehot编码。  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line">class_num = <span class="number">8</span></span><br><span class="line">batch_size = <span class="number">4</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">one_hot</span>(<span class="params">label</span>):</span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    将一维列表转换为独热编码</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    label = label.resize_(batch_size, <span class="number">1</span>)</span><br><span class="line">    m_zeros = torch.zeros(batch_size, class_num)</span><br><span class="line">    <span class="comment"># 从 value 中取值，然后根据 dim 和 index 给相应位置赋值</span></span><br><span class="line">    onehot = m_zeros.scatter_(<span class="number">1</span>, label, <span class="number">1</span>)  <span class="comment"># (dim,index,value)</span></span><br><span class="line">    <span class="keyword">return</span> onehot.numpy()  <span class="comment"># Tensor -&gt; Numpy</span></span><br><span class="line">label = torch.LongTensor(batch_size).random_() % class_num  <span class="comment"># 对随机数取余</span></span><br><span class="line"><span class="built_in">print</span>(one_hot(label))</span><br><span class="line"><span class="comment"># output:</span></span><br><span class="line">[[<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]</span><br><span class="line"> [<span class="number">0.</span> <span class="number">1.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span> <span class="number">0.</span>]]</span><br></pre></td></tr></tbody></table></figure><h2 id="6-防止验证模型时爆显存"><a href="#6-防止验证模型时爆显存" class="headerlink" title="6. 防止验证模型时爆显存"></a>6. 防止验证模型时爆显存</h2><p>验证模型时不需要求导，即不需要梯度计算，关闭autograd，可以提高速度，节约内存。如果不关闭可能会爆显存。  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> torch.no_grad():</span><br><span class="line">    <span class="comment"># 使用model进行预测的代码</span></span><br><span class="line">    <span class="keyword">pass</span></span><br></pre></td></tr></tbody></table></figure><p>感谢知乎用户zhaz 的提醒，我把 torch.cuda.empty_cache() 的使用原因更新一下。</p><p>这是原回答：</p><p>Pytorch 训练时无用的临时变量可能会越来越多，导致 out of memory ，可以使用下面语句来清理这些不需要的变量。</p><p>官网 上的解释为：</p><p>Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.torch.cuda.empty_cache()</p><p>意思就是PyTorch的缓存分配器会事先分配一些固定的显存，即使实际上tensors并没有使用完这些显存，这些显存也不能被其他应用使用。这个分配过程由第一次CUDA内存访问触发的。</p><p>而 torch.cuda.empty_cache() 的作用就是释放缓存分配器当前持有的且未占用的缓存显存，以便这些显存可以被其他GPU应用程序中使用，并且通过 nvidia-smi命令可见。注意使用此命令不会释放tensors占用的显存。</p><p>对于不用的数据变量，Pytorch 可以自动进行回收从而释放相应的显存。</p><h2 id="7-学习率衰减"><a href="#7-学习率衰减" class="headerlink" title="7. 学习率衰减"></a>7. 学习率衰减</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"><span class="keyword">from</span> torch.optim <span class="keyword">import</span> lr_scheduler</span><br><span class="line"><span class="comment"># 训练前的初始化</span></span><br><span class="line">optimizer = optim.Adam(net.parameters(), lr=<span class="number">0.001</span>)</span><br><span class="line">scheduler = lr_scheduler.StepLR(optimizer, <span class="number">10</span>, <span class="number">0.1</span>)  <span class="comment"># # 每过10个epoch，学习率乘以0.1</span></span><br><span class="line"><span class="comment"># 训练过程中</span></span><br><span class="line"><span class="keyword">for</span> n <span class="keyword">in</span> n_epoch:</span><br><span class="line">    scheduler.step()</span><br><span class="line">    ...</span><br></pre></td></tr></tbody></table></figure><h2 id="8-冻结某些层的参数"><a href="#8-冻结某些层的参数" class="headerlink" title="8. 冻结某些层的参数"></a>8. 冻结某些层的参数</h2><p>在加载预训练模型的时候，我们有时想冻结前面几层，使其参数在训练过程中不发生变化。</p><p>我们需要先知道每一层的名字，通过如下代码打印：  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">net = Network()  <span class="comment"># 获取自定义网络结构</span></span><br><span class="line"><span class="keyword">for</span> name, value <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'name: {0}, grad: {1}'</span>.<span class="built_in">format</span>(name, value.requires_grad))</span><br></pre></td></tr></tbody></table></figure><p>假设前几层信息如下：  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">name: cnn.VGG_16.convolution1_1.weight, grad: True</span><br><span class="line">name: cnn.VGG_16.convolution1_1.bias, grad: True</span><br><span class="line">name: cnn.VGG_16.convolution1_2.weight, grad: True</span><br><span class="line">name: cnn.VGG_16.convolution1_2.bias, grad: True</span><br><span class="line">name: cnn.VGG_16.convolution2_1.weight, grad: True</span><br><span class="line">name: cnn.VGG_16.convolution2_1.bias, grad: True</span><br><span class="line">name: cnn.VGG_16.convolution2_2.weight, grad: True</span><br><span class="line">name: cnn.VGG_16.convolution2_2.bias, grad: True</span><br></pre></td></tr></tbody></table></figure><p>后面的True表示该层的参数可训练，然后我们定义一个要冻结的层的列表：  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">no_grad = [</span><br><span class="line">    'cnn.VGG_16.convolution1_1.weight',</span><br><span class="line">    'cnn.VGG_16.convolution1_1.bias',</span><br><span class="line">    'cnn.VGG_16.convolution1_2.weight',</span><br><span class="line">    'cnn.VGG_16.convolution1_2.bias'</span><br><span class="line">]</span><br></pre></td></tr></tbody></table></figure><p>冻结方法如下：  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">net = Net.CTPN()  # 获取网络结构</span><br><span class="line">for name, value in net.named_parameters():</span><br><span class="line">    if name in no_grad:</span><br><span class="line">        value.requires_grad = False</span><br><span class="line">    else:</span><br><span class="line">        value.requires_grad = True</span><br></pre></td></tr></tbody></table></figure><p>冻结后我们再打印每层的信息：  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">name: cnn.VGG_16.convolution1_1.weight, grad: False</span><br><span class="line">name: cnn.VGG_16.convolution1_1.bias, grad: False</span><br><span class="line">name: cnn.VGG_16.convolution1_2.weight, grad: False</span><br><span class="line">name: cnn.VGG_16.convolution1_2.bias, grad: False</span><br><span class="line">name: cnn.VGG_16.convolution2_1.weight, grad: True</span><br><span class="line">name: cnn.VGG_16.convolution2_1.bias, grad: True</span><br><span class="line">name: cnn.VGG_16.convolution2_2.weight, grad: True</span><br><span class="line">name: cnn.VGG_16.convolution2_2.bias, grad: True</span><br></pre></td></tr></tbody></table></figure><p>可以看到前两层的weight和bias的requires_grad都为False，表示它们不可训练。<br>最后在定义优化器时，只对requires_grad为True的层的参数进行更新。  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.01)</span><br></pre></td></tr></tbody></table></figure><h2 id="9-对不同层使用不同学习率"><a href="#9-对不同层使用不同学习率" class="headerlink" title="9. 对不同层使用不同学习率"></a>9. 对不同层使用不同学习率</h2><p>我们对模型的不同层使用不同的学习率。</p><p>还是使用这个模型作为例子：  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">net = Network()  <span class="comment"># 获取自定义网络结构</span></span><br><span class="line"><span class="keyword">for</span> name, value <span class="keyword">in</span> net.named_parameters():</span><br><span class="line">    <span class="built_in">print</span>(<span class="string">'name: {}'</span>.<span class="built_in">format</span>(name))</span><br><span class="line"><span class="comment"># 输出：</span></span><br><span class="line"><span class="comment"># name: cnn.VGG_16.convolution1_1.weight</span></span><br><span class="line"><span class="comment"># name: cnn.VGG_16.convolution1_1.bias</span></span><br><span class="line"><span class="comment"># name: cnn.VGG_16.convolution1_2.weight</span></span><br><span class="line"><span class="comment"># name: cnn.VGG_16.convolution1_2.bias</span></span><br><span class="line"><span class="comment"># name: cnn.VGG_16.convolution2_1.weight</span></span><br><span class="line"><span class="comment"># name: cnn.VGG_16.convolution2_1.bias</span></span><br><span class="line"><span class="comment"># name: cnn.VGG_16.convolution2_2.weight</span></span><br><span class="line"><span class="comment"># name: cnn.VGG_16.convolution2_2.bias</span></span><br></pre></td></tr></tbody></table></figure><p>对 convolution1 和 convolution2 设置不同的学习率，首先将它们分开，即放到不同的列表里：  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">conv1_params = []</span><br><span class="line">conv2_params = []</span><br><span class="line">for name, parms in net.named_parameters():</span><br><span class="line">    if "convolution1" in name:</span><br><span class="line">        conv1_params += [parms]</span><br><span class="line">    else:</span><br><span class="line">        conv2_params += [parms]</span><br><span class="line"># 然后在优化器中进行如下操作：</span><br><span class="line">optimizer = optim.Adam(</span><br><span class="line">    [</span><br><span class="line">        {"params": conv1_params, 'lr': 0.01},</span><br><span class="line">        {"params": conv2_params, 'lr': 0.001},</span><br><span class="line">    ],</span><br><span class="line">    weight_decay=1e-3,</span><br><span class="line">)</span><br></pre></td></tr></tbody></table></figure><p>我们将模型划分为两部分，存放到一个列表里，每部分就对应上面的一个字典，在字典里设置不同的学习率。当这两部分有相同的其他参数时，就将该参数放到列表外面作为全局参数，如上面的<code>weight_decay</code>。</p><p>也可以在列表外设置一个全局学习率，当各部分字典里设置了局部学习率时，就使用该学习率，否则就使用列表外的全局学习率。</p><h2 id="显示训练时间"><a href="#显示训练时间" class="headerlink" title="显示训练时间"></a>显示训练时间</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">for epoch in range(start_epoch, config.epochs):</span><br><span class="line">            start = time.time()</span><br><span class="line">            train_loss, lr = train_epoch(model, optimizer, scheduler, train_loader, device, criterion, epoch, all_step,</span><br><span class="line">                                         writer, logger)</span><br><span class="line">            logger.info('[{}/{}], train_loss: {:.4f}, time: {:.4f}, lr: {}'.format(</span><br><span class="line">                epoch, config.epochs, train_loss, time.time() - start, lr))</span><br></pre></td></tr></tbody></table></figure><p>参考：<a href="https://mp.weixin.qq.com/s?__biz=MzU3NjE4NjQ4MA==&amp;mid=2247485953&amp;idx=2&amp;sn=3ae788b7d643541254ba311f7a7faced&amp;chksm=fd16fb1eca61720870bc58c1a465a346cf2c6a7e8bea39e4b3d582474b595021f3a5b635086d&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1566885137387&amp;sharer_shareid=285785c5623899db73795495779fe8be#rd">https://mp.weixin.qq.com/s?__biz=MzU3NjE4NjQ4MA==&amp;mid=2247485953&amp;idx=2&amp;sn=3ae788b7d643541254ba311f7a7faced&amp;chksm=fd16fb1eca61720870bc58c1a465a346cf2c6a7e8bea39e4b3d582474b595021f3a5b635086d&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1566885137387&amp;sharer_shareid=285785c5623899db73795495779fe8be#rd</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch手册</title>
      <link href="/2019/08/28/20190828-Pytorch%E6%89%8B%E5%86%8C/"/>
      <url>/2019/08/28/20190828-Pytorch%E6%89%8B%E5%86%8C/</url>
      
        <content type="html"><![CDATA[<p>PyTorch 主要提供以下两大特色：</p><ul><li>支持强力GPU加速的Tensor计算能力</li><li>基于tape的具有自动微分求导能力的深度神经网络框架</li></ul><p>PyTorch 主要包含以下组成要素:</p><table><thead><tr><th>组成要素</th><th>描述说明</th></tr></thead><tbody><tr><td>torch</td><td>一个类似于numpy的tensor哭, 提供强力的GPU支持</td></tr><tr><td>torch.autograd</td><td>一个基于tape的具有自动微分求导能力的库, 可以支持几乎所有的tesnor operatioin</td></tr><tr><td>torch.nn</td><td>一个神经网络库, 与autograd深度整合, 可以提供最大限度的灵活性</td></tr><tr><td>torch.multiprocessing</td><td>Python的多线程处理, 可以提供torch Tensors之间的内存共享, 对于加载数据和Hogwild training来说十分有用</td></tr><tr><td>torch.utils</td><td>一些功能类和函数, 如DataLoader, Trainer等等</td></tr><tr><td>torch.legacy(.nn/.optim)</td><td>为了兼容性而存在的一些代码和实现</td></tr></tbody></table><p>Pytorch通常可以作为以下用途使用:</p><ul><li>为了使用GPUs性能的numpy替代品</li><li>可以提供强大灵活力和速度优势的深度学习平台.</li></ul><h2 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h2><h2 id="backends-cudnn"><a href="#backends-cudnn" class="headerlink" title="backends.cudnn"></a>backends.cudnn</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.backends.cudnn.benchmark = <span class="literal">True</span></span><br></pre></td></tr></tbody></table></figure><p>上述设置可以让内置的cudnn的auto-tuner自动寻找最合适当前配置的搞笑算法, 来达到优化运行效率的目标, 在使用时, 应该遵循以下两个准则:</p><ol><li>如果网络的输入数据维度或类型上变化不大, 则该设置可以增加运行效率</li><li>如果网络的输入数据在每次的iteration中都变化的话, 会导致cudnn每次都寻找一遍最优配置, 这样反而 会降低 运行效率.</li></ol><h3 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat()"></a>torch.cat()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.cat(seq, dim=<span class="number">0</span>, out=<span class="literal">None</span>) <span class="comment"># 返回连接后的tensor</span></span><br></pre></td></tr></tbody></table></figure><p>将给定的 tensor 序列 seq 按照维度连接起来. 默认维度为0, 说明会将其在第 0 个维度上进行拼接.(最后的结果是第 0 维度增大, 例如三个2行3列的 tensor 按照第0维度拼接, 最后得到的 tensor 维度为6行3列)</p><h3 id="clamp-x2F-clamp"><a href="#clamp-x2F-clamp" class="headerlink" title="clamp()/clamp_()"></a>clamp()/clamp_()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.clamp(<span class="built_in">input</span>, <span class="built_in">min</span>, <span class="built_in">max</span>, out=<span class="literal">None</span>) -&gt; Tensor</span><br></pre></td></tr></tbody></table></figure><p>将input里面元素全部划分到[min,max]区间内, 小于min的置为min, 大于max的置为max. 如果不指定min或者max,则认为无下界或上界<br>其他调用形式:  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor(<span class="built_in">min</span>, <span class="built_in">max</span>) <span class="comment"># 调用tensor为input, 返回值为out</span></span><br></pre></td></tr></tbody></table></figure><h3 id="device"><a href="#device" class="headerlink" title="device()"></a>device()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda:0"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br></pre></td></tr></tbody></table></figure><h3 id="gather"><a href="#gather" class="headerlink" title="gather()"></a>gather()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.gather(<span class="built_in">input</span>, dim, index, out=<span class="literal">None</span>) -&gt; Tensor</span><br></pre></td></tr></tbody></table></figure><p>沿着dim指定的轴按着index指定的值重新组合成一个新的tensor.  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0</span><br><span class="line">out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1</span><br><span class="line">out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2</span><br></pre></td></tr></tbody></table></figure><p>即假设input是一个 n 维的tensor, 其 size 为 (x0,x1,…,xi−1,xi,xi+1,…,xn−1), 若dim=i, 则 index 必须也是一个 n 维的tensor, 其 size 为 (x0,x1,…,xi−1,y,xi+1,…,xn−1), 其中 y≥1, 而返回的 tensor out 的 size 和 index 的 size 相同.<br>一句来说 gather 的作用就是, 在指定的维度上筛选给给定下标index指示的值, 其他值舍弃.<br><strong>一个例子说明:</strong><br>scores是一个计算出来的分数，类型为[torch.FloatTensor of size 5x1000]<br>而y_var是正确分数的索引，类型为[torch.LongTensor of size 5]<br>容易知道，这里有1000个类别，有5个输入图像，每个图像得出的分数中只有一个是正确的，正确的索引就在y_var中，这里要做的是将正确分数根据索引标号提取出来。  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">scores = model(X_var)  # 分数</span><br><span class="line">scores = scores.gather(1, y_var.view(-1, 1)).squeeze()  #进行提取</span><br></pre></td></tr></tbody></table></figure><p>提取后的scores格式也为[torch.FloatTensor of size 5]<br>这里讲一下变化过程：</p><ol><li>首先要知道之前的scores的size为[5,1000]，而y_var的size为[5]，scores为2维，y_var为1维不匹配，所以先用view将其展开为[5,1]的size，这样维数n就与scroes匹配了。</li><li>接下来进行gather，gather函数中第一个参数为1，意思是在第二维进行汇聚，也就是说通过y_var中的五个值来在scroes中第二维的5个1000中进行一一挑选，挑选出来后的size也为[5,1]，然后再通过squeeze将那个一维去掉，最后结果为[5].<br><strong>Tensor形式</strong></li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor.gather(dim, index) -&gt; Tensor</span><br></pre></td></tr></tbody></table></figure><h3 id="torch-ge"><a href="#torch-ge" class="headerlink" title="torch.ge()"></a>torch.ge()</h3><h3 id="torch-gt"><a href="#torch-gt" class="headerlink" title="torch.gt()"></a>torch.gt()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.gt(<span class="built_in">input</span>, other, out=<span class="literal">None</span>) <span class="comment"># -&gt; Tensor</span></span><br></pre></td></tr></tbody></table></figure><p>根据 input 和 other 的值返回一个二值 tensor, 如果满足大于条件则为1, 不满足则为0.<br>other 可以是能够转换成 input size 的tensor, 也可以是一个 float 标量.</p><h3 id="torch-index-select"><a href="#torch-index-select" class="headerlink" title="torch.index_select()"></a>torch.index_select()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.index_select(<span class="built_in">input</span>, dim, index, out=<span class="literal">None</span>) <span class="comment"># -&gt; Tensor</span></span><br></pre></td></tr></tbody></table></figure><p>返回在 dim 维度上的 index 指明的下标组成的 tensor.<br>返回的 tensor 的维度的数量和 input 是相同的, 但是第 dim 维度的 size 会和 index size大小相同. 其他维度的 size 保持不变.</p><h3 id="torch-le"><a href="#torch-le" class="headerlink" title="torch.le()"></a>torch.le()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.le(<span class="built_in">input</span>, other, out=<span class="literal">None</span>) <span class="comment"># -&gt;Tensor</span></span><br></pre></td></tr></tbody></table></figure><p>按元素计算 input≤other.</p><h3 id="max"><a href="#max" class="headerlink" title="max()"></a>max()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">max</span>(<span class="built_in">input</span>) <span class="comment"># 返回一个Tensor, 代表所有元素中的最大值</span></span><br><span class="line">torch.<span class="built_in">max</span>(<span class="built_in">input</span>,dim,keepdim=<span class="literal">False</span>,out=<span class="literal">None</span>) <span class="comment"># 返回一个元组:(Tensor, LongTensor)</span></span><br></pre></td></tr></tbody></table></figure><p>第二种形式会返回一个元组, 元组内元素类型为: (Tensor, LongTensor), 其中, 前者代表对应 dim 上 reduce 后的最大值, 后者代表最大值在维度 dim 中对应的下标.<br>如果keepdim=True, 则输出的 tensor 的 size 会和输入的相同, 只不过对应 dim 维度上的size为1. 否则, 对应 dim 维度会被 squeeze/reduce, 使得输出的维度比输入的维度少1.  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = torch.randn(<span class="number">4</span>, <span class="number">4</span>)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a</span><br><span class="line">tensor([[-<span class="number">1.2360</span>, -<span class="number">0.2942</span>, -<span class="number">0.1222</span>,  <span class="number">0.8475</span>],</span><br><span class="line">        [ <span class="number">1.1949</span>, -<span class="number">1.1127</span>, -<span class="number">2.2379</span>, -<span class="number">0.6702</span>],</span><br><span class="line">        [ <span class="number">1.5717</span>, -<span class="number">0.9207</span>,  <span class="number">0.1297</span>, -<span class="number">1.8768</span>],</span><br><span class="line">        [-<span class="number">0.6172</span>,  <span class="number">1.0036</span>, -<span class="number">0.6060</span>, -<span class="number">0.2432</span>]])</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>torch.<span class="built_in">max</span>(a, <span class="number">1</span>)</span><br><span class="line">(tensor([ <span class="number">0.8475</span>,  <span class="number">1.1949</span>,  <span class="number">1.5717</span>,  <span class="number">1.0036</span>]), tensor([ <span class="number">3</span>,  <span class="number">0</span>,  <span class="number">0</span>,  <span class="number">1</span>]))</span><br></pre></td></tr></tbody></table></figure><h3 id="mm"><a href="#mm" class="headerlink" title="mm()"></a>mm()</h3><p>注意, 没有torch.mm_版本  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.mm(mat1, mat2, out=<span class="literal">None</span>) <span class="comment"># 返回值为Tensor, 也可以使用out记录返回值</span></span><br></pre></td></tr></tbody></table></figure><p>两矩阵相乘, 矩阵的size需要满足乘法规则<br>其他调用形式:  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor(mat2) <span class="comment"># 调用者为mat1</span></span><br></pre></td></tr></tbody></table></figure><h3 id="norm"><a href="#norm" class="headerlink" title="norm()"></a>norm()</h3><p>返回输入tensor的p-norm标量  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.norm(<span class="built_in">input</span>, p=<span class="number">2</span>) <span class="comment"># 返回一个标量tensor</span></span><br></pre></td></tr></tbody></table></figure><h3 id="numel"><a href="#numel" class="headerlink" title="numel()"></a>numel()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.numel(<span class="built_in">input</span>)  <span class="comment">#返回一个int值</span></span><br></pre></td></tr></tbody></table></figure><p>返回 inpput tensor 中的元素的总个数  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">a = torch.randn(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="built_in">print</span>(torch.numel(a)) <span class="comment"># 120</span></span><br></pre></td></tr></tbody></table></figure><h3 id="ones"><a href="#ones" class="headerlink" title="ones()"></a>ones()</h3><h3 id="randn"><a href="#randn" class="headerlink" title="randn()"></a>randn()</h3><p>标准正太分布随机基础, 传入参数为维度信息</p><h3 id="torch-sort"><a href="#torch-sort" class="headerlink" title="torch.sort()"></a>torch.sort()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.sort(<span class="built_in">input</span>, dim=<span class="literal">None</span>, descending=<span class="literal">False</span>, out=<span class="literal">None</span>) <span class="comment"># 返回 (Tensor, LongTensor)</span></span><br></pre></td></tr></tbody></table></figure><h3 id="sum"><a href="#sum" class="headerlink" title="sum()"></a>sum()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">torch.<span class="built_in">sum</span>(<span class="built_in">input</span>, dtype=<span class="literal">None</span>) <span class="comment"># 返回求和后的Tensor(只有一个元素)</span></span><br><span class="line">torch.<span class="built_in">sum</span>(<span class="built_in">input</span>, dim, keepdim=<span class="literal">False</span>, dtype=<span class="literal">None</span>) <span class="comment"># 返回在dim上reduce的sum和, 如果dim包含多个维度, 则都进行reduce求和.</span></span><br><span class="line"><span class="comment"># reduce这个词很形象, 因为返回的Tensor的维度刚好没有了dim指示的那些维度</span></span><br></pre></td></tr></tbody></table></figure><p>其他形式:  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor.<span class="built_in">sum</span>()</span><br></pre></td></tr></tbody></table></figure><h3 id="torch-t"><a href="#torch-t" class="headerlink" title="torch.t()"></a>torch.t()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.t(<span class="built_in">input</span>) <span class="comment"># 返回转置后的Tensor</span></span><br></pre></td></tr></tbody></table></figure><p>其他形式:  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor.t()</span><br></pre></td></tr></tbody></table></figure><h3 id="unsqueeze"><a href="#unsqueeze" class="headerlink" title="unsqueeze()"></a>unsqueeze()</h3><p>在指定维度上插入一个 singleton 维度(一般用于将单一数据处理用 batch 的形式)  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.unsqueeze(<span class="built_in">input</span>, dim, out=<span class="literal">None</span>) <span class="comment"># -&gt; Tensor</span></span><br></pre></td></tr></tbody></table></figure><p>返回的tensor与input tensor 共享数据</p><p>dim 的取值范围在 [-input.dim()-1, input.dim()+1] 之间, 如果为负值, 则相当于 dim = dim + input.dim() + 1.</p><h3 id="zeros"><a href="#zeros" class="headerlink" title="zeros()"></a>zeros()</h3><h2 id="torch-cuda"><a href="#torch-cuda" class="headerlink" title="torch.cuda"></a>torch.cuda</h2><h3 id="torch-cuda-empty-cache"><a href="#torch-cuda-empty-cache" class="headerlink" title="torch.cuda.empty_cache()"></a>torch.cuda.empty_cache()</h3><p>释放所有未使用的 GPU 内存, 使用这些内存可以被其他 GPU 应用使用, 并且可以被 nvidia-smi 查到.<br>empty_cache() 并不会强制提升供 PyTorch 使用的显卡内存的大小, 查看Memory management</p><h2 id="torch-Tensor"><a href="#torch-Tensor" class="headerlink" title="torch.Tensor"></a>torch.Tensor</h2><p>torch.Tensor 是默认类型 torch.FloatTensor 的别名, 使用 torch.Tenosr 的构造函数创建 tensor 变量时, 传入的是维度信息(注意与 torch.tensor() 的区别):  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">t = torch.Tensor(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>) <span class="comment"># 里面的数值未初始化, 是随机的</span></span><br><span class="line"><span class="built_in">print</span>(t.size()) <span class="comment"># torch.Size([2,3,4])</span></span><br></pre></td></tr></tbody></table></figure><p>torch.LongTesnor 使用方法相似, 只不过数据类型是长整型.</p><h3 id="troch-tensor"><a href="#troch-tensor" class="headerlink" title="troch.tensor()"></a>troch.tensor()</h3><p>创建tensor  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor(data, dtype=<span class="literal">None</span>, device=<span class="literal">None</span>, requires_grad=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure><p>可以利用torch.tensor从python的list数据或者其他序列数据中创建tensor对象  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.tensor([[<span class="number">1</span>,-<span class="number">1</span>],[<span class="number">1</span>,-<span class="number">1</span>]])</span><br><span class="line">torch.tensor(np.array([[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]))</span><br></pre></td></tr></tbody></table></figure><p>注意, torch.tensor()函数总是会对数据进行复制操作, 因此, 如果你仅仅是想将数据的requires_grad标志改变, 那么就应该使用required_grad_()或者detach()函数来避免复制. 同时, 对numpy数组使用torch.as_tensor()将其转换成tensor而无需复制</p><h3 id="torch-Tensor-cpu"><a href="#torch-Tensor-cpu" class="headerlink" title="torch.Tensor.cpu()"></a>torch.Tensor.cpu()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor.cpu()</span><br><span class="line">z = x.cpu()</span><br></pre></td></tr></tbody></table></figure><p>将tensor移动到cpu上, 注意返回值z是cpu上的数据, tensor x 本身的device属性不变</p><h3 id="torch-Tensor-cuda"><a href="#torch-Tensor-cuda" class="headerlink" title="torch.Tensor.cuda()"></a>torch.Tensor.cuda()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor.cuda()</span><br><span class="line">z = x.cuda()</span><br></pre></td></tr></tbody></table></figure><h3 id="torch-Tensor-dim"><a href="#torch-Tensor-dim" class="headerlink" title="torch.Tensor.dim()"></a>torch.Tensor.dim()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor.dim() -&gt; <span class="built_in">int</span></span><br></pre></td></tr></tbody></table></figure><p>返回 tensor 的维度的个数.</p><h3 id="torch-Tensor-max"><a href="#torch-Tensor-max" class="headerlink" title="torch.Tensor.max()"></a>torch.Tensor.max()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor.<span class="built_in">max</span>(dim=<span class="literal">None</span>, keepdim=<span class="literal">False</span>) -&gt; Tensor <span class="keyword">or</span> (Tensor, Tensor)</span><br></pre></td></tr></tbody></table></figure><p>详情见 torch.max()</p><h3 id="torch-Tensor-numel"><a href="#torch-Tensor-numel" class="headerlink" title="torch.Tensor.numel()"></a>torch.Tensor.numel()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor.numel()</span><br></pre></td></tr></tbody></table></figure><p>详见 torch.numel()</p><h3 id="torch-Tensor-to"><a href="#torch-Tensor-to" class="headerlink" title="torch.Tensor.to()"></a>torch.Tensor.to()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor.to(*args, *kwargs)</span><br></pre></td></tr></tbody></table></figure><p>返回一个转移后的tensor, 而自身维持不变  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">t = torch.randn(<span class="number">2</span>,<span class="number">3</span>)</span><br><span class="line">t.to(torch.float64)</span><br><span class="line">t.to(device)</span><br><span class="line">t.to(<span class="string">"cuda:0"</span>)</span><br></pre></td></tr></tbody></table></figure><p>将tensor移动到gpu上, 注意返回值 z 是gpu 上的数据, tensor x 本身的 device 属性不变</p><h3 id="torch-Tensor-numpy"><a href="#torch-Tensor-numpy" class="headerlink" title="torch.Tensor.numpy()"></a>torch.Tensor.numpy()</h3><p>tensor与numpy数组的转换  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor.numpy() <span class="comment"># 返回tensor对应的numpy数组</span></span><br><span class="line">torch.from_numpy(ndarray) <span class="comment"># 将numpy数组ndarray转换成对应的tensor并返回.</span></span><br></pre></td></tr></tbody></table></figure><p>torch.Tensor 实际上是 torch.FloatFensor 的别名</p><h3 id="torch-Tensor-permute"><a href="#torch-Tensor-permute" class="headerlink" title="torch.Tensor.permute()"></a>torch.Tensor.permute()</h3><p>重新排列tensor的维度  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor.permute(*dims) <span class="comment"># 返回一个重新排列维度后的 tensor</span></span><br></pre></td></tr></tbody></table></figure><h3 id="torch-Tensor-unsqueeze"><a href="#torch-Tensor-unsqueeze" class="headerlink" title="torch.Tensor.unsqueeze()"></a>torch.Tensor.unsqueeze()</h3><p>详细可见torch.unsqueeze</p><h3 id="torch-Tensor-expand"><a href="#torch-Tensor-expand" class="headerlink" title="torch.Tensor.expand()"></a>torch.Tensor.expand()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor.expand(*sizes) <span class="comment"># 返回 tensor</span></span><br></pre></td></tr></tbody></table></figure><p>将 tensor 中的 singleton 维度扩展到一个更大的 size.<br>参数 -1 意味着不改变原始的维度<br>新增的维度的元素被被添加到前头, size不能设置为-1.<br>expand 并没有申请新的内存, 而仅仅是在当前已经存在的 tensor 上面创建了新的视图(view), 使得 singleton 维度被扩展成了一个更大的尺寸.<br>Any dimension of size 1 can be expanded to an arbitrary value without new memory.  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1</span>],[<span class="number">2</span>],[<span class="number">3</span>])</span><br><span class="line"><span class="built_in">print</span>(x.size())  <span class="comment"># torch.Size([3,1])</span></span><br><span class="line"><span class="built_in">print</span>(x.expand(<span class="number">3</span>,<span class="number">4</span>)) <span class="comment"># torch.Size([3,4]) # 将维度为1的扩展到任意尺寸</span></span><br><span class="line"><span class="built_in">print</span>(x.expand(-<span class="number">1</span>,<span class="number">4</span>)) <span class="comment"># torch.Size([3,4]) # -1 代表不改变维度</span></span><br></pre></td></tr></tbody></table></figure><p>注意, 只能对 singleton 的维度进行扩展, 如果强行对其他维度扩展, 则会报错.</p><h3 id="torch-Tensor-expand-as"><a href="#torch-Tensor-expand-as" class="headerlink" title="torch.Tensor.expand_as()"></a>torch.Tensor.expand_as()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor.expand_as(other) <span class="comment"># 返回 tensor</span></span><br></pre></td></tr></tbody></table></figure><p>将当前 tensor 扩展到和 other 一样的size.<br>self.expand_as(other) 与 self.expand(other.size()) 等价.</p><h3 id="torch-Tensor-index-fill"><a href="#torch-Tensor-index-fill" class="headerlink" title="torch.Tensor.index_fill_()"></a>torch.Tensor.index_fill_()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor.index_fill_(dim, index, val) <span class="comment"># 返回tensor</span></span><br></pre></td></tr></tbody></table></figure><p>在给定的维度 dim 上, 用 val 将该维度上的 index 坐标的值填充.  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>], [<span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>]], dtype=torch.<span class="built_in">float</span>)</span><br><span class="line">index = torch.tensor([<span class="number">0</span>, <span class="number">2</span>])</span><br><span class="line">x.index_fill_(<span class="number">1</span>, index, -<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line"><span class="comment">#tensor([[-1.,  2., -1.],</span></span><br><span class="line"><span class="comment">#       [-1.,  5., -1.],</span></span><br><span class="line"><span class="comment">#       [-1.,  8., -1.]])</span></span><br></pre></td></tr></tbody></table></figure><h3 id="torch-Tensor-contiguous"><a href="#torch-Tensor-contiguous" class="headerlink" title="torch.Tensor.contiguous()"></a>torch.Tensor.contiguous()</h3><p>返回一个连续的tensor, 数据内容不变  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor.contiguous() <span class="comment"># 如果tensor本身就是连续的, 那么就会返回tensor本身</span></span><br></pre></td></tr></tbody></table></figure><p>这里的 contiguous 指的是内存上的连续, 由于在 PyTorch 中, view 只能用在 contiguous 的 tensor 上面, 而如果在 view 之前使用了 transpose, permute 等操作后, 就需要使用 contiguous 来返回一个 contiguous tensor.<br>在 PyTorch 0.4 版本以后, 增加了 torch.reshape(), 这与 numpy.reshape() 的功能类似, 它大致相当于 tensor.contiguous().view() ?</p><h3 id="torch-Tensor-item"><a href="#torch-Tensor-item" class="headerlink" title="torch.Tensor.item()"></a>torch.Tensor.item()</h3><p>当Tensor中只包含一个元素时, 可以利用该函数返回这个元素的标量</p><h3 id="torch-Tensor-tolist"><a href="#torch-Tensor-tolist" class="headerlink" title="torch.Tensor.tolist()"></a>torch.Tensor.tolist()</h3><p>可以将Tensor转换成列表</p><h3 id="torch-Tensor-zero"><a href="#torch-Tensor-zero" class="headerlink" title="torch.Tensor.zero_()"></a>torch.Tensor.zero_()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.Tensor.zero_()</span><br></pre></td></tr></tbody></table></figure><p>将当前的 tensor 变量全部置为0(原地)</p><h2 id="torch-autograd"><a href="#torch-autograd" class="headerlink" title="torch.autograd"></a>torch.autograd</h2><h3 id="set-grad-enabled"><a href="#set-grad-enabled" class="headerlink" title="set_grad_enabled()"></a>set_grad_enabled()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.autograd.set_grad_enabled(mode)</span><br></pre></td></tr></tbody></table></figure><p>用来控制梯度计算的开关(依据bool类型参数mode决定), 可以当做上下文管理器使用, 也可以当做函数使用  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 当做上下文管理器</span></span><br><span class="line"><span class="keyword">with</span> torch.set_grad_enabled(is_train): <span class="comment"># 注意, 这里省略了autograd</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    optimizer.step()</span><br><span class="line"><span class="comment"># 当做函数使用</span></span><br><span class="line">w1 = torch.Tensor([<span class="number">1</span>], requires=<span class="literal">True</span>)</span><br><span class="line">torch.set_grad_enabled(<span class="literal">True</span>)</span><br><span class="line"><span class="built_in">print</span>(w1.requires_grad) <span class="comment"># True</span></span><br><span class="line">torch.set_grad_enabled(<span class="literal">False</span>)</span><br><span class="line"><span class="built_in">print</span>(w1.requires_grad) <span class="comment"># False</span></span><br></pre></td></tr></tbody></table></figure><h3 id="no-grad"><a href="#no-grad" class="headerlink" title="no_grad()"></a>no_grad()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.autograd.no_grad</span><br></pre></td></tr></tbody></table></figure><p>用于禁用梯度计算的上下文管理器.<br>在测试阶段, 当你确信你不会调用Tensor.backward()时,禁用梯度计算十分有用. 这会降低计算使用内存消耗.  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">x = torch.tensor([<span class="number">1.0</span>], requires_grad=<span class="literal">True</span>)</span><br><span class="line"><span class="keyword">with</span> torch.no_grad(): <span class="comment"># 省略了autograd</span></span><br><span class="line">    <span class="built_in">print</span>(x.requires_grad) <span class="comment"># True, 虽然为True, 但在该上下文中, 会无视掉requires_grad参数, 一律做False处理</span></span><br><span class="line">    y = x*<span class="number">2</span></span><br><span class="line">    <span class="built_in">print</span>(y.requires_grad) <span class="comment"># False, 在当前上下文产生的tensor的requires_grad属性为False</span></span><br><span class="line"><span class="built_in">print</span>(x.requires_grad) <span class="comment"># True</span></span><br></pre></td></tr></tbody></table></figure><h3 id="torch-autograd-Function"><a href="#torch-autograd-Function" class="headerlink" title="torch.autograd.Function"></a>torch.autograd.Function</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.autograd.Function</span><br></pre></td></tr></tbody></table></figure><p>为可微分的 ops 记录 operation history, 同时定义计算公式. </p><p>每一个作用在 tensor 上的 operatin 都会创建一个新的 function 对象, 它会执行计算过程并记录相关信息. 这些信息可以从一个由 functions 组成的有向图中获得. 当 backward() 方法被调用时, 就会利用这些信息在 function 上进行反向传播, 并将梯度传给下一个 Funtion.<br>通常情况下, 当用于需要自定义可自动求导的 ops 时, 可以实现一个 Function 的子类.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Example</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Exp</span>(<span class="title class_ inherited__">Function</span>):</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">ctx, i</span>):</span><br><span class="line">        result = i.exp()</span><br><span class="line">        ctx.save_for_backward(result)</span><br><span class="line"><span class="meta">    @staticmethod</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">backward</span>(<span class="params">ctx, grad_output</span>):</span><br><span class="line">        result, = ctx.saved_tensors</span><br><span class="line">        <span class="keyword">return</span> grad_output*result</span><br></pre></td></tr></tbody></table></figure><p>static forward(ctx, <em>args, kwargs):*</em><br>定义前向计算的逻辑.</p><p>static backward(ctx, *grad_outputs):<br>定义反向传导的逻辑, 如果确定不会使用到反向传播, 则可以不实现该函数.</p><h2 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h2><h3 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.nn.Module</span><br></pre></td></tr></tbody></table></figure><p>所有神经网络Module的基类, 自定义的模型也应该是它的子类.<br>Modules可以包含其他Module(如Linear, Conv2d等等).</p><h3 id="parameters"><a href="#parameters" class="headerlink" title="parameters()"></a>parameters()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> model.parameters():</span><br><span class="line">    <span class="built_in">print</span>(param.data, param.size())</span><br></pre></td></tr></tbody></table></figure><p>state_dict:  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Module.state_dict(destination=<span class="literal">None</span>,prefix=<span class="string">""</span>,keep_vars=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure><p>以字典形式返回整个module的状态</p><h4 id="train"><a href="#train" class="headerlink" title="train"></a>train</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Module.train(mode=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure><p>将module的模式设置为train, 这只对部分module有效, 如Dropout, BatchNorm等, 详细请查看官网.<br>返回值: torch.nn.Module</p><h4 id="training"><a href="#training" class="headerlink" title="training"></a>training</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Module.training <span class="comment"># 属性, 返回一个bool值, 指示当前的模式是否为train</span></span><br></pre></td></tr></tbody></table></figure><h4 id="eval"><a href="#eval" class="headerlink" title="eval"></a>eval</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Module.<span class="built_in">eval</span>() <span class="comment"># 注意, 和train不同, eval为无参函数</span></span><br></pre></td></tr></tbody></table></figure><p>将module的mode设置为evaluation, 同样, 只对部分module起效.</p><h3 id="Linear"><a href="#Linear" class="headerlink" title="Linear"></a>Linear</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.Linear(in_features, out_features, bias=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure><p>全连接层的实现. 输入的shape为 (N,…,infeatures), 输出的shape为 (N,…,outfeatures), 可以看出, 除了最后一维不同外, 其他维度都相同. (通常在使用Linear之前, 会将输入变成二维的矩阵, 其中第一维为batch size, 第二维为特征向量).<br>in_features 和 out_features 可以当做属性用.来获取.</p><h3 id="Conv2d"><a href="#Conv2d" class="headerlink" title="Conv2d"></a>Conv2d</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.nn.Conv2的(in_channels, out_channels, kernel_size, stride=<span class="number">1</span>, padding=<span class="number">0</span>, dilation=<span class="number">1</span>, groups=<span class="number">1</span>, bias=<span class="literal">True</span>)</span><br></pre></td></tr></tbody></table></figure><ul><li>in_channels(int):</li><li>out_channels(int):</li><li>kernel_size(intortuple):</li><li>stride(intortuple, optional):</li></ul><h3 id="MaxPool2d"><a href="#MaxPool2d" class="headerlink" title="MaxPool2d"></a>MaxPool2d</h3><h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax()"></a>Softmax()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">```python</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.nn.Softmax(dim=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure><p>dim指明了需要进行 softmax 的维度, 在这个维度上的值, 加起来和为1.</p><h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.nn.ReLU(inplace=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure><p>输入输出的shape是相同的, 执行relu函数</p><h3 id="torch-nn-Sequential"><a href="#torch-nn-Sequential" class="headerlink" title="torch.nn.Sequential"></a>torch.nn.Sequential</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.nn.Sequential(*args)</span><br></pre></td></tr></tbody></table></figure><h3 id="torch-nn-MSELoss"><a href="#torch-nn-MSELoss" class="headerlink" title="torch.nn.MSELoss"></a>torch.nn.MSELoss</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.nn.MSELoss(size_average=<span class="literal">None</span>, reduce=<span class="literal">None</span>, reduction=<span class="string">"elementwise_mean"</span>)</span><br></pre></td></tr></tbody></table></figure><ol><li>size_average(bool, optional): 弃用(见reduction参数). 默认情况下, loss会计算在每个样本上的平均误差. 如果将size_average置为False, 则计算平方误差总和. 当reduce参数为False时, 忽视该参数</li><li>reduce(bool, optional): 弃用(见reduction参数). reduce参数顾名思义, 就是是否让MSELoss函数返回值的维度减少, 默认为True, 即会将任意维度的输入计算loss后, 返回一个标量(平均or总和取决于size_average), 如果为False, 则说明返回值维度不应该发生变化, 故而返回值就是对每个元素单独进行平方损失计算.<br>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>y = torch.tensor([1,2,3,4], dtype=torch.float)<br>pred_y = torch.tensor([1,1,1,1], dtype=torch.float)<br>loss_fn1 = torch.nn.MSELoss()<br>loss1 = loss_fn1(y, pred_y)<br>loss_fn2 = torch.nn.MSELoss(size_average=False)<br>loss2 = loss_fn2(y, pred_y)<br>loss_fn3 = torch.nn.MSELoss(reduce=False)<br>loss3 = loss_fn3(y, pred_y)<br>print(loss1,loss2,loss3)<br># tensor(3.5000) tensor(14.) tensor([0., 1., 4., 9.])</li></ol><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">  3. reduction(string, optional): 用字符串来替代上面两个参数的作用: “elementwise_mean”(默认) | “sum” | “none” (不进行reduce).</span><br><span class="line"></span><br><span class="line">### torch.nn.functional</span><br><span class="line"></span><br><span class="line">#### conv1d()</span><br><span class="line"></span><br><span class="line">#### conv2d()</span><br><span class="line"></span><br><span class="line">#### relu()</span><br><span class="line">         </span><br><span class="line"></span><br><span class="line">```python</span><br></pre></td></tr></tbody></table></figure><p>torch.nn.functional.relu(input, inplace=True) # 返回 一个 Tenosr</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">#### relu_()</span><br><span class="line">```python</span><br><span class="line">    torch.nn.functional.relu_(input) # relu() 的原地版本</span><br></pre></td></tr></tbody></table></figure><h2 id="torch-optim"><a href="#torch-optim" class="headerlink" title="torch.optim"></a>torch.optim</h2><h3 id="lr-scheduler"><a href="#lr-scheduler" class="headerlink" title="lr_scheduler"></a>lr_scheduler</h3><h4 id="StepLR"><a href="#StepLR" class="headerlink" title="StepLR"></a>StepLR</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.optim.lr_schedulr.StepLR(optimizer,step_size,gamma=<span class="number">0.1</span>,last_epoch=-<span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure><p>每经过step_size次epoch之后, lr就会衰减gamma倍(new_lr=lr×gamma), 初始的lr来自于optimizer中的lr参数.</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"># Observe that all parameters are being optimized</span><br><span class="line">optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)</span><br><span class="line"># Decay LR by a factor of 0.1 every 7 epochs</span><br><span class="line">exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)</span><br></pre></td></tr></tbody></table></figure><h4 id="ExponentialLR"><a href="#ExponentialLR" class="headerlink" title="ExponentialLR"></a>ExponentialLR</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.optim.lr_scheduler.ExponentialLR(optimizer,gamma,last_epoch=-<span class="number">1</span>)</span><br></pre></td></tr></tbody></table></figure><h4 id="CosineAnnealingLR"><a href="#CosineAnnealingLR" class="headerlink" title="CosineAnnealingLR"></a>CosineAnnealingLR</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">## Adam</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.optim.Adam(params,lr=<span class="number">0.001</span>,betas=(<span class="number">0.9</span>,<span class="number">0.999</span>),eps=<span class="number">1e-08</span>,weight_decay=<span class="number">0</span>,amsgrad=<span class="literal">False</span>)</span><br></pre></td></tr></tbody></table></figure><h4 id="conv2d"><a href="#conv2d" class="headerlink" title="conv2d"></a>conv2d</h4><h2 id="torch-utils-data"><a href="#torch-utils-data" class="headerlink" title="torch.utils.data"></a>torch.utils.data</h2><h3 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torch</span>.utils.data.DataLoader(dataset,batch_size=<span class="number">1</span>,shuffle=<span class="literal">False</span>,sampler=<span class="literal">None</span>,batch_sampler=<span class="literal">None</span>,num_workers=<span class="number">0</span>,collate_fn=&lt;function default_collate&gt;,pin_memory=<span class="literal">False</span>,drop_last=<span class="literal">False</span>,timeout=<span class="number">0</span>,worker_init_fn=<span class="literal">None</span>)</span><br></pre></td></tr></tbody></table></figure><p>数据加载器, 将数据集和采样器结合起来, 并且提供单/多线程的迭代器.</p><ul><li>dataset(utils.data.Dataset):</li><li>batch_size(int,optional): batch中的样本个数</li><li>shuffle(bool,optional)</li><li>num_worker(int,optional): 加载数据的线程个数, 0意味着只有一个主线程.<br>方法：</li><li><strong>iter</strong>(self): 可以当做迭代器使用, 如inputs,class_ids=next(iter(dataloaders)), 其中, input的shape为 (N,C,H,W), class_ids的shape为 (N).</li><li><strong>len</strong>(self): 返回数据集的类别数目</li></ul><h2 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h2><h3 id="torchvision-utils"><a href="#torchvision-utils" class="headerlink" title="torchvision.utils"></a>torchvision.utils</h3><h4 id="make-grid"><a href="#make-grid" class="headerlink" title="make_grid"></a>make_grid</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torchvision.utils.make_grid(tensor,nrow=8,padding=2,normalize=False,range=None,scale_each=False,pad_value=0)</span><br></pre></td></tr></tbody></table></figure><p>制作一个关于image的grid, 返回值依然是一个tensor, 只不过尺度变成了3D, 相当于把多个图片拼接在一起了, 直接通过plt.imshow(grid)即可输出网格化以后的图片.</p><ul><li>tensor(Tensor/list): 4D的 mini-batch Tensor, Shape为 (N×C×H×W), 或者是同维度的list.</li></ul><h3 id="torchvision-transforms"><a href="#torchvision-transforms" class="headerlink" title="torchvision.transforms"></a>torchvision.transforms</h3><h4 id="torchvision-transforms-Compose"><a href="#torchvision-transforms-Compose" class="headerlink" title="torchvision.transforms.Compose"></a>torchvision.transforms.Compose</h4><pre><code>    1    2    3    4    5    6    7</code></pre><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torchvision</span>.transforms.Compose(transforms)</span><br><span class="line"><span class="comment"># 使用</span></span><br><span class="line">trans.Compose([</span><br><span class="line">transforms.CenterCrop(<span class="number">10</span>),</span><br><span class="line">transforms.ToTensor(),</span><br><span class="line">])</span><br></pre></td></tr></tbody></table></figure><p>将多个transforms操作组合起来, 注意参数是列表形式</p><h4 id="Transforms-on-PIL-Image"><a href="#Transforms-on-PIL-Image" class="headerlink" title="Transforms on PIL Image"></a>Transforms on PIL Image</h4><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># cv2 image to PIL Image</span><br><span class="line"># skimage to PIL Image</span><br></pre></td></tr></tbody></table></figure><p>注意, 以下操作作用在PIL Image上的</p><h5 id="CenterCrop"><a href="#CenterCrop" class="headerlink" title="CenterCrop"></a>CenterCrop</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torchvision</span>.transform.CenterCrop(size)</span><br></pre></td></tr></tbody></table></figure><p>size参数表示输出的图谱的大小, 如果只传入了一个数字, 则该数字既表示高度, 又表示宽度.</p><h5 id="Resize"><a href="#Resize" class="headerlink" title="Resize"></a>Resize</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torchvision</span>.transforms.Resize(size, interpolation=<span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure><ul><li>size: 期望的输出size.</li><li>interpolation: 插值方法, 默认为双线性插值</li></ul><h5 id="ToTensor"><a href="#ToTensor" class="headerlink" title="ToTensor"></a>ToTensor</h5><pre><code>    1</code></pre><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torchvision</span>.transforms.ToTensor</span><br></pre></td></tr></tbody></table></figure><p>将一个PIL Image或者numpy.ndarray (H×W×C,[0, 255])转换成torch.FloatTensor (C×H×W, [0.0, 1.0]).</p><h5 id="RandomHorizontalFlip"><a href="#RandomHorizontalFlip" class="headerlink" title="RandomHorizontalFlip"></a>RandomHorizontalFlip</h5><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">transforms.RandomHorizontalFlip(p=0.5)</span><br></pre></td></tr></tbody></table></figure><p>在给定概率下对PIL Image随机执行水平翻转操作</p><h5 id="RandomResizedCrop"><a href="#RandomResizedCrop" class="headerlink" title="RandomResizedCrop"></a>RandomResizedCrop</h5><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">torch.transforms.RandomResizedCrop(size, scale=(<span class="number">0.08</span>, <span class="number">1.0</span>), ratio=(<span class="number">0.75</span>, <span class="number">1.3333333333</span>), interpolation=<span class="number">2</span>)</span><br></pre></td></tr></tbody></table></figure><p>对PIL Image随机执行剪裁操作(按照scale和ratio的区间剪裁), 然后将剪裁后的图片放缩都期望的尺寸(默认插值为双线性插值)</p><ul><li>size: 期望得到的尺寸</li><li>scale: 剪裁的面积比例(相对于原始图)</li><li>ratio: 剪裁的宽高比</li><li>interpolation: 默认为:PIL.Image.BILINEAR</li></ul><h3 id="Transforms-on-torch-Tensor"><a href="#Transforms-on-torch-Tensor" class="headerlink" title="Transforms on torch.*Tensor"></a>Transforms on torch.*Tensor</h3><p>注意, 以下操作是作用在tensor上的</p><h4 id="Normalize"><a href="#Normalize" class="headerlink" title="Normalize"></a>Normalize</h4><pre><code>    1</code></pre><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torchvision</span>.transforms.Normalize(mean, std)</span><br></pre></td></tr></tbody></table></figure><p>将图片tensor按照均值mean和标准差std进行归一化, 对于n个channels, 有 mean=(M1, …, Mn), std=(S1,…,Sn).<br><strong>注意, 这个归一化操作是原地进行的</strong></p><h3 id="torchvision-datasets"><a href="#torchvision-datasets" class="headerlink" title="torchvision.datasets"></a>torchvision.datasets</h3><h4 id="ImageFolder"><a href="#ImageFolder" class="headerlink" title="ImageFolder"></a>ImageFolder</h4><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">torchvision</span>.datasets.ImageFolder(root, transform=<span class="literal">None</span>, target_transform=<span class="literal">None</span>, loader=&lt;function default_loader&gt;)</span><br></pre></td></tr></tbody></table></figure><p>一个一般化的数据加载器, 主要针对如下数据排列格式:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">root/dog/x.png</span><br><span class="line">root/dog/y.png</span><br><span class="line">root/dog/z.png</span><br><span class="line">...</span><br><span class="line">root/cat/123.png</span><br><span class="line">root/cat/nsdf3.png</span><br><span class="line">root/cat/asd932_.png</span><br></pre></td></tr></tbody></table></figure><ul><li>root: 根目录路径</li><li>transform(callable,optional): 对图片要做的变换操作</li><li>target_transform(callable,optional): 对target要做的变换操作</li><li>loader: 用于加载给定路径图片的函数<br>属性：</li><li>classes(list): 返回类别的名字列表 class_names</li><li>class_to_idx(dict): 以字典的形式返回(class_name, class_index)</li><li>imgs(list): 返回元组列表: (image path, class_index)<br>方法：</li><li>getitem(index): 根据index返回(sample,target)元组. 可以使用</li><li>len(imagefolder) 返回类别数量</li></ul><h2 id="sort"><a href="#sort" class="headerlink" title="sort()"></a>sort()</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sort(dim=None, descending=False)  # 默认为升序, 返回(Tensor, LongTensor)</span><br></pre></td></tr></tbody></table></figure><p>详见 torch.sort()</p><h2 id="torch-distributed"><a href="#torch-distributed" class="headerlink" title="torch.distributed"></a>torch.distributed</h2><h3 id="torch-distributed-reduce"><a href="#torch-distributed-reduce" class="headerlink" title="torch.distributed.reduce()"></a>torch.distributed.reduce()</h3><h2 id="inspect-模块"><a href="#inspect-模块" class="headerlink" title="inspect 模块"></a>inspect 模块</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">inspect.signature() # 查看函数签名, python3.6以上</span><br><span class="line">inspect.getargspec() # 查看函数签名, python3.6以上</span><br><span class="line">inspect.getsource() # 获取模型的code</span><br><span class="line">inspect.getabsfile() # 获取模块的路径</span><br></pre></td></tr></tbody></table></figure><h2 id="un-normalize"><a href="#un-normalize" class="headerlink" title="un normalize"></a>un normalize</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mean = torch.tensor([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], dtype=torch.float32)</span><br><span class="line">std = torch.tensor([<span class="number">2</span>, <span class="number">2</span>, <span class="number">2</span>], dtype=torch.float32)</span><br><span class="line">normalize = T.Normalize(mean.tolist(), std.tolist())</span><br><span class="line">unnormalize = T.Normalize((-mean / std).tolist(), (<span class="number">1.0</span> / std).tolist())</span><br><span class="line">​~~~s</span><br></pre></td></tr></tbody></table></figure><pre><code></code></pre>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>logging模块</title>
      <link href="/2019/08/28/20190828-logging%E6%A8%A1%E5%9D%97/"/>
      <url>/2019/08/28/20190828-logging%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<h2 id="简单使用"><a href="#简单使用" class="headerlink" title="简单使用"></a>简单使用</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> logging</span><br><span class="line">logging.debug(<span class="string">"debug msg"</span>)</span><br><span class="line">logging.info(<span class="string">"info msg"</span>)</span><br><span class="line">logging.warn(<span class="string">"warn msg"</span>)</span><br><span class="line">logging.error(<span class="string">"error msg"</span>)</span><br><span class="line">logging.critical(<span class="string">"critical msg"</span>)</span><br></pre></td></tr></tbody></table></figure><p>默认情况下, logging模块将日志打印到屏幕上, 只有日志级别高于WARNING的日志信息才回输出</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>numpy实现神经网络</title>
      <link href="/2019/08/28/20190828-numpy%E5%AE%9E%E7%8E%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2019/08/28/20190828-numpy%E5%AE%9E%E7%8E%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># N 为batch size, D_in 为输入维度</span></span><br><span class="line"><span class="comment"># H 为隐藏层的维度, D_out 为输出的维度</span></span><br><span class="line">N, D_in, H, D_out = <span class="number">64</span>, <span class="number">1000</span>, <span class="number">100</span>, <span class="number">10</span></span><br><span class="line"><span class="comment"># 创建随机的输入和输出数据</span></span><br><span class="line">x = np.random.randn(N, D_in) <span class="comment"># N × D_in 的矩阵</span></span><br><span class="line">y = np.random.randn(N, D_out) <span class="comment"># N × D_out 的矩阵</span></span><br><span class="line"><span class="comment"># 对两个隐藏层w1,w2进行初始化</span></span><br><span class="line">w1 = np.random.randn(D_in, H)</span><br><span class="line">w2 = np.random.randn(H, D_out)</span><br><span class="line"><span class="comment"># 设置学习率</span></span><br><span class="line">learning_rate = <span class="number">1e-6</span></span><br><span class="line"><span class="keyword">for</span> t <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">500</span>):</span><br><span class="line">    <span class="comment"># 前向传播: 计算预测结果 y_pred</span></span><br><span class="line">    h = x.dot(w1) <span class="comment"># x维度为64 × 1000, w1维度为 1000 × 100, 计算完以后, h维度为 64 × 100</span></span><br><span class="line">    h_relu = np.maximum(h,<span class="number">0</span>)</span><br><span class="line">    y_pred = h_relu.dot(w2) <span class="comment"># h_relu维度为 64×100, w2维度为100×10, y的维度为64×10</span></span><br><span class="line">    <span class="comment"># 计算损失</span></span><br><span class="line">    loss = np.square(y_pred - y).<span class="built_in">sum</span>()</span><br><span class="line">    <span class="built_in">print</span>(t, loss)</span><br><span class="line">    <span class="comment"># 反向传播根据loss更新w1和w2的值</span></span><br><span class="line">    grad_y_pred = <span class="number">2.0</span>*(y_pred - y) <span class="comment"># 对y_pred求导</span></span><br><span class="line">    grad_w2 = h_relu.T.dot(grad_y_pred) <span class="comment"># 对w2求导, 微分矩阵应该与w2的size相同</span></span><br><span class="line">    grad_h_relu = grad_y_pred.dot(w2.T) <span class="comment"># 对h_relu求导</span></span><br><span class="line">    grad_h = grad_h_relu.copy()</span><br><span class="line">    grad_h[h &lt; <span class="number">0</span>] = <span class="number">0</span> <span class="comment"># 经过relu, 将小于0的梯度归0</span></span><br><span class="line">    grad_w1 = x.T.dot(grad_h)</span><br><span class="line">    <span class="comment"># Update weights</span></span><br><span class="line">    w1 = w1 - learning_rate * grad_w1</span><br><span class="line">    w2 = w2 - learning_rate * grad_w2</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>opencv模块</title>
      <link href="/2019/08/28/20190828-opencv%E6%A8%A1%E5%9D%97/"/>
      <url>/2019/08/28/20190828-opencv%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<h2 id="opencv-基础知识"><a href="#opencv-基础知识" class="headerlink" title="opencv 基础知识"></a>opencv 基础知识</h2><p>cv2.imread 读入的图片, 其shape为(h, w, c), 颜色通道顺序为 (b, g, r)</p><h2 id="常用颜色"><a href="#常用颜色" class="headerlink" title="常用颜色"></a>常用颜色</h2><h2 id="读取图片"><a href="#读取图片" class="headerlink" title="读取图片"></a>读取图片</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">img = cv2.imread(img_path)</span><br></pre></td></tr></tbody></table></figure><h2 id="保存图片"><a href="#保存图片" class="headerlink" title="保存图片"></a>保存图片</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv2.imwrite(save_path, img)</span><br></pre></td></tr></tbody></table></figure><h2 id="文本"><a href="#文本" class="headerlink" title="文本"></a>文本</h2><p>(startX, startY) 为左上角坐标  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv2.putText(img, <span class="string">"text test"</span>, (startX, startY), cv2.FONT_HERSHEY_SIMPLEX, font_size, (B,G,R), thickness)</span><br></pre></td></tr></tbody></table></figure><h2 id="画框"><a href="#画框" class="headerlink" title="画框"></a>画框</h2><p>(x,y) 为左上角坐标<br>(x+h,y+w) 为右下角坐标  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cv2.rectangle(img,(x,y), (x+h,y+w), (<span class="number">0</span>,<span class="number">255</span>,<span class="number">0</span>), thickness)</span><br></pre></td></tr></tbody></table></figure><h2 id="waitKey"><a href="#waitKey" class="headerlink" title="waitKey()"></a>waitKey()</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">keypress = cv2.waitKey(<span class="number">200</span>) <span class="comment"># 200为当前图片的显示持续时间</span></span><br><span class="line"><span class="keyword">if</span> keypress == <span class="built_in">ord</span>(<span class="string">'c'</span>) <span class="comment"># keypress为按键的整数形式, 所以需要用ord将字符类型转换</span></span><br><span class="line"><span class="keyword">if</span> cv2.waitKey(<span class="number">200</span>) == <span class="number">27</span>: <span class="comment"># Decimal 27 = Esc</span></span><br></pre></td></tr></tbody></table></figure><h2 id="opencv与numpy"><a href="#opencv与numpy" class="headerlink" title="opencv与numpy"></a>opencv与numpy</h2><p>opencv的基础类型为numpy.ndarray, 因此可以直接使用 ndarray 的一些属性的方法  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">img = cv2.imread(<span class="string">'./test.jpg'</span>)</span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">type</span>(img)) <span class="comment"># &lt;class 'numpy.ndarray'&gt;</span></span><br><span class="line"><span class="built_in">print</span>(img.shape) <span class="comment"># (500, 1069, 3)  (高, 宽, 通道)</span></span><br></pre></td></tr></tbody></table></figure><p>利用 cv2.merge 方法将 numpy.ndarray 数据转换成opencv的图片数据:  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图片的分辨率为300*200(宽*高)，这里b, g, r设为随机值，注意dtype属性</span></span><br><span class="line">b = np.random.randint(<span class="number">0</span>, <span class="number">255</span>, (<span class="number">200</span>, <span class="number">300</span>), dtype=np.uint8)</span><br><span class="line">g = np.random.randint(<span class="number">0</span>, <span class="number">255</span>, (<span class="number">200</span>, <span class="number">300</span>), dtype=np.uint8)</span><br><span class="line">r = np.random.randint(<span class="number">0</span>, <span class="number">255</span>, (<span class="number">200</span>, <span class="number">300</span>), dtype=np.uint8)</span><br><span class="line"><span class="comment"># 合并通道，形成图片</span></span><br><span class="line">img = cv2.merge([b, g, r])  <span class="comment"># opencv的通道是b在最前,r在最后</span></span><br><span class="line"><span class="comment"># 显示图片</span></span><br><span class="line">cv2.imshow(<span class="string">'test'</span>, img)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyWindow(<span class="string">'test'</span>)</span><br></pre></td></tr></tbody></table></figure><h2 id="通道的拆分与合并"><a href="#通道的拆分与合并" class="headerlink" title="通道的拆分与合并"></a>通道的拆分与合并</h2><p>拆分: cv2.split<br>合并: cv2.merge  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 图片的分辨率为800*200(宽*高)，这里b, g, r设为随机值，注意dtype属性</span></span><br><span class="line">b = np.random.randint(<span class="number">0</span>, <span class="number">255</span>, (<span class="number">200</span>, <span class="number">800</span>), dtype=np.uint8)</span><br><span class="line">g = np.random.randint(<span class="number">0</span>, <span class="number">255</span>, (<span class="number">200</span>, <span class="number">800</span>), dtype=np.uint8)</span><br><span class="line">r = np.random.randint(<span class="number">0</span>, <span class="number">255</span>, (<span class="number">200</span>, <span class="number">800</span>), dtype=np.uint8)</span><br><span class="line"><span class="comment"># 合并通道，形成图片</span></span><br><span class="line">img = cv2.merge([b, g, r])  <span class="comment"># opencv的通道是b在最前,r在最后</span></span><br><span class="line"><span class="comment"># 显示图片</span></span><br><span class="line">cv2.imshow(<span class="string">'test'</span>, img)</span><br><span class="line">cv2.waitKey(<span class="number">0</span>)</span><br><span class="line">cv2.destroyWindow(<span class="string">'test'</span>)</span><br><span class="line"><span class="comment"># 拆分通道, 每个通道都变成了单通道数组</span></span><br><span class="line">[blue, green, red] = cv2.split(img)</span><br></pre></td></tr></tbody></table></figure><h2 id="将-BGR-转换成-RGB-通道顺序"><a href="#将-BGR-转换成-RGB-通道顺序" class="headerlink" title="将 BGR 转换成 RGB 通道顺序"></a>将 BGR 转换成 RGB 通道顺序</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法一:</span></span><br><span class="line">rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)</span><br><span class="line"><span class="comment"># 方法二:</span></span><br><span class="line">rgb_img = img[:, :, [<span class="number">2</span>, <span class="number">1</span>, <span class="number">0</span>]]   <span class="comment"># img[h,w,v]</span></span><br><span class="line">rgb_img = img[:, :, ::-<span class="number">1</span>]</span><br></pre></td></tr></tbody></table></figure><h2 id="PIL-与-cv2-格式互相转换"><a href="#PIL-与-cv2-格式互相转换" class="headerlink" title="PIL 与 cv2 格式互相转换"></a>PIL 与 cv2 格式互相转换</h2><p>PIL.Image读入的图片数据类型不是 numpy 数组, 它的size属性为 (w, h), 利用np.array转换成 numpy 数组后, 它的通道顺序为 (r, g, b)  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> PIL <span class="keyword">import</span> Image</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># PIL to cv2</span></span><br><span class="line">pil_img = Image.<span class="built_in">open</span>(img_path)</span><br><span class="line"><span class="built_in">print</span>(pil_img.size) <span class="comment"># (w, h)</span></span><br><span class="line">np_img = np.array(pil_img)</span><br><span class="line">cv2_img = np_img[:, :, ::-<span class="number">1</span>] <span class="comment"># 交换通道</span></span><br><span class="line"><span class="comment"># cv2 to PIL</span></span><br><span class="line">pil_img = Image.fromarray(cv2_img[:, :, ::-<span class="number">1</span>])</span><br></pre></td></tr></tbody></table></figure><h2 id="用matplotlib显示图像"><a href="#用matplotlib显示图像" class="headerlink" title="用matplotlib显示图像"></a>用matplotlib显示图像</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">b,g,r=cv2.split(img)</span><br><span class="line">img2=cv2.merge([r,g,b])</span><br><span class="line">plt.imshow(img2)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><h2 id="截取子图"><a href="#截取子图" class="headerlink" title="截取子图"></a>截取子图</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># 已知子图左上角坐标 (x1, y1), 右下角坐标(x2, y2)</span><br><span class="line">crop_img = img[y1:y2, x1:x2, :]</span><br></pre></td></tr></tbody></table></figure><h2 id="opencv-核心算法"><a href="#opencv-核心算法" class="headerlink" title="opencv 核心算法"></a>opencv 核心算法</h2><h2 id="cv2"><a href="#cv2" class="headerlink" title="cv2"></a>cv2</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> cv2</span><br><span class="line">image_path = <span class="string">'./test.jpg'</span></span><br><span class="line">src_image = cv2.imread(image_path) <span class="comment"># 读取图片</span></span><br><span class="line">size = src_image.shape  <span class="comment"># 获取图片的尺寸, 返回一个元组: (height, width, depth)</span></span><br><span class="line">copy_image = src_image.copy() <span class="comment"># 复制图片</span></span><br><span class="line">cv2.imwrite(<span class="string">'./dst_test.jpg'</span>, copy_image) <span class="comment"># 保存图片</span></span><br><span class="line">cv2.imshow(<span class="string">'image'</span>, src_image) <span class="comment"># 显示图片</span></span><br><span class="line"><span class="comment"># 利用下标访问指定像素</span></span><br><span class="line"><span class="keyword">for</span> x <span class="keyword">in</span> <span class="built_in">range</span>(src_image.shape[<span class="number">0</span>]): <span class="comment"># 以行为主, 行数=图片height</span></span><br><span class="line">  <span class="keyword">for</span> y <span class="keyword">in</span> <span class="built_in">range</span>(src_image.shape[<span class="number">1</span>]):  <span class="comment"># 列数 = 图片width</span></span><br><span class="line">    src_image[x,y] = (<span class="number">255</span>,<span class="number">0</span>,<span class="number">255</span>)   <span class="comment"># (blue, green, red)  值越高表示对应颜色越显著, 全0为黑, 全255为白</span></span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python手册</title>
      <link href="/2019/08/28/20190828-python%E6%89%8B%E5%86%8C/"/>
      <url>/2019/08/28/20190828-python%E6%89%8B%E5%86%8C/</url>
      
        <content type="html"><![CDATA[<p>最近在学习查找资料的过程之中，看到了大佬的博客，觉得写得很好，也是我现在所欠缺的，所以下面先对大佬的博客进行复现。以供自己日后复习，查找，完善成自己的东西。  </p><h2 id="字符串固定字数，不足的空格补齐"><a href="#字符串固定字数，不足的空格补齐" class="headerlink" title="字符串固定字数，不足的空格补齐"></a>字符串固定字数，不足的空格补齐</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">str.ljust(10) # 左对齐 字符串长10位</span><br><span class="line">rjust，ljust和center三个方法来给字符串补全空格</span><br><span class="line">rjust，向右对其，在左边补空格</span><br><span class="line">ljust，向左对其，在右边补空格</span><br><span class="line">center，让字符串居中，在左右补空格</span><br></pre></td></tr></tbody></table></figure><h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><p>sorted: 返回一个新的 list<br>list.sort(): 改变 list 自身的值<br>reverse 参数: 默认为 False, 升序, True 时变为降序</p><h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><h3 id="循环删除列表元素"><a href="#循环删除列表元素" class="headerlink" title="循环删除列表元素"></a>循环删除列表元素</h3><p>常见错误: 直接删除, 或者正序删除</p><p>正确做法:<br>1.使用 pop, 倒序删除  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">for i in range(len(list)):</span><br><span class="line">   list.pop()</span><br></pre></td></tr></tbody></table></figure><pre><code>2.使用切片, 遍历拷贝列表, 操作原始列表, 用 remove 删除, remove 会操作首个遇到的匹配元素, 相等元素删除, 删除哪个都一样</code></pre><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">for x in enumerate(a[::]):</span><br><span class="line">   a.remove(x)</span><br><span class="line">for x in enumerate(a[::-1]):</span><br><span class="line">   a.remove(x)</span><br></pre></td></tr></tbody></table></figure><h3 id="遍历列表"><a href="#遍历列表" class="headerlink" title="遍历列表:"></a>遍历列表:</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">zz_list = [<span class="string">'a'</span>, <span class="string">'b'</span>, <span class="string">'c'</span>, <span class="string">'d'</span>]</span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">list</span>:</span><br><span class="line">    <span class="built_in">print</span>(index)</span><br><span class="line">    <span class="comment"># 0</span></span><br><span class="line">    <span class="comment"># 1</span></span><br><span class="line">    <span class="comment"># 2</span></span><br><span class="line">    <span class="comment"># 3</span></span><br><span class="line"><span class="keyword">for</span> index <span class="keyword">in</span> <span class="built_in">range</span>(<span class="built_in">len</span>(<span class="built_in">list</span>)):</span><br><span class="line">    <span class="built_in">print</span>(index)</span><br><span class="line">    <span class="comment"># 0</span></span><br><span class="line">    <span class="comment"># 1</span></span><br><span class="line">    <span class="comment"># 2</span></span><br><span class="line">    <span class="comment"># 3</span></span><br><span class="line"><span class="keyword">for</span> index, val <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">list</span>):</span><br><span class="line">    <span class="built_in">print</span>(index, val)</span><br><span class="line">    <span class="comment"># 0 a</span></span><br><span class="line">    <span class="comment"># 1 b</span></span><br><span class="line">    <span class="comment"># 2 c</span></span><br><span class="line">    <span class="comment"># 3 d</span></span><br><span class="line"><span class="comment"># 设置遍历的开始序号, val的输出不变</span></span><br><span class="line"><span class="keyword">for</span> i, val <span class="keyword">in</span> <span class="built_in">enumerate</span>(<span class="built_in">list</span>, <span class="number">2</span>):</span><br><span class="line">    <span class="built_in">print</span>(index, val)</span><br><span class="line">    <span class="comment"># 2 a</span></span><br><span class="line">    <span class="comment"># 3 b</span></span><br><span class="line">    <span class="comment"># 4 c</span></span><br><span class="line">    <span class="comment"># 5 d</span></span><br></pre></td></tr></tbody></table></figure><h3 id="append-方法"><a href="#append-方法" class="headerlink" title="append() 方法"></a>append() 方法</h3><p>追加单个元素</p><h3 id="extend-方法"><a href="#extend-方法" class="headerlink" title="extend() 方法"></a>extend() 方法</h3><p>extend()函数用于在列表末尾一次性追加另一个序列中的多个值(用新列表扩展原来的列表).<br>该方法没有返回值, 会直接在已经存在的列表中添加新的列表内容, extend和+=的作用差不多  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">a= [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]]</span><br><span class="line">b= [[<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>],[<span class="string">'d'</span>,<span class="string">'e'</span>,<span class="string">'f'</span>]]</span><br><span class="line">a.extend(b)</span><br><span class="line"><span class="built_in">print</span>(a)</span><br><span class="line"><span class="comment"># [[1, 2, 3], [4, 5, 6], ['a', 'b', 'c'], ['d', 'e', 'f']]</span></span><br></pre></td></tr></tbody></table></figure><h3 id="序列切片-双冒号"><a href="#序列切片-双冒号" class="headerlink" title="序列切片(双冒号)"></a>序列切片(双冒号)</h3><p>Python序列切片地址可以写为 [开始(包含) : 结束(不包含) : 步长]. 当开始省略的时候, 默认从第0项开始, 当结尾省略的时候, 默认到数组最后, 当步长省略的时候, 默认为1. 步长可以为负数, 代表从右向左取数.  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">a = range(10) # a = [0, 1, 2, 3, 4, 5, 6, 7, 8 ,9]</span><br><span class="line">a[0:9:1] # [0, 1, 2, 3, 4, 5, 6, 7, 8] 包含开始下标, 不包含结束下标</span><br><span class="line">a[1::2] # [1, 3, 5, 7, 9]</span><br><span class="line">a[::3] # [0, 3, 6, 9]</span><br><span class="line">a[::-1] # [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]</span><br><span class="line">a[::-2] # [9, 7, 5, 3, 1]</span><br></pre></td></tr></tbody></table></figure><h2 id="update-方法"><a href="#update-方法" class="headerlink" title="update() 方法"></a>update() 方法</h2><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">dict.update(dict2)</span><br></pre></td></tr></tbody></table></figure><p>将 dict2 中的键值更新到 dict 中, 对于存在的则覆盖原值, 对于不存在的则添加新的键值.</p><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">#!/usr/bin/python</span><br><span class="line">dict = {'Name': 'Zara', 'Age': 7}</span><br><span class="line">dict2 = {'Sex': 'female' }</span><br><span class="line">dict.update(dict2)</span><br><span class="line">print "Value : %s" %  dict</span><br></pre></td></tr></tbody></table></figure><p>以上实例输出结果为：  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Value : {'Age': 7, 'Name': 'Zara', 'Sex': 'female'}</span><br></pre></td></tr></tbody></table></figure><h2 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h2><p>遍历字典:  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">zz_dict = {'x': 1, 'y':2, 'z':3}</span><br></pre></td></tr></tbody></table></figure><p>遍历keys:  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出均为: x y z</span></span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> zz_dict:</span><br><span class="line">    <span class="built_in">print</span>(key)</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> zz_dict.iterkeys():</span><br><span class="line">    <span class="built_in">print</span>(key)</span><br><span class="line"><span class="keyword">for</span> key <span class="keyword">in</span> zz_dict.keys():</span><br><span class="line">    <span class="built_in">print</span>(key)</span><br></pre></td></tr></tbody></table></figure><p>遍历values:  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出均为 1 2 3</span></span><br><span class="line"><span class="keyword">for</span> value <span class="keyword">in</span> zz_dict.itervalues():</span><br><span class="line">    <span class="built_in">print</span>(value)</span><br><span class="line"><span class="keyword">for</span> value <span class="keyword">in</span> zz_dict.values():</span><br><span class="line">    <span class="built_in">print</span>(value)</span><br></pre></td></tr></tbody></table></figure><p>遍历keys和values  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 输出为: x corresponds to 1 (其余两个也一样)</span></span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> zz_dict.iteritems():  <span class="comment"># python3 没有iteritems</span></span><br><span class="line">    <span class="built_in">print</span>(key, <span class="string">"corresponds to"</span>, value)</span><br><span class="line"><span class="keyword">for</span> key, value <span class="keyword">in</span> zz_dict.items():</span><br><span class="line">    <span class="built_in">print</span>(key, <span class="string">"corresponds to"</span>, value)</span><br></pre></td></tr></tbody></table></figure><h2 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h2><h3 id="判断字符串是否为字母或者数字"><a href="#判断字符串是否为字母或者数字" class="headerlink" title="判断字符串是否为字母或者数字"></a>判断字符串是否为字母或者数字</h3><p>str.isalnum() 字母或数字<br>str.isalpha() 字母<br>str.isdigit() 数字<br>str.isspace() 空白符, \t, \n, \r</p><p>isdigit() 和 isnumeric() 的区别  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">num = "1"  #unicode</span><br><span class="line">num.isdigit()   # True</span><br><span class="line">num.isdecimal() # True</span><br><span class="line">num.isnumeric() # True</span><br><span class="line">num = "1" # 全角</span><br><span class="line">num.isdigit()   # True</span><br><span class="line">num.isdecimal() # True</span><br><span class="line">num.isnumeric() # True</span><br><span class="line">num = b"1" # byte</span><br><span class="line">num.isdigit()   # True</span><br><span class="line">num.isdecimal() # AttributeError 'bytes' object has no attribute 'isdecimal'</span><br><span class="line">num.isnumeric() # AttributeError 'bytes' object has no attribute 'isnumeric'</span><br><span class="line">num = "IV" # 罗马数字</span><br><span class="line">num.isdigit()   # True</span><br><span class="line">num.isdecimal() # False</span><br><span class="line">num.isnumeric() # True</span><br><span class="line">num = "四" # 汉字</span><br><span class="line">num.isdigit()   # False</span><br><span class="line">num.isdecimal() # False</span><br><span class="line">num.isnumeric() # True</span><br></pre></td></tr></tbody></table></figure><hr><p>isdigit()<br>True: Unicode数字，byte数字（单字节），全角数字（双字节），罗马数字<br>False: 汉字数字<br>Error: 无</p><p>isdecimal()<br>True: Unicode数字，，全角数字（双字节）<br>False: 罗马数字，汉字数字<br>Error: byte数字（单字节）</p><p>isnumeric()<br>True: Unicode数字，全角数字（双字节），罗马数字，汉字数字<br>False: 无<br>Error: byte数字（单字节）</p><h3 id="str-rstrip"><a href="#str-rstrip" class="headerlink" title="str.rstrip()"></a>str.rstrip()</h3><p>参数:<br>chars: 指定删除的字符(默认为空格或换行符)</p><p>返回值:<br>返回删除指定字符后的新字符串</p><p>备注:<br>删除字符串末尾的指定字符(默认为空格或换行符)  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">str.rstrip([chars])</span><br></pre></td></tr></tbody></table></figure><h3 id="str-strip"><a href="#str-strip" class="headerlink" title="str.strip()"></a>str.strip()</h3><p>参数<br>chars — 移除字符串头尾指定的字符序列。<br>返回值<br>返回移除字符串头尾指定的字符生成的新字符串。<br>备注:  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">str.strip([chars])</span><br></pre></td></tr></tbody></table></figure><h3 id="str-split"><a href="#str-split" class="headerlink" title="str.split()"></a>str.split()</h3><p>参数</p><ul><li>str — 分隔符，默认为所有的空字符，包括空格、换行(\n)、制表符(\t)等。</li><li>num — 分割次数。默认为 -1, 即分隔所有。<br>返回值</li><li>返回分割后的字符串列表。</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">str.split(str="", num=string.count(str)).</span><br></pre></td></tr></tbody></table></figure><h2 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h2><h2 id="reduce-函数"><a href="#reduce-函数" class="headerlink" title="reduce() 函数"></a>reduce() 函数</h2><p>reduce() 函数会对参数序列中元素进行累积。<br>函数将一个数据集合（链表，元组等）中的所有数据进行下列操作：用传给 reduce 中的函数 function（有两个参数）先对集合中的第 1、2 个元素进行操作，得到的结果再与第三个数据用 function 函数运算，最后得到一个结果。<br>reduce() 函数语法：  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">reduce(function, iterable[, initializer])</span><br></pre></td></tr></tbody></table></figure><p>参数</p><ul><li>function — 函数，有两个参数</li><li>iterable — 可迭代对象</li><li>initializer — 可选，初始参数<br>返回值</li><li>返回函数计算结果<br>实例</li></ul><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">```python</span><br><span class="line">&gt;&gt;&gt;<span class="keyword">def</span> <span class="title function_">add</span>(<span class="params">x, y</span>) :            <span class="comment"># 两数相加</span></span><br><span class="line"><span class="meta">... </span>    <span class="keyword">return</span> x + y</span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>reduce(add, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])   <span class="comment"># 计算列表和：1+2+3+4+5</span></span><br><span class="line"><span class="number">15</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>reduce(<span class="keyword">lambda</span> x, y: x+y, [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>])  <span class="comment"># 使用 lambda 匿名函数</span></span><br><span class="line"><span class="number">15</span></span><br></pre></td></tr></tbody></table></figure><h2 id="zip-函数"><a href="#zip-函数" class="headerlink" title="zip() 函数"></a>zip() 函数</h2><p>zip() 函数用于将可迭代的对象作为参数, 将对象中对应的元素打包成一个个 <strong>元组</strong> ,然后返回有这些元组组成的 对象. ( 相比于python2中返回列表的方式, 这样做的好处是节约了不少的内存 )<br>可以用list()转换或者dict()转换将对象转换成相应的数据类型<br>如果各个迭代器的元素个数不一致, 则返回列表长度与最短的对象相同, 多出来的部分会被舍弃, 利用*号操作符, 可以将元组解压成列表.</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">a = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">b = [<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line">c = [<span class="string">'a'</span>,<span class="string">'b'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>,<span class="string">'e'</span>,<span class="string">'f'</span>]</span><br><span class="line">zip_ab = <span class="built_in">zip</span>(a,b)</span><br><span class="line"><span class="built_in">print</span>(zip_ab) <span class="comment"># &lt;zip object at 0x104605348&gt;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">dict</span>(zip_ab)) <span class="comment"># {1: 4, 2: 5, 3: 6}</span></span><br><span class="line"><span class="comment"># !!!注意, 一旦将zip_ab转换成dict以后, zip_ab内部就为空了!! 例如, 再次调用上面的语句:</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">dict</span>(zip_ab)) <span class="comment"># {}</span></span><br><span class="line"><span class="comment"># 但是zip_ab对象本身不会消失, 地址仍然不变</span></span><br><span class="line"><span class="built_in">print</span>(zip_ab) <span class="comment"># &lt;zip object at 0x104605348&gt;</span></span><br><span class="line">zip_abc = <span class="built_in">zip</span>(a,b,c) <span class="comment"># 注意, 三个元素的zip是不能转换成dict类型的</span></span><br><span class="line"><span class="built_in">print</span>(zip_abc) <span class="comment"># &lt;zip object at 0x1046054c8&gt;</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">list</span>(zip_abc)) <span class="comment"># [(1, 4, 'a'), (2, 5, 'b'), (3, 6, 'c')]</span></span><br><span class="line">zip_abc = <span class="built_in">zip</span>(a,b,c)</span><br><span class="line">z_a, z_b, z_c = <span class="built_in">zip</span>(*zip_abc) <span class="comment"># 利用zip(*)可以将zip对象重新解压, 返回类型是元组</span></span><br><span class="line"><span class="built_in">print</span>(z_a) <span class="comment"># (1,2,3)</span></span><br><span class="line"><span class="built_in">print</span>(z_b) <span class="comment"># (4,5,6)</span></span><br><span class="line"><span class="built_in">print</span>(z_c) <span class="comment"># ('a','b','c')</span></span><br></pre></td></tr></tbody></table></figure><h2 id="getattr-函数"><a href="#getattr-函数" class="headerlink" title="getattr() 函数"></a>getattr() 函数</h2><p>getattr()函数用于返回一个对象的属性值, 语法如下</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">getattr(object, name[, default])</span><br></pre></td></tr></tbody></table></figure><p>参数：</p><ul><li>object: 对象</li><li>name: 字符串, 对象属性</li><li>default: 默认返回值, 如果不提供该参数, 在没有对应属性时, 将触发Attributerror<br>实例<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt;<span class="keyword">class</span> <span class="title class_">A</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line"><span class="meta">... </span>    bar = <span class="number">1</span></span><br><span class="line">...</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>a = A()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">getattr</span>(a, <span class="string">'bar'</span>)        <span class="comment"># 获取属性 bar 值</span></span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">getattr</span>(a, <span class="string">'bar2'</span>)       <span class="comment"># 属性 bar2 不存在，触发异常</span></span><br><span class="line">Traceback (most recent call last):</span><br><span class="line">File <span class="string">"&lt;stdin&gt;"</span>, line <span class="number">1</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">AttributeError: <span class="string">'A'</span> <span class="built_in">object</span> has no attribute <span class="string">'bar2'</span></span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">getattr</span>(a, <span class="string">'bar2'</span>, <span class="number">3</span>)    <span class="comment"># 属性 bar2 不存在，但设置了默认值</span></span><br><span class="line"><span class="number">3</span></span><br><span class="line">&gt;&gt;&gt;</span><br></pre></td></tr></tbody></table></figure></li></ul><h2 id="dir-函数"><a href="#dir-函数" class="headerlink" title="dir() 函数"></a>dir() 函数</h2><p>可以查看某个类的所有方法和属性</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">members = [attr for attr in dir(classA)]</span><br></pre></td></tr></tbody></table></figure><ul><li>_var: 在一个模块中以单下划线开头的变量和函数会被默认当做内部函数, 在使用from a_module import * 导入时, 这部分变量和函数不会被导入. 不过如果使用import a_module导入模块时, 仍然可以用a_module._var的形式访问该变量或函数</li><li>var_: 有时候, 一个变量的最适合的名称已经被另一个关键字所占用. 在这种情况下, 可以在名称的末尾附加一个下划线来解决冲突.</li><li>__var: 双下划线前缀会导致Python解释器重写属性名称, 以避免子类中的命名冲突. 举例来说, 如果在class Test中有一个成员__x, 那么当利用内置函数dir(Test)来查看类的属性时, 会发现__x被解释器重命名为_Test__x. 双下划线的名称修饰同样也适用于方法名称.</li><li><strong>var</strong>: 双下划线开头和结尾的是一些 Python 的特殊对象, 如类成员的 <strong>init</strong>, <strong>del</strong>, <strong>name</strong>, <strong>call</strong> 等. Python 官方推荐永远不要讲这样的命名方式应用于自己的变量或函数. 有一种说法是说双下划线建议为类的私有成员, 但是 PEP8 当前的官方版本中并没有明说.</li><li>_: 有时候我们会用一个独立的下划线作为一个名字, 这通常是用来指示某个变量时临时的或者无关紧要的.</li></ul><h2 id="类的特殊方法"><a href="#类的特殊方法" class="headerlink" title="类的特殊方法"></a>类的特殊方法</h2><h3 id="call"><a href="#call" class="headerlink" title="call()"></a>call()</h3><p>在 Python 中, 函数实际上也是一个对象:<br>        1<br>        2<br>        3<br>        f = abs<br>        print(f.<strong>name</strong>) # ‘abs’<br>        print(f(-123)) # 123</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">从上面可以看出, 函数是一个对象, 当它赋给另一个变量时, 该变量也是一个函数对象, 可以起到与原函数相同的效果. 在 Python 中, 一个类实例也可以变成一个可调用对象, 只需要实现一个特殊方法 __call__() 即可. 下面我们举例把 Person 类变成一个可调用对象:  </span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">    class Person(object):</span><br><span class="line">        def __init__(self, name, gender):</span><br><span class="line">            self.name = name</span><br><span class="line">            self.gender = gender</span><br><span class="line">        def __call__(self, friend):</span><br><span class="line">            print("name:", self.name)</span><br><span class="line">            print("friend:", friend)</span><br></pre></td></tr></tbody></table></figure><p>接下来我们就可以将 Person 类的实例对象当做一个函数来使用, 如下所示:  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">p = Person('Bob', 'male')</span><br><span class="line">p('Tim')</span><br><span class="line"># name: Bob</span><br><span class="line"># friend: Tim</span><br></pre></td></tr></tbody></table></figure><h3 id="getitem"><a href="#getitem" class="headerlink" title="getitem()"></a>getitem()</h3><p>凡是在类中定义了 <strong>getitem</strong>() 方法, 那么它的实例对象就是可以通过 [] 操作符来访问指定的成员或进行特定的行为, 大多数情况下会将该方法实现成通过索引来方法元素的形式.  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DataBase</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="built_in">super</span>(DataBase, self).__init__()</span><br><span class="line">        self.vals = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self, key</span>):</span><br><span class="line">        <span class="keyword">return</span> self.vals[key]</span><br></pre></td></tr></tbody></table></figure><h3 id="setitem"><a href="#setitem" class="headerlink" title="setitem()"></a>setitem()</h3><p>使得可以通过 A[3] = 4, B[“a”] = 5 等方式来对类中的元素进行赋值</p><h3 id="file"><a href="#file" class="headerlink" title="file()"></a>file()</h3><p>查看模块的路径</p><h3 id="len"><a href="#len" class="headerlink" title="len()"></a>len()</h3><p>使得类对象可以使用 Python 的内建方法 len(), 返回你自定义的数值.  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">DictDemo</span>:</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self,key,value</span>):</span><br><span class="line">        self.<span class="built_in">dict</span> = {}</span><br><span class="line">        self.<span class="built_in">dict</span>[key] = value</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__getitem__</span>(<span class="params">self,key</span>):</span><br><span class="line">        <span class="keyword">return</span> self.<span class="built_in">dict</span>[key]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__setitem__</span>(<span class="params">self,key,value</span>):</span><br><span class="line">        self.<span class="built_in">dict</span>[key] = value</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__len__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="built_in">len</span>(self.<span class="built_in">dict</span>)</span><br><span class="line">dictDemo = DictDemo(<span class="string">'key0'</span>,<span class="string">'value0'</span>)</span><br><span class="line"><span class="built_in">print</span>(dictDemo[<span class="string">'key0'</span>]) <span class="comment">#value0</span></span><br><span class="line">dictDemo[<span class="string">'key1'</span>] = <span class="string">'value1'</span></span><br><span class="line"><span class="built_in">print</span>(dictDemo[<span class="string">'key1'</span>]) <span class="comment">#value1</span></span><br><span class="line"><span class="built_in">print</span>(<span class="built_in">len</span>(dictDemo)) <span class="comment">#2</span></span><br></pre></td></tr></tbody></table></figure><h3 id="repr"><a href="#repr" class="headerlink" title="repr()"></a>repr()</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Test</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, value=<span class="string">'hello, world!'</span></span>):</span><br><span class="line">        self.data = value</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t = Test()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>t</span><br><span class="line">&lt;__main__.Test at <span class="number">0x7fa91c307190</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span> t</span><br><span class="line">&lt;__main__.Test <span class="built_in">object</span> at <span class="number">0x7fa91c307190</span>&gt;</span><br><span class="line"><span class="comment"># 看到了么？上面打印类对象并不是很友好，显示的是对象的内存地址</span></span><br><span class="line"><span class="comment"># 下面我们重构下该类的__repr__以及__str__，看看它们俩有啥区别</span></span><br><span class="line"><span class="comment"># 重构__repr__</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">TestRepr</span>(<span class="title class_ inherited__">Test</span>):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__repr__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'TestRepr(%s)'</span> % self.data</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tr = TestRepr()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>tr</span><br><span class="line">TestRepr(hello, world!)</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span> tr</span><br><span class="line">TestRepr(hello, world!)</span><br><span class="line"><span class="comment"># 重构__repr__方法后，不管直接输出对象还是通过print打印的信息都按我们__repr__方法中定义的格式进行显示了</span></span><br><span class="line"><span class="comment"># 重构__str__</span></span><br><span class="line">calss TestStr(Test):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__str__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'[Value: %s]'</span> % self.data</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ts = TestStr()</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span>ts</span><br><span class="line">&lt;__main__.TestStr at <span class="number">0x7fa91c314e50</span>&gt;</span><br><span class="line"><span class="meta">&gt;&gt;&gt; </span><span class="built_in">print</span> ts</span><br><span class="line">[Value: hello, world!]</span><br><span class="line"><span class="comment"># 你会发现，直接输出对象ts时并没有按我们__str__方法中定义的格式进行输出，而用print输出的信息却改变了</span></span><br></pre></td></tr></tbody></table></figure><h3 id="str"><a href="#str" class="headerlink" title="str()"></a>str()</h3><p>参见 <strong>repr</strong>() 代码示例</p><h2 id="星号"><a href="#星号" class="headerlink" title="星号 *"></a>星号 *</h2><p>*: 乘法<br>**: 乘幂</p><h3 id="用于函数参数"><a href="#用于函数参数" class="headerlink" title="用于函数参数"></a>用于函数参数</h3><p>单星号: 将所有参数以 元组(tuple) 的形式导入  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">foo</span>(<span class="params">param1, *param2</span>):</span><br><span class="line">    <span class="built_in">print</span>(param1)</span><br><span class="line">    <span class="built_in">print</span>(param2)</span><br><span class="line">foo(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>)</span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># (2,3,4,5)</span></span><br></pre></td></tr></tbody></table></figure><p>双星号: 将所有参数以 字典 的形式导入  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">bar</span>(<span class="params">param1, **param2</span>):</span><br><span class="line">    <span class="built_in">print</span>(param1)</span><br><span class="line">    <span class="built_in">print</span>(param2)</span><br><span class="line">bar(<span class="number">1</span>, a=<span class="number">2</span>, b=<span class="number">3</span>)</span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># {'a': 2, 'b': 3}</span></span><br></pre></td></tr></tbody></table></figure><p>当然这两个用法可以同时出现在一个函数中:  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">fun</span>(<span class="params">a, b=<span class="number">10</span>, *args, **kwargs</span>):</span><br><span class="line">    <span class="built_in">print</span>(a)</span><br><span class="line">    <span class="built_in">print</span>(b)</span><br><span class="line">    <span class="built_in">print</span>(args)</span><br><span class="line">    <span class="built_in">print</span>(kwargs)</span><br><span class="line">fun(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,e=<span class="number">5</span>,f=<span class="number">6</span>)</span><br><span class="line"><span class="comment"># 1</span></span><br><span class="line"><span class="comment"># 2</span></span><br><span class="line"><span class="comment"># (3,4)</span></span><br><span class="line"><span class="comment"># {'e': 5, 'f': 6}</span></span><br></pre></td></tr></tbody></table></figure><h2 id="globals-函数"><a href="#globals-函数" class="headerlink" title="globals() 函数"></a>globals() 函数</h2><p>该函数会以字典类型返回当前位置的全部全局变量</p><h2 id="stripe"><a href="#stripe" class="headerlink" title="stripe()"></a>stripe()</h2><h2 id="readlines"><a href="#readlines" class="headerlink" title="readlines()"></a>readlines()</h2><h2 id="lambda-函数"><a href="#lambda-函数" class="headerlink" title="lambda 函数"></a>lambda 函数</h2><h2 id="3-6新功能-f-string"><a href="#3-6新功能-f-string" class="headerlink" title="3.6新功能 f string"></a>3.6新功能 f string</h2><h2 id="包的导入机制"><a href="#包的导入机制" class="headerlink" title="包的导入机制"></a>包的导入机制</h2><h3 id="模块和包的定义"><a href="#模块和包的定义" class="headerlink" title="模块和包的定义"></a>模块和包的定义</h3><p>模块(module): 用来从逻辑上组织 Python 代码(变量, 函数, 类), 通常是一个.py文件.<br>包(package): 定义了一个由模块和子包组成的 Python 应用程序执行环境, 本质上就是一个有层次的文件目录结果(必须带有一个__init__.py文件)</p><h3 id="import-的搜索路径"><a href="#import-的搜索路径" class="headerlink" title="import 的搜索路径"></a>import 的搜索路径</h3><ol><li>在当前目录下搜索</li><li>在环境变量PYTHONPATH中指定的路径列表中搜索</li><li>在 Python 安装路径的lib库中搜索<br>Python 所有加载的模型信息都存放在sys.modules结构中, 当import一个模块时, 会按如下步骤来进行:</li><li>如果import A, 检查sys.modules中是否已经有A, 如果有则不加载, 如果没有则为A创建module对象, 并加载A;</li><li>如果是from A import B, 先为A创建module对象, 再解析A(此时会加载并执行A中的所有代码), 从中寻找B并填充到A的__dict__中.<br>在导入模块的时候, 模块所在文件夹会自动生成一个__pycache__/module_name.cpython-35.pyc的文件.</li></ol><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">```python</span><br><span class="line"><span class="keyword">import</span> module_name的本质是将module_name.py中的全部代码加载到内存中, 并将其赋值给与模块同名的变量, 这个变量的类型是<span class="keyword">class</span>&lt;module&gt;.</span><br><span class="line"><span class="keyword">from</span> module_name <span class="keyword">import</span> name的本质是将指定的变量或者方法导入到当前的文件中</span><br><span class="line"><span class="keyword">import</span> package_name的本质是执行该包下的__init__.py文件, 在执行文件后, 会在package_name目录下生成一个__pycache__/__init__cpython-<span class="number">35.</span>pyc文件.</span><br><span class="line"><span class="keyword">from</span> package_name <span class="keyword">import</span> *的本质是导入__init__.py文件中的__all__列表(eg. __all__ = [<span class="string">'L2Norm'</span>, <span class="string">'MultiBoxLoss'</span>]).</span><br></pre></td></tr></tbody></table></figure><h3 id="相对导入和绝对导入"><a href="#相对导入和绝对导入" class="headerlink" title="相对导入和绝对导入"></a>相对导入和绝对导入</h3><p>绝对导入:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> A.B</span><br><span class="line"><span class="keyword">from</span> A <span class="keyword">import</span> B</span><br></pre></td></tr></tbody></table></figure><p>相对导入:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> B <span class="comment"># . 代表当前路径</span></span><br><span class="line"><span class="keyword">from</span> ..A <span class="keyword">import</span> B <span class="comment"># .. 代表上层路径, ... 代表上上层路径.</span></span><br></pre></td></tr></tbody></table></figure><p>在没有明确指定包结构的情况下, Python 是根据__name__来决定一个模块在包中的结构的, 如果是__main__, 则它本身就是顶层模块, 没有包结构, 如果是A.B.C结构, 则A是顶层模块. Python 的导入方式的不同具有不同的规则:<br>1.如果是绝对导入, 一个模块只能导入自身的子模块或者和它的顶层模块同级别的模块及其子模块.<br>2.如果是相对导入, 一个模块必须有包结构且只能导入它的顶层模块内部的模块.<br>如果一个模块被直接运行, 则它自己为顶层模块, 不存在层次结构, 所以也找不到上层(..)的相对路径<br>Python2.x 默认为相对路径导入, 而 Python3.x 默认为绝对路径导入, 这样可以避免导入的子包覆盖掉标准库模块. 通常, 在 Python2.x 中, 我们利用下面的语句来使其导入规则遵循 Python3.x</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> __future__ <span class="keyword">import</span> absolute_import</span><br></pre></td></tr></tbody></table></figure><p>absolute_import的意思并不是将所有的导入都视为绝对导入, 而是指禁用隐式相对导入(implicit relative import), 关于隐式的显示的具体区别, 可以看下面的例子, 假设有如下的包结构:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">thing</span><br><span class="line">└── __init__.py</span><br><span class="line">├── books</span><br><span class="line">│ ├── __init__.py</span><br><span class="line">│ ├── adventure.py</span><br><span class="line">│ ├── history.py</span><br><span class="line">│ ├── horror.py</span><br><span class="line">│ └── lovestory.py</span><br><span class="line">├── furniture</span><br><span class="line">│ ├── __init__.py</span><br><span class="line">│ ├── armchair.py</span><br><span class="line">│ ├── bench.py</span><br><span class="line">│ ├── screen.py</span><br><span class="line">│ └── stool.py</span><br></pre></td></tr></tbody></table></figure><p>那么如果想在stool.py中导入bench模块, 则有如下几种方式:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> bench <span class="comment"># 隐式相对导入</span></span><br><span class="line"><span class="keyword">from</span> . <span class="keyword">import</span> bench <span class="comment"># 显式相对导入</span></span><br><span class="line"><span class="keyword">from</span> furniture <span class="keyword">import</span> bench <span class="comment"># 绝对导入</span></span><br></pre></td></tr></tbody></table></figure><p>隐式相对导入没有告诉解释器相对于谁进行导入, 默认相对于当前模块; 而显式相对导入则明确告诉了解释器相对于谁来导入. 以上导入方式的第三种是官方推荐的, 第一种是官方强烈不推荐的, Python3 中第一种导入方式只能用于导入sys.path中的模块.<br>**注意, 还有相对导入的模块不能被直接运行, 会提示如下错误:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Traceback (most recent call last):</span><br><span class="line">  File <span class="string">"test.py"</span>, line <span class="number">8</span>, <span class="keyword">in</span> &lt;module&gt;</span><br><span class="line">    <span class="keyword">from</span> .ssd <span class="keyword">import</span> SSD</span><br><span class="line">ModuleNotFoundError: No module named <span class="string">'__main__.ssd'</span>; <span class="string">'__main__'</span> <span class="keyword">is</span> <span class="keyword">not</span> a package</span><br></pre></td></tr></tbody></table></figure><p>另外存在一种情况就是: 假如有两个模块a.py和b.py放在同一个目录下, 则可以直接在a.py中使用import b来导入模块b. 这是为什么呢? 我们上面说了在 Python3.x 中不能使用这种隐式相对导入, 但是这里却可以成功导入, 这是因为此时我们是直接运行a.py, 所以a.py和b.py的目录没有被当做一个包来处理, 因此不涉及相对导入和绝对导入的概念. 因此相对导入和绝对导入仅仅是针对于包而言的.</p><h3 id="综合距离"><a href="#综合距离" class="headerlink" title="综合距离"></a>综合距离</h3><p>存在目录结构如下所示:</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">dirRoot</span><br><span class="line">└── __init__.py</span><br><span class="line">├── file1.py</span><br><span class="line">├── file2.py</span><br><span class="line">├── dirA</span><br><span class="line">│ ├── __init__.py</span><br><span class="line">│ ├── a1.py</span><br><span class="line">│ └── a2.py</span><br><span class="line">├── dirB</span><br><span class="line">│ ├── __init__.py</span><br><span class="line">│ ├── b1.py</span><br><span class="line">│ └── b2.py</span><br></pre></td></tr></tbody></table></figure><p>直接运行a1.py, 并希望导入a2模块:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a1.py</span></span><br><span class="line"><span class="keyword">import</span> a2 <span class="comment"># 正确, 此时并未将 dirA 当做包来处理, a1.py 和 a2.py 相当于两个独立的模块</span></span><br><span class="line"><span class="keyword">from</span> a2 <span class="keyword">import</span> func_a2 <span class="comment"># 正确</span></span><br><span class="line"><span class="keyword">from</span> .a2 <span class="keyword">import</span> func_a2 <span class="comment"># 错误, 当进行相对导入时, 不能直接运行</span></span><br></pre></td></tr></tbody></table></figure><p>直接运行file1.py, 并希望导入a1模块, 同时a1模块中需要导入a2模块:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># file1.py</span></span><br><span class="line"><span class="keyword">from</span> dirA <span class="keyword">import</span> a1</span><br><span class="line">a1.func_a1() <span class="comment"># a1.py 中的函数</span></span><br><span class="line">a1.func_a2() <span class="comment"># a1.py 中导入了 a2.py 的函数, 可以直接使用</span></span><br><span class="line"><span class="comment"># a1.py</span></span><br><span class="line"><span class="keyword">import</span> a2 <span class="comment"># 错误, 此时由于 dirA 中有 __init__.py 文件, 因此会将 dirA 当做包来处理,</span></span><br><span class="line"><span class="comment"># 由于 Python3.x 不允许使用隐式的相对导入, 因此该语句非法</span></span><br><span class="line"><span class="keyword">from</span> a2 <span class="keyword">import</span> func_a2 <span class="comment"># 错误, 原因同上</span></span><br><span class="line"><span class="keyword">from</span> .a2 <span class="keyword">import</span> func_a2 <span class="comment"># 正确, 当进行相对导入时, 需要使用显式的相对导入</span></span><br></pre></td></tr></tbody></table></figure><p>直接运行file1.py, 并希望导入a1模块, 同时a1模块中需要导入dirB/b1模块(跨文件夹导入):</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># file1.py</span></span><br><span class="line"><span class="keyword">from</span> dirA <span class="keyword">import</span> a1</span><br><span class="line">a1.func_a1() <span class="comment"># a1.py 中的函数</span></span><br><span class="line">a1.func_a2() <span class="comment"># a2.py 中的函数</span></span><br><span class="line">a1.func_b1() <span class="comment"># b1.py 中的函数</span></span><br><span class="line"><span class="comment"># a1.py</span></span><br><span class="line"><span class="keyword">from</span> .a2 <span class="keyword">import</span> func_a2 <span class="comment"># 推荐使用绝对导入 from dirA.a1 import func_a2</span></span><br><span class="line"><span class="keyword">from</span> dirB <span class="keyword">import</span> b1 <span class="comment"># 由于运行的是 file1.py 文件, 因此顶层目录是 dirRoot</span></span><br><span class="line"><span class="keyword">from</span> dirB.b1 <span class="keyword">import</span> func_b1 <span class="comment"># 所以可以直接使用 dirB 包</span></span><br></pre></td></tr></tbody></table></figure><p>直接运行a1.py, 并希望跨目录的导入dirB/b1模块. 由于这种跨目录的导入超越了顶层路径的限制, 因此必须使用sys.path.append()方法来额外添加搜索路径, 否则无法正常导</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># a1.py</span></span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line">sys.path.append(<span class="string">"../"</span>) <span class="comment"># 将 dirA 的上一次目录添加到搜索路径中</span></span><br><span class="line"><span class="keyword">from</span> dirB <span class="keyword">import</span> b1 <span class="comment"># 正确, 注意必须先添加 path, 然后再导入</span></span><br><span class="line"><span class="keyword">from</span> dirB.b1 <span class="keyword">import</span> func_b1 <span class="comment"># 正确</span></span><br><span class="line"><span class="keyword">from</span> .a2 <span class="keyword">import</span> func_a2 <span class="comment"># 这里是错误的, 当直接执行 a1.py 时, a1.py 中不能包含显式相对导入</span></span><br></pre></td></tr></tbody></table></figure><h2 id="获取-python-版本"><a href="#获取-python-版本" class="headerlink" title="获取 python 版本:"></a>获取 python 版本:</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(sys.version_info)</span><br></pre></td></tr></tbody></table></figure><h2 id="获取包的安装位置"><a href="#获取包的安装位置" class="headerlink" title="获取包的安装位置"></a>获取包的安装位置</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">print</span>(cv2)</span><br></pre></td></tr></tbody></table></figure><h2 id="解析-xml-文件"><a href="#解析-xml-文件" class="headerlink" title="解析 xml 文件"></a>解析 xml 文件</h2><p>导入:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">if</span> sys.version_info[<span class="number">0</span>] == <span class="number">2</span>:</span><br><span class="line">    <span class="keyword">import</span> xml.etree.cElementTree <span class="keyword">as</span> ET</span><br><span class="line"><span class="keyword">else</span>:</span><br><span class="line">    <span class="keyword">import</span> xml.etree.ElementTree <span class="keyword">as</span> ET</span><br></pre></td></tr></tbody></table></figure><p>解析:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">xmlfile = ET.parse(xmlfile_path)</span><br><span class="line">root = xmlfile.getroot() <span class="comment"># 获取根节点</span></span><br><span class="line">root.tag <span class="comment"># 标签</span></span><br><span class="line">root.attrib <span class="comment"># 属性字典</span></span><br><span class="line"><span class="keyword">for</span> child <span class="keyword">in</span> root: <span class="comment"># 迭代访问子节点</span></span><br><span class="line">    <span class="built_in">print</span>(child.tag, child.attrib)</span><br><span class="line"><span class="comment"># 可以通过索引访问嵌套节点的内容</span></span><br><span class="line">root[<span class="number">0</span>][<span class="number">1</span>].text</span><br><span class="line">Element.findall() <span class="comment">#</span></span><br><span class="line">Element.find() <span class="comment">#</span></span><br></pre></td></tr></tbody></table></figure><h2 id="python-中-x3D-x3D-和-is-的区别"><a href="#python-中-x3D-x3D-和-is-的区别" class="headerlink" title="python 中 == 和 is 的区别"></a>python 中 == 和 is 的区别</h2><p>== 只用于判断值是否相等<br>is 用于判断两个对象是否为同一个实例<br>小整数对象池: Python 为了优化速度，使用了小整数对象池，避免为整数频繁申请和销毁内存空间。而Python 对小整数的定义是 [-5, 257)，只有数字在-5到256之间它们的id才会相等，超过了这个范围就不行了，同样的道理，字符串对象也有一个类似的缓冲池，超过区间范围内自然不会相等了</p><h2 id="队列-queue"><a href="#队列-queue" class="headerlink" title="队列 queue"></a>队列 queue</h2><p>在 Python3 中, 原来的Queue模块被重命名为queue, 该模块包含以下三类数据结构:</p><ul><li>queue.Queue(maxsize=0): FIFO queue, 先进先出队列, 代表普通队列</li><li>queue.LifoQueue(maxsize=0): LIFO queue, 后进先出队列, 类似栈的作用</li><li>queue.PriorityQueue(maxsize=0): 优先级队列, 类似堆的作用. 默认为小顶堆, 常用形式为元组:(priority_number, data)<br>上面的 maxsize 表明了队列中最大可以容纳的元素数量, 如果超过, 则无法插入. 当 maxsize &lt;= 0 时, 代表元素数量无限制.<br>公有方法(以上三个通用):</li><li>qsize(): 返回 approximate size, qsize() &gt; 0 不保证get()一定 work, 同理, qsize() &lt; maxsize 不保证put()一定 work.</li><li>empty(): 如果队列为空, 返回 True. 和qsize()一样, 不提供保证性.</li><li>full(): 如果队列满, 返回 True. 不提供保证性</li><li>put(item[, block[, timeout]])</li><li>put_nowait(item): 等价于put(item, False)</li><li>get([block[, timeout]])</li><li>get_nowait(): 等价于get(False)</li><li>task_done():</li><li>join():</li></ul><h2 id="堆-heapq"><a href="#堆-heapq" class="headerlink" title="堆 heapq"></a>堆 heapq</h2><p>heapq 模块只有最小堆的功能, 要实现最大堆, 需要在入堆和出堆的时候取反, 并且 heapq 模块只能作用于数值型类型.<br>最大堆: _heapify_max(), _heappop_max()<br>给定一组数据, 创建堆, 两种方式(二者等价):</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> heapq</span><br><span class="line">data = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">2</span>,<span class="number">8</span>,<span class="number">5</span>]</span><br><span class="line">heap = []</span><br><span class="line"><span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">    heapq.heappush(heap, n) <span class="comment"># 方法一 逐个构建</span></span><br><span class="line">heapq.heapify(data) <span class="comment"># 方法二 原地构建, 效率更高</span></span><br></pre></td></tr></tbody></table></figure><p>小顶堆:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">heap = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">2</span>,<span class="number">8</span>,<span class="number">5</span>]</span><br><span class="line">heapq.heapify(heap)</span><br><span class="line">heapq.heappop(heap) <span class="comment"># 返回并删除堆顶</span></span><br><span class="line">heapq.heapreplace(heap, <span class="number">10</span>) <span class="comment"># 删除堆顶并添加新值</span></span><br><span class="line">heapq.heappushpop(heap, <span class="number">10</span>) <span class="comment"># 先将新值加入堆中, 然后立刻弹出堆顶</span></span><br><span class="line"><span class="built_in">print</span>(heap[<span class="number">0</span>]) <span class="comment"># 查看堆顶</span></span><br></pre></td></tr></tbody></table></figure><p>大顶堆:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 方法一: 取负值</span></span><br><span class="line">heap = [-<span class="number">1</span>,-<span class="number">3</span>,-<span class="number">6</span>,-<span class="number">2</span>,-<span class="number">8</span>,-<span class="number">5</span>]</span><br><span class="line"><span class="comment"># 方法二: 内置方法</span></span><br><span class="line">heap = [<span class="number">1</span>,<span class="number">3</span>,<span class="number">6</span>,<span class="number">2</span>,<span class="number">8</span>,<span class="number">5</span>]</span><br><span class="line">heapq._heapify_max(heap) <span class="comment"># max_heap</span></span><br><span class="line"><span class="built_in">print</span>(heap[<span class="number">0</span>]) <span class="comment"># 查看堆顶, 8</span></span><br><span class="line">heapq._heappop_max(heap) <span class="comment"># po from maxheap</span></span><br><span class="line"><span class="built_in">print</span>(heap[<span class="number">0</span>]) <span class="comment"># 6</span></span><br><span class="line">heapq._heapreplace_max(heap, <span class="number">10</span>)</span><br><span class="line"><span class="built_in">print</span>(heap[<span class="number">0</span>]) <span class="comment"># 10</span></span><br><span class="line"><span class="comment"># heapq._heappushpop_max(heap, 10) # 注意, 没有 _heappushpop_max 函数</span></span><br></pre></td></tr></tbody></table></figure><h2 id="Python-刷题常用"><a href="#Python-刷题常用" class="headerlink" title="Python 刷题常用"></a>Python 刷题常用</h2><p>队列:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> Queue</span><br><span class="line">base_queue = Queue.Queue() <span class="comment"># 基本队列, 先进先出</span></span><br><span class="line">base_queue.put(x)</span><br><span class="line">base_queue.get()</span><br><span class="line">lifo_queue = Queue.LifoQueue() <span class="comment"># 先进后出, 类似栈</span></span><br><span class="line">lifo_queue.put(x)</span><br><span class="line">lifo_queue.get()</span><br><span class="line">prio_queue = Queue.PriorityQueue() <span class="comment"># 优先队列, 与C++中priority_queue类似, 可实现堆的功能</span></span><br><span class="line">prio_queue.put(x)</span><br><span class="line">prio_queue.get()</span><br></pre></td></tr></tbody></table></figure><h2 id="numpy-中vstack-hstack-concatenate-和-stack-之间的区别和联系"><a href="#numpy-中vstack-hstack-concatenate-和-stack-之间的区别和联系" class="headerlink" title="numpy 中vstack, hstack, concatenate 和 stack 之间的区别和联系"></a>numpy 中vstack, hstack, concatenate 和 stack 之间的区别和联系</h2><h3 id="concatenate"><a href="#concatenate" class="headerlink" title="concatenate"></a>concatenate</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">numpy.concatenate((a1, a2, ...), axis=0, out=None)</span><br></pre></td></tr></tbody></table></figure><p>concatenate 的作用就是将多个数组序列按照axis指定的维度连接起来, 这些数组序列 a1, a2, … 必须保证 除了 axis 指定维度之外的其他维度具有相同的 shape.<br>注意: 这里的维度指的是a1, a2的维度, 而不是(a1, a2)的维度<br>从维度角度来更好理解 concatenate 的作用<br>concatenate 执行后的 shape 特定是: axis 指定的维度是多个数组序列对应维度的数值和, 而其他维度保持不变. 也就是说不会增加新的维度, 这是 concatenate 与 stack 之间的一个重要的区别.<br>如下所示:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">a1 = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">3</span>]]) <span class="comment"># shape = 3x2</span></span><br><span class="line">a2 = np.array([[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>]]) <span class="comment"># shape = 2 x 2</span></span><br><span class="line"><span class="built_in">print</span>(a1.shape, a2.shape)</span><br><span class="line">concat1 = np.concatenate((a1, a2), axis=<span class="number">0</span>)</span><br><span class="line"><span class="built_in">print</span>(concat1.shape) <span class="comment"># shape 为 [5, 2], 在 0 维度上为 3+2, 其他维度保持不变</span></span><br><span class="line"><span class="built_in">print</span>(concat1) <span class="comment"># a1, a2 维度 0 不同, 一个为 3, 一个为 2, 其他维度相同, 均为 2</span></span><br><span class="line"><span class="comment">#[[1 1]</span></span><br><span class="line"><span class="comment"># [2 2]</span></span><br><span class="line"><span class="comment"># [3 3]</span></span><br><span class="line"><span class="comment"># [1 1]</span></span><br><span class="line"><span class="comment"># [2 2]]</span></span><br><span class="line"><span class="comment">#print(np.concatenate((a1, a2), axis=1)) # 由于维度 0 二者不同, 无法保持不变, 因此报错</span></span><br><span class="line">a1 = np.array([[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]) <span class="comment"># shape = 1x3</span></span><br><span class="line">a2 = np.array([[<span class="number">1</span>, <span class="number">2</span>]]) <span class="comment"># shape = 1x2</span></span><br><span class="line"><span class="built_in">print</span>(a1.shape, a2.shape)</span><br><span class="line">concat2 = np.concatenate((a1, a2), axis=<span class="number">1</span>)</span><br><span class="line"><span class="built_in">print</span>(concat2.shape) <span class="comment"># shape 为 [1, 5]在 1 维度上为 3 + 2, 0 维度上保持 1 不变</span></span><br><span class="line"><span class="built_in">print</span>(concat2)</span><br><span class="line"><span class="comment"># [[1 2 3 1 2]]</span></span><br><span class="line"><span class="comment"># print(np.concatenate((a1, a2), axis=0)) # 维度 1 不同, 报错</span></span><br></pre></td></tr></tbody></table></figure><p>有时候, concatenate的第一个参数只会传送一个一个数组序列, 这时候, 等价于将这个数组序列的第一维的元素看做是多个数组序列作为concatenate的参数进行传递. 如下所示:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">a = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>]]</span><br><span class="line"><span class="built_in">print</span>(np.concatenate(a, axis=<span class="number">0</span>)) <span class="comment"># 该行与下一行等价</span></span><br><span class="line"><span class="built_in">print</span>(np.concatenate((a[<span class="number">0</span>], a[<span class="number">1</span>]), axis=<span class="number">0</span>))</span><br><span class="line">a = [[<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>], [<span class="number">1</span>, <span class="number">2</span>]]</span><br><span class="line"><span class="built_in">print</span>(np.concatenate(a, axis=<span class="number">0</span>)) <span class="comment"># 可以看出, 虽然 a 的第一维度为 2, 第二维度为 3 和 2</span></span><br><span class="line"><span class="comment"># 但是, 我们要将其拆分, 拆分后, a[0], a[1] 的第一维度3和2, 其他维度相同, 因此可以在第一维度上进行连接</span></span><br><span class="line"><span class="built_in">print</span>(np.concatenate((a[<span class="number">0</span>], a[<span class="number">1</span>]), axis=<span class="number">0</span>))</span><br></pre></td></tr></tbody></table></figure><h3 id="stack"><a href="#stack" class="headerlink" title="stack"></a>stack</h3><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">numpy.stack(arrays, axis=0, out=None)</span><br><span class="line">numpy.stack((a1, a2, ...), axis=0, out=None)</span><br></pre></td></tr></tbody></table></figure><p>stack 的作用就是将多个数组序列按照axis指定的维度 堆叠 起来, 这些数组序列 a1, a2, … 必须保证 所有维度都相同, 注意这里与 concatenate 的区别.<br>要更好的理解stack, 可以借助 维度 的概念进行理解, 对于 shape 相同的 k 个数组序列来说, stack 的作用相当于新插入一个维度, 维度的大小为 k, 插入的位置为axis指定的位置. 如下所示:</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a1 = [[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">3</span>]] <span class="comment"># shape = 3x2</span></span><br><span class="line">a2 = [[<span class="number">4</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">6</span>]] <span class="comment"># shape = 3x2</span></span><br><span class="line">a3 = [[<span class="number">7</span>, <span class="number">7</span>], [<span class="number">8</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">9</span>]] <span class="comment"># shape = 3x2</span></span><br><span class="line">a4 = [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>]] <span class="comment"># shape = 3x2</span></span><br><span class="line">stack1 = np.stack((a1, a2, a3, a4), axis=<span class="number">0</span>) <span class="comment"># 新插入维度大小为 4, 位置为第 0 维</span></span><br><span class="line"><span class="built_in">print</span>(stack1.shape) <span class="comment"># shape 为 (4, 3, 2)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'###\n'</span>, stack1) <span class="comment"># 先将 shape 画好, 然后进行填充, 在第 0 维上进行堆叠, 因此 stack1[*][*] = a1[0], a1[1], ..., a4[2]</span></span><br><span class="line">stack2 = np.stack((a1, a2, a3, a4), axis=<span class="number">1</span>) <span class="comment"># 新插入维度大小为 4, 位置为第 1 维</span></span><br><span class="line"><span class="built_in">print</span>(stack2.shape) <span class="comment"># shape 为 (3, 4, 2)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'###\n'</span>, stack2) <span class="comment"># 在第 1 维上进行堆叠, 因此 stack2[*][*] = a1[0], a2[0], a3[0], a1[1], ...</span></span><br><span class="line">stack3 = np.stack((a1, a2, a3, a4), axis=<span class="number">2</span>) <span class="comment"># 新插入维度大小为 4, 位置为第 2 维</span></span><br><span class="line"><span class="built_in">print</span>(stack3.shape) <span class="comment"># shape 为 (3, 2, 4)</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">'###\n'</span>, stack3) <span class="comment"># 在第 2 维上进行堆叠, 因此 stack2[*][*] = [1 4 7 0], [1 4 7 0], [2 5 8 0], ...</span></span><br></pre></td></tr></tbody></table></figure><h3 id="hstack-和-vstack"><a href="#hstack-和-vstack" class="headerlink" title="hstack 和 vstack"></a>hstack 和 vstack</h3><p>hstack 和 vstack 虽然名字中都带有 stack, 但是实际上, 它们和np.stack的关系并不大, 一个明显的区别就是np.stack要求进行堆叠的多个数组序列需要保证 shape 完全相同, 并且堆叠后会新增加一个由axis指定的维度. 实际上, hstack 和 vstack 可以看做是特殊的 concatenate, 它们在某些情况下可以用 concatenate 来代替<br>既然 hstack 和 vstack 是特殊的 concatenate, 也就是说, 它们所接受的多个数组序列在axis指定的维度上可以不同, 而在其他维度上必须相同.<br>vstack: 在垂直方向上将多个数组序列进行堆叠, 相当于在axis=0维度上执行concatenate<br>hstack: 在水平方向上将多个数组序列进行堆叠, 相当于在axis=1维度上执行concatenate</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">a = [[<span class="number">1</span>, <span class="number">1</span>], [<span class="number">2</span>, <span class="number">2</span>], [<span class="number">3</span>, <span class="number">3</span>]] <span class="comment"># shape = 3x2</span></span><br><span class="line">b = [[<span class="number">4</span>, <span class="number">4</span>], [<span class="number">5</span>, <span class="number">5</span>], [<span class="number">6</span>, <span class="number">6</span>]] <span class="comment"># shape = 3x2</span></span><br><span class="line">c = [[<span class="number">7</span>, <span class="number">7</span>], [<span class="number">8</span>, <span class="number">8</span>], [<span class="number">9</span>, <span class="number">9</span>]] <span class="comment"># shape = 3x2</span></span><br><span class="line">d = [[<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>], [<span class="number">0</span>, <span class="number">0</span>]] <span class="comment"># shape = 3x2</span></span><br><span class="line">v = np.vstack((a, b, c, d))</span><br><span class="line"><span class="built_in">print</span>(v.shape) <span class="comment"># (12, 2)</span></span><br><span class="line"><span class="built_in">print</span>(v)</span><br><span class="line">x = np.concatenate((a, b, c, d), axis = <span class="number">0</span>) <span class="comment"># 等价于 vstack</span></span><br><span class="line"><span class="built_in">print</span>(x.shape) <span class="comment"># 12, 2</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br><span class="line">h = np.hstack((a, b, c, d))</span><br><span class="line"><span class="built_in">print</span>(h.shape) <span class="comment"># (3, 8)</span></span><br><span class="line"><span class="built_in">print</span>(h)</span><br><span class="line">x = np.concatenate((a, b, c, d), axis = <span class="number">1</span>) <span class="comment"># 等价于 hstack</span></span><br><span class="line"><span class="built_in">print</span>(x.shape) <span class="comment"># 3, 8</span></span><br><span class="line"><span class="built_in">print</span>(x)</span><br></pre></td></tr></tbody></table></figure><p>需要特别注意, 当多个数组序列是一维数组时, 应该先将一维数组转换成二维数组, 然后才能与相应的 concatenate 进行等价. 这是因为, 在数组序列是一维数组时, concatenate 是无法使用axis=1的, 因此此时的 hstack 相当于是在axis=0上进行 concatenate, 而 vstack 则需要先将数组的 shape 从 (N,) 转换成 (1, N) 后才相当于是在axis=1上进行 concatenate</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">a = np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>, <span class="number">4</span>, <span class="number">5</span>]) <span class="comment"># 当面对的是一维数组时,</span></span><br><span class="line">b = np.array([<span class="number">6</span>, <span class="number">7</span>, <span class="number">8</span>, <span class="number">9</span>, <span class="number">10</span>])</span><br><span class="line">h = np.hstack((a, b))</span><br><span class="line"><span class="built_in">print</span>(h.shape)</span><br><span class="line"><span class="built_in">print</span>(h)</span><br><span class="line">con = np.concatenate((a, b), axis=<span class="number">0</span>) <span class="comment"># 当 a, b 是一维数组时, hstack 相当于在 axis=0 上进行连接</span></span><br><span class="line"><span class="built_in">print</span>(con.shape)</span><br><span class="line"><span class="built_in">print</span>(con)</span><br><span class="line">v = np.vstack((a, b))</span><br><span class="line"><span class="built_in">print</span>(v.shape)</span><br><span class="line"><span class="built_in">print</span>(v)</span><br><span class="line">con = np.concatenate(([a], [b]), axis=<span class="number">0</span>) <span class="comment"># 当 a, b 是一维数组时, vstack 相当于将 a, b 先转换成二维 (1, N), 然后在 axis=0 上进行连接</span></span><br><span class="line"><span class="built_in">print</span>(con.shape)</span><br><span class="line"><span class="built_in">print</span>(con)</span><br></pre></td></tr></tbody></table></figure><h2 id="set-去重"><a href="#set-去重" class="headerlink" title="set 去重"></a>set 去重</h2><p>对于二维列表, 由于 list 的元素也是 list, 在内存中存储的是首元素地址, 无法直接使用 set, 因此需要先将内部的元素全部全换成 tuple 后, 才能使用 list 去重. 如下所示</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">a = <span class="built_in">list</span>()</span><br><span class="line">a.append([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">a.append([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">a.append([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>])</span><br><span class="line">a.append([<span class="number">4</span>, <span class="number">5</span>, <span class="number">6</span>])</span><br><span class="line"><span class="comment"># b = set(a) # 报错</span></span><br><span class="line">b = <span class="built_in">set</span>(<span class="built_in">map</span>(<span class="built_in">tuple</span>, a))</span><br><span class="line"><span class="built_in">print</span>(b) <span class="comment"># {(4, 5, 6), (1, 2, 3)}</span></span><br></pre></td></tr></tbody></table></figure><h2 id="os-sep用法"><a href="#os-sep用法" class="headerlink" title="os.sep用法"></a>os.sep用法</h2><p>ython是跨平台的。在Windows上，文件的路径分隔符是’\’，在Linux上是’/‘。<br>为了让代码在不同的平台上都能运行，那么路径应该写’\’还是’/‘呢？<br>使用os.sep的话，就不用考虑这个了，os.sep根据你所处的平台，自动采用相应的分隔符号。<br>举例<br>Linux下一个路径，/usr/share/python,那么上面的os.sep就是‘/’<br>windows下一个路径，C：\Users\Public\Desktop,那么上面的os.sep就是‘\’.</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">data_dir = os.sep.join(['hello', 'world'])</span><br></pre></td></tr></tbody></table></figure><h2 id="Python3-元组"><a href="#Python3-元组" class="headerlink" title="Python3 元组"></a>Python3 元组</h2><p>Python元组包含了以下内置函数</p><ul><li>len(tuple) 计算元组元素个数。<br>1<br>2<br>3<br>4<br>&gt;&gt;&gt; tuple1 = (‘Google’, ‘Runoob’, ‘Taobao’)<br>&gt;&gt;&gt; len(tuple1)<br>3<br>&gt;&gt;&gt;</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* max(tuple) 返回元组中元素最大值。</span><br><span class="line">      </span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><pre><code>    &gt;&gt;&gt; tuple2 = ('5', '4', '8')    &gt;&gt;&gt; max(tuple2)    '8'    &gt;&gt;&gt;</code></pre><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* min(tuple) 返回元组中元素最小值。</span><br><span class="line">      </span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><pre><code>    &gt;&gt;&gt; tuple2 = ('5', '4', '8')    &gt;&gt;&gt; min(tuple2)    '4'    &gt;&gt;&gt;</code></pre><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line">* tuple(seq) 将列表转换为元组。</span><br><span class="line">      </span><br><span class="line"></span><br></pre></td></tr></tbody></table></figure><pre><code>    &gt;&gt;&gt; list1= ['Google', 'Taobao', 'Runoob', 'Baidu']    &gt;&gt;&gt; tuple1=tuple(list1)    &gt;&gt;&gt; tuple1    ('Google', 'Taobao', 'Runoob', 'Baidu')</code></pre><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"></span><br><span class="line">## 序列化Python对象</span><br><span class="line"></span><br><span class="line">你需要将一个Python对象序列化为一个字节流，以便将它保存到一个文件、存储到数据库或者通过网络传输它。  </span><br><span class="line">对于序列化最普遍的做法就是使用 pickle 模块。为了将一个对象保存到一个文件中，可以这样做</span><br><span class="line"></span><br><span class="line">pickle 对于大型的数据结构比如使用 array 或 numpy 模块创建的二进制数组效率并不是一个高效的编码方式。 如果你需要移动大量的数组数据，你最好是先在一个文件中将其保存为数组数据块或使用更高级的标准编码方式如HDF5 (需要第三方库的支持)。  </span><br><span class="line"></span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"></span><br><span class="line">```python</span><br><span class="line">    In [1]: import pickle</span><br><span class="line">    In [2]: obj = 123,"abcdef", ["ac", 123], {"key": "value", "key1": "value1"}</span><br><span class="line">    In [3]: print(obj)</span><br><span class="line">    (123, 'abcdef', ['ac', 123], {'key': 'value', 'key1': 'value1'})</span><br><span class="line">    In [4]: # 序列化到文件</span><br><span class="line">    In [5]: with open(r'./a.pickle','wb') as f:</span><br><span class="line">       ...:     pickle.dump(obj,f)</span><br><span class="line">       ...:</span><br><span class="line">    In [6]: with open(r'./a.pickle','rb') as f:</span><br><span class="line">       ...:     aa= pickle.load(f)</span><br><span class="line">       ...: print(aa)</span><br><span class="line">       ...:</span><br><span class="line">       ...:</span><br><span class="line">    (123, 'abcdef', ['ac', 123], {'key': 'value', 'key1': 'value1'})</span><br></pre></td></tr></tbody></table></figure><p>参考链接：<br><a href="https://hellozhaozheng.github.io/">https://hellozhaozheng.github.io</a><br><a href="https://www.runoob.com/python/python-tutorial.html">https://www.runoob.com/python/python-tutorial.html</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>skimage模块</title>
      <link href="/2019/08/28/20190828-skimage%E6%A8%A1%E5%9D%97/"/>
      <url>/2019/08/28/20190828-skimage%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<p>比opencv的速度要慢很多, 但是使用起来更加简单, 真的对速度要求很高的话, 一般都会C++和opecv使用. 所以一般情况下, 首先看skimage能否实现, 不行的话再转用opencv  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> skimage</span><br><span class="line"><span class="keyword">from</span> skimage <span class="keyword">import</span> io <span class="comment">#  IO is a submodule. Submodules need to be imported from the parent module explicitly.</span></span><br><span class="line">img = io.imread(<span class="string">"1.jpg"</span>)</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从上往下打印二叉树</title>
      <link href="/2019/08/28/20190828-%E4%BB%8E%E4%B8%8A%E5%BE%80%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
      <url>/2019/08/28/20190828-%E4%BB%8E%E4%B8%8A%E5%BE%80%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：420679<br>本题知识点： 队列 树  </p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>从上往下打印出二叉树的每个节点，同层节点从左至右打印。</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line">/*</span><br><span class="line">struct TreeNode {</span><br><span class="line"><span class="built_in">int</span> val;</span><br><span class="line">struct TreeNode *left;</span><br><span class="line">struct TreeNode *right;</span><br><span class="line">TreeNode(<span class="built_in">int</span> x) :</span><br><span class="line">val(x), left(NULL), right(NULL) {</span><br><span class="line">}</span><br><span class="line">};*/</span><br><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> {</span><br><span class="line">public:</span><br><span class="line">    vector&lt;<span class="built_in">int</span>&gt; PrintFromTopToBottom(TreeNode* root) {</span><br><span class="line">        //队列是先进先出</span><br><span class="line">        queue&lt;TreeNode*&gt; que;</span><br><span class="line">vector&lt;<span class="built_in">int</span>&gt; vec;</span><br><span class="line">que.push(root);//先将整个二叉树放入队列</span><br><span class="line"><span class="keyword">while</span>(!que.empty()) //当队列非空进行循环</span><br><span class="line">{</span><br><span class="line">TreeNode* p;</span><br><span class="line">p = que.front();//先读取队列的首元素</span><br><span class="line">que.pop();//弹出队列的首元素</span><br><span class="line"><span class="keyword">if</span>(p == NULL)</span><br><span class="line"><span class="keyword">continue</span>;//所有元素存入vec后，由于队列中存放着空指针，依然进入循环，但此时p的值为NULL，不执行下面的操作，跳出循环结束</span><br><span class="line">que.push(p-&gt;left);</span><br><span class="line">que.push(p-&gt;right);</span><br><span class="line">vec.push_back(p-&gt;val);</span><br><span class="line">}</span><br><span class="line"><span class="keyword">return</span> vec;</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><p>运行时间：3ms<br>占用内存：464k</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>shutil模块</title>
      <link href="/2019/08/28/20190828-shutil%E6%A8%A1%E5%9D%97/"/>
      <url>/2019/08/28/20190828-shutil%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">shutil.copyfile("old","new") 　　　　  # 复制文件，都只能是文件</span><br><span class="line">shutil.copytree("old","new")　　　　 # 复制文件夹，都只能是目录，且new必须不存在</span><br><span class="line">shutil.copy("old","new")　　　　       # 复制文件/文件夹，复制 old 为 new（new是文件，若不存在，即新建），复制 old 为至 new 文件夹（文件夹已存在）</span><br><span class="line">shutil.move("old","new")  　　　　    # 移动文件/文件夹至 new 文件夹中</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>glob模块</title>
      <link href="/2019/08/28/20190828-glob%E6%A8%A1%E5%9D%97/"/>
      <url>/2019/08/28/20190828-glob%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<p>glob模块是Python最简单的模块之一, 内容非常少, 用它可以查找符合特定规则的文件路径名, 查找文件时只会用到三个匹配符:  </p><ol><li><ul><li>: 匹配0个或多个字符</li></ul></li><li>? : 匹配单个字符</li><li>[] : 匹配指定范围内的字符, 如[0-9]匹配数字</li></ol><h2 id="glob-glob"><a href="#glob-glob" class="headerlink" title="glob.glob()"></a>glob.glob()</h2><p>参数:<br>_(str): 文件路径的正则表达式</p><p>返回值:<br>_(list): 符合正则表达式的文件路径列表</p><p>备注:<br>返回所有匹配的文件路径列表, 它只有一个参数pathname, 定义了文件路径匹配的规则, 这里可以是绝对路径或者相对路径:  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> glob</span><br><span class="line">pathes_list = glob.glob(<span class="string">"~/Pictures/*.jpg"</span>)</span><br><span class="line"><span class="comment"># 获取Pictures下的所有图片</span></span><br><span class="line">relative_pathes_list = glob.glob(<span class="string">"../*.py"</span>)</span><br><span class="line"><span class="comment"># 获取上级目录中的所有.py文件</span></span><br></pre></td></tr></tbody></table></figure><p>在 linux, osx 系统中, 通配符的匹配是大小写区分的, 也就是需要特别指定大小写:  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">extensions = ['jpg', 'JPG', 'jpeg', 'JPEG']</span><br></pre></td></tr></tbody></table></figure><p>但是在 windows 当中, 通配符的匹配是不区分大小写的, 因此只需要指定大小写中的一个即可, 两个都指定的话, 会出现重复的情况  </p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">extensions = ['jpg', 'jpeg']</span><br></pre></td></tr></tbody></table></figure><h2 id="glob-iglob"><a href="#glob-iglob" class="headerlink" title="glob.iglob"></a>glob.iglob</h2><p>获取一个可遍历的对象, 使用它可以逐个获取匹配的文件路径名. 与glob.glob()的区别是: glob.glob()会同时获取到所有的匹配路径, 而glob.iglob()一次只获取一个匹配路径.  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">f = glob.iglob(<span class="string">"../*.py"</span>)</span><br><span class="line"><span class="built_in">print</span> f <span class="comment"># &lt;generator object iglob at 0x00B9FF80&gt;</span></span><br><span class="line"><span class="keyword">for</span> py <span class="keyword">in</span> f:</span><br><span class="line">    <span class="built_in">print</span>(py)</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>广度/宽度优先搜索(BFS)</title>
      <link href="/2019/08/28/BFS/"/>
      <url>/2019/08/28/BFS/</url>
      
        <content type="html"><![CDATA[<h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>广度优先搜索 是最简单的图搜索算法之一， 也是许多重要的图算法的原型。Prime的最小生成树算法和Dijkstra的单源最短路径算法都使用了类似广度优先搜索的思想。<br>给定图G=(V,E) 和一个可以识别的<strong>源</strong> 节点 s，广度优先搜索对图G中的边进行系统性的探索来发现可以从源节点，到达所有的节点。该算法能够计算从源结点s到每个可到达的节点的距离(最小的边数)，同时生成一棵“广度优先搜索树”。该树以源结点s为根节点，包括所有可以从s到达的结点。对于每个从源结点s可以到达的结点v，在广度优先搜索树里从结点s到结点v的简单路径所对应的的就是图G中从结点s到结点v的“最短路径”，即包含最少边数的路径，该算法既可以用于有向图也可以用于无向图。<br>广度优先算法之所以如此得名是因为该算法始终是将已经发现的结点和未发现结点之间的边界，沿其广度方向向外扩展。也就是说，算法需要在发现所有距离源结点s为k的所有结点之后，才会发现距离源结点s为k+1的 其他结点。</p><h2 id="图的概念"><a href="#图的概念" class="headerlink" title="图的概念"></a>图的概念</h2><ul><li>图(graph) 是一种$\textcolor{Blue}{网状数据} $结构， 图是由非空的顶点集合和一个描述顶点之间的关系的集合组成。</li><li>图由顶点和边组成，顶点表示对象，边表示对象之间的连接关系。</li><li>边也可以带权值，称为带权值图。</li></ul><h3 id="无向图术语"><a href="#无向图术语" class="headerlink" title="无向图术语"></a>无向图术语</h3><ul><li>两个顶点之间如果有边连接，视为两个顶点相邻</li><li>相邻顶点间的序列称为路径</li><li>起点和终点重合的路径称为圈</li><li>顶点连接的边数叫做这个顶点的度</li></ul><hr><ul><li><p>没有圈的连通图，就是树</p></li><li><p>没有圈的非连通图，就是森林</p></li><li><p>一棵树的边数等于顶点数-1</p></li><li><p>边数等于顶点数-1 的连通图，就是树</p><p>1<br>2<br>3<br>4<br>5<br>6<br>7<br>8<br>9<br>10<br>11<br>12<br>13<br>14<br>15<br>16<br>17<br>18<br>19</p></li></ul><p>| </p><pre><code>BFS(G,s) \\  for each vertex u \in G.V -{s}\\      u.color = WHITE\\      u.d = \infty\\      u.\pi = NIL\\  s.color = GRAY\\  s.d = 0\\  s.\pi = NIL\\  Q = \emptyset\\  ENQUEUE(Q,s)\\  while Q\neq = \emptyset\\      u = DEQUEUE(Q)\\      for each v \in G.Adj[u]\\          if v.color == WHITE\\              v.color = GRAY\\              v.d = u.d+1\\              v.\pi = u\\              ENQUEUE(Q,v)\\      u.color = BLACK\\    </code></pre><p>—|—  </p><p>广度优先搜索的流程图<br><img src="/2019/08/28/BFS/images/20190828_BFS_bfs.png"></p><h2 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h2><p>POJ3984《迷宫问题》<br>定义一个二维数组：<br>int maze[5][5] = {<br>0, 1, 0, 0, 0,<br>0, 1, 0, 1, 0,<br>0, 0, 0, 0, 0,<br>0, 1, 1, 1, 0,<br>0, 0, 0, 1, 0,<br>};<br>它表示一个迷宫，其中的1表示墙壁，0表示可以走的路，只能横着走或竖着走，不能斜着走，要求编程序找出从左上角到右下角的最短路线。 </p><h3 id="解题思路"><a href="#解题思路" class="headerlink" title="解题思路"></a>解题思路</h3><p>队列是先进后出，后进先出。</p><hr><p>对应于题目的输入数组：<br>0, 1, 0, 0, 0,<br>0, 1, 0, 1, 0,<br>0, 0, 0, 0, 0,<br>0, 1, 1, 1, 0,<br>0, 0, 0, 1, 0,<br>把节点定义为(x,y)，(x,y)表示数组maze的项maze[x][y]。<br>于是起点就是(0,0)，终点是(4,4)。按照刚刚的思路，手工梳理一遍：<br>初始条件：<br>起点Vs为(0,0)<br>终点Vd为(4,4)<br>灰色节点集合Q={}<br>初始化所有节点为白色节点<br>下面开始广度优先搜索：<br>1.起始节点Vs变成灰色，加入队列Q，Q={(0,0)}<br>2.取出队列Q的头一个节点Vn，Vn={0,0}，Q={}<br>3.把Vn={0,0}染成黑色，取出Vn所有相邻的白色节点{(1,0)}<br>4.不包含终点(4,4)，染成灰色，加入队列Q，Q={(1,0)}<br>5.取出队列Q的头一个节点Vn，Vn={1,0}，Q={}<br>6.把Vn={1,0}染成黑色，取出Vn所有相邻的白色节点{(2,0)}<br>7.不包含终点(4,4)，染成灰色，加入队列Q，Q={(2,0)}<br>8.取出队列Q的头一个节点Vn，Vn={2,0}，Q={}<br>9.把Vn={2,0}染成黑色，取出Vn所有相邻的白色节点{(2,1), (3,0)}<br>10.不包含终点(4,4)，染成灰色，加入队列Q，Q={(2,1), (3,0)}<br>11.取出队列Q的头一个节点Vn，Vn={2,1}，Q={(3,0)}<br>12.把Vn={2,1}染成黑色，取出Vn所有相邻的白色节点{(2,2)}<br>13.不包含终点(4,4)，染成灰色，加入队列Q，Q={(3,0), (2,2)}<br>14.持续下去，知道Vn的所有相邻的白色节点中包含了(4,4)……<br>15.此时获得了答案<br><img src="/2019/08/28/BFS/images/20190828_BFS_mgbfs.png"></p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  </code></pre><p>| </p><pre><code>// BFS.cpp : 定义控制台应用程序的入口点。  //    #include "stdafx.h"    #include &lt;iostream&gt;    using namespace std;    int map[5][5];    //相邻四个节点  int borderUponX[4] = { 0, 0, 1, -1 };  int borderUponY[4] = { 1, -1, 0, 0 };    int front = 0, rear = 1;    struct node {      int pre;      int x;      int y;  } path[100];    void print(int i) {//当前节点      if (path[i].pre != -1) {//找到前面那个节点          print(path[i].pre);          cout &lt;&lt; "(" &lt;&lt; path[i].x &lt;&lt; "," &lt;&lt; path[i].y &lt;&lt; ")" &lt;&lt; endl;      }      else {//最前面的那个节点          cout &lt;&lt; "(" &lt;&lt; path[i].x &lt;&lt; "," &lt;&lt; path[i].y &lt;&lt; ")" &lt;&lt; endl;      }  }    void bfsSearch(int x, int y) {      //开始节点（出发），前面没有节点了      path[front].x = x;      path[front].y = y;      path[front].pre = -1;        //当front == rear的时候说明已经走完了所以“相邻”节点      //且都不通      while (front &lt; rear) {          for (int i = 0; i != 4; i++) {              //相邻节点坐标              int pathX = path[front].x + borderUponX[i];              int pathY = path[front].y + borderUponY[i];                //不符合的节点（遇到边界或已经走过了）              if (pathY &lt; 0 || pathX &lt; 0 || pathX &gt; 4 || pathY &gt; 4 || map[pathX][pathY])                  continue;              else {//将front的相邻的可以过去的并且是还没有走过的节点加到路径里面                  map[pathX][pathY] = 1;                  path[rear].x = pathX;                  path[rear].y = pathY;                  path[rear].pre = front;                  rear++;              }              if (pathX == 4 &amp;&amp; pathY == 4) {                  //找到了一条路径，又是第一次找到                  //那么就是最短路径了                  print(rear - 1);                  break;              }          }          front++;      }  }    int main(int argc, char const *argv[])  {      for (int i = 0; i &lt; 5; i++)          for (int j = 0; j &lt; 5; j++)              cin &gt;&gt; map[i][j];        bfsSearch(0, 0);      system("pause");      return 0;  }    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> BFS </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>PIL模块</title>
      <link href="/2019/08/28/PIL%E6%A8%A1%E5%9D%97/"/>
      <url>/2019/08/28/PIL%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<h2 id="导入"><a href="#导入" class="headerlink" title="导入"></a>导入</h2><pre><code>1  </code></pre><p>| </p><pre><code>from PIL import Image    </code></pre><p>—|—  </p><h2 id="读取"><a href="#读取" class="headerlink" title="读取"></a>读取</h2><pre><code>1  </code></pre><p>| </p><pre><code>img = Image.open(filepath)    </code></pre><p>—|—  </p><h2 id="显示"><a href="#显示" class="headerlink" title="显示"></a>显示</h2><pre><code>1  </code></pre><p>| </p><pre><code>img.show()    </code></pre><p>—|—  </p><h2 id="与-numpy-数组的互相转换"><a href="#与-numpy-数组的互相转换" class="headerlink" title="与 numpy 数组的互相转换"></a>与 numpy 数组的互相转换</h2><p>PIL Image 转 numpy 数组  </p><pre><code>1  </code></pre><p>| </p><pre><code>img_to_array = np.array(img)    </code></pre><p>—|—  </p><p>numpy 数组转 PIL Image (注意要确保数组内的值符合 PIL 的要求)  </p><pre><code>1  </code></pre><p>| </p><pre><code>array_to_img = Image.fromarray(img_to_array)    </code></pre><p>—|—  </p><h2 id="PIL-与-cv2-格式互相转换"><a href="#PIL-与-cv2-格式互相转换" class="headerlink" title="PIL 与 cv2 格式互相转换"></a>PIL 与 cv2 格式互相转换</h2><p>PIL.Image读入的图片数据类型不是 numpy 数组, 它的size属性为 (w, h), 利用np.array转换成 numpy 数组后, 它的通道顺序为 (r, g, b)  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  </code></pre><p>| </p><pre><code>from PIL import Image  import numpy as np    # PIL to cv2  pil_img = Image.open(img_path)  print(pil_img.size) # (w, h)  np_img = np.array(pil_img)  cv2_img = np_img[:, :, ::-1] # 交换通道    # cv2 to PIL  pil_img = Image.fromarray(cv2_img[:, :, ::-1])    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch trick</title>
      <link href="/2019/08/28/Pytorch-trick/"/>
      <url>/2019/08/28/Pytorch-trick/</url>
      
        <content type="html"><![CDATA[<p>目录：   </p><ol><li>指定GPU编号</li><li>查看模型每层输出详情</li><li>梯度裁剪</li><li>扩展单张图片维度</li><li>独热编码</li><li>防止验证模型时爆显存</li><li>学习率衰减</li><li>冻结某些层的参数</li><li>对不同层使用不同学习率</li></ol><h2 id="1-指定GPU编号"><a href="#1-指定GPU编号" class="headerlink" title="1. 指定GPU编号"></a>1. 指定GPU编号</h2><ul><li>设置当前使用的GPU设备仅为0号设备，设备名称为 /gpu:0：os.environ[“CUDA_VISIBLE_DEVICES”] = “0”</li><li>设置当前使用的GPU设备为0,1号两个设备，名称依次为 /gpu:0、/gpu:1： os.environ[“CUDA_VISIBLE_DEVICES”] = “0,1” ，根据顺序表示优先使用0号设备,然后使用1号设备。<br>指定GPU的命令需要放在和神经网络相关的一系列操作的前面。</li></ul><h2 id="2-查看模型每层输出详情"><a href="#2-查看模型每层输出详情" class="headerlink" title="2.查看模型每层输出详情"></a>2.查看模型每层输出详情</h2><p>Keras有一个简洁的API来查看模型的每一层输出尺寸，这在调试网络时非常有用。现在在PyTorch中也可以实现这个功能。</p><p>使用很简单，如下用法：  </p><pre><code>1  2  </code></pre><p>| </p><pre><code>from torchsummary import summary  summary(your_model, input_size=(channels, H, W))    </code></pre><p>—|—  </p><p>input_size 是根据你自己的网络模型的输入尺寸进行设置。</p><h2 id="3-梯度裁剪（Gradient-Clipping）"><a href="#3-梯度裁剪（Gradient-Clipping）" class="headerlink" title="3.梯度裁剪（Gradient Clipping）"></a>3.梯度裁剪（Gradient Clipping）</h2><pre><code>1  2  3  4  5  6  7  8  </code></pre><p>| </p><pre><code>import torch.nn as nn    outputs = model(data)  loss= loss_fn(outputs, target)  optimizer.zero_grad()  loss.backward()  nn.utils.clip_grad_norm_(model.parameters(), max_norm=20, norm_type=2)  optimizer.step()    </code></pre><p>—|—  </p><p>nn.utils.clip_grad_norm_ 的参数：</p><ul><li>parameters – 一个基于变量的迭代器，会进行梯度归一化</li><li>max_norm – 梯度的最大范数</li><li>max_norm – 梯度的最大范数<br>知乎用户 不椭的椭圆 提出：梯度裁剪在某些任务上会额外消耗大量的计算时间，可移步评论区查看详情。</li></ul><h2 id="4、扩展单张图片维度"><a href="#4、扩展单张图片维度" class="headerlink" title="4、扩展单张图片维度"></a>4、扩展单张图片维度</h2><p>因为在训练时的数据维度一般都是 (batch_size, c, h, w)，而在测试时只输入一张图片，所以需要扩展维度，扩展维度有多个方法：  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  </code></pre><p>| </p><pre><code>import cv2  import torch    image = cv2.imread(img_path)  image = torch.tensor(image)  print(image.size())    img = image.view(1, *image.size())  print(img.size())    # output:  # torch.Size([h, w, c])  # torch.Size([1, h, w, c])    </code></pre><p>—|—  </p><p>或者  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  </code></pre><p>| </p><pre><code>import cv2  import numpy as np    image = cv2.imread(img_path)  print(image.shape)  img = image[np.newaxis, :, :, :]  print(img.shape)    # output:  # (h, w, c)  # (1, h, w, c)    </code></pre><p>—|—  </p><p>或者  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  </code></pre><p>| </p><pre><code>import cv2  import torch    image = cv2.imread(img_path)  image = torch.tensor(image)  print(image.size())    img = image.unsqueeze(dim=0)    print(img.size())    img = img.squeeze(dim=0)  print(img.size())    # output:  # torch.Size([(h, w, c)])  # torch.Size([1, h, w, c])  # torch.Size([h, w, c])    </code></pre><p>—|—  </p><p>tensor.unsqueeze(dim)：扩展维度，dim指定扩展哪个维度。</p><p>tensor.squeeze(dim)：去除dim指定的且size为1的维度，维度大于1时，squeeze()不起作用，不指定dim时，去除所有size为1的维度。</p><h2 id="5-独热编码"><a href="#5-独热编码" class="headerlink" title="5.独热编码"></a>5.独热编码</h2><p>在PyTorch中使用交叉熵损失函数的时候会自动把label转化成onehot，所以不用手动转化，而使用MSE需要手动转化成onehot编码。  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  </code></pre><p>| </p><pre><code>import torch  class_num = 8  batch_size = 4    def one_hot(label):      """      将一维列表转换为独热编码      """      label = label.resize_(batch_size, 1)      m_zeros = torch.zeros(batch_size, class_num)      # 从 value 中取值，然后根据 dim 和 index 给相应位置赋值      onehot = m_zeros.scatter_(1, label, 1)  # (dim,index,value)        return onehot.numpy()  # Tensor -&gt; Numpy    label = torch.LongTensor(batch_size).random_() % class_num  # 对随机数取余  print(one_hot(label))    # output:  [[0. 0. 0. 1. 0. 0. 0. 0.]   [0. 0. 0. 0. 1. 0. 0. 0.]   [0. 0. 1. 0. 0. 0. 0. 0.]   [0. 1. 0. 0. 0. 0. 0. 0.]]    </code></pre><p>—|—  </p><h2 id="6-防止验证模型时爆显存"><a href="#6-防止验证模型时爆显存" class="headerlink" title="6. 防止验证模型时爆显存"></a>6. 防止验证模型时爆显存</h2><p>验证模型时不需要求导，即不需要梯度计算，关闭autograd，可以提高速度，节约内存。如果不关闭可能会爆显存。  </p><pre><code>1  2  3  </code></pre><p>| </p><pre><code>with torch.no_grad():      # 使用model进行预测的代码      pass    </code></pre><p>—|—  </p><p>感谢知乎用户zhaz 的提醒，我把 torch.cuda.empty_cache() 的使用原因更新一下。</p><p>这是原回答：</p><p>Pytorch 训练时无用的临时变量可能会越来越多，导致 out of memory ，可以使用下面语句来清理这些不需要的变量。</p><p>官网 上的解释为：</p><p>Releases all unoccupied cached memory currently held by the caching allocator so that those can be used in other GPU application and visible innvidia-smi.torch.cuda.empty_cache()</p><p>意思就是PyTorch的缓存分配器会事先分配一些固定的显存，即使实际上tensors并没有使用完这些显存，这些显存也不能被其他应用使用。这个分配过程由第一次CUDA内存访问触发的。</p><p>而 torch.cuda.empty_cache() 的作用就是释放缓存分配器当前持有的且未占用的缓存显存，以便这些显存可以被其他GPU应用程序中使用，并且通过 nvidia-smi命令可见。注意使用此命令不会释放tensors占用的显存。</p><p>对于不用的数据变量，Pytorch 可以自动进行回收从而释放相应的显存。</p><h2 id="7-学习率衰减"><a href="#7-学习率衰减" class="headerlink" title="7. 学习率衰减"></a>7. 学习率衰减</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  </code></pre><p>| </p><pre><code>import torch.optim as optim  from torch.optim import lr_scheduler    # 训练前的初始化  optimizer = optim.Adam(net.parameters(), lr=0.001)  scheduler = lr_scheduler.StepLR(optimizer, 10, 0.1)  # # 每过10个epoch，学习率乘以0.1    # 训练过程中  for n in n_epoch:      scheduler.step()      ...    </code></pre><p>—|—  </p><h2 id="8-冻结某些层的参数"><a href="#8-冻结某些层的参数" class="headerlink" title="8. 冻结某些层的参数"></a>8. 冻结某些层的参数</h2><p>在加载预训练模型的时候，我们有时想冻结前面几层，使其参数在训练过程中不发生变化。</p><p>我们需要先知道每一层的名字，通过如下代码打印：  </p><pre><code>1  2  3  </code></pre><p>| </p><pre><code>net = Network()  # 获取自定义网络结构  for name, value in net.named_parameters():      print('name: {0}, grad: {1}'.format(name, value.requires_grad))    </code></pre><p>—|—  </p><p>假设前几层信息如下：  </p><pre><code>1  2  3  4  5  6  7  8  </code></pre><p>| </p><pre><code>name: cnn.VGG_16.convolution1_1.weight, grad: True  name: cnn.VGG_16.convolution1_1.bias, grad: True  name: cnn.VGG_16.convolution1_2.weight, grad: True  name: cnn.VGG_16.convolution1_2.bias, grad: True  name: cnn.VGG_16.convolution2_1.weight, grad: True  name: cnn.VGG_16.convolution2_1.bias, grad: True  name: cnn.VGG_16.convolution2_2.weight, grad: True  name: cnn.VGG_16.convolution2_2.bias, grad: True    </code></pre><p>—|—  </p><p>后面的True表示该层的参数可训练，然后我们定义一个要冻结的层的列表：  </p><pre><code>1  2  3  4  5  6  </code></pre><p>| </p><pre><code>no_grad = [      'cnn.VGG_16.convolution1_1.weight',      'cnn.VGG_16.convolution1_1.bias',      'cnn.VGG_16.convolution1_2.weight',      'cnn.VGG_16.convolution1_2.bias'  ]    </code></pre><p>—|—  </p><p>冻结方法如下：  </p><pre><code>1  2  3  4  5  6  </code></pre><p>| </p><pre><code>net = Net.CTPN()  # 获取网络结构  for name, value in net.named_parameters():      if name in no_grad:          value.requires_grad = False      else:          value.requires_grad = True    </code></pre><p>—|—  </p><p>冻结后我们再打印每层的信息：  </p><pre><code>1  2  3  4  5  6  7  8  </code></pre><p>| </p><pre><code>name: cnn.VGG_16.convolution1_1.weight, grad: False  name: cnn.VGG_16.convolution1_1.bias, grad: False  name: cnn.VGG_16.convolution1_2.weight, grad: False  name: cnn.VGG_16.convolution1_2.bias, grad: False  name: cnn.VGG_16.convolution2_1.weight, grad: True  name: cnn.VGG_16.convolution2_1.bias, grad: True  name: cnn.VGG_16.convolution2_2.weight, grad: True  name: cnn.VGG_16.convolution2_2.bias, grad: True    </code></pre><p>—|—  </p><p>可以看到前两层的weight和bias的requires_grad都为False，表示它们不可训练。<br>最后在定义优化器时，只对requires_grad为True的层的参数进行更新。  </p><pre><code>1  </code></pre><p>| </p><pre><code>optimizer = optim.Adam(filter(lambda p: p.requires_grad, net.parameters()), lr=0.01)    </code></pre><p>—|—  </p><h2 id="9-对不同层使用不同学习率"><a href="#9-对不同层使用不同学习率" class="headerlink" title="9. 对不同层使用不同学习率"></a>9. 对不同层使用不同学习率</h2><p>我们对模型的不同层使用不同的学习率。</p><p>还是使用这个模型作为例子：  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  </code></pre><p>| </p><pre><code>net = Network()  # 获取自定义网络结构  for name, value in net.named_parameters():      print('name: {}'.format(name))    # 输出：  # name: cnn.VGG_16.convolution1_1.weight  # name: cnn.VGG_16.convolution1_1.bias  # name: cnn.VGG_16.convolution1_2.weight  # name: cnn.VGG_16.convolution1_2.bias  # name: cnn.VGG_16.convolution2_1.weight  # name: cnn.VGG_16.convolution2_1.bias  # name: cnn.VGG_16.convolution2_2.weight  # name: cnn.VGG_16.convolution2_2.bias    </code></pre><p>—|—  </p><p>对 convolution1 和 convolution2 设置不同的学习率，首先将它们分开，即放到不同的列表里：  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  </code></pre><p>| </p><pre><code>conv1_params = []  conv2_params = []    for name, parms in net.named_parameters():      if "convolution1" in name:          conv1_params += [parms]      else:          conv2_params += [parms]    # 然后在优化器中进行如下操作：  optimizer = optim.Adam(      [          {"params": conv1_params, 'lr': 0.01},          {"params": conv2_params, 'lr': 0.001},      ],      weight_decay=1e-3,  )    </code></pre><p>—|—  </p><p>我们将模型划分为两部分，存放到一个列表里，每部分就对应上面的一个字典，在字典里设置不同的学习率。当这两部分有相同的其他参数时，就将该参数放到列表外面作为全局参数，如上面的<code>weight_decay</code>。</p><p>也可以在列表外设置一个全局学习率，当各部分字典里设置了局部学习率时，就使用该学习率，否则就使用列表外的全局学习率。</p><h2 id="显示训练时间"><a href="#显示训练时间" class="headerlink" title="显示训练时间"></a>显示训练时间</h2><pre><code>1  2  3  4  5  6  </code></pre><p>| </p><pre><code>for epoch in range(start_epoch, config.epochs):              start = time.time()              train_loss, lr = train_epoch(model, optimizer, scheduler, train_loader, device, criterion, epoch, all_step,                                           writer, logger)              logger.info('[{}/{}], train_loss: {:.4f}, time: {:.4f}, lr: {}'.format(                  epoch, config.epochs, train_loss, time.time() - start, lr))    </code></pre><p>—|—  </p><p>参考：<a href="https://mp.weixin.qq.com/s?__biz=MzU3NjE4NjQ4MA==&amp;mid=2247485953&amp;idx=2&amp;sn=3ae788b7d643541254ba311f7a7faced&amp;chksm=fd16fb1eca61720870bc58c1a465a346cf2c6a7e8bea39e4b3d582474b595021f3a5b635086d&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1566885137387&amp;sharer_shareid=285785c5623899db73795495779fe8be#rd">https://mp.weixin.qq.com/s?__biz=MzU3NjE4NjQ4MA==&amp;mid=2247485953&amp;idx=2&amp;sn=3ae788b7d643541254ba311f7a7faced&amp;chksm=fd16fb1eca61720870bc58c1a465a346cf2c6a7e8bea39e4b3d582474b595021f3a5b635086d&amp;mpshare=1&amp;scene=1&amp;srcid=&amp;sharer_sharetime=1566885137387&amp;sharer_shareid=285785c5623899db73795495779fe8be#rd</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Pytorch手册</title>
      <link href="/2019/08/28/Pytorch%E6%89%8B%E5%86%8C/"/>
      <url>/2019/08/28/Pytorch%E6%89%8B%E5%86%8C/</url>
      
        <content type="html"><![CDATA[<p>PyTorch 主要提供以下两大特色：</p><ul><li>支持强力GPU加速的Tensor计算能力</li><li>基于tape的具有自动微分求导能力的深度神经网络框架</li></ul><p>PyTorch 主要包含以下组成要素:</p><table><thead><tr><th>组成要素</th><th>描述说明</th></tr></thead><tbody><tr><td>torch</td><td>一个类似于numpy的tensor哭, 提供强力的GPU支持</td></tr><tr><td>torch.autograd</td><td>一个基于tape的具有自动微分求导能力的库, 可以支持几乎所有的tesnor operatioin</td></tr><tr><td>torch.nn</td><td>一个神经网络库, 与autograd深度整合, 可以提供最大限度的灵活性</td></tr><tr><td>torch.multiprocessing</td><td>Python的多线程处理, 可以提供torch Tensors之间的内存共享, 对于加载数据和Hogwild training来说十分有用</td></tr><tr><td>torch.utils</td><td>一些功能类和函数, 如DataLoader, Trainer等等</td></tr><tr><td>torch.legacy(.nn/.optim)</td><td>为了兼容性而存在的一些代码和实现</td></tr></tbody></table><p>Pytorch通常可以作为以下用途使用:</p><ul><li>为了使用GPUs性能的numpy替代品</li><li>可以提供强大灵活力和速度优势的深度学习平台.</li></ul><h2 id="torch"><a href="#torch" class="headerlink" title="torch"></a>torch</h2><h2 id="backends-cudnn"><a href="#backends-cudnn" class="headerlink" title="backends.cudnn"></a>backends.cudnn</h2><pre><code>    1      </code></pre><p>| </p><pre><code>    torch.backends.cudnn.benchmark = True        </code></pre><p>—|—  </p><p>上述设置可以让内置的cudnn的auto-tuner自动寻找最合适当前配置的搞笑算法, 来达到优化运行效率的目标, 在使用时, 应该遵循以下两个准则:</p><ol><li>如果网络的输入数据维度或类型上变化不大, 则该设置可以增加运行效率</li><li>如果网络的输入数据在每次的iteration中都变化的话, 会导致cudnn每次都寻找一遍最优配置, 这样反而 会降低 运行效率.</li></ol><h3 id="torch-cat"><a href="#torch-cat" class="headerlink" title="torch.cat()"></a>torch.cat()</h3><pre><code>     1       </code></pre><p>| </p><pre><code>     torch.cat(seq, dim=0, out=None) # 返回连接后的tensor         </code></pre><p>—|—  </p><p>将给定的 tensor 序列 seq 按照维度连接起来. 默认维度为0, 说明会将其在第 0 个维度上进行拼接.(最后的结果是第 0 维度增大, 例如三个2行3列的 tensor 按照第0维度拼接, 最后得到的 tensor 维度为6行3列)</p><h3 id="clamp-x2F-clamp"><a href="#clamp-x2F-clamp" class="headerlink" title="clamp()/clamp_()"></a>clamp()/clamp_()</h3><pre><code>1  </code></pre><p>| </p><pre><code>torch.clamp(input, min, max, out=None) -&gt; Tensor    </code></pre><p>—|—  </p><p>将input里面元素全部划分到[min,max]区间内, 小于min的置为min, 大于max的置为max. 如果不指定min或者max,则认为无下界或上界<br>其他调用形式:  </p><pre><code>1  </code></pre><p>| </p><pre><code>torch.Tensor(min, max) # 调用tensor为input, 返回值为out    </code></pre><p>—|—  </p><h3 id="device"><a href="#device" class="headerlink" title="device()"></a>device()</h3><pre><code>1  </code></pre><p>| </p><pre><code>device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")    </code></pre><p>—|—  </p><h3 id="gather"><a href="#gather" class="headerlink" title="gather()"></a>gather()</h3><pre><code>1  </code></pre><p>| </p><pre><code>torch.gather(input, dim, index, out=None) -&gt; Tensor    </code></pre><p>—|—  </p><p>沿着dim指定的轴按着index指定的值重新组合成一个新的tensor.  </p><pre><code>1  2  3  </code></pre><p>| </p><pre><code>out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0  out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1  out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2    </code></pre><p>—|—  </p><p>即假设input是一个 n 维的tensor, 其 size 为 (x0,x1,…,xi−1,xi,xi+1,…,xn−1), 若dim=i, 则 index 必须也是一个 n 维的tensor, 其 size 为 (x0,x1,…,xi−1,y,xi+1,…,xn−1), 其中 y≥1, 而返回的 tensor out 的 size 和 index 的 size 相同.<br>一句来说 gather 的作用就是, 在指定的维度上筛选给给定下标index指示的值, 其他值舍弃.<br><strong>一个例子说明:</strong><br>scores是一个计算出来的分数，类型为[torch.FloatTensor of size 5x1000]<br>而y_var是正确分数的索引，类型为[torch.LongTensor of size 5]<br>容易知道，这里有1000个类别，有5个输入图像，每个图像得出的分数中只有一个是正确的，正确的索引就在y_var中，这里要做的是将正确分数根据索引标号提取出来。  </p><pre><code>1  2  </code></pre><p>| </p><pre><code>scores = model(X_var)  # 分数  scores = scores.gather(1, y_var.view(-1, 1)).squeeze()  #进行提取    </code></pre><p>—|—  </p><p>提取后的scores格式也为[torch.FloatTensor of size 5]<br>这里讲一下变化过程：</p><ol><li><p>首先要知道之前的scores的size为[5,1000]，而y_var的size为[5]，scores为2维，y_var为1维不匹配，所以先用view将其展开为[5,1]的size，这样维数n就与scroes匹配了。</p></li><li><p>接下来进行gather，gather函数中第一个参数为1，意思是在第二维进行汇聚，也就是说通过y_var中的五个值来在scroes中第二维的5个1000中进行一一挑选，挑选出来后的size也为[5,1]，然后再通过squeeze将那个一维去掉，最后结果为[5].<br><strong>Tensor形式</strong></p><pre><code>1</code></pre></li></ol><p>| </p><pre><code>     torch.Tensor.gather(dim, index) -&gt; Tensor         </code></pre><p>—|—  </p><h3 id="torch-ge"><a href="#torch-ge" class="headerlink" title="torch.ge()"></a>torch.ge()</h3><h3 id="torch-gt"><a href="#torch-gt" class="headerlink" title="torch.gt()"></a>torch.gt()</h3><pre><code>1  </code></pre><p>| </p><pre><code>torch.gt(input, other, out=None) # -&gt; Tensor    </code></pre><p>—|—  </p><p>根据 input 和 other 的值返回一个二值 tensor, 如果满足大于条件则为1, 不满足则为0.<br>other 可以是能够转换成 input size 的tensor, 也可以是一个 float 标量.</p><h3 id="torch-index-select"><a href="#torch-index-select" class="headerlink" title="torch.index_select()"></a>torch.index_select()</h3><pre><code>1  </code></pre><p>| </p><pre><code>torch.index_select(input, dim, index, out=None) # -&gt; Tensor    </code></pre><p>—|—  </p><p>返回在 dim 维度上的 index 指明的下标组成的 tensor.<br>返回的 tensor 的维度的数量和 input 是相同的, 但是第 dim 维度的 size 会和 index size大小相同. 其他维度的 size 保持不变.</p><h3 id="torch-le"><a href="#torch-le" class="headerlink" title="torch.le()"></a>torch.le()</h3><pre><code>1  </code></pre><p>| </p><pre><code>torch.le(input, other, out=None) # -&gt;Tensor    </code></pre><p>—|—  </p><p>按元素计算 input≤other.</p><h3 id="max"><a href="#max" class="headerlink" title="max()"></a>max()</h3><pre><code>1  2  3  </code></pre><p>| </p><pre><code>torch.max(input) # 返回一个Tensor, 代表所有元素中的最大值    torch.max(input,dim,keepdim=False,out=None) # 返回一个元组:(Tensor, LongTensor)    </code></pre><p>—|—  </p><p>第二种形式会返回一个元组, 元组内元素类型为: (Tensor, LongTensor), 其中, 前者代表对应 dim 上 reduce 后的最大值, 后者代表最大值在维度 dim 中对应的下标.<br>如果keepdim=True, 则输出的 tensor 的 size 会和输入的相同, 只不过对应 dim 维度上的size为1. 否则, 对应 dim 维度会被 squeeze/reduce, 使得输出的维度比输入的维度少1.  </p><pre><code>1  2  3  4  5  6  7  8  </code></pre><p>| </p><pre><code>&gt;&gt;&gt; a = torch.randn(4, 4)  &gt;&gt;&gt; a  tensor([[-1.2360, -0.2942, -0.1222,  0.8475],          [ 1.1949, -1.1127, -2.2379, -0.6702],          [ 1.5717, -0.9207,  0.1297, -1.8768],          [-0.6172,  1.0036, -0.6060, -0.2432]])  &gt;&gt;&gt; torch.max(a, 1)  (tensor([ 0.8475,  1.1949,  1.5717,  1.0036]), tensor([ 3,  0,  0,  1]))    </code></pre><p>—|—  </p><h3 id="mm"><a href="#mm" class="headerlink" title="mm()"></a>mm()</h3><p>注意, 没有torch.mm_版本  </p><pre><code>1  </code></pre><p>| </p><pre><code>torch.mm(mat1, mat2, out=None) # 返回值为Tensor, 也可以使用out记录返回值    </code></pre><p>—|—  </p><p>两矩阵相乘, 矩阵的size需要满足乘法规则<br>其他调用形式:  </p><pre><code>1  </code></pre><p>| </p><pre><code>torch.Tensor(mat2) # 调用者为mat1    </code></pre><p>—|—  </p><h3 id="norm"><a href="#norm" class="headerlink" title="norm()"></a>norm()</h3><p>返回输入tensor的p-norm标量  </p><pre><code>1  </code></pre><p>| </p><pre><code>torch.norm(input, p=2) # 返回一个标量tensor    </code></pre><p>—|—  </p><h3 id="numel"><a href="#numel" class="headerlink" title="numel()"></a>numel()</h3><pre><code>1  </code></pre><p>| </p><pre><code>torch.numel(input)  #返回一个int值    </code></pre><p>—|—  </p><p>返回 inpput tensor 中的元素的总个数  </p><pre><code>1  2  </code></pre><p>| </p><pre><code>a = torch.randn(1,2,3,4,5)  print(torch.numel(a)) # 120    </code></pre><p>—|—  </p><h3 id="ones"><a href="#ones" class="headerlink" title="ones()"></a>ones()</h3><h3 id="randn"><a href="#randn" class="headerlink" title="randn()"></a>randn()</h3><p>标准正太分布随机基础, 传入参数为维度信息</p><h3 id="torch-sort"><a href="#torch-sort" class="headerlink" title="torch.sort()"></a>torch.sort()</h3><pre><code>1  </code></pre><p>| </p><pre><code>torch.sort(input, dim=None, descending=False, out=None) # 返回 (Tensor, LongTensor)    </code></pre><p>—|—  </p><h3 id="sum"><a href="#sum" class="headerlink" title="sum()"></a>sum()</h3><pre><code>1  2  3  4  </code></pre><p>| </p><pre><code>torch.sum(input, dtype=None) # 返回求和后的Tensor(只有一个元素)    torch.sum(input, dim, keepdim=False, dtype=None) # 返回在dim上reduce的sum和, 如果dim包含多个维度, 则都进行reduce求和.  # reduce这个词很形象, 因为返回的Tensor的维度刚好没有了dim指示的那些维度    </code></pre><p>—|—  </p><p>其他形式:  </p><pre><code>1  </code></pre><p>| </p><pre><code>torch.Tensor.sum()    </code></pre><p>—|—  </p><h3 id="torch-t"><a href="#torch-t" class="headerlink" title="torch.t()"></a>torch.t()</h3><pre><code>1  </code></pre><p>| </p><pre><code>torch.t(input) # 返回转置后的Tensor    </code></pre><p>—|—  </p><p>其他形式:  </p><pre><code>1  </code></pre><p>| </p><pre><code>torch.Tensor.t()    </code></pre><p>—|—  </p><h3 id="unsqueeze"><a href="#unsqueeze" class="headerlink" title="unsqueeze()"></a>unsqueeze()</h3><p>在指定维度上插入一个 singleton 维度(一般用于将单一数据处理用 batch 的形式)  </p><pre><code>1  </code></pre><p>| </p><pre><code>torch.unsqueeze(input, dim, out=None) # -&gt; Tensor    </code></pre><p>—|—  </p><p>返回的tensor与input tensor 共享数据</p><p>dim 的取值范围在 [-input.dim()-1, input.dim()+1] 之间, 如果为负值, 则相当于 dim = dim + input.dim() + 1.</p><h3 id="zeros"><a href="#zeros" class="headerlink" title="zeros()"></a>zeros()</h3><h2 id="torch-cuda"><a href="#torch-cuda" class="headerlink" title="torch.cuda"></a>torch.cuda</h2><h3 id="torch-cuda-empty-cache"><a href="#torch-cuda-empty-cache" class="headerlink" title="torch.cuda.empty_cache()"></a>torch.cuda.empty_cache()</h3><p>释放所有未使用的 GPU 内存, 使用这些内存可以被其他 GPU 应用使用, 并且可以被 nvidia-smi 查到.<br>empty_cache() 并不会强制提升供 PyTorch 使用的显卡内存的大小, 查看Memory management</p><h2 id="torch-Tensor"><a href="#torch-Tensor" class="headerlink" title="torch.Tensor"></a>torch.Tensor</h2><p>torch.Tensor 是默认类型 torch.FloatTensor 的别名, 使用 torch.Tenosr 的构造函数创建 tensor 变量时, 传入的是维度信息(注意与 torch.tensor() 的区别):  </p><pre><code>1  2  </code></pre><p>| </p><pre><code>t = torch.Tensor(2,3,4) # 里面的数值未初始化, 是随机的  print(t.size()) # torch.Size([2,3,4])    </code></pre><p>—|—  </p><p>torch.LongTesnor 使用方法相似, 只不过数据类型是长整型.</p><h3 id="troch-tensor"><a href="#troch-tensor" class="headerlink" title="troch.tensor()"></a>troch.tensor()</h3><p>创建tensor  </p><pre><code>1  </code></pre><p>| </p><pre><code>torch.tensor(data, dtype=None, device=None, requires_grad=False)    </code></pre><p>—|—  </p><p>可以利用torch.tensor从python的list数据或者其他序列数据中创建tensor对象  </p><pre><code>1  2  </code></pre><p>| </p><pre><code>torch.tensor([[1,-1],[1,-1]])  torch.tensor(np.array([[1,2,3],[4,5,6]]))    </code></pre><p>—|—  </p><p>注意, torch.tensor()函数总是会对数据进行复制操作, 因此, 如果你仅仅是想将数据的requires_grad标志改变, 那么就应该使用required_grad_()或者detach()函数来避免复制. 同时, 对numpy数组使用torch.as_tensor()将其转换成tensor而无需复制</p><h3 id="torch-Tensor-cpu"><a href="#torch-Tensor-cpu" class="headerlink" title="torch.Tensor.cpu()"></a>torch.Tensor.cpu()</h3><pre><code>1  2  </code></pre><p>| </p><pre><code>torch.Tensor.cpu()  z = x.cpu()    </code></pre><p>—|—  </p><p>将tensor移动到cpu上, 注意返回值z是cpu上的数据, tensor x 本身的device属性不变</p><h3 id="torch-Tensor-cuda"><a href="#torch-Tensor-cuda" class="headerlink" title="torch.Tensor.cuda()"></a>torch.Tensor.cuda()</h3><pre><code>1  2  </code></pre><p>| </p><pre><code>torch.Tensor.cuda()  z = x.cuda()    </code></pre><p>—|—  </p><h3 id="torch-Tensor-dim"><a href="#torch-Tensor-dim" class="headerlink" title="torch.Tensor.dim()"></a>torch.Tensor.dim()</h3><pre><code>1  </code></pre><p>| </p><pre><code>torch.Tensor.dim() -&gt; int    </code></pre><p>—|—  </p><p>返回 tensor 的维度的个数.</p><h3 id="torch-Tensor-max"><a href="#torch-Tensor-max" class="headerlink" title="torch.Tensor.max()"></a>torch.Tensor.max()</h3><pre><code>1  </code></pre><p>| </p><pre><code>torch.Tensor.max(dim=None, keepdim=False) -&gt; Tensor or (Tensor, Tensor)    </code></pre><p>—|—  </p><p>详情见 torch.max()</p><h3 id="torch-Tensor-numel"><a href="#torch-Tensor-numel" class="headerlink" title="torch.Tensor.numel()"></a>torch.Tensor.numel()</h3><pre><code>1  </code></pre><p>| </p><pre><code>torch.Tensor.numel()    </code></pre><p>—|—  </p><p>详见 torch.numel()</p><h3 id="torch-Tensor-to"><a href="#torch-Tensor-to" class="headerlink" title="torch.Tensor.to()"></a>torch.Tensor.to()</h3><pre><code>1  </code></pre><p>| </p><pre><code>torch.Tensor.to(*args, *kwargs)    </code></pre><p>—|—  </p><p>返回一个转移后的tensor, 而自身维持不变  </p><pre><code>1  2  3  4  </code></pre><p>| </p><pre><code>t = torch.randn(2,3)  t.to(torch.float64)  t.to(device)  t.to("cuda:0")    </code></pre><p>—|—  </p><p>将tensor移动到gpu上, 注意返回值 z 是gpu 上的数据, tensor x 本身的 device 属性不变</p><h3 id="torch-Tensor-numpy"><a href="#torch-Tensor-numpy" class="headerlink" title="torch.Tensor.numpy()"></a>torch.Tensor.numpy()</h3><p>tensor与numpy数组的转换  </p><pre><code>1  2  3  </code></pre><p>| </p><pre><code>torch.Tensor.numpy() # 返回tensor对应的numpy数组    torch.from_numpy(ndarray) # 将numpy数组ndarray转换成对应的tensor并返回.    </code></pre><p>—|—  </p><p>torch.Tensor 实际上是 torch.FloatFensor 的别名</p><h3 id="torch-Tensor-permute"><a href="#torch-Tensor-permute" class="headerlink" title="torch.Tensor.permute()"></a>torch.Tensor.permute()</h3><p>重新排列tensor的维度  </p><pre><code>1  </code></pre><p>| </p><pre><code>torch.Tensor.permute(*dims) # 返回一个重新排列维度后的 tensor    </code></pre><p>—|—  </p><h3 id="torch-Tensor-unsqueeze"><a href="#torch-Tensor-unsqueeze" class="headerlink" title="torch.Tensor.unsqueeze()"></a>torch.Tensor.unsqueeze()</h3><p>详细可见torch.unsqueeze</p><h3 id="torch-Tensor-expand"><a href="#torch-Tensor-expand" class="headerlink" title="torch.Tensor.expand()"></a>torch.Tensor.expand()</h3><pre><code>1  </code></pre><p>| </p><pre><code>torch.Tensor.expand(*sizes) # 返回 tensor    </code></pre><p>—|—  </p><p>将 tensor 中的 singleton 维度扩展到一个更大的 size.<br>参数 -1 意味着不改变原始的维度<br>新增的维度的元素被被添加到前头, size不能设置为-1.<br>expand 并没有申请新的内存, 而仅仅是在当前已经存在的 tensor 上面创建了新的视图(view), 使得 singleton 维度被扩展成了一个更大的尺寸.<br>Any dimension of size 1 can be expanded to an arbitrary value without new memory.  </p><pre><code>1  2  3  4  </code></pre><p>| </p><pre><code>x = torch.tensor([1],[2],[3])  print(x.size())  # torch.Size([3,1])  print(x.expand(3,4)) # torch.Size([3,4]) # 将维度为1的扩展到任意尺寸  print(x.expand(-1,4)) # torch.Size([3,4]) # -1 代表不改变维度    </code></pre><p>—|—  </p><p>注意, 只能对 singleton 的维度进行扩展, 如果强行对其他维度扩展, 则会报错.</p><h3 id="torch-Tensor-expand-as"><a href="#torch-Tensor-expand-as" class="headerlink" title="torch.Tensor.expand_as()"></a>torch.Tensor.expand_as()</h3><pre><code>1  </code></pre><p>| </p><pre><code>torch.Tensor.expand_as(other) # 返回 tensor    </code></pre><p>—|—  </p><p>将当前 tensor 扩展到和 other 一样的size.<br>self.expand_as(other) 与 self.expand(other.size()) 等价.</p><h3 id="torch-Tensor-index-fill"><a href="#torch-Tensor-index-fill" class="headerlink" title="torch.Tensor.index_fill_()"></a>torch.Tensor.index_fill_()</h3><pre><code>1  </code></pre><p>| </p><pre><code>torch.Tensor.index_fill_(dim, index, val) # 返回tensor    </code></pre><p>—|—  </p><p>在给定的维度 dim 上, 用 val 将该维度上的 index 坐标的值填充.  </p><pre><code>1  2  3  4  5  6  7  </code></pre><p>| </p><pre><code>x = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9]], dtype=torch.float)  index = torch.tensor([0, 2])  x.index_fill_(1, index, -1)  print(x)  #tensor([[-1.,  2., -1.],  #       [-1.,  5., -1.],  #       [-1.,  8., -1.]])    </code></pre><p>—|—  </p><h3 id="torch-Tensor-contiguous"><a href="#torch-Tensor-contiguous" class="headerlink" title="torch.Tensor.contiguous()"></a>torch.Tensor.contiguous()</h3><p>返回一个连续的tensor, 数据内容不变  </p><pre><code>1  </code></pre><p>| </p><pre><code>torch.Tensor.contiguous() # 如果tensor本身就是连续的, 那么就会返回tensor本身    </code></pre><p>—|—  </p><p>这里的 contiguous 指的是内存上的连续, 由于在 PyTorch 中, view 只能用在 contiguous 的 tensor 上面, 而如果在 view 之前使用了 transpose, permute 等操作后, 就需要使用 contiguous 来返回一个 contiguous tensor.<br>在 PyTorch 0.4 版本以后, 增加了 torch.reshape(), 这与 numpy.reshape() 的功能类似, 它大致相当于 tensor.contiguous().view() ?</p><h3 id="torch-Tensor-item"><a href="#torch-Tensor-item" class="headerlink" title="torch.Tensor.item()"></a>torch.Tensor.item()</h3><p>当Tensor中只包含一个元素时, 可以利用该函数返回这个元素的标量</p><h3 id="torch-Tensor-tolist"><a href="#torch-Tensor-tolist" class="headerlink" title="torch.Tensor.tolist()"></a>torch.Tensor.tolist()</h3><p>可以将Tensor转换成列表</p><h3 id="torch-Tensor-zero"><a href="#torch-Tensor-zero" class="headerlink" title="torch.Tensor.zero_()"></a>torch.Tensor.zero_()</h3><pre><code>1  </code></pre><p>| </p><pre><code>torch.Tensor.zero_()    </code></pre><p>—|—  </p><p>将当前的 tensor 变量全部置为0(原地)</p><h2 id="torch-autograd"><a href="#torch-autograd" class="headerlink" title="torch.autograd"></a>torch.autograd</h2><h3 id="set-grad-enabled"><a href="#set-grad-enabled" class="headerlink" title="set_grad_enabled()"></a>set_grad_enabled()</h3><pre><code>1  </code></pre><p>| </p><pre><code>class torch.autograd.set_grad_enabled(mode)    </code></pre><p>—|—  </p><p>用来控制梯度计算的开关(依据bool类型参数mode决定), 可以当做上下文管理器使用, 也可以当做函数使用  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  </code></pre><p>| </p><pre><code># 当做上下文管理器  with torch.set_grad_enabled(is_train): # 注意, 这里省略了autograd      loss.backward()      optimizer.step()    # 当做函数使用  w1 = torch.Tensor([1], requires=True)  torch.set_grad_enabled(True)  print(w1.requires_grad) # True  torch.set_grad_enabled(False)  print(w1.requires_grad) # False    </code></pre><p>—|—  </p><h3 id="no-grad"><a href="#no-grad" class="headerlink" title="no_grad()"></a>no_grad()</h3><pre><code>1  </code></pre><p>| </p><pre><code>class torch.autograd.no_grad    </code></pre><p>—|—  </p><p>用于禁用梯度计算的上下文管理器.<br>在测试阶段, 当你确信你不会调用Tensor.backward()时,禁用梯度计算十分有用. 这会降低计算使用内存消耗.  </p><pre><code>1  2  3  4  5  6  </code></pre><p>| </p><pre><code>x = torch.tensor([1.0], requires_grad=True)  with torch.no_grad(): # 省略了autograd      print(x.requires_grad) # True, 虽然为True, 但在该上下文中, 会无视掉requires_grad参数, 一律做False处理      y = x*2        print(y.requires_grad) # False, 在当前上下文产生的tensor的requires_grad属性为False  print(x.requires_grad) # True    </code></pre><p>—|—  </p><h3 id="torch-autograd-Function"><a href="#torch-autograd-Function" class="headerlink" title="torch.autograd.Function"></a>torch.autograd.Function</h3><pre><code>1  </code></pre><p>| </p><pre><code>class torch.autograd.Function    </code></pre><p>—|—  </p><p>为可微分的 ops 记录 operation history, 同时定义计算公式. </p><p>每一个作用在 tensor 上的 operatin 都会创建一个新的 function 对象, 它会执行计算过程并记录相关信息. 这些信息可以从一个由 functions 组成的有向图中获得. 当 backward() 方法被调用时, 就会利用这些信息在 function 上进行反向传播, 并将梯度传给下一个 Funtion.<br>通常情况下, 当用于需要自定义可自动求导的 ops 时, 可以实现一个 Function 的子类.</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  </code></pre><p>| </p><pre><code># Example  class Exp(Function):        @staticmethod      def forward(ctx, i):          result = i.exp()          ctx.save_for_backward(result)        @staticmethod      def backward(ctx, grad_output):          result, = ctx.saved_tensors          return grad_output*result    </code></pre><p>—|—  </p><p>static forward(ctx, <em>args, kwargs):*</em><br>定义前向计算的逻辑.</p><p>static backward(ctx, *grad_outputs):<br>定义反向传导的逻辑, 如果确定不会使用到反向传播, 则可以不实现该函数.</p><h2 id="torch-nn"><a href="#torch-nn" class="headerlink" title="torch.nn"></a>torch.nn</h2><h3 id="Module"><a href="#Module" class="headerlink" title="Module"></a>Module</h3><pre><code>1  </code></pre><p>| </p><pre><code>class torch.nn.Module    </code></pre><p>—|—  </p><p>所有神经网络Module的基类, 自定义的模型也应该是它的子类.<br>Modules可以包含其他Module(如Linear, Conv2d等等).</p><h3 id="parameters"><a href="#parameters" class="headerlink" title="parameters()"></a>parameters()</h3><pre><code>1  2  </code></pre><p>| </p><pre><code>for param in model.parameters():      print(param.data, param.size())    </code></pre><p>—|—  </p><p>state_dict:  </p><pre><code>1  </code></pre><p>| </p><pre><code>torch.nn.Module.state_dict(destination=None,prefix="",keep_vars=False)    </code></pre><p>—|—  </p><p>以字典形式返回整个module的状态</p><h4 id="train"><a href="#train" class="headerlink" title="train"></a>train</h4><pre><code>1  </code></pre><p>| </p><pre><code>torch.nn.Module.train(mode=True)    </code></pre><p>—|—  </p><p>将module的模式设置为train, 这只对部分module有效, 如Dropout, BatchNorm等, 详细请查看官网.<br>返回值: torch.nn.Module</p><h4 id="training"><a href="#training" class="headerlink" title="training"></a>training</h4><pre><code>1  </code></pre><p>| </p><pre><code>torch.nn.Module.training # 属性, 返回一个bool值, 指示当前的模式是否为train    </code></pre><p>—|—  </p><h4 id="eval"><a href="#eval" class="headerlink" title="eval"></a>eval</h4><pre><code>1  </code></pre><p>| </p><pre><code>torch.nn.Module.eval() # 注意, 和train不同, eval为无参函数    </code></pre><p>—|—  </p><p>将module的mode设置为evaluation, 同样, 只对部分module起效.</p><h3 id="Linear"><a href="#Linear" class="headerlink" title="Linear"></a>Linear</h3><pre><code>1  </code></pre><p>| </p><pre><code>torch.nn.Linear(in_features, out_features, bias=True)    </code></pre><p>—|—  </p><p>全连接层的实现. 输入的shape为 (N,…,infeatures), 输出的shape为 (N,…,outfeatures), 可以看出, 除了最后一维不同外, 其他维度都相同. (通常在使用Linear之前, 会将输入变成二维的矩阵, 其中第一维为batch size, 第二维为特征向量).<br>in_features 和 out_features 可以当做属性用.来获取.</p><h3 id="Conv2d"><a href="#Conv2d" class="headerlink" title="Conv2d"></a>Conv2d</h3><pre><code>1  </code></pre><p>| </p><pre><code>class torch.nn.Conv2的(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True)    </code></pre><p>—|—  </p><ul><li>in_channels(int):</li><li>out_channels(int):</li><li>kernel_size(intortuple):</li><li>stride(intortuple, optional):</li></ul><h3 id="MaxPool2d"><a href="#MaxPool2d" class="headerlink" title="MaxPool2d"></a>MaxPool2d</h3><h3 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax()"></a>Softmax()</h3><pre><code>    1      </code></pre><p>| </p><pre><code>    class torch.nn.Softmax(dim=None)        </code></pre><p>—|—  </p><p>dim指明了需要进行 softmax 的维度, 在这个维度上的值, 加起来和为1.</p><h3 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h3><pre><code>1  </code></pre><p>| </p><pre><code>torch.nn.ReLU(inplace=False)    </code></pre><p>—|—  </p><p>输入输出的shape是相同的, 执行relu函数</p><h3 id="torch-nn-Sequential"><a href="#torch-nn-Sequential" class="headerlink" title="torch.nn.Sequential"></a>torch.nn.Sequential</h3><pre><code>1  </code></pre><p>| </p><pre><code>class torch.nn.Sequential(*args)    </code></pre><p>—|—  </p><h3 id="torch-nn-MSELoss"><a href="#torch-nn-MSELoss" class="headerlink" title="torch.nn.MSELoss"></a>torch.nn.MSELoss</h3><pre><code>1  </code></pre><p>| </p><pre><code>class torch.nn.MSELoss(size_average=None, reduce=None, reduction="elementwise_mean")    </code></pre><p>—|—  </p><ol><li><p>size_average(bool, optional): 弃用(见reduction参数). 默认情况下, loss会计算在每个样本上的平均误差. 如果将size_average置为False, 则计算平方误差总和. 当reduce参数为False时, 忽视该参数</p></li><li><p>reduce(bool, optional): 弃用(见reduction参数). reduce参数顾名思义, 就是是否让MSELoss函数返回值的维度减少, 默认为True, 即会将任意维度的输入计算loss后, 返回一个标量(平均or总和取决于size_average), 如果为False, 则说明返回值维度不应该发生变化, 故而返回值就是对每个元素单独进行平方损失计算.</p><pre><code>1  2  3  4  5  6  7  8  9  10</code></pre></li></ol><p>| </p><pre><code>     y = torch.tensor([1,2,3,4], dtype=torch.float)       pred_y = torch.tensor([1,1,1,1], dtype=torch.float)       loss_fn1 = torch.nn.MSELoss()       loss1 = loss_fn1(y, pred_y)       loss_fn2 = torch.nn.MSELoss(size_average=False)       loss2 = loss_fn2(y, pred_y)       loss_fn3 = torch.nn.MSELoss(reduce=False)       loss3 = loss_fn3(y, pred_y)       print(loss1,loss2,loss3)       # tensor(3.5000) tensor(14.) tensor([0., 1., 4., 9.])         </code></pre><p>—|—<br>  3. reduction(string, optional): 用字符串来替代上面两个参数的作用: “elementwise_mean”(默认) | “sum” | “none” (不进行reduce).</p><h3 id="torch-nn-functional"><a href="#torch-nn-functional" class="headerlink" title="torch.nn.functional"></a>torch.nn.functional</h3><h4 id="conv1d"><a href="#conv1d" class="headerlink" title="conv1d()"></a>conv1d()</h4><h4 id="conv2d"><a href="#conv2d" class="headerlink" title="conv2d()"></a>conv2d()</h4><h4 id="relu"><a href="#relu" class="headerlink" title="relu()"></a>relu()</h4><pre><code>     1       </code></pre><p>| </p><pre><code>     torch.nn.functional.relu(input, inplace=True) # 返回 一个 Tenosr         </code></pre><p>—|—  </p><h4 id="relu-1"><a href="#relu-1" class="headerlink" title="relu_()"></a>relu_()</h4><pre><code>1  </code></pre><p>| </p><pre><code>torch.nn.functional.relu_(input) # relu() 的原地版本    </code></pre><p>—|—  </p><h2 id="torch-optim"><a href="#torch-optim" class="headerlink" title="torch.optim"></a>torch.optim</h2><h3 id="lr-scheduler"><a href="#lr-scheduler" class="headerlink" title="lr_scheduler"></a>lr_scheduler</h3><h4 id="StepLR"><a href="#StepLR" class="headerlink" title="StepLR"></a>StepLR</h4><pre><code>1  </code></pre><p>| </p><pre><code>class torch.optim.lr_schedulr.StepLR(optimizer,step_size,gamma=0.1,last_epoch=-1)    </code></pre><p>—|—  </p><p>每经过step_size次epoch之后, lr就会衰减gamma倍(new_lr=lr×gamma), 初始的lr来自于optimizer中的lr参数.  </p><pre><code>1  2  3  4  5  </code></pre><p>| </p><pre><code># Observe that all parameters are being optimized  optimizer_ft = optim.SGD(model_ft.parameters(), lr=0.001, momentum=0.9)    # Decay LR by a factor of 0.1 every 7 epochs  exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=7, gamma=0.1)    </code></pre><p>—|—  </p><h4 id="ExponentialLR"><a href="#ExponentialLR" class="headerlink" title="ExponentialLR"></a>ExponentialLR</h4><pre><code>1  </code></pre><p>| </p><pre><code>class torch.optim.lr_scheduler.ExponentialLR(optimizer,gamma,last_epoch=-1)    </code></pre><p>—|—  </p><h4 id="CosineAnnealingLR"><a href="#CosineAnnealingLR" class="headerlink" title="CosineAnnealingLR"></a>CosineAnnealingLR</h4><pre><code>1  2  </code></pre><p>| </p><pre><code>## Adam  class torch.optim.Adam(params,lr=0.001,betas=(0.9,0.999),eps=1e-08,weight_decay=0,amsgrad=False)    </code></pre><p>—|—  </p><h4 id="conv2d-1"><a href="#conv2d-1" class="headerlink" title="conv2d"></a>conv2d</h4><h2 id="torch-utils-data"><a href="#torch-utils-data" class="headerlink" title="torch.utils.data"></a>torch.utils.data</h2><h3 id="DataLoader"><a href="#DataLoader" class="headerlink" title="DataLoader"></a>DataLoader</h3><pre><code>1  </code></pre><p>| </p><pre><code>class torch.utils.data.DataLoader(dataset,batch_size=1,shuffle=False,sampler=None,batch_sampler=None,num_workers=0,collate_fn=&lt;function default_collate&gt;,pin_memory=False,drop_last=False,timeout=0,worker_init_fn=None)    </code></pre><p>—|—  </p><p>数据加载器, 将数据集和采样器结合起来, 并且提供单/多线程的迭代器.</p><ul><li>dataset(utils.data.Dataset):</li><li>batch_size(int,optional): batch中的样本个数</li><li>shuffle(bool,optional)</li><li>num_worker(int,optional): 加载数据的线程个数, 0意味着只有一个主线程.<br>方法：</li><li><strong>iter</strong>(self): 可以当做迭代器使用, 如inputs,class_ids=next(iter(dataloaders)), 其中, input的shape为 (N,C,H,W), class_ids的shape为 (N).</li><li><strong>len</strong>(self): 返回数据集的类别数目</li></ul><h2 id="torchvision"><a href="#torchvision" class="headerlink" title="torchvision"></a>torchvision</h2><h3 id="torchvision-utils"><a href="#torchvision-utils" class="headerlink" title="torchvision.utils"></a>torchvision.utils</h3><h4 id="make-grid"><a href="#make-grid" class="headerlink" title="make_grid"></a>make_grid</h4><pre><code>1  </code></pre><p>| </p><pre><code>torchvision.utils.make_grid(tensor,nrow=8,padding=2,normalize=False,range=None,scale_each=False,pad_value=0)    </code></pre><p>—|—  </p><p>制作一个关于image的grid, 返回值依然是一个tensor, 只不过尺度变成了3D, 相当于把多个图片拼接在一起了, 直接通过plt.imshow(grid)即可输出网格化以后的图片.</p><ul><li>tensor(Tensor/list): 4D的 mini-batch Tensor, Shape为 (N×C×H×W), 或者是同维度的list.</li></ul><h3 id="torchvision-transforms"><a href="#torchvision-transforms" class="headerlink" title="torchvision.transforms"></a>torchvision.transforms</h3><h4 id="torchvision-transforms-Compose"><a href="#torchvision-transforms-Compose" class="headerlink" title="torchvision.transforms.Compose"></a>torchvision.transforms.Compose</h4><pre><code>    1      2      3      4      5      6      7      </code></pre><p>| </p><pre><code>    class torchvision.transforms.Compose(transforms)            # 使用      trans.Compose([          transforms.CenterCrop(10),          transforms.ToTensor(),      ])        </code></pre><p>—|—  </p><p>将多个transforms操作组合起来, 注意参数是列表形式</p><h4 id="Transforms-on-PIL-Image"><a href="#Transforms-on-PIL-Image" class="headerlink" title="Transforms on PIL Image"></a>Transforms on PIL Image</h4><pre><code>1  2  3  </code></pre><p>| </p><pre><code># cv2 image to PIL Image    # skimage to PIL Image    </code></pre><p>—|—  </p><p>注意, 以下操作作用在PIL Image上的</p><h5 id="CenterCrop"><a href="#CenterCrop" class="headerlink" title="CenterCrop"></a>CenterCrop</h5><pre><code>1  </code></pre><p>| </p><pre><code>class torchvision.transform.CenterCrop(size)    </code></pre><p>—|—  </p><p>size参数表示输出的图谱的大小, 如果只传入了一个数字, 则该数字既表示高度, 又表示宽度.</p><h5 id="Resize"><a href="#Resize" class="headerlink" title="Resize"></a>Resize</h5><pre><code>1  </code></pre><p>| </p><pre><code>class torchvision.transforms.Resize(size, interpolation=2)    </code></pre><p>—|—  </p><ul><li>size: 期望的输出size.</li><li>interpolation: 插值方法, 默认为双线性插值</li></ul><h5 id="ToTensor"><a href="#ToTensor" class="headerlink" title="ToTensor"></a>ToTensor</h5><pre><code>    1      </code></pre><p>| </p><pre><code>    class torchvision.transforms.ToTensor        </code></pre><p>—|—  </p><p>将一个PIL Image或者numpy.ndarray (H×W×C,[0, 255])转换成torch.FloatTensor (C×H×W, [0.0, 1.0]).</p><h5 id="RandomHorizontalFlip"><a href="#RandomHorizontalFlip" class="headerlink" title="RandomHorizontalFlip"></a>RandomHorizontalFlip</h5><pre><code>1  </code></pre><p>| </p><pre><code>transforms.RandomHorizontalFlip(p=0.5)    </code></pre><p>—|—  </p><p>在给定概率下对PIL Image随机执行水平翻转操作</p><h5 id="RandomResizedCrop"><a href="#RandomResizedCrop" class="headerlink" title="RandomResizedCrop"></a>RandomResizedCrop</h5><pre><code>1  </code></pre><p>| </p><pre><code>torch.transforms.RandomResizedCrop(size, scale=(0.08, 1.0), ratio=(0.75, 1.3333333333), interpolation=2)    </code></pre><p>—|—  </p><p>对PIL Image随机执行剪裁操作(按照scale和ratio的区间剪裁), 然后将剪裁后的图片放缩都期望的尺寸(默认插值为双线性插值)</p><ul><li>size: 期望得到的尺寸</li><li>scale: 剪裁的面积比例(相对于原始图)</li><li>ratio: 剪裁的宽高比</li><li>interpolation: 默认为:PIL.Image.BILINEAR</li></ul><h3 id="Transforms-on-torch-Tensor"><a href="#Transforms-on-torch-Tensor" class="headerlink" title="Transforms on torch.*Tensor"></a>Transforms on torch.*Tensor</h3><p>注意, 以下操作是作用在tensor上的</p><h4 id="Normalize"><a href="#Normalize" class="headerlink" title="Normalize"></a>Normalize</h4><pre><code>    1      </code></pre><p>| </p><pre><code>    class torchvision.transforms.Normalize(mean, std)        </code></pre><p>—|—  </p><p>将图片tensor按照均值mean和标准差std进行归一化, 对于n个channels, 有 mean=(M1, …, Mn), std=(S1,…,Sn).<br><strong>注意, 这个归一化操作是原地进行的</strong></p><h3 id="torchvision-datasets"><a href="#torchvision-datasets" class="headerlink" title="torchvision.datasets"></a>torchvision.datasets</h3><h4 id="ImageFolder"><a href="#ImageFolder" class="headerlink" title="ImageFolder"></a>ImageFolder</h4><pre><code>1  </code></pre><p>| </p><pre><code>class torchvision.datasets.ImageFolder(root, transform=None, target_transform=None, loader=&lt;function default_loader&gt;)    </code></pre><p>—|—  </p><p>一个一般化的数据加载器, 主要针对如下数据排列格式:  </p><pre><code>1  2  3  4  5  6  7  </code></pre><p>| </p><pre><code>root/dog/x.png  root/dog/y.png  root/dog/z.png  ...  root/cat/123.png  root/cat/nsdf3.png  root/cat/asd932_.png    </code></pre><p>—|—  </p><ul><li>root: 根目录路径</li><li>transform(callable,optional): 对图片要做的变换操作</li><li>target_transform(callable,optional): 对target要做的变换操作</li><li>loader: 用于加载给定路径图片的函数<br>属性：</li><li>classes(list): 返回类别的名字列表 class_names</li><li>class_to_idx(dict): 以字典的形式返回(class_name, class_index)</li><li>imgs(list): 返回元组列表: (image path, class_index)<br>方法：</li><li>getitem(index): 根据index返回(sample,target)元组. 可以使用</li><li>len(imagefolder) 返回类别数量</li></ul><h2 id="sort"><a href="#sort" class="headerlink" title="sort()"></a>sort()</h2><pre><code>1  </code></pre><p>| </p><pre><code>sort(dim=None, descending=False)  # 默认为升序, 返回(Tensor, LongTensor)    </code></pre><p>—|—  </p><p>详见 torch.sort()</p><h2 id="torch-distributed"><a href="#torch-distributed" class="headerlink" title="torch.distributed"></a>torch.distributed</h2><h3 id="torch-distributed-reduce"><a href="#torch-distributed-reduce" class="headerlink" title="torch.distributed.reduce()"></a>torch.distributed.reduce()</h3><h2 id="inspect-模块"><a href="#inspect-模块" class="headerlink" title="inspect 模块"></a>inspect 模块</h2><pre><code>1  2  3  4  </code></pre><p>| </p><pre><code>inspect.signature() # 查看函数签名, python3.6以上  inspect.getargspec() # 查看函数签名, python3.6以上  inspect.getsource() # 获取模型的code  inspect.getabsfile() # 获取模块的路径    </code></pre><p>—|—  </p><h2 id="un-normalize"><a href="#un-normalize" class="headerlink" title="un normalize"></a>un normalize</h2><pre><code>1  2  3  4  5  6  7  </code></pre><p>| </p><pre><code>mean = torch.tensor([1, 2, 3], dtype=torch.float32)  std = torch.tensor([2, 2, 2], dtype=torch.float32)    normalize = T.Normalize(mean.tolist(), std.tolist())    unnormalize = T.Normalize((-mean / std).tolist(), (1.0 / std).tolist())  ​~~~s    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>glob模块</title>
      <link href="/2019/08/28/glob%E6%A8%A1%E5%9D%97/"/>
      <url>/2019/08/28/glob%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<p>glob模块是Python最简单的模块之一, 内容非常少, 用它可以查找符合特定规则的文件路径名, 查找文件时只会用到三个匹配符:  </p><ol><li><ul><li>: 匹配0个或多个字符</li></ul></li><li>? : 匹配单个字符</li><li>[] : 匹配指定范围内的字符, 如[0-9]匹配数字</li></ol><h2 id="glob-glob"><a href="#glob-glob" class="headerlink" title="glob.glob()"></a>glob.glob()</h2><p>参数:<br>_(str): 文件路径的正则表达式</p><p>返回值:<br>_(list): 符合正则表达式的文件路径列表</p><p>备注:<br>返回所有匹配的文件路径列表, 它只有一个参数pathname, 定义了文件路径匹配的规则, 这里可以是绝对路径或者相对路径:  </p><pre><code>1  2  3  4  5  6  </code></pre><p>| </p><pre><code>import glob  pathes_list = glob.glob("~/Pictures/*.jpg")  # 获取Pictures下的所有图片    relative_pathes_list = glob.glob("../*.py")  # 获取上级目录中的所有.py文件    </code></pre><p>—|—  </p><p>在 linux, osx 系统中, 通配符的匹配是大小写区分的, 也就是需要特别指定大小写:  </p><pre><code>1  </code></pre><p>| </p><pre><code>extensions = ['jpg', 'JPG', 'jpeg', 'JPEG']    </code></pre><p>—|—  </p><p>但是在 windows 当中, 通配符的匹配是不区分大小写的, 因此只需要指定大小写中的一个即可, 两个都指定的话, 会出现重复的情况  </p><pre><code>1  </code></pre><p>| </p><pre><code>extensions = ['jpg', 'jpeg']    </code></pre><p>—|—  </p><h2 id="glob-iglob"><a href="#glob-iglob" class="headerlink" title="glob.iglob"></a>glob.iglob</h2><p>获取一个可遍历的对象, 使用它可以逐个获取匹配的文件路径名. 与glob.glob()的区别是: glob.glob()会同时获取到所有的匹配路径, 而glob.iglob()一次只获取一个匹配路径.  </p><pre><code>1  2  3  4  </code></pre><p>| </p><pre><code>f = glob.iglob("../*.py")  print f # &lt;generator object iglob at 0x00B9FF80&gt;  for py in f:      print(py)    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>logging模块</title>
      <link href="/2019/08/28/logging%E6%A8%A1%E5%9D%97/"/>
      <url>/2019/08/28/logging%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<h2 id="简单使用"><a href="#简单使用" class="headerlink" title="简单使用"></a>简单使用</h2><pre><code>1  2  3  4  5  6  7  </code></pre><p>| </p><pre><code>import logging    logging.debug("debug msg")  logging.info("info msg")  logging.warn("warn msg")  logging.error("error msg")  logging.critical("critical msg")    </code></pre><p>—|—  </p><p>默认情况下, logging模块将日志打印到屏幕上, 只有日志级别高于WARNING的日志信息才回输出</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>numpy实现神经网络</title>
      <link href="/2019/08/28/numpy%E5%AE%9E%E7%8E%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/"/>
      <url>/2019/08/28/numpy%E5%AE%9E%E7%8E%B0%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/</url>
      
        <content type="html"><![CDATA[<pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  </code></pre><p>| </p><pre><code>import numpy as np    # N 为batch size, D_in 为输入维度  # H 为隐藏层的维度, D_out 为输出的维度  N, D_in, H, D_out = 64, 1000, 100, 10    # 创建随机的输入和输出数据  x = np.random.randn(N, D_in) # N × D_in 的矩阵  y = np.random.randn(N, D_out) # N × D_out 的矩阵    # 对两个隐藏层w1,w2进行初始化  w1 = np.random.randn(D_in, H)  w2 = np.random.randn(H, D_out)    # 设置学习率  learning_rate = 1e-6  for t in range(500):      # 前向传播: 计算预测结果 y_pred      h = x.dot(w1) # x维度为64 × 1000, w1维度为 1000 × 100, 计算完以后, h维度为 64 × 100      h_relu = np.maximum(h,0)      y_pred = h_relu.dot(w2) # h_relu维度为 64×100, w2维度为100×10, y的维度为64×10        # 计算损失      loss = np.square(y_pred - y).sum()      print(t, loss)        # 反向传播根据loss更新w1和w2的值      grad_y_pred = 2.0*(y_pred - y) # 对y_pred求导      grad_w2 = h_relu.T.dot(grad_y_pred) # 对w2求导, 微分矩阵应该与w2的size相同      grad_h_relu = grad_y_pred.dot(w2.T) # 对h_relu求导      grad_h = grad_h_relu.copy()      grad_h[h &lt; 0] = 0 # 经过relu, 将小于0的梯度归0      grad_w1 = x.T.dot(grad_h)        # Update weights      w1 = w1 - learning_rate * grad_w1      w2 = w2 - learning_rate * grad_w2    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>opencv模块</title>
      <link href="/2019/08/28/opencv%E6%A8%A1%E5%9D%97/"/>
      <url>/2019/08/28/opencv%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<h2 id="opencv-基础知识"><a href="#opencv-基础知识" class="headerlink" title="opencv 基础知识"></a>opencv 基础知识</h2><p>cv2.imread 读入的图片, 其shape为(h, w, c), 颜色通道顺序为 (b, g, r)</p><h2 id="常用颜色"><a href="#常用颜色" class="headerlink" title="常用颜色"></a>常用颜色</h2><h2 id="读取图片"><a href="#读取图片" class="headerlink" title="读取图片"></a>读取图片</h2><pre><code>1  </code></pre><p>| </p><pre><code>img = cv2.imread(img_path)    </code></pre><p>—|—  </p><h2 id="保存图片"><a href="#保存图片" class="headerlink" title="保存图片"></a>保存图片</h2><pre><code>1  </code></pre><p>| </p><pre><code>cv2.imwrite(save_path, img)    </code></pre><p>—|—  </p><h2 id="文本"><a href="#文本" class="headerlink" title="文本"></a>文本</h2><p>(startX, startY) 为左上角坐标  </p><pre><code>1  </code></pre><p>| </p><pre><code>cv2.putText(img, "text test", (startX, startY), cv2.FONT_HERSHEY_SIMPLEX, font_size, (B,G,R), thickness)    </code></pre><p>—|—  </p><h2 id="画框"><a href="#画框" class="headerlink" title="画框"></a>画框</h2><p>(x,y) 为左上角坐标<br>(x+h,y+w) 为右下角坐标  </p><pre><code>1  </code></pre><p>| </p><pre><code>cv2.rectangle(img,(x,y), (x+h,y+w), (0,255,0), thickness)    </code></pre><p>—|—  </p><h2 id="waitKey"><a href="#waitKey" class="headerlink" title="waitKey()"></a>waitKey()</h2><pre><code>1  2  3  4  5  </code></pre><p>| </p><pre><code>keypress = cv2.waitKey(200) # 200为当前图片的显示持续时间    if keypress == ord('c') # keypress为按键的整数形式, 所以需要用ord将字符类型转换    if cv2.waitKey(200) == 27: # Decimal 27 = Esc    </code></pre><p>—|—  </p><h2 id="opencv与numpy"><a href="#opencv与numpy" class="headerlink" title="opencv与numpy"></a>opencv与numpy</h2><p>opencv的基础类型为numpy.ndarray, 因此可以直接使用 ndarray 的一些属性的方法  </p><pre><code>1  2  3  4  </code></pre><p>| </p><pre><code>import cv2  img = cv2.imread('./test.jpg')  print(type(img)) # &lt;class 'numpy.ndarray'&gt;  print(img.shape) # (500, 1069, 3)  (高, 宽, 通道)    </code></pre><p>—|—  </p><p>利用 cv2.merge 方法将 numpy.ndarray 数据转换成opencv的图片数据:  </p><pre><code>1  2  3  4  5  6  7  8  9  10  </code></pre><p>| </p><pre><code># 图片的分辨率为300*200(宽*高)，这里b, g, r设为随机值，注意dtype属性  b = np.random.randint(0, 255, (200, 300), dtype=np.uint8)  g = np.random.randint(0, 255, (200, 300), dtype=np.uint8)  r = np.random.randint(0, 255, (200, 300), dtype=np.uint8)  # 合并通道，形成图片  img = cv2.merge([b, g, r])  # opencv的通道是b在最前,r在最后  # 显示图片  cv2.imshow('test', img)  cv2.waitKey(0)  cv2.destroyWindow('test')    </code></pre><p>—|—  </p><h2 id="通道的拆分与合并"><a href="#通道的拆分与合并" class="headerlink" title="通道的拆分与合并"></a>通道的拆分与合并</h2><p>拆分: cv2.split<br>合并: cv2.merge  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  </code></pre><p>| </p><pre><code># 图片的分辨率为800*200(宽*高)，这里b, g, r设为随机值，注意dtype属性  b = np.random.randint(0, 255, (200, 800), dtype=np.uint8)  g = np.random.randint(0, 255, (200, 800), dtype=np.uint8)  r = np.random.randint(0, 255, (200, 800), dtype=np.uint8)  # 合并通道，形成图片  img = cv2.merge([b, g, r])  # opencv的通道是b在最前,r在最后  # 显示图片  cv2.imshow('test', img)  cv2.waitKey(0)  cv2.destroyWindow('test')  # 拆分通道, 每个通道都变成了单通道数组  [blue, green, red] = cv2.split(img)    </code></pre><p>—|—  </p><h2 id="将-BGR-转换成-RGB-通道顺序"><a href="#将-BGR-转换成-RGB-通道顺序" class="headerlink" title="将 BGR 转换成 RGB 通道顺序"></a>将 BGR 转换成 RGB 通道顺序</h2><pre><code>1  2  3  4  5  </code></pre><p>| </p><pre><code># 方法一:  rgb_img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)  # 方法二:  rgb_img = img[:, :, [2, 1, 0]]   # img[h,w,v]  rgb_img = img[:, :, ::-1]    </code></pre><p>—|—  </p><h2 id="PIL-与-cv2-格式互相转换"><a href="#PIL-与-cv2-格式互相转换" class="headerlink" title="PIL 与 cv2 格式互相转换"></a>PIL 与 cv2 格式互相转换</h2><p>PIL.Image读入的图片数据类型不是 numpy 数组, 它的size属性为 (w, h), 利用np.array转换成 numpy 数组后, 它的通道顺序为 (r, g, b)  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  </code></pre><p>| </p><pre><code>from PIL import Image  import numpy as np    # PIL to cv2  pil_img = Image.open(img_path)  print(pil_img.size) # (w, h)  np_img = np.array(pil_img)  cv2_img = np_img[:, :, ::-1] # 交换通道    # cv2 to PIL  pil_img = Image.fromarray(cv2_img[:, :, ::-1])    </code></pre><p>—|—  </p><h2 id="用matplotlib显示图像"><a href="#用matplotlib显示图像" class="headerlink" title="用matplotlib显示图像"></a>用matplotlib显示图像</h2><pre><code>1  2  3  4  </code></pre><p>| </p><pre><code>b,g,r=cv2.split(img)  img2=cv2.merge([r,g,b])  plt.imshow(img2)  plt.show()    </code></pre><p>—|—  </p><h2 id="截取子图"><a href="#截取子图" class="headerlink" title="截取子图"></a>截取子图</h2><pre><code>1  2  </code></pre><p>| </p><pre><code># 已知子图左上角坐标 (x1, y1), 右下角坐标(x2, y2)  crop_img = img[y1:y2, x1:x2, :]    </code></pre><p>—|—  </p><h2 id="opencv-核心算法"><a href="#opencv-核心算法" class="headerlink" title="opencv 核心算法"></a>opencv 核心算法</h2><h2 id="cv2"><a href="#cv2" class="headerlink" title="cv2"></a>cv2</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  </code></pre><p>| </p><pre><code>import cv2  image_path = './test.jpg'  src_image = cv2.imread(image_path) # 读取图片    size = src_image.shape  # 获取图片的尺寸, 返回一个元组: (height, width, depth)    copy_image = src_image.copy() # 复制图片    cv2.imwrite('./dst_test.jpg', copy_image) # 保存图片    cv2.imshow('image', src_image) # 显示图片    # 利用下标访问指定像素  for x in range(src_image.shape[0]): # 以行为主, 行数=图片height    for y in range(src_image.shape[1]):  # 列数 = 图片width      src_image[x,y] = (255,0,255)   # (blue, green, red)  值越高表示对应颜色越显著, 全0为黑, 全255为白    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>python手册</title>
      <link href="/2019/08/28/python%E6%89%8B%E5%86%8C/"/>
      <url>/2019/08/28/python%E6%89%8B%E5%86%8C/</url>
      
        <content type="html"><![CDATA[<p>最近在学习查找资料的过程之中，看到了大佬的博客，觉得写得很好，也是我现在所欠缺的，所以下面先对大佬的博客进行复现。以供自己日后复习，查找，完善成自己的东西。  </p><h2 id="字符串固定字数，不足的空格补齐"><a href="#字符串固定字数，不足的空格补齐" class="headerlink" title="字符串固定字数，不足的空格补齐"></a>字符串固定字数，不足的空格补齐</h2><pre><code>1  2  3  4  5  </code></pre><p>| </p><pre><code>str.ljust(10) # 左对齐 字符串长10位  rjust，ljust和center三个方法来给字符串补全空格  rjust，向右对其，在左边补空格  ljust，向左对其，在右边补空格  center，让字符串居中，在左右补空格    </code></pre><p>—|—  </p><h2 id="排序"><a href="#排序" class="headerlink" title="排序"></a>排序</h2><p>sorted: 返回一个新的 list<br>list.sort(): 改变 list 自身的值<br>reverse 参数: 默认为 False, 升序, True 时变为降序</p><h2 id="列表"><a href="#列表" class="headerlink" title="列表"></a>列表</h2><h3 id="循环删除列表元素"><a href="#循环删除列表元素" class="headerlink" title="循环删除列表元素"></a>循环删除列表元素</h3><p>常见错误: 直接删除, 或者正序删除</p><p>正确做法:<br>1.使用 pop, 倒序删除  </p><pre><code>1  2  </code></pre><p>| </p><pre><code>for i in range(len(list)):         list.pop()    </code></pre><p>—|—  </p><pre><code>2.使用切片, 遍历拷贝列表, 操作原始列表, 用 remove 删除, remove 会操作首个遇到的匹配元素, 相等元素删除, 删除哪个都一样1  2  3  4  5  </code></pre><p>| </p><pre><code>for x in enumerate(a[::]):         a.remove(x)    for x in enumerate(a[::-1]):             a.remove(x)    </code></pre><p>—|—  </p><h3 id="遍历列表"><a href="#遍历列表" class="headerlink" title="遍历列表:"></a>遍历列表:</h3><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  </code></pre><p>| </p><pre><code>zz_list = ['a', 'b', 'c', 'd']    for index in list:      print(index)      # 0      # 1      # 2      # 3  for index in range(len(list)):      print(index)      # 0      # 1      # 2      # 3  for index, val in enumerate(list):      print(index, val)      # 0 a      # 1 b      # 2 c      # 3 d  # 设置遍历的开始序号, val的输出不变  for i, val in enumerate(list, 2):      print(index, val)      # 2 a      # 3 b      # 4 c      # 5 d    </code></pre><p>—|—  </p><h3 id="append-方法"><a href="#append-方法" class="headerlink" title="append() 方法"></a>append() 方法</h3><p>追加单个元素</p><h3 id="extend-方法"><a href="#extend-方法" class="headerlink" title="extend() 方法"></a>extend() 方法</h3><p>extend()函数用于在列表末尾一次性追加另一个序列中的多个值(用新列表扩展原来的列表).<br>该方法没有返回值, 会直接在已经存在的列表中添加新的列表内容, extend和+=的作用差不多  </p><pre><code>1  2  3  4  5  </code></pre><p>| </p><pre><code>a= [[1,2,3],[4,5,6]]  b= [['a','b','c'],['d','e','f']]  a.extend(b)  print(a)  # [[1, 2, 3], [4, 5, 6], ['a', 'b', 'c'], ['d', 'e', 'f']]    </code></pre><p>—|—  </p><h3 id="序列切片-双冒号"><a href="#序列切片-双冒号" class="headerlink" title="序列切片(双冒号)"></a>序列切片(双冒号)</h3><p>Python序列切片地址可以写为 [开始(包含) : 结束(不包含) : 步长]. 当开始省略的时候, 默认从第0项开始, 当结尾省略的时候, 默认到数组最后, 当步长省略的时候, 默认为1. 步长可以为负数, 代表从右向左取数.  </p><pre><code>1  2  3  4  5  6  </code></pre><p>| </p><pre><code>a = range(10) # a = [0, 1, 2, 3, 4, 5, 6, 7, 8 ,9]  a[0:9:1] # [0, 1, 2, 3, 4, 5, 6, 7, 8] 包含开始下标, 不包含结束下标  a[1::2] # [1, 3, 5, 7, 9]  a[::3] # [0, 3, 6, 9]  a[::-1] # [9, 8, 7, 6, 5, 4, 3, 2, 1, 0]  a[::-2] # [9, 7, 5, 3, 1]    </code></pre><p>—|—  </p><h2 id="update-方法"><a href="#update-方法" class="headerlink" title="update() 方法"></a>update() 方法</h2><pre><code>1  </code></pre><p>| </p><pre><code>dict.update(dict2)    </code></pre><p>—|—  </p><p>将 dict2 中的键值更新到 dict 中, 对于存在的则覆盖原值, 对于不存在的则添加新的键值.</p><h3 id="实例"><a href="#实例" class="headerlink" title="实例"></a>实例</h3><pre><code>1  2  3  4  5  6  7  </code></pre><p>| </p><pre><code>#!/usr/bin/python    dict = {'Name': 'Zara', 'Age': 7}  dict2 = {'Sex': 'female' }    dict.update(dict2)  print "Value : %s" %  dict    </code></pre><p>—|—  </p><p>以上实例输出结果为：  </p><pre><code>1  </code></pre><p>| </p><pre><code>Value : {'Age': 7, 'Name': 'Zara', 'Sex': 'female'}    </code></pre><p>—|—  </p><h2 id="字典"><a href="#字典" class="headerlink" title="字典"></a>字典</h2><p>遍历字典:  </p><pre><code>1  </code></pre><p>| </p><pre><code>zz_dict = {'x': 1, 'y':2, 'z':3}    </code></pre><p>—|—  </p><p>遍历keys:  </p><pre><code>1  2  3  4  5  6  7  8  9  </code></pre><p>| </p><pre><code># 输出均为: x y z  for key in zz_dict:      print(key)    for key in zz_dict.iterkeys():      print(key)    for key in zz_dict.keys():      print(key)    </code></pre><p>—|—  </p><p>遍历values:  </p><pre><code>1  2  3  4  5  6  </code></pre><p>| </p><pre><code># 输出均为 1 2 3  for value in zz_dict.itervalues():      print(value)    for value in zz_dict.values():      print(value)    </code></pre><p>—|—  </p><p>遍历keys和values  </p><pre><code>1  2  3  4  5  6  </code></pre><p>| </p><pre><code># 输出为: x corresponds to 1 (其余两个也一样)  for key, value in zz_dict.iteritems():  # python3 没有iteritems      print(key, "corresponds to", value)    for key, value in zz_dict.items():      print(key, "corresponds to", value)    </code></pre><p>—|—  </p><h2 id="字符串"><a href="#字符串" class="headerlink" title="字符串"></a>字符串</h2><h3 id="判断字符串是否为字母或者数字"><a href="#判断字符串是否为字母或者数字" class="headerlink" title="判断字符串是否为字母或者数字"></a>判断字符串是否为字母或者数字</h3><p>str.isalnum() 字母或数字<br>str.isalpha() 字母<br>str.isdigit() 数字<br>str.isspace() 空白符, \t, \n, \r</p><p>isdigit() 和 isnumeric() 的区别  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  </code></pre><p>| </p><pre><code>num = "1"  #unicode  num.isdigit()   # True  num.isdecimal() # True  num.isnumeric() # True    num = "1" # 全角  num.isdigit()   # True  num.isdecimal() # True  num.isnumeric() # True    num = b"1" # byte  num.isdigit()   # True  num.isdecimal() # AttributeError 'bytes' object has no attribute 'isdecimal'  num.isnumeric() # AttributeError 'bytes' object has no attribute 'isnumeric'    num = "IV" # 罗马数字  num.isdigit()   # True  num.isdecimal() # False  num.isnumeric() # True    num = "四" # 汉字  num.isdigit()   # False  num.isdecimal() # False  num.isnumeric() # True    </code></pre><p>—|—  </p><hr><p>isdigit()<br>True: Unicode数字，byte数字（单字节），全角数字（双字节），罗马数字<br>False: 汉字数字<br>Error: 无</p><p>isdecimal()<br>True: Unicode数字，，全角数字（双字节）<br>False: 罗马数字，汉字数字<br>Error: byte数字（单字节）</p><p>isnumeric()<br>True: Unicode数字，全角数字（双字节），罗马数字，汉字数字<br>False: 无<br>Error: byte数字（单字节）</p><h3 id="str-rstrip"><a href="#str-rstrip" class="headerlink" title="str.rstrip()"></a>str.rstrip()</h3><p>参数:<br>chars: 指定删除的字符(默认为空格或换行符)</p><p>返回值:<br>返回删除指定字符后的新字符串</p><p>备注:<br>删除字符串末尾的指定字符(默认为空格或换行符)  </p><pre><code>1  </code></pre><p>| </p><pre><code>str.rstrip([chars])    </code></pre><p>—|—  </p><h3 id="str-strip"><a href="#str-strip" class="headerlink" title="str.strip()"></a>str.strip()</h3><p>参数<br>chars — 移除字符串头尾指定的字符序列。<br>返回值<br>返回移除字符串头尾指定的字符生成的新字符串。<br>备注:  </p><pre><code>1  </code></pre><p>| </p><pre><code>str.strip([chars])    </code></pre><p>—|—  </p><h3 id="str-split"><a href="#str-split" class="headerlink" title="str.split()"></a>str.split()</h3><p>参数</p><ul><li><p>str — 分隔符，默认为所有的空字符，包括空格、换行(\n)、制表符(\t)等。</p></li><li><p>num — 分割次数。默认为 -1, 即分隔所有。<br>返回值</p></li><li><p>返回分割后的字符串列表。</p><pre><code>1</code></pre></li></ul><p>| </p><pre><code>    str.split(str="", num=string.count(str)).        </code></pre><p>—|—  </p><h2 id="文件"><a href="#文件" class="headerlink" title="文件"></a>文件</h2><h2 id="reduce-函数"><a href="#reduce-函数" class="headerlink" title="reduce() 函数"></a>reduce() 函数</h2><p>reduce() 函数会对参数序列中元素进行累积。<br>函数将一个数据集合（链表，元组等）中的所有数据进行下列操作：用传给 reduce 中的函数 function（有两个参数）先对集合中的第 1、2 个元素进行操作，得到的结果再与第三个数据用 function 函数运算，最后得到一个结果。<br>reduce() 函数语法：  </p><pre><code>1  </code></pre><p>| </p><pre><code>reduce(function, iterable[, initializer])    </code></pre><p>—|—  </p><p>参数</p><ul><li><p>function — 函数，有两个参数</p></li><li><p>iterable — 可迭代对象</p></li><li><p>initializer — 可选，初始参数<br>返回值</p></li><li><p>返回函数计算结果<br>实例</p><pre><code>1  2  3  4  5  6  7</code></pre></li></ul><p>| </p><pre><code>    &gt;&gt;&gt;def add(x, y) :            # 两数相加      ...     return x + y      ...       &gt;&gt;&gt; reduce(add, [1,2,3,4,5])   # 计算列表和：1+2+3+4+5      15      &gt;&gt;&gt; reduce(lambda x, y: x+y, [1,2,3,4,5])  # 使用 lambda 匿名函数      15        </code></pre><p>—|—  </p><h2 id="zip-函数"><a href="#zip-函数" class="headerlink" title="zip() 函数"></a>zip() 函数</h2><p>zip() 函数用于将可迭代的对象作为参数, 将对象中对应的元素打包成一个个 <strong>元组</strong> ,然后返回有这些元组组成的 对象. ( 相比于python2中返回列表的方式, 这样做的好处是节约了不少的内存 )<br>可以用list()转换或者dict()转换将对象转换成相应的数据类型<br>如果各个迭代器的元素个数不一致, 则返回列表长度与最短的对象相同, 多出来的部分会被舍弃, 利用*号操作符, 可以将元组解压成列表.  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  </code></pre><p>| </p><pre><code>a = [1,2,3]  b = [4,5,6]  c = ['a','b','c','d','e','f']    zip_ab = zip(a,b)  print(zip_ab) # &lt;zip object at 0x104605348&gt;  print(dict(zip_ab)) # {1: 4, 2: 5, 3: 6}  # !!!注意, 一旦将zip_ab转换成dict以后, zip_ab内部就为空了!! 例如, 再次调用上面的语句:  print(dict(zip_ab)) # {}  # 但是zip_ab对象本身不会消失, 地址仍然不变  print(zip_ab) # &lt;zip object at 0x104605348&gt;    zip_abc = zip(a,b,c) # 注意, 三个元素的zip是不能转换成dict类型的  print(zip_abc) # &lt;zip object at 0x1046054c8&gt;  print(list(zip_abc)) # [(1, 4, 'a'), (2, 5, 'b'), (3, 6, 'c')]    zip_abc = zip(a,b,c)  z_a, z_b, z_c = zip(*zip_abc) # 利用zip(*)可以将zip对象重新解压, 返回类型是元组  print(z_a) # (1,2,3)  print(z_b) # (4,5,6)  print(z_c) # ('a','b','c')    </code></pre><p>—|—  </p><h2 id="getattr-函数"><a href="#getattr-函数" class="headerlink" title="getattr() 函数"></a>getattr() 函数</h2><p>getattr()函数用于返回一个对象的属性值, 语法如下  </p><pre><code>1  </code></pre><p>| </p><pre><code>getattr(object, name[, default])    </code></pre><p>—|—  </p><p>参数：</p><ul><li><p>object: 对象</p></li><li><p>name: 字符串, 对象属性</p></li><li><p>default: 默认返回值, 如果不提供该参数, 在没有对应属性时, 将触发Attributerror<br>实例</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13</code></pre></li></ul><p>| </p><pre><code>    &gt;&gt;&gt;class A(object):      ...     bar = 1      ...       &gt;&gt;&gt; a = A()      &gt;&gt;&gt; getattr(a, 'bar')        # 获取属性 bar 值      1      &gt;&gt;&gt; getattr(a, 'bar2')       # 属性 bar2 不存在，触发异常      Traceback (most recent call last):        File "&lt;stdin&gt;", line 1, in &lt;module&gt;      AttributeError: 'A' object has no attribute 'bar2'      &gt;&gt;&gt; getattr(a, 'bar2', 3)    # 属性 bar2 不存在，但设置了默认值      3      &gt;&gt;&gt;        </code></pre><p>—|—  </p><h2 id="dir-函数"><a href="#dir-函数" class="headerlink" title="dir() 函数"></a>dir() 函数</h2><p>可以查看某个类的所有方法和属性  </p><pre><code>1  </code></pre><p>| </p><pre><code>members = [attr for attr in dir(classA)]    </code></pre><p>—|—  </p><ul><li>_var: 在一个模块中以单下划线开头的变量和函数会被默认当做内部函数, 在使用from a_module import * 导入时, 这部分变量和函数不会被导入. 不过如果使用import a_module导入模块时, 仍然可以用a_module._var的形式访问该变量或函数</li><li>var_: 有时候, 一个变量的最适合的名称已经被另一个关键字所占用. 在这种情况下, 可以在名称的末尾附加一个下划线来解决冲突.</li><li>__var: 双下划线前缀会导致Python解释器重写属性名称, 以避免子类中的命名冲突. 举例来说, 如果在class Test中有一个成员__x, 那么当利用内置函数dir(Test)来查看类的属性时, 会发现__x被解释器重命名为_Test__x. 双下划线的名称修饰同样也适用于方法名称.</li><li><strong>var</strong>: 双下划线开头和结尾的是一些 Python 的特殊对象, 如类成员的 <strong>init</strong>, <strong>del</strong>, <strong>name</strong>, <strong>call</strong> 等. Python 官方推荐永远不要讲这样的命名方式应用于自己的变量或函数. 有一种说法是说双下划线建议为类的私有成员, 但是 PEP8 当前的官方版本中并没有明说.</li><li>_: 有时候我们会用一个独立的下划线作为一个名字, 这通常是用来指示某个变量时临时的或者无关紧要的.</li></ul><h2 id="类的特殊方法"><a href="#类的特殊方法" class="headerlink" title="类的特殊方法"></a>类的特殊方法</h2><h3 id="call"><a href="#call" class="headerlink" title="call()"></a>call()</h3><p>在 Python 中, 函数实际上也是一个对象:</p><pre><code>    1      2      3      </code></pre><p>| </p><pre><code>    f = abs      print(f.__name__) # 'abs'      print(f(-123)) # 123        </code></pre><p>—|—  </p><p>从上面可以看出, 函数是一个对象, 当它赋给另一个变量时, 该变量也是一个函数对象, 可以起到与原函数相同的效果. 在 Python 中, 一个类实例也可以变成一个可调用对象, 只需要实现一个特殊方法 <strong>call</strong>() 即可. 下面我们举例把 Person 类变成一个可调用对象:  </p><pre><code>1  2  3  4  5  6  7  8  9  </code></pre><p>| </p><pre><code>class Person(object):        def __init__(self, name, gender):          self.name = name          self.gender = gender        def __call__(self, friend):          print("name:", self.name)          print("friend:", friend)    </code></pre><p>—|—  </p><p>接下来我们就可以将 Person 类的实例对象当做一个函数来使用, 如下所示:  </p><pre><code>1  2  3  4  </code></pre><p>| </p><pre><code>p = Person('Bob', 'male')  p('Tim')  # name: Bob  # friend: Tim    </code></pre><p>—|—  </p><h3 id="getitem"><a href="#getitem" class="headerlink" title="getitem()"></a>getitem()</h3><p>凡是在类中定义了 <strong>getitem</strong>() 方法, 那么它的实例对象就是可以通过 [] 操作符来访问指定的成员或进行特定的行为, 大多数情况下会将该方法实现成通过索引来方法元素的形式.  </p><pre><code>1  2  3  4  5  6  7  </code></pre><p>| </p><pre><code>class DataBase(object):        def __init__(self):          super(DataBase, self).__init__()          self.vals = [1,2,3,4,5]      def __getitem__(self, key):          return self.vals[key]    </code></pre><p>—|—  </p><h3 id="setitem"><a href="#setitem" class="headerlink" title="setitem()"></a>setitem()</h3><p>使得可以通过 A[3] = 4, B[“a”] = 5 等方式来对类中的元素进行赋值</p><h3 id="file"><a href="#file" class="headerlink" title="file()"></a>file()</h3><p>查看模块的路径</p><h3 id="len"><a href="#len" class="headerlink" title="len()"></a>len()</h3><p>使得类对象可以使用 Python 的内建方法 len(), 返回你自定义的数值.  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  </code></pre><p>| </p><pre><code>class DictDemo:      def __init__(self,key,value):          self.dict = {}          self.dict[key] = value      def __getitem__(self,key):          return self.dict[key]      def __setitem__(self,key,value):          self.dict[key] = value      def __len__(self):          return len(self.dict)  dictDemo = DictDemo('key0','value0')  print(dictDemo['key0']) #value0  dictDemo['key1'] = 'value1'  print(dictDemo['key1']) #value1  print(len(dictDemo)) #2    </code></pre><p>—|—  </p><h3 id="repr"><a href="#repr" class="headerlink" title="repr()"></a>repr()</h3><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  </code></pre><p>| </p><pre><code>class Test(object):      def __init__(self, value='hello, world!'):          self.data = value    &gt;&gt;&gt; t = Test()  &gt;&gt;&gt; t  &lt;__main__.Test at 0x7fa91c307190&gt;  &gt;&gt;&gt; print t  &lt;__main__.Test object at 0x7fa91c307190&gt;    # 看到了么？上面打印类对象并不是很友好，显示的是对象的内存地址  # 下面我们重构下该类的__repr__以及__str__，看看它们俩有啥区别    # 重构__repr__  class TestRepr(Test):      def __repr__(self):          return 'TestRepr(%s)' % self.data    &gt;&gt;&gt; tr = TestRepr()  &gt;&gt;&gt; tr  TestRepr(hello, world!)  &gt;&gt;&gt; print tr  TestRepr(hello, world!)    # 重构__repr__方法后，不管直接输出对象还是通过print打印的信息都按我们__repr__方法中定义的格式进行显示了    # 重构__str__  calss TestStr(Test):      def __str__(self):          return '[Value: %s]' % self.data    &gt;&gt;&gt; ts = TestStr()  &gt;&gt;&gt; ts  &lt;__main__.TestStr at 0x7fa91c314e50&gt;  &gt;&gt;&gt; print ts  [Value: hello, world!]    # 你会发现，直接输出对象ts时并没有按我们__str__方法中定义的格式进行输出，而用print输出的信息却改变了    </code></pre><p>—|—  </p><h3 id="str"><a href="#str" class="headerlink" title="str()"></a>str()</h3><p>参见 <strong>repr</strong>() 代码示例</p><h2 id="星号"><a href="#星号" class="headerlink" title="星号 *"></a>星号 *</h2><p>*: 乘法<br>**: 乘幂</p><h3 id="用于函数参数"><a href="#用于函数参数" class="headerlink" title="用于函数参数"></a>用于函数参数</h3><p>单星号: 将所有参数以 元组(tuple) 的形式导入  </p><pre><code>1  2  3  4  5  6  </code></pre><p>| </p><pre><code>def foo(param1, *param2):      print(param1)      print(param2)  foo(1,2,3,4,5)  # 1  # (2,3,4,5)    </code></pre><p>—|—  </p><p>双星号: 将所有参数以 字典 的形式导入  </p><pre><code>1  2  3  4  5  6  </code></pre><p>| </p><pre><code>def bar(param1, **param2):      print(param1)      print(param2)  bar(1, a=2, b=3)  # 1  # {'a': 2, 'b': 3}    </code></pre><p>—|—  </p><p>当然这两个用法可以同时出现在一个函数中:  </p><pre><code>1  2  3  4  5  6  7  8  9  10  </code></pre><p>| </p><pre><code>def fun(a, b=10, *args, **kwargs):      print(a)      print(b)      print(args)      print(kwargs)  fun(1,2,3,4,e=5,f=6)  # 1  # 2  # (3,4)  # {'e': 5, 'f': 6}    </code></pre><p>—|—  </p><h2 id="globals-函数"><a href="#globals-函数" class="headerlink" title="globals() 函数"></a>globals() 函数</h2><p>该函数会以字典类型返回当前位置的全部全局变量</p><h2 id="stripe"><a href="#stripe" class="headerlink" title="stripe()"></a>stripe()</h2><h2 id="readlines"><a href="#readlines" class="headerlink" title="readlines()"></a>readlines()</h2><h2 id="lambda-函数"><a href="#lambda-函数" class="headerlink" title="lambda 函数"></a>lambda 函数</h2><h2 id="3-6新功能-f-string"><a href="#3-6新功能-f-string" class="headerlink" title="3.6新功能 f string"></a>3.6新功能 f string</h2><h2 id="包的导入机制"><a href="#包的导入机制" class="headerlink" title="包的导入机制"></a>包的导入机制</h2><h3 id="模块和包的定义"><a href="#模块和包的定义" class="headerlink" title="模块和包的定义"></a>模块和包的定义</h3><p>模块(module): 用来从逻辑上组织 Python 代码(变量, 函数, 类), 通常是一个.py文件.<br>包(package): 定义了一个由模块和子包组成的 Python 应用程序执行环境, 本质上就是一个有层次的文件目录结果(必须带有一个__init__.py文件)</p><h3 id="import-的搜索路径"><a href="#import-的搜索路径" class="headerlink" title="import 的搜索路径"></a>import 的搜索路径</h3><ol><li><p>在当前目录下搜索</p></li><li><p>在环境变量PYTHONPATH中指定的路径列表中搜索</p></li><li><p>在 Python 安装路径的lib库中搜索<br>Python 所有加载的模型信息都存放在sys.modules结构中, 当import一个模块时, 会按如下步骤来进行:</p></li><li><p>如果import A, 检查sys.modules中是否已经有A, 如果有则不加载, 如果没有则为A创建module对象, 并加载A;</p></li><li><p>如果是from A import B, 先为A创建module对象, 再解析A(此时会加载并执行A中的所有代码), 从中寻找B并填充到A的__dict__中.<br>在导入模块的时候, 模块所在文件夹会自动生成一个__pycache__/module_name.cpython-35.pyc的文件.</p><pre><code>1  2  3  4</code></pre></li></ol><p>| </p><pre><code>     import module_name的本质是将module_name.py中的全部代码加载到内存中, 并将其赋值给与模块同名的变量, 这个变量的类型是class&lt;module&gt;.       from module_name import name的本质是将指定的变量或者方法导入到当前的文件中       import package_name的本质是执行该包下的__init__.py文件, 在执行文件后, 会在package_name目录下生成一个__pycache__/__init__cpython-35.pyc文件.       from package_name import *的本质是导入__init__.py文件中的__all__列表(eg. __all__ = ['L2Norm', 'MultiBoxLoss']).         </code></pre><p>—|—  </p><h3 id="相对导入和绝对导入"><a href="#相对导入和绝对导入" class="headerlink" title="相对导入和绝对导入"></a>相对导入和绝对导入</h3><p>绝对导入:  </p><pre><code>1  2  </code></pre><p>| </p><pre><code>import A.B  from A import B    </code></pre><p>—|—  </p><p>相对导入:  </p><pre><code>1  2  </code></pre><p>| </p><pre><code>from . import B # . 代表当前路径  from ..A import B # .. 代表上层路径, ... 代表上上层路径.    </code></pre><p>—|—  </p><p>在没有明确指定包结构的情况下, Python 是根据__name__来决定一个模块在包中的结构的, 如果是__main__, 则它本身就是顶层模块, 没有包结构, 如果是A.B.C结构, 则A是顶层模块. Python 的导入方式的不同具有不同的规则:</p><p>1.如果是绝对导入, 一个模块只能导入自身的子模块或者和它的顶层模块同级别的模块及其子模块.<br>2.如果是相对导入, 一个模块必须有包结构且只能导入它的顶层模块内部的模块.</p><p>如果一个模块被直接运行, 则它自己为顶层模块, 不存在层次结构, 所以也找不到上层(..)的相对路径<br>Python2.x 默认为相对路径导入, 而 Python3.x 默认为绝对路径导入, 这样可以避免导入的子包覆盖掉标准库模块. 通常, 在 Python2.x 中, 我们利用下面的语句来使其导入规则遵循 Python3.x  </p><pre><code>1  </code></pre><p>| </p><pre><code>from __future__ import absolute_import    </code></pre><p>—|—  </p><p>absolute_import的意思并不是将所有的导入都视为绝对导入, 而是指禁用隐式相对导入(implicit relative import), 关于隐式的显示的具体区别, 可以看下面的例子, 假设有如下的包结构:  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  </code></pre><p>| </p><pre><code>thing  └── __init__.py  ├── books  │ ├── __init__.py  │ ├── adventure.py  │ ├── history.py  │ ├── horror.py  │ └── lovestory.py  ├── furniture  │ ├── __init__.py  │ ├── armchair.py  │ ├── bench.py  │ ├── screen.py  │ └── stool.py    </code></pre><p>—|—  </p><p>那么如果想在stool.py中导入bench模块, 则有如下几种方式:  </p><pre><code>1  2  3  </code></pre><p>| </p><pre><code>import bench # 隐式相对导入  from . import bench # 显式相对导入  from furniture import bench # 绝对导入    </code></pre><p>—|—  </p><p>隐式相对导入没有告诉解释器相对于谁进行导入, 默认相对于当前模块; 而显式相对导入则明确告诉了解释器相对于谁来导入. 以上导入方式的第三种是官方推荐的, 第一种是官方强烈不推荐的, Python3 中第一种导入方式只能用于导入sys.path中的模块.<br>**注意, 还有相对导入的模块不能被直接运行, 会提示如下错误:  </p><pre><code>1  2  3  4  </code></pre><p>| </p><pre><code>Traceback (most recent call last):    File "test.py", line 8, in &lt;module&gt;      from .ssd import SSD  ModuleNotFoundError: No module named '__main__.ssd'; '__main__' is not a package    </code></pre><p>—|—  </p><p>另外存在一种情况就是: 假如有两个模块a.py和b.py放在同一个目录下, 则可以直接在a.py中使用import b来导入模块b. 这是为什么呢? 我们上面说了在 Python3.x 中不能使用这种隐式相对导入, 但是这里却可以成功导入, 这是因为此时我们是直接运行a.py, 所以a.py和b.py的目录没有被当做一个包来处理, 因此不涉及相对导入和绝对导入的概念. 因此相对导入和绝对导入仅仅是针对于包而言的.</p><h3 id="综合距离"><a href="#综合距离" class="headerlink" title="综合距离"></a>综合距离</h3><p>存在目录结构如下所示:  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  </code></pre><p>| </p><pre><code>dirRoot  └── __init__.py  ├── file1.py  ├── file2.py  ├── dirA  │ ├── __init__.py  │ ├── a1.py  │ └── a2.py  ├── dirB  │ ├── __init__.py  │ ├── b1.py  │ └── b2.py    </code></pre><p>—|—  </p><p>直接运行a1.py, 并希望导入a2模块:  </p><pre><code>1  2  3  4  </code></pre><p>| </p><pre><code># a1.py  import a2 # 正确, 此时并未将 dirA 当做包来处理, a1.py 和 a2.py 相当于两个独立的模块  from a2 import func_a2 # 正确  from .a2 import func_a2 # 错误, 当进行相对导入时, 不能直接运行    </code></pre><p>—|—  </p><p>直接运行file1.py, 并希望导入a1模块, 同时a1模块中需要导入a2模块:  </p><pre><code>1  2  3  4  5  6  7  8  9  10  </code></pre><p>| </p><pre><code># file1.py  from dirA import a1  a1.func_a1() # a1.py 中的函数  a1.func_a2() # a1.py 中导入了 a2.py 的函数, 可以直接使用    # a1.py  import a2 # 错误, 此时由于 dirA 中有 __init__.py 文件, 因此会将 dirA 当做包来处理,  # 由于 Python3.x 不允许使用隐式的相对导入, 因此该语句非法  from a2 import func_a2 # 错误, 原因同上  from .a2 import func_a2 # 正确, 当进行相对导入时, 需要使用显式的相对导入    </code></pre><p>—|—  </p><p>直接运行file1.py, 并希望导入a1模块, 同时a1模块中需要导入dirB/b1模块(跨文件夹导入):  </p><pre><code>1  2  3  4  5  6  7  8  9  10  </code></pre><p>| </p><pre><code># file1.py  from dirA import a1  a1.func_a1() # a1.py 中的函数  a1.func_a2() # a2.py 中的函数  a1.func_b1() # b1.py 中的函数    # a1.py  from .a2 import func_a2 # 推荐使用绝对导入 from dirA.a1 import func_a2  from dirB import b1 # 由于运行的是 file1.py 文件, 因此顶层目录是 dirRoot  from dirB.b1 import func_b1 # 所以可以直接使用 dirB 包    </code></pre><p>—|—  </p><p>直接运行a1.py, 并希望跨目录的导入dirB/b1模块. 由于这种跨目录的导入超越了顶层路径的限制, 因此必须使用sys.path.append()方法来额外添加搜索路径, 否则无法正常导  </p><pre><code>1  2  3  4  5  6  </code></pre><p>| </p><pre><code># a1.py  import sys  sys.path.append("../") # 将 dirA 的上一次目录添加到搜索路径中  from dirB import b1 # 正确, 注意必须先添加 path, 然后再导入  from dirB.b1 import func_b1 # 正确  from .a2 import func_a2 # 这里是错误的, 当直接执行 a1.py 时, a1.py 中不能包含显式相对导入    </code></pre><p>—|—  </p><h2 id="获取-python-版本"><a href="#获取-python-版本" class="headerlink" title="获取 python 版本:"></a>获取 python 版本:</h2><pre><code>1  </code></pre><p>| </p><pre><code>print(sys.version_info)    </code></pre><p>—|—  </p><h2 id="获取包的安装位置"><a href="#获取包的安装位置" class="headerlink" title="获取包的安装位置"></a>获取包的安装位置</h2><pre><code>1  </code></pre><p>| </p><pre><code>print(cv2)    </code></pre><p>—|—  </p><h2 id="解析-xml-文件"><a href="#解析-xml-文件" class="headerlink" title="解析 xml 文件"></a>解析 xml 文件</h2><p>导入:  </p><pre><code>1  2  3  4  5  </code></pre><p>| </p><pre><code>import sys  if sys.version_info[0] == 2:      import xml.etree.cElementTree as ET  else:      import xml.etree.ElementTree as ET    </code></pre><p>—|—  </p><p>解析:  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  </code></pre><p>| </p><pre><code>xmlfile = ET.parse(xmlfile_path)  root = xmlfile.getroot() # 获取根节点  root.tag # 标签  root.attrib # 属性字典    for child in root: # 迭代访问子节点      print(child.tag, child.attrib)    # 可以通过索引访问嵌套节点的内容  root[0][1].text    Element.findall() #  Element.find() #    </code></pre><p>—|—  </p><h2 id="python-中-x3D-x3D-和-is-的区别"><a href="#python-中-x3D-x3D-和-is-的区别" class="headerlink" title="python 中 == 和 is 的区别"></a>python 中 == 和 is 的区别</h2><p>== 只用于判断值是否相等<br>is 用于判断两个对象是否为同一个实例<br>小整数对象池: Python 为了优化速度，使用了小整数对象池，避免为整数频繁申请和销毁内存空间。而Python 对小整数的定义是 [-5, 257)，只有数字在-5到256之间它们的id才会相等，超过了这个范围就不行了，同样的道理，字符串对象也有一个类似的缓冲池，超过区间范围内自然不会相等了</p><h2 id="队列-queue"><a href="#队列-queue" class="headerlink" title="队列 queue"></a>队列 queue</h2><p>在 Python3 中, 原来的Queue模块被重命名为queue, 该模块包含以下三类数据结构:</p><ul><li>queue.Queue(maxsize=0): FIFO queue, 先进先出队列, 代表普通队列</li><li>queue.LifoQueue(maxsize=0): LIFO queue, 后进先出队列, 类似栈的作用</li><li>queue.PriorityQueue(maxsize=0): 优先级队列, 类似堆的作用. 默认为小顶堆, 常用形式为元组:(priority_number, data)<br>上面的 maxsize 表明了队列中最大可以容纳的元素数量, 如果超过, 则无法插入. 当 maxsize &lt;= 0 时, 代表元素数量无限制.<br>公有方法(以上三个通用):</li><li>qsize(): 返回 approximate size, qsize() &gt; 0 不保证get()一定 work, 同理, qsize() &lt; maxsize 不保证put()一定 work.</li><li>empty(): 如果队列为空, 返回 True. 和qsize()一样, 不提供保证性.</li><li>full(): 如果队列满, 返回 True. 不提供保证性</li><li>put(item[, block[, timeout]])</li><li>put_nowait(item): 等价于put(item, False)</li><li>get([block[, timeout]])</li><li>get_nowait(): 等价于get(False)</li><li>task_done():</li><li>join():</li></ul><h2 id="堆-heapq"><a href="#堆-heapq" class="headerlink" title="堆 heapq"></a>堆 heapq</h2><p>heapq 模块只有最小堆的功能, 要实现最大堆, 需要在入堆和出堆的时候取反, 并且 heapq 模块只能作用于数值型类型.<br>最大堆: _heapify_max(), _heappop_max()</p><p>给定一组数据, 创建堆, 两种方式(二者等价):  </p><pre><code>1  2  3  4  5  6  7  </code></pre><p>| </p><pre><code>import heapq  data = [1,3,6,2,8,5]  heap = []  for d in data:      heapq.heappush(heap, n) # 方法一 逐个构建    heapq.heapify(data) # 方法二 原地构建, 效率更高    </code></pre><p>—|—  </p><p>小顶堆:  </p><pre><code>1  2  3  4  5  6  </code></pre><p>| </p><pre><code>heap = [1,3,6,2,8,5]  heapq.heapify(heap)  heapq.heappop(heap) # 返回并删除堆顶  heapq.heapreplace(heap, 10) # 删除堆顶并添加新值  heapq.heappushpop(heap, 10) # 先将新值加入堆中, 然后立刻弹出堆顶  print(heap[0]) # 查看堆顶    </code></pre><p>—|—  </p><p>大顶堆:  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  </code></pre><p>| </p><pre><code># 方法一: 取负值  heap = [-1,-3,-6,-2,-8,-5]    # 方法二: 内置方法  heap = [1,3,6,2,8,5]  heapq._heapify_max(heap) # max_heap  print(heap[0]) # 查看堆顶, 8  heapq._heappop_max(heap) # po from maxheap  print(heap[0]) # 6  heapq._heapreplace_max(heap, 10)  print(heap[0]) # 10  # heapq._heappushpop_max(heap, 10) # 注意, 没有 _heappushpop_max 函数    </code></pre><p>—|—  </p><h2 id="Python-刷题常用"><a href="#Python-刷题常用" class="headerlink" title="Python 刷题常用"></a>Python 刷题常用</h2><p>队列:  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  </code></pre><p>| </p><pre><code>import Queue  base_queue = Queue.Queue() # 基本队列, 先进先出  base_queue.put(x)  base_queue.get()    lifo_queue = Queue.LifoQueue() # 先进后出, 类似栈  lifo_queue.put(x)  lifo_queue.get()    prio_queue = Queue.PriorityQueue() # 优先队列, 与C++中priority_queue类似, 可实现堆的功能  prio_queue.put(x)  prio_queue.get()    </code></pre><p>—|—  </p><h2 id="numpy-中vstack-hstack-concatenate-和-stack-之间的区别和联系"><a href="#numpy-中vstack-hstack-concatenate-和-stack-之间的区别和联系" class="headerlink" title="numpy 中vstack, hstack, concatenate 和 stack 之间的区别和联系"></a>numpy 中vstack, hstack, concatenate 和 stack 之间的区别和联系</h2><h3 id="concatenate"><a href="#concatenate" class="headerlink" title="concatenate"></a>concatenate</h3><pre><code>1  </code></pre><p>| </p><pre><code>numpy.concatenate((a1, a2, ...), axis=0, out=None)    </code></pre><p>—|—  </p><p>concatenate 的作用就是将多个数组序列按照axis指定的维度连接起来, 这些数组序列 a1, a2, … 必须保证 除了 axis 指定维度之外的其他维度具有相同的 shape.</p><p>注意: 这里的维度指的是a1, a2的维度, 而不是(a1, a2)的维度</p><p>从维度角度来更好理解 concatenate 的作用<br>concatenate 执行后的 shape 特定是: axis 指定的维度是多个数组序列对应维度的数值和, 而其他维度保持不变. 也就是说不会增加新的维度, 这是 concatenate 与 stack 之间的一个重要的区别.</p><p>如下所示:  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  </code></pre><p>| </p><pre><code>import numpy as np  a1 = np.array([[1, 1], [2, 2], [3, 3]]) # shape = 3x2  a2 = np.array([[1, 1], [2, 2]]) # shape = 2 x 2  print(a1.shape, a2.shape)  concat1 = np.concatenate((a1, a2), axis=0)  print(concat1.shape) # shape 为 [5, 2], 在 0 维度上为 3+2, 其他维度保持不变  print(concat1) # a1, a2 维度 0 不同, 一个为 3, 一个为 2, 其他维度相同, 均为 2  #[[1 1]  # [2 2]  # [3 3]  # [1 1]  # [2 2]]  #print(np.concatenate((a1, a2), axis=1)) # 由于维度 0 二者不同, 无法保持不变, 因此报错    a1 = np.array([[1, 2, 3]]) # shape = 1x3  a2 = np.array([[1, 2]]) # shape = 1x2  print(a1.shape, a2.shape)  concat2 = np.concatenate((a1, a2), axis=1)  print(concat2.shape) # shape 为 [1, 5]在 1 维度上为 3 + 2, 0 维度上保持 1 不变  print(concat2)  # [[1 2 3 1 2]]  # print(np.concatenate((a1, a2), axis=0)) # 维度 1 不同, 报错    </code></pre><p>—|—  </p><p>有时候, concatenate的第一个参数只会传送一个一个数组序列, 这时候, 等价于将这个数组序列的第一维的元素看做是多个数组序列作为concatenate的参数进行传递. 如下所示:  </p><pre><code>1  2  3  4  5  6  7  8  9  </code></pre><p>| </p><pre><code>a = [[1, 2, 3], [1, 2, 3]]  print(np.concatenate(a, axis=0)) # 该行与下一行等价  print(np.concatenate((a[0], a[1]), axis=0))    a = [[1, 2, 3], [1, 2]]  print(np.concatenate(a, axis=0)) # 可以看出, 虽然 a 的第一维度为 2, 第二维度为 3 和 2    # 但是, 我们要将其拆分, 拆分后, a[0], a[1] 的第一维度3和2, 其他维度相同, 因此可以在第一维度上进行连接  print(np.concatenate((a[0], a[1]), axis=0))    </code></pre><p>—|—  </p><h3 id="stack"><a href="#stack" class="headerlink" title="stack"></a>stack</h3><pre><code>1  2  </code></pre><p>| </p><pre><code>numpy.stack(arrays, axis=0, out=None)  numpy.stack((a1, a2, ...), axis=0, out=None)    </code></pre><p>—|—  </p><p>stack 的作用就是将多个数组序列按照axis指定的维度 堆叠 起来, 这些数组序列 a1, a2, … 必须保证 所有维度都相同, 注意这里与 concatenate 的区别.</p><p>要更好的理解stack, 可以借助 维度 的概念进行理解, 对于 shape 相同的 k 个数组序列来说, stack 的作用相当于新插入一个维度, 维度的大小为 k, 插入的位置为axis指定的位置. 如下所示:  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  </code></pre><p>| </p><pre><code>a1 = [[1, 1], [2, 2], [3, 3]] # shape = 3x2  a2 = [[4, 4], [5, 5], [6, 6]] # shape = 3x2  a3 = [[7, 7], [8, 8], [9, 9]] # shape = 3x2  a4 = [[0, 0], [0, 0], [0, 0]] # shape = 3x2  stack1 = np.stack((a1, a2, a3, a4), axis=0) # 新插入维度大小为 4, 位置为第 0 维  print(stack1.shape) # shape 为 (4, 3, 2)  print('###\n', stack1) # 先将 shape 画好, 然后进行填充, 在第 0 维上进行堆叠, 因此 stack1[*][*] = a1[0], a1[1], ..., a4[2]    stack2 = np.stack((a1, a2, a3, a4), axis=1) # 新插入维度大小为 4, 位置为第 1 维  print(stack2.shape) # shape 为 (3, 4, 2)  print('###\n', stack2) # 在第 1 维上进行堆叠, 因此 stack2[*][*] = a1[0], a2[0], a3[0], a1[1], ...    stack3 = np.stack((a1, a2, a3, a4), axis=2) # 新插入维度大小为 4, 位置为第 2 维  print(stack3.shape) # shape 为 (3, 2, 4)  print('###\n', stack3) # 在第 2 维上进行堆叠, 因此 stack2[*][*] = [1 4 7 0], [1 4 7 0], [2 5 8 0], ...    </code></pre><p>—|—  </p><h3 id="hstack-和-vstack"><a href="#hstack-和-vstack" class="headerlink" title="hstack 和 vstack"></a>hstack 和 vstack</h3><p>hstack 和 vstack 虽然名字中都带有 stack, 但是实际上, 它们和np.stack的关系并不大, 一个明显的区别就是np.stack要求进行堆叠的多个数组序列需要保证 shape 完全相同, 并且堆叠后会新增加一个由axis指定的维度. 实际上, hstack 和 vstack 可以看做是特殊的 concatenate, 它们在某些情况下可以用 concatenate 来代替</p><p>既然 hstack 和 vstack 是特殊的 concatenate, 也就是说, 它们所接受的多个数组序列在axis指定的维度上可以不同, 而在其他维度上必须相同.</p><p>vstack: 在垂直方向上将多个数组序列进行堆叠, 相当于在axis=0维度上执行concatenate<br>hstack: 在水平方向上将多个数组序列进行堆叠, 相当于在axis=1维度上执行concatenate  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  </code></pre><p>| </p><pre><code>a = [[1, 1], [2, 2], [3, 3]] # shape = 3x2  b = [[4, 4], [5, 5], [6, 6]] # shape = 3x2  c = [[7, 7], [8, 8], [9, 9]] # shape = 3x2  d = [[0, 0], [0, 0], [0, 0]] # shape = 3x2    v = np.vstack((a, b, c, d))  print(v.shape) # (12, 2)  print(v)    x = np.concatenate((a, b, c, d), axis = 0) # 等价于 vstack  print(x.shape) # 12, 2  print(x)    h = np.hstack((a, b, c, d))  print(h.shape) # (3, 8)  print(h)    x = np.concatenate((a, b, c, d), axis = 1) # 等价于 hstack  print(x.shape) # 3, 8  print(x)    </code></pre><p>—|—  </p><p>需要特别注意, 当多个数组序列是一维数组时, 应该先将一维数组转换成二维数组, 然后才能与相应的 concatenate 进行等价. 这是因为, 在数组序列是一维数组时, concatenate 是无法使用axis=1的, 因此此时的 hstack 相当于是在axis=0上进行 concatenate, 而 vstack 则需要先将数组的 shape 从 (N,) 转换成 (1, N) 后才相当于是在axis=1上进行 concatenate</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  </code></pre><p>| </p><pre><code>a = np.array([1, 2, 3, 4, 5]) # 当面对的是一维数组时,  b = np.array([6, 7, 8, 9, 10])    h = np.hstack((a, b))  print(h.shape)  print(h)  con = np.concatenate((a, b), axis=0) # 当 a, b 是一维数组时, hstack 相当于在 axis=0 上进行连接  print(con.shape)  print(con)    v = np.vstack((a, b))  print(v.shape)  print(v)  con = np.concatenate(([a], [b]), axis=0) # 当 a, b 是一维数组时, vstack 相当于将 a, b 先转换成二维 (1, N), 然后在 axis=0 上进行连接  print(con.shape)  print(con)    </code></pre><p>—|—  </p><h2 id="set-去重"><a href="#set-去重" class="headerlink" title="set 去重"></a>set 去重</h2><p>对于二维列表, 由于 list 的元素也是 list, 在内存中存储的是首元素地址, 无法直接使用 set, 因此需要先将内部的元素全部全换成 tuple 后, 才能使用 list 去重. 如下所示  </p><pre><code>1  2  3  4  5  6  7  8  </code></pre><p>| </p><pre><code>a = list()  a.append([1,2,3])  a.append([1,2,3])  a.append([1,2,3])  a.append([4, 5, 6])  # b = set(a) # 报错  b = set(map(tuple, a))  print(b) # {(4, 5, 6), (1, 2, 3)}    </code></pre><p>—|—  </p><h2 id="os-sep用法"><a href="#os-sep用法" class="headerlink" title="os.sep用法"></a>os.sep用法</h2><p>ython是跨平台的。在Windows上，文件的路径分隔符是’\’，在Linux上是’/‘。</p><p>为了让代码在不同的平台上都能运行，那么路径应该写’\’还是’/‘呢？</p><p>使用os.sep的话，就不用考虑这个了，os.sep根据你所处的平台，自动采用相应的分隔符号。</p><p>举例</p><p>Linux下一个路径，/usr/share/python,那么上面的os.sep就是‘/’<br>windows下一个路径，C：\Users\Public\Desktop,那么上面的os.sep就是‘\’.  </p><pre><code>1  </code></pre><p>| </p><pre><code>data_dir = os.sep.join(['hello', 'world'])    </code></pre><p>—|—  </p><h2 id="Python3-元组"><a href="#Python3-元组" class="headerlink" title="Python3 元组"></a>Python3 元组</h2><p>Python元组包含了以下内置函数</p><ul><li><p>len(tuple) 计算元组元素个数。</p><pre><code>1  2  3  4</code></pre></li></ul><p>| </p><pre><code>    &gt;&gt;&gt; tuple1 = ('Google', 'Runoob', 'Taobao')      &gt;&gt;&gt; len(tuple1)      3      &gt;&gt;&gt;        </code></pre><p>—|—  </p><ul><li><p>max(tuple) 返回元组中元素最大值。</p><pre><code>1  2  3  4</code></pre></li></ul><p>| </p><pre><code>    &gt;&gt;&gt; tuple2 = ('5', '4', '8')      &gt;&gt;&gt; max(tuple2)      '8'      &gt;&gt;&gt;        </code></pre><p>—|—  </p><ul><li><p>min(tuple) 返回元组中元素最小值。</p><pre><code>1  2  3  4</code></pre></li></ul><p>| </p><pre><code>    &gt;&gt;&gt; tuple2 = ('5', '4', '8')      &gt;&gt;&gt; min(tuple2)      '4'      &gt;&gt;&gt;        </code></pre><p>—|—  </p><ul><li><p>tuple(seq) 将列表转换为元组。</p><pre><code>1  2  3  4</code></pre></li></ul><p>| </p><pre><code>    &gt;&gt;&gt; list1= ['Google', 'Taobao', 'Runoob', 'Baidu']      &gt;&gt;&gt; tuple1=tuple(list1)      &gt;&gt;&gt; tuple1      ('Google', 'Taobao', 'Runoob', 'Baidu')        </code></pre><p>—|—  </p><h2 id="序列化Python对象"><a href="#序列化Python对象" class="headerlink" title="序列化Python对象"></a>序列化Python对象</h2><p>你需要将一个Python对象序列化为一个字节流，以便将它保存到一个文件、存储到数据库或者通过网络传输它。<br>对于序列化最普遍的做法就是使用 pickle 模块。为了将一个对象保存到一个文件中，可以这样做</p><p>pickle 对于大型的数据结构比如使用 array 或 numpy 模块创建的二进制数组效率并不是一个高效的编码方式。 如果你需要移动大量的数组数据，你最好是先在一个文件中将其保存为数组数据块或使用更高级的标准编码方式如HDF5 (需要第三方库的支持)。  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  </code></pre><p>| </p><pre><code>In [1]: import pickle                                                               In [2]: obj = 123,"abcdef", ["ac", 123], {"key": "value", "key1": "value1"}         In [3]: print(obj)                                                                (123, 'abcdef', ['ac', 123], {'key': 'value', 'key1': 'value1'})    In [4]: # 序列化到文件                                                              In [5]: with open(r'./a.pickle','wb') as f:      ...:     pickle.dump(obj,f)      ...:                                                                             In [6]: with open(r'./a.pickle','rb') as f:      ...:     aa= pickle.load(f)      ...: print(aa)      ...:       ...:                                                                           (123, 'abcdef', ['ac', 123], {'key': 'value', 'key1': 'value1'})    </code></pre><p>—|—  </p><p>参考链接：<br><a href="https://hellozhaozheng.github.io/">https://hellozhaozheng.github.io</a><br><a href="https://www.runoob.com/python/python-tutorial.html">https://www.runoob.com/python/python-tutorial.html</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>shutil模块</title>
      <link href="/2019/08/28/shutil%E6%A8%A1%E5%9D%97/"/>
      <url>/2019/08/28/shutil%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<pre><code>1  2  3  4  5  6  7    </code></pre><p>| </p><pre><code>shutil.copyfile("old","new") 　　　　  # 复制文件，都只能是文件    shutil.copytree("old","new")　　　　 # 复制文件夹，都只能是目录，且new必须不存在    shutil.copy("old","new")　　　　       # 复制文件/文件夹，复制 old 为 new（new是文件，若不存在，即新建），复制 old 为至 new 文件夹（文件夹已存在）    shutil.move("old","new")  　　　　    # 移动文件/文件夹至 new 文件夹中    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>skimage模块</title>
      <link href="/2019/08/28/skimage%E6%A8%A1%E5%9D%97/"/>
      <url>/2019/08/28/skimage%E6%A8%A1%E5%9D%97/</url>
      
        <content type="html"><![CDATA[<p>比opencv的速度要慢很多, 但是使用起来更加简单, 真的对速度要求很高的话, 一般都会C++和opecv使用. 所以一般情况下, 首先看skimage能否实现, 不行的话再转用opencv  </p><pre><code>1  2  3  </code></pre><p>| </p><pre><code>import skimage  from skimage import io #  IO is a submodule. Submodules need to be imported from the parent module explicitly.  img = io.imread("1.jpg")    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>从上往下打印二叉树</title>
      <link href="/2019/08/28/%E4%BB%8E%E4%B8%8A%E5%BE%80%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91/"/>
      <url>/2019/08/28/%E4%BB%8E%E4%B8%8A%E5%BE%80%E4%B8%8B%E6%89%93%E5%8D%B0%E4%BA%8C%E5%8F%89%E6%A0%91/</url>
      
        <content type="html"><![CDATA[<p>时间限制：1秒 空间限制：32768K 热度指数：420679<br>本题知识点： 队列 树  </p><h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>从上往下打印出二叉树的每个节点，同层节点从左至右打印。</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  </code></pre><p>| </p><pre><code>/*  struct TreeNode {      int val;      struct TreeNode *left;      struct TreeNode *right;      TreeNode(int x) :              val(x), left(NULL), right(NULL) {      }  };*/  class Solution {  public:      vector&lt;int&gt; PrintFromTopToBottom(TreeNode* root) {          //队列是先进先出          queue&lt;TreeNode*&gt; que;          vector&lt;int&gt; vec;          que.push(root);//先将整个二叉树放入队列          while(!que.empty()) //当队列非空进行循环          {              TreeNode* p;              p = que.front();//先读取队列的首元素              que.pop();//弹出队列的首元素              if(p == NULL)                  continue;//所有元素存入vec后，由于队列中存放着空指针，依然进入循环，但此时p的值为NULL，不执行下面的操作，跳出循环结束              que.push(p-&gt;left);              que.push(p-&gt;right);              vec.push_back(p-&gt;val);          }          return vec;      }  };    </code></pre><p>—|—  </p><p>运行时间：3ms<br>占用内存：464k</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二维数组中的查找</title>
      <link href="/2019/08/27/20190827-%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E6%9F%A5%E6%89%BE/"/>
      <url>/2019/08/27/20190827-%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E6%9F%A5%E6%89%BE/</url>
      
        <content type="html"><![CDATA[<h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。</p><p>时间限制：1秒 空间限制：32768K 热度指数：1323359</p><h2 id="题目解析"><a href="#题目解析" class="headerlink" title="题目解析"></a>题目解析</h2><p>已知从左到右、从上导线都呈递增关系，令行row为0，令列col为第一行最后一位，然后将target与第一行最后一个数进行比较，若大于这个数则，行数++，若小于这个数，则列数向前递减。题目的要求是 当目标数target既不大于也不小于array[row][col]，认为target==array[row][col]，返回true。其他的情况则返回false。<br>注： 在 c++中获取vector数组的行数和列数的代码与java不一样</p><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">c++:</span><br><span class="line">int col = array[0].size()-1;</span><br><span class="line">int row = array.size();</span><br><span class="line">java:</span><br><span class="line">int col = array[0].length - 1;</span><br><span class="line">int array_len = array.length;</span><br></pre></td></tr></tbody></table></figure><p>c++代码 如下：  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">Solution</span> {</span><br><span class="line">public:</span><br><span class="line">    //行数：</span><br><span class="line">    //example.length</span><br><span class="line">    // 列数：</span><br><span class="line">    //example[<span class="number">0</span>].length    //第<span class="number">0</span>行的列数</span><br><span class="line">    <span class="built_in">bool</span> Find(<span class="built_in">int</span> target, vector&lt;vector&lt;<span class="built_in">int</span>&gt; &gt; array) {</span><br><span class="line">        <span class="keyword">if</span>(array.size()==<span class="number">0</span>)</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">return</span>  false;</span><br><span class="line">        }</span><br><span class="line">        <span class="built_in">int</span> row = <span class="number">0</span>;</span><br><span class="line">        <span class="built_in">int</span> col = array[<span class="number">0</span>].size()-<span class="number">1</span>;</span><br><span class="line">        <span class="built_in">int</span> array_len = array.size();</span><br><span class="line">        <span class="keyword">while</span>(row &lt;array_len &amp;&amp; col&gt;=<span class="number">0</span> )</span><br><span class="line">        {</span><br><span class="line">            <span class="keyword">if</span>(target&gt; array[row][col])</span><br><span class="line">                row++;</span><br><span class="line">            <span class="keyword">else</span> <span class="keyword">if</span>(target&lt; array[row][col])</span><br><span class="line">                col--;</span><br><span class="line">            <span class="keyword">else</span></span><br><span class="line">                <span class="keyword">return</span> true;</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">return</span> false;</span><br><span class="line">    }</span><br><span class="line">};</span><br></pre></td></tr></tbody></table></figure><p>运行时间：11ms<br>占用内存：1500k</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>二维数组中的查找</title>
      <link href="/2019/08/27/%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E6%9F%A5%E6%89%BE/"/>
      <url>/2019/08/27/%E4%BA%8C%E7%BB%B4%E6%95%B0%E7%BB%84%E4%B8%AD%E7%9A%84%E6%9F%A5%E6%89%BE/</url>
      
        <content type="html"><![CDATA[<h2 id="题目描述"><a href="#题目描述" class="headerlink" title="题目描述"></a>题目描述</h2><p>在一个二维数组中（每个一维数组的长度相同），每一行都按照从左到右递增的顺序排序，每一列都按照从上到下递增的顺序排序。请完成一个函数，输入这样的一个二维数组和一个整数，判断数组中是否含有该整数。</p><p>时间限制：1秒 空间限制：32768K 热度指数：1323359</p><h2 id="题目解析"><a href="#题目解析" class="headerlink" title="题目解析"></a>题目解析</h2><p>已知从左到右、从上导线都呈递增关系，令行row为0，令列col为第一行最后一位，然后将target与第一行最后一个数进行比较，若大于这个数则，行数++，若小于这个数，则列数向前递减。题目的要求是 当目标数target既不大于也不小于array[row][col]，认为target==array[row][col]，返回true。其他的情况则返回false。<br>注： 在 c++中获取vector数组的行数和列数的代码与java不一样</p><pre><code>1  2  3  4  5  6  </code></pre><p>| </p><pre><code>c++:  int col = array[0].size()-1;  int row = array.size();  java:  int col = array[0].length - 1;  int array_len = array.length;    </code></pre><p>—|—  </p><p>c++代码 如下：  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  </code></pre><p>| </p><pre><code>class Solution {  public:      //行数：      //example.length      // 列数：      //example[0].length    //第0行的列数      bool Find(int target, vector&lt;vector&lt;int&gt; &gt; array) {          if(array.size()==0)          {              return  false;          }          int row = 0;          int col = array[0].size()-1;          int array_len = array.size();          while(row &lt;array_len &amp;&amp; col&gt;=0 )          {              if(target&gt; array[row][col])                  row++;              else if(target&lt; array[row][col])                  col--;              else                   return true;          }          return false;              }  };    </code></pre><p>—|—  </p><p>运行时间：11ms<br>占用内存：1500k</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>仿射变换</title>
      <link href="/2019/08/26/20190826-%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2/"/>
      <url>/2019/08/26/20190826-%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2/</url>
      
        <content type="html"><![CDATA[<p>欧式变换包括平移和旋转变换</p><p>什么是仿射变换？ 简单的来说“仿射变换” 就是：“线性变换”+平移<br>尺度变换包括相似变换，当x与y变换的尺度相等的时候，叫做相似变换<br>什么是线性变换？</p><h2 id="1-线性变换"><a href="#1-线性变换" class="headerlink" title="1 线性变换"></a>1 线性变换</h2><hr><p>线性变换从几何直观有三个 要点：</p><ul><li>变换前是直线的，变换后依旧是直线</li><li>直线比例保持不变</li><li>变换前是原点，变换后依然是原点</li></ul><hr><h2 id="2-仿射变换"><a href="#2-仿射变换" class="headerlink" title="2 仿射变换"></a>2 仿射变换</h2><p>仿射变换从几何直观只有两个要点：</p><ul><li>变换前是直线的，变换后依然是直线</li><li>直线比例保持不变  </li><li></li></ul><p>与线性变换相比少了原点保持不变这一条</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>仿射变换</title>
      <link href="/2019/08/26/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2/"/>
      <url>/2019/08/26/%E4%BB%BF%E5%B0%84%E5%8F%98%E6%8D%A2/</url>
      
        <content type="html"><![CDATA[<p>欧式变换包括平移和旋转变换</p><p>什么是仿射变换？ 简单的来说“仿射变换” 就是：“线性变换”+平移<br>尺度变换包括相似变换，当x与y变换的尺度相等的时候，叫做相似变换<br>什么是线性变换？</p><h2 id="1-线性变换"><a href="#1-线性变换" class="headerlink" title="1 线性变换"></a>1 线性变换</h2><hr><p>线性变换从几何直观有三个 要点：</p><ul><li>变换前是直线的，变换后依旧是直线</li><li>直线比例保持不变</li><li>变换前是原点，变换后依然是原点</li></ul><hr><h2 id="2-仿射变换"><a href="#2-仿射变换" class="headerlink" title="2 仿射变换"></a>2 仿射变换</h2><p>仿射变换从几何直观只有两个要点：</p><ul><li>变换前是直线的，变换后依然是直线</li><li>直线比例保持不变  </li><li></li></ul><p>与线性变换相比少了原点保持不变这一条</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Matplotlib绘制六种可视化图表</title>
      <link href="/2019/08/24/20190824-pythonplot/"/>
      <url>/2019/08/24/20190824-pythonplot/</url>
      
        <content type="html"><![CDATA[<h2 id="01-折线图"><a href="#01-折线图" class="headerlink" title="01 折线图"></a>01 折线图</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x = np.linspace(<span class="number">0</span>,<span class="number">2</span>,<span class="number">100</span>)</span><br><span class="line">plt.plot(x, x, label=<span class="string">'linear'</span>)</span><br><span class="line">plt.plot(x, x**<span class="number">2</span>, label=<span class="string">'quadratic'</span>)</span><br><span class="line">plt.plot(x, x**<span class="number">3</span>, label=<span class="string">'cubic'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'x label'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'y label'</span>)</span><br><span class="line">plt.title(<span class="string">"Simple Plot"</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20190824_pythonplot_zhexian.png"></p><h2 id="02-散点图"><a href="#02-散点图" class="headerlink" title="02 散点图"></a>02 散点图</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">x = np.arange(<span class="number">0.</span>, <span class="number">5.</span>, <span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># 红色破折号, 蓝色方块 ，绿色三角块</span></span><br><span class="line">plt.plot(x, x, <span class="string">'r--'</span>, x, x**<span class="number">2</span>, <span class="string">'bs'</span>, x, x**<span class="number">3</span>, <span class="string">'g^'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20190824_pythonplot_sandian.png"></p><h2 id="03-直方图"><a href="#03-直方图" class="headerlink" title="03 直方图"></a>03 直方图</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">np.random.seed(<span class="number">19680801</span>)</span><br><span class="line">mu1, sigma1 = <span class="number">100</span>, <span class="number">15</span></span><br><span class="line">mu2, sigma2 = <span class="number">80</span>, <span class="number">15</span></span><br><span class="line">x1 = mu1 + sigma1 * np.random.randn(<span class="number">10000</span>)</span><br><span class="line">x2 = mu2 + sigma2 * np.random.randn(<span class="number">10000</span>)</span><br><span class="line"><span class="comment"># the histogram of the data</span></span><br><span class="line"><span class="comment"># 50：将数据分成50组</span></span><br><span class="line"><span class="comment"># facecolor：颜色；alpha：透明度</span></span><br><span class="line"><span class="comment"># density：是密度而不是具体数值</span></span><br><span class="line">n1, bins1, patches1 = plt.hist(x1, <span class="number">50</span>, density=<span class="literal">True</span>, facecolor=<span class="string">'g'</span>, alpha=<span class="number">1</span>)</span><br><span class="line">n2, bins2, patches2 = plt.hist(x2, <span class="number">50</span>, density=<span class="literal">True</span>, facecolor=<span class="string">'r'</span>, alpha=<span class="number">0.2</span>)</span><br><span class="line"><span class="comment"># n：概率值；bins：具体数值；patches：直方图对象。</span></span><br><span class="line">plt.xlabel(<span class="string">'Smarts'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Probability'</span>)</span><br><span class="line">plt.title(<span class="string">'Histogram of IQ'</span>)</span><br><span class="line">plt.text(<span class="number">110</span>, <span class="number">.025</span>, <span class="string">r'$\mu=100,\ \sigma=15$'</span>)</span><br><span class="line">plt.text(<span class="number">50</span>, <span class="number">.025</span>, <span class="string">r'$\mu=80,\ \sigma=15$'</span>)</span><br><span class="line"><span class="comment"># 设置x，y轴的具体范围</span></span><br><span class="line">plt.axis([<span class="number">40</span>, <span class="number">160</span>, <span class="number">0</span>, <span class="number">0.03</span>])</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20190824_pythonplot_zhifang.png"></p><h2 id="04-柱状图"><a href="#04-柱状图" class="headerlink" title="04 柱状图"></a>04 柱状图</h2><h3 id="4-1-并列柱状图"><a href="#4-1-并列柱状图" class="headerlink" title="4.1 并列柱状图"></a>4.1 并列柱状图</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">size = <span class="number">5</span></span><br><span class="line">a = np.random.random(size)</span><br><span class="line">b = np.random.random(size)</span><br><span class="line">c = np.random.random(size)</span><br><span class="line">x = np.arange(size)</span><br><span class="line"><span class="comment"># 有多少个类型，只需更改n即可</span></span><br><span class="line">total_width, n = <span class="number">0.8</span>, <span class="number">3</span></span><br><span class="line">width = total_width / n</span><br><span class="line"><span class="comment"># 重新拟定x的坐标</span></span><br><span class="line">x = x - (total_width - width) / <span class="number">2</span></span><br><span class="line"><span class="comment"># 这里使用的是偏移</span></span><br><span class="line">plt.bar(x, a,  width=width, label=<span class="string">'a'</span>)</span><br><span class="line">plt.bar(x + width, b, width=width, label=<span class="string">'b'</span>)</span><br><span class="line">plt.bar(x + <span class="number">2</span> * width, c, width=width, label=<span class="string">'c'</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20190824_pythonplot_binlie.png"></p><h3 id="4-2-叠加柱状图"><a href="#4-2-叠加柱状图" class="headerlink" title="4.2 叠加柱状图"></a>4.2 叠加柱状图</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">size = <span class="number">5</span></span><br><span class="line">a = np.random.random(size)</span><br><span class="line">b = np.random.random(size)</span><br><span class="line">c = np.random.random(size)</span><br><span class="line">x = np.arange(size)</span><br><span class="line"><span class="comment"># 这里使用的是偏移</span></span><br><span class="line">plt.bar(x, a, width=<span class="number">0.5</span>, label=<span class="string">'a'</span>,fc=<span class="string">'r'</span>)</span><br><span class="line">plt.bar(x, b, bottom=a, width=<span class="number">0.5</span>, label=<span class="string">'b'</span>, fc=<span class="string">'g'</span>)</span><br><span class="line">plt.bar(x, c, bottom=a+b, width=<span class="number">0.5</span>, label=<span class="string">'c'</span>, fc=<span class="string">'b'</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>, <span class="number">2.5</span>)</span><br><span class="line">plt.legend()</span><br><span class="line">plt.grid(<span class="literal">True</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20190824_pythonplot_diejia.png"></p><h2 id="05-饼图"><a href="#05-饼图" class="headerlink" title="05 饼图"></a>05 饼图</h2><h3 id="5-1-普通饼图"><a href="#5-1-普通饼图" class="headerlink" title="5.1 普通饼图"></a>5.1 普通饼图</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">labels = <span class="string">'Frogs'</span>, <span class="string">'Hogs'</span>, <span class="string">'Dogs'</span>, <span class="string">'Logs'</span></span><br><span class="line">sizes = [<span class="number">15</span>, <span class="number">30</span>, <span class="number">45</span>, <span class="number">10</span>]</span><br><span class="line"><span class="comment"># 设置分离的距离，0表示不分离</span></span><br><span class="line">explode = (<span class="number">0</span>, <span class="number">0.1</span>, <span class="number">0</span>, <span class="number">0</span>)</span><br><span class="line">plt.pie(sizes, explode=explode, labels=labels, autopct=<span class="string">'%1.1f%%'</span>,</span><br><span class="line">        shadow=<span class="literal">True</span>, startangle=<span class="number">90</span>)</span><br><span class="line"><span class="comment"># Equal aspect ratio 保证画出的图是正圆形</span></span><br><span class="line">plt.axis(<span class="string">'equal'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20190824_pythonplot_bingtu1.png"></p><h3 id="5-2-嵌套饼图"><a href="#5-2-嵌套饼图" class="headerlink" title="5.2 嵌套饼图"></a>5.2 嵌套饼图</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 设置每环的宽度</span></span><br><span class="line">size = <span class="number">0.3</span></span><br><span class="line">vals = np.array([[<span class="number">60.</span>, <span class="number">32.</span>], [<span class="number">37.</span>, <span class="number">40.</span>], [<span class="number">29.</span>, <span class="number">10.</span>]])</span><br><span class="line"><span class="comment"># 通过get_cmap随机获取颜色</span></span><br><span class="line">cmap = plt.get_cmap(<span class="string">"tab20c"</span>)</span><br><span class="line">outer_colors = cmap(np.arange(<span class="number">3</span>)*<span class="number">4</span>)</span><br><span class="line">inner_colors = cmap(np.array([<span class="number">1</span>, <span class="number">2</span>, <span class="number">5</span>, <span class="number">6</span>, <span class="number">9</span>, <span class="number">10</span>]))</span><br><span class="line"><span class="built_in">print</span>(vals.<span class="built_in">sum</span>(axis=<span class="number">1</span>))</span><br><span class="line"><span class="comment"># [92. 77. 39.]</span></span><br><span class="line">plt.pie(vals.<span class="built_in">sum</span>(axis=<span class="number">1</span>), radius=<span class="number">1</span>, colors=outer_colors,</span><br><span class="line">       wedgeprops=<span class="built_in">dict</span>(width=size, edgecolor=<span class="string">'w'</span>))</span><br><span class="line"><span class="built_in">print</span>(vals.flatten())</span><br><span class="line"><span class="comment"># [60. 32. 37. 40. 29. 10.]</span></span><br><span class="line">plt.pie(vals.flatten(), radius=<span class="number">1</span>-size, colors=inner_colors,</span><br><span class="line">       wedgeprops=<span class="built_in">dict</span>(width=size, edgecolor=<span class="string">'w'</span>))</span><br><span class="line"><span class="comment"># equal 使得为正圆</span></span><br><span class="line">plt.axis(<span class="string">'equal'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20190824_pythonplot_qiantaobingtu.png"></p><h3 id="5-3-极轴饼图"><a href="#5-3-极轴饼图" class="headerlink" title="5.3 极轴饼图"></a>5.3 极轴饼图</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">np.random.seed(<span class="number">19680801</span>)</span><br><span class="line">N = <span class="number">10</span></span><br><span class="line">theta = np.linspace(<span class="number">0.0</span>, <span class="number">2</span> * np.pi, N, endpoint=<span class="literal">False</span>)</span><br><span class="line">radii = <span class="number">10</span> * np.random.rand(N)</span><br><span class="line">width = np.pi / <span class="number">4</span> * np.random.rand(N)</span><br><span class="line">ax = plt.subplot(<span class="number">111</span>, projection=<span class="string">'polar'</span>)</span><br><span class="line">bars = ax.bar(theta, radii, width=width, bottom=<span class="number">0.0</span>)</span><br><span class="line"><span class="comment"># left表示从哪开始，</span></span><br><span class="line"><span class="comment"># radii表示从中心点向边缘绘制的长度（半径）</span></span><br><span class="line"><span class="comment"># width表示末端的弧长</span></span><br><span class="line"><span class="comment"># 自定义颜色和不透明度</span></span><br><span class="line"><span class="keyword">for</span> r, bar <span class="keyword">in</span> <span class="built_in">zip</span>(radii, bars):</span><br><span class="line">    bar.set_facecolor(plt.cm.viridis(r / <span class="number">10.</span>))</span><br><span class="line">    bar.set_alpha(<span class="number">0.5</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20190824_pythonplot_jizhou.png"></p><h2 id="06-三维图"><a href="#06-三维图" class="headerlink" title="06 三维图"></a>06 三维图</h2><h3 id="6-1-绘制三维散点图"><a href="#6-1-绘制三维散点图" class="headerlink" title="6.1 绘制三维散点图"></a>6.1 绘制三维散点图</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line">data = np.random.randint(<span class="number">0</span>, <span class="number">255</span>, size=[<span class="number">40</span>, <span class="number">40</span>, <span class="number">40</span>])</span><br><span class="line">x, y, z = data[<span class="number">0</span>], data[<span class="number">1</span>], data[<span class="number">2</span>]</span><br><span class="line">ax = plt.subplot(<span class="number">111</span>, projection=<span class="string">'3d'</span>)  <span class="comment"># 创建一个三维的绘图工程</span></span><br><span class="line"><span class="comment">#  将数据点分成三部分画，在颜色上有区分度</span></span><br><span class="line">ax.scatter(x[:<span class="number">10</span>], y[:<span class="number">10</span>], z[:<span class="number">10</span>], c=<span class="string">'y'</span>)  <span class="comment"># 绘制数据点</span></span><br><span class="line">ax.scatter(x[<span class="number">10</span>:<span class="number">20</span>], y[<span class="number">10</span>:<span class="number">20</span>], z[<span class="number">10</span>:<span class="number">20</span>], c=<span class="string">'r'</span>)</span><br><span class="line">ax.scatter(x[<span class="number">30</span>:<span class="number">40</span>], y[<span class="number">30</span>:<span class="number">40</span>], z[<span class="number">30</span>:<span class="number">40</span>], c=<span class="string">'g'</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">'Z'</span>)  <span class="comment"># 坐标轴</span></span><br><span class="line">ax.set_ylabel(<span class="string">'Y'</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'X'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20190824_pythonplot_sanwei.png"></p><h3 id="6-2-绘制三维平面图"><a href="#6-2-绘制三维平面图" class="headerlink" title="6.2 绘制三维平面图"></a>6.2 绘制三维平面图</h3><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line">X = np.arange(-<span class="number">4</span>, <span class="number">4</span>, <span class="number">0.25</span>)</span><br><span class="line">Y = np.arange(-<span class="number">4</span>, <span class="number">4</span>, <span class="number">0.25</span>)</span><br><span class="line">X, Y = np.meshgrid(X, Y)</span><br><span class="line">R = np.sqrt(X**<span class="number">2</span> + Y**<span class="number">2</span>)</span><br><span class="line">Z = np.sin(R)</span><br><span class="line"><span class="comment"># 具体函数方法可用 help(function) 查看，如：help(ax.plot_surface)</span></span><br><span class="line">ax.plot_surface(X, Y, Z, rstride=<span class="number">1</span>, cstride=<span class="number">1</span>, cmap=<span class="string">'rainbow'</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></tbody></table></figure><p><img src="/images/20190824_pythonplot_pingmian3d.png"></p><p>参考链接： <a href="https://mp.weixin.qq.com/s/bMvrle-FRvli0pRNi83waQ">https://mp.weixin.qq.com/s/bMvrle-FRvli0pRNi83waQ</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SSD源码解析</title>
      <link href="/2019/08/24/20190824-ssd/"/>
      <url>/2019/08/24/20190824-ssd/</url>
      
        <content type="html"><![CDATA[<p>论文地址: <a href="https://arxiv.org/pdf/1512.02325.pdf">SSD: Single Shot MultiBox Detector </a><br>非官方代码: <a href="https://github.com/amdegroot/ssd.pytorch">pytorch</a></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>SSD，全称Single Shot MultiBox Detector，是一种One-Stage的方法，它由Wei Liu在ECCV 2016上提出，SSD具有如下主要特点：</p><ul><li>从YOLO中继承了将detection转化为regression的思路，同时一次即可完成网络训练</li><li>基于Faster RCNN中的anchor，提出了相似的prior box</li><li>加入基于特征金字塔（Pyramidal Feature Hierarchy）的检测方式，相当于半个FPN思路</li></ul><h2 id="SSD的-网络结构"><a href="#SSD的-网络结构" class="headerlink" title="SSD的 网络结构"></a>SSD的 网络结构</h2><p><img src="/images/20190824_ssd_ssdarchitecture.png"></p><p>由上图可以看出，SSD的基础网络结构由基础网络VGG16组成，在VGG16基础网络之后接了一个3x3的卷积和一个1x1的卷积做特征融合，然后增加了一个Extra Feature Layers 层，这个层由八个卷积层构成。SSD在前面的基础网络去conv_4_3之后的relu层输出，以及倒数第二层的conv_7_1的relu再加上Extra Feature Layers层的第1,3,5,7层 共有6个 featuremap层，在此基础上对box进行预测。但感觉要提升效果的话可以对基础网络进行更改，增加特征融合等等。</p><h3 id="空洞卷积-Dilation-Conv"><a href="#空洞卷积-Dilation-Conv" class="headerlink" title="空洞卷积(Dilation Conv)"></a>空洞卷积(Dilation Conv)</h3><p>ssd网络里还使用了空洞卷积(Dilation Conv),采用VGG16做基础模型，首先VGG16是在ILSVRC CLS-LOC数据集预训练。然后借鉴了DeepLab-LargeFOV，分别将VGG16的全连接层fc6和fc7转换成 3×3卷积层 conv6和 1×1 卷积层conv7，同时将池化层 pool5 由原来的 stride=2 的 2×2 变成 stride=1 的(猜想是不想reduce特征图大小)，为了配合这种变化，采用了一种 Atrous Algorithm，其实就是conv6采用扩展卷积或带孔卷积（Dilation Conv），其在不增加参数与模型复杂度的条件下指数级扩大卷积的视野，其使用扩张率(dilation rate)参数，来表示扩张的大小，如下图所示，(a)是普通的 3×3 卷积，其视野就是 3×3 ，(b)是扩张率为 1，此时视野变成 7×7 ，(c)扩张率为3时，视野扩大为 15×15 ，但是视野的特征更稀疏了。Conv6采用 3×3 大小但dilation rate=6的扩展卷积。</p><h2 id="Prior-Box"><a href="#Prior-Box" class="headerlink" title="Prior Box"></a>Prior Box</h2><p>SSD中有着类似anchor机制的Prior Box机制，用于来生成先验框，后面将这些先验框与真实的gt进行匹配，然后与预测的进行回归。从而得到物体真实的 位置。<br>SSD的prior Box 按照如下规则生成：</p><ul><li><p>以feature map上每个点的中点为中心（offset=0.5），生成一些列同心的prior box（然后中心点的坐标会乘以step，相当于从feature map位置映射回原图位置）</p></li><li><p>正方形prior box最小边长为’’’pash $min_size$’’’，最大边长为：\sqrt{min_size*max_size}<br>E=mc^2</p></li><li><p>根据相应的aspect ratio，会生成不同个数的长方形 ，长宽为：$ \sqrt{aspect_ratio}<em>min_size$ 和 $1/ \sqrt{aspect_ratio}</em> min_size$ $f(x)=ax+b$</p></li><li><p>最终网络生成固定数量的Prior Box</p></li><li><p>每个feature map 对应prior box的min_size 和max_size 由以下的公式决定，公式中的m是使用feature map的数量(m=6)第一层feature map对应的min_size=S1，max_size=S2；第二层min_size=S2，max_size=S3；其他类推。在原文中，Smin=0.2，Smax=0.9<img src="/images/20190824_ssd_priorbox.png"></p></li></ul><p>| min_size | max_size | def.boxes num<br>—|—|—|—<br>conv4_3 | 30 | 60 | 4<br>fc7 | 60 | 111 | 6<br>conv6_2 | 111 | 162 | 6<br>fc7 | 162 | 213 | 6<br>conv4_3 | 213 | 264 | 4<br>fc7 | 264 | 315 | 4  </p><h2 id="训练策略"><a href="#训练策略" class="headerlink" title="训练策略"></a>训练策略</h2><h3 id="正负样本"><a href="#正负样本" class="headerlink" title="正负样本"></a>正负样本</h3><p>给定输入图像以及每个物体的Ground Truth,首先找到每个Ground True box对应的default box中IOU最大的最为正样本。然后,在剩下的default box中寻找与Ground Truth 的IOU大于0.5的default box作为正样本。一个Ground Truth可能对应多个正样本default box.其他的default box作为负样本。,为了保证样本尽量平衡,SSD采用了hard nagative mining,即对负样本进行抽样,抽样时按照置信度误差(预测背景的置信度越小,误差越大)进行奖序排列,选取误差较大的top-k作为训练的负样本,保证正负样本比例接近1:3。</p><h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p>目标函数为训练过程中的优化标准,目标函数也称损失函数,主要包括位置误差(localization loss,loc) 与置信度误差(confidence loss,conf,分类损失)的加权和,定义为：</p><h1 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h1><h2 id="基础模型定义"><a href="#基础模型定义" class="headerlink" title="基础模型定义"></a>基础模型定义</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># vgg([64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',</span></span><br><span class="line"><span class="comment">#             512, 512, 512], 3)</span></span><br><span class="line"><span class="comment"># This function is derived from torchvision VGG make_layers()</span></span><br><span class="line"><span class="comment"># https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">vgg</span>(<span class="params">cfg, i, batch_norm=<span class="literal">False</span></span>):</span><br><span class="line">    layers = []</span><br><span class="line">    in_channels = i</span><br><span class="line">    <span class="keyword">for</span> v <span class="keyword">in</span> cfg:</span><br><span class="line">        <span class="keyword">if</span> v == <span class="string">'M'</span>:</span><br><span class="line">            layers += [nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>)]</span><br><span class="line">        <span class="keyword">elif</span> v == <span class="string">'C'</span>:</span><br><span class="line">            layers += [nn.MaxPool2d(kernel_size=<span class="number">2</span>, stride=<span class="number">2</span>, ceil_mode=<span class="literal">True</span>)]</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            conv2d = nn.Conv2d(in_channels, v, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> batch_norm:</span><br><span class="line">                layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=<span class="literal">True</span>)]</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                layers += [conv2d, nn.ReLU(inplace=<span class="literal">True</span>)]</span><br><span class="line">            in_channels = v</span><br><span class="line">    pool5 = nn.MaxPool2d(kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>, padding=<span class="number">1</span>)</span><br><span class="line">    conv6 = nn.Conv2d(<span class="number">512</span>, <span class="number">1024</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">6</span>, dilation=<span class="number">6</span>)</span><br><span class="line">    conv7 = nn.Conv2d(<span class="number">1024</span>, <span class="number">1024</span>, kernel_size=<span class="number">1</span>)</span><br><span class="line">    layers += [pool5, conv6,</span><br><span class="line">               nn.ReLU(inplace=<span class="literal">True</span>), conv7, nn.ReLU(inplace=<span class="literal">True</span>)]</span><br><span class="line">    <span class="keyword">return</span> layers</span><br></pre></td></tr></tbody></table></figure><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">base = {</span><br><span class="line">    <span class="string">'300'</span>: [<span class="number">64</span>, <span class="number">64</span>, <span class="string">'M'</span>, <span class="number">128</span>, <span class="number">128</span>, <span class="string">'M'</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="number">256</span>, <span class="string">'C'</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>, <span class="string">'M'</span>,</span><br><span class="line">            <span class="number">512</span>, <span class="number">512</span>, <span class="number">512</span>],</span><br><span class="line">    <span class="string">'512'</span>: [],</span><br><span class="line">}</span><br><span class="line">extras = {</span><br><span class="line">    <span class="string">'300'</span>: [<span class="number">256</span>, <span class="string">'S'</span>, <span class="number">512</span>, <span class="number">128</span>, <span class="string">'S'</span>, <span class="number">256</span>, <span class="number">128</span>, <span class="number">256</span>, <span class="number">128</span>, <span class="number">256</span>],</span><br><span class="line">    <span class="string">'512'</span>: [],</span><br><span class="line">}</span><br><span class="line">mbox = {</span><br><span class="line">    <span class="string">'300'</span>: [<span class="number">4</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">6</span>, <span class="number">4</span>, <span class="number">4</span>],  <span class="comment"># number of boxes per feature map location</span></span><br><span class="line">    <span class="string">'512'</span>: [],</span><br><span class="line">}</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">add_extras</span>(<span class="params">cfg, i, batch_norm=<span class="literal">False</span></span>):</span><br><span class="line">    <span class="comment"># Extra layers added to VGG for feature scaling</span></span><br><span class="line">    layers =[]</span><br><span class="line">    in_channel = i</span><br><span class="line">    flag = <span class="literal">False</span></span><br><span class="line">    <span class="keyword">for</span> k,v <span class="keyword">in</span> <span class="built_in">enumerate</span>(cfg):</span><br><span class="line">        <span class="keyword">if</span> in_channel !=<span class="string">"S"</span>:</span><br><span class="line">            <span class="keyword">if</span> v == <span class="string">"S"</span>:</span><br><span class="line">                layers += [nn.Conv2d(in_channel,cfg[k+<span class="number">1</span>],</span><br><span class="line">                                     kernel_size=(<span class="number">1</span>,<span class="number">3</span>)[flag], stride=<span class="number">2</span>,padding=<span class="number">1</span>)]   <span class="comment"># flag 来控制卷积核是1 还是3</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                layers += [nn.Conv2d(in_channel, v, kernel_size=(<span class="number">1</span>,<span class="number">3</span>)[flag])]</span><br><span class="line">            flag = <span class="keyword">not</span> flag</span><br><span class="line">        in_channel = v</span><br><span class="line">    <span class="keyword">return</span> layers</span><br></pre></td></tr></tbody></table></figure><h2 id="模型Head部分的生成"><a href="#模型Head部分的生成" class="headerlink" title="模型Head部分的生成"></a>模型Head部分的生成</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cfg  [4, 6, 6, 6, 4, 4],  # number of boxes per feature map location</span></span><br><span class="line"><span class="keyword">def</span> <span class="title function_">multibox</span>(<span class="params">vgg, extra_layers, cfg, num_classes</span>):</span><br><span class="line">    loc_layers = []</span><br><span class="line">    conf_layers = []</span><br><span class="line">    vgg_source = [<span class="number">21</span>, -<span class="number">2</span>]</span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="built_in">enumerate</span>(vgg_source):</span><br><span class="line">        loc_layers += [nn.Conv2d(vgg[v].out_channels,</span><br><span class="line">                                 cfg[k] * <span class="number">4</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)]</span><br><span class="line">        conf_layers += [nn.Conv2d(vgg[v].out_channels,</span><br><span class="line">                        cfg[k] * num_classes, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)]</span><br><span class="line">    <span class="comment"># 对extra_layers中的（Conv2d-2_1、Conv2d-4_1、Conv2d-6_1、Conv2d-8_1）层通过卷积提取特征</span></span><br><span class="line">    <span class="keyword">for</span> k, v <span class="keyword">in</span> <span class="built_in">enumerate</span>(extra_layers[<span class="number">1</span>::<span class="number">2</span>], <span class="number">2</span>):</span><br><span class="line">        loc_layers += [nn.Conv2d(v.out_channels, cfg[k]</span><br><span class="line">                                 * <span class="number">4</span>, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)]</span><br><span class="line">        conf_layers += [nn.Conv2d(v.out_channels, cfg[k]</span><br><span class="line">                                  * num_classes, kernel_size=<span class="number">3</span>, padding=<span class="number">1</span>)]</span><br><span class="line">    <span class="keyword">return</span> vgg, extra_layers, (loc_layers, conf_layers)</span><br></pre></td></tr></tbody></table></figure><h2 id="模型先验框的生成"><a href="#模型先验框的生成" class="headerlink" title="模型先验框的生成"></a>模型先验框的生成</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">PriorBox</span>(<span class="title class_ inherited__">object</span>):</span><br><span class="line">    <span class="string">"""Compute priorbox coordinates in center-offset form for each source</span></span><br><span class="line"><span class="string">    feature map.</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, cfg</span>):</span><br><span class="line">        <span class="built_in">super</span>(PriorBox, self).__init__()</span><br><span class="line">        self.image_size = cfg[<span class="string">'min_dim'</span>]</span><br><span class="line">        <span class="comment"># number of priors for feature map location (either 4 or 6)</span></span><br><span class="line">        self.num_priors = <span class="built_in">len</span>(cfg[<span class="string">'aspect_ratios'</span>])</span><br><span class="line">        self.variance = cfg[<span class="string">'variance'</span>] <span class="keyword">or</span> [<span class="number">0.1</span>]</span><br><span class="line">        self.feature_maps = cfg[<span class="string">'feature_maps'</span>]</span><br><span class="line">        self.min_sizes = cfg[<span class="string">'min_sizes'</span>]</span><br><span class="line">        self.max_sizes = cfg[<span class="string">'max_sizes'</span>]</span><br><span class="line">        self.steps = cfg[<span class="string">'steps'</span>]</span><br><span class="line">        self.aspect_ratios = cfg[<span class="string">'aspect_ratios'</span>]</span><br><span class="line">        self.clip = cfg[<span class="string">'clip'</span>]</span><br><span class="line">        self.version = cfg[<span class="string">'name'</span>]</span><br><span class="line">        <span class="keyword">for</span> v <span class="keyword">in</span> self.variance:</span><br><span class="line">            <span class="keyword">if</span> v &lt;= <span class="number">0</span>:</span><br><span class="line">                <span class="keyword">raise</span> ValueError(<span class="string">'Variances must be greater than 0'</span>)</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self</span>):</span><br><span class="line">        mean = []</span><br><span class="line">        <span class="comment">#   'steps': [8, 16, 32, 64, 100, 300],</span></span><br><span class="line">        <span class="comment"># 'feature_maps': [38, 19, 10, 5, 3, 1],</span></span><br><span class="line">        <span class="comment">#  'min_sizes': [21, 45, 99, 153, 207, 261],</span></span><br><span class="line">        <span class="comment"># 'max_sizes': [45, 99, 153, 207, 261, 315],</span></span><br><span class="line">        <span class="keyword">for</span> k, f <span class="keyword">in</span> <span class="built_in">enumerate</span>(self.feature_maps):</span><br><span class="line">            <span class="keyword">for</span> i, j <span class="keyword">in</span> product(<span class="built_in">range</span>(f), repeat=<span class="number">2</span>):</span><br><span class="line">                f_k = self.image_size / self.steps[k]</span><br><span class="line">                <span class="comment"># unit center x,y</span></span><br><span class="line">                cx = (j + <span class="number">0.5</span>) / f_k</span><br><span class="line">                cy = (i + <span class="number">0.5</span>) / f_k</span><br><span class="line">                <span class="comment"># aspect_ratio: 1</span></span><br><span class="line">                <span class="comment"># rel size: min_size</span></span><br><span class="line">                s_k = self.min_sizes[k]/self.image_size</span><br><span class="line">                mean += [cx, cy, s_k, s_k]</span><br><span class="line">                <span class="comment"># aspect_ratio: 1</span></span><br><span class="line">                <span class="comment"># rel size: sqrt(s_k * s_(k+1))</span></span><br><span class="line">                s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))</span><br><span class="line">                mean += [cx, cy, s_k_prime, s_k_prime]</span><br><span class="line">                <span class="comment"># rest of aspect ratios</span></span><br><span class="line">                <span class="keyword">for</span> ar <span class="keyword">in</span> self.aspect_ratios[k]:</span><br><span class="line">                    mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]</span><br><span class="line">                    mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]</span><br><span class="line">        <span class="comment"># back to torch land</span></span><br><span class="line">        output = torch.Tensor(mean).view(-<span class="number">1</span>, <span class="number">4</span>)</span><br><span class="line">        <span class="keyword">if</span> self.clip:</span><br><span class="line">            output.clamp_(<span class="built_in">max</span>=<span class="number">1</span>, <span class="built_in">min</span>=<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></tbody></table></figure><h2 id="MultiBox-损失函数"><a href="#MultiBox-损失函数" class="headerlink" title="MultiBox 损失函数"></a>MultiBox 损失函数</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">MultiBoxLoss</span>(nn.Module):</span><br><span class="line">    <span class="string">"""SSD Weighted Loss Function</span></span><br><span class="line"><span class="string">    Compute Targets:</span></span><br><span class="line"><span class="string">        1) Produce Confidence Target Indices by matching  ground truth boxes</span></span><br><span class="line"><span class="string">           with (default) 'priorboxes' that have jaccard index &gt; threshold parameter</span></span><br><span class="line"><span class="string">           (default threshold: 0.5).</span></span><br><span class="line"><span class="string">        2) Produce localization target by 'encoding' variance into offsets of ground</span></span><br><span class="line"><span class="string">           truth boxes and their matched  'priorboxes'.</span></span><br><span class="line"><span class="string">        3) Hard negative mining to filter the excessive number of negative examples</span></span><br><span class="line"><span class="string">           that comes with using a large number of default bounding boxes.</span></span><br><span class="line"><span class="string">           (default negative:positive ratio 3:1)</span></span><br><span class="line"><span class="string">    Objective Loss:</span></span><br><span class="line"><span class="string">        L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N</span></span><br><span class="line"><span class="string">        Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss</span></span><br><span class="line"><span class="string">        weighted by α which is set to 1 by cross val.</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            c: class confidences,</span></span><br><span class="line"><span class="string">            l: predicted boxes,</span></span><br><span class="line"><span class="string">            g: ground truth boxes</span></span><br><span class="line"><span class="string">            N: number of matched default boxes</span></span><br><span class="line"><span class="string">        See: https://arxiv.org/pdf/1512.02325.pdf for more details.</span></span><br><span class="line"><span class="string">        # 计算目标:</span></span><br><span class="line"><span class="string">        # 输出那些与真实框的iou大于一定阈值的框的下标.</span></span><br><span class="line"><span class="string">        # 根据与真实框的偏移量输出localization目标</span></span><br><span class="line"><span class="string">        # 用难样例挖掘算法去除大量负样本(默认正负样本比例为1:3)</span></span><br><span class="line"><span class="string">        # 目标损失:</span></span><br><span class="line"><span class="string">        # L(x,c,l,g) = (Lconf(x,c) + αLloc(x,l,g)) / N</span></span><br><span class="line"><span class="string">        # 参数:</span></span><br><span class="line"><span class="string">        # c: 类别置信度(class confidences)</span></span><br><span class="line"><span class="string">        # l: 预测的框(predicted boxes)</span></span><br><span class="line"><span class="string">        # g: 真实框(ground truth boxes)</span></span><br><span class="line"><span class="string">        # N: 匹配到的框的数量(number of matched default boxes)</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"><span class="comment">#  MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5, False, args.cuda)</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, num_classes, overlap_thresh, prior_for_matching,</span></span><br><span class="line"><span class="params">                 bkg_label, neg_mining, neg_pos, neg_overlap, encode_target,</span></span><br><span class="line"><span class="params">                 use_gpu=<span class="literal">True</span></span>):</span><br><span class="line">        <span class="built_in">super</span>(MultiBoxLoss, self).__init__()</span><br><span class="line">        self.use_gpu = use_gpu</span><br><span class="line">        self.num_classes = num_classes  <span class="comment"># 列表数  21</span></span><br><span class="line">        self.threshold = overlap_thresh <span class="comment"># 交并比阈值, 0.5</span></span><br><span class="line">        self.background_label = bkg_label <span class="comment"># 背景标签, 0</span></span><br><span class="line">        self.encode_target = encode_target  <span class="comment"># True 没卵用</span></span><br><span class="line">        self.use_prior_for_matching = prior_for_matching  <span class="comment"># True, 没卵用</span></span><br><span class="line">        self.do_neg_mining = neg_mining  <span class="comment"># 负样本和正样本的比例, 3:1</span></span><br><span class="line">        self.negpos_ratio = neg_pos <span class="comment"># 0.5 判定负样本的阈值.</span></span><br><span class="line">        self.neg_overlap = neg_overlap  <span class="comment"># False 没卵用</span></span><br><span class="line">        self.variance = cfg[<span class="string">'variance'</span>]</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, predictions, targets</span>):</span><br><span class="line">        <span class="string">"""Multibox Loss</span></span><br><span class="line"><span class="string">        Args:</span></span><br><span class="line"><span class="string">            predictions (tuple): A tuple containing loc preds, conf preds,</span></span><br><span class="line"><span class="string">            and prior boxes from SSD net.</span></span><br><span class="line"><span class="string">                conf shape: torch.size(batch_size,num_priors,num_classes)</span></span><br><span class="line"><span class="string">                loc shape: torch.size(batch_size,num_priors,4)</span></span><br><span class="line"><span class="string">                priors shape: torch.size(num_priors,4)</span></span><br><span class="line"><span class="string">            targets (tensor): Ground truth boxes and labels for a batch,</span></span><br><span class="line"><span class="string">                shape: [batch_size,num_objs,5] (last idx is the label).</span></span><br><span class="line"><span class="string">        """</span></span><br><span class="line">        loc_data, conf_data, priors = predictions</span><br><span class="line">        <span class="comment"># loc_data: [batch_size, 8732, 4]</span></span><br><span class="line">        <span class="comment"># conf_data: [batch_size, 8732, 21]</span></span><br><span class="line">        <span class="comment"># priors: [8732, 4]  default box 对于任意的图片, 都是相同的, 因此无需带有 batch 维度</span></span><br><span class="line">        num = loc_data.size(<span class="number">0</span>) <span class="comment"># num = batch_size</span></span><br><span class="line">        priors = priors[:loc_data.size(<span class="number">1</span>), :] <span class="comment"># loc_data.size(1) = 8732, 因此 priors 维持不变</span></span><br><span class="line">        num_priors = (priors.size(<span class="number">0</span>)) <span class="comment"># num_priors = 8732</span></span><br><span class="line">        num_classes = self.num_classes <span class="comment"># num_classes = 21 (默认为voc数据集)</span></span><br><span class="line">        <span class="comment"># match priors (default boxes) and ground truth boxes</span></span><br><span class="line">        <span class="comment"># 将priors(default boxes)和ground truth boxes匹配</span></span><br><span class="line">        loc_t = torch.Tensor(num, num_priors, <span class="number">4</span>) <span class="comment"># shape:[batch_size, 8732, 4]</span></span><br><span class="line">        conf_t = torch.LongTensor(num, num_priors)  <span class="comment"># shape:[batch_size, 8732]</span></span><br><span class="line">        <span class="keyword">for</span> idx <span class="keyword">in</span> <span class="built_in">range</span>(num):</span><br><span class="line">            <span class="comment"># targets是列表, 列表的长度为batch_size, 列表中每个元素为一个 tensor,</span></span><br><span class="line">            <span class="comment"># 其 shape 为 [num_objs, 5], 其中 num_objs 为当前图片中物体的数量, 第二维前4个元素为边框坐标, 最后一个元素为类别编号(1~20)</span></span><br><span class="line">            truths = targets[idx][:, :-<span class="number">1</span>].data <span class="comment"># [num_objs, 4]</span></span><br><span class="line">            labels = targets[idx][:, -<span class="number">1</span>].data  <span class="comment"># [num_objs] 使用的是 -1, 而不是 -1:, 因此, 返回的维度变少了</span></span><br><span class="line">            defaults = priors.data <span class="comment"># [8732, 4]</span></span><br><span class="line">            <span class="comment"># from ..box_utils import match</span></span><br><span class="line">            <span class="comment"># 关键函数, 实现候选框与真实框之间的匹配, 注意是候选框而不是预测结果框! 这个函数实现较为复杂, 会在后面着重讲解</span></span><br><span class="line">            <span class="keyword">match</span>(self.threshold, truths, defaults, self.variance, labels,</span><br><span class="line">                  loc_t, conf_t,  )</span><br><span class="line">        <span class="keyword">if</span> self.use_gpu:</span><br><span class="line">            loc_t = loc_t.cuda()</span><br><span class="line">            conf_t = conf_t.cuda()</span><br><span class="line">        <span class="comment"># wrap targets</span></span><br><span class="line">        <span class="comment"># 用Variable封装loc_t, 新版本的 PyTorch 无需这么做, 只需要将 requires_grad 属性设置为 True 就行了</span></span><br><span class="line">        loc_t = Variable(loc_t, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        conf_t = Variable(conf_t, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        pos = conf_t &gt; <span class="number">0</span> <span class="comment"># 筛选出 &gt;0 的box下标(大部分都是=0的)</span></span><br><span class="line">        num_pos = pos.<span class="built_in">sum</span>(dim=<span class="number">1</span>, keepdim=<span class="literal">True</span>) <span class="comment"># 求和, 取得满足条件的box的数量, [batch_size, num_gt_threshold]</span></span><br><span class="line">        <span class="comment"># Localization Loss (Smooth L1)</span></span><br><span class="line">        <span class="comment"># Shape: [batch,num_priors,4]</span></span><br><span class="line">        <span class="comment"># 位置(localization)损失函数, 使用 Smooth L1 函数求损失</span></span><br><span class="line">        <span class="comment"># loc_data:[batch, num_priors, 4]</span></span><br><span class="line">        <span class="comment"># pos: [batch, num_priors]</span></span><br><span class="line">        <span class="comment"># pos_idx: [batch, num_priors, 4], 复制下标成坐标格式, 以便获取坐标值</span></span><br><span class="line">        pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)</span><br><span class="line">        loc_p = loc_data[pos_idx].view(-<span class="number">1</span>, <span class="number">4</span>) <span class="comment"># 获取预测结果值</span></span><br><span class="line">        loc_t = loc_t[pos_idx].view(-<span class="number">1</span>, <span class="number">4</span>)    <span class="comment"># 获取gt值</span></span><br><span class="line">        loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=<span class="literal">False</span>) <span class="comment"># 计算损失</span></span><br><span class="line">        <span class="comment"># Compute max conf across batch for hard negative mining</span></span><br><span class="line">        <span class="comment"># 计算最大的置信度, 以进行难负样本挖掘</span></span><br><span class="line">        <span class="comment"># conf_data: [batch, num_priors, num_classes]</span></span><br><span class="line">        <span class="comment"># batch_conf: [batch, num_priors, num_classes]</span></span><br><span class="line">        batch_conf = conf_data.view(-<span class="number">1</span>, self.num_classes)</span><br><span class="line">        <span class="comment"># conf_t: [batch, num_priors]</span></span><br><span class="line">        <span class="comment"># loss_c: [batch*num_priors, 1], 计算每个priorbox预测后的损失</span></span><br><span class="line">        loss_c = log_sum_exp(batch_conf) - batch_conf.gather(<span class="number">1</span>, conf_t.view(-<span class="number">1</span>, <span class="number">1</span>))</span><br><span class="line">        <span class="comment"># 难负样本挖掘, 按照loss进行排序, 取loss最大的负样本参与更新</span></span><br><span class="line">        <span class="comment"># Hard Negative Mining</span></span><br><span class="line">        loss_c[pos] = <span class="number">0</span>  <span class="comment"># filter out pos boxes for now # 将所有的pos下标的box的loss置为0(pos指示的是正样本的下标)</span></span><br><span class="line">        <span class="comment"># 将 loss_c 的shape 从 [batch*num_priors, 1] 转换成 [batch, num_priors]</span></span><br><span class="line">        loss_c = loss_c.view(num, -<span class="number">1</span>)  <span class="comment"># reshape</span></span><br><span class="line">        <span class="comment"># 进行降序排序, 并获取到排序的下标</span></span><br><span class="line">        _, loss_idx = loss_c.sort(<span class="number">1</span>, descending=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 将下标进行升序排序, 并获取到下标的下标</span></span><br><span class="line">        _, idx_rank = loss_idx.sort(<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># num_pos: [batch, 1], 统计每个样本中的obj个数</span></span><br><span class="line">        num_pos = pos.long().<span class="built_in">sum</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># 根据obj的个数, 确定负样本的个数(正样本的3倍)</span></span><br><span class="line">        num_neg = torch.clamp(self.negpos_ratio*num_pos, <span class="built_in">max</span>=pos.size(<span class="number">1</span>)-<span class="number">1</span>)</span><br><span class="line">        <span class="comment"># 获取到负样本的下标</span></span><br><span class="line">        neg = idx_rank &lt; num_neg.expand_as(idx_rank)</span><br><span class="line">        <span class="comment"># 计算包括正样本和负样本的置信度损失</span></span><br><span class="line">        <span class="comment"># pos: [batch, num_priors]</span></span><br><span class="line">        <span class="comment"># pos_idx: [batch, num_priors, num_classes]</span></span><br><span class="line">        pos_idx = pos.unsqueeze(<span class="number">2</span>).expand_as(conf_data)</span><br><span class="line">        <span class="comment"># neg: [batch, num_priors]</span></span><br><span class="line">        <span class="comment"># neg_idx: [batch, num_priors, num_classes]</span></span><br><span class="line">        neg_idx = neg.unsqueeze(<span class="number">2</span>).expand_as(conf_data)</span><br><span class="line">        <span class="comment"># 按照pos_idx和neg_idx指示的下标筛选参与计算损失的预测数据</span></span><br><span class="line">        conf_p = conf_data[(pos_idx+neg_idx).gt(<span class="number">0</span>)].view(-<span class="number">1</span>, self.num_classes)</span><br><span class="line">        <span class="comment"># 按照pos_idx和neg_idx筛选目标数据</span></span><br><span class="line">        targets_weighted = conf_t[(pos+neg).gt(<span class="number">0</span>)]</span><br><span class="line">        <span class="comment"># 计算二者的交叉熵</span></span><br><span class="line">        loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=<span class="literal">False</span>)</span><br><span class="line">        <span class="comment"># Sum of losses: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N</span></span><br><span class="line">        <span class="comment"># 将损失函数归一化后返回</span></span><br><span class="line">        N = num_pos.data.<span class="built_in">sum</span>()</span><br><span class="line">        loss_l /= N</span><br><span class="line">        loss_c /= N</span><br><span class="line">        <span class="keyword">return</span> loss_l, loss_c</span><br></pre></td></tr></tbody></table></figure><h2 id="match函数的解析"><a href="#match函数的解析" class="headerlink" title="match函数的解析"></a>match函数的解析</h2><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">match</span>(<span class="params">threshold, truths, priors, variances, labels, loc_t, conf_t, idx</span>):</span><br><span class="line">    <span class="comment"># threshold: (float) 确定是否匹配的交并比阈值</span></span><br><span class="line">    <span class="comment"># truths: (tensor: [num_obj, 4]) 存储真实 box 的边框坐标</span></span><br><span class="line">    <span class="comment"># priors: (tensor: [num_priors, 4], 即[8732, 4]), 存储推荐框的坐标, 注意, 此时的框是 default box, 而不是 SSD 网络预测出来的框的坐标, 预测的结果存储在 loc_data中, 其 shape 为[num_obj, 8732, 4].</span></span><br><span class="line">    <span class="comment"># variances: cfg['variance'], [0.1, 0.2], 用于将坐标转换成方便训练的形式(参考RCNN系列对边框坐标的处理)</span></span><br><span class="line">    <span class="comment"># labels: (tensor: [num_obj]), 代表了每个真实 box 对应的类别的编号</span></span><br><span class="line">    <span class="comment"># loc_t: (tensor: [batches, 8732, 4]),</span></span><br><span class="line">    <span class="comment"># conf_t: (tensor: [batches, 8732]),</span></span><br><span class="line">    <span class="comment"># idx: batches 中图片的序号, 标识当前正在处理的 image 在 batches 中的序号</span></span><br><span class="line">    <span class="comment"># jaccard index</span></span><br><span class="line">    overlaps = jaccard(    <span class="comment"># [A, B], 返回任意两个box之间的交并比, overlaps[i][j] 代表box_a中的第i个box与box_b中的第j个box之间的交并比.</span></span><br><span class="line">        truths,</span><br><span class="line">        point_form(priors)</span><br><span class="line">    )</span><br><span class="line">    <span class="comment"># 二部图匹配(Bipartite Matching)</span></span><br><span class="line">    <span class="comment"># [num_objs,1], 得到对于每个 gt box 来说的匹配度最高的 prior box, 前者存储交并比, 后者存储prior box在num_priors中的位置</span></span><br><span class="line">    best_prior_overlap, best_prior_idx = overlaps.<span class="built_in">max</span>(<span class="number">1</span>, keepdim=<span class="literal">True</span>)  <span class="comment"># keepdim=True, 因此shape为[num_objs,1]</span></span><br><span class="line">    <span class="comment"># [1,num_priors] best ground truth for each prior</span></span><br><span class="line">    <span class="comment"># [1, num_priors], 即[1,8732], 同理, 得到对于每个 prior box 来说的匹配度最高的 gt box</span></span><br><span class="line">    best_truth_overlap, best_truth_idx = overlaps.<span class="built_in">max</span>(<span class="number">0</span>, keepdim=<span class="literal">True</span>)</span><br><span class="line">    best_truth_idx.squeeze_(<span class="number">0</span>) <span class="comment"># 上面特意保留了维度(keepdim=True), 这里又都把维度 squeeze/reduce 了, 实际上只需用默认的 keepdim=False 就可以自动 squeeze/reduce 维度.</span></span><br><span class="line">    best_truth_overlap.squeeze_(<span class="number">0</span>)</span><br><span class="line">    best_prior_idx.squeeze_(<span class="number">1</span>)</span><br><span class="line">    best_prior_overlap.squeeze_(<span class="number">1</span>)</span><br><span class="line">    best_truth_overlap.index_fill_(<span class="number">0</span>, best_prior_idx, <span class="number">2</span>)  <span class="comment"># ensure best prior</span></span><br><span class="line">    <span class="comment"># 维度压缩后变为[num_priors], best_prior_idx 维度为[num_objs],</span></span><br><span class="line">    <span class="comment"># 该语句会将与gt box匹配度最好的prior box 的交并比置为 2, 确保其最大, 以免防止某些 gtbox 没有匹配的 priorbox.</span></span><br><span class="line">    <span class="comment"># 假想一种极端情况, 所有的priorbox与某个gtbox(标记为G)的交并比为1, 而其他gtbox分别有一个交并比</span></span><br><span class="line">    <span class="comment"># 最高的priorbox, 但是肯定小于1(因为其他的gtbox与G的交并比肯定小于1), 这样一来, 就会使得所有</span></span><br><span class="line">    <span class="comment"># 的priorbox都与G匹配, 为了防止这种情况, 我们将那些对gtbox来说, 具有最高交并比的priorbox,</span></span><br><span class="line">    <span class="comment"># 强制进行互相匹配, 即令best_truth_idx[best_prior_idx[j]] = j, 详细见下面的for循环</span></span><br><span class="line">    <span class="comment"># TODO refactor: index  best_prior_idx with long tensor</span></span><br><span class="line">    <span class="comment"># ensure every gt matches with its prior of max overlap</span></span><br><span class="line">    <span class="comment"># 注意!!: 因为 gt box 的数量要远远少于 prior box 的数量, 因此, 同一个 gt box 会与多个 prior box 匹配.</span></span><br><span class="line">    <span class="keyword">for</span> j <span class="keyword">in</span> <span class="built_in">range</span>(best_prior_idx.size(<span class="number">0</span>)):</span><br><span class="line">        best_truth_idx[best_prior_idx[j]] = j</span><br><span class="line">        <span class="comment"># best_prior_idx[j] 代表与box_a的第j个box交并比最高的 prior box 的下标, 将与该 gtbox</span></span><br><span class="line">        <span class="comment"># 匹配度最好的 prior box 的下标改为j, 由此,完成了该 gtbox 与第j个 prior box 的匹配.</span></span><br><span class="line">        <span class="comment"># 这里的循环只会进行num_obj次, 剩余的匹配为 best_truth_idx 中原本的值.</span></span><br><span class="line">        <span class="comment"># 这里处理的情况是, priorbox中第i个box与gtbox中第k个box的交并比最高,</span></span><br><span class="line">        <span class="comment"># 即 best_truth_idx[i]= k</span></span><br><span class="line">        <span class="comment"># 但是对于best_prior_idx[k]来说, 它却与priorbox的第l个box有着最高的交并比,</span></span><br><span class="line">        <span class="comment"># 即best_prior_idx[k]=l</span></span><br><span class="line">        <span class="comment"># 而对于gtbox的另一个边框gtbox[j]来说, 它与priorbox[i]的交并比最大,</span></span><br><span class="line">        <span class="comment"># 即但是对于best_prior_idx[j] = i.</span></span><br><span class="line">        <span class="comment"># 那么, 此时, 我们就应该将best_truth_idx[i]= k 修改成 best_truth_idx[i]= j.</span></span><br><span class="line">        <span class="comment"># 即令 priorbox[i] 与 gtbox[j]对应.</span></span><br><span class="line">        <span class="comment"># 这样做的原因: 防止某个gtbox没有匹配的 prior box.</span></span><br><span class="line">    matches = truths[best_truth_idx]          <span class="comment"># Shape: [num_priors,4]</span></span><br><span class="line">    <span class="comment"># truths 的shape 为[num_objs, 4], 而best_truth_idx是一个指示下标的列表, 列表长度为 8732,</span></span><br><span class="line">    <span class="comment"># 列表中的下标范围为0~num_objs-1, 代表的是与每个priorbox匹配的gtbox的下标</span></span><br><span class="line">    <span class="comment"># 上面的表达式会返回一个shape为 [num_priors, 4], 即 [8732, 4] 的tensor, 代表的就是与每个priorbox匹配的gtbox的坐标值.</span></span><br><span class="line">    conf = labels[best_truth_idx] + <span class="number">1</span>        <span class="comment"># 与上面的语句道理差不多, 这里得到的是每个prior box匹配的类别编号, shape 为[8732]</span></span><br><span class="line">    conf[best_truth_overlap &lt; threshold] = <span class="number">0</span> <span class="comment"># 将与gtbox的交并比小于阈值的置为0 , 即认为是非物体框</span></span><br><span class="line">    loc = encode(matches, priors, variances) <span class="comment"># 返回编码后的中心坐标和宽高.</span></span><br><span class="line">    loc_t[idx] = loc    <span class="comment"># [num_priors,4] encoded offsets to learn # 设置第idx张图片的gt编码坐标信息</span></span><br><span class="line">    conf_t[idx] = conf  <span class="comment"># [num_priors] top class label for each prior  设置第idx张图片的编号信息.(大于0即为物体编号, 认为有物体, 小于0认为是背景)</span></span><br></pre></td></tr></tbody></table></figure><p>参考文章：<a href="https://blog.csdn.net/happyday_d/article/details/86021993">https://blog.csdn.net/happyday_d/article/details/86021993</a><br><a href="https://hellozhaozheng.github.io/z_post/PyTorch-SSD">https://hellozhaozheng.github.io/z_post/PyTorch-SSD</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Matplotlib绘制六种可视化图表</title>
      <link href="/2019/08/24/pythonplot/"/>
      <url>/2019/08/24/pythonplot/</url>
      
        <content type="html"><![CDATA[<h2 id="01-折线图"><a href="#01-折线图" class="headerlink" title="01 折线图"></a>01 折线图</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  </code></pre><p>| </p><pre><code>import numpy as np   import matplotlib.pyplot as plt    x = np.linspace(0,2,100)    plt.plot(x, x, label='linear')  plt.plot(x, x**2, label='quadratic')  plt.plot(x, x**3, label='cubic')    plt.xlabel('x label')  plt.ylabel('y label')    plt.title("Simple Plot")    plt.legend()    plt.show()    </code></pre><p>—|—<br><img src="/2019/08/24/pythonplot/images/20190824_pythonplot_zhexian.png"></p><h2 id="02-散点图"><a href="#02-散点图" class="headerlink" title="02 散点图"></a>02 散点图</h2><pre><code>1  2  3  4  5  6  7  8  </code></pre><p>| </p><pre><code>import numpy as np  import matplotlib.pyplot as plt    x = np.arange(0., 5., 0.2)    # 红色破折号, 蓝色方块 ，绿色三角块  plt.plot(x, x, 'r--', x, x**2, 'bs', x, x**3, 'g^')  plt.show()    </code></pre><p>—|—<br><img src="/2019/08/24/pythonplot/images/20190824_pythonplot_sandian.png"></p><h2 id="03-直方图"><a href="#03-直方图" class="headerlink" title="03 直方图"></a>03 直方图</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  </code></pre><p>| </p><pre><code>import numpy as np  import matplotlib.pyplot as plt    np.random.seed(19680801)    mu1, sigma1 = 100, 15  mu2, sigma2 = 80, 15  x1 = mu1 + sigma1 * np.random.randn(10000)  x2 = mu2 + sigma2 * np.random.randn(10000)    # the histogram of the data  # 50：将数据分成50组  # facecolor：颜色；alpha：透明度  # density：是密度而不是具体数值  n1, bins1, patches1 = plt.hist(x1, 50, density=True, facecolor='g', alpha=1)  n2, bins2, patches2 = plt.hist(x2, 50, density=True, facecolor='r', alpha=0.2)    # n：概率值；bins：具体数值；patches：直方图对象。    plt.xlabel('Smarts')  plt.ylabel('Probability')  plt.title('Histogram of IQ')    plt.text(110, .025, r'$\mu=100,\ \sigma=15$')  plt.text(50, .025, r'$\mu=80,\ \sigma=15$')    # 设置x，y轴的具体范围  plt.axis([40, 160, 0, 0.03])  plt.grid(True)  plt.show()    </code></pre><p>—|—<br><img src="/2019/08/24/pythonplot/images/20190824_pythonplot_zhifang.png"></p><h2 id="04-柱状图"><a href="#04-柱状图" class="headerlink" title="04 柱状图"></a>04 柱状图</h2><h3 id="4-1-并列柱状图"><a href="#4-1-并列柱状图" class="headerlink" title="4.1 并列柱状图"></a>4.1 并列柱状图</h3><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  </code></pre><p>| </p><pre><code>import numpy as np  import matplotlib.pyplot as plt  size = 5  a = np.random.random(size)  b = np.random.random(size)  c = np.random.random(size)  x = np.arange(size)    # 有多少个类型，只需更改n即可  total_width, n = 0.8, 3       width = total_width / n    # 重新拟定x的坐标  x = x - (total_width - width) / 2    # 这里使用的是偏移  plt.bar(x, a,  width=width, label='a')  plt.bar(x + width, b, width=width, label='b')  plt.bar(x + 2 * width, c, width=width, label='c')  plt.legend()  plt.show()    </code></pre><p>—|—<br><img src="/2019/08/24/pythonplot/images/20190824_pythonplot_binlie.png"></p><h3 id="4-2-叠加柱状图"><a href="#4-2-叠加柱状图" class="headerlink" title="4.2 叠加柱状图"></a>4.2 叠加柱状图</h3><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  </code></pre><p>| </p><pre><code>import numpy as np  import matplotlib.pyplot as plt    size = 5  a = np.random.random(size)  b = np.random.random(size)  c = np.random.random(size)    x = np.arange(size)    # 这里使用的是偏移  plt.bar(x, a, width=0.5, label='a',fc='r')  plt.bar(x, b, bottom=a, width=0.5, label='b', fc='g')  plt.bar(x, c, bottom=a+b, width=0.5, label='c', fc='b')    plt.ylim(0, 2.5)  plt.legend()  plt.grid(True)  plt.show()    </code></pre><p>—|—<br><img src="/2019/08/24/pythonplot/images/20190824_pythonplot_diejia.png"></p><h2 id="05-饼图"><a href="#05-饼图" class="headerlink" title="05 饼图"></a>05 饼图</h2><h3 id="5-1-普通饼图"><a href="#5-1-普通饼图" class="headerlink" title="5.1 普通饼图"></a>5.1 普通饼图</h3><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  </code></pre><p>| </p><pre><code>import matplotlib.pyplot as plt    labels = 'Frogs', 'Hogs', 'Dogs', 'Logs'  sizes = [15, 30, 45, 10]    # 设置分离的距离，0表示不分离  explode = (0, 0.1, 0, 0)     plt.pie(sizes, explode=explode, labels=labels, autopct='%1.1f%%',          shadow=True, startangle=90)    # Equal aspect ratio 保证画出的图是正圆形  plt.axis('equal')     plt.show()    </code></pre><p>—|—<br><img src="/2019/08/24/pythonplot/images/20190824_pythonplot_bingtu1.png"></p><h3 id="5-2-嵌套饼图"><a href="#5-2-嵌套饼图" class="headerlink" title="5.2 嵌套饼图"></a>5.2 嵌套饼图</h3><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  </code></pre><p>| </p><pre><code>import numpy as np  import matplotlib.pyplot as plt    # 设置每环的宽度  size = 0.3  vals = np.array([[60., 32.], [37., 40.], [29., 10.]])    # 通过get_cmap随机获取颜色  cmap = plt.get_cmap("tab20c")  outer_colors = cmap(np.arange(3)*4)  inner_colors = cmap(np.array([1, 2, 5, 6, 9, 10]))    print(vals.sum(axis=1))  # [92. 77. 39.]    plt.pie(vals.sum(axis=1), radius=1, colors=outer_colors,         wedgeprops=dict(width=size, edgecolor='w'))  print(vals.flatten())  # [60. 32. 37. 40. 29. 10.]    plt.pie(vals.flatten(), radius=1-size, colors=inner_colors,         wedgeprops=dict(width=size, edgecolor='w'))    # equal 使得为正圆  plt.axis('equal')   plt.show()    </code></pre><p>—|—<br><img src="/2019/08/24/pythonplot/images/20190824_pythonplot_qiantaobingtu.png"></p><h3 id="5-3-极轴饼图"><a href="#5-3-极轴饼图" class="headerlink" title="5.3 极轴饼图"></a>5.3 极轴饼图</h3><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  </code></pre><p>| </p><pre><code>import numpy as np  import matplotlib.pyplot as plt    np.random.seed(19680801)    N = 10  theta = np.linspace(0.0, 2 * np.pi, N, endpoint=False)  radii = 10 * np.random.rand(N)  width = np.pi / 4 * np.random.rand(N)    ax = plt.subplot(111, projection='polar')  bars = ax.bar(theta, radii, width=width, bottom=0.0)  # left表示从哪开始，  # radii表示从中心点向边缘绘制的长度（半径）  # width表示末端的弧长    # 自定义颜色和不透明度  for r, bar in zip(radii, bars):      bar.set_facecolor(plt.cm.viridis(r / 10.))      bar.set_alpha(0.5)    plt.show()    </code></pre><p>—|—<br><img src="/2019/08/24/pythonplot/images/20190824_pythonplot_jizhou.png"></p><h2 id="06-三维图"><a href="#06-三维图" class="headerlink" title="06 三维图"></a>06 三维图</h2><h3 id="6-1-绘制三维散点图"><a href="#6-1-绘制三维散点图" class="headerlink" title="6.1 绘制三维散点图"></a>6.1 绘制三维散点图</h3><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  </code></pre><p>| </p><pre><code>import numpy as np  import matplotlib.pyplot as plt  from mpl_toolkits.mplot3d import Axes3D    data = np.random.randint(0, 255, size=[40, 40, 40])    x, y, z = data[0], data[1], data[2]  ax = plt.subplot(111, projection='3d')  # 创建一个三维的绘图工程  #  将数据点分成三部分画，在颜色上有区分度  ax.scatter(x[:10], y[:10], z[:10], c='y')  # 绘制数据点  ax.scatter(x[10:20], y[10:20], z[10:20], c='r')  ax.scatter(x[30:40], y[30:40], z[30:40], c='g')    ax.set_zlabel('Z')  # 坐标轴  ax.set_ylabel('Y')  ax.set_xlabel('X')  plt.show()    </code></pre><p>—|—<br><img src="/2019/08/24/pythonplot/images/20190824_pythonplot_sanwei.png"></p><h3 id="6-2-绘制三维平面图"><a href="#6-2-绘制三维平面图" class="headerlink" title="6.2 绘制三维平面图"></a>6.2 绘制三维平面图</h3><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  </code></pre><p>| </p><pre><code>from matplotlib import pyplot as plt  import numpy as np  from mpl_toolkits.mplot3d import Axes3D    fig = plt.figure()  ax = Axes3D(fig)  X = np.arange(-4, 4, 0.25)  Y = np.arange(-4, 4, 0.25)  X, Y = np.meshgrid(X, Y)  R = np.sqrt(X**2 + Y**2)  Z = np.sin(R)    # 具体函数方法可用 help(function) 查看，如：help(ax.plot_surface)  ax.plot_surface(X, Y, Z, rstride=1, cstride=1, cmap='rainbow')    plt.show()    </code></pre><p>—|—<br><img src="/2019/08/24/pythonplot/images/20190824_pythonplot_pingmian3d.png"></p><p>参考链接： <a href="https://mp.weixin.qq.com/s/bMvrle-FRvli0pRNi83waQ">https://mp.weixin.qq.com/s/bMvrle-FRvli0pRNi83waQ</a></p>]]></content>
      
      
      <categories>
          
          <category> Python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Python </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SSD源码解析</title>
      <link href="/2019/08/24/ssd/"/>
      <url>/2019/08/24/ssd/</url>
      
        <content type="html"><![CDATA[<p>论文地址: <a href="https://arxiv.org/pdf/1512.02325.pdf">SSD: Single Shot MultiBox Detector </a><br>非官方代码: <a href="https://github.com/amdegroot/ssd.pytorch">pytorch</a></p><h1 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h1><p>SSD，全称Single Shot MultiBox Detector，是一种One-Stage的方法，它由Wei Liu在ECCV 2016上提出，SSD具有如下主要特点：</p><ul><li>从YOLO中继承了将detection转化为regression的思路，同时一次即可完成网络训练</li><li>基于Faster RCNN中的anchor，提出了相似的prior box</li><li>加入基于特征金字塔（Pyramidal Feature Hierarchy）的检测方式，相当于半个FPN思路</li></ul><h2 id="SSD的-网络结构"><a href="#SSD的-网络结构" class="headerlink" title="SSD的 网络结构"></a>SSD的 网络结构</h2><p><img src="/2019/08/24/ssd/images/20190824_ssd_ssdarchitecture.png"></p><p>由上图可以看出，SSD的基础网络结构由基础网络VGG16组成，在VGG16基础网络之后接了一个3x3的卷积和一个1x1的卷积做特征融合，然后增加了一个Extra Feature Layers 层，这个层由八个卷积层构成。SSD在前面的基础网络去conv_4_3之后的relu层输出，以及倒数第二层的conv_7_1的relu再加上Extra Feature Layers层的第1,3,5,7层 共有6个 featuremap层，在此基础上对box进行预测。但感觉要提升效果的话可以对基础网络进行更改，增加特征融合等等。</p><h3 id="空洞卷积-Dilation-Conv"><a href="#空洞卷积-Dilation-Conv" class="headerlink" title="空洞卷积(Dilation Conv)"></a>空洞卷积(Dilation Conv)</h3><p>ssd网络里还使用了空洞卷积(Dilation Conv),采用VGG16做基础模型，首先VGG16是在ILSVRC CLS-LOC数据集预训练。然后借鉴了DeepLab-LargeFOV，分别将VGG16的全连接层fc6和fc7转换成 3×3卷积层 conv6和 1×1 卷积层conv7，同时将池化层 pool5 由原来的 stride=2 的 2×2 变成 stride=1 的(猜想是不想reduce特征图大小)，为了配合这种变化，采用了一种 Atrous Algorithm，其实就是conv6采用扩展卷积或带孔卷积（Dilation Conv），其在不增加参数与模型复杂度的条件下指数级扩大卷积的视野，其使用扩张率(dilation rate)参数，来表示扩张的大小，如下图所示，(a)是普通的 3×3 卷积，其视野就是 3×3 ，(b)是扩张率为 1，此时视野变成 7×7 ，(c)扩张率为3时，视野扩大为 15×15 ，但是视野的特征更稀疏了。Conv6采用 3×3 大小但dilation rate=6的扩展卷积。</p><h2 id="Prior-Box"><a href="#Prior-Box" class="headerlink" title="Prior Box"></a>Prior Box</h2><p>SSD中有着类似anchor机制的Prior Box机制，用于来生成先验框，后面将这些先验框与真实的gt进行匹配，然后与预测的进行回归。从而得到物体真实的 位置。<br>SSD的prior Box 按照如下规则生成：</p><ul><li><p>以feature map上每个点的中点为中心（offset=0.5），生成一些列同心的prior box（然后中心点的坐标会乘以step，相当于从feature map位置映射回原图位置）</p></li><li><p>正方形prior box最小边长为’’’pash $min_size$’’’，最大边长为：\sqrt{min_size*max_size}<br>E=mc^2</p></li><li><p>根据相应的aspect ratio，会生成不同个数的长方形 ，长宽为：$ \sqrt{aspect_ratio}<em>min_size$ 和 $1/ \sqrt{aspect_ratio}</em> min_size$ $f(x)=ax+b$</p></li><li><p>最终网络生成固定数量的Prior Box</p></li><li><p>每个feature map 对应prior box的min_size 和max_size 由以下的公式决定，公式中的m是使用feature map的数量(m=6)第一层feature map对应的min_size=S1，max_size=S2；第二层min_size=S2，max_size=S3；其他类推。在原文中，Smin=0.2，Smax=0.9<img src="/2019/08/24/ssd/images/20190824_ssd_priorbox.png"></p></li></ul><p>| min_size | max_size | def.boxes num<br>—|—|—|—<br>conv4_3 | 30 | 60 | 4<br>fc7 | 60 | 111 | 6<br>conv6_2 | 111 | 162 | 6<br>fc7 | 162 | 213 | 6<br>conv4_3 | 213 | 264 | 4<br>fc7 | 264 | 315 | 4  </p><h2 id="训练策略"><a href="#训练策略" class="headerlink" title="训练策略"></a>训练策略</h2><h3 id="正负样本"><a href="#正负样本" class="headerlink" title="正负样本"></a>正负样本</h3><p>给定输入图像以及每个物体的Ground Truth,首先找到每个Ground True box对应的default box中IOU最大的最为正样本。然后,在剩下的default box中寻找与Ground Truth 的IOU大于0.5的default box作为正样本。一个Ground Truth可能对应多个正样本default box.其他的default box作为负样本。,为了保证样本尽量平衡,SSD采用了hard nagative mining,即对负样本进行抽样,抽样时按照置信度误差(预测背景的置信度越小,误差越大)进行奖序排列,选取误差较大的top-k作为训练的负样本,保证正负样本比例接近1:3。</p><h3 id="目标函数"><a href="#目标函数" class="headerlink" title="目标函数"></a>目标函数</h3><p>目标函数为训练过程中的优化标准,目标函数也称损失函数,主要包括位置误差(localization loss,loc) 与置信度误差(confidence loss,conf,分类损失)的加权和,定义为：</p><h1 id="代码解析"><a href="#代码解析" class="headerlink" title="代码解析"></a>代码解析</h1><h2 id="基础模型定义"><a href="#基础模型定义" class="headerlink" title="基础模型定义"></a>基础模型定义</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  </code></pre><p>| </p><pre><code># vgg([64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',  #             512, 512, 512], 3)  # This function is derived from torchvision VGG make_layers()  # https://github.com/pytorch/vision/blob/master/torchvision/models/vgg.py  def vgg(cfg, i, batch_norm=False):      layers = []      in_channels = i      for v in cfg:          if v == 'M':              layers += [nn.MaxPool2d(kernel_size=2, stride=2)]          elif v == 'C':              layers += [nn.MaxPool2d(kernel_size=2, stride=2, ceil_mode=True)]          else:              conv2d = nn.Conv2d(in_channels, v, kernel_size=3, padding=1)              if batch_norm:                  layers += [conv2d, nn.BatchNorm2d(v), nn.ReLU(inplace=True)]              else:                  layers += [conv2d, nn.ReLU(inplace=True)]              in_channels = v      pool5 = nn.MaxPool2d(kernel_size=3, stride=1, padding=1)      conv6 = nn.Conv2d(512, 1024, kernel_size=3, padding=6, dilation=6)      conv7 = nn.Conv2d(1024, 1024, kernel_size=1)      layers += [pool5, conv6,                 nn.ReLU(inplace=True), conv7, nn.ReLU(inplace=True)]      return layers    </code></pre><p>—|—  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  </code></pre><p>| </p><pre><code>base = {      '300': [64, 64, 'M', 128, 128, 'M', 256, 256, 256, 'C', 512, 512, 512, 'M',              512, 512, 512],      '512': [],  }  extras = {      '300': [256, 'S', 512, 128, 'S', 256, 128, 256, 128, 256],      '512': [],  }  mbox = {      '300': [4, 6, 6, 6, 4, 4],  # number of boxes per feature map location      '512': [],  }    def add_extras(cfg, i, batch_norm=False):      # Extra layers added to VGG for feature scaling      layers =[]      in_channel = i      flag = False      for k,v in enumerate(cfg):          if in_channel !="S":              if v == "S":                  layers += [nn.Conv2d(in_channel,cfg[k+1],                                       kernel_size=(1,3)[flag], stride=2,padding=1)]   # flag 来控制卷积核是1 还是3              else:                  layers += [nn.Conv2d(in_channel, v, kernel_size=(1,3)[flag])]                flag = not flag            in_channel = v      return layers    </code></pre><p>—|—  </p><h2 id="模型Head部分的生成"><a href="#模型Head部分的生成" class="headerlink" title="模型Head部分的生成"></a>模型Head部分的生成</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  </code></pre><p>| </p><pre><code># cfg  [4, 6, 6, 6, 4, 4],  # number of boxes per feature map location  def multibox(vgg, extra_layers, cfg, num_classes):      loc_layers = []      conf_layers = []      vgg_source = [21, -2]      for k, v in enumerate(vgg_source):          loc_layers += [nn.Conv2d(vgg[v].out_channels,                                   cfg[k] * 4, kernel_size=3, padding=1)]          conf_layers += [nn.Conv2d(vgg[v].out_channels,                          cfg[k] * num_classes, kernel_size=3, padding=1)]      # 对extra_layers中的（Conv2d-2_1、Conv2d-4_1、Conv2d-6_1、Conv2d-8_1）层通过卷积提取特征      for k, v in enumerate(extra_layers[1::2], 2):          loc_layers += [nn.Conv2d(v.out_channels, cfg[k]                                   * 4, kernel_size=3, padding=1)]          conf_layers += [nn.Conv2d(v.out_channels, cfg[k]                                    * num_classes, kernel_size=3, padding=1)]      return vgg, extra_layers, (loc_layers, conf_layers)    </code></pre><p>—|—  </p><h2 id="模型先验框的生成"><a href="#模型先验框的生成" class="headerlink" title="模型先验框的生成"></a>模型先验框的生成</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  </code></pre><p>| </p><pre><code>class PriorBox(object):      """Compute priorbox coordinates in center-offset form for each source      feature map.      """      def __init__(self, cfg):          super(PriorBox, self).__init__()          self.image_size = cfg['min_dim']          # number of priors for feature map location (either 4 or 6)          self.num_priors = len(cfg['aspect_ratios'])          self.variance = cfg['variance'] or [0.1]          self.feature_maps = cfg['feature_maps']          self.min_sizes = cfg['min_sizes']          self.max_sizes = cfg['max_sizes']          self.steps = cfg['steps']          self.aspect_ratios = cfg['aspect_ratios']          self.clip = cfg['clip']          self.version = cfg['name']          for v in self.variance:              if v &lt;= 0:                  raise ValueError('Variances must be greater than 0')        def forward(self):          mean = []          #   'steps': [8, 16, 32, 64, 100, 300],          # 'feature_maps': [38, 19, 10, 5, 3, 1],          #  'min_sizes': [21, 45, 99, 153, 207, 261],          # 'max_sizes': [45, 99, 153, 207, 261, 315],          for k, f in enumerate(self.feature_maps):              for i, j in product(range(f), repeat=2):                  f_k = self.image_size / self.steps[k]                  # unit center x,y                  cx = (j + 0.5) / f_k                  cy = (i + 0.5) / f_k                    # aspect_ratio: 1                  # rel size: min_size                  s_k = self.min_sizes[k]/self.image_size                  mean += [cx, cy, s_k, s_k]                    # aspect_ratio: 1                  # rel size: sqrt(s_k * s_(k+1))                  s_k_prime = sqrt(s_k * (self.max_sizes[k]/self.image_size))                  mean += [cx, cy, s_k_prime, s_k_prime]                    # rest of aspect ratios                  for ar in self.aspect_ratios[k]:                      mean += [cx, cy, s_k*sqrt(ar), s_k/sqrt(ar)]                      mean += [cx, cy, s_k/sqrt(ar), s_k*sqrt(ar)]          # back to torch land          output = torch.Tensor(mean).view(-1, 4)          if self.clip:              output.clamp_(max=1, min=0)          return output    </code></pre><p>—|—  </p><h2 id="MultiBox-损失函数"><a href="#MultiBox-损失函数" class="headerlink" title="MultiBox 损失函数"></a>MultiBox 损失函数</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  </code></pre><p>| </p><pre><code>class MultiBoxLoss(nn.Module):      """SSD Weighted Loss Function      Compute Targets:          1) Produce Confidence Target Indices by matching  ground truth boxes             with (default) 'priorboxes' that have jaccard index &gt; threshold parameter             (default threshold: 0.5).          2) Produce localization target by 'encoding' variance into offsets of ground             truth boxes and their matched  'priorboxes'.          3) Hard negative mining to filter the excessive number of negative examples             that comes with using a large number of default bounding boxes.             (default negative:positive ratio 3:1)      Objective Loss:          L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N          Where, Lconf is the CrossEntropy Loss and Lloc is the SmoothL1 Loss          weighted by α which is set to 1 by cross val.          Args:              c: class confidences,              l: predicted boxes,              g: ground truth boxes              N: number of matched default boxes          See: https://arxiv.org/pdf/1512.02325.pdf for more details.              # 计算目标:          # 输出那些与真实框的iou大于一定阈值的框的下标.          # 根据与真实框的偏移量输出localization目标          # 用难样例挖掘算法去除大量负样本(默认正负样本比例为1:3)          # 目标损失:          # L(x,c,l,g) = (Lconf(x,c) + αLloc(x,l,g)) / N          # 参数:          # c: 类别置信度(class confidences)          # l: 预测的框(predicted boxes)          # g: 真实框(ground truth boxes)          # N: 匹配到的框的数量(number of matched default boxes)      """  #  MultiBoxLoss(cfg['num_classes'], 0.5, True, 0, True, 3, 0.5, False, args.cuda)      def __init__(self, num_classes, overlap_thresh, prior_for_matching,                   bkg_label, neg_mining, neg_pos, neg_overlap, encode_target,                   use_gpu=True):          super(MultiBoxLoss, self).__init__()          self.use_gpu = use_gpu          self.num_classes = num_classes  # 列表数  21           self.threshold = overlap_thresh # 交并比阈值, 0.5          self.background_label = bkg_label # 背景标签, 0          self.encode_target = encode_target  # True 没卵用          self.use_prior_for_matching = prior_for_matching  # True, 没卵用          self.do_neg_mining = neg_mining  # 负样本和正样本的比例, 3:1          self.negpos_ratio = neg_pos # 0.5 判定负样本的阈值.          self.neg_overlap = neg_overlap  # False 没卵用          self.variance = cfg['variance']        def forward(self, predictions, targets):          """Multibox Loss          Args:              predictions (tuple): A tuple containing loc preds, conf preds,              and prior boxes from SSD net.                  conf shape: torch.size(batch_size,num_priors,num_classes)                  loc shape: torch.size(batch_size,num_priors,4)                  priors shape: torch.size(num_priors,4)                targets (tensor): Ground truth boxes and labels for a batch,                  shape: [batch_size,num_objs,5] (last idx is the label).          """          loc_data, conf_data, priors = predictions          # loc_data: [batch_size, 8732, 4]          # conf_data: [batch_size, 8732, 21]          # priors: [8732, 4]  default box 对于任意的图片, 都是相同的, 因此无需带有 batch 维度          num = loc_data.size(0) # num = batch_size          priors = priors[:loc_data.size(1), :] # loc_data.size(1) = 8732, 因此 priors 维持不变          num_priors = (priors.size(0)) # num_priors = 8732          num_classes = self.num_classes # num_classes = 21 (默认为voc数据集)            # match priors (default boxes) and ground truth boxes          # 将priors(default boxes)和ground truth boxes匹配          loc_t = torch.Tensor(num, num_priors, 4) # shape:[batch_size, 8732, 4]          conf_t = torch.LongTensor(num, num_priors)  # shape:[batch_size, 8732]          for idx in range(num):              # targets是列表, 列表的长度为batch_size, 列表中每个元素为一个 tensor,              # 其 shape 为 [num_objs, 5], 其中 num_objs 为当前图片中物体的数量, 第二维前4个元素为边框坐标, 最后一个元素为类别编号(1~20)              truths = targets[idx][:, :-1].data # [num_objs, 4]              labels = targets[idx][:, -1].data  # [num_objs] 使用的是 -1, 而不是 -1:, 因此, 返回的维度变少了              defaults = priors.data # [8732, 4]              # from ..box_utils import match              # 关键函数, 实现候选框与真实框之间的匹配, 注意是候选框而不是预测结果框! 这个函数实现较为复杂, 会在后面着重讲解              match(self.threshold, truths, defaults, self.variance, labels,                    loc_t, conf_t,  )          if self.use_gpu:              loc_t = loc_t.cuda()              conf_t = conf_t.cuda()          # wrap targets          # 用Variable封装loc_t, 新版本的 PyTorch 无需这么做, 只需要将 requires_grad 属性设置为 True 就行了          loc_t = Variable(loc_t, requires_grad=False)          conf_t = Variable(conf_t, requires_grad=False)            pos = conf_t &gt; 0 # 筛选出 &gt;0 的box下标(大部分都是=0的)          num_pos = pos.sum(dim=1, keepdim=True) # 求和, 取得满足条件的box的数量, [batch_size, num_gt_threshold]            # Localization Loss (Smooth L1)          # Shape: [batch,num_priors,4]          # 位置(localization)损失函数, 使用 Smooth L1 函数求损失          # loc_data:[batch, num_priors, 4]          # pos: [batch, num_priors]          # pos_idx: [batch, num_priors, 4], 复制下标成坐标格式, 以便获取坐标值          pos_idx = pos.unsqueeze(pos.dim()).expand_as(loc_data)          loc_p = loc_data[pos_idx].view(-1, 4) # 获取预测结果值          loc_t = loc_t[pos_idx].view(-1, 4)    # 获取gt值          loss_l = F.smooth_l1_loss(loc_p, loc_t, size_average=False) # 计算损失            # Compute max conf across batch for hard negative mining          # 计算最大的置信度, 以进行难负样本挖掘          # conf_data: [batch, num_priors, num_classes]          # batch_conf: [batch, num_priors, num_classes]          batch_conf = conf_data.view(-1, self.num_classes)            # conf_t: [batch, num_priors]          # loss_c: [batch*num_priors, 1], 计算每个priorbox预测后的损失          loss_c = log_sum_exp(batch_conf) - batch_conf.gather(1, conf_t.view(-1, 1))                    # 难负样本挖掘, 按照loss进行排序, 取loss最大的负样本参与更新          # Hard Negative Mining          loss_c[pos] = 0  # filter out pos boxes for now # 将所有的pos下标的box的loss置为0(pos指示的是正样本的下标)          # 将 loss_c 的shape 从 [batch*num_priors, 1] 转换成 [batch, num_priors]          loss_c = loss_c.view(num, -1)  # reshape          # 进行降序排序, 并获取到排序的下标          _, loss_idx = loss_c.sort(1, descending=True)          # 将下标进行升序排序, 并获取到下标的下标          _, idx_rank = loss_idx.sort(1)          # num_pos: [batch, 1], 统计每个样本中的obj个数          num_pos = pos.long().sum(1, keepdim=True)          # 根据obj的个数, 确定负样本的个数(正样本的3倍)          num_neg = torch.clamp(self.negpos_ratio*num_pos, max=pos.size(1)-1)          # 获取到负样本的下标          neg = idx_rank &lt; num_neg.expand_as(idx_rank)                    # 计算包括正样本和负样本的置信度损失          # pos: [batch, num_priors]          # pos_idx: [batch, num_priors, num_classes]          pos_idx = pos.unsqueeze(2).expand_as(conf_data)            # neg: [batch, num_priors]          # neg_idx: [batch, num_priors, num_classes]          neg_idx = neg.unsqueeze(2).expand_as(conf_data)          # 按照pos_idx和neg_idx指示的下标筛选参与计算损失的预测数据          conf_p = conf_data[(pos_idx+neg_idx).gt(0)].view(-1, self.num_classes)          # 按照pos_idx和neg_idx筛选目标数据          targets_weighted = conf_t[(pos+neg).gt(0)]          # 计算二者的交叉熵          loss_c = F.cross_entropy(conf_p, targets_weighted, size_average=False)            # Sum of losses: L(x,c,l,g) = (Lconf(x, c) + αLloc(x,l,g)) / N          # 将损失函数归一化后返回          N = num_pos.data.sum()          loss_l /= N          loss_c /= N          return loss_l, loss_c    </code></pre><p>—|—  </p><h2 id="match函数的解析"><a href="#match函数的解析" class="headerlink" title="match函数的解析"></a>match函数的解析</h2><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  </code></pre><p>| </p><pre><code>def match(threshold, truths, priors, variances, labels, loc_t, conf_t, idx):      # threshold: (float) 确定是否匹配的交并比阈值      # truths: (tensor: [num_obj, 4]) 存储真实 box 的边框坐标      # priors: (tensor: [num_priors, 4], 即[8732, 4]), 存储推荐框的坐标, 注意, 此时的框是 default box, 而不是 SSD 网络预测出来的框的坐标, 预测的结果存储在 loc_data中, 其 shape 为[num_obj, 8732, 4].      # variances: cfg['variance'], [0.1, 0.2], 用于将坐标转换成方便训练的形式(参考RCNN系列对边框坐标的处理)      # labels: (tensor: [num_obj]), 代表了每个真实 box 对应的类别的编号      # loc_t: (tensor: [batches, 8732, 4]),      # conf_t: (tensor: [batches, 8732]),      # idx: batches 中图片的序号, 标识当前正在处理的 image 在 batches 中的序号        # jaccard index      overlaps = jaccard(    # [A, B], 返回任意两个box之间的交并比, overlaps[i][j] 代表box_a中的第i个box与box_b中的第j个box之间的交并比.          truths,          point_form(priors)      )        # 二部图匹配(Bipartite Matching)      # [num_objs,1], 得到对于每个 gt box 来说的匹配度最高的 prior box, 前者存储交并比, 后者存储prior box在num_priors中的位置      best_prior_overlap, best_prior_idx = overlaps.max(1, keepdim=True)  # keepdim=True, 因此shape为[num_objs,1]      # [1,num_priors] best ground truth for each prior      # [1, num_priors], 即[1,8732], 同理, 得到对于每个 prior box 来说的匹配度最高的 gt box      best_truth_overlap, best_truth_idx = overlaps.max(0, keepdim=True)      best_truth_idx.squeeze_(0) # 上面特意保留了维度(keepdim=True), 这里又都把维度 squeeze/reduce 了, 实际上只需用默认的 keepdim=False 就可以自动 squeeze/reduce 维度.      best_truth_overlap.squeeze_(0)      best_prior_idx.squeeze_(1)      best_prior_overlap.squeeze_(1)            best_truth_overlap.index_fill_(0, best_prior_idx, 2)  # ensure best prior      # 维度压缩后变为[num_priors], best_prior_idx 维度为[num_objs],      # 该语句会将与gt box匹配度最好的prior box 的交并比置为 2, 确保其最大, 以免防止某些 gtbox 没有匹配的 priorbox.        # 假想一种极端情况, 所有的priorbox与某个gtbox(标记为G)的交并比为1, 而其他gtbox分别有一个交并比      # 最高的priorbox, 但是肯定小于1(因为其他的gtbox与G的交并比肯定小于1), 这样一来, 就会使得所有      # 的priorbox都与G匹配, 为了防止这种情况, 我们将那些对gtbox来说, 具有最高交并比的priorbox,      # 强制进行互相匹配, 即令best_truth_idx[best_prior_idx[j]] = j, 详细见下面的for循环          # TODO refactor: index  best_prior_idx with long tensor      # ensure every gt matches with its prior of max overlap      # 注意!!: 因为 gt box 的数量要远远少于 prior box 的数量, 因此, 同一个 gt box 会与多个 prior box 匹配.      for j in range(best_prior_idx.size(0)):          best_truth_idx[best_prior_idx[j]] = j          # best_prior_idx[j] 代表与box_a的第j个box交并比最高的 prior box 的下标, 将与该 gtbox          # 匹配度最好的 prior box 的下标改为j, 由此,完成了该 gtbox 与第j个 prior box 的匹配.          # 这里的循环只会进行num_obj次, 剩余的匹配为 best_truth_idx 中原本的值.          # 这里处理的情况是, priorbox中第i个box与gtbox中第k个box的交并比最高,          # 即 best_truth_idx[i]= k          # 但是对于best_prior_idx[k]来说, 它却与priorbox的第l个box有着最高的交并比,          # 即best_prior_idx[k]=l          # 而对于gtbox的另一个边框gtbox[j]来说, 它与priorbox[i]的交并比最大,          # 即但是对于best_prior_idx[j] = i.          # 那么, 此时, 我们就应该将best_truth_idx[i]= k 修改成 best_truth_idx[i]= j.          # 即令 priorbox[i] 与 gtbox[j]对应.          # 这样做的原因: 防止某个gtbox没有匹配的 prior box.      matches = truths[best_truth_idx]          # Shape: [num_priors,4]      # truths 的shape 为[num_objs, 4], 而best_truth_idx是一个指示下标的列表, 列表长度为 8732,      # 列表中的下标范围为0~num_objs-1, 代表的是与每个priorbox匹配的gtbox的下标      # 上面的表达式会返回一个shape为 [num_priors, 4], 即 [8732, 4] 的tensor, 代表的就是与每个priorbox匹配的gtbox的坐标值.      conf = labels[best_truth_idx] + 1        # 与上面的语句道理差不多, 这里得到的是每个prior box匹配的类别编号, shape 为[8732]      conf[best_truth_overlap &lt; threshold] = 0 # 将与gtbox的交并比小于阈值的置为0 , 即认为是非物体框      loc = encode(matches, priors, variances) # 返回编码后的中心坐标和宽高.      loc_t[idx] = loc    # [num_priors,4] encoded offsets to learn # 设置第idx张图片的gt编码坐标信息      conf_t[idx] = conf  # [num_priors] top class label for each prior  设置第idx张图片的编号信息.(大于0即为物体编号, 认为有物体, 小于0认为是背景)    </code></pre><p>—|—  </p><p>参考文章：<a href="https://blog.csdn.net/happyday_d/article/details/86021993">https://blog.csdn.net/happyday_d/article/details/86021993</a><br><a href="https://hellozhaozheng.github.io/z_post/PyTorch-SSD">https://hellozhaozheng.github.io/z_post/PyTorch-SSD</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SqueezeNet</title>
      <link href="/2019/08/20/20190820-squeezenet/"/>
      <url>/2019/08/20/20190820-squeezenet/</url>
      
        <content type="html"><![CDATA[<p>论文地址: <a href="https://arxiv.org/pdf/1602.07360.pdf">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</a><br>非官方代码: <a href="https://github.com/arvention/SqueezeNet-PyTorch">pytorch</a></p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>这篇文章是DeepScal，加州大学伯克利分校，以及斯坦福大学在ICLR 2017发表的一篇文章。文章的主要目的是为了压缩模型，提高运行速度。这篇文章主要提出了SqueezeNet: 使用少量参数保持精度。</p><h2 id="结构设计策略"><a href="#结构设计策略" class="headerlink" title="结构设计策略"></a>结构设计策略</h2><p>这篇文章的首要目标是在保持准确率 的同时，有几个参数的CNN架构。这篇文章在设计CNN架构的时候采取了三个主要策略。这篇文章的主要模块 是Fire模块。</p><ul><li>用1x1的卷积核代替3x3的卷积核，从而减少参数量。1x1 卷积的参数比3x3的卷积核少了 9X.</li><li>减少3x3 卷积输入通道的数量。假设有一个卷积层, 它完全由3x3 卷积组成。此层中参数的总数量为：(输入通道数) <em>(过滤器数)</em> (3 * 3)。要在squeeze层中将输入的通道数减少。</li><li>在网络中减少下采样(maxpooling)实现, 以便卷积层具有较大的特征图。</li></ul><h2 id="Fire-Module"><a href="#Fire-Module" class="headerlink" title="Fire Module"></a>Fire Module</h2><p>Fire Module是将原来一层conv层变成两层：squeeze层+expand层，各自带上Relu激活层。在squeeze层里面全是1x1的卷积kernel，数量记为S11；在expand层里面有1x1和3x3的卷积kernel，expand层之后将1x1和3x3的卷积output feature maps在channel维度cat。<br><img src="/images/20190820_squeezenet_fire.png"></p><hr><p>自己手推的一张图，字比较丑，也没时间重现写一下。<br><img src="/images/20190820_squeezenet_squeezenet.jpg"></p><h3 id="fire-moudle的pytorch代码"><a href="#fire-moudle的pytorch代码" class="headerlink" title="fire moudle的pytorch代码"></a>fire moudle的pytorch代码</h3><p>很奇怪的是论文中用的是3个1x1，以及expand用的是4个1x1的卷积核和4个 3x3的卷积核，但是pytroch版本的代码并没有体现出来。  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">class</span> <span class="title class_">fire</span>(nn.Module):</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self, inplanes,squeeze_planes, expand_planes</span>):</span><br><span class="line">        <span class="built_in">super</span>(fire,self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(inplanes,squeeze_planes, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.bn1 = nn.BatchNorm2d(squeeze_planes)</span><br><span class="line">        self.relu1 = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(squeeze_planes, expand_planes, kernel_size=<span class="number">1</span>, stride=<span class="number">1</span>)</span><br><span class="line">        self.bn2 = nn.BatchNorm2d(expand_planes)</span><br><span class="line">        self.conv3 = nn.Conv2d(squeeze_planes,expand_planes,kernel_size=<span class="number">3</span>, stride=<span class="number">1</span>,padding=<span class="number">1</span>)</span><br><span class="line">        self.bn3 = nn.BatchNorm2d(expand_planes)</span><br><span class="line">        self.relu2 = nn.ReLU(inplace=<span class="literal">True</span>)</span><br><span class="line">        <span class="comment"># using MSR initialization</span></span><br><span class="line">        <span class="keyword">for</span> m <span class="keyword">in</span> self.modules():</span><br><span class="line">            <span class="keyword">if</span> <span class="built_in">isinstance</span>(m,nn.Conv2d):</span><br><span class="line">                n = m.kernel_size[<span class="number">0</span>]*m.kernel_size[<span class="number">1</span>]*m.in_channels</span><br><span class="line">                m.weight.data.normal_(<span class="number">0</span>,math.sqrt(<span class="number">2.</span>/n))</span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self,x</span>):</span><br><span class="line">        x = self.conv1(x)</span><br><span class="line">        x = self.bn1(x)</span><br><span class="line">        x = self.relu1(x)</span><br><span class="line">        out1 = self.conv2(x)</span><br><span class="line">        out1 = self.bn2(out1)</span><br><span class="line">        out2 = self.conv3(x)</span><br><span class="line">        out2 = self.bn3(out2)</span><br><span class="line">        out = torch.cat([out1,out2],<span class="number">1</span>)</span><br><span class="line">        out = self.relu2(out)</span><br><span class="line">        <span class="keyword">return</span> out</span><br></pre></td></tr></tbody></table></figure><h2 id="SqueezeNet的具体网络结构"><a href="#SqueezeNet的具体网络结构" class="headerlink" title="SqueezeNet的具体网络结构"></a>SqueezeNet的具体网络结构</h2><p><img src="/images/20190820_squeezenet_struct.png"></p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>imagenet数据上比较了alexnet，可以看到准确率差不多的情况下，squeezeNet模型参数数量显著降低了（下表倒数第三行），参数减少50X；如果再加上deep compression技术，压缩比可以达到461X！还是不错的结果。<br><img src="/images/20190820_squeezenet_params.png"></p><p>参考文章：<a href="https://blog.csdn.net/xbinworld/article/details/50897870">https://blog.csdn.net/xbinworld/article/details/50897870</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>SqueezeNet</title>
      <link href="/2019/08/20/squeezenet/"/>
      <url>/2019/08/20/squeezenet/</url>
      
        <content type="html"><![CDATA[<p>论文地址: <a href="https://arxiv.org/pdf/1602.07360.pdf">SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and &lt;0.5MB model size</a><br>非官方代码: <a href="https://github.com/arvention/SqueezeNet-PyTorch">pytorch</a></p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>这篇文章是DeepScal，加州大学伯克利分校，以及斯坦福大学在ICLR 2017发表的一篇文章。文章的主要目的是为了压缩模型，提高运行速度。这篇文章主要提出了SqueezeNet: 使用少量参数保持精度。</p><h2 id="结构设计策略"><a href="#结构设计策略" class="headerlink" title="结构设计策略"></a>结构设计策略</h2><p>这篇文章的首要目标是在保持准确率 的同时，有几个参数的CNN架构。这篇文章在设计CNN架构的时候采取了三个主要策略。这篇文章的主要模块 是Fire模块。</p><ul><li>用1x1的卷积核代替3x3的卷积核，从而减少参数量。1x1 卷积的参数比3x3的卷积核少了 9X.</li><li>减少3x3 卷积输入通道的数量。假设有一个卷积层, 它完全由3x3 卷积组成。此层中参数的总数量为：(输入通道数) <em>(过滤器数)</em> (3 * 3)。要在squeeze层中将输入的通道数减少。</li><li>在网络中减少下采样(maxpooling)实现, 以便卷积层具有较大的特征图。</li></ul><h2 id="Fire-Module"><a href="#Fire-Module" class="headerlink" title="Fire Module"></a>Fire Module</h2><p>Fire Module是将原来一层conv层变成两层：squeeze层+expand层，各自带上Relu激活层。在squeeze层里面全是1x1的卷积kernel，数量记为S11；在expand层里面有1x1和3x3的卷积kernel，expand层之后将1x1和3x3的卷积output feature maps在channel维度cat。<br><img src="/2019/08/20/squeezenet/images/20190820_squeezenet_fire.png"></p><hr><p>自己手推的一张图，字比较丑，也没时间重现写一下。<br><img src="/2019/08/20/squeezenet/images/20190820_squeezenet_squeezenet.jpg"></p><h3 id="fire-moudle的pytorch代码"><a href="#fire-moudle的pytorch代码" class="headerlink" title="fire moudle的pytorch代码"></a>fire moudle的pytorch代码</h3><p>很奇怪的是论文中用的是3个1x1，以及expand用的是4个1x1的卷积核和4个 3x3的卷积核，但是pytroch版本的代码并没有体现出来。  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  </code></pre><p>| </p><pre><code>class fire(nn.Module):      def __init__(self, inplanes,squeeze_planes, expand_planes):          super(fire,self).__init__()          self.conv1 = nn.Conv2d(inplanes,squeeze_planes, kernel_size=1, stride=1)          self.bn1 = nn.BatchNorm2d(squeeze_planes)          self.relu1 = nn.ReLU(inplace=True)          self.conv2 = nn.Conv2d(squeeze_planes, expand_planes, kernel_size=1, stride=1)          self.bn2 = nn.BatchNorm2d(expand_planes)          self.conv3 = nn.Conv2d(squeeze_planes,expand_planes,kernel_size=3, stride=1,padding=1)          self.bn3 = nn.BatchNorm2d(expand_planes)          self.relu2 = nn.ReLU(inplace=True)              # using MSR initialization              for m in self.modules():              if isinstance(m,nn.Conv2d):                  n = m.kernel_size[0]*m.kernel_size[1]*m.in_channels                  m.weight.data.normal_(0,math.sqrt(2./n))      def forward(self,x):          x = self.conv1(x)          x = self.bn1(x)          x = self.relu1(x)          out1 = self.conv2(x)          out1 = self.bn2(out1)          out2 = self.conv3(x)          out2 = self.bn3(out2)            out = torch.cat([out1,out2],1)          out = self.relu2(out)            return out    </code></pre><p>—|—  </p><h2 id="SqueezeNet的具体网络结构"><a href="#SqueezeNet的具体网络结构" class="headerlink" title="SqueezeNet的具体网络结构"></a>SqueezeNet的具体网络结构</h2><p><img src="/2019/08/20/squeezenet/images/20190820_squeezenet_struct.png"></p><h2 id="实验结果"><a href="#实验结果" class="headerlink" title="实验结果"></a>实验结果</h2><p>imagenet数据上比较了alexnet，可以看到准确率差不多的情况下，squeezeNet模型参数数量显著降低了（下表倒数第三行），参数减少50X；如果再加上deep compression技术，压缩比可以达到461X！还是不错的结果。<br><img src="/2019/08/20/squeezenet/images/20190820_squeezenet_params.png"></p><p>参考文章：<a href="https://blog.csdn.net/xbinworld/article/details/50897870">https://blog.csdn.net/xbinworld/article/details/50897870</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多层感知机的反向传播</title>
      <link href="/2019/08/19/20190819-MLP-Back-Propagation/"/>
      <url>/2019/08/19/20190819-MLP-Back-Propagation/</url>
      
        <content type="html"><![CDATA[<p>全连接神经网络是形式上最简单的神经网络，反向传播算法是一种常用的训练神经网络的算法，理解全连接神经网络中的反向传播算法是理解其他更加复杂网络中反向传播算法的重要基础。</p><p><img src="/images/20190819_MLP-Back-Propagation_mlp1.jpg"> <img src="/images/20190819_MLP-Back-Propagation_mlp2.jpg"></p><p>参考链接： <a href="https://zhuanlan.zhihu.com/p/61863634">https://zhuanlan.zhihu.com/p/61863634</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MobileNets</title>
      <link href="/2019/08/19/20190819-MobileNets/"/>
      <url>/2019/08/19/20190819-MobileNets/</url>
      
        <content type="html"><![CDATA[<p>论文地址:<a href="https://arxiv.org/abs/1704.04861">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications </a><br>非官方代码:<a href="https://github.com/marvis/pytorch-mobilenet">pytorch/models</a>  </p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这篇文章是谷歌在2017针对手机等嵌入式设备提出的一种轻量级深层网络，这篇论文主要的贡献点在于提出了一种深度可分离卷积。</p><ul><li>主要解决的问题是注重优化延迟，同时也兼顾了模型的大小，不像有些模型虽然参数量比较小，但是速度也是慢的可以。</li><li>MobileNets使用了大量的3 × 3的卷积核，极大地减少了计算量（1/8到1/9之间），同时准确率下降的很少，相比其他的方法确有优势。</li></ul><h2 id="深度可分离卷积示例"><a href="#深度可分离卷积示例" class="headerlink" title="深度可分离卷积示例"></a>深度可分离卷积示例</h2><p><img src="/images/20190819_MobileNets_MobileNets.jpg"></p><h2 id="模型结构和训练"><a href="#模型结构和训练" class="headerlink" title="模型结构和训练"></a>模型结构和训练</h2><p>MobileNets结构建立在上述深度可分解卷积中（只有第一层是标准卷积）。该网络允许我们探索网络拓扑，找到一个适合的良好网络。其具体架构在表1说明。除了最后的全连接层，所有层后面跟了batchnorm和ReLU，最终输入到softmax进行分类。图3对比了标准卷积和分解卷积的结构，二者都附带了BN和ReLU层。按照作者的计算方法，MobileNets总共28层（1 + 2 × 13 + 1 = 28）</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MobileNetsV2</title>
      <link href="/2019/08/19/20190819-MobileNetsV2/"/>
      <url>/2019/08/19/20190819-MobileNetsV2/</url>
      
        <content type="html"><![CDATA[<p>论文地址:<a href="https://arxiv.org/pdf/1801.04381.pdf">MobileNetV2: Inverted Residuals and Linear Bottlenecks </a><br>非官方代码:<a href="https://github.com/tonylins/pytorch-mobilenet-v2">pytorch</a></p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>这篇文章是谷歌在2019提出来的文章在MobileNets 基础上做的改进。</p><h2 id="深度可分离卷积示例"><a href="#深度可分离卷积示例" class="headerlink" title="深度可分离卷积示例"></a>深度可分离卷积示例</h2><ul><li>首先在Xception 中被广泛使用</li><li>好处： 理论上可以成倍的减少卷积层的时间复杂度和空间复杂度<img src="/images/20190819_MobileNetsV2_MobileNets.jpg"></li></ul><h2 id="文章内容"><a href="#文章内容" class="headerlink" title="文章内容"></a>文章内容</h2><h3 id="与MobileNets-的对比"><a href="#与MobileNets-的对比" class="headerlink" title="与MobileNets 的对比"></a>与MobileNets 的对比</h3><ul><li>相同点<ul><li>都采用 Depth-wise (DW) 卷积搭配 Point-wise (PW) 卷积的方式来提特征</li></ul></li><li>不同点<ul><li>Linear Bottleneck</li><li>V2 在 DW 卷积之前新加了一个 PW 卷积。<ul><li>DW卷积由于本身的计算特性不能改变通道数的能力。若通道数很少的话，DW在提取地低纬特征，效果可能并不会好。</li><li>在每个DW之前，增加了PW用于升维，这样DW可以更好的提取特征</li></ul></li><li>V2 去掉了第二个 PW 的激活函数<ul><li>激活函数在高维空间能够有效的增加非线性，而在低维空间时则会破坏特征</li><li>第二个 PW 的主要功能就是降维</li></ul></li></ul></li></ul><hr><h3 id="与ResNet的对比"><a href="#与ResNet的对比" class="headerlink" title="与ResNet的对比"></a>与ResNet的对比</h3><ul><li>相同点<ul><li>MobileNet V2 借鉴 ResNet，都采用了 1x1-&gt;3x3-&gt;1x1的模式</li><li>MobileNet V2 借鉴 ResNet，同样使用 Shortcut 将输出与输入相加</li></ul></li><li>不同点<ul><li>Inverted Residual Block</li><li>ResNet 使用 标准卷积 提特征，MobileNet 始终使用 DW卷积 提特征</li><li>ResNet 先降维 (0.25倍)、卷积、再升维，而 MobileNet V2 则是 先升维 (6倍)、卷积、再降维。直观的形象上来看，ResNet 的微结构是沙漏形，而 MobileNet V2 则是纺锤形，刚好相反。因此论文作者将 MobileNet V2 的结构称为 Inverted Residual Block。使用DW卷积而作的适配，特征提取能够在高维进行</li></ul></li></ul><hr><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ul><li>MobileNets 与MobileNets V2在模型结构上的对比</li><li>MobileNetsV2 的卷积层数比V1要多，但是时间复杂度，以及空间复杂度，以及在cpu上的推理时间要远远优于MobileNets</li></ul><p>参考文章：<a href="https://zhuanlan.zhihu.com/p/33075914">https://zhuanlan.zhihu.com/p/33075914</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分治算法</title>
      <link href="/2019/08/19/20190819-%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95/"/>
      <url>/2019/08/19/20190819-%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>问题： 给定一组数，要求从中找出第k小的元素。<br>分析：<br>这里通过快速排序算法来解决次问题。记一趟快速排序后，左子集中的元素个数为nleft，则选择问题，可能是一下几种情况之一： </p><ul><li>nleft等于k-1，则枢纽值即为所求； </li><li>nleft大于k-1,则继续在左子树中找； </li><li>nleft小于k-1,则继续在右子集中找<br>C++代码实现</li></ul><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// t5.cpp : 定义控制台应用程序的入口点。</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">"stdafx.h"</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">"stdafx.h"</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;time.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">quickSelect</span><span class="params">(<span class="type">int</span> a[], <span class="type">int</span> l, <span class="type">int</span> r, <span class="type">int</span> k)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line"><span class="type">int</span> p = <span class="built_in">rand</span>() % (r - l + <span class="number">1</span>) + l;</span><br><span class="line"><span class="type">int</span> pivot = a[p];</span><br><span class="line">{<span class="type">int</span> t = a[p]; a[p] = a[r]; a[r] = t; }</span><br><span class="line"><span class="type">int</span> i = l,j = r;</span><br><span class="line"><span class="keyword">while</span> (i &lt; j)</span><br><span class="line">{</span><br><span class="line"><span class="keyword">while</span> (i &lt; j&amp;&amp;a[i] &lt; pivot) i++;</span><br><span class="line"><span class="keyword">if</span> (i &lt; j) {</span><br><span class="line">a[j] = a[i];</span><br><span class="line">j--;</span><br><span class="line">}</span><br><span class="line"><span class="keyword">while</span> (i &lt; j&amp;&amp;a[i] &gt;pivot) j--;</span><br><span class="line"><span class="keyword">if</span> (i &lt; j) {</span><br><span class="line">a[i] = a[j];</span><br><span class="line">i++;</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line">a[i] = pivot;</span><br><span class="line">p = i;</span><br><span class="line"><span class="keyword">if</span> (i - l + <span class="number">1</span> == k) <span class="keyword">return</span> a[i];</span><br><span class="line"><span class="comment">// j + 1, right, k - (j - left + 1)1)</span></span><br><span class="line"><span class="keyword">if</span> (i - l + <span class="number">1</span> &lt; k) <span class="keyword">return</span> <span class="built_in">quickSelect</span>(a, i+<span class="number">1</span>, r, k-i+l<span class="number">-1</span>);</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">return</span>  <span class="built_in">quickSelect</span>(a, l, i<span class="number">-1</span>, k);</span><br><span class="line">}</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line"><span class="type">int</span> a[] = { <span class="number">1</span>,<span class="number">4</span>,<span class="number">54</span>,<span class="number">8</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">45</span>,<span class="number">58</span>,<span class="number">27</span>,<span class="number">8</span>,<span class="number">25</span>,<span class="number">26</span>,<span class="number">21</span>,<span class="number">12</span> };</span><br><span class="line"><span class="built_in">printf_s</span>(<span class="string">"%d\n"</span>, <span class="built_in">quickSelect</span>(a, <span class="number">0</span>, <span class="number">14</span>, <span class="number">6</span>));</span><br><span class="line"><span class="built_in">system</span>(<span class="string">"pause"</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多层感知机的反向传播</title>
      <link href="/2019/08/19/MLP-Back-Propagation/"/>
      <url>/2019/08/19/MLP-Back-Propagation/</url>
      
        <content type="html"><![CDATA[<p>全连接神经网络是形式上最简单的神经网络，反向传播算法是一种常用的训练神经网络的算法，理解全连接神经网络中的反向传播算法是理解其他更加复杂网络中反向传播算法的重要基础。</p><p><img src="/2019/08/19/MLP-Back-Propagation/images/20190819_MLP-Back-Propagation_mlp1.jpg"> <img src="/2019/08/19/MLP-Back-Propagation/images/20190819_MLP-Back-Propagation_mlp2.jpg"></p><p>参考链接： <a href="https://zhuanlan.zhihu.com/p/61863634">https://zhuanlan.zhihu.com/p/61863634</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MobileNetsV2</title>
      <link href="/2019/08/19/MobileNetsV2/"/>
      <url>/2019/08/19/MobileNetsV2/</url>
      
        <content type="html"><![CDATA[<p>论文地址:<a href="https://arxiv.org/pdf/1801.04381.pdf">MobileNetV2: Inverted Residuals and Linear Bottlenecks </a><br>非官方代码:<a href="https://github.com/tonylins/pytorch-mobilenet-v2">pytorch</a></p><h2 id="介绍"><a href="#介绍" class="headerlink" title="介绍"></a>介绍</h2><p>这篇文章是谷歌在2019提出来的文章在MobileNets 基础上做的改进。</p><h2 id="深度可分离卷积示例"><a href="#深度可分离卷积示例" class="headerlink" title="深度可分离卷积示例"></a>深度可分离卷积示例</h2><ul><li>首先在Xception 中被广泛使用</li><li>好处： 理论上可以成倍的减少卷积层的时间复杂度和空间复杂度<img src="/2019/08/19/MobileNetsV2/images/20190819_MobileNetsV2_MobileNets.jpg"></li></ul><h2 id="文章内容"><a href="#文章内容" class="headerlink" title="文章内容"></a>文章内容</h2><h3 id="与MobileNets-的对比"><a href="#与MobileNets-的对比" class="headerlink" title="与MobileNets 的对比"></a>与MobileNets 的对比</h3><ul><li>相同点<ul><li>都采用 Depth-wise (DW) 卷积搭配 Point-wise (PW) 卷积的方式来提特征</li></ul></li><li>不同点<ul><li>Linear Bottleneck</li><li>V2 在 DW 卷积之前新加了一个 PW 卷积。<ul><li>DW卷积由于本身的计算特性不能改变通道数的能力。若通道数很少的话，DW在提取地低纬特征，效果可能并不会好。</li><li>在每个DW之前，增加了PW用于升维，这样DW可以更好的提取特征</li></ul></li><li>V2 去掉了第二个 PW 的激活函数<ul><li>激活函数在高维空间能够有效的增加非线性，而在低维空间时则会破坏特征</li><li>第二个 PW 的主要功能就是降维</li></ul></li></ul></li></ul><hr><h3 id="与ResNet的对比"><a href="#与ResNet的对比" class="headerlink" title="与ResNet的对比"></a>与ResNet的对比</h3><ul><li>相同点<ul><li>MobileNet V2 借鉴 ResNet，都采用了 1x1-&gt;3x3-&gt;1x1的模式</li><li>MobileNet V2 借鉴 ResNet，同样使用 Shortcut 将输出与输入相加</li></ul></li><li>不同点<ul><li>Inverted Residual Block</li><li>ResNet 使用 标准卷积 提特征，MobileNet 始终使用 DW卷积 提特征</li><li>ResNet 先降维 (0.25倍)、卷积、再升维，而 MobileNet V2 则是 先升维 (6倍)、卷积、再降维。直观的形象上来看，ResNet 的微结构是沙漏形，而 MobileNet V2 则是纺锤形，刚好相反。因此论文作者将 MobileNet V2 的结构称为 Inverted Residual Block。使用DW卷积而作的适配，特征提取能够在高维进行</li></ul></li></ul><hr><h2 id="结论"><a href="#结论" class="headerlink" title="结论"></a>结论</h2><ul><li>MobileNets 与MobileNets V2在模型结构上的对比</li><li>MobileNetsV2 的卷积层数比V1要多，但是时间复杂度，以及空间复杂度，以及在cpu上的推理时间要远远优于MobileNets</li></ul><p>参考文章：<a href="https://zhuanlan.zhihu.com/p/33075914">https://zhuanlan.zhihu.com/p/33075914</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MobileNets</title>
      <link href="/2019/08/19/MobileNets/"/>
      <url>/2019/08/19/MobileNets/</url>
      
        <content type="html"><![CDATA[<p>论文地址:<a href="https://arxiv.org/abs/1704.04861">MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications </a><br>非官方代码:<a href="https://github.com/marvis/pytorch-mobilenet">pytorch/models</a>  </p><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>这篇文章是谷歌在2017针对手机等嵌入式设备提出的一种轻量级深层网络，这篇论文主要的贡献点在于提出了一种深度可分离卷积。</p><ul><li>主要解决的问题是注重优化延迟，同时也兼顾了模型的大小，不像有些模型虽然参数量比较小，但是速度也是慢的可以。</li><li>MobileNets使用了大量的3 × 3的卷积核，极大地减少了计算量（1/8到1/9之间），同时准确率下降的很少，相比其他的方法确有优势。</li></ul><h2 id="深度可分离卷积示例"><a href="#深度可分离卷积示例" class="headerlink" title="深度可分离卷积示例"></a>深度可分离卷积示例</h2><p><img src="/2019/08/19/MobileNets/images/20190819_MobileNets_MobileNets.jpg"></p><h2 id="模型结构和训练"><a href="#模型结构和训练" class="headerlink" title="模型结构和训练"></a>模型结构和训练</h2><p>MobileNets结构建立在上述深度可分解卷积中（只有第一层是标准卷积）。该网络允许我们探索网络拓扑，找到一个适合的良好网络。其具体架构在表1说明。除了最后的全连接层，所有层后面跟了batchnorm和ReLU，最终输入到softmax进行分类。图3对比了标准卷积和分解卷积的结构，二者都附带了BN和ReLU层。按照作者的计算方法，MobileNets总共28层（1 + 2 × 13 + 1 = 28）</p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> PyTorch </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>分治算法</title>
      <link href="/2019/08/19/%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95/"/>
      <url>/2019/08/19/%E5%88%86%E6%B2%BB%E7%AE%97%E6%B3%95/</url>
      
        <content type="html"><![CDATA[<p>问题： 给定一组数，要求从中找出第k小的元素。<br>分析：<br>这里通过快速排序算法来解决次问题。记一趟快速排序后，左子集中的元素个数为nleft，则选择问题，可能是一下几种情况之一： </p><ul><li><p>nleft等于k-1，则枢纽值即为所求； </p></li><li><p>nleft大于k-1,则继续在左子树中找； </p></li><li><p>nleft小于k-1,则继续在右子集中找<br>C++代码实现</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44</code></pre></li></ul><p>| </p><pre><code>    // t5.cpp : 定义控制台应用程序的入口点。      //            #include "stdafx.h"      #include "stdafx.h"      #include&lt;stdio.h&gt;      #include&lt;stdlib.h&gt;      #include&lt;time.h&gt;      #include&lt;math.h&gt;      #include&lt;string.h&gt;            int quickSelect(int a[], int l, int r, int k)      {          int p = rand() % (r - l + 1) + l;          int pivot = a[p];          {int t = a[p]; a[p] = a[r]; a[r] = t; }          int i = l,j = r;          while (i &lt; j)          {              while (i &lt; j&amp;&amp;a[i] &lt; pivot) i++;              if (i &lt; j) {                  a[j] = a[i];                  j--;              }              while (i &lt; j&amp;&amp;a[i] &gt;pivot) j--;              if (i &lt; j) {                  a[i] = a[j];                  i++;              }          }          a[i] = pivot;          p = i;          if (i - l + 1 == k) return a[i];          // j + 1, right, k - (j - left + 1)1)          if (i - l + 1 &lt; k) return quickSelect(a, i+1, r, k-i+l-1);          else return  quickSelect(a, l, i-1, k);      }      int main()      {          int a[] = { 1,4,54,8,3,7,45,58,27,8,25,26,21,12 };          printf_s("%d\n", quickSelect(a, 0, 14, 6));          system("pause");          return 0;      }        </code></pre><p>—|—  </p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法竞赛入门经典第四章</title>
      <link href="/2019/08/18/20190818-algorithm4/"/>
      <url>/2019/08/18/20190818-algorithm4/</url>
      
        <content type="html"><![CDATA[<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// t4.cpp : 定义控制台应用程序的入口点。</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">"stdafx.h"</span></span></span><br><span class="line"><span class="comment">//素数判断方法2</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdio.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;time.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;string.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> Max 1000000</span></span><br><span class="line"><span class="meta">#<span class="keyword">define</span> maxn 100</span></span><br><span class="line"><span class="type">int</span> left, chance; <span class="comment">//还需要left位置，错chance之后就会输</span></span><br><span class="line"><span class="type">char</span> s[maxn], s2[maxn]; <span class="comment">//答案是字符串s, 玩家猜的字母序列是s2</span></span><br><span class="line"><span class="type">int</span> win, lose; <span class="comment">// win=1 表示已经赢了; lose=1 表示已经输了</span></span><br><span class="line"><span class="function"><span class="type">long</span> <span class="title">fac</span><span class="params">(<span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line"><span class="keyword">if</span> (<span class="number">0</span> == n || <span class="number">1</span> == n)</span><br><span class="line">{</span><br><span class="line"><span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">}</span><br><span class="line"><span class="keyword">else</span></span><br><span class="line">{</span><br><span class="line"><span class="keyword">return</span> <span class="built_in">fac</span>(n - <span class="number">1</span>)*n;</span><br><span class="line">}</span><br><span class="line">}</span><br><span class="line"><span class="comment">// 非递归的阶乘方法</span></span><br><span class="line"><span class="function"><span class="type">long</span> <span class="title">fact</span><span class="params">(<span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line"><span class="type">long</span> iRes = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">{</span><br><span class="line">iRes *= i;</span><br><span class="line">}</span><br><span class="line"><span class="keyword">return</span> iRes;</span><br><span class="line">}</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">combination</span><span class="params">(<span class="type">int</span> m, <span class="type">int</span> n)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line"><span class="type">long</span> iRes = <span class="built_in">fac</span>(n) / (<span class="built_in">fac</span>(m)*<span class="built_in">fac</span>(n - m));</span><br><span class="line"><span class="comment">// long iRes = fact(n) /(fac(m)*fac(n-m));</span></span><br><span class="line"><span class="built_in">printf_s</span>(<span class="string">"%ld %.2lf \n"</span>, iRes, (<span class="type">double</span>)<span class="built_in">clock</span>() / CLOCKS_PER_SEC);</span><br><span class="line"><span class="keyword">return</span> iRes;</span><br><span class="line">}</span><br><span class="line"><span class="comment">// 刽子手游戏---guess函数</span></span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">guess</span><span class="params">(<span class="type">char</span> ch)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line"><span class="type">int</span> bad = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="built_in">strlen</span>(s); i++)</span><br><span class="line"><span class="keyword">if</span> (s[i] == ch) { left--; s[i] = <span class="string">' '</span>; bad = <span class="number">0</span>; }</span><br><span class="line"><span class="keyword">if</span> (bad) --chance;</span><br><span class="line"><span class="keyword">if</span> (!chance) lose = <span class="number">1</span>;</span><br><span class="line"><span class="keyword">if</span> (!left) win = <span class="number">1</span>;</span><br><span class="line">}</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">关键:</span></span><br><span class="line"><span class="comment">1 用素数筛选法先预处理，默认刚开始全为素数，然后对素数的倍数标记为非素数，for(int j = i*i ; j &lt;= 10000 ; j += i){iPrimeArr[j] = 1;}</span></span><br><span class="line"><span class="comment">2 通过开根号判断素数时，可以用floor,注意浮点数加上0.5，int iRadical = floor(sqrt(n*1.0) + 0.5);</span></span><br><span class="line"><span class="comment">3 可以用assert()对输入的合法性进行校验，assert(n &gt;= 5 &amp;&amp; n &lt;= 10000);void assert(int exp),如果表达式的值为0则退出。*/</span></span><br><span class="line"><span class="comment">//int sum = 1;</span></span><br><span class="line"><span class="comment">//for (int i = 3; i &lt;= Max; i += 2)</span></span><br><span class="line"><span class="comment">//{</span></span><br><span class="line"><span class="comment">////因为偶数除了2 都不是质数</span></span><br><span class="line"><span class="comment">//int j;</span></span><br><span class="line"><span class="comment">//for (j = 2; j &lt;= (int)sqrt(i); j++)//利用上述结论判断</span></span><br><span class="line"><span class="comment">//if (i%j == 0) break;</span></span><br><span class="line"><span class="comment">//if (j &gt; (int)sqrt(i))</span></span><br><span class="line"><span class="comment">//sum++;</span></span><br><span class="line"><span class="comment">//}</span></span><br><span class="line"><span class="comment">//printf_s("Time used = %0.2f s\n", (double)clock() / CLOCKS_PER_SEC);</span></span><br><span class="line"><span class="comment">//printf_s("%d\n", sum);</span></span><br><span class="line"><span class="comment">//int a, b;</span></span><br><span class="line"><span class="comment">//scanf_s("%d %d", &amp;a, &amp;b);</span></span><br><span class="line"><span class="comment">//combination(a, b);</span></span><br><span class="line"><span class="comment">//刽子手游戏</span></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">游戏规则是这样的：计算机想一个单词让你猜，你每次可以猜一个字母。 如果单词里有那个字母，所有该字母会显示出来；如果没有那个字母，则计算机会在一幅“刽子手”画上填一笔。 这幅画一共需要7笔就能完成，因此你最多只能错6次。 注意，猜一个已经猜过的字母也算错。</span></span><br><span class="line"><span class="comment">在本题中，你的任务是编写一个“裁判”程序，输入单词和玩家的猜测，判断玩家赢了（You win.）、 输了（You lose.）还是放弃了（You chickened out.）。 每组数据包含3行，第1行是游戏编号（-1为输入结束标记），第2行是计算机想的单词，第3行是玩家的猜测。 后两行保证只含小写字母。</span></span><br><span class="line"><span class="comment">*/</span></span><br><span class="line"><span class="comment">//char ans[21];</span></span><br><span class="line"><span class="comment">//char gus[28];</span></span><br><span class="line"><span class="comment">//int times, yes;</span></span><br><span class="line"><span class="comment">//int chances = 7;</span></span><br><span class="line"><span class="comment">//int win = 0;</span></span><br><span class="line"><span class="comment">//int lose = 0;</span></span><br><span class="line"><span class="comment">//scanf_s("%d", &amp;times);</span></span><br><span class="line"><span class="comment">//while (times != -1)</span></span><br><span class="line"><span class="comment">//{</span></span><br><span class="line"><span class="comment">//printf_s("Round %d\n", times);</span></span><br><span class="line"><span class="comment">//scanf_s("%s\n %s", ans, gus);</span></span><br><span class="line"><span class="comment">//printf_s("Round %s &amp;s\n", ans, gus);</span></span><br><span class="line"><span class="comment">//yes = 0;</span></span><br><span class="line"><span class="comment">//win = 0, lose = 0, chances = 7;</span></span><br><span class="line"><span class="comment">//for (int i = 0; i &lt; strlen(gus); i++)</span></span><br><span class="line"><span class="comment">//{</span></span><br><span class="line"><span class="comment">//int flag = 0;</span></span><br><span class="line"><span class="comment">//for (int j = 0; j &lt; strlen(ans); j++)</span></span><br><span class="line"><span class="comment">//{</span></span><br><span class="line"><span class="comment">//if (ans[j] == gus[i])</span></span><br><span class="line"><span class="comment">//{</span></span><br><span class="line"><span class="comment">//yes++;</span></span><br><span class="line"><span class="comment">//flag = 1;//找到之后不退出，因为有一个有相同的字母</span></span><br><span class="line"><span class="comment">//}</span></span><br><span class="line"><span class="comment">//if (flag == 0)</span></span><br><span class="line"><span class="comment">//{</span></span><br><span class="line"><span class="comment">//chances--;</span></span><br><span class="line"><span class="comment">//}</span></span><br><span class="line"><span class="comment">//if (chances == 0)</span></span><br><span class="line"><span class="comment">//{</span></span><br><span class="line"><span class="comment">//lose = 1;</span></span><br><span class="line"><span class="comment">//printf_s("You lose.\n");</span></span><br><span class="line"><span class="comment">//}</span></span><br><span class="line"><span class="comment">//else if (yes == strlen(ans))</span></span><br><span class="line"><span class="comment">//{</span></span><br><span class="line"><span class="comment">//win = 1;</span></span><br><span class="line"><span class="comment">//printf_s("You win.\n");</span></span><br><span class="line"><span class="comment">//break;</span></span><br><span class="line"><span class="comment">//}</span></span><br><span class="line"><span class="comment">//if (win != 1 &amp;&amp; lose != 1)</span></span><br><span class="line"><span class="comment">//{</span></span><br><span class="line"><span class="comment">//printf("You chickened out.\n");</span></span><br><span class="line"><span class="comment">//}</span></span><br><span class="line"><span class="comment">//scanf_s("%d", &amp;times);</span></span><br><span class="line"><span class="comment">//}</span></span><br><span class="line"><span class="comment">//}</span></span><br><span class="line"><span class="comment">//}</span></span><br><span class="line"><span class="type">int</span> rnd;</span><br><span class="line"><span class="keyword">while</span> (<span class="built_in">scanf_s</span>(<span class="string">"%d%s%s"</span>, &amp;rnd, &amp;s, &amp;s2) == <span class="number">3</span> &amp;&amp; rnd != <span class="number">-1</span>)</span><br><span class="line">{</span><br><span class="line"><span class="built_in">printf_s</span>(<span class="string">"%Round %d\n"</span>, rnd);</span><br><span class="line">win = lose = <span class="number">0</span>;</span><br><span class="line">left = <span class="built_in">strlen</span>(s);</span><br><span class="line">chance = <span class="number">7</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="type">int</span> i = <span class="number">0</span>; i &lt; <span class="built_in">strlen</span>(s2); i++)</span><br><span class="line">{</span><br><span class="line"><span class="built_in">guess</span>(s2[i]); <span class="comment">//猜一个字母</span></span><br><span class="line"><span class="keyword">if</span> (win || lose) <span class="keyword">break</span>; <span class="comment">//检查状态</span></span><br><span class="line">}</span><br><span class="line"><span class="comment">//根据结果进行输出</span></span><br><span class="line"><span class="keyword">if</span> (win) <span class="built_in">printf_s</span>(<span class="string">"You win.\n"</span>);</span><br><span class="line"><span class="keyword">else</span> <span class="keyword">if</span> (lose) <span class="built_in">printf_s</span>(<span class="string">"You lose.\n"</span>);</span><br><span class="line"><span class="keyword">else</span> <span class="built_in">printf_s</span>(<span class="string">"You chickened out.\n"</span>);</span><br><span class="line">}</span><br><span class="line"><span class="built_in">system</span>(<span class="string">"pause"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法竞赛入门经典第四章</title>
      <link href="/2019/08/18/algorithm4/"/>
      <url>/2019/08/18/algorithm4/</url>
      
        <content type="html"><![CDATA[<pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  157    </code></pre><p>| </p><pre><code>// t4.cpp : 定义控制台应用程序的入口点。  //    #include "stdafx.h"  //素数判断方法2  #include&lt;stdio.h&gt;  #include&lt;stdlib.h&gt;  #include&lt;time.h&gt;  #include&lt;math.h&gt;  #include&lt;string.h&gt;  #define Max 1000000  #define maxn 100  int left, chance; //还需要left位置，错chance之后就会输  char s[maxn], s2[maxn]; //答案是字符串s, 玩家猜的字母序列是s2  int win, lose; // win=1 表示已经赢了; lose=1 表示已经输了    long fac(int n)   {      if (0 == n || 1 == n)      {          return 1;      }      else      {          return fac(n - 1)*n;      }  }    // 非递归的阶乘方法   long fact(int n)  {      long iRes = 1;      for (int i = 1; i &lt;= n; i++)      {          iRes *= i;      }      return iRes;  }  int combination(int m, int n)  {      long iRes = fac(n) / (fac(m)*fac(n - m));        // long iRes = fact(n) /(fac(m)*fac(n-m));        printf_s("%ld %.2lf \n", iRes, (double)clock() / CLOCKS_PER_SEC);      return iRes;  }  // 刽子手游戏---guess函数  void guess(char ch)  {      int bad = 1;      for (int i = 0; i &lt; strlen(s); i++)          if (s[i] == ch) { left--; s[i] = ' '; bad = 0; }      if (bad) --chance;      if (!chance) lose = 1;      if (!left) win = 1;  }  int main()  {        /*      关键:      1 用素数筛选法先预处理，默认刚开始全为素数，然后对素数的倍数标记为非素数，for(int j = i*i ; j &lt;= 10000 ; j += i){iPrimeArr[j] = 1;}      2 通过开根号判断素数时，可以用floor,注意浮点数加上0.5，int iRadical = floor(sqrt(n*1.0) + 0.5);      3 可以用assert()对输入的合法性进行校验，assert(n &gt;= 5 &amp;&amp; n &lt;= 10000);void assert(int exp),如果表达式的值为0则退出。*/        //int sum = 1;      //for (int i = 3; i &lt;= Max; i += 2)      //{      ////因为偶数除了2 都不是质数      //int j;      //for (j = 2; j &lt;= (int)sqrt(i); j++)//利用上述结论判断      //if (i%j == 0) break;      //if (j &gt; (int)sqrt(i))      //sum++;      //}      //printf_s("Time used = %0.2f s\n", (double)clock() / CLOCKS_PER_SEC);      //printf_s("%d\n", sum);        //int a, b;      //scanf_s("%d %d", &amp;a, &amp;b);      //combination(a, b);        //刽子手游戏      /*      游戏规则是这样的：计算机想一个单词让你猜，你每次可以猜一个字母。 如果单词里有那个字母，所有该字母会显示出来；如果没有那个字母，则计算机会在一幅“刽子手”画上填一笔。 这幅画一共需要7笔就能完成，因此你最多只能错6次。 注意，猜一个已经猜过的字母也算错。      在本题中，你的任务是编写一个“裁判”程序，输入单词和玩家的猜测，判断玩家赢了（You win.）、 输了（You lose.）还是放弃了（You chickened out.）。 每组数据包含3行，第1行是游戏编号（-1为输入结束标记），第2行是计算机想的单词，第3行是玩家的猜测。 后两行保证只含小写字母。      */      //char ans[21];      //char gus[28];      //int times, yes;      //int chances = 7;      //int win = 0;      //int lose = 0;      //scanf_s("%d", &amp;times);      //while (times != -1)      //{      //printf_s("Round %d\n", times);      //scanf_s("%s\n %s", ans, gus);      //printf_s("Round %s &amp;s\n", ans, gus);      //yes = 0;      //win = 0, lose = 0, chances = 7;      //for (int i = 0; i &lt; strlen(gus); i++)       //{      //int flag = 0;      //for (int j = 0; j &lt; strlen(ans); j++)      //{      //if (ans[j] == gus[i])      //{      //yes++;      //flag = 1;//找到之后不退出，因为有一个有相同的字母      //}      //if (flag == 0)      //{      //chances--;      //}      //if (chances == 0)      //{      //lose = 1;      //printf_s("You lose.\n");        //}      //else if (yes == strlen(ans))      //{      //win = 1;      //printf_s("You win.\n");      //break;      //}      //if (win != 1 &amp;&amp; lose != 1)      //{      //printf("You chickened out.\n");      //}      //scanf_s("%d", &amp;times);      //}      //}      //}      int rnd;      while (scanf_s("%d%s%s", &amp;rnd, &amp;s, &amp;s2) == 3 &amp;&amp; rnd != -1)      {          printf_s("%Round %d\n", rnd);          win = lose = 0;          left = strlen(s);          chance = 7;          for (int i = 0; i &lt; strlen(s2); i++)          {              guess(s2[i]); //猜一个字母              if (win || lose) break; //检查状态            }          //根据结果进行输出          if (win) printf_s("You win.\n");          else if (lose) printf_s("You lose.\n");          else printf_s("You chickened out.\n");      }      system("pause");      return 0;  }    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LNMS</title>
      <link href="/2019/08/16/20190816-LNMS/"/>
      <url>/2019/08/16/20190816-LNMS/</url>
      
        <content type="html"><![CDATA[<h2 id="locality-NMS"><a href="#locality-NMS" class="headerlink" title="locality NMS"></a>locality NMS</h2><p>LNMS是在EAST文本检测中提出的．主要原因：文本检测面临的是成千上万个几何体，如果用普通的NMS，其计算复杂度，n是几何体的个数，这是不可接受的．对上述时间复杂度问题，EAST提出了基于行合并几何体的方法，当然这是基于邻近几个几何体是高度相关的假设．注意：这里合并的四边形坐标是通过两个给定四边形的得分进行加权平均的，也就是说这里是“平均”而不是”选择”几何体*,目的是减少计算量．<br>基本步骤<br>1.先对所有的output box集合结合相应的阈值（大于阈值则进行合并，小于阈值则不和并），依次遍历进行加权合并，得到合并后的bbox集合；<br>2.对合并后的bbox集合进行标准的NMS操作  </p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">def</span> <span class="title function_">detect</span>(<span class="params">score_map, geo_map, timer, score_map_thresh=<span class="number">1e-5</span>, box_thresh=<span class="number">1e-8</span>, nms_thres=<span class="number">0.1</span></span>):</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    restore text boxes from score map and geo map</span></span><br><span class="line"><span class="string">    :param score_map: bs* 128 * 128 * 1</span></span><br><span class="line"><span class="string">    :param geo_map: ## geo_map = bs * 128 * 128 * 5</span></span><br><span class="line"><span class="string">    :param timer:</span></span><br><span class="line"><span class="string">    :param score_map_thresh: threshhold for score map</span></span><br><span class="line"><span class="string">    :param box_thresh: threshhold for boxes</span></span><br><span class="line"><span class="string">    :param nms_thres: threshold for nms</span></span><br><span class="line"><span class="string">    :return:</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(score_map.shape) == <span class="number">4</span>:</span><br><span class="line">        score_map = score_map[<span class="number">0</span>, :, :, <span class="number">0</span>]</span><br><span class="line">        geo_map = geo_map[<span class="number">0</span>, :, :, ]</span><br><span class="line">    <span class="comment"># filter the score map</span></span><br><span class="line">    xy_text = np.argwhere(score_map &gt; score_map_thresh)</span><br><span class="line">    <span class="comment"># sort the text boxes via the y axis</span></span><br><span class="line">    xy_text = xy_text[np.argsort(xy_text[:, <span class="number">0</span>])]</span><br><span class="line">    <span class="comment"># restore</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    text_box_restored = restore_rectangle(xy_text[:, ::-<span class="number">1</span>]*<span class="number">4</span>, geo_map[xy_text[:, <span class="number">0</span>], xy_text[:, <span class="number">1</span>], :]) <span class="comment"># N*4*2</span></span><br><span class="line">    <span class="comment">#print('{} text boxes before nms'.format(text_box_restored.shape[0]))</span></span><br><span class="line">    boxes = np.zeros((text_box_restored.shape[<span class="number">0</span>], <span class="number">9</span>), dtype=np.float32)</span><br><span class="line">    boxes[:, :<span class="number">8</span>] = text_box_restored.reshape((-<span class="number">1</span>, <span class="number">8</span>))</span><br><span class="line">    boxes[:, <span class="number">8</span>] = score_map[xy_text[:, <span class="number">0</span>], xy_text[:, <span class="number">1</span>]]</span><br><span class="line">    timer[<span class="string">'restore'</span>] = time.time() - start</span><br><span class="line">    <span class="comment"># 得到box 的坐标以及分数</span></span><br><span class="line">    <span class="comment"># nms part</span></span><br><span class="line">    start = time.time()</span><br><span class="line">    <span class="comment"># boxes = nms_locality.nms_locality(boxes.astype(np.float64), nms_thres)</span></span><br><span class="line">    boxes = lanms.merge_quadrangle_n9(boxes.astype(<span class="string">'float32'</span>), nms_thres)</span><br><span class="line">    timer[<span class="string">'nms'</span>] = time.time() - start</span><br><span class="line">    <span class="keyword">if</span> boxes.shape[<span class="number">0</span>] == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="literal">None</span>, timer</span><br><span class="line">    <span class="comment"># here we filter some low score boxes by the average score map, this is different from the orginal paper</span></span><br><span class="line">    <span class="keyword">for</span> i, box <span class="keyword">in</span> <span class="built_in">enumerate</span>(boxes):</span><br><span class="line">        mask = np.zeros_like(score_map, dtype=np.uint8)</span><br><span class="line">        cv2.fillPoly(mask, box[:<span class="number">8</span>].reshape((-<span class="number">1</span>, <span class="number">4</span>, <span class="number">2</span>)).astype(np.int32) // <span class="number">4</span>, <span class="number">1</span>)</span><br><span class="line">        boxes[i, <span class="number">8</span>] = cv2.mean(score_map, mask)[<span class="number">0</span>]</span><br><span class="line">    boxes = boxes[boxes[:, <span class="number">8</span>] &gt; box_thresh]</span><br><span class="line">    <span class="keyword">return</span> boxes, timer</span><br></pre></td></tr></tbody></table></figure><hr><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> shapely.geometry <span class="keyword">import</span> Polygon</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">intersection</span>(<span class="params">g, p</span>):</span><br><span class="line">    <span class="comment">#取g,p中的几何体信息组成多边形</span></span><br><span class="line">    g = Polygon(g[:<span class="number">8</span>].reshape((<span class="number">4</span>, <span class="number">2</span>)))</span><br><span class="line">    p = Polygon(p[:<span class="number">8</span>].reshape((<span class="number">4</span>, <span class="number">2</span>)))</span><br><span class="line">    <span class="comment"># 判断g,p是否为有效的多边形几何体</span></span><br><span class="line">    <span class="keyword">if</span> <span class="keyword">not</span> g.is_valid <span class="keyword">or</span> <span class="keyword">not</span> p.is_valid:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="comment"># 取两个几何体的交集和并集</span></span><br><span class="line">    inter = Polygon(g).intersection(Polygon(p)).area</span><br><span class="line">    union = g.area + p.area - inter</span><br><span class="line">    <span class="keyword">if</span> union == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="number">0</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> inter/union</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">weighted_merge</span>(<span class="params">g, p</span>):</span><br><span class="line">    <span class="comment"># 取g,p两个几何体的加权（权重根据对应的检测得分计算得到）</span></span><br><span class="line">    g[:<span class="number">8</span>] = (g[<span class="number">8</span>] * g[:<span class="number">8</span>] + p[<span class="number">8</span>] * p[:<span class="number">8</span>])/(g[<span class="number">8</span>] + p[<span class="number">8</span>])</span><br><span class="line">    <span class="comment">#合并后的几何体的得分为两个几何体得分的总和</span></span><br><span class="line">    g[<span class="number">8</span>] = (g[<span class="number">8</span>] + p[<span class="number">8</span>])</span><br><span class="line">    <span class="keyword">return</span> g</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">standard_nms</span>(<span class="params">S, thres</span>):</span><br><span class="line">    <span class="comment">#标准NMS</span></span><br><span class="line">    order = np.argsort(S[:, <span class="number">8</span>])[::-<span class="number">1</span>]</span><br><span class="line">    keep = []</span><br><span class="line">    <span class="keyword">while</span> order.size &gt; <span class="number">0</span>:</span><br><span class="line">        i = order[<span class="number">0</span>]</span><br><span class="line">        keep.append(i)</span><br><span class="line">        ovr = np.array([intersection(S[i], S[t]) <span class="keyword">for</span> t <span class="keyword">in</span> order[<span class="number">1</span>:]])</span><br><span class="line">        inds = np.where(ovr &lt;= thres)[<span class="number">0</span>]</span><br><span class="line">        order = order[inds+<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">return</span> S[keep]</span><br><span class="line"><span class="keyword">def</span> <span class="title function_">nms_locality</span>(<span class="params">polys, thres=<span class="number">0.3</span></span>):</span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">    locality aware nms of EAST</span></span><br><span class="line"><span class="string">    :param polys: a N*9 numpy array. first 8 coordinates, then prob</span></span><br><span class="line"><span class="string">    :return: boxes after nms</span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    S = []    <span class="comment">#合并后的几何体集合</span></span><br><span class="line">    p = <span class="literal">None</span>   <span class="comment">#合并后的几何体</span></span><br><span class="line">    <span class="keyword">for</span> g <span class="keyword">in</span> polys:</span><br><span class="line">        <span class="keyword">if</span> p <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span> <span class="keyword">and</span> intersection(g, p) &gt; thres:    <span class="comment">#若两个几何体的相交面积大于指定的阈值，则进行合并</span></span><br><span class="line">            p = weighted_merge(g, p)</span><br><span class="line">        <span class="keyword">else</span>:    <span class="comment">#反之，则保留当前的几何体</span></span><br><span class="line">            <span class="keyword">if</span> p <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">                S.append(p)</span><br><span class="line">            p = g</span><br><span class="line">    <span class="keyword">if</span> p <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">None</span>:</span><br><span class="line">        S.append(p)</span><br><span class="line">    <span class="keyword">if</span> <span class="built_in">len</span>(S) == <span class="number">0</span>:</span><br><span class="line">        <span class="keyword">return</span> np.array([])</span><br><span class="line">    <span class="keyword">return</span> standard_nms(np.array(S), thres)</span><br><span class="line"><span class="keyword">if</span> __name__ == <span class="string">'__main__'</span>:</span><br><span class="line">    <span class="comment"># 343,350,448,135,474,143,369,359</span></span><br><span class="line">    <span class="built_in">print</span>(Polygon(np.array([[<span class="number">343</span>, <span class="number">350</span>], [<span class="number">448</span>, <span class="number">135</span>],</span><br><span class="line">                            [<span class="number">474</span>, <span class="number">143</span>], [<span class="number">369</span>, <span class="number">359</span>]])).area)</span><br></pre></td></tr></tbody></table></figure><p>参考博客： <a href="https://www.jianshu.com/p/4934875f7eb6">https://www.jianshu.com/p/4934875f7eb6</a></p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法竞赛入门经典第三章</title>
      <link href="/2019/08/16/20190816-algorithm3/"/>
      <url>/2019/08/16/20190816-algorithm3/</url>
      
        <content type="html"><![CDATA[<hr><p>今天心情不好，所以就敲了这一点代码，希望明天的状态能好一点！</p><figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br></pre></td><td class="code"><pre><span class="line">// t3.cpp : 定义控制台应用程序的入口点。</span><br><span class="line">//</span><br><span class="line"><span class="comment">#include "stdafx.h"</span></span><br><span class="line"><span class="comment">#include "stdio.h"</span></span><br><span class="line"><span class="comment">#include &lt;stdlib.h&gt;</span></span><br><span class="line"><span class="comment">#include&lt;math.h&gt;</span></span><br><span class="line"><span class="comment">#include&lt;time.h&gt;</span></span><br><span class="line"><span class="comment">#define maxn 10000000 +10</span></span><br><span class="line">char s[maxn];</span><br><span class="line"><span class="built_in">int</span> main()</span><br><span class="line">{</span><br><span class="line">//题目<span class="number">1</span> 统计个数</span><br><span class="line">/*<span class="built_in">int</span> count =<span class="number">0</span>;</span><br><span class="line"><span class="built_in">int</span> temp;</span><br><span class="line"><span class="keyword">while</span> (~scanf_s(<span class="string">"%d"</span>, &amp;temp))</span><br><span class="line">{</span><br><span class="line">count++;</span><br><span class="line">}</span><br><span class="line">printf_s(<span class="string">"%d"</span>, count);*/  //显示不出来</span><br><span class="line">//输入一些数，求最大值、最小值和平均数</span><br><span class="line">//<span class="built_in">int</span> <span class="built_in">min</span>, <span class="built_in">max</span>, n, <span class="built_in">sum</span>,count = <span class="number">0</span>;</span><br><span class="line">//<span class="built_in">float</span> avg;</span><br><span class="line">//scanf_s(<span class="string">"%d"</span>, &amp;n);</span><br><span class="line">//<span class="built_in">max</span> = n;</span><br><span class="line">//<span class="built_in">min</span> = n;</span><br><span class="line">//<span class="built_in">sum</span> = n;</span><br><span class="line">//count++;</span><br><span class="line">//<span class="keyword">while</span> (scanf_s(<span class="string">"%d"</span>, &amp;n)!=EOF)</span><br><span class="line">//{</span><br><span class="line">//count++;</span><br><span class="line">//<span class="built_in">sum</span> += n;</span><br><span class="line">//<span class="keyword">if</span> (n &gt; <span class="built_in">max</span>)</span><br><span class="line">//<span class="built_in">max</span> = n;</span><br><span class="line">//<span class="keyword">if</span> (n &lt; <span class="built_in">min</span>)</span><br><span class="line">//<span class="built_in">min</span> = n;</span><br><span class="line">//}</span><br><span class="line">//avg = <span class="built_in">sum</span>*<span class="number">1.0</span> / count;</span><br><span class="line">//printf_s(<span class="string">"%d %d %f\n"</span>, <span class="built_in">max</span>,<span class="built_in">min</span>,avg);</span><br><span class="line">//<span class="number">5.</span>输入一些数 ，求出他们的方差</span><br><span class="line">/*double ave, <span class="built_in">sum</span> = <span class="number">0</span>, varance, psum = <span class="number">0</span>;</span><br><span class="line"><span class="built_in">int</span> n.a[<span class="number">110</span>], count = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">while</span> (scanf_s(<span class="string">"%d"</span>, &amp;n) != EOF)</span><br><span class="line">{</span><br><span class="line">a[count++] = n;</span><br><span class="line"><span class="built_in">sum</span> += n;</span><br><span class="line">}</span><br><span class="line">avg = <span class="built_in">sum</span>*<span class="number">1.0</span> / count;</span><br><span class="line"><span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>; i &lt; count; i++)</span><br><span class="line">psum += (a[i] - ave)*(a[i] - ave);</span><br><span class="line">variance = psum / count;</span><br><span class="line"><span class="built_in">print</span>(<span class="string">"%lf"</span>, variance);*/</span><br><span class="line">scanf_s(<span class="string">"%s"</span>, s);</span><br><span class="line"><span class="built_in">int</span> tot = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>;s[i]; i++) {</span><br><span class="line"><span class="keyword">if</span> (s[i] == <span class="string">'1'</span>) tot++;</span><br><span class="line">}</span><br><span class="line">printf_s(<span class="string">"%d\n"</span>, tot);</span><br><span class="line">system(<span class="string">"pause"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>LNMS</title>
      <link href="/2019/08/16/LNMS/"/>
      <url>/2019/08/16/LNMS/</url>
      
        <content type="html"><![CDATA[<h2 id="locality-NMS"><a href="#locality-NMS" class="headerlink" title="locality NMS"></a>locality NMS</h2><p>LNMS是在EAST文本检测中提出的．主要原因：文本检测面临的是成千上万个几何体，如果用普通的NMS，其计算复杂度，n是几何体的个数，这是不可接受的．对上述时间复杂度问题，EAST提出了基于行合并几何体的方法，当然这是基于邻近几个几何体是高度相关的假设．注意：这里合并的四边形坐标是通过两个给定四边形的得分进行加权平均的，也就是说这里是“平均”而不是”选择”几何体*,目的是减少计算量．<br>基本步骤<br>1.先对所有的output box集合结合相应的阈值（大于阈值则进行合并，小于阈值则不和并），依次遍历进行加权合并，得到合并后的bbox集合；<br>2.对合并后的bbox集合进行标准的NMS操作  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  </code></pre><p>| </p><pre><code>def detect(score_map, geo_map, timer, score_map_thresh=1e-5, box_thresh=1e-8, nms_thres=0.1):      '''      restore text boxes from score map and geo map      :param score_map: bs* 128 * 128 * 1      :param geo_map: ## geo_map = bs * 128 * 128 * 5      :param timer:      :param score_map_thresh: threshhold for score map      :param box_thresh: threshhold for boxes      :param nms_thres: threshold for nms      :return:      '''      if len(score_map.shape) == 4:          score_map = score_map[0, :, :, 0]          geo_map = geo_map[0, :, :, ]      # filter the score map      xy_text = np.argwhere(score_map &gt; score_map_thresh)      # sort the text boxes via the y axis      xy_text = xy_text[np.argsort(xy_text[:, 0])]      # restore      start = time.time()      text_box_restored = restore_rectangle(xy_text[:, ::-1]*4, geo_map[xy_text[:, 0], xy_text[:, 1], :]) # N*4*2      #print('{} text boxes before nms'.format(text_box_restored.shape[0]))      boxes = np.zeros((text_box_restored.shape[0], 9), dtype=np.float32)      boxes[:, :8] = text_box_restored.reshape((-1, 8))      boxes[:, 8] = score_map[xy_text[:, 0], xy_text[:, 1]]      timer['restore'] = time.time() - start        # 得到box 的坐标以及分数            # nms part      start = time.time()      # boxes = nms_locality.nms_locality(boxes.astype(np.float64), nms_thres)      boxes = lanms.merge_quadrangle_n9(boxes.astype('float32'), nms_thres)      timer['nms'] = time.time() - start      if boxes.shape[0] == 0:          return None, timer        # here we filter some low score boxes by the average score map, this is different from the orginal paper      for i, box in enumerate(boxes):          mask = np.zeros_like(score_map, dtype=np.uint8)          cv2.fillPoly(mask, box[:8].reshape((-1, 4, 2)).astype(np.int32) // 4, 1)          boxes[i, 8] = cv2.mean(score_map, mask)[0]      boxes = boxes[boxes[:, 8] &gt; box_thresh]      return boxes, timer    </code></pre><p>—|—  </p><hr><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  </code></pre><p>| </p><pre><code>import numpy as np  from shapely.geometry import Polygon    def intersection(g, p):      #取g,p中的几何体信息组成多边形      g = Polygon(g[:8].reshape((4, 2)))      p = Polygon(p[:8].reshape((4, 2)))        # 判断g,p是否为有效的多边形几何体      if not g.is_valid or not p.is_valid:          return 0        # 取两个几何体的交集和并集      inter = Polygon(g).intersection(Polygon(p)).area      union = g.area + p.area - inter      if union == 0:          return 0      else:          return inter/union    def weighted_merge(g, p):      # 取g,p两个几何体的加权（权重根据对应的检测得分计算得到）      g[:8] = (g[8] * g[:8] + p[8] * p[:8])/(g[8] + p[8])        #合并后的几何体的得分为两个几何体得分的总和      g[8] = (g[8] + p[8])      return g    def standard_nms(S, thres):      #标准NMS      order = np.argsort(S[:, 8])[::-1]      keep = []      while order.size &gt; 0:          i = order[0]          keep.append(i)          ovr = np.array([intersection(S[i], S[t]) for t in order[1:]])          inds = np.where(ovr &lt;= thres)[0]          order = order[inds+1]        return S[keep]    def nms_locality(polys, thres=0.3):      '''      locality aware nms of EAST      :param polys: a N*9 numpy array. first 8 coordinates, then prob      :return: boxes after nms      '''      S = []    #合并后的几何体集合      p = None   #合并后的几何体      for g in polys:          if p is not None and intersection(g, p) &gt; thres:    #若两个几何体的相交面积大于指定的阈值，则进行合并              p = weighted_merge(g, p)          else:    #反之，则保留当前的几何体              if p is not None:                  S.append(p)              p = g      if p is not None:          S.append(p)      if len(S) == 0:          return np.array([])      return standard_nms(np.array(S), thres)    if __name__ == '__main__':      # 343,350,448,135,474,143,369,359      print(Polygon(np.array([[343, 350], [448, 135],                              [474, 143], [369, 359]])).area)    </code></pre><p>—|—  </p><p>参考博客： <a href="https://www.jianshu.com/p/4934875f7eb6">https://www.jianshu.com/p/4934875f7eb6</a></p>]]></content>
      
      
      <categories>
          
          <category> 深度学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> OpenCV </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法竞赛入门经典第三章</title>
      <link href="/2019/08/16/algorithm3/"/>
      <url>/2019/08/16/algorithm3/</url>
      
        <content type="html"><![CDATA[<hr><p>今天心情不好，所以就敲了这一点代码，希望明天的状态能好一点！</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  </code></pre><p>| </p><pre><code>// t3.cpp : 定义控制台应用程序的入口点。  //    #include "stdafx.h"  #include "stdio.h"  #include &lt;stdlib.h&gt;  #include&lt;math.h&gt;  #include&lt;time.h&gt;    #define maxn 10000000 +10  char s[maxn];  int main()  {      //题目1 统计个数        /*int count =0;      int temp;      while (~scanf_s("%d", &amp;temp))      {          count++;      }      printf_s("%d", count);*/  //显示不出来      //输入一些数，求最大值、最小值和平均数      //int min, max, n, sum,count = 0;      //float avg;      //scanf_s("%d", &amp;n);      //max = n;      //min = n;      //sum = n;      //count++;      //while (scanf_s("%d", &amp;n)!=EOF)       //{      //count++;      //sum += n;      //if (n &gt; max)      //max = n;      //if (n &lt; min)      //min = n;      //}      //avg = sum*1.0 / count;      //printf_s("%d %d %f\n", max,min,avg);      //5.输入一些数 ，求出他们的方差      /*double ave, sum = 0, varance, psum = 0;      int n.a[110], count = 0;      while (scanf_s("%d", &amp;n) != EOF)      {          a[count++] = n;          sum += n;      }      avg = sum*1.0 / count;      for (int i = 0; i &lt; count; i++)          psum += (a[i] - ave)*(a[i] - ave);      variance = psum / count;      print("%lf", variance);*/      scanf_s("%s", s);      int tot = 0;      for (int i = 0;s[i]; i++) {          if (s[i] == '1') tot++;      }      printf_s("%d\n", tot);      system("pause");      return 0;  }    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法竞赛入门经典第二章</title>
      <link href="/2019/08/15/20190815-algorithm2/"/>
      <url>/2019/08/15/20190815-algorithm2/</url>
      
        <content type="html"><![CDATA[<figure class="highlight python"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br><span class="line">156</span><br><span class="line">157</span><br><span class="line">158</span><br><span class="line">159</span><br><span class="line">160</span><br><span class="line">161</span><br><span class="line">162</span><br><span class="line">163</span><br><span class="line">164</span><br><span class="line">165</span><br><span class="line">166</span><br><span class="line">167</span><br></pre></td><td class="code"><pre><span class="line">// t2.cpp : 定义控制台应用程序的入口点。</span><br><span class="line">//</span><br><span class="line"><span class="comment">#include "stdafx.h"</span></span><br><span class="line"><span class="comment">#include "stdio.h"</span></span><br><span class="line"><span class="comment">#include &lt;stdlib.h&gt;</span></span><br><span class="line"><span class="comment">#include&lt;math.h&gt;</span></span><br><span class="line"><span class="comment">#include&lt;time.h&gt;</span></span><br><span class="line"><span class="built_in">int</span> main()</span><br><span class="line">{</span><br><span class="line">////3n+<span class="number">1</span>问题、</span><br><span class="line">//<span class="built_in">int</span> n, count = <span class="number">0</span>; //当n过大的时候 <span class="number">2</span>*n溢出</span><br><span class="line">//</span><br><span class="line">//scanf_s(<span class="string">"%d"</span>, &amp;n);</span><br><span class="line">//long long n2 = n;</span><br><span class="line">//<span class="keyword">while</span> (n2 &gt; <span class="number">1</span>)</span><br><span class="line">//{</span><br><span class="line">//<span class="keyword">if</span> (n2 % <span class="number">2</span> == <span class="number">1</span>) n2 = n2 * <span class="number">3</span> + <span class="number">1</span>;</span><br><span class="line">//<span class="keyword">else</span> n2 /= <span class="number">2</span>;</span><br><span class="line">//count++;</span><br><span class="line">//}</span><br><span class="line">//printf_s(<span class="string">"%d\n"</span>, count);</span><br><span class="line">//近似计算</span><br><span class="line">/*double <span class="built_in">sum</span> = <span class="number">0</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">0</span>;; i++) {</span><br><span class="line">double term = <span class="number">1.0</span> / (i * <span class="number">2</span> + <span class="number">1</span>);</span><br><span class="line"><span class="keyword">if</span> (i % <span class="number">2</span> == <span class="number">0</span>) <span class="built_in">sum</span> += term;</span><br><span class="line"><span class="keyword">else</span> <span class="built_in">sum</span> -= term;</span><br><span class="line"><span class="keyword">if</span> (term &lt; <span class="number">1e-6</span>) <span class="keyword">break</span>;</span><br><span class="line">}</span><br><span class="line">printf_s(<span class="string">"%.6f\n"</span>, <span class="built_in">sum</span>);*/</span><br><span class="line">// 阶乘之和 只保存后六位</span><br><span class="line">//<span class="built_in">int</span> n, S = <span class="number">0</span>;</span><br><span class="line">//scanf_s(<span class="string">"%d"</span>, &amp;n);</span><br><span class="line">//<span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">//{</span><br><span class="line">//<span class="built_in">int</span> factorial = <span class="number">1</span>;</span><br><span class="line">//<span class="keyword">for</span> (<span class="built_in">int</span> j = <span class="number">1</span>; j &lt;= i; j++)</span><br><span class="line">//factorial *= j;</span><br><span class="line">//S += factorial;</span><br><span class="line">//}</span><br><span class="line">//printf_s(<span class="string">"%d\n"</span>, S % <span class="number">1000000</span>);</span><br><span class="line">// 阶乘之和<span class="number">2</span>, 优化版本</span><br><span class="line">//<span class="built_in">int</span> n, S = <span class="number">0</span>;</span><br><span class="line">//const <span class="built_in">int</span> MOD = <span class="number">1000000</span>;</span><br><span class="line">//scanf_s(<span class="string">"%d"</span>, &amp;n);</span><br><span class="line">//<span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">1</span>; i &lt;= n; i++)</span><br><span class="line">//{</span><br><span class="line">//<span class="built_in">int</span> factorial = <span class="number">1</span>;</span><br><span class="line">//<span class="keyword">for</span> (<span class="built_in">int</span> j = <span class="number">1</span>; j &lt;= i; j++)</span><br><span class="line">//factorial = (factorial *j)%MOD;</span><br><span class="line">//S += factorial;</span><br><span class="line">//}</span><br><span class="line">//printf_s(<span class="string">"%d\n"</span>, S %MOD);</span><br><span class="line">//printf_s(<span class="string">"Time used = %.2f\n"</span>, (double)clock() / CLOCKS_PER_SEC); // 得到程序运行的时间 单位：秒</span><br><span class="line">// 数据统计</span><br><span class="line">    //习题<span class="number">2.1</span> 水仙花数目</span><br><span class="line">/*<span class="built_in">int</span> <span class="built_in">sum</span>;</span><br><span class="line"><span class="keyword">for</span> (<span class="built_in">int</span> i = <span class="number">1</span>; i &lt; <span class="number">10</span>; i++)</span><br><span class="line"><span class="keyword">for</span> (<span class="built_in">int</span> k = <span class="number">0</span>; k &lt; <span class="number">10</span>; k++)</span><br><span class="line"><span class="keyword">for</span> (<span class="built_in">int</span> j = <span class="number">0</span>; j &lt; <span class="number">10</span>; j++)</span><br><span class="line">{</span><br><span class="line"><span class="built_in">sum</span> = i * <span class="number">100</span> + <span class="number">10</span> * k + j;</span><br><span class="line"><span class="keyword">if</span>(<span class="built_in">sum</span> == i*i*i+j*j*j + k*k*k)</span><br><span class="line">printf_s(<span class="string">"%d "</span>, <span class="built_in">sum</span>);</span><br><span class="line">}*/</span><br><span class="line">//习题<span class="number">2.2</span> 韩信点兵 相传韩信才智过人，从不直接清点自己军队的人数，只要让士兵先后以三人一排、五人一排、七人一排地变换队形，</span><br><span class="line">//而他每次只掠一眼队伍的排尾就知道总人数了。输入包含多组数据，每组数据包含<span class="number">3</span>个非负整数a，b，c，表示每种队形排尾的人数（a＜<span class="number">3</span>，b＜<span class="number">5</span>，c＜<span class="number">7</span>），</span><br><span class="line">//输出总人数的最小值（或报告无解）。已知总人数不小于<span class="number">10</span>，不超过<span class="number">100</span>。输入到文件结束为止。</span><br><span class="line">//<span class="built_in">int</span> i, a, b, c;</span><br><span class="line">//scanf_s(<span class="string">"%d%d%d"</span>, &amp;a, &amp;b, &amp;c);</span><br><span class="line">//<span class="keyword">for</span> (i = <span class="number">0</span>; i &lt;= <span class="number">100</span>; i++) {</span><br><span class="line">//<span class="keyword">if</span> (i % <span class="number">3</span> == a &amp;&amp; i % <span class="number">5</span> == b &amp;&amp; i % <span class="number">7</span> == c)</span><br><span class="line">//printf_s(<span class="string">"%d\n"</span>, i);</span><br><span class="line">//</span><br><span class="line">//}</span><br><span class="line">//<span class="keyword">if</span> (i % <span class="number">3</span> != a &amp;&amp; i % <span class="number">5</span> != b &amp;&amp; i % <span class="number">7</span> != c &amp;&amp; i&gt;<span class="number">100</span>)</span><br><span class="line">//printf_s(<span class="string">"No answer\n"</span>);</span><br><span class="line">    //习题<span class="number">2.3</span> 倒三角形</span><br><span class="line">//<span class="built_in">int</span> n, /* 输出n行; n&lt;=<span class="number">20</span> */</span><br><span class="line">//i, /* 打印第i行 */</span><br><span class="line">//j;</span><br><span class="line">//scanf_s(<span class="string">"%d"</span>, &amp;n);</span><br><span class="line">//<span class="keyword">for</span> (i = <span class="number">1</span>; i &lt;= n; i = i + <span class="number">1</span>) {</span><br><span class="line">///* 在第i行，打印(i-<span class="number">1</span>)个空格 */</span><br><span class="line">//<span class="keyword">for</span> (j = <span class="number">1</span>; j &lt;= i - <span class="number">1</span>; j = j + <span class="number">1</span>)       printf_s(<span class="string">" "</span>);</span><br><span class="line">///* 在第i行，打印(<span class="number">2</span>*n-<span class="number">2</span>*i+<span class="number">1</span>)个<span class="comment"># */</span></span><br><span class="line">//<span class="keyword">for</span> (j = <span class="number">1</span>; j &lt;= (<span class="number">2</span> * n - <span class="number">2</span> * i + <span class="number">1</span>); j = j + <span class="number">1</span>)  printf_s(<span class="string">"#"</span>);</span><br><span class="line">//printf_s(<span class="string">"\n"</span>);  /* 输出结束后换行，否则所有的<span class="comment">#号在同一行输出 */</span></span><br><span class="line">//}</span><br><span class="line">//习题<span class="number">2.4</span> 子序列的和 输入两个正整数n＜m＜<span class="number">10</span> <span class="number">6</span> ，输出 ，保留<span class="number">5</span>位小数。输入包含多组数据， 注：陷阱就是在n特别大时如果直接n*n就会溢出，所以只能连除两次</span><br><span class="line">//<span class="built_in">int</span> count = <span class="number">0</span>;</span><br><span class="line">//<span class="keyword">while</span> (<span class="number">1</span>) {</span><br><span class="line">//<span class="built_in">int</span> n = <span class="number">0</span>;</span><br><span class="line">//<span class="built_in">int</span> m = <span class="number">0</span>;</span><br><span class="line">//scanf_s(<span class="string">"%d"</span>, &amp;n);</span><br><span class="line">//scanf_s(<span class="string">"%d"</span>, &amp;m);</span><br><span class="line">//<span class="keyword">if</span> (n == m&amp;&amp;n == <span class="number">0</span>) {</span><br><span class="line">//<span class="keyword">break</span>;</span><br><span class="line">//}</span><br><span class="line">//count++;</span><br><span class="line">//double <span class="built_in">sum</span> = <span class="number">0</span>;</span><br><span class="line">//<span class="keyword">for</span> (<span class="built_in">int</span> i = n; i &lt;= m; i++) {</span><br><span class="line">//<span class="built_in">sum</span> += <span class="number">1.0</span> / i / i;</span><br><span class="line">//}</span><br><span class="line">//printf_s(<span class="string">"Case %d:%.5f\n"</span>, count, <span class="built_in">sum</span>);</span><br><span class="line">//}</span><br><span class="line">    //习题<span class="number">2.5</span> 分数化小数（decimal）</span><br><span class="line">    //输入正整数a，b，c，输出a / b的小数形式，精确到小数点后c位。a，b≤<span class="number">10</span> ^ <span class="number">6</span>，c≤<span class="number">100</span>。输入包含多组数据，结束标记为a＝b＝c＝<span class="number">0</span>。</span><br><span class="line"> //   <span class="built_in">int</span> count = <span class="number">0</span>;</span><br><span class="line">//<span class="keyword">while</span> (<span class="number">1</span>) {</span><br><span class="line">//<span class="built_in">int</span> a, b, c;</span><br><span class="line">//<span class="built_in">int</span> k, d, i;</span><br><span class="line">//scanf_s(<span class="string">"%d"</span>, &amp;a);</span><br><span class="line">//scanf_s(<span class="string">"%d"</span>, &amp;b);</span><br><span class="line">//scanf_s(<span class="string">"%d"</span>, &amp;c);</span><br><span class="line">//<span class="keyword">if</span> (a == <span class="number">0</span>&amp;&amp;b == <span class="number">0</span> &amp;&amp;c==<span class="number">0</span> ) {</span><br><span class="line">//<span class="keyword">break</span>;</span><br><span class="line">//}</span><br><span class="line">//count++;</span><br><span class="line">//<span class="keyword">for</span> (i = <span class="number">0</span>; i&lt;c - <span class="number">1</span>; i++)</span><br><span class="line">//{</span><br><span class="line">///*机智地把余数放大十倍，使之除以b并取模*/</span><br><span class="line">//k = (k%b) * <span class="number">10</span>;</span><br><span class="line">//printf_s(<span class="string">"Case %d:%.5f\n"</span>, count, k / b);</span><br><span class="line">//}</span><br><span class="line">//k = (k%b) * <span class="number">10</span>;</span><br><span class="line">//d = (k%b) * <span class="number">10</span> / b;</span><br><span class="line">//<span class="keyword">if</span> (d &gt;= <span class="number">5</span>)//判断第c+<span class="number">1</span>位小数是否大于等于<span class="number">5</span>，<span class="keyword">if</span> yes,第c位小数要进<span class="number">1</span></span><br><span class="line">//{</span><br><span class="line">//printf_s(<span class="string">"Case %d:%.5f\n"</span>, count, k / b + <span class="number">1</span>);</span><br><span class="line">//}</span><br><span class="line">//<span class="keyword">else</span></span><br><span class="line">//{</span><br><span class="line">//printf_s(<span class="string">"Case %d:%.5f\n"</span>, count, k / b);</span><br><span class="line">//}</span><br><span class="line">//</span><br><span class="line">//}</span><br><span class="line">    <span class="built_in">int</span> abc,<span class="keyword">def</span>,ghi;</span><br><span class="line">    <span class="built_in">int</span> a[<span class="number">10</span>],count=<span class="number">0</span>;</span><br><span class="line">    memset(a,<span class="number">0</span>,sizeof(a)); // 将a数组中的值全部设置为<span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> (abc = <span class="number">123</span>;abc &lt; <span class="number">333</span>;abc ++) { // 基本可以确定abc的最小值和最大值</span><br><span class="line">        <span class="keyword">def</span> = <span class="number">2</span> * abc;</span><br><span class="line">        ghi = <span class="number">3</span> * abc;</span><br><span class="line">        // 设置数组中所有对应的<span class="number">9</span>位数字位置的值<span class="number">1</span></span><br><span class="line">        a[abc/<span class="number">100</span>] = <span class="number">1</span>; // a</span><br><span class="line">        a[abc/<span class="number">10</span>%<span class="number">10</span>] = <span class="number">1</span>; // b</span><br><span class="line">        a[abc%<span class="number">10</span>] = <span class="number">1</span>; // c</span><br><span class="line">        a[<span class="keyword">def</span>/<span class="number">100</span>] = <span class="number">1</span>; // d</span><br><span class="line">        a[<span class="keyword">def</span>/<span class="number">10</span>%<span class="number">10</span>] = <span class="number">1</span>; // e</span><br><span class="line">        a[<span class="keyword">def</span>%<span class="number">10</span>] = <span class="number">1</span>; // f</span><br><span class="line">        a[ghi/<span class="number">100</span>] = <span class="number">1</span>; // g</span><br><span class="line">        a[ghi/<span class="number">10</span>%<span class="number">10</span>] = <span class="number">1</span>; // h</span><br><span class="line">        a[ghi%<span class="number">10</span>] = <span class="number">1</span>; // i</span><br><span class="line">        <span class="built_in">int</span> i;</span><br><span class="line">        <span class="keyword">for</span> (i=<span class="number">1</span>;i&lt;=<span class="number">9</span>;i++) {</span><br><span class="line">            count += a[i];</span><br><span class="line">        }</span><br><span class="line">        <span class="keyword">if</span> (count == <span class="number">9</span>) {</span><br><span class="line">            printf_s(<span class="string">"%d %d %d\n"</span>,abc,<span class="keyword">def</span>,ghi);</span><br><span class="line">        }</span><br><span class="line">        // 重置count 和a数组</span><br><span class="line">        count = <span class="number">0</span>;</span><br><span class="line">        memset(a,<span class="number">0</span>,sizeof(a));</span><br><span class="line">    }</span><br><span class="line">system(<span class="string">"pause"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法竞赛入门经典第二章</title>
      <link href="/2019/08/15/algorithm2/"/>
      <url>/2019/08/15/algorithm2/</url>
      
        <content type="html"><![CDATA[<pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  111  112  113  114  115  116  117  118  119  120  121  122  123  124  125  126  127  128  129  130  131  132  133  134  135  136  137  138  139  140  141  142  143  144  145  146  147  148  149  150  151  152  153  154  155  156  157  158  159  160  161  162  163  164  165  166  167  168  169  170  171  172  173  174  175  176  177  178  179  180  181  182  183  184  185  186  187  </code></pre><p>| </p><pre><code>// t2.cpp : 定义控制台应用程序的入口点。  //  #include "stdafx.h"  #include "stdio.h"  #include &lt;stdlib.h&gt;  #include&lt;math.h&gt;  #include&lt;time.h&gt;    int main()  {      ////3n+1问题、      //int n, count = 0; //当n过大的时候 2*n溢出      //      //scanf_s("%d", &amp;n);      //long long n2 = n;      //while (n2 &gt; 1)      //{      //if (n2 % 2 == 1) n2 = n2 * 3 + 1;      //else n2 /= 2;      //count++;      //}      //printf_s("%d\n", count);          //近似计算      /*double sum = 0;      for (int i = 0;; i++) {          double term = 1.0 / (i * 2 + 1);          if (i % 2 == 0) sum += term;          else sum -= term;          if (term &lt; 1e-6) break;      }      printf_s("%.6f\n", sum);*/      // 阶乘之和 只保存后六位      //int n, S = 0;      //scanf_s("%d", &amp;n);      //for (int i = 1; i &lt;= n; i++)      //{      //int factorial = 1;      //for (int j = 1; j &lt;= i; j++)      //factorial *= j;      //S += factorial;          //}      //printf_s("%d\n", S % 1000000);      // 阶乘之和2, 优化版本      //int n, S = 0;      //const int MOD = 1000000;      //scanf_s("%d", &amp;n);      //for (int i = 1; i &lt;= n; i++)      //{      //int factorial = 1;      //for (int j = 1; j &lt;= i; j++)      //factorial = (factorial *j)%MOD;      //S += factorial;          //}      //printf_s("%d\n", S %MOD);      //printf_s("Time used = %.2f\n", (double)clock() / CLOCKS_PER_SEC); // 得到程序运行的时间 单位：秒      // 数据统计      //习题2.1 水仙花数目      /*int sum;      for (int i = 1; i &lt; 10; i++)          for (int k = 0; k &lt; 10; k++)              for (int j = 0; j &lt; 10; j++)              {                  sum = i * 100 + 10 * k + j;                  if(sum == i*i*i+j*j*j + k*k*k)                      printf_s("%d ", sum);              }*/      //习题2.2 韩信点兵 相传韩信才智过人，从不直接清点自己军队的人数，只要让士兵先后以三人一排、五人一排、七人一排地变换队形，      //而他每次只掠一眼队伍的排尾就知道总人数了。输入包含多组数据，每组数据包含3个非负整数a，b，c，表示每种队形排尾的人数（a＜3，b＜5，c＜7），      //输出总人数的最小值（或报告无解）。已知总人数不小于10，不超过100。输入到文件结束为止。      //int i, a, b, c;      //scanf_s("%d%d%d", &amp;a, &amp;b, &amp;c);      //for (i = 0; i &lt;= 100; i++) {      //if (i % 3 == a &amp;&amp; i % 5 == b &amp;&amp; i % 7 == c)      //printf_s("%d\n", i);      //      //}      //if (i % 3 != a &amp;&amp; i % 5 != b &amp;&amp; i % 7 != c &amp;&amp; i&gt;100)      //printf_s("No answer\n");      //习题2.3 倒三角形      //int n, /* 输出n行; n&lt;=20 */      //i, /* 打印第i行 */      //j;      //scanf_s("%d", &amp;n);      //for (i = 1; i &lt;= n; i = i + 1) {      ///* 在第i行，打印(i-1)个空格 */      //for (j = 1; j &lt;= i - 1; j = j + 1)       printf_s(" ");      ///* 在第i行，打印(2*n-2*i+1)个# */      //for (j = 1; j &lt;= (2 * n - 2 * i + 1); j = j + 1)  printf_s("#");      //printf_s("\n");  /* 输出结束后换行，否则所有的#号在同一行输出 */      //}      //习题2.4 子序列的和 输入两个正整数n＜m＜10 6 ，输出 ，保留5位小数。输入包含多组数据， 注：陷阱就是在n特别大时如果直接n*n就会溢出，所以只能连除两次      //int count = 0;      //while (1) {      //int n = 0;      //int m = 0;      //scanf_s("%d", &amp;n);      //scanf_s("%d", &amp;m);      //if (n == m&amp;&amp;n == 0) {      //break;      //}      //count++;      //double sum = 0;      //for (int i = n; i &lt;= m; i++) {      //sum += 1.0 / i / i;      //}      //printf_s("Case %d:%.5f\n", count, sum);      //}      //习题2.5 分数化小数（decimal）       //输入正整数a，b，c，输出a / b的小数形式，精确到小数点后c位。a，b≤10 ^ 6，c≤100。输入包含多组数据，结束标记为a＝b＝c＝0。             //   int count = 0;      //while (1) {      //int a, b, c;      //int k, d, i;      //scanf_s("%d", &amp;a);      //scanf_s("%d", &amp;b);      //scanf_s("%d", &amp;c);      //if (a == 0&amp;&amp;b == 0 &amp;&amp;c==0 ) {      //break;      //}      //count++;      //for (i = 0; i&lt;c - 1; i++)      //{      ///*机智地把余数放大十倍，使之除以b并取模*/      //k = (k%b) * 10;      //printf_s("Case %d:%.5f\n", count, k / b);      //}      //k = (k%b) * 10;      //d = (k%b) * 10 / b;      //if (d &gt;= 5)//判断第c+1位小数是否大于等于5，if yes,第c位小数要进1       //{      //printf_s("Case %d:%.5f\n", count, k / b + 1);      //}      //else      //{      //printf_s("Case %d:%.5f\n", count, k / b);      //}      //      //}      int abc,def,ghi;      int a[10],count=0;        memset(a,0,sizeof(a)); // 将a数组中的值全部设置为0        for (abc = 123;abc &lt; 333;abc ++) { // 基本可以确定abc的最小值和最大值          def = 2 * abc;          ghi = 3 * abc;            // 设置数组中所有对应的9位数字位置的值1          a[abc/100] = 1; // a          a[abc/10%10] = 1; // b          a[abc%10] = 1; // c            a[def/100] = 1; // d          a[def/10%10] = 1; // e          a[def%10] = 1; // f            a[ghi/100] = 1; // g          a[ghi/10%10] = 1; // h          a[ghi%10] = 1; // i            int i;          for (i=1;i&lt;=9;i++) {              count += a[i];          }            if (count == 9) {              printf_s("%d %d %d\n",abc,def,ghi);          }            // 重置count 和a数组          count = 0;          memset(a,0,sizeof(a));      }          system("pause");      return 0;          }    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法竞赛入门经典第一章</title>
      <link href="/2019/08/14/20190814-algorithm1/"/>
      <url>/2019/08/14/20190814-algorithm1/</url>
      
        <content type="html"><![CDATA[<figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// t1.cpp : 定义控制台应用程序的入口点。</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">"stdafx.h"</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">"stdio.h"</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stdlib.h&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;math.h&gt;</span></span></span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line"><span class="comment">//printf("%.1f\n", 8.0 / 5.0);  // %点后面是保留几位浮点小数1.6</span></span><br><span class="line"><span class="comment">//printf("%.1f\n", 8 / 5);  // %点后面是保留几位浮点小数 结果为0</span></span><br><span class="line"><span class="comment">//printf("%d\n", 8.0 / 5.0);  // 整数值用%d输出，实数用%f输出 # int 是2字节 ，float是4字节 溢出</span></span><br><span class="line"><span class="comment">////整数/整数等与整数，浮点数除以浮点数等于浮点数</span></span><br><span class="line"><span class="comment">//printf("%.8f\n", 1 + 2 * sqrt(3) / (5 - 0.1));</span></span><br><span class="line"><span class="comment">//int a, b;</span></span><br><span class="line"><span class="comment">//scanf_s("%d%d", &amp;a, &amp;b);  //scanf_s 中的占位符要和变量的数据类型一一对应，要在变量前面加上“&amp;”符号</span></span><br><span class="line"><span class="comment">//printf("%d\n", a+b);</span></span><br><span class="line"><span class="comment">//const double pi = acos(-1.0);</span></span><br><span class="line"><span class="comment">//double r, h, s1, s2, s;</span></span><br><span class="line"><span class="comment">//scanf_s("%1f%1f", &amp;r, &amp;h);</span></span><br><span class="line"><span class="comment">//s1 = pi*r*r;</span></span><br><span class="line"><span class="comment">//s2 = 2 * pi*r*h;</span></span><br><span class="line"><span class="comment">//s = s1*2.0 + s2;</span></span><br><span class="line"><span class="comment">//printf_s("Area = %.3f\n", s);</span></span><br><span class="line"><span class="comment">//交换变量</span></span><br><span class="line"><span class="comment">//int a, b, m, n;</span></span><br><span class="line"><span class="comment">//scanf_s("%d%d", &amp;a, &amp;b);</span></span><br><span class="line"><span class="comment">///*a = a + b;</span></span><br><span class="line"><span class="comment">//b = a - b;</span></span><br><span class="line"><span class="comment">//a = a - b;*/</span></span><br><span class="line"><span class="comment">//printf_s("%d %d\n", b, a);</span></span><br><span class="line"><span class="comment">//int a, b, m, n;</span></span><br><span class="line"><span class="comment">//scanf_s("%d%d", &amp;n, &amp;m);</span></span><br><span class="line"><span class="comment">///*a = a + b;</span></span><br><span class="line"><span class="comment">//b = a - b;</span></span><br><span class="line"><span class="comment">//a = a - b;*/</span></span><br><span class="line"><span class="comment">//a = (4*n-m) / 2;</span></span><br><span class="line"><span class="comment">//b = n - a;</span></span><br><span class="line"><span class="comment">//if (m % 2 == 1 || a &lt; 0 || b &lt; 0)</span></span><br><span class="line"><span class="comment">//printf_s("No answer!\n");</span></span><br><span class="line"><span class="comment">//else</span></span><br><span class="line"><span class="comment">//printf_s("Answer is chicken %d   rabbit  %d\n", a, b);</span></span><br><span class="line"><span class="comment">//exp1 输入三个整数，求出他们的平均数，保留三位小数</span></span><br><span class="line"><span class="comment">//int a, b, c;</span></span><br><span class="line"><span class="comment">//scanf_s("%d%d%d", &amp;a, &amp;b, &amp;c);</span></span><br><span class="line"><span class="comment">//printf_s("%.3f", (a + b + c) / 3.0);</span></span><br><span class="line"><span class="comment">//exp2 输入华氏温度f,输出对应的摄氏温度c,保留3位小数.提示 : c = 5(f-32)/9.</span></span><br><span class="line"><span class="comment">//float f, c;</span></span><br><span class="line"><span class="comment">//scanf_s("%f", &amp;f);</span></span><br><span class="line"><span class="comment">//c = 5*(f - 32) / 9.0;</span></span><br><span class="line"><span class="comment">//printf_s("摄制温度为： %.3f", c);</span></span><br><span class="line"><span class="comment">//exp3</span></span><br><span class="line"><span class="comment">//int  n;</span></span><br><span class="line"><span class="comment">//scanf_s("%d", &amp;n);</span></span><br><span class="line"><span class="comment">//printf_s("%d", n*(n + 1) / 2);</span></span><br><span class="line"><span class="comment">//exp4  输入正整数《360 ，输出正弦余弦函数值。提示：使用数学函数</span></span><br><span class="line"><span class="comment">//int  n;</span></span><br><span class="line"> <span class="comment">//   #define PI 3.1415926</span></span><br><span class="line"><span class="comment">//scanf_s("%d", &amp;n);</span></span><br><span class="line"><span class="comment">//if (n &lt; 0 || n&gt;360)</span></span><br><span class="line"><span class="comment">//printf_s("error");</span></span><br><span class="line"><span class="comment">//else</span></span><br><span class="line"><span class="comment">//{</span></span><br><span class="line"><span class="comment">//printf_s("%.2f\n", cos(n*PI/180));</span></span><br><span class="line"><span class="comment">//printf_s("%.2f", sin(n*PI / 180));</span></span><br><span class="line"><span class="comment">//}</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line">    <span class="comment">// exp5 一件衣服95元，若消费满300元可以打85折，输入购买衣服的简书，输出需要支付的金额。保留两位小数</span></span><br><span class="line"><span class="comment">//int n,money;</span></span><br><span class="line"><span class="comment">//scanf_s("%d", &amp;n);</span></span><br><span class="line"><span class="comment">//money = n * 95;</span></span><br><span class="line"><span class="comment">//if (money &gt; 300)</span></span><br><span class="line"><span class="comment">//printf_s("%.2f", money* 0.85);</span></span><br><span class="line"><span class="comment">//else</span></span><br><span class="line"><span class="comment">//printf_s("%.2f", money);</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//exp6  判断三个边是否能构成直角三角形 边长均为正整数</span></span><br><span class="line"><span class="comment">//int a, b, c;</span></span><br><span class="line"><span class="comment">//scanf_s("%d%d%d", &amp;a, &amp;b, &amp;c);</span></span><br><span class="line"><span class="comment">//if (((a*a + b*b)== c*c) || ((a*a + c*c) == b*b) || ((c*c + b*b) == a*a))</span></span><br><span class="line"><span class="comment">//printf_s("yes\n");</span></span><br><span class="line"><span class="comment">//else</span></span><br><span class="line"><span class="comment">//printf_s("not a triangle\n");</span></span><br><span class="line"><span class="comment">//exp7 判断一个年是不是闰年 能被4整除但是不能被100整除的年份</span></span><br><span class="line"><span class="comment">//int n;</span></span><br><span class="line"><span class="comment">//scanf_s("%d", &amp;n);</span></span><br><span class="line"><span class="comment">//if (n % 4 == 0 &amp;&amp; n % 100 != 0)</span></span><br><span class="line"><span class="comment">//printf_s("yes\n");</span></span><br><span class="line"><span class="comment">//else</span></span><br><span class="line"><span class="comment">//printf_s("no\n");</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="comment">//q1: int类型的最大值和最小值是多少 2^(n - 1)   - 1 -2^(n - 1)</span></span><br><span class="line"><span class="comment">//q2; double浮点数能精确到多少位小数？ 遵循IEEE标准的8字节（64位）的double能表示的有效数字的位数是：15 ~ 16</span></span><br><span class="line"><span class="comment">//q3: double类型的最大正数值和最小正数值</span></span><br><span class="line"><span class="comment">//q4: 逻辑运算符号的优先级</span></span><br><span class="line"><span class="built_in">system</span>(<span class="string">"pause"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><p>思考题解析： <a href="https://blog.csdn.net/panderang/article/details/54096426">https://blog.csdn.net/panderang/article/details/54096426</a></p>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>算法竞赛入门经典第一章</title>
      <link href="/2019/08/14/algorithm1/"/>
      <url>/2019/08/14/algorithm1/</url>
      
        <content type="html"><![CDATA[<pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89  90  91  92  93  94  95  96  97  98  99  100  101  102  103  104  105  106  107  108  109  110  </code></pre><p>| </p><pre><code>// t1.cpp : 定义控制台应用程序的入口点。  //    #include "stdafx.h"  #include "stdio.h"  #include &lt;stdlib.h&gt;  #include&lt;math.h&gt;       int main()  {      //printf("%.1f\n", 8.0 / 5.0);  // %点后面是保留几位浮点小数1.6      //printf("%.1f\n", 8 / 5);  // %点后面是保留几位浮点小数 结果为0      //printf("%d\n", 8.0 / 5.0);  // 整数值用%d输出，实数用%f输出 # int 是2字节 ，float是4字节 溢出      ////整数/整数等与整数，浮点数除以浮点数等于浮点数      //printf("%.8f\n", 1 + 2 * sqrt(3) / (5 - 0.1));      //int a, b;      //scanf_s("%d%d", &amp;a, &amp;b);  //scanf_s 中的占位符要和变量的数据类型一一对应，要在变量前面加上“&amp;”符号      //printf("%d\n", a+b);        //const double pi = acos(-1.0);      //double r, h, s1, s2, s;      //scanf_s("%1f%1f", &amp;r, &amp;h);      //s1 = pi*r*r;      //s2 = 2 * pi*r*h;      //s = s1*2.0 + s2;      //printf_s("Area = %.3f\n", s);      //交换变量      //int a, b, m, n;      //scanf_s("%d%d", &amp;a, &amp;b);      ///*a = a + b;      //b = a - b;      //a = a - b;*/      //printf_s("%d %d\n", b, a);      //int a, b, m, n;      //scanf_s("%d%d", &amp;n, &amp;m);      ///*a = a + b;      //b = a - b;      //a = a - b;*/      //a = (4*n-m) / 2;      //b = n - a;      //if (m % 2 == 1 || a &lt; 0 || b &lt; 0)      //printf_s("No answer!\n");      //else      //printf_s("Answer is chicken %d   rabbit  %d\n", a, b);      //exp1 输入三个整数，求出他们的平均数，保留三位小数      //int a, b, c;      //scanf_s("%d%d%d", &amp;a, &amp;b, &amp;c);      //printf_s("%.3f", (a + b + c) / 3.0);      //exp2 输入华氏温度f,输出对应的摄氏温度c,保留3位小数.提示 : c = 5(f-32)/9.      //float f, c;      //scanf_s("%f", &amp;f);      //c = 5*(f - 32) / 9.0;      //printf_s("摄制温度为： %.3f", c);      //exp3      //int  n;      //scanf_s("%d", &amp;n);      //printf_s("%d", n*(n + 1) / 2);      //exp4  输入正整数《360 ，输出正弦余弦函数值。提示：使用数学函数        //int  n;     //   #define PI 3.1415926        //scanf_s("%d", &amp;n);      //if (n &lt; 0 || n&gt;360)      //printf_s("error");      //else      //{      //printf_s("%.2f\n", cos(n*PI/180));      //printf_s("%.2f", sin(n*PI / 180));      //}      //      // exp5 一件衣服95元，若消费满300元可以打85折，输入购买衣服的简书，输出需要支付的金额。保留两位小数      //int n,money;      //scanf_s("%d", &amp;n);      //money = n * 95;      //if (money &gt; 300)      //printf_s("%.2f", money* 0.85);      //else      //printf_s("%.2f", money);      //      //exp6  判断三个边是否能构成直角三角形 边长均为正整数      //int a, b, c;      //scanf_s("%d%d%d", &amp;a, &amp;b, &amp;c);      //if (((a*a + b*b)== c*c) || ((a*a + c*c) == b*b) || ((c*c + b*b) == a*a))      //printf_s("yes\n");      //else      //printf_s("not a triangle\n");      //exp7 判断一个年是不是闰年 能被4整除但是不能被100整除的年份      //int n;      //scanf_s("%d", &amp;n);      //if (n % 4 == 0 &amp;&amp; n % 100 != 0)      //printf_s("yes\n");      //else      //printf_s("no\n");      //      //q1: int类型的最大值和最小值是多少 2^(n - 1)   - 1 -2^(n - 1)      //q2; double浮点数能精确到多少位小数？ 遵循IEEE标准的8字节（64位）的double能表示的有效数字的位数是：15 ~ 16      //q3: double类型的最大正数值和最小正数值      //q4: 逻辑运算符号的优先级                                 system("pause");        return 0;  }    </code></pre><p>—|—  </p><p>思考题解析： <a href="https://blog.csdn.net/panderang/article/details/54096426">https://blog.csdn.net/panderang/article/details/54096426</a></p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文本检测调研</title>
      <link href="/2019/07/29/20190729-%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E8%B0%83%E7%A0%94/"/>
      <url>/2019/07/29/20190729-%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E8%B0%83%E7%A0%94/</url>
      
        <content type="html"><![CDATA[<p>文本检测问题，广义上来说可以看做是一个目标检测的问题，但是相当于目标检测要简单的多。因为目标检测往往除了背景还有其他的类，而文本检测，只需要检测背景和文本类两个问题。因此可以采用目标检测或者分割的方法来进行文本检测。<br>而视频中的文本检测，也可以看做是视频中的目标检测中的一种，感觉应该也可以用视频中的目标检测+跟踪来做。</p><h4 id="通常目标跟踪面临的极大难点："><a href="#通常目标跟踪面临的极大难点：" class="headerlink" title="通常目标跟踪面临的极大难点："></a>通常目标跟踪面临的极大难点：</h4><p>物体变形、亮度变化、快速移动、背景干扰覆盖。其中最主要的三个难题分别是目标背景的变化，物体本身的变化，光照强度的变化。</p><h5 id="光流法"><a href="#光流法" class="headerlink" title="光流法"></a>光流法</h5><h5 id="帧间差分法"><a href="#帧间差分法" class="headerlink" title="帧间差分法"></a>帧间差分法</h5><h5 id="背景差分法"><a href="#背景差分法" class="headerlink" title="背景差分法"></a>背景差分法</h5>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 计算机视觉 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>文本检测调研</title>
      <link href="/2019/07/29/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E8%B0%83%E7%A0%94/"/>
      <url>/2019/07/29/%E6%96%87%E6%9C%AC%E6%A3%80%E6%B5%8B%E8%B0%83%E7%A0%94/</url>
      
        <content type="html"><![CDATA[<p>文本检测问题，广义上来说可以看做是一个目标检测的问题，但是相当于目标检测要简单的多。因为目标检测往往除了背景还有其他的类，而文本检测，只需要检测背景和文本类两个问题。因此可以采用目标检测或者分割的方法来进行文本检测。<br>而视频中的文本检测，也可以看做是视频中的目标检测中的一种，感觉应该也可以用视频中的目标检测+跟踪来做。</p><h4 id="通常目标跟踪面临的极大难点："><a href="#通常目标跟踪面临的极大难点：" class="headerlink" title="通常目标跟踪面临的极大难点："></a>通常目标跟踪面临的极大难点：</h4><p>物体变形、亮度变化、快速移动、背景干扰覆盖。其中最主要的三个难题分别是目标背景的变化，物体本身的变化，光照强度的变化。</p><h5 id="光流法"><a href="#光流法" class="headerlink" title="光流法"></a>光流法</h5><h5 id="帧间差分法"><a href="#帧间差分法" class="headerlink" title="帧间差分法"></a>帧间差分法</h5><h5 id="背景差分法"><a href="#背景差分法" class="headerlink" title="背景差分法"></a>背景差分法</h5>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 目标检测 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>c++中级教程 顺序容器的定义</title>
      <link href="/2019/07/26/20190726-%E9%A1%BA%E5%BA%8F%E5%AE%B9%E5%99%A8%E7%9A%84%E5%AE%9A%E4%B9%89/"/>
      <url>/2019/07/26/20190726-%E9%A1%BA%E5%BA%8F%E5%AE%B9%E5%99%A8%E7%9A%84%E5%AE%9A%E4%B9%89/</url>
      
        <content type="html"><![CDATA[<h2 id="顺序容器的定义"><a href="#顺序容器的定义" class="headerlink" title="顺序容器的定义"></a>顺序容器的定义</h2><ul><li><h3 id="顺序容器"><a href="#顺序容器" class="headerlink" title="顺序容器"></a>顺序容器</h3></li><li><p>vector</p></li><li><p>list</p></li><li><p>deque</p></li><li><h3 id="顺序容器适配器"><a href="#顺序容器适配器" class="headerlink" title="顺序容器适配器"></a>顺序容器适配器</h3></li><li><p>stack</p></li><li><p>queue</p></li><li><p>priority_queue</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>c++中级教程  STL queue</title>
      <link href="/2019/07/26/20190726-STLqueue/"/>
      <url>/2019/07/26/20190726-STLqueue/</url>
      
        <content type="html"><![CDATA[<h2 id="STL-queue"><a href="#STL-queue" class="headerlink" title="STL queue"></a>STL queue</h2><ul><li>队列： FIFO 先进先出</li><li>自适应容器（容器适配器）</li><li>栈适配器 STL queue</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">queue&lt;int, deque&lt;int&gt;&gt;   q;</span><br><span class="line">queue&lt;int, list&lt;int&gt;&gt;   q;</span><br><span class="line">q.empty()</span><br><span class="line">q.size()</span><br><span class="line">q.front()</span><br><span class="line">q.back()</span><br><span class="line">q.pop()</span><br><span class="line">q.push(item)</span><br></pre></td></tr></tbody></table></figure><h3 id="可以用list和deque做queue"><a href="#可以用list和deque做queue" class="headerlink" title="可以用list和deque做queue"></a>可以用list和deque做queue</h3><p>先进先出，后进后出  </p><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// queue.cpp : 定义控制台应用程序的入口点。</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">"stdafx.h"</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;list&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;deque&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">queue&lt;<span class="type">int</span>, deque&lt;<span class="type">int</span>&gt;&gt; a;</span><br><span class="line">queue&lt;<span class="type">int</span>, list&lt;<span class="type">int</span>&gt;&gt; b;</span><br><span class="line"><span class="comment">//queue&lt;int, vector&lt;int&gt;&gt; c;  不可以，因为vector不能进行两端操作</span></span><br><span class="line"><span class="comment">//队列有什么用途？？？</span></span><br><span class="line">queue&lt;<span class="type">int</span>&gt; q;</span><br><span class="line">q.<span class="built_in">push</span>(<span class="number">10</span>);</span><br><span class="line">q.<span class="built_in">push</span>(<span class="number">5</span>);</span><br><span class="line">q.<span class="built_in">push</span>(<span class="number">-1</span>);</span><br><span class="line">q.<span class="built_in">push</span>(<span class="number">20</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">"现在队列里有"</span> &lt;&lt; q.<span class="built_in">size</span>() &lt;&lt; <span class="string">"个数据 "</span> &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="string">"队首的数据："</span> &lt;&lt; q.<span class="built_in">front</span>() &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; <span class="string">"队尾的数据："</span> &lt;&lt; q.<span class="built_in">back</span>() &lt;&lt; endl;</span><br><span class="line">q.<span class="built_in">pop</span>();</span><br><span class="line">cout &lt;&lt; <span class="string">"新的队首的数据："</span> &lt;&lt; q.<span class="built_in">front</span>() &lt;&lt; endl;</span><br><span class="line"><span class="keyword">while</span> (q.<span class="built_in">size</span>() != <span class="number">0</span>)</span><br><span class="line">{</span><br><span class="line">cout &lt;&lt; <span class="string">" 删除"</span> &lt;&lt; a.<span class="built_in">front</span>() &lt;&lt; endl;</span><br><span class="line">q.<span class="built_in">pop</span>();</span><br><span class="line">}</span><br><span class="line"><span class="keyword">if</span> (q.<span class="built_in">empty</span>())</span><br><span class="line">{</span><br><span class="line">cout &lt;&lt; <span class="string">"队列为空！"</span>&lt;&lt;endl;</span><br><span class="line">}</span><br><span class="line"><span class="built_in">system</span>(<span class="string">"pause"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><h2 id="优先级队列-priority-queue"><a href="#优先级队列-priority-queue" class="headerlink" title="优先级队列 priority_queue"></a>优先级队列 priority_queue</h2><ul><li>自适应容器（容器适配器）：不能使用list</li><li>最大值优先级队列、最小值优先级队列(值越大，优先级越高，值越小优先级越高)</li><li>优先级队列适配器 STL priority_queue</li></ul><hr><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">priority_queue&lt;int, deque&lt;int&gt;&gt; pg1;</span><br><span class="line">//对队列里的数据进行随机操作，所以不能使用list</span><br><span class="line">priority_queue&lt;int, vector&lt;int&gt;&gt; pg2; //vector是默认的</span><br><span class="line">//谓词</span><br><span class="line">priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt; pg2; //vector是默认的，最小优先队列</span><br><span class="line">pg.empty()</span><br><span class="line">pg.size()</span><br><span class="line">pg.top()</span><br><span class="line">pg.pop()</span><br><span class="line">pg.push(item)</span><br><span class="line">~~~~~~~~~~</span><br></pre></td></tr></tbody></table></figure><hr><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// priority_queue.cpp : 定义控制台应用程序的入口点。</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">"stdafx.h"</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;deque&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;queue&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">priority_queue&lt;<span class="type">int</span>, deque&lt;<span class="type">int</span>&gt;&gt; pg1;</span><br><span class="line">    <span class="comment">//对队列里的数据进行随机操作，所以不能使用list</span></span><br><span class="line">priority_queue&lt;<span class="type">int</span>, vector&lt;<span class="type">int</span>&gt;&gt; pg2; <span class="comment">//vector是默认的</span></span><br><span class="line"><span class="comment">//谓词</span></span><br><span class="line">priority_queue&lt;<span class="type">int</span>, vector&lt;<span class="type">int</span>&gt;, greater&lt;<span class="type">int</span>&gt; pg2; <span class="comment">//vector是默认的，最小优先队列</span></span><br><span class="line">pg2.<span class="built_in">push</span>(<span class="number">10</span>);</span><br><span class="line">pg2.<span class="built_in">push</span>(<span class="number">5</span>);</span><br><span class="line">pg2.<span class="built_in">push</span>(<span class="number">-1</span>);</span><br><span class="line">pg2.<span class="built_in">push</span>(<span class="number">20</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">"优先级队列一共有： "</span> &lt;&lt; pg2.<span class="built_in">size</span>() &lt;&lt; <span class="string">"个数据"</span> &lt;&lt; endl;</span><br><span class="line">cout &lt;&lt; pg2.<span class="built_in">top</span>() &lt;&lt; endl;</span><br><span class="line"><span class="keyword">while</span> (!pg2.<span class="built_in">empty</span>())</span><br><span class="line">{</span><br><span class="line">cout &lt;&lt; <span class="string">"从优先级队列里删除： "</span> &lt;&lt; pg2.<span class="built_in">top</span>() &lt;&lt; endl;</span><br><span class="line">pg2.<span class="built_in">pop</span>();</span><br><span class="line">}</span><br><span class="line"><span class="built_in">system</span>(<span class="string">"pause"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br><span class="line">优先级队列一共有： <span class="number">4</span>个数据</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">从优先级队列里删除： 20</span><br><span class="line">从优先级队列里删除： 10</span><br><span class="line">从优先级队列里删除： 5</span><br><span class="line">从优先级队列里删除： -1</span><br><span class="line">请按任意键继续. . .</span><br><span class="line">~~</span><br></pre></td></tr></tbody></table></figure><hr>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>c++中级教程  STL queue</title>
      <link href="/2019/07/26/STLqueue/"/>
      <url>/2019/07/26/STLqueue/</url>
      
        <content type="html"><![CDATA[<h2 id="STL-queue"><a href="#STL-queue" class="headerlink" title="STL queue"></a>STL queue</h2><ul><li><p>队列： FIFO 先进先出</p></li><li><p>自适应容器（容器适配器）</p></li><li><p>栈适配器 STL queue</p><pre><code>1  2  3  4  5  6  7  8</code></pre></li></ul><p>| </p><pre><code>    queue&lt;int, deque&lt;int&gt;&gt;   q;      queue&lt;int, list&lt;int&gt;&gt;   q;      q.empty()      q.size()      q.front()      q.back()      q.pop()      q.push(item)        </code></pre><p>—|—  </p><h3 id="可以用list和deque做queue"><a href="#可以用list和deque做queue" class="headerlink" title="可以用list和deque做queue"></a>可以用list和deque做queue</h3><p>先进先出，后进后出  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  </code></pre><p>| </p><pre><code>// queue.cpp : 定义控制台应用程序的入口点。  //    #include "stdafx.h"  #include&lt;iostream&gt;  #include&lt;queue&gt;  #include&lt;list&gt;  #include&lt;deque&gt;  using namespace std;      int main()  {      queue&lt;int, deque&lt;int&gt;&gt; a;      queue&lt;int, list&lt;int&gt;&gt; b;      //queue&lt;int, vector&lt;int&gt;&gt; c;  不可以，因为vector不能进行两端操作      //队列有什么用途？？？      queue&lt;int&gt; q;        q.push(10);      q.push(5);      q.push(-1);      q.push(20);      cout &lt;&lt; "现在队列里有" &lt;&lt; q.size() &lt;&lt; "个数据 " &lt;&lt; endl;      cout &lt;&lt; "队首的数据：" &lt;&lt; q.front() &lt;&lt; endl;      cout &lt;&lt; "队尾的数据：" &lt;&lt; q.back() &lt;&lt; endl;      q.pop();      cout &lt;&lt; "新的队首的数据：" &lt;&lt; q.front() &lt;&lt; endl;      while (q.size() != 0)      {          cout &lt;&lt; " 删除" &lt;&lt; a.front() &lt;&lt; endl;          q.pop();      }      if (q.empty())      {          cout &lt;&lt; "队列为空！"&lt;&lt;endl;      }      system("pause");      return 0;  }    </code></pre><p>—|—  </p><h2 id="优先级队列-priority-queue"><a href="#优先级队列-priority-queue" class="headerlink" title="优先级队列 priority_queue"></a>优先级队列 priority_queue</h2><ul><li>自适应容器（容器适配器）：不能使用list</li><li>最大值优先级队列、最小值优先级队列(值越大，优先级越高，值越小优先级越高)</li><li>优先级队列适配器 STL priority_queue</li></ul><hr><pre><code>1  2  3  4  5  6  7  8  9  10  11  </code></pre><p>| </p><pre><code>priority_queue&lt;int, deque&lt;int&gt;&gt; pg1;  //对队列里的数据进行随机操作，所以不能使用list  priority_queue&lt;int, vector&lt;int&gt;&gt; pg2; //vector是默认的                                  //谓词  priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt; pg2; //vector是默认的，最小优先队列  pg.empty()  pg.size()  pg.top()  pg.pop()  pg.push(item)  ~~~~~~~~~~    </code></pre><p>—|—  </p><hr><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  </code></pre><p>| </p><pre><code>// priority_queue.cpp : 定义控制台应用程序的入口点。  //    #include "stdafx.h"  #include&lt;iostream&gt;  #include&lt;deque&gt;  #include&lt;vector&gt;  #include&lt;queue&gt;    using namespace std;  int main()  {      priority_queue&lt;int, deque&lt;int&gt;&gt; pg1;      //对队列里的数据进行随机操作，所以不能使用list      priority_queue&lt;int, vector&lt;int&gt;&gt; pg2; //vector是默认的                                      //谓词      priority_queue&lt;int, vector&lt;int&gt;, greater&lt;int&gt; pg2; //vector是默认的，最小优先队列        pg2.push(10);      pg2.push(5);      pg2.push(-1);      pg2.push(20);      cout &lt;&lt; "优先级队列一共有： " &lt;&lt; pg2.size() &lt;&lt; "个数据" &lt;&lt; endl;      cout &lt;&lt; pg2.top() &lt;&lt; endl;      while (!pg2.empty())      {          cout &lt;&lt; "从优先级队列里删除： " &lt;&lt; pg2.top() &lt;&lt; endl;          pg2.pop();      }      system("pause");      return 0;  }  优先级队列一共有： 4个数据  20  从优先级队列里删除： 20  从优先级队列里删除： 10  从优先级队列里删除： 5  从优先级队列里删除： -1  请按任意键继续. . .    ~~    </code></pre><p>—|—  </p><hr>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>c++中级教程 顺序容器的定义</title>
      <link href="/2019/07/26/%E9%A1%BA%E5%BA%8F%E5%AE%B9%E5%99%A8%E7%9A%84%E5%AE%9A%E4%B9%89/"/>
      <url>/2019/07/26/%E9%A1%BA%E5%BA%8F%E5%AE%B9%E5%99%A8%E7%9A%84%E5%AE%9A%E4%B9%89/</url>
      
        <content type="html"><![CDATA[<h2 id="顺序容器的定义"><a href="#顺序容器的定义" class="headerlink" title="顺序容器的定义"></a>顺序容器的定义</h2><ul><li><h3 id="顺序容器"><a href="#顺序容器" class="headerlink" title="顺序容器"></a>顺序容器</h3></li><li><p>vector</p></li><li><p>list</p></li><li><p>deque</p></li><li><h3 id="顺序容器适配器"><a href="#顺序容器适配器" class="headerlink" title="顺序容器适配器"></a>顺序容器适配器</h3></li><li><p>stack</p></li><li><p>queue</p></li><li><p>priority_queue</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>firstblog</title>
      <link href="/2019/07/25/20190725-firstblog/"/>
      <url>/2019/07/25/20190725-firstblog/</url>
      
        <content type="html"><![CDATA[<p>以上是摘要  </p><h2 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h2><p><img src="/images/20190725_firstblog_kenan.png"></p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 其他 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>c++中级教程  STL stack</title>
      <link href="/2019/07/25/20190725-STL-stack/"/>
      <url>/2019/07/25/20190725-STL-stack/</url>
      
        <content type="html"><![CDATA[<h2 id="STL-stack"><a href="#STL-stack" class="headerlink" title="STL stack"></a>STL stack</h2><ul><li>(堆) 栈： LIFO 后进先出</li><li>自适应容器（容器适配器）</li><li>栈适配器 STL stack</li></ul><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">stack&lt;int, deque&lt;int&gt;&gt;   s;</span><br><span class="line">stack&lt;int, vector&lt;int&gt;&gt;   s;</span><br><span class="line">stack&lt;int, list&lt;int&gt;&gt;   s;</span><br><span class="line">s.empty()</span><br><span class="line">s.size()</span><br><span class="line">s.pop()</span><br><span class="line">s.push(item)</span><br></pre></td></tr></tbody></table></figure><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// stack1.cpp : 定义控制台应用程序的入口点。</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">"stdafx.h"</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;vector&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;list&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;stack&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">stack&lt;<span class="type">int</span>, deque&lt;<span class="type">int</span>&gt;&gt;   a;</span><br><span class="line">stack&lt;<span class="type">int</span>, vector&lt;<span class="type">int</span>&gt;&gt;  b;</span><br><span class="line">stack&lt;<span class="type">int</span>, list&lt;<span class="type">int</span>&gt;&gt;    c;</span><br><span class="line">stack&lt;<span class="type">int</span>&gt;               d; <span class="comment">//默认用deque</span></span><br><span class="line"><span class="comment">//什么是堆栈？ 先进后出，后进先出</span></span><br><span class="line">d.<span class="built_in">push</span>(<span class="number">25</span>);</span><br><span class="line">d.<span class="built_in">push</span>(<span class="number">10</span>);</span><br><span class="line">d.<span class="built_in">push</span>(<span class="number">1</span>);</span><br><span class="line">d.<span class="built_in">push</span>(<span class="number">5</span>);</span><br><span class="line"><span class="type">int</span> x = <span class="number">0</span>;</span><br><span class="line">cout &lt;&lt; <span class="string">"现在栈里一共有："</span> &lt;&lt; d.<span class="built_in">size</span>() &lt;&lt; <span class="string">"个数据。"</span> &lt;&lt; endl;</span><br><span class="line"><span class="keyword">while</span> (d.<span class="built_in">empty</span>() == <span class="literal">false</span>)</span><br><span class="line">{</span><br><span class="line">x = d.<span class="built_in">top</span>(); <span class="comment">//查看数据并且返回</span></span><br><span class="line">d.<span class="built_in">pop</span>();<span class="comment">//删除，不返回</span></span><br><span class="line">cout &lt;&lt; x &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"><span class="comment">//x = d.top(); //查看数据并且返回</span></span><br><span class="line"><span class="comment">//d.pop();//删除，不返回</span></span><br><span class="line"><span class="comment">//cout &lt;&lt; x &lt;&lt; endl;</span></span><br><span class="line">cout &lt;&lt; <span class="string">"现在栈里一共有："</span> &lt;&lt; d.<span class="built_in">size</span>() &lt;&lt; <span class="string">"个数据。"</span> &lt;&lt; endl;</span><br><span class="line"><span class="built_in">system</span>(<span class="string">"pause"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br><span class="line">现在栈里一共有：<span class="number">4</span>个数据。</span><br></pre></td></tr></tbody></table></figure><figure class="highlight plaintext"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">现在栈里一共有：0个数据。</span><br><span class="line">请按任意键继续. . .</span><br></pre></td></tr></tbody></table></figure>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>c++中级教程(一)</title>
      <link href="/2019/07/25/20190725-%E9%A1%BA%E5%BA%8F%E5%AE%B9%E5%99%A8-STL-deque-%E7%B1%BB/"/>
      <url>/2019/07/25/20190725-%E9%A1%BA%E5%BA%8F%E5%AE%B9%E5%99%A8-STL-deque-%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<h2 id="顺序容器-STL-deque-类"><a href="#顺序容器-STL-deque-类" class="headerlink" title="顺序容器 STL deque 类"></a>顺序容器 STL deque 类</h2><ul><li><p>deque是一个动态数组</p></li><li><p>deque与vector 非常类似</p></li><li><p>deque可以在数组开头和末尾插入和删除数据</p></li></ul><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// demo3.cpp : 定义控制台应用程序的入口点。</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">"stdafx.h"</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;deque&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">&lt;algorithm&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">deque&lt;<span class="type">int</span>&gt; a;</span><br><span class="line">a.<span class="built_in">push_back</span>(<span class="number">3</span>);</span><br><span class="line">a.<span class="built_in">push_back</span>(<span class="number">4</span>);</span><br><span class="line">a.<span class="built_in">push_back</span>(<span class="number">5</span>);</span><br><span class="line">a.<span class="built_in">push_back</span>(<span class="number">6</span>);</span><br><span class="line"><span class="comment">//vector只能push_back</span></span><br><span class="line">a.<span class="built_in">push_front</span>(<span class="number">2</span>);</span><br><span class="line">a.<span class="built_in">push_front</span>(<span class="number">1</span>);</span><br><span class="line">a.<span class="built_in">push_front</span>(<span class="number">9</span>);</span><br><span class="line">a.<span class="built_in">push_front</span>(<span class="number">8</span>);</span><br><span class="line"><span class="keyword">for</span> (<span class="type">size_t</span> nCount = <span class="number">0</span>; nCount &lt; a.<span class="built_in">size</span>(); ++nCount)</span><br><span class="line">{</span><br><span class="line">cout &lt;&lt;<span class="string">"a["</span>&lt;&lt;nCount&lt;&lt;<span class="string">"]"</span> &lt;&lt; <span class="string">"= "</span> &lt;&lt; a[nCount] &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; endl&lt;&lt;endl;</span><br><span class="line">a.<span class="built_in">pop_front</span>();<span class="comment">// 前面删除</span></span><br><span class="line">a.<span class="built_in">pop_back</span>();<span class="comment">// 后面删除</span></span><br><span class="line">cout &lt;&lt; <span class="string">"删除之后："</span> &lt;&lt; endl;</span><br><span class="line"><span class="comment">/*for (size_t nCount = 0; nCount &lt; a.size(); ++nCount)</span></span><br><span class="line"><span class="comment">{</span></span><br><span class="line"><span class="comment">cout &lt;&lt; "a[" &lt;&lt; nCount &lt;&lt; "]" &lt;&lt; a[nCount] &lt;&lt; endl;</span></span><br><span class="line"><span class="comment">}*/</span></span><br><span class="line">deque&lt;<span class="type">int</span>&gt;::iterator iElementLocater; <span class="comment">//这边使用了迭代器  distence 可以计算当前</span></span><br><span class="line"><span class="keyword">for</span> (iElementLocater = a.<span class="built_in">begin</span>();</span><br><span class="line">iElementLocater != a.<span class="built_in">end</span>();</span><br><span class="line">++iElementLocater)</span><br><span class="line">{</span><br><span class="line"><span class="type">size_t</span> nOffset = <span class="built_in">distance</span>(a.<span class="built_in">begin</span>(), iElementLocater);<span class="comment">//distence 可以计算当前下标与begin开始的，距离正好是下标</span></span><br><span class="line">cout &lt;&lt; <span class="string">"a["</span> &lt;&lt; nOffset &lt;&lt; <span class="string">"]"</span> &lt;&lt; <span class="string">"= "</span>&lt;&lt;*iElementLocater &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line"><span class="built_in">system</span>(<span class="string">"pause"</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br><span class="line">a[<span class="number">0</span>]= <span class="number">8</span></span><br><span class="line">a[<span class="number">1</span>]= <span class="number">9</span></span><br><span class="line">a[<span class="number">2</span>]= <span class="number">1</span></span><br><span class="line">a[<span class="number">3</span>]= <span class="number">2</span></span><br><span class="line">a[<span class="number">4</span>]= <span class="number">3</span></span><br><span class="line">a[<span class="number">5</span>]= <span class="number">4</span></span><br><span class="line">a[<span class="number">6</span>]= <span class="number">5</span></span><br><span class="line">a[<span class="number">7</span>]= <span class="number">6</span></span><br><span class="line">删除之后：</span><br><span class="line">a[<span class="number">0</span>]= <span class="number">9</span></span><br><span class="line">a[<span class="number">1</span>]= <span class="number">1</span></span><br><span class="line">a[<span class="number">2</span>]= <span class="number">2</span></span><br><span class="line">a[<span class="number">3</span>]= <span class="number">3</span></span><br><span class="line">a[<span class="number">4</span>]= <span class="number">4</span></span><br><span class="line">a[<span class="number">5</span>]= <span class="number">5</span></span><br><span class="line">请按任意键继续. . .</span><br></pre></td></tr></tbody></table></figure><hr><p>順序容器 STL list 類</p><ul><li>实例化std::list对象</li><li>在list开头插入元素</li><li>在list末尾插入元素</li><li>在list中间插入元素</li><li>删除list中的元素</li><li>对list中的元素进行反转和排序</li></ul><figure class="highlight javascript"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// list.cpp : 定义控制台应用程序的入口点。</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line">#include <span class="string">"stdafx.h"</span></span><br><span class="line">#include&lt;iostream&gt;</span><br><span class="line">#include&lt;list&gt;</span><br><span class="line">using namespace std;</span><br><span class="line"><span class="keyword">void</span> <span class="title class_">PrintListContent</span>(<span class="keyword">const</span> list&lt;int&gt;&amp; listInput);</span><br><span class="line">int <span class="title function_">main</span>(<span class="params"></span>)</span><br><span class="line">{</span><br><span class="line">list&lt;int&gt; a;</span><br><span class="line">list&lt;int&gt; b;</span><br><span class="line">b.<span class="title function_">push_back</span>(<span class="number">100</span>);</span><br><span class="line">b.<span class="title function_">push_back</span>(<span class="number">200</span>);</span><br><span class="line">b.<span class="title function_">push_back</span>(<span class="number">300</span>);</span><br><span class="line">b.<span class="title function_">push_back</span>(<span class="number">400</span>);</span><br><span class="line">b.<span class="title function_">push_back</span>(<span class="number">500</span>);</span><br><span class="line"><span class="title class_">PrintListContent</span>(b);</span><br><span class="line">cout &lt;&lt; endl;</span><br><span class="line">a.<span class="title function_">push_front</span>(<span class="number">4</span>);</span><br><span class="line">a.<span class="title function_">push_front</span>(<span class="number">3</span>);</span><br><span class="line">a.<span class="title function_">push_front</span>(<span class="number">2</span>);</span><br><span class="line">a.<span class="title function_">push_front</span>(<span class="number">1</span>);</span><br><span class="line">a.<span class="title function_">push_back</span>(<span class="number">5</span>);</span><br><span class="line"><span class="comment">//使用链表数据，不能使用下标，只能使用迭代器</span></span><br><span class="line">list&lt;int&gt;::iterator iter;</span><br><span class="line">iter = a.<span class="title function_">begin</span>();</span><br><span class="line">a.<span class="title function_">insert</span>(iter, <span class="number">10</span>);<span class="comment">// 在begin前面插入10，第一个参数迭代器，指定插入的位置</span></span><br><span class="line">a.<span class="title function_">insert</span>(a.<span class="title function_">end</span>(),<span class="number">10</span>);</span><br><span class="line"><span class="title class_">PrintListContent</span>(a);</span><br><span class="line"><span class="comment">//将b插入到a之中</span></span><br><span class="line">a.<span class="title function_">insert</span>(a.<span class="title function_">begin</span>(), b.<span class="title function_">begin</span>(), b.<span class="title function_">end</span>());</span><br><span class="line">a.<span class="title function_">insert</span>(iter,++b.<span class="title function_">begin</span>(),--b.<span class="title function_">end</span>())</span><br><span class="line"><span class="title class_">PrintListContent</span>(a);</span><br><span class="line"><span class="title function_">system</span>(<span class="string">"pause"</span>);</span><br><span class="line"><span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br><span class="line"><span class="keyword">void</span> <span class="title class_">PrintListContent</span>(<span class="keyword">const</span> list&lt;int&gt;&amp; listInput)</span><br><span class="line">{</span><br><span class="line"><span class="comment">//会是一个底层 const，即其所指对象可以改变，但不能改变其所指对象的值。</span></span><br><span class="line">list&lt;int&gt;::const_iterator iter;</span><br><span class="line"><span class="keyword">for</span> (iter = listInput.<span class="title function_">begin</span>(); iter != listInput.<span class="title function_">end</span>(); ++iter)</span><br><span class="line">{</span><br><span class="line">cout &lt;&lt; *iter &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">}</span><br></pre></td></tr></tbody></table></figure><hr><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// listdelet.cpp : 定义控制台应用程序的入口点。</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">"stdafx.h"</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;list&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">PrintListContent</span><span class="params">(<span class="type">const</span> list&lt;<span class="type">int</span>&gt;&amp; listInput)</span></span>;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">list&lt;<span class="type">int</span>&gt; a;</span><br><span class="line">a.<span class="built_in">push_front</span>(<span class="number">4</span>);</span><br><span class="line">a.<span class="built_in">push_front</span>(<span class="number">3</span>);</span><br><span class="line">list&lt;<span class="type">int</span>&gt;::iterator iElementValueTwo;</span><br><span class="line">iElementValueTwo = a.<span class="built_in">insert</span>(a.<span class="built_in">begin</span>(),<span class="number">2</span>); <span class="comment">// inset 才返回迭代器迭代器指向这个位置</span></span><br><span class="line">a.<span class="built_in">push_front</span>(<span class="number">1</span>);</span><br><span class="line">a.<span class="built_in">push_back</span>(<span class="number">0</span>);</span><br><span class="line">cout &lt;&lt; <span class="string">"删除之前"</span> &lt;&lt; endl;</span><br><span class="line"><span class="built_in">PrintListContent</span>(a);</span><br><span class="line"><span class="comment">// 删除2</span></span><br><span class="line">cout &lt;&lt; <span class="string">"删除之后"</span> &lt;&lt; endl;</span><br><span class="line">a.<span class="built_in">erase</span>(iElementValueTwo);</span><br><span class="line"><span class="comment">//a.erase(a.beigin(),iElementValueTwo); 删除从第一个迭代器到第二个迭代器所有的数据</span></span><br><span class="line"><span class="built_in">PrintListContent</span>(a);</span><br><span class="line"><span class="built_in">system</span>(<span class="string">"pause"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">PrintListContent</span><span class="params">(<span class="type">const</span> list&lt;<span class="type">int</span>&gt;&amp; listInput)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line"><span class="comment">//会是一个底层 const，即其所指对象可以改变，但不能改变其所指对象的值。</span></span><br><span class="line">cout &lt;&lt; <span class="string">"{"</span>;</span><br><span class="line">list&lt;<span class="type">int</span>&gt;::const_iterator iter;</span><br><span class="line"><span class="keyword">for</span> (iter = listInput.<span class="built_in">begin</span>(); iter != listInput.<span class="built_in">end</span>(); ++iter)</span><br><span class="line">{</span><br><span class="line">cout &lt;&lt; *iter &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; <span class="string">"}"</span> &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">删除之前</span><br><span class="line">{<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> <span class="number">0</span> }</span><br><span class="line">删除之后</span><br><span class="line">{<span class="number">1</span> <span class="number">3</span> <span class="number">4</span> <span class="number">0</span> }</span><br><span class="line">请按任意键继续. . .</span><br></pre></td></tr></tbody></table></figure><hr><figure class="highlight cpp"><table><tbody><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// list3.cpp : 定义控制台应用程序的入口点。</span></span><br><span class="line"><span class="comment">//</span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span> <span class="string">"stdafx.h"</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;iostream&gt;</span></span></span><br><span class="line"><span class="meta">#<span class="keyword">include</span><span class="string">&lt;list&gt;</span></span></span><br><span class="line"><span class="keyword">using</span> <span class="keyword">namespace</span> std;</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">PrintListContent</span><span class="params">(<span class="type">const</span> list&lt;<span class="type">int</span>&gt;&amp; listInput)</span></span>;</span><br><span class="line"><span class="function"><span class="type">int</span> <span class="title">main</span><span class="params">()</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line">list&lt;<span class="type">int</span>&gt; a;</span><br><span class="line">a.<span class="built_in">push_front</span>(<span class="number">4</span>);</span><br><span class="line">a.<span class="built_in">push_front</span>(<span class="number">3</span>);</span><br><span class="line">a.<span class="built_in">push_front</span>(<span class="number">2</span>);</span><br><span class="line">a.<span class="built_in">push_front</span>(<span class="number">1</span>);</span><br><span class="line"><span class="built_in">PrintListContent</span>(a);</span><br><span class="line">cout &lt;&lt; <span class="string">"反转之后的数据："</span> &lt;&lt; endl;</span><br><span class="line">a.<span class="built_in">reverse</span>();</span><br><span class="line"><span class="built_in">PrintListContent</span>(a);</span><br><span class="line">list&lt;<span class="type">int</span>&gt; b;</span><br><span class="line">b.<span class="built_in">push_front</span>(<span class="number">4</span>);</span><br><span class="line">b.<span class="built_in">push_front</span>(<span class="number">53</span>);</span><br><span class="line">b.<span class="built_in">push_front</span>(<span class="number">24</span>);</span><br><span class="line">b.<span class="built_in">push_front</span>(<span class="number">132</span>);</span><br><span class="line"><span class="built_in">PrintListContent</span>(b);</span><br><span class="line">cout &lt;&lt; <span class="string">"排序之后的数据："</span> &lt;&lt; endl;</span><br><span class="line">b.<span class="built_in">sort</span>();</span><br><span class="line"><span class="built_in">PrintListContent</span>(b);</span><br><span class="line"><span class="built_in">system</span>(<span class="string">"pause"</span>);</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">}</span><br><span class="line"><span class="function"><span class="type">void</span> <span class="title">PrintListContent</span><span class="params">(<span class="type">const</span> list&lt;<span class="type">int</span>&gt;&amp; listInput)</span></span></span><br><span class="line"><span class="function"></span>{</span><br><span class="line"><span class="comment">//会是一个底层 const，即其所指对象可以改变，但不能改变其所指对象的值。</span></span><br><span class="line">cout &lt;&lt; <span class="string">"{"</span>;</span><br><span class="line">list&lt;<span class="type">int</span>&gt;::const_iterator iter;</span><br><span class="line"><span class="keyword">for</span> (iter = listInput.<span class="built_in">begin</span>(); iter != listInput.<span class="built_in">end</span>(); ++iter)</span><br><span class="line">{</span><br><span class="line">cout &lt;&lt; *iter &lt;&lt; <span class="string">" "</span>;</span><br><span class="line">}</span><br><span class="line">cout &lt;&lt; <span class="string">"}"</span> &lt;&lt; endl;</span><br><span class="line">}</span><br><span class="line">{<span class="number">1</span> <span class="number">2</span> <span class="number">3</span> <span class="number">4</span> }</span><br><span class="line">反转之后的数据：</span><br><span class="line">{<span class="number">4</span> <span class="number">3</span> <span class="number">2</span> <span class="number">1</span> }</span><br><span class="line">{<span class="number">132</span> <span class="number">24</span> <span class="number">53</span> <span class="number">4</span> }</span><br><span class="line">排序之后的数据：</span><br><span class="line">{<span class="number">4</span> <span class="number">24</span> <span class="number">53</span> <span class="number">132</span> }</span><br><span class="line">请按任意键继续. . .</span><br></pre></td></tr></tbody></table></figure><hr>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>c++中级教程  STL stack</title>
      <link href="/2019/07/25/STL-stack/"/>
      <url>/2019/07/25/STL-stack/</url>
      
        <content type="html"><![CDATA[<h2 id="STL-stack"><a href="#STL-stack" class="headerlink" title="STL stack"></a>STL stack</h2><ul><li><p>(堆) 栈： LIFO 后进先出</p></li><li><p>自适应容器（容器适配器）</p></li><li><p>栈适配器 STL stack</p><pre><code>1  2  3  4  5  6  7</code></pre></li></ul><p>| </p><pre><code>    stack&lt;int, deque&lt;int&gt;&gt;   s;      stack&lt;int, vector&lt;int&gt;&gt;   s;      stack&lt;int, list&lt;int&gt;&gt;   s;      s.empty()      s.size()      s.pop()      s.push(item)        </code></pre><p>—|—  </p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  </code></pre><p>| </p><pre><code>// stack1.cpp : 定义控制台应用程序的入口点。  //    #include "stdafx.h"  #include &lt;iostream&gt;  #include &lt;vector&gt;  #include &lt;list&gt;  #include &lt;stack&gt;  using namespace std;  int main()  {      stack&lt;int, deque&lt;int&gt;&gt;   a;      stack&lt;int, vector&lt;int&gt;&gt;  b;      stack&lt;int, list&lt;int&gt;&gt;    c;      stack&lt;int&gt;               d; //默认用deque      //什么是堆栈？ 先进后出，后进先出        d.push(25);      d.push(10);      d.push(1);      d.push(5);      int x = 0;      cout &lt;&lt; "现在栈里一共有：" &lt;&lt; d.size() &lt;&lt; "个数据。" &lt;&lt; endl;      while (d.empty() == false)      {          x = d.top(); //查看数据并且返回          d.pop();//删除，不返回           cout &lt;&lt; x &lt;&lt; endl;      }              //x = d.top(); //查看数据并且返回      //d.pop();//删除，不返回       //cout &lt;&lt; x &lt;&lt; endl;      cout &lt;&lt; "现在栈里一共有：" &lt;&lt; d.size() &lt;&lt; "个数据。" &lt;&lt; endl;      system("pause");      return 0;  }  现在栈里一共有：4个数据。  5  1  10  25  现在栈里一共有：0个数据。  请按任意键继续. . .    </code></pre><p>—|—</p>]]></content>
      
      
      <categories>
          
          <category> C++ </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>firstblog</title>
      <link href="/2019/07/25/firstblog/"/>
      <url>/2019/07/25/firstblog/</url>
      
        <content type="html"><![CDATA[<p>以上是摘要  </p><h2 id="Hello-World"><a href="#Hello-World" class="headerlink" title="Hello World"></a>Hello World</h2><p><img src="/2019/07/25/firstblog/images/20190725_firstblog_kenan.png"></p>]]></content>
      
      
      <categories>
          
          <category> 其他 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>c++中级教程(一)</title>
      <link href="/2019/07/25/%E9%A1%BA%E5%BA%8F%E5%AE%B9%E5%99%A8-STL-deque-%E7%B1%BB/"/>
      <url>/2019/07/25/%E9%A1%BA%E5%BA%8F%E5%AE%B9%E5%99%A8-STL-deque-%E7%B1%BB/</url>
      
        <content type="html"><![CDATA[<h2 id="顺序容器-STL-deque-类"><a href="#顺序容器-STL-deque-类" class="headerlink" title="顺序容器 STL deque 类"></a>顺序容器 STL deque 类</h2><ul><li><p>deque是一个动态数组</p></li><li><p>deque与vector 非常类似</p></li><li><p>deque可以在数组开头和末尾插入和删除数据</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58  59  60  61  62  63</code></pre></li></ul><p>| </p><pre><code>    // demo3.cpp : 定义控制台应用程序的入口点。      //            #include "stdafx.h"      #include &lt;iostream&gt;      #include &lt;deque&gt;      #include &lt;algorithm&gt;      using namespace std;            int main()      {          deque&lt;int&gt; a;          a.push_back(3);          a.push_back(4);          a.push_back(5);          a.push_back(6);          //vector只能push_back          a.push_front(2);          a.push_front(1);          a.push_front(9);          a.push_front(8);                for (size_t nCount = 0; nCount &lt; a.size(); ++nCount)          {              cout &lt;&lt;"a["&lt;&lt;nCount&lt;&lt;"]" &lt;&lt; "= " &lt;&lt; a[nCount] &lt;&lt; endl;          }          cout &lt;&lt; endl&lt;&lt;endl;                a.pop_front();// 前面删除          a.pop_back();// 后面删除          cout &lt;&lt; "删除之后：" &lt;&lt; endl;          /*for (size_t nCount = 0; nCount &lt; a.size(); ++nCount)          {              cout &lt;&lt; "a[" &lt;&lt; nCount &lt;&lt; "]" &lt;&lt; a[nCount] &lt;&lt; endl;          }*/          deque&lt;int&gt;::iterator iElementLocater; //这边使用了迭代器  distence 可以计算当前          for (iElementLocater = a.begin();              iElementLocater != a.end();              ++iElementLocater)          {              size_t nOffset = distance(a.begin(), iElementLocater);//distence 可以计算当前下标与begin开始的，距离正好是下标              cout &lt;&lt; "a[" &lt;&lt; nOffset &lt;&lt; "]" &lt;&lt; "= "&lt;&lt;*iElementLocater &lt;&lt; endl;          }          system("pause");          return 0;      }      a[0]= 8      a[1]= 9      a[2]= 1      a[3]= 2      a[4]= 3      a[5]= 4      a[6]= 5      a[7]= 6            删除之后：      a[0]= 9      a[1]= 1      a[2]= 2      a[3]= 3      a[4]= 4      a[5]= 5      请按任意键继续. . .        </code></pre><p>—|—  </p><hr><p>順序容器 STL list 類</p><ul><li><p>实例化std::list对象</p></li><li><p>在list开头插入元素</p></li><li><p>在list末尾插入元素</p></li><li><p>在list中间插入元素</p></li><li><p>删除list中的元素</p></li><li><p>对list中的元素进行反转和排序</p><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  57  58</code></pre></li></ul><p>| </p><pre><code>    // list.cpp : 定义控制台应用程序的入口点。      //            #include "stdafx.h"      #include&lt;iostream&gt;      #include&lt;list&gt;      using namespace std;            void PrintListContent(const list&lt;int&gt;&amp; listInput);      int main()      {          list&lt;int&gt; a;          list&lt;int&gt; b;                b.push_back(100);          b.push_back(200);          b.push_back(300);          b.push_back(400);          b.push_back(500);          PrintListContent(b);                cout &lt;&lt; endl;                      a.push_front(4);          a.push_front(3);          a.push_front(2);                a.push_front(1);          a.push_back(5);                //使用链表数据，不能使用下标，只能使用迭代器          list&lt;int&gt;::iterator iter;          iter = a.begin();          a.insert(iter, 10);// 在begin前面插入10，第一个参数迭代器，指定插入的位置          a.insert(a.end(),10);                PrintListContent(a);          //将b插入到a之中          a.insert(a.begin(), b.begin(), b.end());          a.insert(iter,++b.begin(),--b.end())                    PrintListContent(a);                    system("pause");          return 0;      }                  void PrintListContent(const list&lt;int&gt;&amp; listInput)      {          //会是一个底层 const，即其所指对象可以改变，但不能改变其所指对象的值。          list&lt;int&gt;::const_iterator iter;          for (iter = listInput.begin(); iter != listInput.end(); ++iter)          {              cout &lt;&lt; *iter &lt;&lt; endl;          }      }        </code></pre><p>—|—  </p><hr><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  </code></pre><p>| </p><pre><code>// listdelet.cpp : 定义控制台应用程序的入口点。  //    #include "stdafx.h"  #include&lt;iostream&gt;  #include&lt;list&gt;  using namespace std;  void PrintListContent(const list&lt;int&gt;&amp; listInput);      int main()  {      list&lt;int&gt; a;      a.push_front(4);      a.push_front(3);        list&lt;int&gt;::iterator iElementValueTwo;      iElementValueTwo = a.insert(a.begin(),2); // inset 才返回迭代器迭代器指向这个位置      a.push_front(1);      a.push_back(0);      cout &lt;&lt; "删除之前" &lt;&lt; endl;      PrintListContent(a);      // 删除2      cout &lt;&lt; "删除之后" &lt;&lt; endl;        a.erase(iElementValueTwo);      //a.erase(a.beigin(),iElementValueTwo); 删除从第一个迭代器到第二个迭代器所有的数据      PrintListContent(a);      system("pause");      return 0;  }      void PrintListContent(const list&lt;int&gt;&amp; listInput)  {      //会是一个底层 const，即其所指对象可以改变，但不能改变其所指对象的值。      cout &lt;&lt; "{";      list&lt;int&gt;::const_iterator iter;      for (iter = listInput.begin(); iter != listInput.end(); ++iter)      {          cout &lt;&lt; *iter &lt;&lt; " ";      }      cout &lt;&lt; "}" &lt;&lt; endl;  }  删除之前  {1 2 3 4 0 }  删除之后  {1 3 4 0 }  请按任意键继续. . .    </code></pre><p>—|—  </p><hr><pre><code>1  2  3  4  5  6  7  8  9  10  11  12  13  14  15  16  17  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53  54  55  56  </code></pre><p>| </p><pre><code>// list3.cpp : 定义控制台应用程序的入口点。  //    #include "stdafx.h"  #include&lt;iostream&gt;  #include&lt;list&gt;  using namespace std;  void PrintListContent(const list&lt;int&gt;&amp; listInput);    int main()  {      list&lt;int&gt; a;      a.push_front(4);      a.push_front(3);      a.push_front(2);        a.push_front(1);        PrintListContent(a);      cout &lt;&lt; "反转之后的数据：" &lt;&lt; endl;      a.reverse();      PrintListContent(a);      list&lt;int&gt; b;      b.push_front(4);      b.push_front(53);      b.push_front(24);        b.push_front(132);        PrintListContent(b);      cout &lt;&lt; "排序之后的数据：" &lt;&lt; endl;      b.sort();      PrintListContent(b);      system("pause");        return 0;  }    void PrintListContent(const list&lt;int&gt;&amp; listInput)  {      //会是一个底层 const，即其所指对象可以改变，但不能改变其所指对象的值。      cout &lt;&lt; "{";      list&lt;int&gt;::const_iterator iter;      for (iter = listInput.begin(); iter != listInput.end(); ++iter)      {          cout &lt;&lt; *iter &lt;&lt; " ";      }      cout &lt;&lt; "}" &lt;&lt; endl;  }  {1 2 3 4 }  反转之后的数据：  {4 3 2 1 }  {132 24 53 4 }  排序之后的数据：  {4 24 53 132 }  请按任意键继续. . .    </code></pre><p>—|—  </p><hr>]]></content>
      
      
      <categories>
          
          <category> 算法 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> C++ </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
