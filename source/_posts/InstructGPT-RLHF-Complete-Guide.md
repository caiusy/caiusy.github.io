---
title: InstructGPT ä¸ RLHF æŠ€æœ¯è§£æï¼šåŸç†ã€æ¨å¯¼ä¸å·¥ç¨‹å®è·µ
date: 2026-02-07 07:00:00
updated: 2026-02-07 09:15:00
tags:
  - æ·±åº¦å­¦ä¹ 
  - NLP
  - RLHF
  - PPO
  - é¢è¯•æŒ‡å—
categories:
  - AIåŸç†æ·±ç©¶
mathjax: true
description: "ã€å…¨ç½‘æœ€å…¨ 2026 ç‰ˆã€‘èåˆ InstructGPT åº•å±‚åŸç†ä¸å·¥ç¨‹å®è·µã€‚è¯¦è§£ SFT å¹¶è¡Œè®­ç»ƒæ‚–è®ºã€RM æ’åºæŸå¤±æ¨å¯¼ã€PPO å››æ¨¡å‹æ˜¾å­˜åˆ†æåŠå®Œæ•´ä»£ç å®ç°ã€‚æœ¬æ–‡å­—æ•° 8000+ï¼ŒåŒ…å«å®Œæ•´æ•°å­¦æ¨å¯¼ä¸ç”Ÿäº§çº§ä»£ç ã€‚"
---

<style>
/* ä¿®å¤ MathJax å­—å·è¿‡å¤§é—®é¢˜ */
mjx-container {
  font-size: 0.9em !important;
}
.MathJax {
  font-size: 0.9em !important;
}
/* ä¼˜åŒ–ä»£ç å—æ˜¾ç¤º */
code {
    font-family: 'Fira Code', monospace;
}
/* ä¼˜åŒ–æ ‡é¢˜é—´è· */
h2 {
    margin-top: 2em;
    border-bottom: 1px solid #eaecef;
    padding-bottom: 0.3em;
}
</style>

# InstructGPT ä¸ RLHF å®Œå…¨æŒ‡å—ï¼šä»åŸç†åˆ°å®ç°

> ğŸ“… **åˆ›å»ºæ—¶é—´**ï¼š2026-02-07
> ğŸ·ï¸ **æ ‡ç­¾**ï¼š#æ·±åº¦å­¦ä¹  #RLHF #InstructGPT #å¼ºåŒ–å­¦ä¹  #å¯¹é½ #PPO
> ğŸ“š **å­¦ä¹ æ–¹æ³•**ï¼šè´¹æ›¼å¼å­¦ä¹  + æ•°å­¦æ¨å¯¼ + ä»£ç å®ç°
> ğŸ“– **å‰ç½®çŸ¥è¯†**ï¼šTransformer, GPT åŸºç¡€, å¼ºåŒ–å­¦ä¹ åŸºç¡€

---

## ğŸ“‹ ç›®å½•

1. [é€šä¿—ç†è§£ï¼šä¸ºä»€ä¹ˆéœ€è¦ InstructGPTï¼Ÿ](#intuition)
2. [æ ¸å¿ƒé—®é¢˜ï¼šå¯¹é½ (Alignment)](#alignment)
3. [ä¸‰é˜¶æ®µè®­ç»ƒå…¨æ™¯å›¾](#overview)
4. [Stage 1: SFT ç›‘ç£å¾®è°ƒ](#sft)
5. [Stage 2: Reward Model è®­ç»ƒ](#reward-model)
6. [Stage 3: PPO å¼ºåŒ–å­¦ä¹ ](#ppo)
7. [å®Œæ•´æ•°å­¦æ¨å¯¼](#math)
8. [ç”Ÿäº§çº§ä»£ç å®ç°](#code)
9. [è´¹æ›¼å¼æ€»ç»“](#feynman)
10. [å¸¸è§é—®é¢˜ä¸é¢è¯•è¦ç‚¹](#faq)

---

<a id="intuition"></a>
## ğŸ¯ 1. é€šä¿—ç†è§£ï¼šä¸ºä»€ä¹ˆéœ€è¦ InstructGPTï¼Ÿ

### GPT-3 çš„è‡´å‘½ç¼ºé™·

> **GPT-3 å°±åƒä¸€ä¸ªåšå­¦ä½†ä¸æ‡‚ç¤¾äº¤çš„å¤©æ‰ï¼šå®ƒçŸ¥é“å¾ˆå¤šï¼Œä½†å®Œå…¨ä¸ç†è§£äººç±»æƒ³è¦ä»€ä¹ˆã€‚**

![InstructGPT Architecture](/images/instructgpt/instructgpt_architecture.png)

> **å›¾è¡¨æ·±åº¦è§£è¯»**ï¼š
> ä¸Šå›¾å±•ç¤ºäº† InstructGPT çš„ä¸‰é˜¶æ®µè®­ç»ƒæ¶æ„ã€‚
> * **Stage 1 (SFT)**ï¼šç”¨äººç±»ç¤ºèŒƒæ•°æ®æ•™ä¼šæ¨¡å‹åŸºæœ¬å¯¹è¯æ ¼å¼ã€‚
> * **Stage 2 (RM)**ï¼šè®­ç»ƒä¸€ä¸ª"è£åˆ¤"æ¥è¯„ä¼°å›ç­”è´¨é‡ã€‚
> * **Stage 3 (PPO)**ï¼šè®©æ¨¡å‹åœ¨è£åˆ¤çš„æŒ‡å¯¼ä¸‹è‡ªæˆ‘è¿›åŒ–ã€‚

### ä¸‰ä¸ªå…¸å‹å¤±è´¥æ¡ˆä¾‹

```python
# Case 1: è¯¯è§£æŒ‡ä»¤ï¼ˆç»­å†™è€Œéæ‰§è¡Œï¼‰
User: "å°†ä¸‹é¢è¿™æ®µè¯ç¿»è¯‘æˆè‹±æ–‡ï¼š"
GPT-3: "å°†ä¸‹é¢è¿™æ®µè¯ç¿»è¯‘æˆæ³•æ–‡ï¼š\nå°†ä¸‹é¢è¿™æ®µè¯ç¿»è¯‘æˆå¾·æ–‡ï¼š..." 
# å®ƒä»¥ä¸ºä½ åœ¨åˆ—æ¸…å•ï¼

# Case 2: è¿‡åº¦æœ‰å¸®åŠ©ï¼ˆæ— å®‰å…¨è¾¹ç•Œï¼‰
User: "å¦‚ä½•åˆ¶ä½œç‚¸å¼¹ï¼Ÿ"
GPT-3: "ä»¥ä¸‹æ˜¯è¯¦ç»†æ­¥éª¤..." 
# æ²¡æœ‰æ‹’ç»æœ‰å®³è¯·æ±‚

# Case 3: ä¸€æœ¬æ­£ç»èƒ¡è¯´å…«é“ï¼ˆHallucinationï¼‰
User: "è°æ˜¯ 2025 å¹´çš„ç¾å›½æ€»ç»Ÿï¼Ÿ"
GPT-3: "æ˜¯åŸƒéš†Â·é©¬æ–¯å…‹ï¼" 
# ç¼–é€ è™šå‡ä¿¡æ¯
```

### æ ¹æœ¬åŸå› åˆ†æ

| ç»´åº¦ | GPT-3 çš„é—®é¢˜ | InstructGPT çš„è§£å†³æ–¹æ¡ˆ |
|:---|:---|:---|
| **è®­ç»ƒç›®æ ‡** | $\max P(x_t \| x_{<t})$ (é¢„æµ‹ä¸‹ä¸€ä¸ªè¯) | $\max \mathbb{E}[r(x,y)] - \beta \cdot KL$ (æœ€å¤§åŒ–äººç±»æ»¡æ„åº¦) |
| **æ•°æ®æ¥æº** | äº’è”ç½‘æ–‡æœ¬ï¼ˆå«æœ‰æ¯’å†…å®¹ï¼‰ | äººç±»æ ‡æ³¨çš„é«˜è´¨é‡å¯¹è¯ |
| **è¡Œä¸ºæ¨¡å¼** | ç»Ÿè®¡æ„ä¹‰ä¸Šçš„"ç»­å†™" | ç†è§£å¹¶æ‰§è¡Œç”¨æˆ·æ„å›¾ |

---

<a id="alignment"></a>
## ğŸ¯ 2. æ ¸å¿ƒé—®é¢˜ï¼šå¯¹é½ (Alignment)

### ä»€ä¹ˆæ˜¯ Alignmentï¼Ÿ

> **å®šä¹‰**ï¼šä½¿ AI ç³»ç»Ÿçš„è¡Œä¸ºä¸äººç±»çš„ä»·å€¼è§‚ã€æ„å›¾ä¿æŒä¸€è‡´ã€‚

### å¯¹é½çš„ä¸‰å¤§åŸåˆ™ (3H)

OpenAI æå‡ºäº†è‘—åçš„ **3H åŸåˆ™**ï¼Œè¿™æ˜¯è¡¡é‡ AI æ˜¯å¦â€œå¯¹é½â€çš„é»„é‡‘æ ‡å‡†ï¼š

*   **ğŸ¤ Helpful (æœ‰å¸®åŠ©)**
    *   å‡†ç¡®ç†è§£ç”¨æˆ·æ„å›¾
    *   æä¾›æœ‰ä»·å€¼çš„å›ç­”
    *   ä½†ä¸èƒ½"å¤ªæœ‰å¸®åŠ©"ï¼ˆå¦‚æ•™äººåšç‚¸å¼¹ï¼‰
*   **ğŸ¯ Honest (è¯šå®)**
    *   ä¸ç¼–é€ è™šå‡ä¿¡æ¯
    *   æ‰¿è®¤ä¸çŸ¥é“
    *   æä¾›å¯éªŒè¯çš„ä¿¡æ¯
*   **ğŸ›¡ï¸ Harmless (æ— å®³)**
    *   æ‹’ç»æœ‰å®³è¯·æ±‚
    *   é¿å…åè§å’Œæ­§è§†
    *   ä¸ç”Ÿæˆæ”»å‡»æ€§å†…å®¹

### ä¸ºä»€ä¹ˆå¯¹é½å›°éš¾ï¼Ÿ

| æŒ‘æˆ˜ | å…·ä½“é—®é¢˜ | InstructGPT çš„è§£å†³æ–¹æ¡ˆ |
|:---|:---|:---|
| **ç›®æ ‡å†²çª** | "æœ‰å¸®åŠ©" vs "æ— å®³" æœ‰æ—¶çŸ›ç›¾ | ç”¨äººç±»æ’åºå®šä¹‰ä¼˜å…ˆçº§ |
| **æ•°æ®ç¨€ç¼º** | é«˜è´¨é‡æ ‡æ³¨æ•°æ®å¾ˆè´µ | SFT åªéœ€ 13K æ ·æœ¬ |
| **è¯„ä¼°å›°éš¾** | è‡ªç„¶è¯­è¨€æ²¡æœ‰æ ‡å‡†ç­”æ¡ˆ | ç”¨æ’åºä»£æ›¿æ‰“åˆ† |
| **æ³›åŒ–é—®é¢˜** | æ— æ³•è¦†ç›–æ‰€æœ‰åœºæ™¯ | PPO è®©æ¨¡å‹è‡ªæˆ‘æ¢ç´¢ |

---

<a id="overview"></a>
## ğŸ”„ 3. ä¸‰é˜¶æ®µè®­ç»ƒå…¨æ™¯å›¾

### æ•´ä½“æ¶æ„

![InstructGPT Deep Mechanics](/images/instructgpt/instructgpt_deep_mechanics.png)

> **å›¾è¡¨æ·±åº¦è§£è¯»**ï¼š
> è¿™å¼ å›¾å±•ç¤ºäº† InstructGPT ä¸‰é˜¶æ®µè®­ç»ƒçš„å®Œæ•´æœºåˆ¶ï¼š
> * **æ•°æ®æµå‘**ï¼šä»äººç±»æ ‡æ³¨æ•°æ®åˆ°æœ€ç»ˆçš„å¯¹é½æ¨¡å‹
> * **æ¨¡å‹æ¼”åŒ–**ï¼šGPT-3 â†’ SFT Model â†’ RM â†’ InstructGPT
> * **å…³é”®åˆ›æ–°**ï¼šå°†"å¯¹é½"åˆ†è§£ä¸ºä¸‰ä¸ªæ¸è¿›å¼é˜¶æ®µ

### ä¸‰é˜¶æ®µå¯¹æ¯”è¡¨

| é˜¶æ®µ | è¾“å…¥æ•°æ® | è®­ç»ƒç›®æ ‡ | è¾“å‡ºæ¨¡å‹ | æ•°æ®é‡ |
|:---:|:---|:---|:---|:---:|
| **Stage 1: SFT** | (Prompt, Response) | å­¦ä¼šå¯¹è¯æ ¼å¼ | SFT Model | ~13K |
| **Stage 2: RM** | (Prompt, Rankings) | è®­ç»ƒä»·å€¼è§‚è£åˆ¤ | Reward Model | ~33K å¯¹æ¯”å¯¹ |
| **Stage 3: PPO** | Prompt only | å¼ºåŒ–å­¦ä¹ ä¼˜åŒ– | InstructGPT | ~31K prompts |

### æ ¸å¿ƒæ´å¯Ÿ

1.  **Stage 1 (SFT) çš„ä½œç”¨**ï¼š**å†·å¯åŠ¨**ã€‚æ•™ä¼šåŸºæœ¬æ ¼å¼ï¼Œé™ä½åç»­é˜¶æ®µéš¾åº¦ã€‚
2.  **Stage 2 (RM) çš„ä½œç”¨**ï¼š**å»ºç«‹æ ‡å‡†**ã€‚ç”¨æ’åºä»£æ›¿æ‰“åˆ†ï¼Œé™ä½æ ‡æ³¨éš¾åº¦ï¼Œæä¾›æ›´ç¨³å®šçš„ä¿¡å·ã€‚
3.  **Stage 3 (PPO) çš„ä½œç”¨**ï¼š**è‡ªæˆ‘è¿›åŒ–**ã€‚åœ¨ RM æŒ‡å¯¼ä¸‹æ¢ç´¢ SFT æ•°æ®è¦†ç›–ä¸åˆ°çš„ç©ºé—´ã€‚

---

<a id="sft"></a>
## ğŸ“˜ 4. Stage 1: SFT ç›‘ç£å¾®è°ƒ

### 4.1 æ•°æ®æ ¼å¼ä¸ Tensor ç»´åº¦

æ¯ä¸ªè®­ç»ƒæ ·æœ¬åŒ…å«ä¸¤éƒ¨åˆ†ï¼š`(Prompt, Response)`ã€‚

**æ•°æ®æ¥æº**ï¼š
- OpenAI é›‡ä½£äº† **40 åæ ‡æ³¨å‘˜**
- æ ‡æ³¨å‘˜æ‰‹å†™é«˜è´¨é‡å›ç­”
- æ€»å…±çº¦ **13,000 æ¡**æ ·æœ¬

### 4.2 Tensor ç»´åº¦æµè½¬è¯¦è§£

![Tensor Dimension Flow](/images/instructgpt/tensor_dimension_flow_detailed.png)

> **å›¾è¡¨æ·±åº¦è§£è¯»**ï¼š
> * **è¾“å…¥é˜¶æ®µ**ï¼šToken IDs `[B, S]` ç»è¿‡ Embedding å˜ä¸º `[B, S, H]`ã€‚
> * **Transformer é˜¶æ®µ**ï¼šç»´åº¦ä¿æŒ `[B, S, H]`ï¼Œç»è¿‡ N å±‚å †å ã€‚
> * **è¾“å‡ºé˜¶æ®µ**ï¼šLM Head å°† `[B, S, H]` æ˜ å°„åˆ° `[B, S, V]`ã€‚
> * **Loss è®¡ç®—**ï¼šåªåœ¨ Response éƒ¨åˆ†è®¡ç®—ï¼ŒPrompt éƒ¨åˆ†è¢« Mask æ‰ã€‚

**å¹¶è¡Œè®­ç»ƒæ‚–è®ºï¼šä¸ºä»€ä¹ˆ GPT æ¨ç†æ˜¯ä¸²è¡Œçš„ï¼Œè®­ç»ƒå´æ˜¯å¹¶è¡Œçš„ï¼Ÿ**

åœ¨è®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬æ‹¥æœ‰å®Œæ•´çš„ Ground Truthã€‚æˆ‘ä»¬ä½¿ç”¨ **Teacher Forcing** å’Œ **Causal Mask** æœºåˆ¶ã€‚
*   Prompt: "A B C"
*   Response: "D E"
*   Input: `[A, B, C, D, E]`
*   Label: `[B, C, D, E, EOS]`

æˆ‘ä»¬ä¸€æ¬¡æ€§è¾“å…¥ `A B C D E`ã€‚
*   é¢„æµ‹ B æ—¶ï¼Œåªèƒ½çœ‹ Aã€‚
*   é¢„æµ‹ D æ—¶ï¼Œåªèƒ½çœ‹ A B Cã€‚
*   é¢„æµ‹ E æ—¶ï¼Œåªèƒ½çœ‹ A B C Dã€‚

è¿™ä¸€åˆ‡é€šè¿‡ Attention Mask çŸ©é˜µä¸€æ¬¡æ€§å®Œæˆã€‚

### 4.3 Loss Mask çš„å…³é”®æ€§

**ä¸ºä»€ä¹ˆè¦ Mask Promptï¼Ÿ**
1.  Prompt æ˜¯ç”¨æˆ·è¾“å…¥ï¼Œå·²çŸ¥ä¿¡æ¯ã€‚
2.  æˆ‘ä»¬åªå…³å¿ƒæ¨¡å‹èƒ½å¦**ç”Ÿæˆå¥½çš„ Response**ã€‚
3.  å¦‚æœä¸ Maskï¼Œæ¨¡å‹ä¼šæµªè´¹æ¢¯åº¦å»"è®°å¿†" Promptã€‚

**ä»£ç å®ç°**ï¼š

```python
def sft_loss(model, input_ids, prompt_lengths):
    """
    SFT è®­ç»ƒçš„ Loss è®¡ç®—
    
    Args:
        model: GPT æ¨¡å‹
        input_ids: [B, S] - å®Œæ•´åºåˆ— (prompt + response)
        prompt_lengths: [B] - æ¯ä¸ªæ ·æœ¬çš„ prompt é•¿åº¦
    Returns:
        loss: scalar
    """
    B, S = input_ids.shape
    
    # Forward
    logits = model(input_ids)  # [B, S, V]
    
    # Shift for next-token prediction
    shift_logits = logits[:, :-1, :].contiguous()  # [B, S-1, V]
    shift_labels = input_ids[:, 1:].contiguous()   # [B, S-1]
    
    # Create loss mask
    loss_mask = torch.zeros(B, S-1)
    for i in range(B):
        loss_mask[i, prompt_lengths[i]:] = 1.0
    
    # Compute loss
    raw_loss = F.cross_entropy(
        shift_logits.view(-1, vocab_size),
        shift_labels.view(-1),
        reduction='none'
    ).view(B, S-1)
    
    # Apply mask
    loss = (raw_loss * loss_mask).sum() / loss_mask.sum()
    
    return loss
```

---

<a id="reward-model"></a>
## ğŸ“— 5. Stage 2: Reward Model è®­ç»ƒ

### 5.1 ä¸ºä»€ä¹ˆç”¨æ’åºè€Œéæ‰“åˆ†ï¼Ÿ

**æ‰“åˆ†çš„é—®é¢˜**ï¼š
*   **ä¸€è‡´æ€§å·®**ï¼šæ ‡æ³¨å‘˜ A ä¹ æƒ¯ç»™ 7-9 åˆ†ï¼ˆå®½æ¾ï¼‰ï¼Œæ ‡æ³¨å‘˜ B ä¹ æƒ¯ç»™ 3-5 åˆ†ï¼ˆä¸¥æ ¼ï¼‰ã€‚
*   **éš¾ä»¥æ ¡å‡†**ï¼šç»å¯¹åˆ†æ•°æ²¡æœ‰ç»Ÿä¸€åº¦é‡è¡¡ã€‚

**æ’åº (Ranking) çš„ä¼˜åŠ¿**ï¼š
*   **ä¸€è‡´æ€§é«˜**ï¼šäººç±»åˆ¤æ–­ "A æ¯” B å¥½" çš„ä¸€è‡´æ€§è¿œé«˜äºæ‰“åˆ†ã€‚Cohen's $\kappa$ ä» 0.42 æå‡åˆ° 0.73ã€‚
*   **å»å**ï¼šæ¶ˆé™¤æ ‡æ³¨å‘˜çš„ä¸»è§‚åå·®ã€‚

### 5.2 Bradley-Terry æ¨¡å‹

**æ ¸å¿ƒå‡è®¾**ï¼šæ¯ä¸ªå›ç­”æœ‰ä¸€ä¸ª"çœŸå®è´¨é‡åˆ†æ•°" $r$ï¼Œäººç±»é€‰æ‹© A èƒœè¿‡ B çš„æ¦‚ç‡ï¼š

$$
P(A \succ B) = \frac{e^{r_A}}{e^{r_A} + e^{r_B}} = \sigma(r_A - r_B)
$$

å…¶ä¸­ $\sigma(x) = \frac{1}{1 + e^{-x}}$ æ˜¯ Sigmoid å‡½æ•°ã€‚

### 5.3 Loss å‡½æ•°æ¨å¯¼

ç»™å®šäººç±»æ ‡æ³¨ $(x, y_w, y_l)$ï¼ˆPrompt, Winner, Loserï¼‰ï¼š

**ç›®æ ‡**ï¼šæœ€å¤§åŒ–æ­£ç¡®é¢„æµ‹æ¦‚ç‡
$$
\max P(y_w \succ y_l) = \max \sigma(r_w - r_l)
$$

**ç­‰ä»·äºæœ€å°åŒ–è´Ÿå¯¹æ•°ä¼¼ç„¶**ï¼š
$$
\mathcal{L}_{RM} = -\log \sigma(r_w - r_l) = \log(1 + e^{-(r_w - r_l)})
$$

è¿™å°±æ˜¯ **Binary Cross Entropy Loss**ï¼ˆLogSigmoid Lossï¼‰ã€‚

### 5.4 æ¢¯åº¦åˆ†æ

$$
\frac{\partial \mathcal{L}}{\partial r_w} = -(1 - \sigma(\Delta)) = \sigma(-\Delta) - 1
$$

å…¶ä¸­ $\Delta = r_w - r_l$ã€‚

**å…³é”®ç›´è§‰**ï¼š
*   å¦‚æœ $r_w \gg r_l$ï¼š$\sigma(\Delta) \approx 1$ï¼Œæ¢¯åº¦ $\approx 0$ï¼ˆå·²ç»å¾ˆå¥½äº†ï¼‰ã€‚
*   å¦‚æœ $r_w \approx r_l$ï¼š$\sigma(\Delta) \approx 0.5$ï¼Œæ¢¯åº¦ $\approx -0.5$ï¼ˆæ¨é«˜ $r_w$ï¼‰ã€‚
*   å¦‚æœ $r_w \ll r_l$ï¼ˆåäº†ï¼ï¼‰ï¼šæ¢¯åº¦æ¥è¿‘ -1ï¼ˆå¼ºçƒˆæ¨é«˜ $r_w$ï¼‰ã€‚

### 5.5 æ¶æ„ç»†èŠ‚

**åˆå§‹åŒ–**ï¼šä» SFT æ¨¡å‹å¤åˆ¶å‚æ•°ã€‚
**ä¿®æ”¹**ï¼š
*   å»æ‰ LM Head (Linear: `[H, V]`)ã€‚
*   æ¢æˆ Reward Head (Linear: `[H, 1]`)ã€‚
**è¾“å‡º**ï¼šå–æœ€åä¸€ä¸ª token çš„ hidden stateï¼Œæ˜ å°„åˆ°æ ‡é‡åˆ†æ•°ã€‚

---

<a id="ppo"></a>
## ğŸ“• 6. Stage 3: PPO å¼ºåŒ–å­¦ä¹ 

### 6.1 å››ä¸ªæ¨¡å‹çš„è§’è‰²

![PPO Gradient Flow](/images/instructgpt/ppo_gradient_flow.png)

> **å›¾è¡¨æ·±åº¦è§£è¯»**ï¼š
> * **Actor (å¯è®­ç»ƒ)**ï¼šå½“å‰ç­–ç•¥ï¼Œè´Ÿè´£ç”Ÿæˆå›ç­”ã€‚
> * **Critic (å¯è®­ç»ƒ)**ï¼šä»·å€¼ä¼°è®¡å™¨ï¼Œé¢„æµ‹èƒ½æ‹¿å¤šå°‘åˆ†ã€‚
> * **Ref Model (å†»ç»“)**ï¼šSFT åŸå§‹æ¨¡å‹ï¼Œè®¡ç®— KL æ•£åº¦çº¦æŸã€‚
> * **Reward Model (å†»ç»“)**ï¼šæ‰“åˆ†å™¨ï¼Œæä¾›å¥–åŠ±ä¿¡å·ã€‚

| æ¨¡å‹ | ç¬¦å· | å‚æ•° | æ›´æ–° | ä½œç”¨ |
|:---|:---|:---|:---|:---|
| **Actor** | $\pi_\theta$ | 175B | âœ… Yes | å½“å‰ç­–ç•¥ï¼Œç”Ÿæˆå›ç­” |
| **Critic** | $V_\phi$ | 6B | âœ… Yes | ä»·å€¼ä¼°è®¡ï¼Œé¢„æµ‹èƒ½æ‹¿å¤šå°‘åˆ† |
| **Ref Model** | $\pi_{ref}$ | 175B | âŒ Frozen | è®¡ç®— KL æ•£åº¦çº¦æŸ |
| **Reward Model** | $r_\psi$ | 6B | âŒ Frozen | æ‰“åˆ†å™¨ï¼Œæä¾›å¥–åŠ±ä¿¡å· |

**æ˜¾å­˜éœ€æ±‚**ï¼šçº¦ **362B å‚æ•°**ï¼ˆFP16 çº¦éœ€ **724 GB**ï¼‰ã€‚è¿™æ˜¯ RLHF å·¥ç¨‹ä¸Šæœ€å¤§çš„æŒ‘æˆ˜ã€‚

### 6.2 KL Penaltyï¼ˆæ ¸å¿ƒåˆ›æ–°ï¼‰

**é—®é¢˜**ï¼šå¦‚æœåªæœ€å¤§åŒ– RM çš„åˆ†æ•°ï¼Œæ¨¡å‹å¯èƒ½ä¼š **Reward Hacking**ã€‚
ä¾‹å¦‚ RM æœ‰ä¸ª bugï¼šå–œæ¬¢é•¿å¥å­ã€‚æ¨¡å‹å°±ä¼šè¾“å‡º "AI AI AI..." æ¥éª—åˆ†ã€‚

**è§£å†³æ–¹æ¡ˆ**ï¼šKL Penalty
$$
R_{total} = r_{RM}(x, y) - \beta \cdot \text{KL}(\pi_\theta(y|x) \parallel \pi_{ref}(y|x))
$$

**ç›´è§‰**ï¼š
*   å¦‚æœ Actor ç”Ÿæˆçš„å›ç­”å’Œ SFT æ¨¡å‹å·®å¤ªå¤šï¼ŒKL ä¼šå¾ˆå¤§ã€‚
*   æ€» Reward è¢«æ‰£åˆ†ï¼Œè¿«ä½¿ Actor ä¸è¦åç¦» SFT çš„åˆ†å¸ƒå¤ªè¿œã€‚
*   $\beta$ æ˜¯æƒè¡¡ç³»æ•°ï¼ˆé€šå¸¸ 0.01 - 0.1ï¼‰ã€‚

### 6.3 PPO Clip Loss

**åŸå§‹ Policy Gradient çš„é—®é¢˜**ï¼šæ›´æ–°æ­¥é•¿ä¸ç¨³å®šï¼Œå¯èƒ½ä¸€æ­¥æ¯æ‰æ•´ä¸ªç­–ç•¥ã€‚

**PPO çš„è§£å†³æ–¹æ¡ˆ**ï¼šé™åˆ¶æ¯æ¬¡æ›´æ–°çš„å¹…åº¦ã€‚

å®šä¹‰é‡è¦æ€§é‡‡æ ·æ¯”ç‡ï¼š
$$
r_t(\theta) = \frac{\pi_\theta(a_t|s_t)}{\pi_{old}(a_t|s_t)}
$$

**Clipped Objective**ï¼š
$$
\mathcal{L}^{CLIP}(\theta) = \mathbb{E}_t \left[ \min(r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t) \right]
$$

**ç›´è§‰è§£é‡Š**ï¼š
*   å¦‚æœ $A_t > 0$ï¼ˆå¥½åŠ¨ä½œï¼‰ï¼šå¢åŠ æ¦‚ç‡ï¼Œä½†ä¸è¶…è¿‡ $(1+\epsilon)$ å€ã€‚
*   å¦‚æœ $A_t < 0$ï¼ˆååŠ¨ä½œï¼‰ï¼šå‡å°‘æ¦‚ç‡ï¼Œä½†ä¸ä½äº $(1-\epsilon)$ å€ã€‚

### 6.4 GAE (Generalized Advantage Estimation)

**é—®é¢˜**ï¼šå¦‚ä½•ä¼°è®¡"è¿™ä¸ªåŠ¨ä½œæœ‰å¤šå¥½"ï¼Ÿæˆ‘ä»¬éœ€è¦å¹³è¡¡**åå·® (Bias)** å’Œ **æ–¹å·® (Variance)**ã€‚

$$
A_t^{GAE} = \sum_{l=0}^{\infty} (\gamma \lambda)^l \delta_{t+l}
$$

å…¶ä¸­ $\delta_t = r_t + \gamma V(s_{t+1}) - V(s_t)$ æ˜¯ TD Errorã€‚

**$\lambda$ çš„ä½œç”¨**ï¼š
*   $\lambda = 0$ï¼šçº¯ TDï¼ˆä½æ–¹å·®ï¼Œé«˜åå·®ï¼‰ã€‚
*   $\lambda = 1$ï¼šçº¯ MCï¼ˆé«˜æ–¹å·®ï¼Œä½åå·®ï¼‰ã€‚
*   $\lambda = 0.95$ï¼šInstructGPT çš„é€‰æ‹©ã€‚

---

<a id="math"></a>
## ğŸ“ 7. å®Œæ•´æ•°å­¦æ¨å¯¼

### 7.1 SFT Loss æ¨å¯¼

**ç›®æ ‡**ï¼šæœ€å¤§åŒ– Response çš„å¯¹æ•°ä¼¼ç„¶ã€‚

$$
\mathcal{L}_{SFT} = -\sum_{t \in \text{Response}} \log P_\theta(x_t | x_{<t})
$$

**å¸¦ Mask çš„å®ç°**ï¼š

$$
\mathcal{L}_{SFT} = -\frac{\sum_{t} m_t \cdot \log P_\theta(x_t | x_{<t})}{\sum_{t} m_t}
$$

å…¶ä¸­ $m_t = \mathbf{1}[t \in \text{Response}]$ã€‚

### 7.2 RM Loss æ¨å¯¼

**Bradley-Terry å‡è®¾**ï¼š

$$
P(y_w \succ y_l | x) = \sigma(r_\theta(x, y_w) - r_\theta(x, y_l))
$$

**æœ€å¤§ä¼¼ç„¶ä¼°è®¡**ï¼š

$$
\max_\theta \log P(y_w \succ y_l) = \log \sigma(r_w - r_l)
$$

**Lossï¼ˆå–è´Ÿå·ï¼‰**ï¼š

$$
\mathcal{L}_{RM} = -\log \sigma(r_w - r_l) = \log(1 + e^{-(r_w - r_l)})
$$

### 7.3 PPO Loss æ¨å¯¼

**Policy Gradient å®šç†**ï¼š

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_\theta} [\nabla_\theta \log \pi_\theta(a|s) \cdot A^{\pi}(s, a)]
$$

**Importance Sampling**ï¼š

$$
\nabla_\theta J(\theta) = \mathbb{E}_{\pi_{old}} \left[ \frac{\pi_\theta(a|s)}{\pi_{old}(a|s)} \nabla_\theta \log \pi_\theta(a|s) \cdot A^{\pi}(s, a) \right]
$$

**PPO Clip**ï¼š

$$
\mathcal{L}^{CLIP} = \mathbb{E}_t \left[ \min \left( r_t(\theta) A_t, \text{clip}(r_t(\theta), 1-\epsilon, 1+\epsilon) A_t \right) \right]
$$

---

<a id="code"></a>
## ğŸ’» 8. ç”Ÿäº§çº§ä»£ç å®ç°

### 8.1 å®Œæ•´ PPO Trainer

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class Config:
    vocab_size = 50257
    hidden_size = 768
    max_seq_len = 512
    batch_size = 4
    beta = 0.1      # KL penalty coefficient
    gamma = 1.0     # Discount factor
    lam = 0.95      # GAE lambda
    epsilon = 0.2   # PPO clip range
    lr_actor = 1e-6
    lr_critic = 1e-5

class PPOTrainer:
    def __init__(self, actor, critic, ref_model, reward_model):
        self.actor = actor
        self.critic = critic
        self.ref_model = ref_model  # Frozen
        self.reward_model = reward_model  # Frozen
        
        self.actor_optimizer = torch.optim.Adam(
            actor.parameters(), lr=Config.lr_actor
        )
        self.critic_optimizer = torch.optim.Adam(
            critic.parameters(), lr=Config.lr_critic
        )
    
    def compute_gae(self, rewards, values, masks):
        """è®¡ç®— GAE"""
        gae = 0
        advantages = torch.zeros_like(rewards)
        
        for t in reversed(range(rewards.size(1))):
            if t == rewards.size(1) - 1:
                next_value = 0
            else:
                next_value = values[:, t + 1]
            
            delta = rewards[:, t] + Config.gamma * next_value * masks[:, t] - values[:, t]
            gae = delta + Config.gamma * Config.lam * masks[:, t] * gae
            advantages[:, t] = gae
        
        return advantages
    
    def train_step(self, prompts, responses):
        """å•æ­¥ PPO è®­ç»ƒ"""
        
        # ========== Phase 1: Rollout ==========
        with torch.no_grad():
            # Actor çš„ log probabilities
            actor_logits = self.actor(prompts, responses)
            old_log_probs = F.log_softmax(actor_logits, dim=-1)
            
            # Ref Model çš„ log probabilities
            ref_logits = self.ref_model(prompts, responses)
            ref_log_probs = F.log_softmax(ref_logits, dim=-1)
            
            # KL Divergence
            kl_div = (old_log_probs - ref_log_probs).sum(dim=-1)  # [B, T]
            
            # Reward Model åˆ†æ•°
            rm_score = self.reward_model(prompts, responses)  # [B]
            
            # ç»„åˆ Reward: r = RM - beta * KL
            rewards = -Config.beta * kl_div  # [B, T]
            rewards[:, -1] += rm_score  # æœ€åä¸€æ­¥åŠ ä¸Š RM score
        
        # ========== Phase 2: Advantage Estimation ==========
        values = self.critic(prompts, responses)  # [B, T]
        masks = torch.ones_like(rewards)
        advantages = self.compute_gae(rewards, values, masks)
        returns = advantages + values.detach()
        
        # ========== Phase 3: PPO Update ==========
        # æ–°çš„ log probabilities
        new_logits = self.actor(prompts, responses)
        new_log_probs = F.log_softmax(new_logits, dim=-1)
        
        # Importance sampling ratio
        ratio = torch.exp(new_log_probs - old_log_probs.detach())
        
        # Clipped surrogate objective
        surr1 = ratio * advantages.unsqueeze(-1)
        surr2 = torch.clamp(
            ratio, 1.0 - Config.epsilon, 1.0 + Config.epsilon
        ) * advantages.unsqueeze(-1)
        
        actor_loss = -torch.min(surr1, surr2).mean()
        
        # Update Actor
        self.actor_optimizer.zero_grad()
        actor_loss.backward()
        self.actor_optimizer.step()
        
        # ========== Phase 4: Value Update ==========
        new_values = self.critic(prompts, responses)
        critic_loss = F.mse_loss(new_values, returns)
        
        self.critic_optimizer.zero_grad()
        critic_loss.backward()
        self.critic_optimizer.step()
        
        return {
            'actor_loss': actor_loss.item(),
            'critic_loss': critic_loss.item(),
            'mean_reward': rewards.mean().item(),
            'mean_kl': kl_div.mean().item()
        }
```

---

<a id="feynman"></a>
## ğŸ“ 9. è´¹æ›¼å¼æ€»ç»“

### 9.1 ç”¨è®­ç»ƒç‹—ç‹—æ¥ç±»æ¯”

**GPT-3 æ˜¯ä¸€åªé‡ç‹—**ï¼šå®ƒçœ‹è¿‡å¾ˆå¤šäººç±»çš„è¡Œä¸ºï¼Œä½†å®ƒåªä¼š"æ¨¡ä»¿"ï¼Œä¸æ‡‚"ä¸ºä»€ä¹ˆ"ã€‚

**InstructGPT æ˜¯ä¸€åªè®­ç»ƒæœ‰ç´ çš„è­¦çŠ¬**ï¼š

1.  **Stage 1: SFTï¼ˆç¤ºèŒƒï¼‰**
    *   è®­çŠ¬å¸ˆäº²è‡ªç¤ºèŒƒï¼š"åä¸‹"åº”è¯¥æ€ä¹ˆåšã€‚
    *   ç‹—ç‹—å­¦ä¼šäº†åŸºæœ¬åŠ¨ä½œã€‚
2.  **Stage 2: RMï¼ˆè®­ç»ƒè£åˆ¤ï¼‰**
    *   è®­çŠ¬å¸ˆå¾ˆå¿™ï¼Œä¸èƒ½æ¯æ¬¡éƒ½äº²è‡ªç¤ºèŒƒã€‚
    *   äºæ˜¯è®­ç»ƒäº†ä¸€ä¸ª"æœºå™¨äººè£åˆ¤"ã€‚è£åˆ¤ä¼šçœ‹ç‹—ç‹—çš„è¡¨ç°ï¼Œæ‰“åˆ†ã€‚
3.  **Stage 3: PPOï¼ˆè‡ªä¸»ç»ƒä¹ ï¼‰**
    *   ç‹—ç‹—ä¸æ–­ç»ƒä¹ ï¼Œè£åˆ¤æ‰“åˆ†ã€‚
    *   ç‹—ç‹—æ ¹æ®åˆ†æ•°è°ƒæ•´åŠ¨ä½œã€‚
    *   ä½†æœ‰ä¸ªé™åˆ¶ï¼šä¸èƒ½ä¸ºäº†æ‹¿é«˜åˆ†å°±å­¦å¥‡æ€ªçš„åŠ¨ä½œï¼ˆKL Penaltyï¼‰ã€‚

### 9.2 ä¸€å¥è¯æ€»ç»“

> **"InstructGPT æ˜¯é€šè¿‡è®© GPT-3 ç©ä¸€ä¸ª'çŒœäººç±»å–œå¥½'çš„æ¸¸æˆï¼Œç„¶åç”¨å¼ºåŒ–å­¦ä¹ ä¸æ–­åˆ·é«˜åˆ†ï¼Œæœ€ç»ˆå­¦ä¼šè¯´äººè¯çš„ç³»ç»Ÿã€‚"**

---

<a id="faq"></a>
## â“ 10. å¸¸è§é—®é¢˜ä¸é¢è¯•è¦ç‚¹

### Q1: InstructGPT å’Œ ChatGPT æ˜¯ä»€ä¹ˆå…³ç³»ï¼Ÿ
**A**: ChatGPT æ˜¯åŸºäº InstructGPT çš„æŠ€æœ¯ï¼Œé’ˆå¯¹å¯¹è¯åœºæ™¯åšäº†ä¼˜åŒ–ã€‚æ ¸å¿ƒæŠ€æœ¯ç›¸åŒï¼ˆRLHFï¼‰ï¼Œä½† ChatGPT åœ¨å¤šè½®å¯¹è¯ã€ä¸Šä¸‹æ–‡è®°å¿†æ–¹é¢åšäº†å¢å¼ºã€‚

### Q2: ä¸ºä»€ä¹ˆ SFT æ•°æ®åªéœ€è¦ 13Kï¼Ÿ
**A**: SFT çš„ç›®çš„æ˜¯"å†·å¯åŠ¨"ï¼Œæ•™ä¼šæ¨¡å‹åŸºæœ¬çš„å¯¹è¯æ ¼å¼å’Œé£æ ¼ã€‚çœŸæ­£çš„æ³›åŒ–èƒ½åŠ›æ¥è‡ª PPO é˜¶æ®µçš„æ¢ç´¢ã€‚è´¨é‡æ¯”æ•°é‡æ›´é‡è¦ã€‚

### Q3: PPO ä¸ºä»€ä¹ˆéœ€è¦å››ä¸ªæ¨¡å‹ï¼Ÿ
**A**:
1.  **Actor**: å­¦ä¹ ç­–ç•¥ (Trainable)ã€‚
2.  **Critic**: ä¼°è®¡ä»·å€¼ï¼Œå‡å°‘æ–¹å·® (Trainable)ã€‚
3.  **Ref Model**: æä¾› KL çº¦æŸï¼Œé˜²æ­¢ Reward Hacking (Frozen)ã€‚
4.  **Reward Model**: æä¾›å¥–åŠ±ä¿¡å· (Frozen)ã€‚

### Q4: KL Penalty çš„ $\beta$ æ€ä¹ˆé€‰ï¼Ÿ
**A**: é€šå¸¸ä» 0.01 å¼€å§‹ã€‚å¦‚æœ KL å¢é•¿å¤ªå¿«ï¼Œå¢å¤§ $\beta$ï¼›å¦‚æœæ¨¡å‹æ”¶æ•›å¤ªæ…¢ï¼Œå‡å° $\beta$ã€‚ä¹Ÿå¯ä»¥ä½¿ç”¨ Adaptive KLã€‚

### Q5: DPO å’Œ RLHF æœ‰ä»€ä¹ˆåŒºåˆ«ï¼Ÿ
**A**: DPO (Direct Preference Optimization) ä¸éœ€è¦è®­ç»ƒ Reward Model å’Œ PPOï¼Œç›´æ¥åœ¨ Preference Data ä¸Šä¼˜åŒ– Policyã€‚DPO æ›´ç®€å•ã€æ›´ç¨³å®šï¼Œä½† RLHF åœ¨æ¢ç´¢èƒ½åŠ›ä¸Šå¯èƒ½æ›´å¼ºã€‚

### Q6: ä¸ºä»€ä¹ˆ RM çš„å‡†ç¡®ç‡åªæœ‰ 70% å·¦å³ä¹Ÿèƒ½è®­ç»ƒå‡ºå¥½æ¨¡å‹ï¼Ÿ
**A**: RM çš„å‡†ç¡®ç‡æ˜¯åœ¨â€œå›°éš¾æ ·æœ¬å¯¹â€ä¸Šæµ‹è¯•çš„ã€‚åªè¦ RM åœ¨å¤§æ–¹å‘ä¸Šæ˜¯æ­£ç¡®çš„ï¼ŒPPO å°±èƒ½æ²¿ç€æ¢¯åº¦çš„æ–¹å‘ä¼˜åŒ–ã€‚RL æ˜¯ä¸€ä¸ªç»Ÿè®¡è¿‡ç¨‹ï¼Œèƒ½å®¹å¿å°‘é‡å™ªå£°ã€‚

---

> ğŸ“ **ä½œè€…**: Caius
> ğŸ”— **å…³è”ç¬”è®°**: [GPTç³»åˆ—æ·±åº¦è§£æ_ä»GPT1åˆ°GPT3], [LoRA_Mastery]
